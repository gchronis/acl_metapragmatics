,corpus_id,sentence,start_idx,end_idx
0,241583436,"As shown in the Table 1 , unsupervised TMN generates less informative responses and suffers from a higher degree of hallucination.",20,21
1,241583436,"We asked annotators to compare the hallucination and informativeness between our method and PLATO-KAG w/o EK, with results summarized in Table 5 .",6,7
2,241583436,It is notable that the tie score of hallucination from PLATO-KAG w/o EK is a little inflated.,8,9
3,241583436,"As comparison, the knowledge keeps its independence and integrity in our response generation, which helps reduce the hallucination.",19,20
4,241583436,"To evaluate the criteria of hallucination, the human annotators were provided with referenced knowledge and allowed to use search engine to check the factual correctness.",5,6
5,237940600,Example 4: Dialogue sample with a bad domain focus and a hallucination.,12,13
6,237940440,-Incorrect Slot Values (Inc. Slot): Whether the response includes any slot values not given in the MR (a specific type of hallucination).,24,25
7,237940440,"On the other hand, it has a higher deletion and hallucination error rate since during decoding time, BSWD is free to insert or drop content in order to achieve the desired style.",11,12
8,247613284,"2021) explored tackling knowledge hallucination by incorporating control codes, which act as stylistic controls that encourage the model to generate responses that are faithful to the provided evidence.",5,6
9,227217599,The following error from FACT-LG-ST-RMR shows multiple hallucination of the exhibit item's creation time.,13,14
10,227217599,"Though the RST models yielded less dramatic improvements on comparisons in the challenge set, it is worth emphasizing that the RST models produce significantly fewer repetitions, omissions and hallucination compared to the FACT models (Figs.",30,31
11,237420426,"The second group is linking errors, including, e.g., hallucination of entities, return of unsorted results, and missing an aggregation step.",11,12
12,235294035,"2019) , the most likely token when decoded may not be factually accurate, or supported by the retrieved evidence, commonly referred to as a hallucination (Rohrbach et al.,",27,28
13,235294035,"2020) could be applied, however, task-specific fine-tuning would still be required to condition the generation based on the factual error to mitigate hallucination.",29,30
14,231802262,"Between the annotators, a combined total of 524 examples were labelled for both for hallucination and repetition.",15,16
15,231802262,"The overall performance was very high: for 95% of the inputs, both systems exhibited no signs of hallucination or repetition.",20,21
16,231802262,The second example contains a hallucination for both the length normalised and reranked systems -the input clearly states that the customer rating was '3 out of 5'.,5,6
17,245218843,But it is still challenging to improve the quality of summaries generated by different models and decrease the hallucination at the same time.,18,19
18,233296711,"Sensitivity of CLIP-S to hallucination Prior work has demonstrated that, for many literal description tasks, humans often prefer correctness in captions over specificity (Rohrbach et al.,",6,7
19,233296711,"2018 )'s finding that ""object hallucination can not be always predicted based on the traditional sentence metrics"" using a corpus derived from Shekhar et al. (",7,8
20,218596261,"We also note that GPT2 has high SER for the fully-unseen domain; upon inspection, we see slot hallucination from GPT2 within alarm 1, while Seq2Seq/CVAE never hallucinate.",21,22
21,248218576,"For dialog-level human evaluation, we measure hallucination (Hallu.)",9,10
22,231951460,"For example, state-ofthe-art models trained on existing datasets exhibit entity hallucination, generating names of entities that are not present in the source document.",15,16
23,231951460,We propose a set of new metrics to quantify the entity-level factual consistency of generated summaries and we show that the entity hallucination problem can be alleviated by simply filtering the training data.,24,25
24,231951460,We call this the entity hallucination problem.,5,6
25,231951460,"In fact, the source document mentioned a study involving people in Italy and Netherlands; ""UK"" was a result of model hallucination.",24,25
26,231951460,Precision-source: We propose precision-source (prec s ) to quantify the degree of hallucination with respect to the source: prec s = N (h ∩ s)/N (h).,18,19
27,231951460,Low prec s means hallucination is severe.,4,5
28,231951460,We hypothesize that the hallucination problem is largely caused by the training data itself.,4,5
29,231951460,"This way, we ensure that our filtered dataset does not contain hallucination of entities (prec s = 1) in the ground truth summary.",12,13
30,231951460,"As we shall see in Table 3 , entity-based data filtering reduces hallucination of the trained model and the effect is especially significant in the XSUM dataset.",14,15
31,231951460,"Precision-target and recall-target: Although the precision-source (prec s ) metric quantifies the degree of entity hallucination with respect to the source document, it does not capture the entitylevel accuracy of the generated summary with respect to the ground truth summary.",23,24
32,231951460,"In all datasets, data filtering leads to higher prec s scores, indicating that entity hallucination can be alleviated by this simple technique.",16,17
33,231951460,"For example in XSUM, the prec s is increased from 93.6% to 98.2%, indicating a significant reduction in entity hallucination.",23,24
34,231951460,In Table 4 we provide qualitative examples where the model trained on the original data produces hallucination and the entity-level data filtering removes such hallucination.,16,17
35,231951460,In Table 4 we provide qualitative examples where the model trained on the original data produces hallucination and the entity-level data filtering removes such hallucination.,26,27
36,231951460,We propose precision-source score prec s to quantify the degree of entity hallucination.,14,15
37,231951460,We found that the ground truth summaries of the XSUM dataset contain a high level of entity hallucination.,17,18
38,231951460,We propose a simple entity-level data filtering technique to remove such hallucination in the training data.,13,14
39,231951460,"Overall, combining our proposed approaches significantly reduces entity hallucination and leads to higher entity level metrics with minimal degradation of the ROUGE scores.",9,10
40,234358027,"We believe this is caused by the fact that ROUGE is already highly optimized by the MLE model and it is used as initialization for R-C; the ""hard"" examples where the MLE model couldn't produce good ROUGE scores may be inherently problematic (e.g. hallucination in the ground truth summary); focusing on these examples by R-C can therefore make the model weaker on other examples.",50,51
41,234358027,Our model is able to rectify factual errors found in MLE such as 1) entity hallucination and errors (Example 1 and 2 in Table 5 ) and 2) relations and co-reference (see Table 1 and Example 3 in Table 5 ).,16,17
42,234358027,"Another recent work tackles the hallucination problem in abstractive text summarization via post processing on the generated summary (Chen et al.,",5,6
43,248780434,"For example, we would like to know whether breaking the softmax bottleneck could reduce the hallucination of LMs (e.g., outputting queen when the reasonable next words should be king or woman) and increase the coherence of the generated text.",16,17
44,222142312,"In addition, these models inherit the backbone of generative models that suffer from hallucination despite the regularization from complex knowledge graphs or text entailment signals.",14,15
45,222142312,"However, encoder-decoder architectures widely used in abstractive summarization systems are inherently difficult to control and prone to hallucination (Vinyals and Le, 2015; Koehn and Knowles, 2017; Lee et al.,",20,21
46,222142312,"In other words, their decoder regenerates the whole sequence token by token with a pointer-generator, which inherits the backbone of generative models that suffer from hallucination.",29,30
47,231821638,"However it has often been shown to face some adequacy problems such as hallucination, repetition or omission of information.",13,14
48,231821638,It also reduces hallucination by using the constraints that all data records must be used only once.,3,4
49,236881439,"When categorizing the errors made by different systems, it is important to be careful with terms such as hallucination and omission, since these are process-level (pertaining to the system) rather than product-level (pertaining to the output) descriptions of the errors.",19,20
50,236881439,"System problems in end-to-end systems are harder to identify, 4 Furthermore, terms like hallucination may be seen as unnecessary anthropomorphisms that trivialise mental illness.",19,20
51,236881439,"For example, the term 'hallucination' is almost exclusively used with end-to-end systems, as it is common for these systems to add phrases in the output text that are not grounded in the input.",6,7
52,236881439,"The authors find that, despite their efforts to prevent it, their model still suffers from hallucination.",17,18
53,236881439,"They identify two kinds of hallucination: either the model associates an existing value with the wrong data point, or it simply predicts an irrelevant token.",5,6
54,218869893,"However, also Latin or Greek rooted words can be very common in our daily language thus be easily understood by medical non experts, such as hallucination.",27,28
55,235294302,We found that through hyperparameter tuning we could largely eliminate the hallucination from UQAT5 answers but we were unable to make BART more abstractive.,11,12
56,249642728,"We discuss challenges related to productionization and how these models can be used to mitigate hallucination and bias, both pressing challenges in the field of text generation.",15,16
57,249642728,2020) are less prone to different types of hallucination since the models cannot produce arbitrary outputs.,9,10
58,249642728,"Specifically, they reduce the likelihood of different types of hallucination (Malmi et al.,",10,11
59,202541578,"Furthermore, both models: c) require much less training data compared to the seq2seq models, d) are more controllable and interpretable than seq2seq models due to the small vocabulary of edit operations, e) are less prone to typical seq2seq model errors, such as hallucination.",50,51
60,202541578,"The likelihood of others, such hallucination and abrupt sentence ending, is at least greatly reduced.",6,7
61,202541578,"Qualitative analysis of the model outputs suggests that our tagging approach is less affected by the common errors of the seq2seq models, such as hallucination and abrupt sentence ending.",25,26
62,249625525,"2021) , which investigates factual hallucination in dialogue retrieval-generation models with a factbased knowledge base such as Wikipedia.",6,7
63,236460192,"Existing approaches for the Table-to-Text task suffer from issues such as missing information, hallucination and repetition.",18,19
64,236460192,"led to improved fluency, current systems still suffer from issues such as lack of coverage (where the generated text misses information present in the source), repetition (where the generated text repeats information) and hallucination (where the generated text asserts information not present in the source) (Lee et al.,",39,40
65,236460192,We still observe hallucination from both IRL and RL fine-tuned models.,3,4
66,248779912,"To minimize the changes of hallucination in the generation, we also delexicalize words in the an- et al.,",5,6
67,233204359,"The only hallucination the evaluators found is a numerical error, reported by Pegasus-CNN in the following generated description: Patient's weight has dropped from 201 pounds to 201 pounds.",2,3
68,218487229,"Though usually excelling at producing fluent text, it suffers from the problem of information missing, repetition and ""hallucination"".",20,21
69,218487229,"In practice, it often suffers from the problem of information missing, repetition and ""hallucination"" (Dušek et al.,",16,17
70,218487229,"2) Explicitly building the correspondence between segments and data records can potentially reduce the hallucination, as noted in (Wu et al.,",15,16
71,218487229,"Our model always attends to one single slot instead of averaging over the whole inputs, the chance of hallucination is largely reduced.",19,20
72,218487229,"Results on WebNLG Discussions In summary, our models generates most diverse outputs, achieves similar or better performances in word-overlap automatic metrics while significantly reduces the information hallucination, repetition and missing problems.",30,31
73,218487229,An example of hallucination is shown in Table 4 .,3,4
74,218487229,"The proposed model significantly alleviates the information hallucination, repetition and missing problems without sacrificing the fluency and diversity.",7,8
75,189999604,"When an opening bracket token (e.g., [name) is generated, it is not accepted if it isn't a child of the current parent node in the input tree, or has already been generated in the current subtree, thereby preventing repetition and hallucination of arguments or acts.",48,49
76,189999604,"The remaining examples all contained legitimate model errors, like content hallucination, or a wrong slot being produced despite a correct non-terminal.",11,12
77,189999604,"Compared to our approach, grammar-based realizers can prevent hallucination entirely, though at the expense of developing an explicit grammar.",11,12
78,226281842,"Tree accuracy is also used in production to guard against hallucination: if tree accuracy fails, we fall back on templates to ensure correct response generation, even if it is less natural.",10,11
79,236460018,"We propose a copy mechanism for pre-trained models, that uses general placeholders covering table contents and results of pre-executed numerical operations to avoid fact hallucination. •",29,30
80,236460018,"As a final dataset, we used only sentences classified as belonging to the data description category to reduce fact hallucination. •",20,21
81,247594360,"However, besides the well-known hallucination issue, the implicit knowledge learned through language modeling objective over text struggles at reflecting up-to-date knowledge from text and structured data for answering open-domain questions.",7,8
82,247594360,2020) propose customized loss functions to reduce model hallucination during generation.,9,10
83,236477402,"We also propose a modality hallucination approach to impute review representations at test time, providing the first realistic evaluation framework for this challenging task. * *",5,6
84,236477402,"Inspired by missing modality hallucination methods (Hoffman et al.,",4,5
85,236477402,"A comparison among the different loss functions, without hallucination, is summarised in Table 2 .",9,10
86,236477402,"Table 3 : We report the review hallucination results; for the uncertainty-aware methods, we used λ sof t ≡ 0.8.",7,8
87,236477402,"The HLA method with hallucination achieves both the best performance in this experiment, and the largest relative improvement (t-test, p < .05) upon the abstract-only case, as shown in Table 4 .",4,5
88,236477402,"Hard-labels with hallucination is the method that performs relatively closest to its ceiling performance, but this can be explained by the ceiling being comparatively low in the hard-labels case.",4,5
89,236477402,"In terms of a final method recommendation: we recommend the learnt attenuation based HLA, due to its better performance along with modality hallucination and the fact that it does not require the presence of multiple reviewer recommendations even at training-time.",24,25
90,236477402,"In cases where the true reviews cannot be assumed to be present in test/deployment, our hallucination approach allows for improvement of results compared to excluding reviews altogether.",19,20
91,236477402,"When using only abstracts, PCC drops to .08 and .04 respectively, whereas by using hallucination we observe .08 and .05, indicating that the true review representations are required for good uncertainty prediction.",16,17
92,236477402,"Even with the application of our review representation hallucination, the performance gap from the ceiling set by using the true review representations is still high.",8,9
93,236477402,"Finally, we have shown in our study that only a representation of the abstract is required as the input both for acceptance and hallucination modelling.",24,25
94,208527035,"However, note that in (6a), the hallucination took place in the RNN model, while the other two Transformer models cope with it and find the right place to finish the inference.",10,11
95,232417140,"The first example shows the problem of hallucination (Koehn and Knowles, 2017) , hinting towards negative impacts of domain shifts.",7,8
96,229923747,"However, as the models are trained on single domains, when facing out-of-domain test sentences, they could suffer from hallucination, i.e. produce translations unrelated to the input sentences.",25,26
97,207870601,"Besides that, some further problems of NMT that do not seem to be related to coreference at first glance (such as translation of unknown words and proper names or the hallucination of additional words) cause coreference-related errors.",32,33
98,202770038,"Each pair is assigned a 0-1 score to indicate (1) whether the target is fluent in grammar, (2) whether the target faithfully conveys the source information without hallucination and (3) whether the target is considered human-generated or machine-generated (like a 0/1 Turing test).",34,35
99,237571703,"Second, our back-translation technique also inherit some of the problems of machine translation generated texts like hallucination (Raunak et al.,",19,20
100,247694170,"2015) , adopting the entailment and contradiction labels, and splitting the neutral label into three sub-categories: hallucination, offtopic responses and generic responses.",21,22
101,247694170,Most details are correct and the hallucination is subtle.,6,7
102,250390859,"2015) , adopting the entailment and contradiction labels, and splitting the neutral label into three subcategories: hallucination, off-topic responses and generic responses.",19,20
103,250390859,Most details are correct and the hallucination is subtle.,6,7
104,220047809,"2018) show that this dataset is large enough, at 3.3M examples, to significantly alleviate content hallucination.",19,20
105,220047809,"As a result, current captioning models trained on Conceptual Captions avoid content hallucination but also introduce different, more subtle and harderto-detect issues related to possible context hallucinations (i.e., is this actually the new general manager?)",13,14
106,220047809,"However, the rate of Meta captions increases by around 25% in the model outputs, which points to potential context hallucination effects introduced by these models.",22,23
107,237941123,"During training, we exploit occurrence planning of personal named entities and coreference information to improve temporal coherence and to minimize hallucination in neural generation.",21,22
108,219966792,"Most teams demonstrate utility of data hallucination and augmentation, ensembles, and multilingual training for low-resource languages.",6,7
109,219966792,"This model is further improved (flexica-03-1) by introducing a data hallucination technique which is based on phonotactic modelling of extremely low-resource languages (Shcherbakov et al.,",14,15
110,219966792,LTI and Flexica teams also observed positive effects of multilingual training and data hallucination on low-resource languages.,13,14
111,219966792,Data augmentation techniques such as hallucination helped fill in the gaps and allowed networks to generalize to unseen inputs.,5,6
112,233231308,The dashed curves in Figure 6 show that ReLA-g associates such hallucination pairs with clearly higher null rate for the cross-attention across different layers.,13,14
113,226283913,It is well-known that abstractive summaries are subject to hallucination-including material that is not supported by the original text.,11,12
114,226283913,"It was in the previous eruption that more than a dozen people were killed, hence a hallucination of at least 11 people killed and at least 20 injured in the new eruption.",17,18
115,226283913,"Such high levels of factual hallucination raise serious concern about the usefulness of abstractive summarization, especially if one believes that summaries (whether extractive or abstractive) should contain a mixture of general and specific information (Louis and Nenkova, 2011) .",5,6
116,226283913,"Moreover, unlike people's names (which are also frequently hallucinated), quantity entities are rarely referred to anaphorically, avoiding the need to resolve anaphoric expressions, making them an excellent testbed for the study of hallucination.",39,40
117,226283913,2019) have argued convincingly that ROUGE is inadequate as a measure of hallucination and factual correctness.,13,14
118,4717344,g. Jo's leaving was a hallucination.,6,7
119,216867290,"Neural models have a known problem of hallucination (Rohrbach et al.,",7,8
120,216867290,"They use latent variable for products and reviews to address the hallucination issue, while at the same time allowing it to capture information from the set of reviews on the same entity.",11,12
121,248506182,"Conclusion In this paper, we introduce an embedding hallucination method for data augmentation for few-shot learning, based on cWGAN.",9,10
122,53034346,We also observed that the LSTM output sometimes presents hallucination (overgeneration) cases.,9,10
123,214802856,"This is especially relevant with neural sequence-to-sequence models, as they are highly susceptible to memorization (Aharoni and Goldberg, 2018) and hallucination (Lee et al.,",28,29
124,236777459,"We find that unclear user utterances are a major source of generative errors such as ignoring, hallucination, unclearness and repetition.",17,18
125,236777459,"Among these examples, the more basic errors (repetitive, unclear, hallucination, ignoring) become less common, and the errors relating to reasoning or social abilities (redundant, logical, insulting) are more common.",13,14
126,236777459,"Other positive relationships include unclear user with Misheard, repetitive and redundant bot with Repetition, unclear bot with Clarification, bot hallucination and ignoring with Misheard, and bot insulting with Criticism.",22,23
127,14633379,Example generated text Content selection and hallucination We randomly sample 50 entities from DEV and manually annotate the Wikipedia and system text.,6,7
128,235186832,2020d) introduce new inputoutput matching and embedding similarity losses to alleviate hallucination issues.,12,13
129,215768688,"Finally, we discuss common errors among sentences which received low scores from annotators, identifying issues for future researchers to address including hallucination, anonymization, and repetition.",23,24
130,215768688,"We recommend attempting to find solutions to the common issues that led to low scores even from state-of-the-art systems, such as anonymization of infrequent concepts, unnecessary repetition of words, and hallucination.",39,40
131,247939224,"These errors will result in sequences that degenerate over the sequence length resulting in incoherent text, lack of vocabulary diversity, and detachment from the source sequence resulting in hallucination, and/or word-and phrase-level repetition.",30,31
132,227230264,We also propose a method to combine the analogy-motivated approach with data hallucination or augmentation.,14,15
133,227230264,"2020) , designing data hallucination techniques to generate synthetic data based on existing labeled data (Silfverberg et al.,",5,6
134,227230264,"Model architecture engineering and data hallucination and augmentation techniques have seen consistent performance gains in current literature, but the effect of cross-lingual transfer for morphological inflection is less consistent.",5,6
135,227230264,"2020; Singer and Kann, 2020; Murikinati and Anastasopoulos, 2020; Scherbakov, 2020 ) and the baseline system show the positive effect of data hallucination, which has also been evidenced by previous studies (Silfverberg et al.,",28,29
136,227230264,"Is data hallucination complementary to augmenting training data by using multiple source forms, or are the two strategies orthogonal, particularly in low-resource scenarios?",2,3
137,227230264,"For the second question, we conduct an experiment to combine the previous strategy with a data hallucination approach, and find that the two approaches are complementary in general, and that using both approaches is especially helpful when the training data is extremely limited (< 1,000 training examples).",17,18
138,227230264,"1-source+hallucination model The approach of using each individual form or each pair of forms in a paradigm to predict a target form is essentially a method to augment the training data, but this data augmentation approach is different from the data augmentation method of ""hallucination,"" where synthetic ""plausible"" data are generated based on known labeled data and added to the training data for the morphological inflection model.",48,49
139,227230264,"Both augmentation by reformatting training data and data hallucination have produced improvement in neural model performance for morphological inflection in low-resource settings, but to our knowledge no work has analyzed whether the two data augmentation approaches are complementary to each other.",8,9
140,227230264,"Each type of architecture is trained in four different ways with identical hyperparameters: training one model for each language with and without data hallucination, or training one model per language group with and without data hallucination.",24,25
141,227230264,"Each type of architecture is trained in four different ways with identical hyperparameters: training one model for each language with and without data hallucination, or training one model per language group with and without data hallucination.",37,38
142,227230264,"Though the 2-source model has an average accuracy lower than baseline trm-hal-single and has a larger variance, its average accuracy is still higher than the baseline Transformer model trained without data hallucination, i.e. baseline trm-single.",38,39
143,227230264,"The analogy-motivated approach of reformatting given data (Liu and Hulden, 2020) and the data hallucination approach (Anastasopoulos and Neubig, 2019) are complementary and can be profitably combined to improve the result.",19,20
144,227230264,"This is true for the data hallucination approach as well, as is shown in plot (b).",6,7
145,227230264,"Plot (d) shows that reformatting the data has similar effects as data hallucination, but that reformatting is in general more effective.",14,15
146,227230264,This provides additional support to the benefit of combining the analogy-motivated data reformatting approach and the data hallucination approach.,19,20
147,227230264,We also see that the contribution of data augmentation by either data reformatting with more analogy sources or data hallucination tends to decrease as the average paradigm completion rate increases.,19,20
148,227230264,"Still, data reformatting by analogy demonstrates the advantage over data hallucination, as reflected by the line for the 1-source+1-crosstable model being above the regression line for the trm-hal-single model in plot (d) and the cross of the trend lines in plot (a) and plot (c) coming at a higher paradigm completion rate level than in plot (b).",11,12
149,227230264,"lv1out: leave-1-out, 1src+1: 1-source+1-crosstable, 1src+2: 1-source+2-crosstable, 1src+h: 1-source+hallucination, sing, h.sing, shrd, h.shrd are results copied from SIGMORPHON 2020 shared task 0 results for the Transformer baseline models trained per language without (sing) or with (h.sing) data hallucination, or per language group without (shrd) or with (h.shrd) data hallucination.",63,64
150,227230264,"lv1out: leave-1-out, 1src+1: 1-source+1-crosstable, 1src+2: 1-source+2-crosstable, 1src+h: 1-source+hallucination, sing, h.sing, shrd, h.shrd are results copied from SIGMORPHON 2020 shared task 0 results for the Transformer baseline models trained per language without (sing) or with (h.sing) data hallucination, or per language group without (shrd) or with (h.shrd) data hallucination.",79,80
151,227230264,We further explore whether the data reformatting approach is orthogonal to data hallucination.,12,13
152,215754150,It also employs a weakly-supervised Semantic Fidelity Classifier (SFC) to detect and avoid generation errors (such as hallucination and omission).,22,23
153,215754150,"Similarly, in semantic fidelity classification, we aim to determine if the text is ""accurate"" or contains some ""omission"", ""repetition"", ""hallucination"", or ""value errors"" given the data.",30,31
154,238260199,We found that hallucination is a frequent problem for PEGASUS-M. This is hardly surprising given that the model has no way of knowing that it is expected to generate a factual headline summarizing the input.,3,4
155,226262296,"Second, we augment the training data by generating 10,000 artificial instances with the implementation in the SIGMORPHON 2020 shared task 0 baseline of the data hallucination method proposed by (Anastasopoulos and Neubig, 2019) .",26,27
156,233436947,Another potential area of improvement could be the usage of different data hallucination techniques like in Shcherbakov et al. (,12,13
157,233231232,"While established data augmentation techniques can be employed to alleviate this shortcoming by introducing a copying bias through hallucinating synthetic new word forms using the alphabet in the language at hand, our experiment results show that, to be more effective, the hallucination process needs to pay attention to substrings of syllable-like length rather than individual characters.",44,45
158,233231232,"We show that when inducing a copy bias in the model by hallucinating new lemmata, or by hallucinating new inflected forms, the method of hallucination is much more effective if it is sensitive to substrings of syllable-like length rather than individual characters or stems.",26,27
159,233231232,"Our best models achieve substantial improvement upon earlier state-of-the-art data hallucination methods (Silfverberg et al.,",16,17
160,233231232,"The ""common practice"" is represented by g l e i c h e _ n g l e i c h e s t Generate dummy-stem and replace stem part Find common substrings as stem Align lemma and target-form Output e i z i e n t s a t i _ n e i z i e n t s a t i s t g l e i c h e _ n g l e i c h e s t gleichen V;SBJV;PRS;2;PL gleichest eizientsatin V;SBJV;PRS;2;PL eizientsatist (2) hallucination  previous years' shared tasks and related work (Cotterell et al.,",99,100
161,233231232,2017) found that it is very effective to augment training data in low-resource situations with a data hallucination approach by replacing a hypothesized stem of the training triples with a random string.,20,21
162,233231232,Anastasopoulos and Neubig (2019) improves this data hallucination method by taking into discontinuous stems into consideration as well; this is the best data hallucination method so far.,9,10
163,233231232,Anastasopoulos and Neubig (2019) improves this data hallucination method by taking into discontinuous stems into consideration as well; this is the best data hallucination method so far.,26,27
164,233231232,"Specifically, both data hallucination methods (illustrated in Figure 2 (b) ) take as input a triple from the training set, aligns the lemma and the target form with the alignment method from SIGMORPHON 2016 shared task baseline (Cotterell et al.,",4,5
165,233231232,"The substring-based data hallucination we propose, +hall-2k-substr, achieves accuracies which are substantially higher than other methods for most languages.",5,6
166,233231232,The consistent advantage of +hall-2k-substr implies that substrings of syllable-like length is more helpful than individual characters for data hallucination.,24,25
167,233231232,"Common practice vs ""wug test"" Figure 1 plots the Transformer accuracies with standard deviations in the common-practice setting, vanilla ""wug test""-like setting, and ""wug test""-like setting with data augmentation by the substring-based data hallucination methods (+hall-2k-substr) .",43,44
168,233231232,"We propose to augment the training data with substring-based data hallucination, and achieve substantial improvement over previous data hallucination methods.",12,13
169,233231232,"We propose to augment the training data with substring-based data hallucination, and achieve substantial improvement over previous data hallucination methods.",21,22
170,233231232,"6  The performance of the encoder-decoder with exact hard monotonic attention model for the original shared task setup, the ""wug test""-like setup with or without our best data hallucination augmentation is presented in Figure 5 .",33,34
171,233231232,"Wu and Cotterell, 2019) in the common-practice setting (left), ""wug test""-like setting (middle), and ''wug test""-like setting with our best data hallucination method (right) Figure 6 : ""Wug test"" results by the encoder-decoder with exact hard monotonic attention model (Wu and Cotterell, 2019) , with or without different data augmentation methods.",33,34
172,227230911,"Despite eventual hallucination problems, human evaluation showed that our model has more consistent performance, improving overall quality.",2,3
173,227230911,"We assume that part of this result is related to known issues in semantic neural models, such as hallucination, which could influence both automatic and human evaluation results, in particular for unseen entities.",19,20
174,247165895,"One translation problem associated with over-estimation is hallucination (Wang and Sennrich, 2020) , where NMT models generate fluent translation but is unrelated to the input.",9,10
175,247165895,"2018) to evaluate the model's tendency of generating hallucination under noisy input, to which NMT models are highly sensitive (Belinkov and Bisk, 2018) .",10,11
176,247165895,2018) to count a translation as hallucination under perturbation (HUP) when: (1) BLEU between reference sentence and translation of unperturbed sentence is bigger than 5 and (2) BLEU between the translation of perturbed sentence and the translation of unperturbed sentence is lower than 3.,7,8
177,247165895,We calculate the percentage of hallucination as the HUP score.,5,6
178,196183567,"Albeit being appealing for producing fluent and diverse sentences, neural NLG models often suffer from a severe issue of content hallucination (Reiter, 2018a) , which refers to the problem that the generated texts often contain information that is irrelevant to or contradicted with the input.",21,22
179,196183567,"In this paper, we propose a simple, automatic recipe towards reducing hallucination for neural surface realisers by enhancing the semantic equivalence between pairs of MRs and utterances.",13,14
180,196183567,"Discussion In this paper, we present a simple recipe to reduce the hallucination problem in neural language generation: introducing a language understanding module to implement confidence-based iterative data refinement.",13,14
181,222272417,"The main shortcoming is unnecessity for extractive models, and omission and intrinsic hallucination for abstractive models.",13,14
182,222272417,"Another common hallucination involves rhetorical but irrelevant sentences, e.g., ""Click here for more news"".",2,3
183,227217079,"Additionally, a direct comparison between end-to-end and pipeline approaches suggests that pipeline approaches lead to improved output quality, and decreases data hallucination and data omission; two challenges for datasets compiled using unedited texts from publicly available sources (Castro Ferreira et al.,",27,28
184,235253772,"Filippova (2020) introduces a ""hallucination knob"" to reduce the amount of hallucinations in the generated text.",7,8
185,15828439,"This is all well-known, and is treated in most grammatical frameworks by hallucinating an item in the canonical position, and then remembering that hallucination up to the point at which the out-ofplace item is encountered.",27,28
186,15828439,"Exactly how the hallucination is remembered varies from one framework to another, with uni cation grammars generally carrying information about it on a category-valued feature usually called slash.",3,4
187,15828439,"Why go to the bother of nding the sponsor, and then remembering that you have hallucinated something, and then cancelling the hallucination against the sponsor?",23,24
188,247863667,Extrinsic hallucination: The incorrect action problem and the incorrect object problem mentioned before belong to intrinsic hallucinations which extract information from the email but matches it in a wrong way.,1,2
189,247475925,"By carefully designing experiments, we identify two representative characteristics of the data gap in source: (1) style gap (i.e., translated vs. natural text style) that leads to poor generalization capability; (2) content gap that induces the model to produce hallucination content biased towards the target language.",49,50
190,248512883,"This leads to aggressive anticipation during inference, resulting in the hallucination phenomenon.",11,12
191,248512883,"Ignoring the long-distance reordering may cause unnecessarily high latency, or encourage aggressive anticipation, resulting in the hallucination phenomenon (Müller et al.,",20,21
192,248512883,"In the first example, wait-k predicts the sentence ""demonstrative is one of the major languages in the world's languages,"" which is clearly hallucination.",29,30
193,248266308,"Another serious error type is event transition hallucination, where both the predicted event path and its corresponding inference fail to maintain the logic coherence, such as the fourth case.",7,8
194,220050117,Prior works refer this to hallucination phenomenon.,5,6
195,220050117,"Although neural generators can be trained end-to-end, they suffer from hallucination phenomenon (Balakrishnan et al.,",15,16
196,220050117,"2) To avoid trivial solutions, we require that f (x) = x. However, due to the hallucination phenomenon, it is possible to miss or misplace slot value in generated templates (Wen et al.,",21,22
197,220050117,"Nevertheless, end-to-end neural-based generators suffer from hallucination problem and are hard to avoid generating slot-inconsistent utterance (Balakrishnan et al.,",13,14
198,237605175,"Large language models are known to suffer from the hallucination problem in that they are prone to output statements that are false or inconsistent, indicating a lack of knowledge.",9,10
199,237605175,"While impressive, without strong task-specific fine-tuning these models are prone to outputting false or inconsistent statements, often also referred to as hallucination (Logan et al.,",27,28
200,226262278,"Annotation Example E (High, 3.00) I (High, 3.00) I'm a soldier now.. I'm so tired of depression, insomnia, and hallucination.",30,31
201,248218448,"This category is similar to extrinsic hallucination in the summarization literature (Maynez et al.,",6,7
202,248218448,"With this interpretation, a ""false positive"" (affecting precision) occurs when the simplified sentence contains information not present in the source, i.e., introduces a ""hallucination"".",31,32
203,233219933,"Training with a high α also increases the likelihood of hallucination, misspelling, and repetition.",10,11
204,233219933,"The most common hallucination found in all settings, and especially with high α, was the inclusion Hallucination: The evidence is up-to-date as of February 2016.",3,4
205,247595358,This is connected to faithfulness and reducing hallucination of QA system.,7,8
206,247450557,"However, they suffer from problems common in text generation like hallucination (Rohrbach et al.,",11,12
207,235254178,"The third type is based on hallucination (Wang et al.,",6,7
208,248779957,"Data augmentation strategies are increasingly important in NLP pipelines for low-resourced and endangered languages, and in neural morphological inflection, augmentation by so called data hallucination is a popular technique.",28,29
209,248779957,This paper presents a detailed analysis of inflection models trained with and without data hallucination for the low-resourced Canadian Indigenous language Gitksan.,14,15
210,248779957,"Our analysis reveals evidence for a concatenative inductive bias in augmented models-in contrast to models trained without hallucination, they strongly prefer affixing inflection patterns over suppletive ones.",19,20
211,248779957,"However, data hallucination dramatically reduces prediction accuracy for reduplicative forms due to a misanalysis of reduplication as affixation.",3,4
212,248779957,"While the overall impact of data hallucination for unseen lexemes remains positive, our findings call for greater qualitative analysis and more varied evaluation conditions in testing automatic inflection systems.",6,7
213,248779957, The same model trained without hallucination exhibits no preference.,6,7
214,248779957,"In this paper, we investigate the data hallucination strategy, a relatively commonplace strategy (Anastasopoulos and Neubig, 2019; Silfverberg et al.,",8,9
215,248779957,"1  Given the relatively small size of our paradigm dataset, further described in Section 2, we investigate whether data hallucination is an effective strategy for mitigating overfitting.",22,23
216,248779957,"In accordance with recent results (Liu and Hulden, 2021) , we find that data hallucination improves performance in ""wug test"" (Berko, 1958) like conditions: where no inflectional variant of a lexeme was witnessed during training.",17,18
217,248779957,"Surprisingly, however, we also find that data hallucination significantly worsens performance for lexemes which were partially observed during training; that is at least one of the inflectional variants of the lexeme was present in the training data.",9,10
218,248779957,These findings motivated a controlled error analysis of our PCFP system to discover why data hallucination generalizes to the unobserved test setting but seemingly slashes performance in the observed test setting.,15,16
219,248779957,"First, we find that the model trained without hallucination is ""often right for the wrong reason"" (Mc-Coy et al.,",9,10
220,248779957,"That is, we find that the model trained without hallucination relies on a brittle memorization strategy.",10,11
221,248779957,"Second, we find evidence that data hallucination introduces an inductive bias towards concatenative morphology: where inflection is accomplished by appending affixes to a word stem.",7,8
222,248779957,We find that the MAX strategy combined with data hallucination selects a simpler transformation: In Fig.,9,10
223,248779957,"Conversely, the model trained without hallucination exhibits no strong preference over either transformation.",6,7
224,248779957,"Since concatenative morphology is the dominant inflection process in Gitksan, this inductive bias serves the hallucination model well in inflecting unfamiliar lexemes during testing.",16,17
225,248779957,"Data hallucination, however, can be damaging depending on the morphophonological phenomena at hand.",1,2
226,248779957,"We obtain the hallucination method, number of hallucinated examples (10, 000), and implementation from Anastasopoulos and Neubig (2019) .",3,4
227,248779957,"That hallucination improves performance on unfamiliar lexemes has been previously observed (Liu and Hulden, 2021) .",1,2
228,248779957,"We also find, however, that hallucination worsens performance on familiar lexemes.",7,8
229,248779957,Why does accuracy drop by nearly 50% for the non-hallucination model across the two testing conditions?,12,13
230,248779957,How does hallucination improve performance on unfamiliar lexemes?,2,3
231,248779957,And why does hallucination reduce performance on familiar lexeme paradigms?,3,4
232,248779957,"This error analysis enables us to systematically characterize the effects hallucination has on the Transformer model in inflection, demonstrating that the effects can be both beneficial and adverse.",10,11
233,248779957,"Further, it can explain the stronger performance of the hallucination model in predicting forms in Π wug : this inductive bias towards concatenative morphology can generalize well to unfamiliar lexemes given the prevalence of concatenative morphology in the Gitksan dataset.",10,11
234,248779957,"This resemblance, however, is sufficient to confuse prominent data hallucination techniques (Anastasopoulos and Neubig, 2019; Silfverberg et al.,",11,12
235,248779957,"Unfortunately, the hallucination strategy could impair the model's ability to perform reduplication, given that the number of examples of reduplication would become smaller relative to the size of the complete dataset.",3,4
236,248779957,"Indeed, we find strong evidence that the hallucination model is unable to perform reduplication.",8,9
237,248779957,"We find that the standard model is able to predict the 12 instances of reduplication in Π wug and Π obs with .92 accuracy, while the hallucination model slashes this proficiency to a mere .25.",27,28
238,248779957,Reduplication is pronounced in the Gitksan dataset and causes problems for current data hallucination methods.,13,14
239,248779957,"However, it is by no means the only phenomenon where data hallucination can generate incorrect inflection patterns.",12,13
240,248779957,"Thus, it is possible that hallucination's inability to preserve morphological phenomena like reduplication and lenition explain the drop in performance on the observed paradigms.",6,7
241,248779957,"9 Approaches that try to perform data hallucination incorporating the target language's structure have been explored (Lane and Bird, 2020) , but it's unclear how to generalize this method without expert knowledge of the target language.",7,8
242,248779957,"General Discussion In this paper, we explore the effect of data hallucination on the Gitksan language that is currently underserved in NLP.",12,13
243,248779957,"At the same time, we find that data hallucination alleviates the need for memorization significantly, generalizing well to unfamiliar lexemes (Section 3) with an inductive bias towards concatenative morphology (Section 4).",9,10
244,248779957,"Data hallucination, however, is not universally beneficial: we find it reduces the model's capacity to recognize common morphophonological phenomena (Section 4), limiting the performance improvements it can bring.",1,2
245,248779957,"Although our study was conducted on a single language, we note that our characterization of data hallucination could be informative for languages other than Gitksan.",17,18
246,248779957,"As Section 4 demonstrates, data hallucination can encourage the model to apply voicing in incorrect contexts.",6,7
247,248779957,"In English, data hallucination could give rise to erroneously conditioned allomorphy: for instance, hallucination can generate a synthetic past tense inflection example mar -> mard from a gold standard training example such as like -> liked.",4,5
248,248779957,"In English, data hallucination could give rise to erroneously conditioned allomorphy: for instance, hallucination can generate a synthetic past tense inflection example mar -> mard from a gold standard training example such as like -> liked.",16,17
249,248779957,"Overall, our work suggests common data augmentation strategies for NLP like data hallucination merit closer inspection and that further innovations in data augmentation for computational morphology are desirable.",13,14
250,235097229,It refers to the hallucination phenomenon that the summary sometimes distorts or fabricates the facts in the article.,4,5
251,237485423, There also exist some hallucination issues.,5,6
252,233231373,"Finally, we elucidate the phenomenon of hallucination amplification in popular data-generation processes such as Backtranslation and sequence-level Knowledge Distillation.",7,8
253,233231373,Two main hallucination phenomena have been reported in the existing literature: 1.,2,3
254,233231373,"We introduce corpus-level noise into NMT parallel corpora and show that specific noise patterns interact with sequence to sequence training dynamics in different ways to generate the prominent hallucination patterns reported in the literature (Lee et al.,",30,31
255,233231373,"We demonstrate the phenomenon of hallucination amplification in the outputs generated using Backtranslation (Edunov et al.,",5,6
256,233231373,"Manual detection of hallucinations, however, is an impediment for fast experimental cycles, and in this work, besides explaining the generation of such natural hallucinations (i.e. hallucinations generated without any source perturbation), we also propose an approximate corpus level hallucination detection algorithm to aid faster analysis.",45,46
257,233231373,"Hallucinations under Perturbations (HP): For a given input source sequence, a model is considered to generate a hallucination under perturbation, if the generated translations for perturbed and unperturbed sequences differ drastically.",20,21
258,233231373,"Natural Hallucinations (NH): For a given unperturbed input source sequence, a model is considered to generate a natural hallucination if the generated translation is severely inadequate (fluent or otherwise).",21,22
259,233231373,We then compare the hallucination behaviour under perturbation of the most-memorized samples with random samples using the hallucination detection algorithm proposed in Lee et al. (,4,5
260,233231373,We then compare the hallucination behaviour under perturbation of the most-memorized samples with random samples using the hallucination detection algorithm proposed in Lee et al. (,19,20
261,233231373,"The figure shows that as the memorization values increase, the number of unique (Unique HP) as well as total hallucinations (Total HP) keeps increasing as well, demonstrating a strong positive correlation between hallucination frequency and memorization values.",38,39
262,233231373,"2019) , which have a tendency of producing deficient attention maps, the fact that this phenomenon extends to memorized samples as well further helps establish the link between memorization and hallucination under perturbation.",32,33
263,233231373,"Hypothesis 2 posits the simplest explanation for the generation of natural hallucinations: that the phenomenon is caused by the presence of invalid references in the training data, and that specific patterns of such corpus-level noise cause specific hallucination patterns to emerge.",41,42
264,233231373,Establishing a causal link between corpus-level noise patterns and hallucination types could greatly ease diagnosing the origins of such cases.,11,12
265,233231373,"Further, since, there is hallucination amplification going from a parallel corpus to the KD data generated (using noisy models trained on the parallel corpus), downstream systems trained on the KD data will be impacted in terms of hallucinations as well.",6,7
266,233231373,We also showed that specific noise patterns in the training corpora lead to specific well-known hallucination patterns.,17,18
267,220284509,"We also describe the ancillary techniques that we experimented with, such as hallucination, language vector injection, sparsemax loss and adversarial language network alongside our approach to select the related language(s) for training.",13,14
268,220284509,"Furthermore, we describe hallucination, sparsemax/sparseloss, adversarial language network and language vector injection techniques that we prototyped to improve the performance of our system.",4,5
269,220284509,"Moreover, (Anastasopoulos and Neubig, 2019) augment low resource datasets with data hallucination.",15,16
270,220284509,"We also describe the different supporting techniques that we implemented, such as hallucination, language vector injection, adversarial language traning and sparsemax.",13,14
271,249204477,"Annotators were provided with clear guidelines where they had to strictly follow two main rules: • This evaluation is NOT about flagging any mistranslation/hallucination/deletion errors, but only cases where such errors are critical and lead to catastrophic consequences, as outlined in the Taxonomy of Errors. •",26,27
272,249204477,"When it comes to translating offensive language, we recognise the use of different 'strategies' including literal translation, transliteration, omission, random translation (hallucination) or substitution of one strong word with another milder word and vice versa.",28,29
273,53235843,"Moreover, we consider that integrating these two perspectives could contribute to alleviate the issues that current approaches for generating descriptions from source code change present, such as the hallucination in the generation, where generated descriptions are syntactically correct but that do not keep any semantic relationship with the actual code change, and also the inability of the model to produce descriptions with a relevant amount of detail.",30,31
274,234487207,"An important note is that the model suffers from hallucination, a common problem in sequence-to-sequence models.",9,10
275,234487207,"However, despite the model capabilities in terms of ""header"" generation, we also observe cases of hallucination in the parts of the messages that lie outside ""headers"".",19,20
276,237558722,"Moreover, a checkbox was provided so the annotators could indicate any problem in the data such as wrong information or hallucination, i.e. verbalization of information which is not contained in the meaning representation.",21,22
277,241583243,"Qualitative Analysis There are five main types of errors produced by OCR: (1) over-segmentation, (2) undersegmentation, (3) misrecognized character, (4) missing character, (5) hallucination.",40,41
278,241583243,"Last, in the hallucination error example, where there are characters which have no equivalent in the target text, our model correctly deletes the spurious characters.",4,5
279,227210092,"However, most works don't go beyond extracting causality between adjacent events, and so lack the ability to capture causality in non-adjacent concept level, e.g. genetics and hallucination.",32,33
280,233394001,Better models can also be constructed to focus on cases of partial hallucination or incorrect responses.,12,13
281,211042511,"We randomly selected 50 generated outputs (from samples of Table 3 , line 2) and 4 annotators manually annotated the MR to show deletion, insertion, substitution and hallucination of slots where hallucination refers to realization of a wrong slot value.",31,32
282,211042511,"We randomly selected 50 generated outputs (from samples of Table 3 , line 2) and 4 annotators manually annotated the MR to show deletion, insertion, substitution and hallucination of slots where hallucination refers to realization of a wrong slot value.",35,36
283,53244958,2017 ) the classical seq2seq models suffer from two commonly known problems: repetition of subsequences and wording off-topic (referred to as hallucination in the following).,25,26
284,53244958,Pointer-Generator Network (T2T+pg): deals with hallucination problem by having the ability to copy rare or unseen words during training while having the ability to generate words at the same time.,9,10
285,53244958,"Q1 to 4 were specifically designed to measure the recurrent weakness of seq2seq models: content selection, repetition, hallucination and bad segment connection.",20,21
286,53244958,"The T2T+pg, on the other hand, is not so behind when it comes to information coverage, however this models suffers from the problem of hallucination as it can be seen in its last sentence.",27,28
287,250179900,Ils définissent une hallucination comme étant une information se trouvant dans le résumé que l'on ne peut déduire à partir du document source.,3,4
288,250179900,"En effet, il est bien mentionné qu'il a été fait chevalier commandeur de l'Ordre de l'Empire britannique mais cela ne signifie pas qu'il est britannique ; c'est donc une hallucination intrinsèque car plusieurs éléments du document sont associés sans être pour autant déductibles à partir de celui-ci.",30,31
289,250179900,Cela n'est pas mentionné dans le document ; c'est par conséquent une hallucination extrinsèque.,12,13
290,235258298,"The insights obtained via this error analysis give rise to devise more sophisticated methods for the task of multidocument summarization addressing these errors, of which the most severe is the hallucination of facts.",31,32
291,235258247,"Contributing factors include a lack of control (Reiter, 2019) ; issues with hallucination of nongrounded output (Nie et al.,",15,16
292,234482105,"We highlight the difficulties of this task and discuss it in the context of memorization, generalization, and hallucination.",19,20
293,234482105,"We further describe the difficulty of detecting this memorization for the categories of memorization, generalization, and hallucination.",18,19
294,234482105,"Studies on memorization tend to focus on either memorization vs. generalization or memorization vs. hallucination (Nie et al.,",14,15
295,234482105,"Due to the difficulty of distinguishing among the three categories of memorization, generalization, and hallucination, we follow previous work and refer to both memorized and generalized generations as memorized samples for the rest of the paper.",16,17
296,220284290,"For example, the data hallucination method by Anastasopoulos and Neubig (2019) automatically creates non-existing ""words"" to augment morphological inflection data, which alleviates the label bias problem in the generation model.",5,6
297,220284290,"Note that our augmentation method is theoretically orthogonal to the hallucination method (Anastasopoulos and Neubig, 2019), and could be combined to further improve the performance of the baseline models for low-resource languages.",10,11
298,245855873,"The key point is to identify whether the translation will lead to misleading or more serious consequences, e.g. the translation involves critical mistranslation, hallucination or critical content deletion.",25,26
299,235422493,"Topic drift and ""hallucination"" of information are endemic to these models (Wiseman et al.,",4,5
300,250391087,The student forcing training also provides a certain accuracy gain but presents mixed results when used together with data hallucination.,19,20
301,237329050,"In this work, we assess the rate of object hallucination in multimodal conversational agents playing the GuessWhat?!",10,11
302,237329050,"We also analyse where hallucinations tend to occur more often through the dialogue: hallucinations are less frequent in earlier turns, cause a cascade hallucination effect, and are often preceded by negative answers, which have been shown to be harder to ground.",25,26
303,237329050,This issue is generally referred to as hallucination.,7,8
304,237329050,"In this paper, however, we focus only on entity hallucination.",11,12
305,237329050,"The authors found that many participants in this study value more the correctness of the caption compared to a fine-grained description of the image, thus providing evidence that hallucination represents a major issue.",31,32
306,237329050,"Questioner agent, and we highlight their strengths and weaknesses with a focus on the issue of hallucination.",17,18
307,237329050,"Our results confirm that hallucination heavily affects the output of generative models playing GuessWhat?!,",4,5
308,237329050,"Moreover, our results reveal that the rate of object hallucination increases across the dialogue turns.",10,11
309,237329050,"The main contributions of this paper can be summarized as follows: • We investigate the issue of hallucination, an understudied problem in visual dialogue, by taking GuessWhat?!",18,19
310,237329050,2021) design a set of new metrics to quantify the degree of entity hallucination in summaries.,14,15
311,237329050,"Similarly to these works, we focus on entity hallucination, and on inconsistencies with respect to the visual context, instead of the linguistic one.",9,10
312,237329050,"These works focus on words belonging to different parts of speech, like proper nouns, adjectives, and verbs, while we only focus on entity hallucination and leave for future work the analysis of attribute hallucination.",27,28
313,237329050,"These works focus on words belonging to different parts of speech, like proper nouns, adjectives, and verbs, while we only focus on entity hallucination and leave for future work the analysis of attribute hallucination.",37,38
314,237329050,"2018) investigate the problem of object hallucination in image captioning, the closest task to our work.",7,8
315,237329050,"Moreover, they found that models with a more reliable visual representation hallucinate less, suggesting that a robust processing of the visual input is important for reducing hallucination.",28,29
316,237329050,"We extend this claim by looking at hallucination, an under-studied but crucial issue in Visual Dialogues.",7,8
317,237329050,"We compare the models with respect to their accuracy in the guessing game and the quality of the generated dialogues, with a focus on the phenomenon of hallucination.",28,29
318,237329050,"The fine-grained visual input representation of these two models leads to a consistent reduction in hallucinations, confirming that a strong visual processing is critical for avoiding hallucination (Rohrbach et al.,",29,30
319,237329050,"In order to understand this discrepancy between accuracy and hallucination, we compared dialogues that contain at least one hallucinated entity with dialogues not affected by this issue.",9,10
320,237329050,"Interestingly, the drop in accuracy between the two settings reveals a degree of severity from the severe hallucination encountered in BL (-10%) to the mild one in LXMERT-GDSE and GDSE (-7%) till the almost harmless one in VLP and HUMAN (-5%).",18,19
321,237329050,"2018) , the authors found that hallucinated entities tend to be mentioned towards the end of the sentence, and they hypothesise that some of the preceding words in the image caption may have triggered hallucination.",36,37
322,237329050,"To study the effect of hallucinations on the Question Generator, we compute how often hallucinated tokens occur in consecutive turns, i.e. the percentage of turns consisting of two consecutive questions containing at least one hallucination each, over all the turns containing at least one hallucination.",36,37
323,237329050,"To study the effect of hallucinations on the Question Generator, we compute how often hallucinated tokens occur in consecutive turns, i.e. the percentage of turns consisting of two consecutive questions containing at least one hallucination each, over all the turns containing at least one hallucination.",47,48
324,237329050,"The case of the word ""bike"" is illustrated by the example in 5-b, where rather than a hallucination, we simply have a not rigorous use of the work ""bike"" to refer to motorbikes.",22,23
325,237329050,"Instead, the other models frequently hallucinate entities that are not in the human hallucination list or have low frequency; we conjecture this means that the rate of real hallucinations is lower for VLP than for the other models.",14,15
326,237329050,"For instance, the example in Figure 6 illustrates a case of fake hallucination for VLP and LXMERT-GDSE and of real hallucination for the other two models.",13,14
327,237329050,"For instance, the example in Figure 6 illustrates a case of fake hallucination for VLP and LXMERT-GDSE and of real hallucination for the other two models.",23,24
328,237329050,"Conclusion Entity hallucination is one of the major problems that affect natural language generation systems in many NLP tasks, from machine translation to image captioning.",2,3
329,237329050,2018) for image captioning to assess the models' rate of object hallucination.,13,14
330,237329050,"Finally, we highlight the importance of going beyond the simple CHAIR metric to evaluate the impact of hallucination.",18,19
331,237329050,"Further work is needed to design new decoding strategies for natural language generation systems and to explore the relation between hallucination and repetitions, another major issue that heavily affects the quality of machine-generated data as recently highlighted in Testoni and Bernardi (2020) .",20,21
332,237329050,"Moreover, as the example in Figure 1 (right) shows, attribute hallucination plays an important role in the quality of the generated output, and it has not received much attention from the research community.",14,15
333,237491596,"The other columns report relevant statistics of the dialogues: percentage of games with at least one repeated question verbatim, hallucination rate (CHAIR-s), percentage of positive answers in the final turn (% yes Last Turn), and percentage of consecutive questions not seen at training time (lexical overlap, % novel q t−1 , q t per dialogue).",21,22
334,198975147,"Using the latter to hallucinate the visual scene leads to an increase of 0.10 in correlation, and an even higher increase (+0.13) is obtained when the hallucination is induced by a linguistic description of the scene.",29,30
335,235390569,"While this can lead to increased fluency, E2E methods often produce repetitions, hallucination and/or omission of important content for data-to-text (Dušek et al.,",14,15
336,235390569,"In contrast, AGGGEN shows much better coverage over the input triples while keeping a low level of hallucination (low ""Add"" 9 We also trained and tested models on the cleaned E2E data.",18,19
337,220047754,"2019) : First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al.,",11,12
338,220047754,Strengths: (1) Corr-F/A are more sensitive to content-level hallucination than BERTScore.,17,18
339,220047754,"Weaknesses: (1) Corr-F is weaker in identifying token-level hallucination, 14 as in Example 3 in Table 5 .",15,16
340,220047754,"To demonstrate the robustness of our metrics, we evaluate the same sys-14 Token-level hallucination means an incorrect token within an otherwise correct fact structure.",16,17
341,220047754,Content-level hallucination happens when whole facts or arguments are hallucinated.,3,4
342,220047754,"Our metric is more sensitive to perturbations of the facts in the target summary, which resemble common hallucination phenomena of neural decoders (see Figure 2 -3 in Appendix A for examples).",18,19
343,220379134,"Finally, Row 4 has a hallucination, service is not in the MR but it in the second sentence of the realization.",6,7
344,236477623,"Counts above 6 were achieved when the same phrase was repeatedly generated, which is a phenomenon known as hallucination (Wang and Sennrich, 2020 ).",19,20
345,248780400,"As candidates, we consider named entities, which generally constitute common hallucination errors (Dziri et al.,",12,13
346,237259814,"From the results in Table 5 and the generation examples, we can observe hallucination is reduced by incorporating reasoning paths and the reasoning path selection mechanism.",14,15
347,6846540,"The categories were: function word drop, content word drop, syntactic error (with a reasonable meaning), semantic error (regardless of syntax), word order issues, and function word mistranslation and ""hallucination"".",39,40
348,248157473,"However, current models suffer from a severe limitation, generating summaries that are not factually consistent, that is, the content of the summary does not meet the   facts of the source document, an issue also known as hallucination.",42,43
349,248157473,"Second, both methods are able to detect the hallucination six years, which was never mentioned in the source document.",9,10
350,195317077,"Training a model in which output fine-grained entities are not present in the additional input labels is problematic, because it would again require the model to perform both fine-grained entity recognition from pixels as well as caption generation (known to result in hallucination and mis-identification issues, see Sharma et al. (",48,49
351,235097442,We target the problems of hallucination and missing information in the neural conversions by accepting only those neural-generated hypotheses that lie in the range of 0.8 and 1.2 times the length of the questionanswer concatenation.,5,6
352,247862902,"We argue that the boost is due to that the synthetic training data from ED2LM is with less generation hallucination (18% according to the manual annotation), thus including few training noise.",19,20
353,247862902,We further demonstrate that ED2LM could generate questions with less hallucination.,10,11
354,221703021,"The inherent noise in the data collection process further hampers training with models often being prone to hallucination (Song et al.,",17,18
355,249018149,"In AMR, omission/hallucination is captured by (non-)existence of the corresponding structure (see Fig.",5,6
356,52176506,"In this work, we propose a new image relevance metric to evaluate current models with veridical visual labels and assess their rate of object hallucination.",25,26
357,52176506,"We analyze how captioning model architectures and learning objectives contribute to object hallucination, explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination.",12,13
358,52176506,"We analyze how captioning model architectures and learning objectives contribute to object hallucination, explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination.",16,17
359,52176506,"We analyze how captioning model architectures and learning objectives contribute to object hallucination, explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination.",36,37
360,52176506,"Our analysis yields several interesting findings, including that models which score best on standard sentence metrics do not always have lower hallucination and that models which hallucinate more tend to make errors driven by language priors. *",22,23
361,52176506,We refer to this issue as object hallucination.,7,8
362,52176506,"Moreover, the sentence metrics do not always appropriately penalize such hallucination.",11,12
363,52176506,Our proposed metrics (CHAIRs and CHAIRi) reflect hallucination.,9,10
364,52176506,"For many visually impaired who value correctness over coverage, hallucination is an obvious concern.",10,11
365,52176506,"Besides being poorly received by humans, object hallucination reveals an internal issue of a captioning model, such as not learning a very good representation of the visual scene or overfitting to its loss function.",8,9
366,52176506,"In this paper we assess the phenomenon of object hallucination in contemporary captioning models, and consider several key questions.",9,10
367,52176506,The first question we aim to answer is: Which models are more prone to hallucination?,15,16
368,52176506,"To measure object hallucination, we propose a new metric, CHAIR (Caption Hallucination Assessment with Image Relevance), which captures image relevance of the generated captions.",3,4
369,52176506,The second question we raise is: What are the likely causes of hallucination?,13,14
370,52176506,"While hallucination may occur due to a number of reasons, we believe the top factors include visual misclassification and over-reliance on language priors.",1,2
371,52176506,"Finally, we ask: How well do the standard metrics capture hallucination?",12,13
372,52176506,"As we largely rely on these metrics, it is important to understand how well they capture the hallucination phenomenon.",18,19
373,52176506,"In Figure 1 we show how two sentences, from NBT with hallucination and from TopDown model (Anderson et al.,",12,13
374,52176506,"As we see, hallucination is not always appropriately penalized.",4,5
375,52176506,We then investigate ways to assess object hallucination risk with the standard metrics.,7,8
376,52176506,"Next we discuss the notions of image and language model consistency, which we use to reason about the causes of hallucination.",21,22
377,52176506,"Qualitatively, we observe that both annotations are important to capture hallucination.",11,12
378,52176506,"Empirically, we verify that using only segmentation labels or only reference captions leads to higher hallucination (and practically incorrect) rates.",16,17
379,52176506,"We illustrate image and language consistency in Figure 2 , i.e. the hallucination error (""fork"") is more consistent with the Language Model predictions than with the Image Model predictions.",12,13
380,52176506,We use these consistency measures in Section 3.3 to help us investigate the causes of hallucination.,15,16
381,52176506,"Evaluation In this section we present the findings of our study, where we aim to answer the questions posed in Section 1: Which models are more prone to hallucination?",30,31
382,52176506,What are the likely causes of hallucination?,6,7
383,52176506,How well do the standard metrics capture hallucination?,7,8
384,52176506,Baseline Captioning Models We compare object hallucination across a wide range of models.,6,7
385,52176506,"To better evaluate how each component of a model might influence hallucination, we ""deconstruct"" the Top-Down model by gradually removing components until it is equivalent to the FC model.",11,12
386,52176506,"When shifting to more difficult training scenarios in which new combinations of objects are seen at test time, hallucination consistently increases (Table 2 ).",19,20
387,52176506,Table 1 presents object hallucination on the Karpathy Test set.,4,5
388,52176506,"2 Unlike other models, the GAN model uses a stronger visual network (ResNet-152) which could explain the lower hallucination rate for both the baseline and the GAN model.",21,22
389,52176506,"2017) decreases, implying that the GAN loss actually helps decrease hallucination.",12,13
390,52176506,We find that generally beam search decreases hallucination.,7,8
391,52176506,Next we review the hallucination behavior on the Robust Test set (Table 2 ).,4,5
392,52176506,"For almost all models the hallucination increases on the Robust split (e.g. for TopDown from 8.4% to 11.4% of sentences), indicating that the issue of   hallucination is more critical in scenarios where test examples can not be assumed to have the same distribution as train examples.",5,6
393,52176506,"For almost all models the hallucination increases on the Robust split (e.g. for TopDown from 8.4% to 11.4% of sentences), indicating that the issue of   hallucination is more critical in scenarios where test examples can not be assumed to have the same distribution as train examples.",31,32
394,52176506,We again note that attention is helpful for decreasing hallucination.,9,10
395,52176506,We note that the NBT model actually has lower hallucination scores on the robust split.,9,10
396,52176506,"Similarly to the Robust Test set, we see hallucination increase substantially on this split.",9,10
397,52176506,"For example, for the Top-Down model hallucination increases from 8.4% to 12.1% for CHAIRs and 6.0% to 9.1% for CHAIRi.",9,10
398,52176506,We find no obvious correlation between the average length of the generated captions and the hallucination rate.,15,16
399,52176506,"Moreover, vocabulary size does not correlate with hallucination either, i.e. models with more diverse descriptions may actually hallucinate less.",8,9
400,52176506,"We notice that hallucinated objects tend to be mentioned towards the end of the sentence (on average at position 6, with average sentence length 9), suggesting that some of the preceding words may have triggered hallucination.",39,40
401,52176506,"We find that often words like ""sitting"" and ""top"" precede the ""dining table"" hallucination, implying the two common scenarios: a person ""sitting at the table"" and an object ""sitting on top of the table"" (Figure 3 , row 1, examples 1, 2).",19,20
402,52176506,"Similar observations can be made for other objects, e.g. word ""kitchen"" often precedes ""sink"" hallucination (Figure 3 , row 1, example 3) and ""laying"" precedes ""bed"" (Figure 3, row 1, example 4).",19,20
403,52176506,In this section we investigate the likely causes of object hallucination.,10,11
404,52176506,We rely on the deconstructed TopDown models to analyze the impact of model components on hallucination.,15,16
405,52176506,"First, we summarize the hallucination analysis on the deconstructed TopDown models (Table 3 ).",5,6
406,52176506,"Also, similar to LRCN vs. FC in Table 1 , initializing the LSTM hidden state with image features, as opposed to inputting image features at each time step, leads to lower hallucination (Single Layer vs. FC).",34,35
407,52176506,"Now we investigate what causes hallucination using the deconstructed TopDown models and the image consistency and language consistency scores, introduced in Sections 2.2 and 2.3 which capture how consistent the hallucinations errors are with image-/ language-only models.",5,6
408,52176506,"We note that models with less hallucination tend to make errors consistent with the image model, whereas models with more hallucination tend to make errors consistent with the language model.",6,7
409,52176506,"We note that models with less hallucination tend to make errors consistent with the image model, whereas models with more hallucination tend to make errors consistent with the language model.",21,22
410,52176506,This implies that models with less hallucination are better at integrating knowledge from an image into the sentence generation process.,6,7
411,52176506,2015) capture hallucination.,3,4
412,52176506,"In  the absence of hallucination, i.e. 1−CHAIRs; we find that SPICE consistently correlates higher with 1−CHAIRs.",5,6
413,52176506,We further analyze the metrics in terms of their predictiveness of hallucination risk.,11,12
414,52176506,Predictiveness means that a certain score should imply a certain percentage of hallucination.,12,13
415,52176506,For each model and a score interval (e.g. 10 − 20) we compute the percentage of captions without hallucination (1−CHAIRs).,20,21
416,52176506,"Consequently, object hallucination can not be always predicted based on the traditional sentence metrics.",3,4
417,52176506,Does hallucination impact generation of other words?,1,2
418,52176506,This implies that hallucination impacts sentence quality beyond simply naming an incorrect object.,3,4
419,52176506,"We observe that one hallucination may lead to another, e.g. hallucinating a ""cat"" leading to hallucinating a ""chair"", hallucinating a ""dog"" -to a ""frisbee"".",4,5
420,52176506,Discussion In this work we closely analyze hallucination in object captioning models.,7,8
421,52176506,"2016) , though we focus specifically on hallucination.",8,9
422,52176506,"However, we focus on carefully quantifying and characterizing one important type of error: object hallucination.",16,17
423,52176506,"Furthermore, hallucination does not always agree with the output of standard captioning metrics.",2,3
424,52176506,"For instance, the popular self critical loss increases CIDEr score, but also the amount of hallucination.",17,18
425,52176506,"Additionally, attention lowers hallucination, but it appears that much of the gain from attention models is due to access to the underlying convolutional features as opposed the attention mechanism itself.",4,5
426,52176506,"Furthermore, we see that models with stronger image consistency frequently hallucinate fewer objects, suggesting that strong visual processing is important for avoiding hallucination.",24,25
427,52176506,"Our CHAIR metric gives a way to evaluate the phenomenon of hallucination, but other image relevance metrics e.g. those that incorporate missed salient objects, should also be investigated.",11,12
428,182952524,"Meanwhile, SEQ2SEQ frequently echos words from OP, and both SEQ2SEQ and SEQ2SEQAUG suffer from the problems of ""hallucination"" (e.g., the first sentence in SEQ2SEQAUG) and repetition (e.g., the second and third sentences in SEQ2SEQ).",20,21
429,235293831,"With limited number of entities and concepts as input, generation systems are often incapable of producing long text with rich content, resulting in hallucination (Wiseman et al.,",25,26
430,220286777,"Our analysis of systems' performance shows positive effects of newly introduced data hallucination technique that we employed in one of neural systems, especially in low-resource scenarios.",13,14
431,220286777,We propose a variation of data hallucination technique that significantly improves the results of neural models in low-resource settings.,6,7
432,220286777,Organizers also provide a variation of the model that uses data hallucination technique from Anastasopoulos and Neubig (2019) to improve performance in low-resource languages.,11,12
433,237414674,"Guiding text-to-text generation with explicit constraints has been recently shown to help alleviate model hallucination and improve factual consistency (Mao et al.,",18,19
434,245144787,"2019) , or hallucination (Maynez et al.,",4,5
435,216641852,"Neural models are known to be prone to hallucination, i.e., generating text that is fluent but not faithful to the source (Vinyals and Le, 2015; Koehn and Knowles, 2017; Lee et al.,",8,9
436,216641852,"This hallucination phenomenon has been widely observed in other existing data-to-text datasets (Lebret et al.,",1,2
437,227228320,"In order to generate synthetic samples, we use data hallucination technique proposed in Anastasopoulos and Neubig (2019) .",10,11
438,248721759,"Related Work This work is related to the growing body of research into factual consistency and hallucination in text generation models, particularly for summa-rization (Cao et al.,",16,17
439,102350878,"This confirms that many of the most informative words in terms of clinical sentiment (e.g. 'hallucination', 'depressed', 'employed', etc.)",17,18
440,247158838,"While reducing hallucination is not the focus of this work, knowledge-grounded generation techniques can be used to alleviate this problem.",2,3
441,233296648,"As a first step to addressing these issues, we propose a novel token-level, reference-free hallucination detection task and an associated annotated dataset named HADES (HAllucination DEtection dataSet) 1 .",20,21
442,233296648,Existing work has sought to detect hallucination and quantitatively measure generation consistency against a provided reference.,6,7
443,233296648,"Such reference-based hallucination detection has been proposed for abstractive summarization (Maynez et al.,",4,5
444,233296648,"One common setup for qualitatively measuring the level of hallucination is performed at sentenceor document-level (Dhingra et al.,",9,10
445,233296648,"As an alternative, at decoding time of an NLG system, we suggest that if the locus of hallucination can be identified at the token level, it may be possible to guide beam search or suppress the probability of certain tokens at real-time.",19,20
446,233296648,"To this end, we propose a reference-free, tokenlevel hallucination detection task and introduce an annotated training and benchmark testing dataset that we call HADES (HAllucination DEtection dataSet).",12,13
447,233296648,We expect the token-level property of this task to foster the development of models that can detect fine-grained signals of potential hallucination.,25,26
448,233296648,"In conjunction with consulting context to identify self-contradictory statements and access to commonsense and world knowledge, such fine-grained signals, when detected, should further mitigate real-time hallucination.",34,35
449,233296648,"Our contributions include: 1) We propose a reference-free, token-level hallucination detection task for free-form text generation.",16,17
450,233296648,We also present comprehensive analyses on the statistical features to shed light on what is commonly recognized as hallucination in crowd-sourced judgments and its salient characteristics in free-form text generation.,18,19
451,233296648,Task Overview We formulate our hallucination detection task as a binary classification task.,5,6
452,233296648,"modified the plan with the intention of using it to take control of german forces 10 , to directly attack 11 the ss , and arrest the ss leaders 12 …  tion"" 2 (abbreviated as ""H"") or a ""not hallucination"" (abbreviated as ""N "") label to the highlighted spans.",46,47
453,233296648,"Data Annotation We ended up with ∼1M perturbed text segments in the pool after contextual perturbation, not all of which contain hallucination, as the BERT model can generate factual information given that it is pretrained on a rich open web corpus.",23,24
454,233296648,The annotators were asked to determine whether the perturbed text spans are H (hallucination) or N (not hallucination) with the original text in terms of factualness and semantic coherence given the context.,14,15
455,233296648,The annotators were asked to determine whether the perturbed text spans are H (hallucination) or N (not hallucination) with the original text in terms of factualness and semantic coherence given the context.,20,21
456,233296648,"Out of 12,719 annotated instances, 86.12% instances reach consensus and are included in HADES dataset; 78.47% instances reach ≥ 80% agreement among annotators, e.g. 4/5 or 5/6 vote for ""hallucination"" label; 71.24% instances reach 100% agreement in the annotation.",36,37
457,233296648,"We found that with randomly sampled instances, the annotated label distribution is heavily skewed toward the ""hallucination"" class.",18,19
458,233296648,"For each round 7 , we first retrain a hallucination detection model (initiated with BERT) based on the annotated instances in the previous rounds.",9,10
459,233296648,"To filter out trivial instances and focus on the more useful cases, we use a heuristic rule for the automatic screening by abandoning instances where the detection model assigns low or high probability to ""hallucination"" class (the threshold varies in different rounds to yield reasonable number of candidates).",36,37
460,233296648,"We also remove a large portion of obvious hallucination instances where the target text span is recognized as a DATE or NAME, and replaced by a different DATE 8 or NAME.",8,9
461,233296648,"In the initial rounds of annotation, we observed extreme label imbalance (around 90% are H class) between H (hallucination) and N (not hallucination) cases.",23,24
462,233296648,"In the initial rounds of annotation, we observed extreme label imbalance (around 90% are H class) between H (hallucination) and N (not hallucination) cases.",29,30
463,233296648,"In the final dataset, ""hallucination"" cases slightly outnumber ""not hallucination"" cases, with a ratio of 54.5%/45.5%.",6,7
464,233296648,"In the final dataset, ""hallucination"" cases slightly outnumber ""not hallucination"" cases, with a ratio of 54.5%/45.5%.",13,14
465,233296648,We summarize some typical hallucination types seen in the HADES dataset in Fig 3 .,4,5
466,233296648,"From a POS perspective, around two-thirds of verbs and verbal phrases in the dataset are identified as ""not hallucination"", while in other types of words/phrases, ""hallucination"" cases are in the majority, e.g., most adverbs (ADV), adjectives (ADJ) and acronyms of proper nouns (PROPN) are labeled as ""hallucination"".",22,23
467,233296648,"From a POS perspective, around two-thirds of verbs and verbal phrases in the dataset are identified as ""not hallucination"", while in other types of words/phrases, ""hallucination"" cases are in the majority, e.g., most adverbs (ADV), adjectives (ADJ) and acronyms of proper nouns (PROPN) are labeled as ""hallucination"".",35,36
468,233296648,"From a POS perspective, around two-thirds of verbs and verbal phrases in the dataset are identified as ""not hallucination"", while in other types of words/phrases, ""hallucination"" cases are in the majority, e.g., most adverbs (ADV), adjectives (ADJ) and acronyms of proper nouns (PROPN) are labeled as ""hallucination"".",67,68
469,233296648,"However, of the 10% of remaining instances, over 90% are ""hallucination"" cases.",15,16
470,233296648,"Statistical and model-based features To analyze the characteristics of hallucinations in HADES, we compute the correlation between a selected group of statistical/model-based features and hallucination labels.",31,32
471,233296648,"Consequently, many overconfident generation outputs are likely to fall into hallucination.",11,12
472,233296648,We observe no strong correlation between hallucination labels and TF-IDF or PPMI as demonstrated in Table 1B .,6,7
473,233296648,"Baseline Models As an initial step towards tackling the proposed hallucination detection task and benchmarking methods, we create several baseline detection models 11 .",10,11
474,233296648,"Specifically, for an input text segment, we finetune a pretrained model M to predict binary hallucination labels y for each given text span.",17,18
475,233296648,"We then map w to a binary hallucination label y ∈ {0, 1} with a MLP network using tanh as activation.",7,8
476,233296648,"As discussed in Sec.2, HADES can serve as benchmark for hallucination detection in both offline (model can see bidirectional context) and online (only preceding context can be leveraged) settings.",11,12
477,233296648,"In both settings, the predictions for ""not hallucination"" cases have higher F1 scores than ""hallucination"" cases.",9,10
478,233296648,"In both settings, the predictions for ""not hallucination"" cases have higher F1 scores than ""hallucination"" cases.",18,19
479,233296648,"12 To identify the clear winner among baseline models, we report the significant tests for the baseline models in Table 3 and Table 2 as follows: For the offline setting (Table 2 ), there is no obvious winner among pretrained models, e.g. RoBERTa wins in ACC; XLNet wins in F1 for not hallucination cases; BERT wins in G-mean.",58,59
480,233296648,"For the online setting (Table 3 ), we ran significant tests for the mean performance (over 5 runs) between GPT-2 and BERT; GPT-2 and XLNet; GPT-2 and RoBERTa, the differences in terms of ACC; G-mean; F1 scores for both hallucination and not hallucination labels are significant (alpha=0.01) after Bonferroni correction.",50,51
481,233296648,"For the online setting (Table 3 ), we ran significant tests for the mean performance (over 5 runs) between GPT-2 and BERT; GPT-2 and XLNet; GPT-2 and RoBERTa, the differences in terms of ACC; G-mean; F1 scores for both hallucination and not hallucination labels are significant (alpha=0.01) after Bonferroni correction.",53,54
482,233296648,"Context matters in HADES To investigate extent to which contextual information helps the hallucination detection in HADES, we run BERT-large detection model with different context lengths and characterize its performance in both online and offline settings in Fig 6 .",13,14
483,233296648,This observation highlights the importance of context in hallucination detection.,8,9
484,233296648,"Interestingly, the detection model predicts the high hallucination risk on ""structures and buildings"", which has subtle differences with ""total greenhouse area including enclosed structures"" (included in the instruments).",8,9
485,233296648,"Related Work Reference-based Hallucination Detection Apart from human verification (Chen and Bansal, 2018) , researchers have developed effective reference-based methods which automatically detect hallucination in the generated text using statistical n-gram matching (Dhingra et al.,",30,31
486,233296648,Our approach differs from them in that we investigate the reference-free hallucination detection scenario.,13,14
487,233296648,"Reference-free Detection Approaches Reference-free hallucination detection is closely related to fake news detection (Zellers et al.,",8,9
488,233296648,The proposed hallucination detection aims to examine the text in a finer granularity than fake news detection and fact checking.,2,3
489,233296648,Conclusions We have proposed a token-level reference-free hallucination detection task and introduced a benchmark dataset HADES for identifying fine granularity hallucination in free-form text generation.,11,12
490,233296648,Conclusions We have proposed a token-level reference-free hallucination detection task and introduced a benchmark dataset HADES for identifying fine granularity hallucination in free-form text generation.,24,25
491,233296648,"To create this dataset, we perturbed texts to simulate hallucination in NLG system, and performed an interative model-in-the-loop annotation approach to annotate the perturbed text in an imbalanced label scenario.",10,11
492,233296648,We hope that the proposed task and dataset will shed light on high-resolution hallucination detection in freeform text generation and will eventually lead to real-time hallucination prevention.,15,16
493,233296648,We hope that the proposed task and dataset will shed light on high-resolution hallucination detection in freeform text generation and will eventually lead to real-time hallucination prevention.,29,30
494,233296648,"We support this goal with a novel reference-free, tokenlevel hallucination task and the corresponding annotated dataset HADES.",12,13
495,233296648,We design our model to detect hallucination to factual statement.,6,7
496,233296648,Strong correlation between hallucination labels and TF-IDF or PPMI features is not observed.,3,4
497,233296648,"Since observe a label imbalance between ""hallucination"" (H) and ""not hallucination"" (N ) in the initial rounds of annotation, we employ subsampling to rebalance the label distribution in Sec 3.3.",7,8
498,233296648,"Since observe a label imbalance between ""hallucination"" (H) and ""not hallucination"" (N ) in the initial rounds of annotation, we employ subsampling to rebalance the label distribution in Sec 3.3.",15,16
499,237491581,"Lastly, we propose a simple method to mitigate over-reliance on parametric knowledge which minimizes hallucination and improves out-of-distribution generalization by 4% − 7%.",17,18
500,237491581,"Lastly, as a memorization mitigation strategy, we demonstrate that training with our substituted instances not only reduces hallucination to negligible levels, but also improves F1 by 4% to 7% on out-of-distribution (OOD) examples, thereby generalizing more effectively by learning to prioritize contextual knowledge.",19,20
501,196189472,"The 'groundless information', which is also known as the 'hallucination' problem, remains a long-standing problem in NLG.",13,14
502,235658519,Property B checks for consistency between the article and the summary; while Property C assesses the hallucination implicitly by limiting the knowledge domain to the input article and identifying additional information present in the summary.,17,18
503,27442870,"Different approaches have different strengths: some are precision-oriented, some recalloriented; some better at considering context but more prone to hallucination.",24,25
504,227127272,We check if the text contains any omissions or hallucinations in two steps (see Figure 1 contain hallucination.,18,19
505,227127272,"The final output of our metric is either 4-way (denoted as FINE): OK (i.e., all NLI checks passed), omission, hallucination or omission+hallucination (based on the failed checks), or 2-way (denoted as ROUGH) where the latter three results are collapsed into not_OK.",28,29
506,224704949,2019) is that they treat data records mentioned in the summaries as regular tokens which often results in hallucination problem.,19,20
507,224704949,"In particular, the huge improvement on the content selection metric suggests that by learning how to generate summaries in terms of data variables rather than actual data values, our approach largely addresses the hallucination problem.",35,36
508,224704949,"It correctly mentions the first team name, but then after that it suffers from the hallucination problem as it repeats a memorized summary which it was trained on.",16,17
509,224704949,"The most common error is the fact hallucination, which is demonstrated in Figure 4 .",7,8
510,224704949,Our error analysis on 50 random samples indicates that hallucination can occur in two main ways in our model.,9,10
511,224704949,"The second hallucination method is when the model simply predicts a token which is irrelevant to the data, which we see commonly in other data-to-text works (Wiseman et al.,",2,3
512,226282223,"Since this particular NLG system had no information available for the next game, the above sentence is pure hallucination.",19,20
513,226282223,Statements made in the generated texts about points-per-half are often hallucination because of this.,14,15
514,233219059,"2019) , a phenomenon also known as neural hallucination (Rohrbach et al.,",9,10
515,233241131,"Examples of extraction, abstraction, and hallucination are highlighted in Figure 2 .",7,8
516,233241131,We now demonstrate how we can draw from all three modes of analysis to study the problem of hallucination in summarization systems.,18,19
517,233241131,"Through the unified view of SUMMVIS, we analyze the example shown in Figure 4 and demonstrate the existence of hallucination, suggest a possible cause, and show how a common evaluation metric prefers hallucinated entities over faithful descriptors in this case.",20,21
518,233241131,We now show how artifacts in the reference summaries may explain this hallucination.,12,13
519,249642180,"While recent deep learning-based summarization methods have significantly advanced the quality of AI-generated summaries, they face some common issues, including hallucination, also known as contextual inconsistency (Maynez et al.,",26,27
520,241583525,"On the reference side, then again our metric is an optimistic approximation of omissions and hallucination in the sense that we consider every other references having a similarity score > 0.",16,17
521,241583525,"We then show how our metric can discriminate generators, and analyze omission and hallucination rates using WebNLG (Gardent et al.,",14,15
522,241583525,"To this end, we follow the methodology introduced in Section 3.4 and compute the estimated omission and hallucination rates w.r.t to the capacity of underlying rankers.",18,19
523,241583525,"In this experiment, we are interested in comparing Neural vs Non-Neural architectures, and see if our metric captures the implicit omission/hallucination behavior of neural generators.",26,27
524,241583525,"On the gold annotations, we obtained 0.41 and 0.37 omission and hallucination rates respectively.",12,13
525,241583525,"Regarding the teams' statistics, we denote higher omission and hallucination rates for 3 out 5 neural systems.",11,12
526,241583525,"Interestingly, the estimated omission and hallucination rates of Adapt are quite high, while having a high semantic score from the human evaluation.",6,7
527,241583525,"On the contrary, Tilburg-NMT has low omission and hallucination rates while having a low human evaluation score.",11,12
528,241583525,"Melbourne has low omission and hallucination rates, which corroborates the fact that it was a good system.",5,6
529,241583525,"In future experiments, we would like to compute the exact omission and hallucination rates of each team.",13,14
530,237513633,"In the future, we plan to extend our methods to other problems such as generic utterances and hallucination (Mielke et al.,",18,19
531,226237288,"makes the model less prone to common neural model errors such as hallucination, which allows us to control the semantic accuracy to a great extent using only simple heuristics and language model rescoring.",12,13
532,248227301,"Knowledge-grounded conversational models are known to suffer from producing factually invalid statements, a phenomenon commonly called hallucination.",19,20
533,248227301,"In this work, we investigate the underlying causes of this phenomenon: is hallucination due to the training data, or to the models?",14,15
534,248227301,"2020) , are well-known to generate factually incorrect statements, a phenomenon commonly called hallucination (Dziri et al.,",17,18
535,248227301,"A large commonality in the majority of prior work seeks to address hallucination by ameliorating the model (Shuster et al.,",12,13
536,248227301,"This kind of optimization will likely push the models to replicate and even amplify the hallucination behaviour at test time (Bender et al.,",15,16
537,248227301,"Our analysis reveals surprisingly that more than 60% of the responses are hallucinated in the three datasets, with major hallucination modes that manifest principally through the expression of subjective information (e.g., thoughts, beliefs, feelings, intentions, personal experiences) and the expression of unsupported objective factual information.",21,22
538,248227301,"Further, to understand if neural conversational models make this hallucination more severe, we annotate responses generated by several state-of-the-art models, including ones that are designed to alleviate hallucinations.",10,11
539,248227301,We seek to answer the following questions: (Q1) How much hallucination exists in the benchmarks?,13,14
540,248227301,CMU-DOG contains 61.4% responses that are purely hallucinated against only 16.2% responses that are fully entailing the source knowledge and TOPICALCHAT has similar results (63.9% hallucination v.s. 22.9% entailment).,31,32
541,248227301,Q2) What are the hallucination strategies used in human-human data?,5,6
542,248227301,"Hallucination Amplification in Models Next, we investigate how much models amplify the hallucination phenomenon at inference time.",13,14
543,248227301,Q3) Do state-of-the-art conversational models amplify hallucination?,13,14
544,248227301,"For example, GPT2 amplifies full hallucination by 19.2% in WOW, 15% in CMU-DOG and 15.1% in TOP-ICALCHAT.",6,7
545,248227301,This suggests that hallucination patterns are easier to learn than entailment.,3,4
546,248227301,"Overall, these results demonstrate that hallucination is not only a reflection of training data issues, but also a consequence of the weaknesses of models.",6,7
547,248227301,2016) caused by teacher forcing can make hallucination worse as the model may over-rely on previously predicted words which in turn can aggravate error propagation.,8,9
548,248227301,We conjecture that models-when conditioned on factual knowledge-often assign the highest probability mass to the correct response and sampling based on other distributions (e.g. top-k or nucleus) may invite hallucination in the generation process.,37,38
549,248227301,We leave investigating the role of each factors to hallucination amplification for future work. (,9,10
550,248227301,Q4) What are the hallucination strategies used by models?,5,6
551,248227301,"Surprisingly, different models use different strategies for hallucination.",8,9
552,248227301,Conclusion Our investigations demonstrate empirically that hallucination is a prevalent issue in both dialog benchmarks and models.,6,7
553,248227301,"However, even if we come up with a model that is robust enough against hallucination, it will be ultimately bounded by the data quality.",15,16
554,248227301,We argue that fixing the models or the data to enforce faithfulness is a highly non-trivial task without an in-depth understanding of the various sources of hallucination.,30,31
555,248227301,H Limitation The main goal of this work is to present a data quality audit by gaining an in-depth understanding of the various types of hallucination in both gold and machine-generated responses.,27,28
556,248227301,We do not investigate the root causes of hallucination in the models.,8,9
557,248227301,Future studies can extend our work to explore the main causes of hallucination in the models and study the problem of hallucination in multilingual datasets.,12,13
558,248227301,Future studies can extend our work to explore the main causes of hallucination in the models and study the problem of hallucination in multilingual datasets.,21,22
559,248227301,I Hallucination in CMU-DoG and TopicalChat Figure 3 shows the hallucination breakdown in CMU-DOG and TOPICALCHAT benchamrks.,12,13
560,218487279,"We consider three types of unfaithful errors: (i) hallucination error-creating content not present in the input, (ii) out-of-context error-generating facts without including required context or within   incorrect context, and (iii) deletion or substitution error-mistakenly deleting or substituting subjects, objects, or clauses.",11,12
561,218487279,"Interestingly, human-written summaries are also discerned to contain a nontrivial amount of hallucination errors.",15,16
562,218487279,"Nevertheless, with regard to hallucination errors, we do not see such pattern; there is even a slightly reversed relation with both cloze scores and ROUGE scores, wherein summaries with more hallucination errors tend to score higher.",5,6
563,218487279,"Nevertheless, with regard to hallucination errors, we do not see such pattern; there is even a slightly reversed relation with both cloze scores and ROUGE scores, wherein summaries with more hallucination errors tend to score higher.",34,35
564,196189186,"However, it suffers from the hallucination problem so that it generates some unseen facts, which is not faithful to the source input.",6,7
565,250390545,"Transference (25 mins) • Modality transfer: losses, hallucination, crossmodal transfer. •",11,12
566,236477655,"For abstractive summarization, there exists a serious problem known as hallucination (Maynez et al.,",11,12
567,248810934,"We refer the readers to Appendix-B.1 for the details about hallucination detection, as well as the algorithm and discussion of grammatically for the remove method.",12,13
568,248965506,"Extractive Summarization Version To support extractive summarization settings, for example when hallucination is forbidden, we created a corresponding extractive version of our method.",12,13
569,237571691,"Though not focusing on evaluation, our work highlights that models can produce a significant amount of world knowledge which should be evaluated differently instead of as extrinsic hallucination (Maynez et al.,",28,29
570,243865218,"On the other hand, summary B which is generated from the respective Hi-En codeswitched conversation also misses relevant information (MI) but also shows a case of instrinsic hallucination.",32,33
571,243865218,Phrases exhibiting intrinsic hallucination in the summary are shown in ( ).,3,4
572,221802772,"While it is likely that some of the generated query contexts involve unfaithful or nonfactual information due to hallucination in text generation (Mao et al.,",18,19
573,233033613,"However, training on curtailed content further aggravates ""hallucination"" in existing abstractive models (Maynez et al.,",9,10
574,233033613,"Three types of unfaithful errors are considered: (i) hallucination-fabricating content not present in the input, (ii) deletion-incorrectly  summary covers important information of an aspect when compared with the reference.",11,12
575,233033613,"Moreover, both models that use efficient attentions reduce unfaithfulness, especially hallucination errors, when compared with the full attention model, which only reads 1024 tokens.",12,13
576,236469319,"This suggests that the generation model is better at producing fluent texts, but can easily suffer from hallucination.",18,19
577,236486151,find that such examples are complementary to data hallucination and yield improved results in datasparse settings.,8,9
578,236486151,"While this synthetic dataset is inspired by hallucination techniques (Anastasopoulos and Neubig, 2019; Silfverberg et al.,",7,8
579,236486151,"Notably, some of these high scores are achieved on languages that were difficult for the baseline systems; the score for Tajik beats the transformer baseline (56%), perhaps due to data sparsity, since baselines regularized using data hallucination perform better (93%).",43,44
580,13865400,"In order to improve the quality of the generated utterances (avoiding the generation of non-words or the hallucination of named entities), we exploit a priori knowledge in the form of a weighted finitestate automaton that constrains the generated strings of characters to either conform to a predefined vocabulary of words, or to originate in portions of the semantic input.",20,21
581,239016608,"Across different datasets (CNNDM, XSUM, MEDIASUM) and summary properties, such as abstractiveness and hallucination, we study what the model learns at different stages of its fine-tuning process.",18,19
582,239016608,"On the other hand, factual errors, such as hallucination of unsupported facts, are learnt in the later stages, though this behavior is more varied across domains.",10,11
583,239016608,We show that copy behavior is learnt early while hallucination is learnt in the later stages.,9,10
584,218538004,"Previous work has established a link between domain shift and hallucination in NMT (Koehn and Knowles, 2017; Müller et al.,",10,11
585,218538004,"In this paper, we will aim to also establish an empirical link between hallucination and exposure bias.",14,15
586,218538004,"Such a link will deepen our understanding of the hallucination problem, but also has practical relevance, e.g. to help predicting in which settings the use of sequence-level objectives is likely to be helpful.",9,10
587,218538004,"Our experiments show that MRT indeed improves quality more in out-of-domain settings, and reduces the amount of hallucination.",22,23
588,218538004,"In this section, we report on additional experiments to establish more direct links between exposure bias and domain robustness, hallucination, and the beam search problem.",21,22
589,218538004,"2019) , considering a translation a hallucination if it is (partially) fluent, but unrelated in content to the source text (inadequate).",7,8
590,218538004,We consider this preferable to producing a complete hallucination.,8,9
591,218538004,"7 In other words, producing a hallucination will incur a small penalty at each time step (compared to producing the reference), presumably due to a higher reliance on the source signal, lessening the risk of error propagation and hallucinations.",7,8
592,218538004,"Hence, it is much less likely that a hallucination is kept in the beam, or will overtake a good translation in overall probability, reducing the practical impact of the model's overreliance on its history.",9,10
593,224818197,"Previous work has found that models can fall into a hallucination mode where ""the decoder ignores context from the encoder and samples from its language mode"" (Koehn and Knowles, 2017; Lee et al.,",10,11
594,224818197,"Wang and Sennrich (2020) empirically link the hallucination mode to exposure bias (Ranzato et al.,",9,10
595,207847180,"Several authors present anecdotal evidence for NMT systems occasionally falling into a hallucination mode where translations are grammatically correct but unrelated to the source sentence (Arthur et al.,",12,13
596,207847180,Our manual evaluation shows that hallucination is more pronounced in out-of-domain translation.,5,6
597,207847180,We therefore expect methods that alleviate the hallucination problem to indirectly improve domain robustness.,7,8
598,207847180,"As a means to reduce hallucination, we experiment with several techniques and assess their effectiveness in improving domain robustness: reconstruction (Tu et al.,",5,6
599,207847180,We hypothesize that hallucination is more common in out-of-domain settings.,3,4
600,207847180,"Reconstruction having a bias for adequacy We show that reconstruction is limiting hallucination on out-of-domain data, reducing the percentage of inadequate translations by 5 percentage points on average.",12,13
601,207847180,Our experiments to reduce NMT hallucination and improve adequacy can be summarized as follows.,5,6
602,234763107,"To test this hypothesis, we define a hallucination as a translation that has a CHRF2 score of less than 0.01 when compared to the reference, inspired by Lee et al. (",8,9
603,234763107,"Given this definition of hallucination, Figure 5 shows that on average, MBR assigns a lower utility score to hypotheses that are hallucinations.",4,5
604,237099273,The second technique that we explore is delexicalizing the slot values in order to mitigate model hallucination.,16,17
605,237099273,"The highest c&g% of 92.5 is achieved when input is lexed structured and output is delexed structured: it is 2.3% higher than performance of the model with the same lexed structured input but with lexed structured output, which is due to the lower possibility of hallucination when the model output is delexed.",48,49
606,226246333,"Moreover, the gap between EM and EM any , the biggest for BART, shows the proportion of errors due to subtle differences within the resolved annotation, as opposed to errors caused by serious problems such as hallucination. (",39,40
607,248779872,"2021) and hallucination (Zhou et al.,",3,4
608,243865569,"To do so, they use simple sentence transformations to create artificial omission, repetition, hallucination and value errors.",16,17
609,195750845,Data cleaning gave an improvement of more than 5 BLEU points with substantial reduction in the hallucination of the model for the winning team.,16,17
610,248780378,"2021) showed that the related external knowledge can be exploited to address critical issues, such as factual incorrectness and hallucination, in dialogue systems.",21,22
611,221971299,"However, several issues persist: coherence of output and the semblance of mere repetition/hallucination of tokens from the training data (Moryossef et al.,",16,17
612,237604955,ipation rate in the pseudo-references and the hallucination rate in the hypotheses. •,9,10
613,237604955,"2 , a wait-2 model learns to output y 1 =""there"" after observing x 1 x 2 =""-˝Ñ"" (china 's) which seems to induce a good anticipation (""-˝Ñ..."" $ ""There ...""), but it could be a wrong hallucination in many other contexts (e.g., ""-˝Ñ WS à $"" $ ""Chinese streets are crowded"", not ""There ..."").",53,54
614,237604955,"We further define the k-anticipation rate (AR k ) of an (x, y, a) triple under wait-k policy to be: AR k (x, y, a) = 1 |y| X |y| t=1 A k (t, a) Hallucination Rate of Hypotheses The goal of reducing the anticipation rate during the training of a simultaneous translation model is to avoid hallucination at testing time.",74,75
615,237604955,A target word ŷt is a hallucination if it can not be aligned to any source word.,6,7
616,237604955,"Formally, based on word alignment a, whether target word ŷt is a hallucination is H(t, a) = 1[{(s, t) 2 a} = ?]",14,15
617,237604955,"We further define hallucination rate HR as HR(x, ŷ, a) = 1 |ŷ| X |ŷ | t=1 H(t, a) To avoid non-faithful contextual alignments, we use IBM Model 1 (Brown et al.,",3,4
618,237604955,2013) as the word aligner (Model 2 for anticipation and Model 1 for hallucination) and train it on the training set.,15,16
619,237604955,"2019) BLEU "" 31.8 32.6 35.9 37.9 39.4 +1.7 ( 5.0%) Pseudo-Refs HR% # 5.5 7.4 5.4 5.2 4.6 1.3 (18.9%) *+Top 40% BLEU "" 32.3 34.3 36.4 38.4 38.8 +2.2 ( 6.5%) Pseudo-Refs HR% # 5.9 5.8 5.3 5.1 5.3 1.4 (20.3%) Table 1 : BLEU scores and hallucination rates (HR) of Zh!En wait-k models on the test set against the original 4 references. (",67,68
620,237604955,Chinese-to-English (single-reference BLEU) k=1 k=3 k=5 k=7 k=9 Avg.4 Training-Refs (*) 10.9 12.1 13.0 13.7 13.8 *+Top 40% Pseudo-Refs 12.6 14.2 13.9 14.2 14.1 +1.1 (7.5%) hallucination rate.,46,47
621,237604955,7 ) and hallucination (Tab.,3,4
622,248779944,If intersection-over-union is above a fixed threshold we keep this object for the hallucination process.,17,18
623,248779944,"This result sets the upper bound on HOLM, because it cannot achieve better hallucination than the ground-truth FoVs.",15,16
624,248779944,"In Table 4 , we compare methods that only use task data for object hallucination and HOLM with external sources such as pre-trained LM.",14,15
625,248779944,"First, the hallucination process solely conditions on the current field of view.",3,4
626,248779944,Conditioning on these sources of infor-mation could improve the hallucination accuracy by getting more targeted information from the language model.,11,12
627,248779944,"Second, we assume a fixed lexicon of object labels for hallucination.",11,12
628,248779944,"For both the visual side i.e., the object detector, and the language side i.e., the language model, when an unknown object appears the system cannot use this object for hallucination.",34,35
629,248779944,We strictly compared unimodal approaches for hallucination.,6,7
630,226254579,We further demonstrate how to use the token-level hallucination labels to define a fine-grained loss over the target sequence in low-resource MT and achieve significant improvements over strong baseline methods.,10,11
631,226254579,"Our proposed task specifically focuses on hallucination errors, and we define these errors in a simpler way with only binary labels, which we argue makes them simpler to use and more conducive to labeling at large scale.",6,7
632,226254579,The proposed hallucination detection method (described below) is also applicable to the word-level quality estimation task as demonstrated in §5.4.,2,3
633,226254579,We measure hallucination for two conditional sequence generation tasks -abstractive summarization and MT.,2,3
634,226254579,"To learn token-level hallucination prediction for general conditional sequence generations tasks, we propose a novel method that creates synthetic ""hallucinated"" data and finetunes a pretrained language model (Liu et al.,",5,6
635,226254579,"Predicting hallucination labels at the token level provides a tool for diagnosing and interpreting model outputs, which allows us to flag potential risks when the model is applied to previously unseen inputs.",1,2
636,226254579,"Additionally, we show how to use these token-level hallucination labels in two case studies to improve self-training (Scudder, 1965) and learning from noisy mined bitext (Koehn et al.,",11,12
637,226254579,"However, most outputs are only partially erroneous (see examples in Appendix E.3) and the rest of the output is still useful for training, as we show by introducing different token-level loss truncation schemes that use our proposed hallucination detection methods.",43,44
638,226254579,"2020) we define any span g i , • • • , g i+j (j >= 0) in G as being ""hallucinated"" if it is not supported by the source input S. 2 More specifically, we consider two types of hallucination, which are not mutually exclusive: Extrinsic hallucinations: a span g i , • • • , g i+j in G consists of additional content without clear grounding in the input.",47,48
639,226254579,"Note that multiword phrases can also be marked intrinsic hallucinations, such as ""this is a book"" being hallucinated from ""this is not a book"", where ""this is"" is a minimal span corresponding to the hallucination.",42,43
640,226254579,"The above definitions are for illustrative purposes; we do not explicitly label whether a hallucination is intrinsic or extrinsic, only whether one exists at all.",15,16
641,226254579,"We achieved moderate agreement (FK≈0.56) on the token-level hallucination annotations and substantial agreement (FK≈0.67) on the sentence-level annotations, while Maynez et al. (",12,13
642,226254579,"First, although we have made detailed annotation guidelines following the definition of hallucination in § 2, it could still be difficult for annotators to distinguish between ungrammatical translations and hallucinations.",13,14
643,226254579,Token-level Hallucination Detection We propose a general-purpose method for tokenlevel hallucination detection for conditional sequence generation tasks.,14,15
644,226254579,"Given the source input S, we first formulate the task of token-level hallucination detection as a sequence labeling problem where a binary label is predicted at each position G t of the machine generation G. One straightforward way of learning this task is to train a model with supervised data in the form of ((S, G), L G ) where L G are the labels at every position of G that indicate if each word is a hallucinated one or not.",15,16
645,226254579,"More specifically, we take target sequence T and create a hallucinated version of it denoted T with associated hallucination labels for each token in T .",19,20
646,226254579,"Generation of hallucinated sentences To control this synthetic hallucination process, we build on a pre-trained denoising autoencoder, which maps a corrupted sentence back to the original text it was derived from, learning to reconstruct missing words that have been arbitrarily masked out.",8,9
647,226254579,"Assigning labels with edit-distance can not always guarantee correct labels, but we find that this simple approach provides sufficiently high quality training data for effective hallucination detection in practice.",28,29
648,226254579,Then we minimize the standard classification loss L pred over the pseudo hallucination labels L T on top of the final hidden vectors of each token in T as shown in Fig.,12,13
649,226254579,We find that such multi-task learning objective helps learn better representations of the input and further improves performance on predicting hallucination labels.,22,23
650,226254579,"Evaluation Tasks and Data We examine hallucination in abstractive text summarization and machine translation (MT) tasks, using the models and datasets described below.",6,7
651,226254579,"2019; Koehn and Knowles, 2017) has shown that translation models are particularly prone to hallucination when tested out of domain.",17,18
652,226254579,"We describe other hyperparameters, including training of MT models, in the Appendix B and C. Evaluation of hallucination prediction In Tab.",19,20
653,226254579,"1, we present the F1 of token-level hallucination labels across six benchmark datasets for MT and abstractive summarization (full results of precision, recall and F1 are presented in Tabs.",10,11
654,226254579,We compare with three baseline methods that we proposed for this new task: (1) The alignment-based method uses a word alignment model for hallucination assessment.,28,29
655,226254579,"3) We go further by exploiting synonyms to assess hallucination in the summarization task where we use WordNet (Miller, 1998) to find synonyms of nouns, verbs, adjectives and adverbs of the target summary and the source article; we predict a target as being hallucinated if its synonym can not be found in the set of the source synonyms.",10,11
656,226254579,2) We can see that even though our model learns hallucination prediction with reference T during training (Sec.,11,12
657,226254579,"Case Study I: Improving Self Training in Machine Translation Predicting hallucination labels at token-level not only allows us to flag potential risks in generation models, but also opens up the possibility of providing fine-grained signals which can be used to define new learning objectives.",11,12
658,226254579,"In this section and the following one, we demonstrate how to leverage the hallucination labels to reduce adverse effects of noisy training instances.",14,15
659,226254579,"Specifically, we show that the fine-grained hallucination signals allow for improved semi-supervised learning ( §6) and training with noisy parallel data ( §7).",9,10
660,226254579,"We propose to use our token-level hallucination predictions to define a fine-grained loss during training in MT, by penalizing errors less on tokens that more likely to be hallucinated.",8,9
661,226254579,"First, we predict the token-level hallucination labels on the target side of the pseudo parallel data D p .",8,9
662,226254579,"In this section, we demonstrate that token-level hallucination labels can allow us to make better use of noisy data to and improve the overall translation quality.",10,11
663,226254579,"First, we train a token-level hallucination prediction system with the combined parallel data from all the three language pairs (as Hindi is related to Nepali).",8,9
664,226254579,"2019) that achieve the best overall performance for both language pairs among all the submissions to select the top-scored 1M, 2M, 3M, 4M, 5M, and 10M data (in English tokens) and predict the token-level hallucination labels on the target side.",52,53
665,226254579,"Conclusions This work proposed a new task of token-level hallucination detection, created human-annotated benchmark datasets, proposed a method for unsupervised learning of hallucination detectors, and showed that the models can be used to define fine grained losses that improve MT training.",11,12
666,226254579,"Conclusions This work proposed a new task of token-level hallucination detection, created human-annotated benchmark datasets, proposed a method for unsupervised learning of hallucination detectors, and showed that the models can be used to define fine grained losses that improve MT training.",28,29
667,226254579,"We demonstrate the remark performance of the proposed hallucination detection method in several downstream tasks, including word-level quality estimation and noisy neural machine translation.",8,9
668,226254579,"In the future, we hope to create a large-scale pretrained hallucination detector for any dataset or model, and also would extend our method to data-to-text generation scenarios.",13,14
669,226254579,We are also interested in investigating how to leverage our detection methods to mitigate hallucination problems in conditional sequence generation.,14,15
670,218571335,"In case the worker labels a sentence as unfaithful, we conduct a simple error analysis by asking them to indicate if the sentence contains information that is absent from or conflicting with the source document, which corresponds to hallucination and contradiction errors, respectively.",40,41
671,218571335,We further observe conflicting information is more common among models trained on CNN/DM while hallucination is more common among models trained on XSum.,16,17
672,15402420,"ing, amalgam, annotated, applied, apply, applying, approximated, association, asymmetric, augmented, availability, available, average, back-off, base, baum-welch, begin, bitexts, bunetsu, candidate, candidates, cat, central, chinese, choose, chunk-based, class, closely, collecting, combination, compare, compared, compares, computed, concludes, consequently, contributed, convention, corpora, correspondence, corrupts, cost, counts, coverage, crucial, currently, decades, decoding, defines, denote, dependent, depending, determine, dictionaries, direct, directions, disadvantages, distinction, dominated, dynamic, efforts, english-chinese, english-spanish, enumerate, eojeol, eq, equations, errors, evaluation, excellent, expansion, explicitly, extracts, failed, fairly, final, finally, fit, flat-start, followed, form, formalisms, formulation, generation, gis, give, grouped, hallucination, halogen, handle, heuristic-based, hidden, highly, hill-climbing, hmm-based, hypothesis, ideal, identified, identify, identity, immediate, implemented, improved, improves, incorporate, increase, influence, initial, initialize, inspired, interchanging, introduces, investigations, involve, kate, kind, learning, learns, letter, letters, lexical, likelihood, link, list, longer, lowercase, main, make, makes, mapping, maximal, maximizes, means, modeling, modified, names, needed, nitrogen, nodes, occupy, omitting, optimal, outperform, overcome, parse, parser, part, part-ofspeech, path, performed, play, plays, popular, pos, positions, power, precision, probable, produce, programming, promising, real-valued, reason, recall, recent, recently, recognition, recursion, recursively, reduction, reductions, refine, relative, relying, renormalization, representation, require, requires, research, restricting, reveal, sample, sampling, satisfactory, segments, semantic, sequences, setting, shortcomings, showed, significant, significantly, similarity, similarly, simple, simplicity, situation, space, speech, spelling, state-of-the-art, step, strategies, string, strong, studies, summaries, summarization, supervised, syntactic, tags, task-specific, technique, techniques, technologies, terms, testing, threshold, translationrelated, transliteration, tree, trees, trellis, type, underlying, unrealistic, unsupervised, uppercase, value, viterbi, wanted, ways, well-formedness, well-founded, widely, widespread, works, written, wtop, yasmet, years, yields Table 6 : Term Frequencies of 'Noisy' Reference Index Terms",186,187
673,236912977,"For each summary sentence with a hallucination, semantically and lexically similar document sentences are highlighted on demand.",6,7
674,174797747,Divergence can be viewed as hallucination in the reference text itself.,5,6
675,174797747,PARENT deals with hallucination by discounting n-grams which do not overlap with either the reference or the table.,3,4
676,237442270,2021) on several RAG variants found that the KB was able to reduce the amount of hallucination in generating dialogue.,17,18
677,233289483,"BEGIN models the task of evaluating groundedness as an NLI task and examples are annotated with five labels: entailment, contradiction, hallucination, off-topic and generic, where the last three are all considered to be neutral from an NLI perspective.",23,24
678,237010906,hallucination Translation is absolutely unrelated to the source text.,0,1
679,237010906,"shall we meet tomorrow"" is considered as hallucination. """,8,9
680,237010906,"Another phenomenon with a high discrepancy is hallucination: this type of errors is inadequate by its definition, but is often perceived as comprehensible.",7,8
681,241583364,"Finally, hallucination represents an interesting and specific phenomenon.",2,3
682,231632658,"On the other hand, the random selection method has the issue of hallucination, where the generated sequences contain information (i.e., ""senior year"" and ""University of Texas"") not present in the table.",13,14
683,231632658,"Potential Bias During the experiment on tableto-text generation, we have pointed out that large pre-trained language models could be susceptible to hallucination (case study in Table 5 ).",26,27
684,231632658,"Moreover, the random sampling baseline could lead to the issue of hallucination.",12,13
685,225070343,"In sum, we make the following contributions: We introduce a challenging new text editing task, wherein a model must learn to edit text in response to a user command, while drawing on grounding to avoid problems of hallucination (Wiseman et al.,",41,42
686,237558767,"In addition, the models have a tendency to generate facts that may be factually inaccurate, referred to as factual hallucination.",21,22
687,249062554,"RST-Aware Attention (R.A.A.) To better encode the RST tree structure in language model training and to improve the coherence of text by reducing the occurrence of hallucination in long-form generated text, we propose a novel attention scheme, called RST-aware attention.",30,31
688,235898896,Objective Voice One form of hallucination is when a dialogue agent might share personal stories or opinions.,5,6
689,235898896,2020) used control phrases as controllable inputs to decrease hallucination as a form of content planning.,10,11
690,241583641,"This has been shown to directly correlate with the extent of hallucination in pretrained language models (Yang et al.,",11,12
691,241583641,"We only pretrain the token-level model, since pretraining sentence-level models without measures such as topic guidance (Kang and Hovy, 2020) typically leads to hallucination.",31,32
692,244909449,Previous studies commonly assume that hallucination is an undesirable behavior in abstractive summarization systems.,5,6
693,244909449,"cause of model hallucination (Kang and Hashimoto, 2020; Wang and Sennrich, 2020) and propose methods that reduce the frequency of all hallucinations (Filippova, 2020; Zhao et al.,",3,4
694,244909449,"Based on this assumption, we propose to use the prior and posterior probabilities as the key features in a simple classifier that predicts an entity's hallucination status and factuality.",27,28
695,244909449,"Due to the lack of fine-grained hallucination annotation, we create an entity-level hallucination and factuality annotation on the XSUM dataset.",8,9
696,244909449,"Due to the lack of fine-grained hallucination annotation, we create an entity-level hallucination and factuality annotation on the XSUM dataset.",17,18
697,244909449,"Based on this hypothesis, we propose a novel approach for entity-level hallucination detection and factuality checking.",14,15
698,244909449,iii) We create a set of entity-level hallucination annotations.,10,11
699,244909449,"Recently, many methods have been proposed to reduce model hallucination.",10,11
700,244909449,2021) use entity chains to mitigate the hallucination problem in the generation of abstractive summaries.,8,9
701,244909449,Filippova (2020) proposed a method for controlling hallucination in data-to-text generation task.,9,10
702,244909449,Our work differs in that we focus on both hallucination and factuality.,9,10
703,244909449,"The Prior & Posterior Probability of an Entity We now define the prior and posterior probabilities of an entity, which we will use to predict its hallucination and factuality statuses.",27,28
704,244909449,"Training a Discriminator To classify the hallucination and factuality statuses of a given entity, we need to train a discriminator model.",6,7
705,244909449,"Since the classifier is used for entity hallucination and factuality assessment, we refer to it as ENTFA.",7,8
706,244909449,We train two classifiers for hallucination detection and factuality checking tasks respectively.,5,6
707,244909449,"The commonly used teacher forcing training encourages the model to blindly imitate the training data, which leads to model hallucination at inference time (Kang and Hashimoto, 2020) .",20,21
708,244909449,"Formally, we have: R(s t , a t ) = −r nfe , if a t is non-factual p MLE (a t |s t ), otherwise 4 Dataset 4.1 XENT dataset To study entity hallucination and factuality in abstractive summarization, we need annotations of entity-or token-level hallucination.",40,41
709,244909449,"Formally, we have: R(s t , a t ) = −r nfe , if a t is non-factual p MLE (a t |s t ), otherwise 4 Dataset 4.1 XENT dataset To study entity hallucination and factuality in abstractive summarization, we need annotations of entity-or token-level hallucination.",57,58
710,244909449,"We manually labeled each entity with one of the following three tags: non-hallucinated, factual hallucination, and non-factual hallucination.",18,19
711,244909449,"We manually labeled each entity with one of the following three tags: non-hallucinated, factual hallucination, and non-factual hallucination.",24,25
712,244909449,"Therefore, it is not a hallucination.",6,7
713,244909449,"In this case, the entity is considered as an intrinsic hallucination (Maynez et al.,",11,12
714,244909449,Table 3 shows the distribution of entities by hallucination and factuality status in our labeled dataset.,8,9
715,244909449,We show an example for each hallucination type in Table 2 .,6,7
716,244909449,2020) released a set of factuality and hallucination annotations for XSUM.,8,9
717,244909449,"Compared with our labeling approach, their annotation has a lower granularity and does not distinguish between factual and non-factual hallucination.",22,23
718,244909449,"Therefore, we filter out entities in the extrinsic hallucination span that also appear in the source document.",9,10
719,244909449,Evaluation Tasks Entity-level Hallucination & Factuality Classification We evaluate our method on entity-level hallucination and factuality classification tasks on XENT and MENT.,17,18
720,244909449,"For each entity in the summary, the model predicts a hallucination label and a factuality label.",11,12
721,244909449,We will conduct factual and hallucination assessments separately for comparison with the baselines.,5,6
722,244909449,2020) frame the hallucination detection task as a sequence labeling task.,4,5
723,244909449,They train a hallucination labeling model on synthetic data.,3,4
724,244909449,Evaluation Results on XENT Table 4 shows the evaluation results of our classifiers and baselines in terms of both entity factuality and hallucination status classification.,22,23
725,244909449,"On the hallucination detection task, the overlap-based and synonym-based baselines achieve relatively high accuracy.",2,3
726,244909449,"For hallucination classification, the reason computing word overlap with the source does not completely solve the hallucination detection problem is that hallucination is defined based on the semantic relationship between the source and the summary.",1,2
727,244909449,"For hallucination classification, the reason computing word overlap with the source does not completely solve the hallucination detection problem is that hallucination is defined based on the semantic relationship between the source and the summary.",17,18
728,244909449,"For hallucination classification, the reason computing word overlap with the source does not completely solve the hallucination detection problem is that hallucination is defined based on the semantic relationship between the source and the summary.",22,23
729,244909449,"For hallucination classification, the overlap-based feature has the most significant impact on model performance.",1,2
730,244909449,"Conclusion In this paper, we investigate the hallucination and factuality problems in abstractive summarization.",8,9
731,244909449,Our approach outperforms five base-    line models on both factuality classification and hallucination detection tasks on human-annotated datasets.,13,14
732,244909449,Our approach is limited to entity-level hallucination and factuality classification.,8,9
733,244909449,We compare with other models separately in terms of factuality and hallucination classification in Section 6.1.,11,12
734,244909449,Label it as non-factual hallucination. (,6,7
735,244909449,Label the entity as intrinsic hallucination.,5,6
736,244909449,"For a factual hallucination e k , we can infer that the knowledge of e k is in XSUM if σ(e k ) is large.",3,4
737,244909449,A.7 Compare with Filippova (2020)'s Work Filippova (2020)'s work on data-to-text generation shows that low posterior probability from a CLM during decoding indicates hallucination.,33,34
738,225067799,This stylistic change is also reflected in the change in hallucination; the use of Rachel Jones is likely caused by the appearance of the name of a minister Rachel Haves in an article on Welsh politics found in the 100-aug subset.,10,11
739,214342012,"Cette requête trouve 15 résultats pertinents parmi les 20 analysés ; -Les mots-clés hallu, allu, hallucination fournissent 2 messages de non-adhérence et plusieurs messages avec des contenus proches (7 messages avec l'hallucination comme effet indésirable non-recherché, 11 messages où les patients souffrent d'hallucinations) ; -Le mot-clé planer fournit 9 messages attendu de type J'ai déjà posté quelques sujets à propos de ce fléau qu'est le stilnox (...) je prends du stilnox, pour m'évader, pour planer"".",19,20
740,241583411,We consider two types of unfaithful errors: (i) hallucination error and (ii) logical error.,11,12
741,241583411,"1-to-5), and hallucination error(HE.)",6,7
742,220284617,"In our submission we built upon our previous work (Anastasopoulos and Neubig, 2019) , utilizing cross-lingual transfer from related languages, data hallucination, and a series of training techniques and regularizers.",27,28
743,220284617,"The data hallucination technique would identify the substring sqwe as a stem-like region, and replace its characters with random ones.",2,3
744,220284617,"Single-Language Systems for High Resource Languages For languages with more than 20,000 training examples, we decided to not use crosslingual transfer nor data hallucination, as systems in previous SIGMORPHON shared tasks achieved very competitive performance on such high-resource settings without these additions.",26,27
745,220284617,"For languages with less than 20,000 but more than 10,000 training examples, we used our data hallucination process to create 10,000 additional training examples to be used for training.",17,18
746,220284617,"Cross-Lingual Transfer from a Single Language For some languages we decided to use a single, high-resource related language to combine into our training to perform cross-lingual transfer, along with data hallucination.",38,39
747,220284617,"We provide a complete list of these settings: • for Middle High German (gmh) we used German (deu), • for Middle Low German (gml) we used German (deu) also bypassing data hallucination, • for Swiss German (gsw) we used German (deu), • for North Frisian (frr) we used Dutch (nld), • for Kannada (kan) we used Telugu (tel), • for Telugu (tel) we used Kannada (kan), • for Asturian (ast) we used Galician (glg), • for Friulian (fur) we used French (fra), • for Ladin (lad) we used Friulian (fur), • for Venetian (vec) we used Italian (vec), • for Anglo-Norman (xno) we used Middle French (frm), • for Azerbaijani (aze) we used Turkish (tur), • for Khakas (kjh) we used Turkish (tur), but not including data hallucination, and • for Võro (vro) we used Estonian (est).",41,42
748,220284617,"We provide a complete list of these settings: • for Middle High German (gmh) we used German (deu), • for Middle Low German (gml) we used German (deu) also bypassing data hallucination, • for Swiss German (gsw) we used German (deu), • for North Frisian (frr) we used Dutch (nld), • for Kannada (kan) we used Telugu (tel), • for Telugu (tel) we used Kannada (kan), • for Asturian (ast) we used Galician (glg), • for Friulian (fur) we used French (fra), • for Ladin (lad) we used Friulian (fur), • for Venetian (vec) we used Italian (vec), • for Anglo-Norman (xno) we used Middle French (frm), • for Azerbaijani (aze) we used Turkish (tur), • for Khakas (kjh) we used Turkish (tur), but not including data hallucination, and • for Võro (vro) we used Estonian (est).",195,196
749,220284617,"We suspect this is due to the fact that the data hallucination technique, which is crucial for such low resource settings, is not appropriate for capturing the vowel harmony of Evenki along with its agglutinating morphological patterns -the hallucinated data do not follow these patterns and hence do not guide the model towards learning them.",11,12
750,220284617,"As for Cree, we suspect that the problem lies again in the data hallucination process: the polysynthetic and fusional nature of Cree verb inflected forms is too complicated to be modeled by the simple characterlevel alignment model which is the first step for hallucination.",14,15
751,220284617,"As for Cree, we suspect that the problem lies again in the data hallucination process: the polysynthetic and fusional nature of Cree verb inflected forms is too complicated to be modeled by the simple characterlevel alignment model which is the first step for hallucination.",45,46
752,233296346,We further show that our bootstrapping methods substantially outperform hallucination-based methods commonly used for overcoming the annotation bottleneck in morphological reinflection tasks.,9,10
753,233296346,"Data Hallucination The methods introduced here to combat the annotation bottleneck are somewhat orthogonal to the method of data hallucination introduced in Anastasopoulos and Neubig ( 2019 ), as we aim for identifying new real examples from a vocabulary, rather than hallucinate nonce ones.",19,20
754,233296346,"Furthermore, note that even with the hallucinated examples, the baselines still underperform compared to COMB models without hallucination.",19,20
755,233296346,"2017) that focused on data augmentation, and Anastasopoulos and Neubig (2019) that modified the model itself with a separate features encoder and introduced the now commonly-used hallucination method for data augmentation.",32,33
756,233296346,This contribution is orthogonal and can be applied in tandem with hallucination approaches.,11,12
757,233296346,"When applied separately, our method outperforms both hallucination and current state of the art models for lowresourced settings.",8,9
758,247222714,"We report three categories of typical failures: ""hallucination"", ""missing slot"", and ""wrong slot"".",9,10
759,247447755,"Too much noise could distort semantics and encourage hallucination, whereas too little will encourage copying.",8,9
760,233296059,"In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG).",14,15
761,233296059,NEURAL PATH HUNTER leverages a separate tokenlevel fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph.,14,15
762,233296059,"Moreover, empirical evidence for hallucination in Language Models (LM) runs contrary to known studies that these large models are capable of recalling factual knowledge, e.g. entities and relations in a KG, (Roberts et al.,",5,6
763,233296059,"In this work, we focus on address-ing the open problem of hallucination of factually invalid statements in knowledge grounded dialogue systems where the source of knowledge is a KG.",14,15
764,233296059,"We first identify prominent modes of hallucination by conducting a systematic human study on generated responses which reveals one major source of hallucination as the (mis)-use of wrong entities to describe factual content (Kryscinski et al.,",6,7
765,233296059,"We first identify prominent modes of hallucination by conducting a systematic human study on generated responses which reveals one major source of hallucination as the (mis)-use of wrong entities to describe factual content (Kryscinski et al.,",22,23
766,233296059,"To do so, the module combines a token-level hallucination critic that masks out entities of concern in an utterance, followed by a pre-trained nonautoregressive LM which prescribes contextual representations for each masked entity.",11,12
767,233296059,"We propose NEURAL PATH HUNTER, which leverages facts supplied by a KG to reduce hallucination in any machine-generated response. •",15,16
768,233296059,2020) for KG-grounded dialogue systems we hypothesize-among other possible mechanisms-hallucination can take form as either intrinsic or extrinsic to the provided KG.,16,17
769,233296059,An extrinsic hallucination corresponds to an utterance that brings a new span of text that does not correspond to a valid triple in G k c .,2,3
770,233296059,"From the perspective of definition 2.1, an utterance that might be partially faithful is still guilty of hallucination if there exists any injection of knowledge not authentically captured in G k c .",18,19
771,233296059,"1 contains an external hallucination as the entity in question ""Jay Roach"" did not direct the movie ""Titanic"" and it is not supported within the 1-hop subgraph.",4,5
772,233296059,An intrinsic hallucination corresponds to an utterance that A: Do you know the book The Witches?,2,3
773,233296059,Each annotator is tasked to first identify the presence of hallucination in the generated response when provided the dialogue history and KG triples.,10,11
774,233296059,"For samples where hallucination is present, we further ask the human annotators to identify whether the hallucination is extrinsic, intrinsic or both.",3,4
775,233296059,"For samples where hallucination is present, we further ask the human annotators to identify whether the hallucination is extrinsic, intrinsic or both.",17,18
776,233296059,A hallucination occurs the least in dialogue responses generated using a greedy decoding scheme.,1,2
777,233296059,"Conversely, top-k sampling results in the highest hallucination percentage (40.33%).",10,11
778,233296059,"Increased diversity in response generation -i.e.(less generic), is positively correlated with an increase in hallucination e.g. Nucleus=0.9.",16,17
779,233296059,Observation 1 indicates that the dominant mode of hallucination for all decoding strategies in KGgrounded dialogue systems is extrinsic rather than intrinsic.,8,9
780,233296059,"Observation 2 suggests that the modelwhen conditioned on factual knowledge-often assigns the highest probability mass to the correct response and sampling based on other distributions (e.g. top-k) invites hallucination in the generation process-a fact also observed in language modelling (Keskar et al.,",34,35
781,233296059,NPH is composed of two modules: A token-level hallucination critic and an entity mention retriever.,11,12
782,233296059,These entity mentions may not be faithful at all if they do not belong to either a [SBJ] or [OBJ] in M n (extrinsic hallucination) or they could inject false relationships between mentions via an unsupported path in G k c by incorrectly utilizing a [PRE] (intrinsic hallucination).,29,30
783,233296059,These entity mentions may not be faithful at all if they do not belong to either a [SBJ] or [OBJ] in M n (extrinsic hallucination) or they could inject false relationships between mentions via an unsupported path in G k c by incorrectly utilizing a [PRE] (intrinsic hallucination).,56,57
784,233296059,"Token-level hallucination critic To enforce faithfulness via refinement, we first identify the exact sources of hallucination in a given response.",3,4
785,233296059,"Token-level hallucination critic To enforce faithfulness via refinement, we first identify the exact sources of hallucination in a given response.",18,19
786,233296059,"Based on the findings of human judgement in Tab.2 and §2.1, we find hallucination errors in a dataset like OpenDialKG are often associated with entity mentions such as names of people, movies titles, locations, etc.",15,16
787,233296059,"To flag entities of concern, we design a token-level hallucination critic C that consumes D, K n , xn+1 and outputs the set of hallucinated entity mentions M c .",12,13
788,233296059,We explore two corruption processes that convert a regular clean ground-truth response x n+1 to its corresponding hallucinated one xn+1 based on the type of hallucination we might expect to encounter -i.e.,27,28
789,233296059,"For example, the response ""Crescendo was written by Becca Fitzpatrick"" → ""Becca Fitzpatrick was written by Crescendo"" results in an intrinsic hallucination as in this case [PRE] is not bidirectional.",26,27
790,233296059,"As there are no established metrics for this task, we consider a suite of task-specific and automated metrics to assess the different components of NPH and the degree of hallucination present.",32,33
791,233296059,We consider 3 different hallucination metrics M1-M3 that provide a multifaceted measure of performance.,4,5
792,233296059,"In the rest of the experiments, we take RoBERTa-Intrin-Extrin as the hallucination classifier C. Q2: Reducing Hallucinations We evaluate the ability of NPH in fixing hallucination in generated responses in the three response generation baselines.",16,17
793,233296059,"In the rest of the experiments, we take RoBERTa-Intrin-Extrin as the hallucination classifier C. Q2: Reducing Hallucinations We evaluate the ability of NPH in fixing hallucination in generated responses in the three response generation baselines.",31,32
794,233296059,We present the results in Table 3 which show the degree of hallucination prior to and after applying NPH on each response generation method.,12,13
795,233296059,We find that NPH consistently performs favourably in reducing hallucination across FeQA and the hallucination Critic.,9,10
796,233296059,We find that NPH consistently performs favourably in reducing hallucination across FeQA and the hallucination Critic.,14,15
797,233296059,"Human Evaluation In addition to the automated hallucination metrics, we conduct human evaluation to assess NPH's ability to reduce hallucination.",7,8
798,233296059,"Human Evaluation In addition to the automated hallucination metrics, we conduct human evaluation to assess NPH's ability to reduce hallucination.",21,22
799,233296059,We provide human annotators with 200 hallucinated responses per baseline( §4.2) as identified by our hallucination critic §3.2.,18,19
800,233296059,"We see that the hallucination critic achieves a precision of 97.5% for GPT2-KB responses, 95.5% for AdapterBot and 97.0% for GPT2-KE.",4,5
801,233296059,"Recently, little progress has been made for studying hallucination in open-domain dialog systems.",9,10
802,233296059,2021) study hallucination in knowledge-grounded dialogue systems and introduce a the BEGIN benchmark for measuring groundedness in dialogue systems.,3,4
803,233296059,"Conclusion In this work, we investigate the open problem of hallucination in KG-grounded dialogue systems and demonstrate that these models are more susceptible to extrinsic hallucinations which predominantly manifest as the injection of erroneous entities.",11,12
804,233296059,We empirically observe that NPH is capable of reducing hallucination when paired with a number of base dialogue models with relative improvements of 20.35% over vanilla GPT2 on FeQA.,9,10
805,233296059,We consider 2 other metrics that focus on measuring the degree of hallucination in the generated responses: Hallucination Critic We use our trained tokenlevel hallucination critic as a sentence-level hallucination detector.,12,13
806,233296059,We consider 2 other metrics that focus on measuring the degree of hallucination in the generated responses: Hallucination Critic We use our trained tokenlevel hallucination critic as a sentence-level hallucination detector.,25,26
807,233296059,We consider 2 other metrics that focus on measuring the degree of hallucination in the generated responses: Hallucination Critic We use our trained tokenlevel hallucination critic as a sentence-level hallucination detector.,32,33
808,233296059,"As input, the critic receives the dialogue history, the gold triples and the generated response and the output is a binary label indicating hallucination or not.",25,26
809,233296059,"F Human Evaluation of NPH responses Analogous to evaluating modes of hallucination, we solicit human evaluation from Appen 6 where we train English-speaking annotators for the task before starting the evaluation process.",11,12
810,233296059,"To highlight this point, let us consider the following example: In this example, we notice that the original response contains hallucination as ""John Green"" did not write the book titled ""The Sea of Monsters"".",23,24
811,248085885,"After that, we investigate and discover an interesting relationship between framing bias and hallucination, an important safety-related problem in generation tasks.",14,15
812,248085885,"To the best of our knowledge, this aspect of hallucination has not been previously discussed.",10,11
813,248085885,"In addition, we use a hallucination metric to evaluate if the generations contain any unfaithful hallu-cinatory information because the existence of such hallucinatory generations can make the summary fake news ( §5.3).",6,7
814,248085885,"Hallucination Metric Recent studies have shown that neural sequence models can suffer from the hallucination of additional content not supported by the input (Reiter, 2018; Wiseman et al.,",14,15
815,248085885,"Although not directly related to the goal of NEUS, we evaluate the hallucination level of the generations in our work.",13,14
816,248085885,"We choose a hallucination metric called FeQA (Durmus et al.,",3,4
817,248085885,This is a question-answering-based metric built on the assumption that the same answers will be derived from hallucination-free generation and the source document when asked the same questions.,21,22
818,248085885,One of the major contributors to high bias in the MDS models is probably the hallucination because MDS models portray drastically poor hallucination performance than all the other models (both the MDS models PEGASUSMULTI and BARTMULTI achieve SOURCE: <Left> Title: Here Are The 81 People And Entities Close To Trump Democrats Are Investigating.,15,16
819,248085885,One of the major contributors to high bias in the MDS models is probably the hallucination because MDS models portray drastically poor hallucination performance than all the other models (both the MDS models PEGASUSMULTI and BARTMULTI achieve SOURCE: <Left> Title: Here Are The 81 People And Entities Close To Trump Democrats Are Investigating.,22,23
820,248085885,11 This suggests that the framing bias of MDS models may be related to the hallucination of politically biased content.,15,16
821,248085885,Further Analysis and Discussion Q: Is hallucination contributing to the high framing bias in MDS models?,7,8
822,248085885,"This analysis unveils the overlooked danger of hallucination, which is the risk of introducing political framing bias in summary generations.",7,8
823,248085885,Note that this problem is not confined to MDS models only because other baseline models also have room for improvement in terms of the FeQA hallucination score.,25,26
824,248085885,"However, our results and analysis suggest that hallucination is another contributor to framing bias.",8,9
825,248085885,Leveraging hallucination mitigation techniques would be a valuable future direction for the NEUS task.,1,2
826,248085885,"Moreover, our work can also be used to facilitate hallucination research as well.",10,11
827,248085885,"The proposed framing bias met-ric could also be adapted to the hallucination problem without a ""neutral"" reference.",13,14
828,248085885,"The source input can substitute the ""neutral"" reference to measure if the generated summary is more politically biased than the source -a potential indication of political hallucination.",28,29
829,247446806,"It also implements a templating strategy of target text with data variables (e.g., cells, axis labels) to alleviate hallucination problems.",22,23
830,247446806,"4a ,b are quite fluent, they contain hallucination errors.",9,10
831,226226686,"Encouragingly, hallucination (generating details not in the original document) is not a major issue for these models (notwithstanding that almost 20% of bad samples contain hallucinations).",2,3
832,220282266,The authors also use a data hallucination technique similar to the one of Silfverberg et al. (,6,7
833,220282266,"Interestingly, the improvements are significant also when we use hallucinated data, which indicates that our transliteration preprocessing step is orthogonal to monolingual data augmentation through hallucination.",27,28
834,220282266,"It is of note, though, that hallucination also does not offer any improvements in these language pairs.",8,9
835,220282266,"We also note that the improvements are orthogonal to those obtained by data augmentation through hallucination, even in typologically distant languages.",15,16
836,201058388,"In addition, we investigate the effects of cross-lingual transfer from single and multiple languages, as well as monolingual data hallucination.",23,24
837,201058388,"This is achieved through the combination of a novel decoder architecture, a training regime that alleviates the need for costly structural biases that force attention monotonicity, and a data hallucination technique.",31,32
838,201058388,"Overall, we hallucinated 10,000 examples for each lowresource language, creating an additional hallucinated dataset H. A visualized example of our hallucination process is outlined in Figure 2 .",22,23
839,201058388,"2017) have proposed a data hallucination method conceptually quite similar to ours, which treats the single longest common continuous substring between lemma and form as the stem.",6,7
840,201058388,Monolingual data hallucination is crucial due to the distance of the languages.,2,3
841,201058388,"Although the large improvements are also proportionally reflected in the test set, these numbers are not directly comparable to the rest, as the development set data were used in the hallucination process.",32,33
842,201058388,"For fairer comparison to crosslingual transfer, we repeated the hallucination process as many times as candidate transfer languages and we report the mean and standard deviation of the test set accuracy.",10,11
843,201058388,"Encouragingly, this entails that hallucination is a viable option for entire language families without a single high-resource representative or low-resource isolates.",5,6
844,201058388,"The use of sparsemax in conjunction with our twostep decoder process, as well as along our data hallucination technique, presents a promising direction towards even better results in the future.",18,19
845,201058388,"Conclusion With this work we advance the state-of-the-art for morphological inflection on low-resource languages by 15 points, through a novel architecture, data hallucination, and a variety of training techniques.",32,33
846,201058388,Monolingual data hallucination is crucial due to the distance of the languages.,2,3
847,207852642,"We also find that the most common error is omitting information, rather than hallucination.",14,15
848,237558712,"Rotowire poses multiple challenges for neural systems: it requires content selection and production of longer texts, and its human-written training texts are themselves not always grounded in data, which makes neural models more susceptible to hallucination.",40,41
849,237416518,"These include hallucination effects (generated phrases not supported or contradictory to the source data), missing facts (generated text does not include input information), intersentence incoherence, and repetitiveness in the generated text.",2,3
850,237558790,We use this property of the texts to measure the performance of models with respect to the following errors: hallucination of content; omission of content; and repetitions of content.,20,21
851,203952980,"The donation persuasion system trained with TransferTransfo and our model has some common problems, such as inconsistency, lack of logic, and hallucination.",24,25
852,232404053,We investigate their relationship in both image captioning and data-to-text generation and propose a simple extension to beam search to reduce hallucination.,25,26
853,232404053,Our analysis shows that higher predictive uncertainty corresponds to a higher chance of hallucination.,13,14
854,232404053,Epistemic uncertainty is more indicative of hallucination than aleatoric or total uncertainties.,6,7
855,232404053,It helps to achieve better results of trading performance in standard metric for less hallucination with the proposed beam search variant.,14,15
856,232404053,"However, along with these improvements, researchers find that neural models are more prone to a phenomenon called hallucination, where models generate description tokens that are not supported by the source inputs.",19,20
857,232404053,2018) attributes object hallucination in image captioning to visual misclassification and over-reliance on language priors; Nie et al. (,4,5
858,232404053,2019) believes hallucination in neural surface realization comes from the misalignment between meaning representations and their corresponding references in the dataset; Müller et al. (,3,4
859,232404053,We believe that there is a common theme across all the hallucination explanations in conditional NLG tasks: predictive uncertainty.,11,12
860,232404053,This study draws connections between hallucination and predictive uncertainty and empirically investigates their relationship in image captioning and data-to-text generation tasks.,5,6
861,232404053,We propose an uncertainty-aware beam search algorithm to reduce the chance of hallucination by penalizing parts or the entirety of the predictive uncertainty during model decoding.,14,15
862,232404053,Our contributions are: • We draw connections between hallucination and predictive uncertainty across various conditional natural language generation tasks and empirically investigate their relationship. •,9,10
863,232404053,We propose an uncertainty-aware beam search approach for hallucination reduction to demonstrate that lowering uncertainty can lead to less hallucination. •,10,11
864,232404053,We propose an uncertainty-aware beam search approach for hallucination reduction to demonstrate that lowering uncertainty can lead to less hallucination. •,21,22
865,232404053,We show that uncertainty decomposition helps to achieve better trade-offs between hallucination and performance.,13,14
866,232404053,"2 Hallucination and Predictive Uncertainty Hallucination Probability In general, hallucination refers to the phenomenon where the model generates false information not supported by the input.",10,11
867,232404053,"For example, in the context of image captioning, hallucination can be defined as generating captions that contain descriptions not present in the given image.",10,11
868,232404053,"Apparently, hallucination is context-dependent which means we need to look at a certain context c i and determine whether the next token prediction y i is hallucinated or not.",2,3
869,232404053,"The probability of hallucination at the current step is simply: P (y i ∈ V (c i ) h ) = v∈V (c i ) h p(y i = v|c i ) (2) Practically, it is hard to automatically determine the context-dependent set V (c i ) h .",3,4
870,232404053,"In specific restrictive applications, the context-dependent set can be relaxed to a context-independent one to reduce the complexity of determining hallucination.",25,26
871,232404053,The second source of uncertainty is directly related to hallucination probability.,9,10
872,232404053,"Although no monotonic relationship can be derived, a near-zero hallucination probability requires a near-zero value of the second source of uncertainty.",12,13
873,232404053,This observation prompts us to investigate the relationship between hallucination and predictive uncertainty in practice.,9,10
874,232404053,We are interested in whether the relationship with hallucination is the same for both types of uncertainties.,8,9
875,232404053,Hallucination Probability at Different Uncertainty Levels The first question we want to investigate is whether hallucination probabilities change at different predictive uncertainty levels.,15,16
876,232404053,Results and Discussions Figure 2 shows the object hallucination percentages at different predictive uncertainty levels.,8,9
877,232404053,The transformer model seems to have a higher hallucination chance at high uncertainty levels than the other three models.,8,9
878,232404053,"In fact, the transformer model has an overall lowest hallucination percentage among all four models.",10,11
879,232404053,"Beyond object hallucination Aside from object hallucination, we also analyze verbs generated by the models to see whether a similar relationship holds for other types of token generations.",2,3
880,232404053,"Beyond object hallucination Aside from object hallucination, we also analyze verbs generated by the models to see whether a similar relationship holds for other types of token generations.",6,7
881,232404053,"Noticeably, the transformer model also has a higher action hallucination rate at high uncertainty levels.",10,11
882,232404053,"For example, Figure 3 Epistemic and aleatoric uncertainties As we could decompose the total uncertainty into two parts, we are interested in which part is more indicative of hallucination.",30,31
883,232404053,Table 2 shows the Pear- son correlation coefficients between hallucination (binary) and epistemic/aleatoric uncertainty for all four models.,9,10
884,232404053,"We can see that both parts of uncertainty are weakly correlated with hallucination, while epistemic uncertainty is more indicative of hallucination across all four models compared to aleatoric uncertainty.",12,13
885,232404053,"We can see that both parts of uncertainty are weakly correlated with hallucination, while epistemic uncertainty is more indicative of hallucination across all four models compared to aleatoric uncertainty.",21,22
886,232404053,"Neural models are prone to hallucination in data-to-text generation tasks compared to traditional template-based systems, and methods are proposed to improve faithfulness (Wiseman et al.,",5,6
887,232404053,"In this section, we discuss the relationship between predictive uncertainty and hallucination in data-to-text generation with ToTTo dataset (Parikh et al.,",12,13
888,232404053,"Reducing Hallucination Uncertainty-Aware Beam Search Because of the positive correlation between hallucination probability and predictive uncertainty, it is straightforward to incorporate uncertainty into the caption generation process to reduce hallucination.",13,14
889,232404053,"Reducing Hallucination Uncertainty-Aware Beam Search Because of the positive correlation between hallucination probability and predictive uncertainty, it is straightforward to incorporate uncertainty into the caption generation process to reduce hallucination.",32,33
890,232404053,"Therefore, we expect to see a trade-off between the quality of generated captions and the chance of hallucination.",20,21
891,232404053,"In the first example, we can see that a medium penalty weight of 20 not only helps avoid the hallucination of a table but also adds correct information about the color of the flowers.",20,21
892,232404053,Both the number of objects and hallucination percentage decrease as we increase the weight λ.,6,7
893,232404053,"In comparison, epistemic UABS keeps the generic response rates low while achieving lower hallucination rates.",14,15
894,232404053,We can see that a relatively small penalty weight leads to a reduced hallucination chance (hence more faithful) with a cost on the BLEU score and fluency.,13,14
895,232404053,Related Work Hallucination There are many pieces of anecdotal evidence of hallucination presented in various NLG tasks.,11,12
896,232404053,2018) analyzes object hallucination focusing on the objects that appeared in the MSCOCO segmentation challenge.,4,5
897,232404053,They propose the CHAIR metric to quantify the severity of object hallucination.,11,12
898,232404053,Therefore hallucination is caused by an over-reliance on the language priors.,1,2
899,232404053,2019) believes that the origin of the hallucination problem in neural surface realization comes from the data side.,8,9
900,232404053,2019) examines hallucination in neural machine translation and observes that the phenomenon is most common in out-of-domain settings.,3,4
901,232404053,"For example, domain shift and data misalignment are known to lead to a higher level of epistemic uncertainty (Kendall and Gal, 2017) which makes hallucination a more severe problem.",28,29
902,232404053,Discussion and Conclusions We investigate the relationship between hallucination and predictive uncertainty in image captioning and data-to-text generation tasks and show that predictions with higher uncertainty are more prone to hallucination.,8,9
903,232404053,Discussion and Conclusions We investigate the relationship between hallucination and predictive uncertainty in image captioning and data-to-text generation tasks and show that predictions with higher uncertainty are more prone to hallucination.,34,35
904,232404053,"In particular, epistemic uncertainty is more indicative of hallucination than aleatoric uncertainty.",9,10
905,232404053,We propose uncertainty-aware beam search to incorporate uncertainty into the decoding process to reduce hallucination.,16,17
906,232404053,"The relationship between hallucination and uncertainty, therefore, can be much more complicated than a linear one.",3,4
907,232404053,The proposed UABS reduces hallucination by limiting the total uncertainty of the generated text.,4,5
908,231861515,"In order to assess the frequency of hallucination and supererogation, we randomly selected 100 toxic comments from the test set and manually labeled the generated sentences with the nonmutually exclusive labels ""contains supererogation"" and ""contains hallucination"".",7,8
909,231861515,"In order to assess the frequency of hallucination and supererogation, we randomly selected 100 toxic comments from the test set and manually labeled the generated sentences with the nonmutually exclusive labels ""contains supererogation"" and ""contains hallucination"".",39,40
910,231861515,We counted on average 17% of generated sentences with surrerogation and 34% of generated sentences showing hallucination (often local).,18,19
911,231861515,"We observe that the longer the input comment, the more prone to hallucination is the generated text.",13,14
912,231861515,"While supererogation and hallucination can be explained by the probabilistic nature of generation, we assume that position reversal is due to bias in the dataset, where toxic comments are correlated with negative comments.",3,4
913,231861515,"Table 7 : Examples of automatically transferred test sentences by our system, valid rewriting, and highlighted flaws failure in attribute transfer or fluency, h h h h h h supererogation, @ @ @ @ @ @ @ position reversal, and @ @ @ @ @ h h h h h hallucination.",55,56
914,231861515,"Input trump is doing right things, he is great president, a lot better than 44th one from kenya lot of good people support trump, only crooked and dumb people against CAE-T5 trump is doing great, i know, he is better than most of the other @ @ @ @ h h h h  @ @ @ @ @ @ @ position reversal, and @ @ @ @ @ h h h h h hallucination.",82,83
915,231861515,"Table 10 : Examples of automatically transferred civil test sentences by our system, valid rewriting, and highlighted flaws failure in attribute transfer or fluency, h h h h h h supererogation, @ @ @ @ @ @ @ position reversal, and @ @ @ @ @ h h h h h hallucination.",56,57
916,235313949,"Artificially increasing p gen does not significantly change this behavior, introducing increased rates of synonym substitution as well as increased rates of non-faithful hallucination.",26,27
917,235313949,"This comes after a hallucination, ""firefighters are continuing to search for a man"" even though the article describes the rescue from the rollercoaster crash in the past tense.",4,5
918,235352668,AgreeSum could allow practitioners of abstractive summarization systems to carefully study topics related to faithfulness and hallucination.,16,17
919,235352668,"It is timely to revisit the problem also because recent stronger neural abstractive summarization models are prone to hallucination, as are neural text generation in general (Wiseman et al.,",18,19
920,235352668,Summarization hallucination and evaluation.,1,2
921,235352668,"Non-hallucination is a necessary but not sufficient condition for performing well in AgreeSum: the summary not only needs to be entailed in the union of the articles, but also must be entailed in each of the articles.",2,3
922,235352668,"Unfortunately, recent works have shown the difficulty of identifying and mitigating hallucination (Maynez et al.,",12,13
923,235352668,"2021) have made progress in creating token-level hallucination detectors which rely on negative (i.e., hallucination) data augmentation to train.",10,11
924,235352668,"2021) have made progress in creating token-level hallucination detectors which rely on negative (i.e., hallucination) data augmentation to train.",19,20
925,235352668,2) Model B4 (merging encodings and decode) is omitted from the table given very poor ROUGE performance (∼10) and extensive hallucination.,25,26
926,235352668,"In addition, we see that ASM reduces hallucination compared to B2 and B3.",8,9
927,235352668,One way to approximate hallucination is by the number of clusters in which none of the articles entail the generated summary.,4,5
928,235352668,"A hallucination or non-entailment 10 can have major text-span overlaps with the reference, thereby having a large ROUGE score.",1,2
929,235352668,"Thus, practitioners need to rely on and determine the desired tradeoff between the following two automatic metrics: (1) ROUGE as a coarse proxy for summary quality and informedness, and (2) entailment-related and hallucination-related metrics (Section 6.1).",41,42
930,250390650,"However, claiming ""no evidence"" and ""In fact"" can easily lead to misinformation or hallucination as it is almost impossible to guarantee that there is no evidence on some topics.",18,19
931,250390650,"Because it is difficult for non-expert annotators to distinguish fact from misinformation, and harmless hallucination from problematic hallucination, additional factchecking procedures would be desirable to solve this problem.",17,18
932,250390650,"Because it is difficult for non-expert annotators to distinguish fact from misinformation, and harmless hallucination from problematic hallucination, additional factchecking procedures would be desirable to solve this problem.",20,21
933,51876975,"We hypothesize that the hallucination issue for COCO-based models comes from the high correlations present in the COCO data (e.g., if there is a kid at a table, there is also cake).",4,5
934,51876975,"COCO only contains natural images, and therefore a cartoon image like the fourth one results in massive hallucination effects for COCO-trained models (""stuffed animal"", ""fish"", ""side of car"").",18,19
935,51876975,"A significant weakness of these metrics is that hallucination effects are under-penalized (a small precision penalty for tokens with no correspondent in the reference), compared to human judgments that tend to dive dramatically in the presence of hallucinations.",8,9
936,51876975,"The results indicate that such models achieve better performance, and avoid some of the pitfalls seen with COCO-trained models, such as object hallucination.",26,27
937,235593406,"Furthermore, such models are able to create plausible but not necessarily true arguments -a problem known as content hallucination - (Zellers et al.,",19,20
938,235593406,"Second, while our approach reduces the risks of content hallucination, an additional step, where the accuracy of the generated text is checked against the provided knowledge (Nie et al.,",10,11
939,248118521,"If the knowledge candidate set is contaminated by fake news, the response generated by the dialogue system is likely to suffer from the ""hallucination"" issue.",25,26
940,248780263,"In this way, we disable hallucination of free text by the model, while keeping the graph structure intact with the MASK placeholder.",6,7
941,248780263,We masked the unseen source with copying to avoid excessive hallucination in program prediction.,10,11
942,234334020,"Here, the second sentence is a hallucination error.",7,8
943,247793148,"8 We also evaluate the number of omission and hallucination errors (i.e., facts missing or added, respectively) using a metric from Dušek and Kasner (2020) based on a RoBERTa model (Liu et al.,",9,10
944,239015959,"With recent developments in learned evaluation metrics that penalize the model for hallucination, fluency, etc.,",12,13
945,236034557,"On the one hand, we showed that this results in less model hallucination, and more factually correct generations.",13,14
946,237347122,"Noticeably, we observe some hallucination facts (False Positives) where some diseases are mistakenly described as positive in the medical reports.",5,6
947,232269604,"In order to make sure the model paid attention to the low resource training data, we either oversampled it 100 times or used data hallucination (Anastasopoulos and Neubig, 2019) to generate synthetic examples.",25,26
948,222290728,We propose to estimate the noise degree by comparing the source with the target thus obtaining a hallucination knob.,17,18
949,222290728,In our work we are concerned with the former hallucination kind which is primarily caused by imperfect quality of the training data.,9,10
950,222290728,"In contrast to the described approaches, our proposal is to train the model on the data as is without modifying the decoding (and encoding) architecture but instead introduce a handle on the input side to control the degree of hallucination (Fig.",42,43
951,222290728,"With this ""hallucination knob"" one can minimize (or maximize) the amount of unsupported information in the output during generation (Fig.",3,4
952,222290728,"The hallucination or noise degree of every training instance is estimated separately and converted into a categorical value which becomes part of the input, like in a controlled generation setting (Ficler and Goldberg, 2017; Raffel et al.,",1,2
953,222290728,We define a special vocabulary of hallucination degrees and add such a degree as a prefix to the input for every datapoint.,6,7
954,222290728,"Both methods give us a hallucination score hal ∈ [0, 1] for every sourcetarget pair.",5,6
955,222290728,"During training, the data2text model learns an embedding for each of the five tags and during inference the tag with the lowest hallucination value, hal 0, is used (Fig.",23,24
956,222290728,The smaller loss of LM on the next token (french) will signalize the presence of a hallucination.,18,19
957,222290728,"2019) , which was designed to generate hallucination-free output.",8,9
958,222290728,It differs from LM in that the latter takes no input and the only difference to the controlled models is that they prepend the input with a single hallucination tag.,28,29
959,222290728,Figure 2 shows how the amount of hallucinations in the output increases following the value of the hallucination knob.,17,18
960,226283600,"ii) in the supervised setting, such as table-to-text generation, partially matching the input and target sequences can effectively avoid hallucination generation, i.e. text mentions extra information than what is present in the source.",26,27
961,236087808,"In particular, for each pair, we asked the operators: (a) to approve it without any modifications if it was a valid pair, (b) if the pair was not perfect, but easily amendable, to modify it, (c) if the CN is completely irrelevant, or does not follow NGO's guidelines, to discard the pair regardless of HS quality, (d) whenever there are facts or statistics in the CN, check veracity of the information to avoid possible LM hallucination effects.",95,96
962,236087808,"Still the lack of a ""commonsense knowledge"" can produce funny results that are beyond the scope of hallucination (Zellers et al.,",19,20
963,247940075,"2020) , and the generation of knowledge-bound CNs, which allows the production of CNs based on grounded and up-todate facts and plausible arguments, avoiding the hallucination phenomena (Chung et al.,",32,33
964,166228140,"Generated output from neural NLG systems often contain errors such as hallucination, repetition or contradiction.",11,12
965,166228140,"By contrast, neural NLG systems need only a well collected dataset to train their models and generate fluent sounding utterances but have notable problems, such as hallucination and a general lack of adequacy (Wiseman et al.,",28,29
966,225067001,"However, this results in the generation of Wikipedia-like sentences and hallucination if a certain expected input triple is missing.",13,14
967,225067001,This reduces conformity to Wikipedia sentence structure and hence reduces hallucination.,10,11
968,225067001,"Such groups are not available for a large KG and using one triple at a time for inference would lead to hallucination as training uses multi- Generation For each entity subgraph, we concatenate all its triples as before.",21,22
969,225067001,"Following prior work, the generated text is rated for two aspects-fluency and semantics, on a scale of 1-5, where 1 means not at all fluent/does not capture meaning at all and 5 means completely fluent/fully captures meaning with no hallucination.",50,51
970,237364020,"Instead, we opted for Gigaword and Wikihow, which are datasets with substantial abstraction without as much hallucination problems as XSum.",18,19
971,237364020,"2020) specifically target hallucination of quantities in generated summaries, and train a verification model that they use to re-rank summaries such that summaries containing quantities consistent with the source article are up-ranked.",4,5
972,233296487,"We analyze the typical hallucination phenomenon by different types of neural summarization systems, in hope to provide insights for future work on the direction.",4,5
973,233296487,"It suffers from extrinsic hallucination, where information not present in the source document was generated.",4,5
974,233296487,"2020) , who have annotated hallucination categories of generated summaries from four neural summariazaiton models: PTGEN (See et al.,",6,7
975,222291532,"Furthermore, we want a natural distribution of errors produced by generation models, such as wrong subject or objects for a verb or hallucination of new content.",24,25
976,222291532,"For example, it is better at identifying text hallucination, or cases where the subject object relation between words change, but has comparatively lesser accuracy over changes such as synonym replacements.",9,10
977,207624,"For this, we define a binary hallucination sequence h K 1 : if h k = 0, then NULL → v k ; if h k = 1 then e a k → v k .",7,8
978,207624,"It necessarily depends on the hallucination variable p(a j |a j−1 , h j ; l) =    1 a j = a j−1 , h j = 0 0 a j = a j−1 , h j = 0 a(a j |a j−1 ; l) h j = 1 This formulation allows target phrases to be inserted without disrupting the Markov dependencies of phrases aligned to actual source words.",5,6
979,207624,The hallucination process is a simple i.i.d.,1,2
980,207624,"The hallucination process is motivated by the use of NULL alignments into Markov alignment models as done by (Och and Ney, 2003) .",1,2
981,207624,"It can be calculated recursively (omitting the hallucination process, for simplicity) as α j (i, φ) = i ′ ,φ ′ α j−φ (i ′ , φ ′ )a(i|i ′ , l) • η • n(φ; e i ) • t(f j−φ+1 |e i ) • j j ′ =j−φ+2 t 2 (f j ′ |e i ) This recursion is over a trellis of l(N + 1)m nodes.",8,9
982,243838026,"On the other hand, some of the predictions reveal common mistakes from sequence-to-sequence models, such as repetition or omission of tokens and ""hallucination"" of new unwanted information.",29,30
983,220285629,"Thus, DTLM is able to take advantage of existing multi-lingual text corpora, such as Wikipedia, to improve its accuracy on P2G. Since we have no access to any corpora of phonetic transcriptions, the language model component is not used for G2P. Data Augmentation Inspired by the data hallucination technique for neural model training (Silfverberg et al.,",53,54
984,219965661,"For languages with small training sets, we also perform hallucination pretraining (Anastasopoulos and Neubig, 2019) , where we generate pseudo training instances for the task, based on suffixation and prefixation rules collected from the original dataset.",10,11
985,219965661,"While this might be effective, we believe that using multitask training in combination with hallucination pretraining can give the model enough information to learn the task well, while staying true to the specific structure of each individual language.",15,16
986,219965661,"Methods In this section, we introduce our models for Tasks 0 and 2 and describe all approaches we use, such as multitask training, hallucination pretraining and ensembling.",26,27
987,219965661,"Hallucination Pretraining Another effective tool to improve training in the low-resource setting is data hallucination (Anastasopoulos and Neubig, 2019).",16,17
988,219965661,"Using hallucination, new pseudo-instances are generated for training, based on suffixation and prefixation rules collected from the original dataset.",1,2
989,219965661,"Ablation Studies Our systems use three components on top of the vanilla transformer: a copy mechanism, multitask training and hallucination pretraining.",21,22
990,219965661,"Comparing models 2 and 4, which are both trained on the original dataset, pretrained with hallucination and differ only by the use of the copy mechanism, we are able to see that adding this component slightly improves performance by 0.06−0.16%.",17,18
991,219965661,"In order to examine the effect of hallucination pretraining on our submitted models, we now compare the pointergenerator transformers trained on the multitask data with and without hallucination pretraining (models 1 and 5).",7,8
992,219965661,"In order to examine the effect of hallucination pretraining on our submitted models, we now compare the pointergenerator transformers trained on the multitask data with and without hallucination pretraining (models 1 and 5).",28,29
993,219965661,"For Task 0, we further added multitask training and hallucination pretraining.",10,11
994,216080466,"I could go on and on about the negative aspects of the movie, like the terrible acting and the lengthy scenes where Cage is looking for the girl, has a hallucination, followed by another hallucination, followed by a dream sequence-with a hallucination, etc.,",32,33
995,216080466,"I could go on and on about the negative aspects of the movie, like the terrible acting and the lengthy scenes where Cage is looking for the girl, has a hallucination, followed by another hallucination, followed by a dream sequence-with a hallucination, etc.,",37,38
996,216080466,"I could go on and on about the negative aspects of the movie, like the terrible acting and the lengthy scenes where Cage is looking for the girl, has a hallucination, followed by another hallucination, followed by a dream sequence-with a hallucination, etc.,",47,48
997,250390522,2022) : The team from Heinrich-Heine-Universität Düsseldorf developed a system with a self-attention Transformer architecture with bigram hallucination.,24,25
998,250390496,We further apply data hallucination and lemma copying to augment training data.,4,5
999,250390496,"We instead use the data hallucination approach by Anastasopoulos and Neubig (2019) , which synthesizes new training examples from existing gold standard training examples by identifying a (possibly discontinuous) word stem and replacing this with a random character sequence.",5,6
1000,250390496,"In addition to data hallucination, we experiment with another data augmentation technique: lemma copying (Liu and Hulden, 2022) , where the model is trained to copy input lemmas from the test set in order to adapt the model more closely to the test data.",4,5
1001,250390496,"In our experiments, this method ultimately delivers better performance than data hallucination.",12,13
1002,250390496,Our experiments show that student forcing can deliver small improvements for some languages but does not outperform data hallucination.,18,19
1003,250390496,We investigate data hallucination and lemma copying as ways to prompt better generalization to lemmas missing from the training set.,3,4
1004,250390496,Lemma Copying The data hallucination method introduced by Anastasopoulos and Neubig (2019) can sometimes create invalid examples due to phonological alternations as noted by Samir and Silfverberg (2022) .,4,5
1005,250390496,"Although data hallucination has been shown to counter overfitting in such scenarios, we additionally adopt the student forcing approach described by Nicolai and Silfverberg (2020) .",2,3
1006,250390496,"Under both data conditions, we train models using the following procedure: We first augment the training data using data hallucination or lemma copying.",21,22
1007,250390496,"SF refers to student forcing, HALL to data hallucination and COPY to lemma copying.",9,10
1008,250390496,Results from data hallucination HALL are mixed.,3,4
1009,250390496,"Conclusion In this work, we advance the generation performance of inflectional forms with a joint effort including reverse positional encoding, data hallucination, copying lemmas, and student forcing.",23,24
1010,250390496,"Our findings suggest that not only is data hallucination beneficial for low-resource morphological inflection, but that it is a necessary step in the inflectional pipeline.",8,9
1011,250390496,"SF refers to student forcing, HALL to data hallucination and COPY to lemma copying.",9,10
1012,250390457,"We found that, bigram hallucination provides better inferences only for English and Arabic and only when the number of hallucinations remains low.",5,6
1013,250390457,"This extreme data sparsity calls for the use of data hallucination techniques commonly used for lowresourced NLP development (Chen et al.,",10,11
1014,250390457,"System Description In this section, we describe the neural network architecture, the data hallucination process and the submissions.",15,16
1015,250390457,In order to check this hypothesis we have set up an alternative hallucination procedure.,12,13
1016,250390457,"As can be seen from the plots, the proposed bigram hallucination algorithm provides better results for English and Arabic data, if we do not produce too many hallucinations (1,000 hallucinations are better than 10,000, which was the original size in Anastasopoulos and Neubig (2019) ).",11,12
1017,250390457,"As the models trained on the proposed bigram hallucination algorithm provides better results on the development set for English and Arabic with 1000 hallucinations across all training sizes, this would have been our alternate submission.",8,9
1018,250390457,Our experiment with restricting the hallucination process to generate forms that are phonotactically attested (bigram) in the training data revealed that its benefit was found only in very restricted conditions depending on the amount of hallucinated samples and the specific language (and presumably the inflectional pattern).,5,6
1019,250390457,Our findings are in agreement with the detailed error analyses of data hallucination techniques by Samir and Silfverberg (2022) which concluded that hallucination is not a one-size-fits-all technique and it must be used with caution and requires closer inspection depending on the type of morphological inflections.,12,13
1020,250390457,Our findings are in agreement with the detailed error analyses of data hallucination techniques by Samir and Silfverberg (2022) which concluded that hallucination is not a one-size-fits-all technique and it must be used with caution and requires closer inspection depending on the type of morphological inflections.,24,25
1021,221949193,"In: (15) Brandmanden placerede sine sko i skabet (REFL) we compute the sentence perplexity replacing the reflexive pronoun sine with a feminine 6 In the context of examples such as Example ( 9 ) and ( 10 ), using an anti-reflexive pronoun in the target translation may seem more like a hallucination than violating grammatical constraints, and we acknowledge that in machine translation, as well as in language modeling, the difference concerning existing gender bias challenge datasets is less pronounced than with NLI and coreference resolution.",59,60
1022,1530336,The hallucination operation is implemented by the function EXTEND-STATE.,1,2
1023,1530336,"It is invoked when the function EXPAND-STATE returns the empty set (as will happen when input state's link-group list is empty) and returns states with the new link-groups added on, one for each of the allowed hallucination classes.",46,47
1024,1530336,"These states are assigned a feature noting the hallucination, sub-categorized by the semantic class of the hallucinated object.",8,9
1025,1530336,"A state descended from one extended by hallucination cannot be extended again, and if it runs out of link-groups before connecting all fragments it is declared ""dead"" and removed from the queue.",7,8
1026,1714108,"Trial and error led us to choose a1=1, a2=10, a3=0, which penalizes hallucination of incorrect constituents (modeled by [Q \ T]) more heavily than a shortfall in completeness (modeled by IQ n TI).",15,16
1027,216869577,"Existing training procedures for models seek to match the underlying distribution, leading to models that replicate and sometimes even amplify unwanted behaviors such as hallucination during generation.",25,26
1028,216869577,We show an example of a reference that requires hallucination in Figure 2 .,9,10
1029,216869577,"Studying the log loss of these examples 1 , we note that the average log loss of titles that require new facts is over 1.7× the average loss of the titles that are directly entailed (Table 1 ) and the high-loss examples are clearly dominated by examples which require hallucination (Figure 3 ).",52,53
1030,216869577,"In fact, we find that over 80% of examples with greater than 40 log loss requires some form of hallucination.",21,22
1031,216869577,We present examples of titles in Figure 6 that require factual hallucination and can be directly entailed from context.,11,12
1032,237355093,"Finally, Wang and Sennrich (2020) discusses the potential link between exposure bias and hallucination in the context of machine translation.",16,17
1033,218487159,"We posit that if data hallucination has been shown to improve performance in the language-agnostic setting (Anastasopoulos and Neubig, 2019; Silfverberg et al.,",5,6
1034,218487159,"2017) , than it is likely that linguistically-informed hallucination can provide a similar reinforcement in Kunwinjku.",11,12
1035,218487159,"Looking at the errors, we find that the imprecise predictions were all applied to instances about which the system was already wrong in previous models, meaning that the impact of reduplicative hallucination between models was only positive.",33,34
1036,237440458,"To validate this last hypothesis, we carried out an hallucination analysis on the results of the domain shift experiments.",10,11
1037,237440458,"Although their interest was in detecting those sentences that induced the generation of hallucinations after introducing spurious tokens in the input, we adapted it to automatically measure the number of input sentences in a test set for which the corresponding output seems to be an hallucination.",46,47
1038,237440458,"If the sentence-level adjusted BLEU of the lowercased emitted translation is below a certain threshold (10 in our experiments), it is taken as a sign of hallucination.",31,32
1039,237440458,"We evaluate the tendency to produce hallucination of the baseline as compared to our MTL DA approach combining the auxiliary tasks reverse, mono and replace, and to the SwitchOut, RAML and SwitchOut+RAML systems.",6,7
1040,237440458,We therefore count the number of sentences that induce an hallucination on one of the systems but not on the other.,10,11
1041,237440458,"which shows an hallucination in the form of a repetition, whereas the MTL DA method gives a better ""Um ein Objekt auszuwählen."".",3,4
1042,237440458,15 C Hallucinations We motivate here the choice of thresholds used in the hallucination detection approach discussed in Sec.,13,14
1043,237440458,"We thus consider that if no single token co-occurs, an hallucination is happening.",13,14
1044,237440458,"Domain Table 10 : An output sentence, emitted by the baseline system, which was labelled as an hallucination (adjusted BLEU of 1.74).",19,20
1045,237440458,"At the bottom, a more extreme example of hallucination with an adjusted BLEU of 0.",9,10
1046,237507028,"2020) , meaning it contains hallucination; SUMMARY 2 is a repetition of ""the"".",6,7
1047,218487034,1 Our human annotated summaries for faithfulness and factuality will be released at https://github.com/google-researchdatasets/xsum hallucination annotations.,14,15
1048,218487034,Factual Hallucinations in Summarization A summary S of a document D contains a factual hallucination if it contains information not found in D that is factually correct.,14,15
1049,218487034,"Finally, since we conclude that hallucination is a problem on this dataset, then we can safely conclude it is a problem for summarization datasets with longer summaries, as modeling longer-distance dependencies and discourse structures make the task harder.",6,7
1050,218487034,"Experiments and Results The main focus of this work is not to propose a solution to hallucination related issues, but to achieve a better understanding of hallucinations in abstractive summarization through their human assessment.",16,17
1051,218487034,"For summaries containing hallucinations, annotators were tasked with (i) identifying those text spans that were unfaithful to the article and (ii) for each text span, annotating whether the hallucination was intrinsic or extrinsic.",34,35
1052,218487034,"However, the percentage of system summaries with intrinsic hallucination was much higher than in gold summaries (7.4% vs others).",9,10
1053,218487034,"The copy mechanism in PTGEN is good at copying from the source (showing the least percentage of extrinsic hallucination of 63.3%), but the mechanism lacks the inference capability and is prone to generate a summary that is not supported by the document (19.9% intrinsic hallucination).",19,20
1054,218487034,"The copy mechanism in PTGEN is good at copying from the source (showing the least percentage of extrinsic hallucination of 63.3%), but the mechanism lacks the inference capability and is prone to generate a summary that is not supported by the document (19.9% intrinsic hallucination).",50,51
1055,218487034,The BERTS2S showed the least number of intrinsic hallucination (16.9%) among all four abstractive systems.,8,9
1056,218487034,"2017) had the lowest extrinsic hallucination (63.3%), but BERTS2S reported the highest number of faithful summaries.",6,7
1057,218487034,"Finally, there were 1869 document-summary pairs where the summaries were marked with at least one intrinsic or extrinsic hallucination.",21,22
1058,218487034,along with the hallucination assessment.,3,4
1059,218487034,"Similar to the performance on extrinsic hallucination in Table 2 , the TCONVS2S abstracts also displayed the highest number of contradictions.",6,7
1060,218487034,We adapted the QA framework to assess hallucination in model generated summaries; a faithful model will generate a summary that only has information that is supported by its document.,7,8
1061,218487034,The GOLD abstracts were the least accurate due to a high number of extrinsic hallucination in them.,14,15
1062,218487034,"We found that (i) tackling hallucination is a critical challenge for abstractive summarization, perhaps the most critical, (ii) NLU-driven pretraining in neural text generators is key to generate informative, coherent, faithful and factual abstracts, but it is still far from solving the problem; and (iii) measures such as ROUGE or BERTScore will not be sufficient when studying the problem; semantic inference-based automatic measures are better representations of true summarization quality.",7,8
1063,218487034,"Our result demonstrates that (i) the effect of hallucination in BERTS2S is more local than what we observe in PTGEN and (ii) despite a lower number of extrinsically hallucinated spans or documents in PTGEN compared to that in BERTS2S (2.85 vs 3.04 spans per document, 63.3% vs 64.1% documents), the total number of words that were annotated as extrinsic hallucination is much higher in PTGEN than in BERTS2S (12075 vs 9302 words).",10,11
1064,218487034,"Our result demonstrates that (i) the effect of hallucination in BERTS2S is more local than what we observe in PTGEN and (ii) despite a lower number of extrinsically hallucinated spans or documents in PTGEN compared to that in BERTS2S (2.85 vs 3.04 spans per document, 63.3% vs 64.1% documents), the total number of words that were annotated as extrinsic hallucination is much higher in PTGEN than in BERTS2S (12075 vs 9302 words).",69,70
1065,218487034,"The numbers in ""Hallucinated"" columns show the percentage of summaries out of 500 where at least one word was annotated by all three annotators as an intrinsic (I) or extrinsic (E) hallucination.",37,38
1066,218487034,"When a summary is not marked with any hallucination, it is ""faithful"" (1-I∪E).",8,9
1067,2262674,"Another related phenomenon is that of statistical hallucination, e.g., the translation of AlswdAn w (literally, Sudan and) into enterprises and banks.",7,8
1068,235313847,Filippova (2020) ablates model input to control the degree of hallucination.,12,13
1069,235166394,"Despite its success, NMT still suffers from the hallucination problem, generating fluent but inadequate translations.",9,10
1070,235166394,"However, NMT faces the hallucination problem, i.e., translations are fluent but inadequate to the source sentences.",5,6
1071,235166394,"Apparently, the target sentence is a hallucination and will harm the model performance.",7,8
1072,235166394,"We conjecture the reason is that the target sentence is not corresponding to the source sentence in the training corpus, i.e., the target sentence is a hallucination.",28,29
1073,235166394,"NMT suffers from the hallucination and inadequacy problem for a long time (Tu et al.,",4,5
1074,235166394,"When the proportion of tokens  with negative Margin in a target sentence is greater than 30% or 40%, the sentence is most likely to be a hallucination.",30,31
1075,235166394,"In case 1, the base model generates ""on Tuesday"", which is unrelated to the source sentence, i.e., hallucination, and under-translates ""November 5"" and ""the website of the Chinese embassy in Mongolia"" information in the source sentence.",23,24
1076,235313435,"2019) , the generated results usually suffer from the phenomenon of hallucination (Nie et al.,",12,13
1077,235313435,"The text generation models such as Transformer-s2s and GPT2-finetune suffer from the problem of hallucination (Maynez et al.,",18,19
1078,40418416,"Awareness of this goal, or vision (or hallucination), has led me to make the claim that the delineations of system type, system purpose, and user type, and the evaluations that purport to measure them, are ultimately of less importance than the ""core technology"" that will eventually lead to the achievement of the goal.",9,10
1079,244345901,"In order for a summarizer to generate an accurate extrinsic hallucination, the summarizer must possess external world knowledge.",10,11
1080,236486103,2020) addressed the problem of unsupported information in the generated summaries known as factual hallucination.,15,16
1081,248780162,"This problem is known as the hallucination problem (Roller et al.,",6,7
1082,221819253,"A proper training scheme for the low-resource setting can be leveraged in future work, so it can reduce hallucination issue in the LSTM model.",21,22
1083,34362120,"Les patients ne seraient plus conscients d'être à l'origine de la PI produite et la percevraient alors comme venant d'un agent externe, transformant cette pensée verbale en hallucination.",28,29
1084,34362120,La troisième condition expérimentale était une condition hallucinatoire (HAV) pendant laquelle il était demandé aux patients de laisser libre cours à leurs HAVs et de signaler les début et fin de chaque hallucination par un appui sur le bip.,34,35
1085,199879758,4 Do you have any sort of hallucination and delusion?,7,8
1086,199879758,He does not have any sort of hallucination and delusion.,7,8
1087,249151889,"Despite coupling with process knowledge, we find that DLMs are still prone to hallucination, i.e., generating redundant, irrelevant, and unsafe FQs.",14,15
1088,249151889,"Using performance analysis based on MCC scores, we show that PRI-MATE is appropriate for identifying questions in PHQ-9 that could guide generative DLMs towards controlled FQ generation (with minimal hallucination) suitable for aiding triaging.",33,34
1089,238743793,We find that hallucinated entities are almost always generated in OOD vocab cases by the BERT model (99% of explanations consist of entities that do not exist in the premise and the hypothesis) and the hallucination rate is also high (around 60%) for the E-SNLI-pretrained model.,38,39
1090,238743793,"However, the hallucination rate is much lower for IND vocab cases (Fig.",3,4
1091,238743793,"But when k = 4, we observe a high hallucination rate (> 50%) for IND vocab cases (Fig.",10,11
1092,238743793,We also notice that pretraining on E-SNLI leads to models with much lower hallucination rates for all test cases.,15,16
1093,238743793,"4b show that both the BERT model and E-SNLI-pretrained model (trained with k = 4) hallucinate for OOD vocab, and the hallucination rate is slightly worse for OOD templates.",28,29
1094,235097634,We propose a method to compute image representations specific to each sentential context to minimize hallucination caused by sequence-to-sequence approaches and to further eliminate redundant content by exploiting diverse sentence states.,15,16
1095,235097634,We present a neural approach for producing radiology reports from images in a sentence-bysentence order to pinpoint more targeted and precise medical information from the input images and at the same time minimize hallucination from neural text generation.,35,36
1096,233365287,"Another major problem is the fact they suffer from a form of overfitting known as 'hallucination', where ungrounded output text is produced.",16,17
