,corpus_id,sentence,start_idx,end_idx
0,10247031,"makes prohibited) an Actant Slot (ASlot) of symbol s. The set of PUTs is denoted T and defined as the disjoint union of T D , the ranges of γ γ γ, γ γ γ 1 and γ γ γ 0 , plus the prime universal PUT and the prime absurd PUT ⊥ (eq.",10,11
1,10247031,"Thus as one goes down the hierarchy of unit types, an ASlot with symbol s is introduced by the radix {γ γ γ(s)} and first defines an optional ASlot for any unit type t ∩ more specific than {γ γ γ(s)}, as long as t ∩ is not more specific than the obligat {γ γ γ 1 (s)} (resp.",14,15
2,10247031,"5 The powerset of X is the set of all subsets of X: 2 X Finally, to each CSymbol is assigned a signature that specifies the type of units that are linked through a relation having this symbol.",39,40
3,1329203,"When parsing the context sentence, we replace each citation content with a <CITATION> symbol, in order to remove the contextual bias.",16,17
4,9374715,"A cross in this matrix designates an alignment valid in both directions, while the symbol indicates an uni-directional alignment (for has been aligned with einen, but not the other way round).",15,16
5,14430680,"We use "" "" as the symbol of the combination operator.",6,7
6,52121142,"Likely, this is related to the fact that character-level features such as casing, letter n-grams or appurtenance to the symbol and/or number class are useful in identifying proper names, dates, location etc.,",25,26
7,184486914,"2 for an example) is a set of ordered triples (i, j, ), where i and j are positions in the sentence (or a distinguished root symbol) and ∈ L is the label of the edge i → j in the tree; each position occurs exactly once as the first element in a triple.",32,33
8,250390920,Columns 2 and 3 did not contain useful information (only an underscore symbol was present).,13,14
9,18627932,"Finally, FilterFST traverses L to find all edges with a [S] symbol.",14,15
10,18627932,"From each one, it performs a depth-first search to find all paths to an edge with an [E] symbol, accumulating all [S]-and [E]-delimited spans.",23,24
11,18969658," For hashtags the '#' symbol is removed and the tag is split using the capital-letter rule described in (Hauff & Houben, 2011) .",7,8
12,16428009,"It relies on information about gene candidates, contexts of the symbol and external knowledge sources.",11,12
13,16428009,3) Gene candidate retrieval retrieves gene candidates for each gene symbol.,11,12
14,16428009,"4) Matching includes various algorithms that match contexts of a gene symbol to information about its gene candidates according to the mappings obtained in mapper, and returns similarity values between them.",12,13
15,16428009,5) Normalization ranking ranks gene candidates based on results of Matching for each gene symbol appearing in a text.,15,16
16,16428009,Pre.-2 is computed as the average of the precision of disambiguation for each gene symbol.,14,15
17,14458091,Those non-terminals include one symbol that presumes that the terms describe a pathological state and one that presumes the opposite. (,6,7
18,114910146,"A symbol that identifies the resolution, consist- ing of a number corresponding to the session, and a number corresponding to the ordinal position of this resolution in the series of resolutions adopted in this session; in the present example, this is 59/34.",1,2
19,114910146,"From the HTML format, the multiple language versions for the same resolution symbol (the identification numbers introduced earlier) were aligned, using the assumption that the translations were strict at the level of formatting as well as at the level of content.",13,14
20,114910146,"To this end, these documents generally contain all relevant context and make heavy use of fully-explicit and unambiguous document symbol  references.",22,23
21,114910146,"While it is not practical to identify all possible symbol variations, 7 we have developed a set of regular expressions to locate and mark a significant proportion of the symbols in the corpus.",9,10
22,6773023,"Such images often represent a mathematical symbol, which is important information to retain; • hr tags proved to be an important indicator for dividing header cells from data cells.",6,7
23,2300829,"It should be also noted, that stems are generated only for words from a given vocabulary (for other words OOV symbol is placed) and proper names, foreign words, spellings and abbreviations are recognized and special symbols are inserted instead of stems as in following example: plan|plan|5 był|być|106 w|*letter|0 pełni|pełnia|9 gotowy|gotowy|18 w|*letter|0 dziewięćdziesiątym|dziewięćdziesiąty|255 ósmym|ósmy|255 roku|rok|93 nosił|nosić|106 nazwę|nazwa|10 digital|oov|-2 Millennium|OOV|- 2 Copyright|OOV|-2 act|OOV|-2 .|.| Our tool uses Windows-1250 Eastern Europe character encoding, thus it was necessary to convert data from/to UTF-8 encoding used by all other tools.",22,23
24,211132778,"original uymam'=an wa isam=an hi okake ta modified uymam=an wa isam=an hi okake ta Though the equal symbol ('=') does not represent a sound, we keep it because it is used in almost all of the Ainu documents and provides grammatical information.",23,24
25,211132778,"CTC augments the output symbol set with the ""blank"" symbol 'ϕ'.",4,5
26,211132778,"CTC augments the output symbol set with the ""blank"" symbol 'ϕ'.",11,12
27,211132778,"p(L|X) = ∑ Π ∈B −1 (L) |Π |=|X| p(Π |X) (1) B is a function to contract the outputs of RNNs, so B −1 (L) means the set of symbol sequences which is reduced to L. The model is trained to maximize (1).",39,40
28,211132778,"=' and the special symbol '⟨wb⟩', which means a word boundary, are added to make it possible to convert the output into a sequence of words like the 'original' in Table 3 .",5,6
29,211132778,The former repeatedly replaces the most common character pair with a new single symbol until the vocabulary becomes the intended size.,13,14
30,211132778,The output of the syllable model is rewritten into words using the '⟨wb⟩' symbol.),15,16
31,9974205,The symbol @ denotes a null word.,1,2
32,6220464,"Since the probability of the input sequence should be summed up for all possible paths from the initial state, the probability P(L) of the input sequence L will be expressed as follows (state transition and word occurrence probabilities are assumed to depend on a single preceding state, i.e., are based on bigram model): n+1 P(L) = H wi,n+iEL so,n+1 i=1 A little modification is necessary in the forward and backward probabilities since some transition with a symbol may come from distinct states with the same name.",87,88
33,9411474,A special symbol 'NIL' represents any word not included in the concept.,2,3
34,15058909,"Figure 1 shows examples of the two problems with the MICG solution, where the square symbol denotes the chunk to be memorized and the diamond symbol denotes the chunk to be induced.",16,17
35,15058909,"Figure 1 shows examples of the two problems with the MICG solution, where the square symbol denotes the chunk to be memorized and the diamond symbol denotes the chunk to be induced.",26,27
36,11375700,"N k :i k /M k :j k , where N l , M l are NIL or a terminal or nonterminal symbol for language L 1 and L 2 , respectively, and i l , j l are natural numbers for the rank of the phrase in the sequence for L 1 and L 2 respectively (for NIL categories a special rank 0 is assumed).",24,25
37,203591833,"We encode the pair of mentions and their textual context as two consecutive sequences in BERT, separated by a special symbol.",21,22
38,203591833,The model encodes the pairs and their context in a sequence separated by a special symbol.,15,16
39,219305238,"The chosen translation for each word is the first-ranked one in the bilingual dictionary, or a special symbol if there is no possible translation for that term.",20,21
40,183084,"An ITG consists of a set of nonterminal symbols, a set of L 0 symbols, a set of L 1 symbols, a set of rules and a start symbol.",31,32
41,183084,"We notice that the only significance of the sets of nonterminal, L 0 and L 1 symbols is to categorize the symbols that occur in the rules, and the identity of the start symbol constitutes a per-grammar constant.",35,36
42,183084,"We need one symbol for each of the nonterminal, L 0 and L 1 symbols of the ITG, as well as a meta symbol to separate rules and determine whether they are straight or inverted (unary rules are assumed to be straight).",3,4
43,183084,"We need one symbol for each of the nonterminal, L 0 and L 1 symbols of the ITG, as well as a meta symbol to separate rules and determine whether they are straight or inverted (unary rules are assumed to be straight).",25,26
44,183084,"For conditional description length, rules that are found in Ψ can be excluded when measuring the length of Φ. Consider the following toy ITG: S → A, A → ⟨AA⟩, A → [AA] , A → have/有, A → yes/有, A → yes/是 which is conditioned on the following ITG: S → A, A → ⟨AA⟩, A → [AA] , A → • • • , • • • Its serialized form would be: []Ahave有[]Ayes有[]Ayes是 Assuming a uniform distribution over the symbols, each symbol will require −lg 1 N bits to encode (where N is the number of different symbols in the ITG).",106,107
45,183084,"The above toy ITG has 8 symbols, meaning that each symbol requires 3 bits.",11,12
46,183084,"The ITG is initialized with all sentence pairs as biterminals: S → A A → e 0..T 0 /f 0..V 0 A → e 0..T 1 /f 0..V 1 ... A → e 0..T N /f 0..V N where S is the start symbol, A is the nonterminal, N is the number of sentence pairs, T i is the length of the i th output sentence (making e 0..T i the i th output sentence), and V i is the length of the i th input sentence (making f 0..V i the i th input sentence).",56,57
47,13607604,The symbol nil means don't care.,1,2
48,7715096,"2007) and gene symbol disambiguation (Xu et al.,",4,5
49,17548543,"We remove the # symbol, all @ mentions and links and perform lower case conversion.",4,5
50,9257301,Each transition is labeled with an input symbol from an input alphabet; an output symbol from an output alphabet; an origin state; a destination state; and a weight.,7,8
51,9257301,Each transition is labeled with an input symbol from an input alphabet; an output symbol from an output alphabet; an origin state; a destination state; and a weight.,15,16
52,9257301,"If every transition in the transducer has the same input and output symbol, then the WFST represents a weighted finite-state automaton.",12,13
53,9257301,"The symbol represents the empty string, which allows the transition to be traversed without consuming any symbol.",1,2
54,9257301,"The symbol represents the empty string, which allows the transition to be traversed without consuming any symbol.",17,18
55,9257301,"The φ (or failure) symbol on a transition also allows it to be traversed without consuming any symbol, but it differs from in only allowing traversal if the symbol being matched does not label any other transition leaving the same state, i.e., it encodes the semantics of otherwise, which is useful for language models.",6,7
56,9257301,"The φ (or failure) symbol on a transition also allows it to be traversed without consuming any symbol, but it differs from in only allowing traversal if the symbol being matched does not label any other transition leaving the same state, i.e., it encodes the semantics of otherwise, which is useful for language models.",19,20
57,9257301,"The φ (or failure) symbol on a transition also allows it to be traversed without consuming any symbol, but it differs from in only allowing traversal if the symbol being matched does not label any other transition leaving the same state, i.e., it encodes the semantics of otherwise, which is useful for language models.",31,32
58,9257301,"Strings in the rules may be parsed in one of three different ways: as a sequence of bytes (the default), as utf8 encodings, or according to a user-provided symbol table.",35,36
59,9257301,"With the --save symbols flag, the transducers can be saved out into fars with appropriate symbol tables.",16,17
60,9257301,"followed by either byte, utf8, or an identifier holding a symbol table.",12,13
61,9257301,"All other characters following the backslash are uninterpreted, so that we can use \"" and \' to insert an actual quote (double) quote symbol instead of terminating the string.",28,29
62,9257301,"The second way is to use UTF8 parsing by using the special keyword, e.g.: Finally, we can load a symbol table and split the string using the fst field separator flag (found in fst/src/lib/symbol-table.cc) and then perform symbol table lookups.",22,23
63,9257301,"The second way is to use UTF8 parsing by using the special keyword, e.g.: Finally, we can load a symbol table and split the string using the fst field separator flag (found in fst/src/lib/symbol-table.cc) and then perform symbol table lookups.",43,44
64,9257301,"The second way is to use UTF8 parsing by using the special keyword, e.g.: Finally, we can load a symbol table and split the string using the fst field separator flag (found in fst/src/lib/symbol-table.cc) and then perform symbol table lookups.",50,51
65,9257301,"Symbol tables can be loaded using the SymbolTable built-in function: arctic_symbol_table = SymbolTable['/path/to/bears.symtab']; pb = ""polar bear"".arctic_symbol_table; One can also create temporary symbols on the fly by enclosing a symbol name inside brackets within an FST string.",41,42
66,9257301,"All of the text inside the brackets will be taken to be part of the symbol name, and future encounters of the same symbol name will map to the same label.",15,16
67,9257301,"All of the text inside the brackets will be taken to be part of the symbol name, and future encounters of the same symbol name will map to the same label.",24,25
68,9257301,"The optional keywords byte (default), utf8 or the name of a symbol table can be used to specify the parsing mode for the strings.",14,15
69,9257301,"Thus StringFile ['strings_file', utf8, my_symtab] would parse a sequence of tab-separated pairs, using utf8 parsing for the left-hand string, and the symbol table my symtab for the right-hand string.",33,34
70,9257301,This has the benefit of including all start and stop symbol functionality while avoiding common pitfalls that arise with explicit symbols.,10,11
71,9257301,Start and end of the sequence are not represented via transitions in the automaton or symbols in the symbol table .,18,19
72,9257301,The symbol table provided during counting is kept with the model FSTs.,1,2
73,9257301,"Since this is a bigram model, histories consist of at most one prior symbol from the vocabulary.",14,15
74,9257301,"Only the 'a' history state and the unigram state are final states, since our example string ends with the symbol 'a'. (",22,23
75,9257301,"The transitions are backoff transitions, and the weights on each n-gram transition are negative log counts of that symbol occurring following the history that the state represents.",21,22
76,9257301,"There are utilities related to input and output, including ngramsymbols, which produces a symbol table from a corpus; ngramread, which reads in textual count files and models in ARPA format and encodes them as an FST; ngramprint which prints n-gram counts or ARPA format text files; and ngraminfo which displays information about the model, such as number of n-grams of various orders.",15,16
77,53093304,"Instead we train the decoder to focus on a single input symbol at each time-step and ""self-attend"" by shifting the input cursor with one position at a time.",11,12
78,53093304,Take every symbol in the output sequence and check if it aligns with a symbol in the input sequence (based on the alignments produced by the algorithm in Figure 1 ); 2.,2,3
79,53093304,Take every symbol in the output sequence and check if it aligns with a symbol in the input sequence (based on the alignments produced by the algorithm in Figure 1 ); 2.,14,15
80,53093304,"If the output symbol does not align with any character, instruct the decoder to generate it (the case of the arbitrary character in the vocabulary); 3.",3,4
81,53093304,"If the output symbol aligns, instruct the decoder to generate INC symbols until the focus-index would reach the corresponding input character, and then generate an COPY symbol; 4.",3,4
82,53093304,"If the output symbol aligns, instruct the decoder to generate INC symbols until the focus-index would reach the corresponding input character, and then generate an COPY symbol; 4.",30,31
83,53093304,"When the sequence is completely generated, instruct the decoder to generate an EOS symbol.",14,15
84,53093304,"Notice that after copying the first symbol ('F') to the output, the oracle immediately generates the vocabulary item ' Ȃ', because it is not aligned with any symbol in the source lemma.",6,7
85,53093304,"Notice that after copying the first symbol ('F') to the output, the oracle immediately generates the vocabulary item ' Ȃ', because it is not aligned with any symbol in the source lemma.",34,35
86,53093304,"However, the next (3rd) symbol in the destination string is aligned with a character in the source string and the index is incremented with two INC commands.",7,8
87,53100399,"For every symbol (s i ) in the input text, the decision for tokenization or sentence splitting (after s i ) is generated using a softmax layer that takes as input 4 distinct vectors (final output states) of: 1.",2,3
88,53100399,Forward Network: A unidirectional LSTM that sees the input symbol by symbol in natural order; 2.,10,11
89,53100399,Forward Network: A unidirectional LSTM that sees the input symbol by symbol in natural order; 2.,12,13
90,53100399,"Peek Network: A unidirectional LSTM , that peeks at a limited window of symbols 6 in front of the current symbol -the input is fed to the network in reverse order; 3.",21,22
91,53100399,Partial Word Embeddings (PWE) Network: It is often the case that we are able to generate valid (known) words made up of symbols from the previously tokenized word up to the current symbol.,37,38
92,53100399,"Similarly to a Finite State Transducer (FST) we train a model to output any symbol from the alphabet and three additional special symbols: <COPY>, <INC> and <EOS>.",16,17
93,53100399,The output symbol list for the decoder to learn is: <COPY>a<INC><INC><COPY>a<EOS> 7 .,2,3
94,2087764,"Finally, semantic relations are encoded using WordNet's pointers and an additional symbol for Wikipedia relations (r), which can also specify the source of the relation (e.g., FROM IT means that the relation was harvested from the Italian Wikipedia).",13,14
95,17662280,"Edit-Distance-Based Model Given a sentence sG from a general corpus and a sentence sR from the test set or in-domain corpus, the edit distance for these two sequences is defined as the minimum number of edits, i.e. symbol insertions, deletions and substitutions, for transforming sG into sR. There are several different implementations of the edit-distance-based retrieval model.",45,46
96,219309259,"To make this feature in the same form with the bag-of-words, we appended a symbol to each of the neighboring words' morphological root forms to mark whether this word is in the left or in the right of the target word.",19,20
97,219309259,"In this part, punctuation symbol and stop words are not removed.",5,6
98,6784042,"v M }, where A = {a S i S j } is a R × R matrix of transition probabilities, B = {b S j (v k )} are the observation symbol probability distributions and π = {π S j } is the initial state distribution.",38,39
99,31845635,The symbol based tokenizer is targeted for Japanese and Chinese where we have to look after each symbol and decide whether to split.,1,2
100,31845635,The symbol based tokenizer is targeted for Japanese and Chinese where we have to look after each symbol and decide whether to split.,17,18
101,31845635,"The features are simply the current symbol and one symbol to the left and right, with two possible classes: SPLIT (split after the current symbol) and NONE.",6,7
102,31845635,"The features are simply the current symbol and one symbol to the left and right, with two possible classes: SPLIT (split after the current symbol) and NONE.",9,10
103,31845635,"The features are simply the current symbol and one symbol to the left and right, with two possible classes: SPLIT (split after the current symbol) and NONE.",27,28
104,31845635,"For example, currency tokens like ""7000e "" should always expand as the first variable part plus the last symbol separately if that last symbol is ""e "", while the opposite example ""e 7000"" should be represented as a static first symbol followed by a variable string (here the numeric amount).",20,21
105,31845635,"For example, currency tokens like ""7000e "" should always expand as the first variable part plus the last symbol separately if that last symbol is ""e "", while the opposite example ""e 7000"" should be represented as a static first symbol followed by a variable string (here the numeric amount).",25,26
106,31845635,"For example, currency tokens like ""7000e "" should always expand as the first variable part plus the last symbol separately if that last symbol is ""e "", while the opposite example ""e 7000"" should be represented as a static first symbol followed by a variable string (here the numeric amount).",46,47
107,31845635,"The 57 tokenization models do not include the symbol tokenization models for Japanese and Chinese, which are even smaller; also, there were other languages that were pre-tokenized, so no models were created for them (details in section 3.1).",8,9
108,219303777,"In the DIMAP dictionary, frame names are entered as dictionary entries beginning with the symbol ""#"" and frame elements are entered beginning with the symbol ""@"".",15,16
109,219303777,"In the DIMAP dictionary, frame names are entered as dictionary entries beginning with the symbol ""#"" and frame elements are entered beginning with the symbol ""@"".",27,28
110,53246621,"Corrupt symbol filter -validates whether neither the source nor the target sentence contains words that contain question marks between letters (e.g., 'flie?en' instead of 'fließen', 'gr??ere' instead of 'größere', etc.).",1,2
111,53246621,"As most of potentially invalid (due to encoding corruption) sentence pairs are captured by the foreign word filter and the corrupt symbol filter, this filter provides just a minor addition -the list of invalid characters that are not included in valid alphabets consists of just four characters.",23,24
112,21732361,"We did, however, experiment with different encoder and decoder cell types and add slight modifications to the data iterator module for it to automatically read the multilingual multi-way training data in equal batches for each translation direction and prepend the target language symbol at the beginning of each source sentence.",46,47
113,12742267,"Only one standard of quotation marks and apostrophes were used, hyphenated tokens were split and the hyphens were replaced with a special symbol.",23,24
114,1744713,"18 , 'maruhosi' (symbol O) is used by only younger informants.",6,7
115,219304276,"Felber, in the ""Terminology Manual"" (Felber, 1984) , defines a term as ""any conventional symbol representing a concept defined in a subject field"".",21,22
116,24089,"The symbol ""6.~"" indicates the head noun.",1,2
117,10717132,"For the development of dictation systems, it was necessary to create a specific corpus that would: 1) better capture the speaking characteristics of speakers when dictating text to a computer and 2) contain spoken commands common to dictation scenarios (e.g., punctuation, formatting, special symbol, and action commands).",51,52
118,10717132,"2003) created a speech recognition system for Greek that could handle special symbol and formatting commands, however they were introduced only in the speech recognition system and were omitted from their speech corpus.",13,14
119,10717132,"The DSC had to contain four types of spoken commands -punctuation, special symbol, formatting, and action commands.",13,14
120,10717132,The example shows usage of two spoken commands (two special symbol commands for an exclamation mark and the emoticon of a smiling face).,11,12
121,10717132,"The most frequent commands are punctuation commands (over 68%), followed by special symbol commands (over 22%).",16,17
122,2848410,"For instance, the verb, noun and adjective tags, in both languages were prefixed with a common symbol, given that verbadjective, noun-verb, noun-adjective and the other combinations are typical for Romanian-English translation equivalents that do not preserve the POS.",19,20
123,23446493,"Note that the underscore symbol '_' denotes any value of a grammatical category, e.g., infp(_) denotes infinitival phrase of any aspect.",4,5
124,235623770,"For instance, cells containing a currency symbol and negative monetary amounts such as $(1,234), are often split into three physical cells $ (1,234 ) so that the currency symbols and numbers align with other similar contents across rows.",7,8
125,60513490,"By clicking on the pen symbol to the left of the word 'montera' (mount, assemble), the user gets access to a comment field in the database.",5,6
126,7400436,"A probabilistic SCFG is defined by the 6-tuple G = N , T e , T f , L, S, λ , where N is a finite set of nonterminals, T e , T f are finite sets of terminal symbols, L is a set of paired production rules, S is a start symbol ∈ N , and λ is a set of parameters that define a probability distribution of derivations under G. Each rule in L has the form A → α; β , where A ∈ N , α ∈ N ∪ T e + , β ∈ N ∪ T f + , and N ⊆ N .",60,61
127,7400436,"While SMT methods are often applied with no linguistic knowledge at all (and are therefore blind as to whether paired inputs and outputs are NL strings or something else), it was not clear how well they would cope with the task of mapping from number/symbol vectors to NL strings.",49,50
128,11148999,"s,t) k−n+1 ) where s refers to a source symbol (t for target) and (s,t) k to the k th tuple of the given bilingual sentence pair.",12,13
129,7442467,"From a computational standpoint, implementing the recombination strategy requires a one-toone alignment between the lexical graphemic and phonemic representations, where each grapheme is matched with the corresponding phoneme (a null symbol is used to account for the cases where the lengths of these representations differ).",35,36
130,1034412,the second module converts the graphemic portions of the input message into a phonemic string using a set of manually encoded non-deterministic letter-to-phone rules; these rules notably encode the possibility for each symbol to encode its spelling (eg.,39,40
131,1034412,"Upon recognition of any such sequence, two transitions loop back to the initial state: one carries the input symbol '#', which is used whenever a word separator is encountered; the other is an ε transition, which allows to re-segment the input stream.",20,21
132,9021168,"To cope with this problem, all words observed only once in the training data were mapped into the special symbol oov.",20,21
133,47910365,"Our strategy consisted of trying to match substrings in the hash-tag against our single-word lexicon, either as the whole string (minus the hash symbol) or in Camel-Case.",29,30
134,237581301,"Note that thanks to asynchronous decoding, introduced in Section 2.4.2, we are also in a position to simulate other dependency patterns, where each symbol e 2 t is generated conditioned on e 1 <t+k , e 2 <t , thus reproducing the chained model of Le et al. (",26,27
135,219300918,"The second model, referred as Clean, uses embedding trained on data with an additional preprocessing step: every symbol that is not either a letter or a number is removed.",20,21
136,52010197,"For the sequence generation task, we used a recurrent layer to add a temporality followed by a softmax layer that gives one character at the time until the word end symbol ] is given.",31,32
137,52010197,"We define as weak abbreviation, a symbol that replaces few characters of a word, while a strong abbreviation reduces a long title to only two or three words.",7,8
138,52010197,The last added symbol is a blank label that allows the system to provide an out-of-character answer after the end of a word.,3,4
139,221970864,"Generate f J 1 conditioned on y I 0 and a J 1 1 As is custom, target sentences are completed with a ""null"" symbol, conventionally at index 0.",27,28
140,221970864,"In the case of the null links, the agreement term should reward configurations where one source word is aligned with the null symbol in one direction, and is not aligned to any target word in the other direction.",23,24
141,198847395,A hyphenation symbol (-) is used whenever the system participated in fewer than 4 tests in the batch.,2,3
142,198847395,The symbol (-) is used when systems don't provide exact answers for a particular type of question.,1,2
143,21704183,It was also decided to remove the elision symbol in Mboshi.,8,9
144,21704183,"2017b; Bedrosian, 1996) and a symbol for silence.",8,9
145,19347174,A hyphenation symbol (-) is used whenever the system participated in fewer than 4 tests in the batch.,2,3
146,19347174,The symbol (-) is used when systems don't provide exact answers for a particular type of question.,1,2
147,14130578,2 ⌃ is the suppression symbol.,5,6
148,14130578,"We say that a string T fulfills substring kanonymity when every substring of T that does not contain the suppression symbol ?,",20,21
149,14682487,"2016) ) MTI First Line Index, Default MTI, BioASQ Filtering Table 3 : Average ranks for each system across the batches of the task 4a for the measures MiF and LCA-F. A hyphenation symbol (-) is used whenever the system participated in less than 4 times in the batch.",38,39
150,14682487,"didn't provide exact answers for a particular kind of questions we used the symbol ""-"".",14,15
151,697661,"To make symbol processing simpler, hypergraphs are often reduced to lists of ordered binary relations between concepts, as it is shown in Figure 1 for the sentence (1) The cat sat on the mat.",2,3
152,697661,"To differentiate ALs from UWs, ALs are attached to UWs by the symbol "".@"".",13,14
153,6483272,"2Ai is the ith item of the lexico-syntactic expression A, and n is the number of items in A. An item can be either a lemma, a punctuation mark, a symbol, or a tag (N P, LIST, etc.).",35,36
154,8583047,"1 ), and that the following symbol is 'c', for which no transition exists.",7,8
155,8583047,"To circumvent the fact that a lattice does not always finish with an endof-phrase symbol, which can be the case because of segmentation based on time, an end-of-phrase symbol is added before the final state of each lattice.",16,17
156,8583047,"To circumvent the fact that a lattice does not always finish with an endof-phrase symbol, which can be the case because of segmentation based on time, an end-of-phrase symbol is added before the final state of each lattice.",36,37
157,15955820,In Example (2) the symbol > denotes the used CCG forward application combinator.,6,7
158,1263200,"The symbol '/' means ""or"".",1,2
159,221970224,"Seeking to bridge the gap between formal theory and applied practice, we focus on the proof nets of linear logic, a lean graphical calculus that does away with the bureau-cratic symbol-manipulation overhead characteristic of conventional prooftheoretic presentations ( §2).",34,35
160,221970224,"The first is an encoder/decoder-based supertagger that converts raw text sentences into linear logic judgements by dynamically constructing contextual type assignments, one primitive symbol at a time.",28,29
161,221970224,"The contextualized representations are fed into a Sinkhorn layer, tasked with finding the valid permutation that brings primitive symbol occurrences into alignment.",19,20
162,221970224,"Given contextualized representations for each primitive symbol within a proof frame, scores can be simply computed as the inter-representation dot-product attention.",6,7
163,221970224,"Symbol Embeddings In order to best utilize the small, structure-rich vocabulary of the decoder, we opt for lower-dimensional, positiondependent symbol embeddings.",26,27
164,221970224,"2020) and embed decoder symbols as continuous functions in the complex space, associating each output symbol s ∈ V with a magnitude embedding r s ∈ R 128 and a frequency embedding ω s ∈ R 128 .",17,18
165,221970224,"A symbol s occurring in position p in the proof frame is then assigned a vector ṽs,p = r s e jωsp ∈ C 128 .",1,2
166,221970224,"Throughout training, we validate by measuring the per-symbol and per-sentence typing accuracy of the greedily decoded proof frame, as well as the linking accuracy under the assumption of an errorfree decoding.",10,11
167,221970224,"We first parse the decoded symbol sequences, discarding beams containing subsequences that do not meet the inductive constructors of the type grammar.",5,6
168,221970224,"We provide strict teacher guidance when learning axiom links, whereby the network is provided with the original proof frame symbol sequence instead of the predicted one.",20,21
169,221970224,"To speed up computation, positive and negative indexes are arranged perlength rather than type for each batch; this allows us to process symbol transformations, dot-product attentions and Sinkhorn activations in parallel for many types across many sentences.",24,25
170,11137445,Two actions are allowed: copy one symbol from y or z into the solution and move one or both cursors.,7,8
171,11137445,"This is for instance the case in our running example where the symbol d in doer aligns to the one in reader, which puzzles the synchronization.",12,13
172,11137445,"To solve this problem, we exploit a property on symbol counts that an analogical relation must fulfill (Lepage, 1998) : where A is the alphabet on which the forms are built, and |x| c stands for the number of occurrences of symbol c in x. [x : y = z : t] ⇒ |x| c + |t| c = |y| c + |z| c ∀c ∈ A Our search strategy (named TC) begins by selecting an x-form in the input space.",10,11
173,11137445,"To solve this problem, we exploit a property on symbol counts that an analogical relation must fulfill (Lepage, 1998) : where A is the alphabet on which the forms are built, and |x| c stands for the number of occurrences of symbol c in x. [x : y = z : t] ⇒ |x| c + |t| c = |y| c + |z| c ∀c ∈ A Our search strategy (named TC) begins by selecting an x-form in the input space.",46,47
174,53589436,"A CFG is a 4-tuple G = (N, W, R, S) where N and W are respectively the non-terminal and terminal symbols, R a finite set of rules of the form A → β, with A ∈ N and β ∈ (N ∪ W ) * , and S ∈ N the start symbol.",65,66
175,53589436,Our grammars will be used to analyze the structure of complete utterances and the start symbol S will always correspond to the sentence top-level.,15,16
176,53589436,"To induce word segmentation from parse trees, we will consider that each span covered by the non-terminal symbol Word defines a linguistic word, even though in a fully unsupervised setting, this non-terminal might actually correspond to larger or smaller linguistic units.",20,21
177,53589436,"and will do so based only on the identity of the root symbol, i.e. without any certainty regarding the linguistic status of the collected sequences.",12,13
178,204788606,10  Source sentences include an end-of-sentence (EOS) symbol (corresponding to w J in our notation) and target sentences include both a beginning-of-sentence (BOS) and an EOS symbol.,14,15
179,204788606,10  Source sentences include an end-of-sentence (EOS) symbol (corresponding to w J in our notation) and target sentences include both a beginning-of-sentence (BOS) and an EOS symbol.,41,42
180,204788606,"For the computation of weights αij in the word-length bias extension (Equation ( 7 )), we arbitrarily attribute a length of 1 to the EOS symbol on the source side.",30,31
181,204788606,Ω 0 is the BOS symbol.,5,6
182,221970143,"With respect to the neural baseline, the gains are maximal when the morphologically rich language is on the target side: in this situation, character-based representations help to differentiate the translation model for the rare words, which in the baseline versions all correspond to the same UNK symbol.",52,53
183,22952702,"For each alignment link, the color of the special symbol ""$$$"" encodes its label: green for ""sure"", violet for ""partial"", etc.",10,11
184,1187320,"symbol refers to the concatenation of word strings): The choice of x and y will be guided by the sum W of the association scores between each source and target words of a block (X,Y ) ∈ {A, Ā} × {B, B}: S = A. Ā with A = [s 1 . . .",0,1
185,196623838,"It also escapes all whitespaces (by replacing them with a meta symbol), so that its tokenization is fully reversible.",12,13
186,1707857,"The probability assigned to a sentence pair by the translation model is estimated by using the n-gram assumption: p(s J 1 , t I 1 ) = where s refers to a source symbol (t for target) and (s, t) k to the k th tuple of the given bilingual sentence pairs.",36,37
187,14218893,"The functions f (w i ) ∈ F include the rules as detailed in the following list (List 1): List 1: Set of regular expression rules (F ) Return ""T rue"" if the wi contains Punctuation marks otherwise return ""F alse"" Return ""T rue"" if the wi is list of Punctuation marks otherwise return ""F alse"" Return ""T rue"" if the wi contains digits otherwise return ""F alse"" Return ""T rue"" if the wi number otherwise return ""F alse"" Return ""T rue"" if all letters of wi are capitalized otherwise return ""F alse"" allNumber Return ""T rue"" if the wi starts with capital letter otherwise return ""F alse"" Return ""T rue"" if the wi has""URL"" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has ""Email"" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has ""Abbreviation"" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has ""Arrow"" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has ""Time "" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has ""NumberWithCommas"" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has symbol representing ""RT:retweeting"" form otherwise return ""F alse"" Return ""T rue"" if the wi has symbol representing ""At-Mention"" form otherwise return ""F alse"" Return ""T rue"" if the wi has symbol representing ""hash-tagh"" form otherwise return ""F alse"" M is a set of orthographic transformations that maps a string to another string via a simple surface level transformation.",258,259
188,14218893,"The functions f (w i ) ∈ F include the rules as detailed in the following list (List 1): List 1: Set of regular expression rules (F ) Return ""T rue"" if the wi contains Punctuation marks otherwise return ""F alse"" Return ""T rue"" if the wi is list of Punctuation marks otherwise return ""F alse"" Return ""T rue"" if the wi contains digits otherwise return ""F alse"" Return ""T rue"" if the wi number otherwise return ""F alse"" Return ""T rue"" if all letters of wi are capitalized otherwise return ""F alse"" allNumber Return ""T rue"" if the wi starts with capital letter otherwise return ""F alse"" Return ""T rue"" if the wi has""URL"" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has ""Email"" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has ""Abbreviation"" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has ""Arrow"" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has ""Time "" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has ""NumberWithCommas"" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has symbol representing ""RT:retweeting"" form otherwise return ""F alse"" Return ""T rue"" if the wi has symbol representing ""At-Mention"" form otherwise return ""F alse"" Return ""T rue"" if the wi has symbol representing ""hash-tagh"" form otherwise return ""F alse"" M is a set of orthographic transformations that maps a string to another string via a simple surface level transformation.",281,282
189,14218893,"The functions f (w i ) ∈ F include the rules as detailed in the following list (List 1): List 1: Set of regular expression rules (F ) Return ""T rue"" if the wi contains Punctuation marks otherwise return ""F alse"" Return ""T rue"" if the wi is list of Punctuation marks otherwise return ""F alse"" Return ""T rue"" if the wi contains digits otherwise return ""F alse"" Return ""T rue"" if the wi number otherwise return ""F alse"" Return ""T rue"" if all letters of wi are capitalized otherwise return ""F alse"" allNumber Return ""T rue"" if the wi starts with capital letter otherwise return ""F alse"" Return ""T rue"" if the wi has""URL"" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has ""Email"" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has ""Abbreviation"" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has ""Arrow"" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has ""Time "" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has ""NumberWithCommas"" pattern otherwise return ""F alse"" Return ""T rue"" if the wi has symbol representing ""RT:retweeting"" form otherwise return ""F alse"" Return ""T rue"" if the wi has symbol representing ""At-Mention"" form otherwise return ""F alse"" Return ""T rue"" if the wi has symbol representing ""hash-tagh"" form otherwise return ""F alse"" M is a set of orthographic transformations that maps a string to another string via a simple surface level transformation.",304,305
190,16424077,"Figure 1 : ""b,d,g,s deletion"" ""b,d,g,s deletion"" GC:0 <=> V: _ V: $:; (The symbol $ marks the weak grade and GC is the set of gradating consonants.)",37,38
191,219310038,Infini te loop is avoided in just the same manner also in a more complex case where every input 5 symbol is a well-formed word but they are lined up in a wrong way.,20,21
192,248780493,no boundary) after symbol y t .,4,5
193,9063641,Nodes are labeled by an alphabetical symbol and contain a (possibly empty) set of pointers to forms.,6,7
194,21703463,"For all test languages except Modern Greek, word forms are orthographically transcribed, and administered to the network one symbol at a time as raw letter strings (starting with the start-of-word symbol '#' and ending with the end-of-word symbol '$'), with no information about their morphological structure.",20,21
195,21703463,"For all test languages except Modern Greek, word forms are orthographically transcribed, and administered to the network one symbol at a time as raw letter strings (starting with the start-of-word symbol '#' and ending with the end-of-word symbol '$'), with no information about their morphological structure.",37,38
196,21703463,"For all test languages except Modern Greek, word forms are orthographically transcribed, and administered to the network one symbol at a time as raw letter strings (starting with the start-of-word symbol '#' and ending with the end-of-word symbol '$'), with no information about their morphological structure.",50,51
197,21703463,"The strength of the connection between consecutively activated BMUs is trained through the following principles of discriminative learning: given the input bigram ab, the connection strength between the BMU that get mostly activated for a at time t and the BMU for b at time t+1 will: (i) increase if a often precedes b in training (entrenchment), (ii) decrease if b is often preceded by a symbol other than a (competition).",76,77
198,21703463,"Thus, due to the prediction-driven bias of the temporal layer of re-entrant connections, strong expectations over upcoming input symbols account for successful serial word processing, with processing accuracy being a function of how confident the TSOM is about the position of the current symbol in the input string.",50,51
199,21703463,"Proceduraly, by presenting one symbol at a time on the input layer, a TSOM is prompted to complete the current input string by anticipating the upcoming BMU to be activated.",5,6
200,21703463,"Anticipation/prediction scores across input words are calculated by incrementally assigning each correctly anticipated symbol in the input form a 1-point score, i.e. the anticipation score of the preceding symbol incremented by 1.",15,16
201,21703463,"Anticipation/prediction scores across input words are calculated by incrementally assigning each correctly anticipated symbol in the input form a 1-point score, i.e. the anticipation score of the preceding symbol incremented by 1.",33,34
202,21703463,"In particular, we show how inflectional systems of different complexity (independent variables) affect TSOM processing, by focusing on symbol prediction rate as a dependent variable.",22,23
203,21703463,"Figure 2 plots, for each language, the rate of symbol prediction in serial word processing.",11,12
204,21703463,"Across our selected languages, verb forms in regular paradigms are systematically more predictable (p-value To investigate in more detail the impact of inflectional complexity on processing, we fitted an LMER of symbol prediction for each language, with classes of morphological regularity (regulars vs. irregulars) and morphological structure (stem vs. suffix) as fixed effects (Figure 4 ).",36,37
205,21703463,"The marginal plots in Figure 4 better show a clear serial processing effect of the distance of an input symbol to the stem-ending boundary, over and above the length of the input string.",19,20
206,21703463,"Besides, for all languages, there is a deeper drop in prediction rate at the stem-suffix boundary (for x = 0 as the first symbol of the suffix) in regular forms.",28,29
207,32045912,"We were able to identify both explicit indicators, such as *1* and *2* indicating respectively the third and the first radical, and implicit indicators such for example, the symbol @ which indicates the change of the second radical without necessarily allowing its identification.",35,36
208,5773952,"s, t) k−n+1 ), where s refers to a source symbol (resp.",13,14
209,6681594,"We restrict ourselves to dependency tree analyses, in which each word depends on exactly one parent, either another word or a dummy root symbol as shown in the figure.",25,26
210,6681594,"x n } E x = {(i, j) : i = j, (i, j) ∈ [0 : n] × [1 : n]} That is, G x is a graph with the sentence words and the dummy root symbol as vertices and a directed edge between every pair of distinct words and from the root symbol to every word.",51,52
211,6681594,"x n } E x = {(i, j) : i = j, (i, j) ∈ [0 : n] × [1 : n]} That is, G x is a graph with the sentence words and the dummy root symbol as vertices and a directed edge between every pair of distinct words and from the root symbol to every word.",68,69
212,5740667,"G 1 G 2 G 1P G 2P 1837 The grammar extracted from the above example consists of 5 context-free rules, where seg is the top-level symbol and np, noun and adv are nonterminals.",31,32
213,5740667,Note that at least one terminal symbol must occur in the RHS of the rules.,6,7
214,5740667,"In order to avoid nonterminating recursion, we only allow rules with at least one terminal symbol on the RHS.",16,17
215,16064236," The grammar extracted from the parse in table 9 consists of 4 context-free rules, where <snt> is the top-level symbol and <np>, <n> are non-terminals.",27,28
216,16064236,Note that at least one terminal symbol must occur in the RHS of the rules.,6,7
217,13920600,1997) replaces a hyperedge with a nonterminal symbol by a hypergraph.,8,9
218,13920600,The rank of a terminal or nonterminal symbol is the number of its tentacles.,7,8
219,13920600,The rank k of a HR grammar is the maximum number of tentacles of a nonterminal symbol.,16,17
220,13920600,"Proof (sketch) From TAG to HR 2 , we start with a TAG G and categorise the different adjunction nodes, introducing new symbols whenever two nodes labelled by the same symbol of N A either select a different set of trees or differ with respect to obligatory adjunction to obtain a TAG G which is equivalent to G up to a relabelling of the members of N A .",33,34
221,217380681,The pronunciation errors are marked with an asterisk symbol to make the use of the proposed database viable for both automatic speech recognition and pronunciation error detection research.,8,9
222,15933929,"Below, the symbol (D represents suggested paths, and the black symbol O the points we want to make.",3,4
223,15933929,"Below, the symbol (D represents suggested paths, and the black symbol O the points we want to make.",13,14
224,402014,"For w ∈ Σ , w(i) denotes the i th symbol in w. In this context, definition (2) can be re-stated as: Definition 3 (Analogical proportion in (Σ ,.)) (",11,12
225,219306987,The predictive unit is one token (word or punctuation symbol).,10,11
226,1910842,"This is achieved by indicating the semantic uniformities which are latent under the different lexicalizations of the same markers or of identical relations, and by reducing these diversities of lexical forms to one single symbol reflecting their uniformity.",35,36
227,241583614,"We introduce a pronunciation element that corresponds to a vowel, a consonant, the long sound symbol, or a special operator (voicing V, semi-voicing P, or lowercasing S) in a kana character sequence.",17,18
228,241583614,"1) Words whose POS is symbol, space, person name, or number are excluded. (",6,7
229,241583614,"The first rule Seg-PP changes segmentation tags t s i+1:i+k to I∈ T seg when k consecutive characters x i:i+k are the same vowel kana, long sound symbol, mora nasal, or mora consonant characters, according to our finding that such cases were rare from our preliminary experiment.",34,35
230,27834461,The network automatically learns the meaning of the symbol from the regularity of the training data and then calculates its embedding.,8,9
231,891846,"Matrix A contains the transition probabilities of the hidden states, matrix B contains the probability of occurrence of an observation symbol given the hidden state, and vector 7r contains the initial probabilities of the hidden state.",21,22
232,891846,"In equations ( 1 )-(3), qt is the hidden state of the system at time t, Si is the /th possible hidden state of the system, Ot is the observation symbol at time t, and Vm is the m th possible observable symbol.",35,36
233,891846,"In equations ( 1 )-(3), qt is the hidden state of the system at time t, Si is the /th possible hidden state of the system, Ot is the observation symbol at time t, and Vm is the m th possible observable symbol.",48,49
234,891846,"In this formulation, the sequence of phonemes produced by the system can be seen as the observation-symbol sequence of an HMM that uses the graphemic forms as a hidden-state sequence.",19,20
235,891846,"With this statement, the PTGC problem can be restated as follows: Given the observation-symbol sequence O(t) (phonemes) and the HMM A, find the hidden-state sequence Q(t) (graphemes) that maximizes the probability P(O I Q, ,~).",17,18
236,891846,"The model's parameters can be estimated using the definition formulas, since both the hidden-state and the observation-symbol sequences are known during the training phase of the conversion system.",22,23
237,891846,a. b. Every hidden state should produce one observation symbol.,9,10
238,891846,"According to the physical meaning given to the hidden states and the observation symbols of the HMM used, there cannot be hidden states (graphemes) that do not produce an observable symbol (phoneme).",34,35
239,891846,"To overcome this problem, the hidden-state alphabet and the observation-symbol alphabet should contain not only single characters (single graphemes or phonemes respectively) but also clusters.",14,15
240,891846,"For example (in Greek): grapheme ~ is pronounced/ks/e.g., ¢4&-ksfdi 'vinegar' grapheme ~ is pronounced/k/e.g., ~aA6-kald 'good' grapheme rT is pronounced/s/e.g., c~c~O~-saff 'lucid' graphemes ~cr are pronounced/ks/e.g., ~¢~-c~cr~-4kstasi 'ecstasy' In this example the pair of phonemes/ks/is considered a single phonemic symbol.",81,82
241,891846,"Accordingly, the pair ""~cr"" is also considered a single graphemic state since it is pronounced as /ks/. As can be seen, in order to disambiguate the case of ~cr the phonemic symbol/ks/and the graphemic state ~cr must be introduced.",35,36
242,891846,b. Errors at symbol (state) level: The system counts one error for every graphemic symbol (unit grapheme) that does not match with the corresponding symbol of the correct graphemic transcription.,3,4
243,891846,b. Errors at symbol (state) level: The system counts one error for every graphemic symbol (unit grapheme) that does not match with the corresponding symbol of the correct graphemic transcription.,17,18
244,891846,b. Errors at symbol (state) level: The system counts one error for every graphemic symbol (unit grapheme) that does not match with the corresponding symbol of the correct graphemic transcription.,29,30
245,891846,"This distinction was made because the first error type (word error) is more important from the user's point of view, while the second type (symbol error) is a more objective measure of the performance of the system.",29,30
246,891846,"In addition to these error types, the average symbol errors per incorrect word were counted.",9,10
247,891846,symbol conversion success rate for the first position (percentage).,0,1
248,17886046,"Let Σ denote a set of terminal symbols in the source language, ∆ a set of terminal symbols in the target language, V a set of non-terminal symbols, S a start symbol and R a set of rewrite rules.",36,37
249,17886046,"Hierarchical Back-off Model In the previous models, the generative process is represented as a rewrite process starting from the symbol S, which can incorporate only minimal rules.",22,23
250,17886046,"Otherwise, a Hiero rule is generated to fallback to smaller spans represented in each non-terminal symbol X in the rule.",18,19
251,17886046,"In particular, we resample ⟨d p , θ p ⟩, the pair of discount and strength parameters for phrases from a distribution: [θ p ] |φp| dp [θ p ] np 1 ∏ ⟨s,t⟩ |φp| ∏ k=1 [1 − d p ] (c ⟨s,t⟩ −1) 1 (16) where [ ] denotes a generalized Pochhammer symbol, and c ⟨s,t⟩ the number of customers of phrase pair ⟨s, t⟩. We resample the pair ⟨d r , θ r ⟩ in the same way as ⟨d p , θ p ⟩. The hyperparameter γ b is resampled from distribution: (c back + γ b • G b )(c base + γ b • G b ) (c back + c base + γ b ) 2 , ( 17 ) where ϕ, used in the generative process for either terminal or non-terminal symbol type i ∼ Bernoulli(ϕ α ), is resampled from a distribution: ∏ ⟨α/β⟩∈Base Bernoulli(ϕ |α| ) c ⟨α/β⟩ , ( 18 ) where c ⟨α/β⟩ denotes the number of customers of rule ⟨α/β⟩, and Base denotes a set of rules generated from the base measure.",68,69
252,17886046,"In particular, we resample ⟨d p , θ p ⟩, the pair of discount and strength parameters for phrases from a distribution: [θ p ] |φp| dp [θ p ] np 1 ∏ ⟨s,t⟩ |φp| ∏ k=1 [1 − d p ] (c ⟨s,t⟩ −1) 1 (16) where [ ] denotes a generalized Pochhammer symbol, and c ⟨s,t⟩ the number of customers of phrase pair ⟨s, t⟩. We resample the pair ⟨d r , θ r ⟩ in the same way as ⟨d p , θ p ⟩. The hyperparameter γ b is resampled from distribution: (c back + γ b • G b )(c base + γ b • G b ) (c back + c base + γ b ) 2 , ( 17 ) where ϕ, used in the generative process for either terminal or non-terminal symbol type i ∼ Bernoulli(ϕ α ), is resampled from a distribution: ∏ ⟨α/β⟩∈Base Bernoulli(ϕ |α| ) c ⟨α/β⟩ , ( 18 ) where c ⟨α/β⟩ denotes the number of customers of rule ⟨α/β⟩, and Base denotes a set of rules generated from the base measure.",165,166
253,17886046,"During this extraction process, we limit the source or target terminal symbol size of phrase pairs to 5.",12,13
254,18730548,"If the input parse tree has an empty category, it is treated as an NP with an unknown lexical symbol.",20,21
255,18730548,"If the input parse tree has an empty category, it is treated as a noun phrases with a known lexical symbol.",21,22
256,16313885,"The parser consists of five actions: shift-X consumes the next input word, w i , from the queue and pushes a non-terminal symbol (or a POS label) as a tree of X → w i .",28,29
257,16313885,"We employ beam search which starts from an axiom consisting of a stack with a special symbol eps , and ends when we reach a goal item (Zhang and Clark, 2009) .",16,17
258,17118688,"EM Alignment The alignment algorithm at the heart of our approach is standard: it is a Baum-Welch algorithm, extended to map symbol sub-sequences and not only 1-1 alignments.",25,26
259,233181513,"Additionally, alternative representations include substitution with respect to kana: substitution of the long vowel kana by the long sound symbol (e.g., おいしfor おいしい oishī 'tasty'), substitution of upper/lowercase kana by the other case (e.g., ゎたし for わたし watashi 'me'), and phonetic or visual substitution of kana characters by Latin letters and symbols (e.g., かわＥ for かわい い kawaī 'cute' and こωにちは for こんにちは konnichiwa 'hello').",21,22
260,233181513,"Third, out of 413 F-F cases, 148 tokens were complicated variant forms, including a combination of historical kana orthography and the insertion of the long sound symbol (i), a combination of the character type variant and sound change variant (j), a variant written in romaji (k).",31,32
261,17461313,"More formally, P e, f ; θ x , θ t are the probability of phrase pairs e, f , which is parameterized by a phrase pair distribution θ t and a symbol distribution θ x .",35,36
262,14681834,We assume that the relation symbol abt' originated from the lexical information is replaced with the more specific relation symbol result' in the process of such inference.,5,6
263,14681834,We assume that the relation symbol abt' originated from the lexical information is replaced with the more specific relation symbol result' in the process of such inference.,20,21
264,30485956,"The length of the plotted line indicates the range in word error rate reported over all systems, and the one.standard-deviation points about the mean are indicated with a ""+"" symbol.",34,35
265,219308815,We assume that the relation symbol abt′ originating from the lexical information is replaced with the more specific relation symbol result′ in the process of such inference.,5,6
266,219308815,We assume that the relation symbol abt′ originating from the lexical information is replaced with the more specific relation symbol result′ in the process of such inference.,19,20
267,53621398,We stripped the hashtag symbol (#) from all tokens where it applies. •,4,5
268,222303396,"The hash symbol ""#"" is removed from all hashtags. •",2,3
269,222303396,"In order to normalize the hashtags, we separate the hash symbol (""#"") from the hashtags, split all the hashtags written in camel-case into the individual parts and lowercase them.",11,12
270,218628841,We can model insertions and deletions by introducing an additional symbol ε into the character confusion matrix.,10,11
271,218628841,"c K ), where K is the length of x i , with the following procedure (Figure 3 ): (1) We insert the ε symbol before the first and after every character of x i to get an extended token x i = (ε, c 1 , ε, . . . ,",28,29
272,218628841,"It describes how likely it is to insert a nonempty character c = ε and it is uniform over the set of all characters from the alphabet Σ, except the ε symbol. (",32,33
273,218628841,"It is uniform over the set of all characters from the alphabet Σ, except the source character c and the ε symbol. (",22,23
274,17173188,"the second word e.g. /dern-dern4-dern/ •• (V) intensively walk /norn-norn4-norn/••(V) intensively sleep In fact, only the reduplication of the same sound is accepted in the written text, and a special symbol, namely /mai-yamok/ is attached to the original word to represent the reduplication.",43,44
275,17178436,"1)John y Jane llegaron tarde al trabajo porque ∅ ∅ 1 se durmieron (John and Jane were late for work because [they]∅ ∅ over-slept) 1 The symbol ∅ will always show the position of the In this particular case, a free conjunction does not imply conjunctions 2 that join coordinated noun and prepositional phrases.",31,32
276,18468911,"Speculating that when there were two or more omissions in the same sentence we would never find two instances of the same omissibility condition, we provided just one symbol for each type of zero element.",29,30
277,5847362,"The tagger is a modified version of the Brill tagger, which assigns a part-of-speech tag to each word or symbol.",24,25
278,1610582,"The symbol ~ shows the position of the omitted pronoun).. Target languages with typical elliptical (zero) constructions corresponding to source English pronouns are Italian, Thai, Chinese or Japanese.",1,2
279,7461604,"For example, in the sentence (1), our system has not selected the right antecedent (The French term ""communication"" and the Spanish term ""comunicaci6n') due to the symbol ""(inverted commas) has been tagged as a new word, and in our grammar we have not foreseen this in a np, so the coordination of both np have failed. (",36,37
280,10347682,The symbol • denotes internal product and exp() acts element-wise.,1,2
281,219310288,Here sentence will be the initial symbol of the grammar and the partial parsing will be applied with the rules shown in Figure 4 .,6,7
282,7646419,"For each non-terminal node spanning f with its left child spanning f 1 and its right child spanning f 2 , if the non-terminal symbol is str, the reordered strings will be concatenated in order as f = f 1 f 2 , and if the non-terminal symbol is inv, the reordered strings will be concatenated in inverted order as f = f 2 f 1 .",28,29
283,7646419,"For each non-terminal node spanning f with its left child spanning f 1 and its right child spanning f 2 , if the non-terminal symbol is str, the reordered strings will be concatenated in order as f = f 1 f 2 , and if the non-terminal symbol is inv, the reordered strings will be concatenated in inverted order as f = f 2 f 1 .",54,55
284,7646419,"To ease explanation, we represent each node in the derivation as d = s, l, c, c + 1, r , where s is the node's symbol (str, inv, or term), while l and r are the leftmost and rightmost indices of the span that d covers.",32,33
285,2906863,"The traditional flat ITG generative probability for a particular phrase (or sentence) pair P f lat ( e, f ; θ x , θ t ) is parameterized by a phrase table θ t and a symbol distribution θ x .",39,40
286,2906863,"In FLAT, only minimal phrases generated after P x outputs the terminal symbol TERM are generated from P t , and thus only minimal phrases are memorized by the model.",13,14
287,2906863,"Similarly to FLAT, HIER assigns a probability P hier ( e, f ; θ x , θ t ) to phrase pairs, and is parameterized by a phrase table θ t and a symbol distribution θ x .",36,37
288,2906863,Generate symbol x from P x (x; θ x ).,1,2
289,2906863,"As previously described, FLAT first generates from the symbol distribution P x , then from the phrase distribution P t , while HIER generates directly from P t , which falls back to divide-and-conquer based on P x when necessary.",9,10
290,2906863,"In order to solve these problems, we reformulate the model so that each phrase length l = |f |+|e| has its own phrase parameters θ t,l and symbol parameters θ x,l , which are given separate priors: θ t,l ∼ P Y (s, d, P dac,l ) θ x,l ∼ Dirichlet(α) We will call this model HLEN.",30,31
291,2400894,"Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting.",11,12
292,2400894,"Third, a packed forest is generated using a variant of Earley's algorithm (Earley, 1970) starting from the unique root symbol.",24,25
293,2400894,"Spurious ambiguity during the generation step is further reduced by encoding the tree local contextual information in each non-terminal symbol, such as parent and sibling labels, using the state representation in Earley's algorithm.",21,22
294,2400894,"The network consists of several arcs, each of which represents an alternative word at that position, including the empty symbol, ϵ. This pairwise alignment strategy is prone to spurious insertions and repetitions due to alignment errors such as in Figure 1 (a) in which ""green"" in the third hypothesis is aligned with ""forest"" in the skeleton.",21,22
295,2400894,"Each node in V is represented as X @p where X ∈ N is a non-terminal symbol and p is an address (Shieber et al.,",18,19
296,2400894,"Third, a forest is generated from the unique root symbol of the extracted grammar through non-terminal rewriting.",10,11
297,2400894,"Forest Generation Given the extracted grammar, we apply a variant of Earley's algorithm (Earley, 1970) which can generate strings in a left-to-right manner from the unique root symbol, TOP.",36,37
298,2400894,"Here, we replace each non-terminal symbol by the state representation of Earley's algorithm corresponding to the sequence of prediction steps starting from TOP.",8,9
299,16259679,"A feature representing the type of token: word, number, symbol or punctuation.",12,13
300,6892734,The symbol s is used to denote additional signs of syntactic complexity while v denotes words with verbal POS tags.,1,2
301,6892734,"In the patterns, words with particular parts of speech are denoted by the symbol w with the relevant Penn Treebank tag appended as a subscript.",14,15
302,6892734,Signs of syntactic complexity are denoted by the symbol s with the abbreviation of the functional class appended as a subscript.,8,9
303,2609063,The symbol • denotes internal product and exp() acts element-wise.,1,2
304,5794284,"X → γ, α, ∼ (2) In the notation above, X is a non-terminal symbol, γ is a source-side string of terminal and non-terminal symbols, and α is a target-side one.",21,22
305,235187329,"Moreover, as P is usually laborious to obtain, they proposed a vanilla error model, which assumes that all types of edit operations are equally likely: c ∈ Σ\{ε} P ins (c|ε) = P del (ε|c) = c ∈ Σ\{c, ε} P subst (c|c), where c and c are the original and the perturbed characters, respectively, Σ is an alphabet, and ε is a symbol introduced to model insertion and deletions.",80,81
306,235187329,"The whitespace characters are first replaced with a placeholder symbol ""¬"".",9,10
307,14039776,"Between two and five million people in the European Union could benefit from symbols or symbol-related text as a means of written communication (Keskinen et al.,",15,16
308,5030780,"Finally, the head of a term is the rightmost non-symbol token (i.e. a word) which can be determined from the part-ofspeech tags.",12,13
309,45598818,If their choice of a symbol is completely off track (such as a grammar rule beginning with a closing parenthesis) they are warned right away.,5,6
310,1086656,"Any variable (a symbol whose print-name begins with "" "") which occurs on the right-hand side of an expression is unified with all other occurrences of the same variable in the rule.",4,5
311,8410134,"The formula motion, a type symbol, is true at the node denoted by the nominal n 0 in the structure of Fig.",6,7
312,44172025,d l are the respective dependencies that link these fillers to the head v; EOS is a special symbol to mark the end of string.,19,20
313,44172025,"I.e., for computing inside probabilities, given symbol a as input, instead of considering A → θ as rewrite rules and updating the parse chart only by asserting θ in it, we also consider B → θ bs in which instead of θ we assert α × θs in the IO table, where α is the r 2 coefficient correlations of embeddings for a and bs.",8,9
314,44172025,"2006) , during split, a non-terminal symbol (which represents a random variable in the underlying probabilistic model) is split and its related production rules are duplicated independently of its parent, or sibling nodes.",10,11
315,59807388,"This includes the symbol tables, type unification and type inference information, as well as the instructions associated with lexical entries and phrase structure schemata.",3,4
316,59807388,"The f i ve global static storage areas are associated with this compilation: the symbol tables for types and features, the table for fe ature structure unification instructions, the table for type instructions, and the table for feature-value instructions.",15,16
317,51876770,"The symbol < > in the supertags signifies the spot for the lexical anchor, while * marks the foot node of auxiliary trees and ↓ represents a substitution site.",1,2
318,33178012,"In the pre-processing step, all pairs of lexical items in the input dictionary must be first mapped onto a common symbol space.",23,24
319,33178012,"To build the common symbol space, we generate all possible (l, t i ) tuples and we assign them unique identifiers-i.e., (l, t i ) → s .",4,5
320,33178012,"Finally, these tuples and their assigned identifiers are flattened in a symbol table t: for instance, if (l, t i ) are assigned to the unique identifier s, then the entries of (l, s) and (t i , s) are stored in this table t. Note that the mappings in t are not necessarily one-to-one.",12,13
321,33178012,"To build the common symbol space, we extracted an English-to-Farsi translation dictionary from the English Wiktionary dump of January 2017, containing translations for 7500 lexical items in English.",4,5
322,33178012,These 7500 entries were converted to a symbol table t of size 17760.,7,8
323,2328969,"Following (Briscoe and Carroll, 1993) , conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration: a stack, representing the control memory of the LR parser, and a lookahead sequence, here limited to one symbol.",47,48
324,2328969,an LR state; the first can be either a grammar symbol or an embedded stack.,11,12
325,2328969,"Although the state is the only relevant component of the pair for parsing, in the figure, for presentational purposes, we omit the state and instead show only the symbol/embedded stack component (despite the misleading presence of embedded stacks, actions are executed in constant time).",31,32
326,2328969,"Let # %$ be the lookahead symbol, the leftmost symbol in the not yet shifted suffix.",7,8
327,2328969,"Let # %$ be the lookahead symbol, the leftmost symbol in the not yet shifted suffix.",11,12
328,2328969,"The first, naive one considers only the current automaton state (the state at the top of the stack, ¥ ) and the lookahead symbol, # %$ .",26,27
329,2328969,"For any instantaneous description as described above, we trivially define the ' ($ 1) 2 $ 43 , for any action $ 65 7& , as a probability estimate for $ , given the current state ¥ § and lookahead symbol # %$ : # 2 B3 = maxh # PA ¥ § has an action bpack(X,l) for some X or ¥ has some action reduce for a tree with # nonempty leaves i .",44,45
330,59759934,"non-terminal A, and there is a production A ➔ X 1 X 2 X3 •.. X n in the grammar, then the automaton must have a path labeled X 1 X 2 X 3 ... X n starting at q 0 • This is usually represented by saying that each state in the path contains a ""dotted item"" for the proq.uction, starting with A ➔ • X 1 X 2 X 3 ... X n at q0 , with the dot moving one symbol ahead at each state in the path.",91,92
331,59759934,The dot being in front of a symbol Xi represents the fact that we expect to see the expansion of Xi in the string.,7,8
332,59759934,goto's are traditionally made on a symbol basis.,7,8
333,59759934,"To refer to the symbol (label) of a node n, we use symb(n).",4,5
334,59759934,"f is used to label anchors to represent the absence of a terminal symbol (the ""empty"" label).",13,14
335,59759934,"To define a parsing table for a grammar G with goal symbol S, we first extend G by adding one new tree called start, with two nodes: the root, labeled with a fresh symbol ST, marked NA; and ST 's single child, a substitution node labeled S. Then, let I be the set of all equivalence classes of items of G under �-Let N be the set of symbols, T � N be the set of symbols that appear in some anchor, and N the set of non-negative integers.",11,12
336,59759934,"To define a parsing table for a grammar G with goal symbol S, we first extend G by adding one new tree called start, with two nodes: the root, labeled with a fresh symbol ST, marked NA; and ST 's single child, a substitution node labeled S. Then, let I be the set of all equivalence classes of items of G under �-Let N be the set of symbols, T � N be the set of symbols that appear in some anchor, and N the set of non-negative integers.",37,38
337,59759934,"Although reduce and bpack actions do not actually depend on a terminal symbol for the current algorithm, in practice the table could be so constrained, either by having the algorithm extended to an LR( l) version or by using empirical evidence to reduce/resolve conflicts.",12,13
338,59759934,"More precisely, an element of the stack is a pair (X, q), where q is a state of the parsing table, and X is either a grammar symbol or another stack.",33,34
339,59759934,"Two operations are defined over it : look, which returns the leftmost symbol of input or $ if input is the null sequence; and advance, which removes the leftmost symbol from input.",13,14
340,59759934,"Two operations are defined over it : look, which returns the leftmost symbol of input or $ if input is the null sequence; and advance, which removes the leftmost symbol from input.",32,33
341,219309891,"The significant effects per features are highlighted using the following symbols per research question: The ♣ symbol represents within domain results, ♠ across domains, ♦ within languages, and ■ across languages.",17,18
342,11808126,"As previously presented in Figure 3 , on each line of the figure : the first symbol represents the syntactic function ('SUBJ'= subject, 'N'=noun modifier, 'H'=head, etc.);",16,17
343,3080309,"1991) is a tuple G = (N, T, V, P, S) where N is a finite set of non-terminals with a function dim: N → N determining the fan-out of each A ∈ N ; T and V are disjoint finite sets of terminals and variables; S ∈ N is the start symbol with dim(S) = 1.",65,66
344,3080309,"We use the following additional notation: For a rule γ ∈ P , lhs(γ) gives the LHS non-terminal; lhs(γ, i) gives the ith argument of the LHS and lhs(γ, i, j) its jth symbol; rhs(γ, k) gives the kth RHS non-terminal; and rhs(γ, k, l) gives the lth component of the kth RHS element (starting with index 0 in all four cases).",43,44
345,3080309,"We introduce a new start symbol S / ∈ N that expands to S and use 0, ε, {ε : S } as start state.",5,6
346,3080309,"m} ∪ {⊥} such that δ(γ k,i ) = j if there is a l such that lhs(γ, k, i) = rhs(γ, j − 1, l), and δ(γ k,i ) = ⊥ if lhs(γ, k, i) ∈ T ∪ {⊥} (intuitively, a δ value j tells us that the next symbol to process is a variable that Call: is an argument of the jth RHS non-terminal); and Θ is a finite set of transitions.",71,72
347,3080309,"Θ contains the following transitions (see figure 2 ): S → [S ]S α 0,0 → [α 0,0 ]A β 0,1 → [β 0,1 ]A Predict: S → α 0,0 A → β 0,0 A → γ 0,0 Scan: β 0,0 a → β 0,1 β 1,1 a → β 1,2 γ 0,0 a → γ 0,1 γ 1,0 b → γ 1,1 Publish: α 0,2 → ret β 1,2 → ret γ 1,1 → ret Suspend: [α 0,1 ]ret → α 0,2 [β 1,0 ]ret → β 1,1 [α 0,0 ]β 0,2 → α 0,1 [β 0,2 ] [α 0,0 ]γ 0,1 → α 0,1 [γ 0,1 ] [β 0,1 ]β 0,2 → β 0,2 [β 0,2 ] [β 0,1 ]γ 0,1 → β 0,2 [γ 0,1 ] Resume: α 0,1 [β 0,2 ] → [α 0,1 ]β 1,0 α 0,1 [γ 0,1 ] → [α 0,1 ]γ 1,0 β 1,0 [β 0,2 ] → [β 1,0 ]β 1,0 β 1,0 [γ 0,1 ] → [β 1,0 ]γ 1,0 • Call transitions start a new thread, either for the start symbol or for a daughter non-terminal.",222,223
348,3080309,"Scan reads a LHS terminal while scanning the next input symbol: γ k,i lhs(γ,k,i) → γ k,i+1 if lhs(γ, k, i) ∈ T . •",10,11
349,3080309,"Shift: Whenever we have p : q on top of the stack and an edge from q to q labeled with the next input symbol and an address p , we add the input symbol followed by pp : q to the stack.",25,26
350,3080309,"Shift: Whenever we have p : q on top of the stack and an edge from q to q labeled with the next input symbol and an address p , we add the input symbol followed by pp : q to the stack.",35,36
351,52872375,"Definition 1 (LCFRS) A Linear Context-Free Rewriting System (LCFRS) is a tuple N, T, V, P, S where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rules A(α 1 , . . . ,",81,82
352,52872375,"The definition of a probabilistic LCFRS is a straightforward extension of the definition of PCFG and thus it follows (Levy 2005; Kato, Seki, and Kasami 2006) that: Definition 4 (PLCFRS) A probabilistic LCFRS (PLCFRS) is a tuple N, T, V, P, S, p such that N, T, V, P, S is an LCFRS and p : P → [0..1] a function such that for all A ∈ N: Σ A( x)→ Φ∈P p(A( x) → Φ) = 1 PLCFRS with non-terminals {S, A, B}, terminals {a} and start symbol S: 0.2 : S(X) → A(X) 0 .8 : S(XY) → B(X, Y) 0.7 : A(aX) → A(X) 0 .3 : A(a) → ε 0.8 : B(aX, aY) → B(X, Y) 0.2 : B(a, a) → ε Figure 5 Sample PLCFRS.",125,126
353,52872375,"$ α i be the string obtained from concatenating the components of α, separated by a new symbol $ / ∈ (V ∪ T).",18,19
354,52872375,"PLCFRS with non-terminals {S, A, B, B , T a }, terminals {a} and start symbol S: of the S item in the agenda is updated and then the goal item is the top agenda item and therefore parsing has been successful.",24,25
355,9287023,"J) 米国産リンゴの (beikoku-san-ringo-no) / 第１便が (dai-ichi-bin-ga); (E)The first cargo / of apples imported from the U.S. Here, Japanese and English expressions are divided by the symbol "";"", and ""/"" means a bunsetsu boundary.",45,46
356,14094139,"2 by the term t 0 : t 0 = to_love(John, claims(Bill), Mary, seems) Here 'to_love' is an operation symbol with rank 4; this corresponds to the fact that during the derivation, four trees are substituted/adjoined into the elementary tree for to_love.",26,27
357,14094139,"Note that in order to capture all information contained in a derivation tree, each operation symbol needs to encapsulate not only an elementary tree γ but also the specific addresses at which trees were substituted/adjoined into γ.",16,17
358,14094139,"Since adjunctions may be optional, there will in general be several (albeit a bounded number) of operation symbols per elementary tree, and in particular several different versions of the symbol 'to_love'.",33,34
359,14094139,This can be formalized by interpreting the operation symbols in t as operations on derived trees: The symbol to_love for example can be interpreted as a function that takes four derived trees as arguments and returns the tree obtained by substituting/adjoining these trees at the specified nodes of the elementary tree for to_love.,18,19
360,14094139,"The yield of the derived tree corresponding to a derivation tree t can be obtained by associating with each operation symbol in t a yield function (Weir, 1988) .",20,21
361,14094139,The yield function associated with an operation symbol in a derivation tree can be extracted in a systematic way; see Boullier (1999) for a procedure that performs this extraction in the formalism of range concatenation grammars.,7,8
362,14094139,"Then the symbol to_love has a straightforward reading as constructing a new dependency tree for the full sentence (1): Preserve all the old dependencies, and add new edges from to_love to the roots of the dependency trees associated with the subterms.",2,3
363,14094139,"To obtain the tree t 1 , we apply the following rule to t 0 : q 0 , to_love(x 1 , x 2 , x 3 , x 4 ) → q 2 , x 2 ( q 4 , x 4 (to_love ( q 1 , x 1 , q 3 , x 3 ))) The informal reading of this rule is: 'To translate an input tree of the form to_love(t 1 , t 2 , t 3 , t 4 ): translate the subtrees t 1 and t 3 (corresponding to John and Mary); attach the outputs of these translations as arguments of the modified symbol to_love ; attach the resulting tree to the output of the translation of t 4 (seems); and attach the tree resulting from that to the output of the translation of t 2 (claims).",116,117
364,6794841,"1987) is a tuple G = (N, T, V, P, S) where a) N is a finite set of non-terminals with a function dim: N → N determining the fan-out of each A ∈ N ; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rewriting rules A(α 1 , . . . ,",71,72
365,2630297,"1987) is a tuple G = (N, T, V, P, S) where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N ; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rewriting rules A(α 1 , . . . ,",72,73
366,2630297,"A reduction is, roughly, 3 An extra top rule is added in order to give the PLCFRS parser a unique start symbol in case more than one word has the root node as head, i.e., in case more than one rule with root as LHS label is extracted.",23,24
367,2630297,"$ α i be the string obtained form concatenating the components of α, separated by a new symbol $ / ∈ (V ∪ T ).",18,19
368,9819795,"1987) is a tuple N, T, V, P, S where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N ; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rules A(α1, . . . ,",68,69
369,14805503,"For a given pair (τ , λ), we define the set ξ as the one that contains the synsets present in both trees: ξ = τ ∩ λ ∀α ∈ τ, β ∈ λ (3) Therefore, the similarity rate between τ and λ, denoted by the symbol ψ, would be defined as: ψ(τ, λ) = ν∈ξ φ(ν) (4) One should note that a requirement of our system's similarity measure would be to be independent of the hypothesis length.",55,56
370,16216325,"Definition 1 A Tree Adjoining Grammar (TAG) is a tuple G = (V N , V T , S, I, A) where V N and V T are disjoint alphabets of non-terminal and terminal symbols, respectively, S ∈ V N is the start symbol, and I and A are finite sets of initial and auxiliary trees, respectively.",53,54
371,13043944,"Features produced from the annotations are: string -the original, unmodified text of each token; root -the lemmatised, lower-case form of the token; category -the part-of-speech (POS) tag, a symbol that represents a grammatical category such as determiner, present-tense verb, past-tense verb, singular noun, etc.;",42,43
372,14419861,"We start by predicting the S-predicate: [S( φ) → Φ, 0, 1, 0 , ρinit] S( φ) → Φ ∈ P Scan: Whenever the next symbol after the dot is the next terminal in the input, we can scan it: [A( φ) → Φ, pos, i, j , ρ] [A( φ) → Φ, pos + 1, i, j + 1 , ρ ′ ] φ(i, j+1) = wpos+1 where ρ ′ is ρ updated with ρ(i, j + 1) = pos, pos + 1 .",38,39
373,10271924,"error ""~%when-j-appoint:cannot output appoint event.-%"")) (not-an-antecedent position-entity) (setq tense-verb-symbol (intern tense-verb-word)) (format *trace-output* ""~%-S ~S"" verb-span tense-verb-symbol) (setq new-event (assert-event :predicate 'appoint",31,32
374,10271924,"error ""~%when-j-appoint:cannot output appoint event.-%"")) (not-an-antecedent position-entity) (setq tense-verb-symbol (intern tense-verb-word)) (format *trace-output* ""~%-S ~S"" verb-span tense-verb-symbol) (setq new-event (assert-event :predicate 'appoint",59,60
375,15874344,"In our case, each word in the string is considered as a symbol.",13,14
376,15378527,"First, all links, urls and usernames (these last ones can be easily recognized because their first character is always the symbol @) were removed.",23,24
377,15378527,"Then, the hashtags were transformed to words by removing its first character (that is, the symbol #).",18,19
378,40351744,Let X p be the symbol for the node at position p if this is a substitution node.,5,6
379,2108955,Here sentence will be the initial symbol of the grammar and the partial parsing will be applied with the rules shown in Figure 4 .,6,7
380,231880,"The formula that provides the similarity rate between the dependency trees of the text and the hypothesis in our system, denoted by the symbol ψ, is shown in Equation 3 : ψ(τ, λ) = ν∈ξ φ(ν) (3) where τ and λ represent the text's and hypothesis' syntactic dependency trees, respectively, and ξ is the set that contains all synsets present in both trees, being ξ = τ ∩ λ ∀α ∈ τ, β ∈ λ.",24,25
381,231880,"As we can observe in Equation 3, ψ depends on another function, denoted by the symbol φ, which provides the relevance of a synset.",17,18
382,711437,"1987 ) is a tuple N, T, V, P, S where a) N is a finite set of non-terminals with a function dim: N → N; dim(A) is called the fanout of A and determines the dimension of the tuples in the yield of A; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rules A(α 1 , . . . ,",78,79
383,14684926,"Chiang (2005) introduced a hierarchical phrasebased translation model that combined the strength of the phrase-based approach and a synchronous-CFG formalism (Aho and Ullman, 1969 ): A rewrite system initiated from a start symbol which synchronously rewrites paired nonterminals.",40,41
384,15408109,We abstract all consecutive words in one special symbol 'W '.,8,9
385,15144574,The grammar object consists of its start symbol and a hash table to retrieve a set of production rules with nonterminal symbols as keys.,7,8
386,3151217,The symbol $ is used to mark the sentence start and the sentence end.,1,2
387,14852917,"For the meanings of each symbol in each model, refer to Brown et al. (",5,6
388,17759492,"Treatment of NPSs Sentence-final Structure in Japanese Employing MWEs as NPCIs enabled us to describe the outermost structure of a Japanese sentence by the following production rules; (7) S 0 →BP * •PRED, (8) S i →S i-1 •m i , (1≦i≦n), where S 0 denotes a kernel sentence; BP, a basic phrase called bunsetsu; PRED, a predicate of the kernel sentence; S i , a sentence, m i, a NPCI and a symbol '*', closure operator on the concatenation, '•'.",90,91
389,6990536,"Hierarchical Phrase-based Model Extension Hierarchical Phrase-based Model Hierarchical phrase-based model (Chiang, 2007) induces rules of the form X →< γ, α, ∼, w > (3) where X is a non-terminal symbol, γ is a sequence string of non-terminals and source terminals, α is a sequence string of non-terminals and target terminals.",47,48
390,6990536,Source-side unaligned word (wa) is moved to the front or right of the prealigned symbol (X1).,18,19
391,5059618,"The NULL symbol at index 0 is also a lexical entry in which no morpheme is aligned from the channel target morpheme, such as ""masu"" and ""ka"" in this Japanese example.",2,3
392,16977542,"The underlying entities from the original CFG are: The starting symbol S (also the starting symbol of the transform), the internal rule C −→ D 1 . .",11,12
393,16977542,"The underlying entities from the original CFG are: The starting symbol S (also the starting symbol of the transform), the internal rule C −→ D 1 . .",17,18
394,16649373,"The symbol ""$"" denotes a null constituent occupying the position of the governor on which this MWE depends.",1,2
395,1644817,The tool also generates symbol tables as needed.,4,5
396,1644817,"The toolkit provides a lexicon generation tool in the form of lexi-con2fst.py, and this tool supports closure, and auxiliary symbol generation natively.",21,22
397,1644817,"The tool further generates necessary symbol tables, a list of monophones, and a list of any auxiliary symbols that are added during construction.",5,6
398,1644817,The output consists of the textformat WFST and associated symbol tables.,9,10
399,1644817,"The tool further supports a wide selection of common features of WFST cascade generation including semiring selection, auxiliary symbol support, and fundamental WFST operations such as composition, determinization, and minimization via the OpenFst library.",19,20
400,1644817,Auxiliary symbol replacement is handled automatically in a manner dependent on the set of build commands issued by the user.,1,2
401,13393486,The lexical information may be an orthographic expression or a variable represented by symbol X i .,13,14
402,13393486,This symbol is followed by a numeral and is used to enclose a sequence of morphemes.,1,2
403,13393486,This symbol is used to enclose a morpheme.,1,2
404,13393486,A few orthographic expressions can be defined inside one symbol so that words that can be paraphrased in the same way can be stored as one pattern. :,9,10
405,13393486,This symbol is used to enclose a morpheme.,1,2
406,6786607,"In the following example, a segmentation border is described by the symbol "";"".",12,13
407,11259781,"Other than the baseline word distance-based reordering model and the Equation (4) itself, we tried eight different approximations of Equation (4) as shown in Table 2 , where, the symbol in the left column is the shorthand for the reordering model in the right column.",37,38
408,12936453,where X denotes a nontermina.l symbol.,5,6
409,203269227,"In contrast to RNNbased approaches, the Universal Transformer processes all symbols at the same time and refines its interpretation by processing every symbol in parallel over multiple recurrent processing steps while making use of self-attention mechanism and devoting more attention to ambiguous words (Gouws and Dehghani, 2018) .",23,24
410,16123383,It is estimated that between two and five million people in the European Union could benefit from symbols or symbol-related text as a means of written communication [2].,19,20
411,16123383,"9] introduce Image-Oriented Communication Aid, an interface using the Widgit symbol set, allowing users to build picture-supported messages on a touch screen computer.",14,15
412,14982427,"Otherwise, it is the non-terminal symbol of the lowest common parent of descendants of v l in the constituent tree.",8,9
413,9671238,The braces signify exactly one occurrence of an element of the character set abbreviated by the symbol h; we assume here that h abbreviates the upper and lower case letters of the alphabet.,16,17
414,9671238,The next symbol + specifies that there ..... must.,2,3
415,13111639,"EM Alignment The alignment algorithm at the heart of our approach is standard: it is a Baum-Welch algorithm, extended to map symbol sub-sequences and not only 1-1 alignments.",25,26
416,11616352,"Redundant instances are usually those with as fp headword a punctuation mark, a symbol etc.",14,15
417,13274555,"For example, both Montague semantics and the situation semantics assume that we can represent the dog named Morris in the real world as some symbol like MORRIS in the formal system and that the relation between Morris and MORRIS is fixed and universal [3, 2] .",25,26
418,13274555,Another problem is that the range of what is meant by a symbol differs among the users of an information retrieval system.,12,13
419,13274555,We define the meaning of one symbol as the space configuration around the symbol.,6,7
420,13274555,We define the meaning of one symbol as the space configuration around the symbol.,13,14
421,13274555,"Moreover, we extend tile notion of symbol to ally point in semantic space, so that we can treat entities for which a symbol is I~ot assigned.",7,8
422,13274555,"Moreover, we extend tile notion of symbol to ally point in semantic space, so that we can treat entities for which a symbol is I~ot assigned.",24,25
423,13274555,"Ill other words, we identify a symbol with a point in space.",7,8
424,13274555,It must be noted here that tile same symbol can be put in more than one location.,8,9
425,13274555,This allows a symbol to have several meanings depending on context.,3,4
426,7048421,"The tradition of substituting non-ASCII characters like äöüõ with some other symbol has originally arisen from the wide usage of non-customized keyboards, but this need has largely disappeared as we can see for example from spellings like ykskõik pro ükskõik 'no matter'.",13,14
427,473898,"In the following example, the omission is represented by the symbol 13 (the symbol does not appear in the correct translation into English). (",11,12
428,473898,"In the following example, the omission is represented by the symbol 13 (the symbol does not appear in the correct translation into English). (",15,16
429,42695101,"Every two-level rule says what output side symbol corresponds to which input side symbol, given a certain context.",9,10
430,42695101,"Every two-level rule says what output side symbol corresponds to which input side symbol, given a certain context.",15,16
431,42695101,Imagine that the input symbol has only one potential corresponding output symbol.,4,5
432,42695101,Imagine that the input symbol has only one potential corresponding output symbol.,11,12
433,42695101,"Now if the context does not match, then it is impossible to have this symbol correspondence in the path, and a path cannot be built.",15,16
434,42695101,"In a similar vein, one can define pruning contexts for input symbols that have more than one corresponding output symbol.",20,21
435,42695101,The symbol > denotes a morpheme border; {s}{i}{d} constitute -sid and the ∅ alternation (see Figure 4 ).,1,2
436,42695101,The affixes -sid and ∅ must be described symbol-by-symbol via the rules on Figure 4 .,8,9
437,42695101,The affixes -sid and ∅ must be described symbol-by-symbol via the rules on Figure 4 .,12,13
438,42695101,"Notice that {s} is used as the crucial symbol to define the rest of the affix, thus these rules really relate affixes to context, not just the symbol itself.",10,11
439,42695101,"Notice that {s} is used as the crucial symbol to define the rest of the affix, thus these rules really relate affixes to context, not just the symbol itself.",31,32
440,42695101,"Pairing the non-failed surface side strings with the lexical side from Figure 3 , and removing the morpheme border symbol >, will yield the desired result of Figure 1 .",21,22
441,16100761,"Note that P (←) in the table denotes the paraphrasability of the inverted paraphrases, from right to left direction, and the symbol * indicates that this direction is also judged as paraphrasable, i.e., these two words are determined to be paraphrasable with each other.",25,26
442,21714170,"The first is Epitran's tir-Ethi mapping, which is a faithful one-toone transliteration wherein each Ethiopic symbol is realized as a consonant-vowel sequence (e.g., ትግርኛ → tɨɡɨrɨɲa).",21,22
443,3234156,"Based on the position of the pronoun, the next punctuation symbol is searched to define the boundaries of the relative clause.",11,12
444,225062746,"In this way, structural As shown above, structural transformations are tagged after the symbol ""//"" at the end of each line.",15,16
445,225062746,"The symbol ""+"" means linking two operation units.",1,2
446,16771519,With symbol decoding COV prunes many nodes before statistics decoding and the search space of COV is about10-20% less than that of HMM.,1,2
447,16771519,The fifth part is about the algorithm of symbol decoding.,8,9
448,16771519,"In particular, this example needn't any probability computation and can get the final state sequence just symbol comparing.",18,19
449,16771519,In this example after the symbol comparing and elimination there remains only one path for the sentence and the path is the final tagging result.,5,6
450,16771519,So this sentence is tagged without any probability computation but only with the symbol comparing.,13,14
451,16771519,The process of symbol comparing and elimination is called symbol decoding.,3,4
452,16771519,The process of symbol comparing and elimination is called symbol decoding.,9,10
453,16771519,1 2 3 4 5 *B*-*B* *B*-领 导 领导- 强调 强调- 深入 深入-细 致 B-B B-n n-v v-a a-a B-vn v-ad ad-ad B-v 6 7 8 9 10 细致-的 的-工作 工作-作风 作风- *E* *E*- *E* a-u u-v vn-n n-E E-E u-n u-vn Most times there may be more than one possible paths remained after symbol decoding and then the Viterbi algorithm will be applied to get the best tagging sequence.,102,103
454,16771519,"Although HMM also applies Viterbi for decoding, the search space of HMM is bigger than that of COV because COV has eliminated many impossible states in the step of symbol decoding.",30,31
455,16771519,Here we will describe the symbol decoding algorithm in detail.,5,6
456,16771519,"First we define the suffix and prefix of a state sequence: Suffix of i n i q q ... 1   is defined as i n i q q ... 2   Prefix of i n i q q ...   is defined as 1 1 ...    i n i q q The symbol decoding algorithm is as follows: Input: word sequence S= (2) Backward from right to left A. If a node E i-1 is eliminated in step (1) for it doesn't have any child node in i e , then the relation between E i-1 and its parent node E i-2 will also be eliminated.",60,61
457,16771519,Backward to the left end of the sequence and the process of symbol decoding finishes.,12,13
458,16771519,Due to the symbol decoding many states have been pruned in COV and the search space statistics decoding is reduced accordingly.,3,4
459,2706681,The exclamation symbol '!',2,3
460,10524556,"When compared to symbol-based finite state approaches, our method leads to smaller grammars and automata, which usually better approximate a given language.",3,4
461,10524556,Unifiability as a test criterion in a transition is a generalization over symbol equality.,12,13
462,10524556,"They provide a stronger expressiveness, since they create dynamic value assignments on the automaton transitions and thus exceed the strict locality of constraints in an atomic symbol approach.",27,28
463,9273574,"Definition 4 Given a sentence w and a phrase structure tree representation for it, Zw, the set of binding constraints for T,v is the set ~R,,={(¢ r ~) I 9, ~veP(N), r is a symbol, re {d, b, b(.) } },",42,43
464,9273574,The binding relation will be denoted by the symbol I. The differences between binding and coreference are at both the structural and the interpretative level.,8,9
465,9273574,9 We will denote such a minimal relation with the symbol c and call it 'coreference' tout court.,10,11
466,9273574,"Definition 5 A indexation set for a sentence w is the set ~3 w= { ( ~ r u,/) I q), ~/~ P(N), r is a symbol and re {c, c(.) ,",31,32
467,9273574,"This condition could be captured within the framework proposed here by explicitly introducing dominance, say, by means of a relation symbol sl and, then, by requiring that no circular paths are in ~w such that their strings are in the language (siLl) +.",22,23
468,21710652,"To restrict the size of the input vocabulary, one can choose to set a frequency cutoff for the character n-grams (ngram_cutoff ): all n-grams not in the resulting vocabulary are mapped to an unknown-ngram-symbol.",43,44
469,21710652,Another option to reduce the vocabulary is to only use lowercase characters and add a special symbol <cap> to the vector and assign it the number of capitals in the word (parameter capital).,16,17
470,21710652,"In the above examples, '@' is the padding symbol and the number of padding symbols that has to be added to obtain the length of the longest sentence in the whole dataset is often large.",11,12
471,21710652,We also introduce a beginning-of-sentence symbol <bos> to be able to predict the first word in the sentence.,9,10
472,3059344,"For the third step we  Our second analysis concerns axiom patterns, which we obtain by replacing all atomic terms with a symbol meaning either individual, class, property, datatype or literal.",23,24
473,3059344,"Thus for example the axioms Admiral Sailor and Dog Animal are both reduced to the form C A C A , where the symbol C A means 'any atomic class term'.",23,24
474,16889062,"If the word is shorter than n, it is padded with a special symbol.",14,15
475,8030755,"Each row in this matrix represents a different aligned sentence, and each column represents the alignment of each symbol.",19,20
476,8030755,The special symbol '-' is used to represent the positions of non-alignment points in a sentence.,2,3
477,8030755,"A weighted directed acyclic graph of words is created from the MSA alignment matrix, The graph construction consists of creating as many nodes as columns in the alignment matrix plus one for the final state and as many arcs as cells in the matrix that contain a symbol different to '-'.",48,49
478,8030755,"The arcs with the same source, destination, and symbol are joined, and the weights are obtained by normalizing these counters (Calvo et al.,",10,11
479,2326390,We thereafter use symbol B i to indicate the ith element in the buffer.,3,4
480,8129553,"The restrictions are listed as follows:  If the previous, the current and the next characters are all English or numbers, we would fix the current tag to be ""M"";  If the previous and the next characters are both English or numbers, while the current character is a connective symbol such as ""-"", ""/"", ""_"", ""\"" etc.,",57,58
481,646210,"8In tim following, the symbol .L is meant to denote the undefined element.",5,6
482,17786749,"It is typical in the language modelliug community to represent such words with the symbol UNK, and to calculate the probability for the occurrence of UNK in the test corpus using one of three main strategies.",14,15
483,1807352,"Non-content words, that share the same output tag (the symbol notag in our system), could be also considered to specialize the model.",13,14
484,207905156,The § symbol indicates that there is a significant difference between static and dynamic ST cases.,2,3
485,44023968,"Fixations: starting time, end time and duration of fixation, as well as character offset and word id of fixated symbol in the source or target window.",23,24
486,51878798,The decoding continues until the end-of-sentence symbol is generated.,10,11
487,2647976,"Compounds are identified with a specific non-terminal symbol ""MWX"" where X is the part-of-speech of the expression.",9,10
488,204848146,"We compared 2 strategies to define it: in exhaustive vocabulary mode, hapaxes are replaced at training time by a UNK symbol, with probability 0.5.",22,23
489,18398544,The symbol (-) indicates that the score was significantly degraded compared to that of the domain adaptation (p < 0.05).,1,2
490,13038257,"In Figure 2 , only segment 4 has been locked, which is indicated by a different background colour and a lock symbol.",22,23
491,64848874,The symbol ( †) denotes a score that was significantly improved compared to that of corpus concatenation.,1,2
492,12591723,"For each iteration, a symbol of the grammar is splitted in several symbols according to the different syntactic behaviors of the symbol that occur in a treebank.",5,6
493,12591723,"For each iteration, a symbol of the grammar is splitted in several symbols according to the different syntactic behaviors of the symbol that occur in a treebank.",22,23
494,14818452,"If an input passage contains at least one spelling error, the output format is ""PID [, location, correction]+"", where the symbol ""+"" indicates there is one or more instance of the predicting element ""[, location, correction]"". """,26,27
495,9742832,"For each iteration, a symbol of the grammar is splitted in several symbols according to the different syntactic behaviors of the symbol that occur into a treebank.",5,6
496,9742832,"For each iteration, a symbol of the grammar is splitted in several symbols according to the different syntactic behaviors of the symbol that occur into a treebank.",22,23
497,5720907,"Synthesis As discussed in [Zaharin89] , synthesis in MT is not the same as in generation of natural languages, Generation is the process of generating all grammatical sentences in a language from a given axiom (here, an axiom may be structurally more complex than a single symbol, depending on the linguistic model and grammar formalism adopted).",51,52
498,16815309,"start rules or rules containing axiom trees, as in the axiom or the start symbol S in the classical context free grammar).",15,16
499,16815309,"In order to keep track of such non- VP NP (0/ 13) (0/0_1 ) I V NP n ( 1_2/1_2) (0/2_3) p (0/4_6) ( 0_ 1/()_ I ) I (3_4/3_4) /4 de (2_3/2 ' 3) (4_5/4_5) (5_6/S 6) ruh'14.?1 projective correspondences, we introduce the use of index variables to record the interval corresponding to each symbol appearing in the STRING (as illustrated on the right).",78,79
500,2144766,"Each token is associated with information such as its type (word, number, punctuation, ...), its alphabet (Latin, Greek), its case (lowercase word, capitalized word, ...), and other information for the other symbols (opening or closing punctuation symbol, ...).",52,53
501,7164694,"A WFST is a finitestate automaton which each transition is composed of an input symbol, an output symbol and a weight.",14,15
502,7164694,"A WFST is a finitestate automaton which each transition is composed of an input symbol, an output symbol and a weight.",18,19
503,62722590,"The choice of an augmented context free gralmnar means that the rules are basically in the form of strings of symbols, where each symbol is augmented with an annotated tree structure.",24,25
504,18039149,The symbol L indicates a lexicon set.,1,2
505,18039149,"f ′ (w, c f i ) = e∈W max{0, 1 − δ(e, c f i )} (4) The set W indicates an instance set of w existing in c f i , and the symbol e indicates an element in W. That is, f (w, c f i ) = |W|.",42,43
506,18039149,"In Twitter, the hashtag ""#"" symbol is used to mark keywords or topics in a tweet.",8,9
507,18039149,"The symbol X indicates a set of ⟨u, t⟩ instances in which the u in ⟨u, t⟩ is detected using a method as the source tweet on t and Y indicates a set of ⟨u, t⟩ instances in which the u in ⟨u, t⟩ is actually source tweet on t. We also used F-measure index 2 * P recision * Recall P recision+Recall as a summary of the above measures.",1,2
508,16079625,We add nodes meaning the endmarker symbol after the last node of each ngram history.,6,7
509,16079625,"When we reach the node w n−1 , we continue searching for an endmarker symbol.",14,15
510,16079625,"If the symbol is found, we know that the ngram history w n−1 1 is in the trie.",2,3
511,16079625,We use the endmarker symbol (<#>) for branch discrimination.,4,5
512,16079625,"In prior work, the endmarker symbol has been used to indicate whether an ngram is in the trie.",6,7
513,16079625,We use the endmarker symbol to indicate nodes which are end-of-history words.,4,5
514,16079625,"By using the endmarker symbol, target words can be treated the same as ordinary nodes because all target words are positioned after <#>.",4,5
515,16079625,"First, we trace ""eat"" → ""I"", then trace to the endmarker symbol <#> and find the word ""fish"".",17,18
516,16079625,We do not need to check for endmarker symbol transition because the endmarker symbol <#> is seen for all nodes except target word nodes.,8,9
517,16079625,We do not need to check for endmarker symbol transition because the endmarker symbol <#> is seen for all nodes except target word nodes.,13,14
518,16079625,This means that all endmarker symbol transitions are ensured to be correct and the CHECK array slots of endmarker symbols do not need to be used.,5,6
519,16079625,"When a query for an unknown ngram encounters an endmarker symbol node, the value of the CHECK array is never matched because the corresponding value stored there is negative.",10,11
520,1585700,FORM: Word form or punctuation symbol.,6,7
521,2410140,We retain character pairs differing from each other by at most one symbol in Cangjie codes that tend to be highly similar in shape.,12,13
522,10043882,"where Σ is the relative variance-covariance q × q matrix of the random effects (now q = 4I), σ 2 is the variance of the per-observation term , the symbol ⊥ denotes independence of random variables, and N indicates the multivariate normal distribution.",36,37
523,13858533,"Throughout sections 4 and 5, we mark a score achieved by NMT with the symbol * if this is better than the score of its best competitor at statistical significance level 0.01.",15,16
524,14954974,"There is such a phenomenon in the TreeBank corpus that in the outer layer of a complete verb phrase a ""VP"" symbol was marked repeatedly.",23,24
525,14954974,"Thus, if there is a repetition of a verb phrase marked symbol , we will find two verb phrases at least in the result which satisfy the condition that they share the same Chinese character while their verb phrase tagging are different and the cause of difference is only due to the additional ""(VP)"".",12,13
526,14954974,"According to the fact, we have created a table for each category of verb phrases with the row standing for the index of the verb phrase and the column standing for the relative position of the verb compounds to storing the entire symbol of verb compounds in it and compared the values in column.",43,44
527,14954974,"In a category of verb phrases, if each verb phrase is marked completely, the quantity of symbol belonging to every phrase will be the same.",18,19
528,28356271,Normalization This group bundles all problem classes that deal with symbol resolution.,10,11
529,498640,Trubetzkoy proposed that in such cases we use a different symbol to denote the underspecified or neutralised sound.,10,11
530,5063705,A state of the HMM represents an abstract class of a part of the input symbol sequence.,15,16
531,25325277,"In these neural models, a common strategy was to feed in the morphological tag of the form to be predicted along with the input into the network, where each subtag was its own symbol.",35,36
532,3162692,Each punctuation symbol is considered a separate token.,2,3
533,2545941,"Alternatively, we could generalize over this set of rules by introducing the symbol L N to denote a unit of level N (so that L 0 means text-phrase, L 1 means text-clause, etc.).",13,14
534,7717167,The structure has the symbol V as one of its leaf nodes; this is the position of the main verb.,4,5
535,39718481,The approach abstracts over issues of 1-to-many sound-symbol correspondence in English.,13,14
536,563442,Each token (English word) is represented as a four-byte pointer into a symbol table (dictionary).,16,17
537,563442,"Suffixes are terminated in Figure 6 after the first end of document symbol, unlike before, where suffixes were terminated with the end of corpus symbol.",12,13
538,563442,"Suffixes are terminated in Figure 6 after the first end of document symbol, unlike before, where suffixes were terminated with the end of corpus symbol.",26,27
539,563442,"The aggregation step is illustrated in snapshots C and D of Figure 7 by the two arrows with the ""+"" combination symbol pointing at a value of df in an output statement.",23,24
540,173188653,"A sequence of types is then simply the concatenation of their corresponding representations, where type boundaries can be marked by a special separation symbol.",24,25
541,173188653,"Predicting the next atomic symbol requires for the network to be able to model local, close-range dependencies as ordained by the type-level syntax.",4,5
542,173188653,"The network accepts a sequence of words as input, and as output produces a (longer) sequence of tokens, where each token can be an atomic type, a logical connective or an auxiliary separation symbol that marks type boundaries.",38,39
543,173188653,"At timestep t, the network is tasked with modeling the probability distribution of the next atomic symbol a t , conditional on all previous predictions a 0 , . . . ,",17,18
544,173188653,"Secondly, since there are no pretrained embeddings for the output tokens, we jointly train the Transformer alongside an atomic symbol embedding layer.",21,22
545,173188653,"The third line presents the desired output sequence, with types decomposed to atomic symbol sequences under polish notation, and # used as a type separator.",14,15
546,173188653,"Accuracy is reported on the type-level; that is, during evaluation, we predict atomic symbol sequences, then collapse subtype sequences into full types and compare the result against the ground truth.",18,19
547,173188653,"Digram Encoding Predicting type sequences one atomic symbol or connective at a time provides the vocabulary to construct new types, but results in elongated target output sequence lengths 5 .",7,8
548,173188653,"As a countermeasure, we experiment with digram encoding, creating new atomic symbols by iteratively applying pairwise merges of the most frequent intra-type symbol digrams (Gage, 1994) , a practice already shown to improve generalization for translation tasks (Sennrich et al.,",26,27
549,173188653,"Even more interestingly, for models trained on non-zero merges it is often the case that a type is put together using the correct atomic elements that together constitute a merged symbol, rather than the merged shorthand trained on.",33,34
550,173188653,"2017) , our atomic symbol embeddings may be further utilized by downstream tasks, as a highly refined source of type-level information.",5,6
551,1288087,"The negation symbol ""¬"" in the Table 6 and Table 7 means that the word in corresponding position is not the one in the brackets.",2,3
552,7573506,"The following rule demonstrates the usage of variables in a rule: <np aa=""NP&G&N"">\w</np> -> (<""A&G&Nd"">,<""A&G&Ni"">*)?<""N@&G&Ni""> Here &G and &N are variables for one symbol only and ensure the agreement on gender and number.",34,35
553,333513,"Character type information Character type, like Kanji, Hiragana, Katakana, alphabet, number or symbol, etc.",17,18
554,225063150,The symbol ⊙ is the Hadamard product or element-wise multiplication.,1,2
555,16936514,"For example, since the error in hmtnwt is word-final, its two right-context slots are empty, hence the '#' symbol for both features 9 and 10.",27,28
556,16540126,"On the other hand, there is also an illegalcharacter error where a hand-written symbol is not a legal Chinese character (thus not collected in a dictionary).",16,17
557,219530530,symbol ('infection asymptomatic ?,0,1
558,220057135,"Many have proposed the use of a dummy CLS token to generate the feature vector, where CLS is a special symbol added in front of every sequence during pre-training, with its final hidden state always used as the aggregate sequence representation for classification tasks, referring to CLS pooling.",21,22
559,12746295,"bols to be aligned across sequences are restricted to a fixed set, our symbol set is not fixed or certain because the symbols correspond to medical events in clinical narratives.",14,15
560,12746295,"Moreover, we cannot have fixed scores for symbol transformations since our transformations correspond to coreference and temporal relations between the medical events across sequences.",9,10
561,12746295,"Alignment using a Weighted Finite State Representation A weighted finite-state transducer (WFST) is an automaton in which each transition between states is associated with an input symbol, an output symbol, and a weight (Mohri et al.,",30,31
562,12746295,"Alignment using a Weighted Finite State Representation A weighted finite-state transducer (WFST) is an automaton in which each transition between states is associated with an input symbol, an output symbol, and a weight (Mohri et al.,",34,35
563,12746295,"In order to accommodate the temporal relations before and after, we insert a null symbol after every medical event in each sequence in the scoring matrix.",15,16
564,227231606,t n where t i such that i ≥ 1 is the i th token of the sentence and t 0 is a dummy root symbol.,25,26
565,5128435,"Grammars An IG is a tuple G = {Σ, C, S, P, phon}, where Σ is the terminal alphabet, C the non-terminal alphabet, S ∈ C the initial symbol, P is a set of PTDs with node labels in C × P, and phon is a function from anchors in P to Σ. The structure obtained from parsing is a syntactic tree, a totally ordered tree in which all nodes are labelled with a non-terminal.",39,40
566,6250952,"Game logs have also a lot of platform-dependent particularities in the way they encode emoticons, e.g. the :smile: symbol, which is a placeholder to show a smile icon, is split as three different tokens (: smile :) by the tokeniser.",23,24
567,481512,"nt(β n )) | β ∈ A, n = rk(β), X = lab(β r )} ∪ {X A − → ε A | X A ∈ N A } The ε-rules X A − → ε A for each symbol X A account for adjunction sites where no adjunction takes place.",47,48
568,481512,"With the same variable t, we define accordingly: feat(γ i ) = top : t bot : bot(γr ) if γ i = γ r top : top(γ i ) bot : bot(γ i ) otherwise (4) Finally, we add ε-rules (X A , top : v bot : v ) − → ε A for each symbol X A in order to account for adjunction sites where no adjunction takes place.",65,66
569,2111966,"To extend the approach to the representation of semantic information as introduced in 2, clause (4) is modified as follows: Goal ::= Dimension+=Description | Name | Goal ∨ Goal | Goal ∧ Goal Note that, with this modification, the XMG language no longer correspond to a Definite Clause Grammar but to an Extended Definite Clause Grammar (see (Van Roy, 1990 )) where the symbol += represents the accumulation of information for each dimension.",75,76
570,18427149,Note also that we use only the reflexive transitive closure of precedence between sibling nodes and it is explicitly stated with the symbol ≺ * .,22,23
571,18427149,The ⊥ symbol indicates that this two colours cannot be merged and hence two nodes labelled with these colours cannot be merged.,2,3
572,11427756,The candidate terms sharing the same metaphone symbol are selected based on term frequencies.,7,8
573,10134201,→) symbol is a candidate for left (resp.,2,3
574,5892229,"s n where s 0 is a dummy root symbol, we consider the directed graph D = (V, A) with V = {0, . . .",9,10
575,14506329,"2007; Koo and Collins, 2005; Yu and Lam, 2008) suggested a summation in the numerator and the denominator of equation 1: p(y I 1 |x J 1 ) = A e I i=1 H(A,y i−1 ,y i ,x J 1 ) Ã ỹI 1 ,I(A) e I i=1 H( Ã,ỹ i−1 ,ỹ i ,x J 1 ) (3) Three types of features h l (y i−1 , y i , x J 1 ) were used to support the conditional probability, first source-n-gram features depending only on one target symbol y i and a combination of source symbols x A(j)+γ 2 A(j)+γ 1 relative to the currently aligned source word x A(j) (with γ 1 ≤ γ 2 ), with γ 1 , γ 2 = −5, . . . ,",114,115
576,14506329,"At each source symbol the last target symbol is continued, a new one begins, or two target symbols begin.",3,4
577,14506329,"At each source symbol the last target symbol is continued, a new one begins, or two target symbols begin.",7,8
578,14506329,Each target symbol is labeled to make this mapping bijective.,2,3
579,14506329,At each source symbol all aligned target words are known and at each target symbol all aligned source symbols are known.,3,4
580,14506329,At each source symbol all aligned target words are known and at each target symbol all aligned source symbols are known.,14,15
581,26604137,"For instance in context-free grammars the derived tree is built from a sequence of substitutions of a nonterminal symbol with a string of symbols, whereas in tree adjoining grammars (TAGs) a derivation is a sequence of substitutions and adjunctions over elementary trees.",20,21
582,26604137,"w n ) where w k is the word at position k and w 0 is a dummy root symbol, a derivation is a triplet (d, s, l) defined as follows.",19,20
583,34723102,"The symbol ""+"" denotes a non-null list of the preceding components.",1,2
584,34723102,"In the actual implementation of the SCFG rules, we designed each of the rules in the rule set to be deterministic, by using regular expressions for obtaining a unique match for a terminal symbol.",35,36
585,218974142,"Finally, a distinctive tokenization strategy is adopted in ATDT with respect to at-mentions that are always split by separating the @ symbol from the username.",24,25
586,1326398,"PCFG-LA Let us recall that PCFG-LAs are defined as tuples G = (N , T , H, R H , S, p) where: • N is a set of observed non-terminals, among which S is the distinguished initial symbol, • T is a set of terminals (words), • H is a set of latent annotations or hidden states, • R H is a set of annotated rules, of the form • p : R H → R ≥0 defines the probabilities associated with rules conditioned on their left-hand side.",50,51
587,1326398,"Like Petrov and Klein (2007) , we impose that the initial symbol S has only one latent annotation.",13,14
588,3874199,The Guangyun system is in a similar manner with a smaller phonetic symbol set.,12,13
589,43603711,"The objective involves three kinds of errors: a next symbol error for predicting the next word in a phrase, a semantic error for the comparison of the summary phrase representations, and a reconstruction error for the autoencoder.",10,11
590,43603711,"There are three kinds of prediction error, which we denote: the next symbol E o error, the semantic error E c , and the reconstruction error E a .",14,15
591,43603711,Equation ( 5 ) represents the sum of the next symbol error distance between each input r k in the source-side phrase and the output prediction o k .,10,11
592,9410054,"-13 Punctuation Features the ratio between punctuation characters and other characters, the number of words containing a full stop, the number of sentences produced by an off-the-shelf sentence splitter for each segment (included in NLTK (Bird, 2006 )), whether or not the segment contains a dash, an ellipsis, and whether or not the segment ends with a punctuation symbol.",71,72
593,2711495,Each symbol of the grammar is enriched with annotation symbols behaving as subclasses of this symbol.,1,2
594,2711495,Each symbol of the grammar is enriched with annotation symbols behaving as subclasses of this symbol.,15,16
595,35789919,"2009) , we save space by using symbol-level compression.",8,9
596,12723491,Split each annotation of each symbol into n (usually 2) new annotations and create rules with the new annotated symbols.,5,6
597,12723491,This is done by mapping rare words in the training data to a special UNKNOWN terminal symbol and estimating rule probabilities in the usual way.,16,17
598,12723491,2 are the result of applying the rare-word-to-unknown-symbol transformation.,15,16
599,12723491,"The greater the proportion of the probability mass that we reserve for the unknown word section of the grammar, the more performance suffers on the known yet rare words since these are the words which are mapped to the UNKNOWN symbol.",41,42
600,12723491,Now the word recovered is mapped to the symbol UNK-ed and the only edge which is added to the chart at this position is the one corresponding to the rule V BD → UNK-ed.,8,9
601,9080688,"For the learning framework, we take all sentences in the News Commentary domain (training, development and test sets) from WMT-09 (∼ 75K) and extract those containing one OOV word that is not a proper name, symbol or number (∼ 15% of the sentences).",43,44
602,5590763,An RNN can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence.,17,18
603,5590763,"3) From this learned distribution, it is straightforward to sample a new sequence by iteratively sampling a symbol at each time step.",19,20
604,5590763,The encoder is an RNN that reads each symbol of an input sequence x sequentially.,8,9
605,5590763,"As it reads each symbol, the hidden state of the RNN changes according to Eq. (",4,5
606,5590763,"After reading the end of the sequence (marked by an end-of-sequence symbol), the hidden state of the RNN is a summary c of the whole input sequence.",16,17
607,5590763,The decoder of the proposed model is another RNN which is trained to generate the output sequence by predicting the next symbol y t given the hidden state h t .,21,22
608,5590763,"Hence, the hidden state of the decoder at time t is computed by, h t = f h t−1 , y t−1 , c , and similarly, the conditional distribution of the next symbol is P (y t |y t−1 , y t−2 , . . . ,",36,37
609,5590763,"The input matrix between each input symbol x t and the hidden unit is approximated with two lower-rank matrices, and the output matrix is approximated similarly.",6,7
610,17324091,"Our entry point into the MRS is the elementary predication (EP), and our pipeline algorithm explores the three main EP components: arguments and associated variables, label, and predicate symbol.",34,35
611,1739888,It uses the words and POS tags around the edges of the covered by the current non-terminal symbol.,19,20
612,1739888,"W represents a word, P represents the part-of-speech of the word, C represents the sum of the chunk containing the word, F represents the first word of the chunk containing the word, L represents the last word of the chunk containing the word, S represents that the word is a non-terminal symbol or not.",62,63
613,16868700,"For that we collected and documented the file naming conventions in the dataset, discovering the following patterns: • file names of the aligned pair differing only in the language (e.g. ""Movie Title en.txt"" and ""Movie Title fr.txt"") • file names starting with the same 4-to-5-digit ID (e.g. ""12345 en.txt"" and ""12345nl6.txt"") • file names containing the same 9-symbol ID (digits and capital letters), followed by a 3character language code (e.g. ""Deutsche Titel AXGM0102A DEU.PAC"" and ""English Title-AXGM0102A ENG.PAC"") Table 2 : Results of automatic language identification, contrasted with manually specified language pairs; the ""Other"" languages do not include Italian and Danish, as these are not covered in the SUMAT project. •",77,78
614,16868700,"8-symbol file names starting with the same movie ID (4 letters) and a 2-character language code (e.g. ""MISSENDC.txt"" and ""MISSNLDV.TXT"") Even while comparing file names, it is inefficient to try to align a document to all other documents, so we trimmed the search space by comparing only files within the same genre and domain.",2,3
615,43550048,Although it is practically impossible to automatically sort the New York documents by categories using the UN symbol (an alphanumeric ID contained in all documents issued to the Official Document System 16 ).,17,18
616,7814136,"The parse tree is binarized and the symbol @ is used to indicate the extra tags generated in tree binarization (for example, @N P in Figure 2 ).",7,8
617,7814136,"To remove the duplications, a merge operation is carried out as follows: suppose treelets T 1 and T 2 are extracted from the same sentences while sharing the same root symbol, if T 1 is also a treelet of T 2 and their reordering regions AB overlap, then T 2 is merged into T 1 .",32,33
618,12081317,This is shown by prefacing the username with an @ symbol.,10,11
619,12081317,This feature has been replaced with the following word after # symbol.,11,12
620,18203211,A negated context includes any word in the target phrase or context that is following a negation word 1 up to the next following punctuation symbol. •,25,26
621,1538633,"When all the children of a node have been exhausted, the ""#"" symbol is added, and when all the nodes at some depth have been exhausted, the ""$"" is added.",15,16
622,2176031,"DEF contains up to four types of primitives: the basic independent primitive , the other independent primitive , the relation primitive , and the symbol primitive , where the basic independent primitive and the other independent primitive are used to indicate the semantics of a concept and the others are used to indicate syntactical relationships.",25,26
623,13455658,"Discussion In this paper I have showed that a traditional symbol-based grammatical approach can be as good as, or even superior to, a data-based machine learning approach, in specific domains where the language and the possible actions are restricted.",10,11
624,1649928,"Formally, an ITG is a tuple ⟨N, Σ, ∆, R, S⟩, where N is a finite nonempty set of nonterminal symbols, Σ is a finite set of terminal symbols in L 0 , ∆ is a finite set of terminal symbols in L 1 , R is a finite nonempty set of inversion transduction rules and S ∈ N is a designated start symbol.",70,71
625,1649928,"An inversion transduction rule is restricted to take one of the following forms: S → [A] , A → [ ψ + ] , A → ⟨ψ + ⟩ where S ∈ N is the start symbol, A ∈ N is a nonterminal symbol, and ψ + is a nonempty sequence of nonterminals and biterminals.",39,40
626,1649928,"An inversion transduction rule is restricted to take one of the following forms: S → [A] , A → [ ψ + ] , A → ⟨ψ + ⟩ where S ∈ N is the start symbol, A ∈ N is a nonterminal symbol, and ψ + is a nonempty sequence of nonterminals and biterminals.",47,48
627,1649928,"A biterminal is a pair of symbol strings: Σ * × ∆ * , where at least one of the strings have to be nonempty.",6,7
628,1649928,"The brackets are frequently left out when there is only one element on the righthand side, which means that S → [A] is shortened to S → A. Like CFGs, ITGs also have a 2-normal form, analogous to the Chomsky normal form for CFGs, where the rules are further restricted to only the following four forms: S → A, A → [BC] , A → ⟨BC⟩, A → e/f where S ∈ N is the start symbol, A, B, C ∈ N are nonterminal symbols and e/f is a biterminal string.",91,92
629,1649928,"A bracketing ITG, or BITG, has only one nonterminal symbol (other than the dedicated start symbol), which means that the nonterminals carry no information at all other than the fact that their yields are discrete unit.",11,12
630,1649928,"A bracketing ITG, or BITG, has only one nonterminal symbol (other than the dedicated start symbol), which means that the nonterminals carry no information at all other than the fact that their yields are discrete unit.",18,19
631,1649928,"Generation derives a bisentence, a sentence pair, from the start symbol.",12,13
632,1649928,Transduction derives a sentence in one language from a sentence in the other language and the start symbol.,17,18
633,1649928,Biparsing verifies that a given bisentence can be derived from the start symbol.,12,13
634,8332545,Note the symbol @ indicates the right-binarization symbols (e.g. @V P in the figure).,2,3
635,353451,"The decoder, on the other hand, generates each target word at a time, until the end-of-sentence symbol is generated.",23,24
636,12351972,"Each EP has a predicate symbol, and a label (also called handle) that identifies the EPs position within the MRS structure.",5,6
637,8224426,"2007) which encodes an SCFG as a trie in which each node represents a (partial or completed) rule, and a node has outgoing edges for each possible continuation of the rule in the grammar, either a source-side terminal symbol or pair of non-terminal-symbols.",45,46
638,8224426,"However, in CYK+, we cannot yet know if the rule can be completed with a cell (3, x) containing symbol NP, since the cell (3, 4) may be processed after cell (1, 2).",25,26
639,8224426,"In this recursive step, we search for a symbol NP in any cell (3, x), and upon finding it in cell (3, 4), add S as type-1 item to cell (1, 4).",9,10
640,8224426,"After cube pruning, the chart cell is filled with a mapping from a non-terminal symbol to an object that compactly represents a collection of translation hypotheses and associated scores.",17,18
641,8224426,"For each start position and non-terminal symbol, we maintain an array of possible end positions and the corresponding chart entry, as illustrated in Table 1.",8,9
642,8224426,"Using the previous example, instead of searching all cells (3, x) for a symbol NP, we only need to retrieve the array corresponding to start position 3 and symbol NP to obtain the array of cells which can continue the partial production.",17,18
643,8224426,"Using the previous example, instead of searching all cells (3, x) for a symbol NP, we only need to retrieve the array corresponding to start position 3 and symbol NP to obtain the array of cells which can continue the partial production.",33,34
644,8224426,"Hopkins and Langmead (2010) show that the complexity of parsing n-ary rules is determined by the number of choice points, i.e. non-terminals that are initial, consecutive, or final, since terminal symbols in the rule constrain which cells are possible application contexts of a non-terminal symbol.",56,57
645,8224426,Williams and Koehn (2012) describe a parsing algorithm motivated by Hopkins and Langmead (2010) in which they store the grammar in a compact trie with source terminal symbols or a generic gap symbol as edge labels.,36,37
646,11336213,"For each end-of-sequence symbol that is selected among the highest scoring candidates the beam-width is reduced by one, until the beamwidth reaches zero.",7,8
647,14984736,"Formally, an ITG is a tuple ⟨N, Σ, ∆, R, S⟩, where N is a finite nonempty set of nonterminal symbols, Σ is a finite set of terminal symbols in L 0 , ∆ is a finite set of terminal symbols in L 1 , R is a finite nonempty set of inversion transduction rules and S ∈ N is a designated start symbol.",70,71
648,14984736,"A normalform ITG consists of rules in one of the following four forms: S → A, A → [BC] , A → ⟨BC⟩, A → e/f where S ∈ N is the start symbol, A, B, C ∈ N are nonterminal symbols and e/f is a biterminal.",40,41
649,14984736,"A biterminal is a pair of symbol strings: Σ * × ∆ * , where at least one of the strings have to be nonempty.",6,7
650,14984736,"Modeling of unary rules (with start symbol on the left hand side) although similar, is beyond the scope of this paper.",7,8
651,11485102,The objective of the normalization is folding English letters with similar phonetic to the same letter or symbol. (,17,18
652,11485102,The objective of the normalization is folding English letters with similar phonetic to one letter or symbol.,16,17
653,11485102,The objective of the normalization is folding English letters with similar or close phonetic to same letter or symbol. (,18,19
654,202764484,"For an input sentence, we use a convolution filter W p to slide along s n as [w t : • • • : w t+u−1 ] * W p + b, where * is the convolution symbol, b ∈ R is a bias and p represents the p th filter in a filter set.",40,41
655,1249398,"The current root node of the subtree is either the non-terminal symbol PP or S. Relying on these identified subtrees, we introduced a few more features.",13,14
656,3827043,"Consider that head constituents are always determined by their syntactic symbol and their neighbors, whose order and relations strongly affects the head labeling.",10,11
657,7731140,"It deals with data sparsity by grouping frequent tag sequences into a single token, and also by grouping tags and tag sequences into categories, each assigned a single symbol.",30,31
658,53234666,"Instead, reviewers will issue a gesture in a very specific context, and so a proofreading symbol may change its meaning.",17,18
659,53234666,Both gestures would popup a menu where the user could select deterministically the symbol to insert; see Figure 4 .,13,14
660,14358582,The target non-terminal vocabulary furthermore contains an initial non-terminal symbol Q. Source sides of the rules are not decorated with syntactic annotation.,13,14
661,14358582,"The source non-terminal vocabulary contains a single generic non-terminal symbol X. In addition to the extracted grammar, the translation system makes use of a special glue grammar with an initial rule, glue rules, a final rule, and top rules.",13,14
662,14358582,"The (explicit) target non-terminal vocabulary N E thus also contains only the generic non-terminal symbol X, just like the source non-terminal vocabulary N F .",20,21
663,14358582,"3 In cases where a span is still not covered by a symbol, we nevertheless memorize a sourceside syntactic label vector but indicate the failure for the uncovered non-terminal with a special label.",12,13
664,8744838,The NRCHashtag Sentiment Lexicon is based on the common practice that users use the # symbol to emphasis on a topic or a word.,15,16
665,14608227,"In this transducer, each transition is labelled with a symbol representing an SL pattern and a symbol representing an operation on a TL pattern.",10,11
666,14608227,"In this transducer, each transition is labelled with a symbol representing an SL pattern and a symbol representing an operation on a TL pattern.",17,18
667,17565181,"Words that do not belong to the following POS tags are removed (stop POS tag process): noun 7 , verb, adjective, adverb, adnominal, interjection, filler, symbol-alphabet, and unknown.",33,34
668,17565181,"ellipsis], °[degree symbol], д [a character often used in Japanese emoticons], 行く ""go"" positive #11 USER [normalized user name], 食べる ""eat"", 美味しい ""delicious"", 飲む ""drink"", 屋 ""shop"", 料理 ""meal"", ラーメン ""ramen"", 店 ""shop"", コーヒー ""coffee"", 肉 ""meat"" positive #30 !,",6,7
669,549380,"The resulting architecture for the statistical machine translation approach is shown in Alignment Models The alignment model P r(f J 1 , a J 1 |e I 1 ) introduces a 'hidden' alignment a = a J 1 , which describes 1 The notational convention will be as follows: we use the symbol P r(•) to denote general probability distributions with (nearly) no specific assumptions.",55,56
670,549380,"In contrast, for model-based probability distributions, we use the generic symbol p(•).",14,15
671,688770,For the tweetnews dataset we also removed the hashtag symbol ('#') prior to applying the Stanford tools.,9,10
672,13270994,"A target phrase is a sequence of target word symbols followed by a special stop symbol, a fixed-length sequence of scores, and a sequence of alignment points followed again by a special stop symbol.",15,16
673,13270994,"A target phrase is a sequence of target word symbols followed by a special stop symbol, a fixed-length sequence of scores, and a sequence of alignment points followed again by a special stop symbol.",37,38
674,13270994,"Size reduction is achieved by compressing the symbol sequence of a target phrase collection using symbol-wise Huffman coding (Huffman, 1952; Moffat, 1989) .",7,8
675,13270994,"Size reduction is achieved by compressing the symbol sequence of a target phrase collection using symbol-wise Huffman coding (Huffman, 1952; Moffat, 1989) .",15,16
676,13270994,For target phrase words and alignment points one special stop symbol has to be added.,10,11
677,13270994,Then the decision function d can determine the type of a given symbol based on its encoded numerical value.,12,13
678,13270994,"If t i is not aligned with any source word, it is encoded as a plain symbol of type 1.",17,18
679,13270994,"|t|} do if J i = ∅ then t ← t • e 1 (t i ) else r ← min{r(s j , t i ) : j ∈ J i } k ← min{j : j ∈ J i ∧ r(s j , t i ) = r} if k = i then t ← t • e 2 (t i ) else t ← t • e 3 (t i ) end Â ← Â \ { k, i } end end return t, Â end Algorithm 1: Rank encoding only the rank r is encoded (symbol type 2).",107,108
680,13270994,"Otherwise, k and r are encoded together as one symbol (symbol type 3).",10,11
681,13270994,"Otherwise, k and r are encoded together as one symbol (symbol type 3).",12,13
682,13270994,Each symbol is decoded using the appropriate decoding function based on the symbol type.,1,2
683,13270994,Each symbol is decoded using the appropriate decoding function based on the symbol type.,12,13
684,13270994,"For symbols of type 3, the source word position j is recovered from the symbol and the target word is looked-up, a point j, i is added to the alignment.",15,16
685,3859597,"When the character ""•"" appears in the training data, it is generally used as a connections symbol in a foreign person name, such as ""圣•约翰 (Saint John)"".",19,20
686,3859597,"A similar rule is designed to unite consecutive digits on the sides of the symbol ""."",",14,15
687,18559684,Note that our rule set keeps the convenient property of the standard hierarchical grammar that the initial symbol S needs to be expanded in the leftmost cells of the CYK chart only.,17,18
688,18559684,A maximum length constraint of 10 is applied to all non-terminals but the initial symbol S .,16,17
689,18559684,"Dropping Length Constraints In order to find out if we lose performance by applying the maximum length constraint of 10 to all non-terminals but the initial symbol S during decoding, we optimized systems with no length constraints.",28,29
690,3863077,"b ) is a wrong one, while the probability may be not small enough, (c) is also wrong, but the probability is very small due to the symbol vb.",32,33
691,3863077,"The expression after the first ""="" is the meaning of the word, and the symbol ""+"" indicates the melding of meaning.",17,18
692,14946256,"Simple lexical features: Relative document length differences, number overlap, case overlap, and stock symbol named entity recognition. •",17,18
693,6420375,"The following are examples of the hierarchical CFG rules extracted from the Chinese-English sentence pair (Aozhou shi yu Beihan you bangjiao de shaoshu guojia zhiyi, Australia is one of the few countries that have diplomatic relations with North Korea) [3] : X →< yu X 1 you X 2 , have X 2 with X 1 > (8) X →< X 1 de X 2 , the X 2 that X 1 > (9) Hierarchical rules are extracted from the training corpus by subtracting continuous phrase-pairs attested in the translation table recursively from longer phrases and replacing them with the non-terminal symbol X. Non-terminals in hierarchical rules act as placeholders that are replaced with other phrases during translation in a bottom-up fashion.",117,118
694,13355088,"On the other hand, throughout most transformations, some edges must be reversed to make a tree, so we need a symbol to indicate a edge on the tree is reversed during transformation.",23,24
695,13355088,The auxiliary labels are listed below: • Label with following ∼R: The symbol ∼R means this directed edge is reversed from the original directed graph. •,14,15
696,13355088,N ] followed by label: The symbol [N ] (N is an integer) represents the head of the edge.,7,8
697,7345848,We treat top nodes as a special semantic role TOP whose predicate is a dummy root symbol.,16,17
698,12176066,"AllPunc, whether the token is a punctuation or a symbol.",10,11
699,12176066,"ContainsPunc, whether the token contains a punctuation or a symbol letter.",10,11
700,7025582,The notational convention will be as follows: we use the symbol P r(•) to denote general probability distributions with (nearly) no specific assumptions.,11,12
701,7025582,"In contrast, for model-based probability distributions, we use the generic symbol p(•).",14,15
702,7025582,"In Figure 1 , the blue (with plus symbol) and green line (with star symbol) plots the classification error rate on the newswire articles and on the newsgroup articles respectively.",9,10
703,7025582,"In Figure 1 , the blue (with plus symbol) and green line (with star symbol) plots the classification error rate on the newswire articles and on the newsgroup articles respectively.",17,18
704,7025582,The red line (with circle symbol) indicates the error rate in both domains.,6,7
705,18401158,We use the symbol Pr( . ),3,4
706,18401158,"In contrast, for model-based probability distributions, we use generic symbol p( . )",13,14
707,15554960,Three techniques are developed to achieve these two goals: 1) converting words into integer symbol IDs (vocId); 2) cross-indexing the phrase translation models to reduce the redundancy in model representation; and 3) serializing the model structure to make it directly accessible from disk.,16,17
708,15554960,"Since we are not decoding from left-to-right, the sentence start symbol are not available to the partial hypotheses until the whole sentence is decoded.",15,16
709,3850920,"Language is a hierarchical symbol system, which allows sentiment analysis to be conducted on different language levels.",4,5
710,2013509,"Basic help As can be seen in Figure 1 , each word tile has a ""+"" symbol on the left which, when tapped, inserts a new empty tile above that word, enabling the post-editor to insert a new word.",18,19
711,6358777,"At the start of summer the decoder used translation rules with a single generic non-terminal symbol, later syntactic categories were used, and by the end of the summer the translation rules included semantic elements such as named entities and modalities.",17,18
712,6358777,"Generic symbols in translation rules (e.g., the non-terminal symbol ""X"") were replaced with structured information at multiple levels of abstraction, using a tree-grafting approach, as described in more detail in the following sections.",12,13
713,6358777,"Hiero uses grammars with a single nonterminal symbol ""X"" rather than using linguistically informed non-terminal symbols.",7,8
714,17326466,"16 Those values which are significantly better are marked with the symbol ↑, whereas values significantly worse are marked with ↓. As can be seen, the two decision-tree-based approaches are, in general, better than the heuristic-based approach.",11,12
715,17693367,The best model was obtained by training the NERsuite with UMLS dictionary in case-number-symbol normalization mode.,17,18
716,2580007,"The start symbol is *S. Results We In order to avoid generating incorrect meaning representations when it is not confident, KRISP uses a threshold and if the confidence (probability) of the best semantic derivation is below this threshold, it does not generate any meaning representation.",2,3
717,12964167,Each initial phrase is marked with the nonterminal symbol of the closest matching node as described in the above subsection.,8,9
718,12964167,"When producing hierarchical rules, the gaps are labelled with the non-terminal symbols of the corresponding phrases instead of the original generic non-terminal X. It is important to point out that the syntax information is extracted from the target side only, but the substitution of the corresponding non-terminal symbol is carried out both on the source and the target sides (with the same non-terminal on both sides).",55,56
719,12964167,"The first one be denoted by p h (Y |d) (h for ""head"") and reflect the probability that the derivation d under consideration of the additional non-terminal symbols has Y ∈ S as its starting symbol.",43,44
720,12899478,"Synchronous rules in Hiero take the form as in (1): X → α, γ, ∼ (1) where X is the nonterminal (NT) symbol, and α and γ are the source and target phrases, which contain combinations of terminal and nonterminals in the source and target language.",30,31
721,12899478,The ∼ symbol indicate a one-to-one correspondence between NTs in α and γ.,2,3
722,6780908,"The restrictions are list as follows: If the previous, the current and the next characters are all English or numbers, we would fix the current tag to be ""M""; If the previous and the next characters are both English or numbers, while the current character is a connective symbol such as ""-"", ""/"", ""_"", ""\"" etc.,",55,56
723,39215210,"Notice, that a hypothesis is final if it ends with the special end-of-sentence symbol that occurs at the end of all N-best strings.",18,19
724,27611775,"The symbol ‡ marks morphological attributes that they propagated from the noun (for example, an adjective's case is copied from the noun it modifies).",1,2
725,14614236,"Wu(1997) proposes a simple ITG which only consists of five types of rules: The Constituent Structure Constraint ] A A [ A a ⎯→ ⎯ > < ⎯→ ⎯ A A A a j i/v u A bij ⎯→ ⎯ ε ε / u A i bi ⎯→ ⎯ j /v A j b ε ε ⎯→ ⎯ Where A is the only non-terminal symbol, and [] and <> represent the two operations which generate outputs in straight and inverted orientation respectively.",70,71
726,5126936,"Doermann and Yao presented a system for modeling the OCR output errors, and they described some symbol and page models to simulate the degraded images during scanning, decomposing and recognition (Doermann & Yao, 1995) .",17,18
727,13567024,"An ITG can always be written in 2-normal form and is represented by a tuple ⟨N, V 0 , V 1 , R, S⟩ where N is a set of non-terminals, V 0 and V 1 are the vocabularies of L 0 and L 1 respectively, R is a set of transduction rules and S ∈ N is the start symbol.",68,69
728,1274371,"Therefore, we define a sentence break symbol, <b>, that is used at the beginning and at the end of a chunk.",7,8
729,1016771,Hierarchical rules are extracted from the training corpus by subtracting continuous phrase-pairs attested in the translation table recursively from longer phrases and replacing them with the nonterminal symbol X. Nonterminals in hierarchical rules act as placeholders that are replaced with other phrases during translation in a bottomup fashion.,29,30
730,13416751,"The transformation of a parallel corpus into a corpus of single sentences is performed with the help of statistical alignments: each word (or substring) is joined with its translation in the output sentence, creating an extended symbol.",40,41
731,31481386,y t−1 (where y 0 is a conventional sentence delimiter symbol).,11,12
732,237558786,Table 3 : An example of the use of split points (the split position is marked here using the <wall /> token) to constrain the decoding process (Skillman here is an out-of-vocabulary word and is marked with a '|' symbol.).,50,51
733,14576857,"If any of the left or right arguments in C do not exist, it is replaced with X symbol.",19,20
734,14576857,For example the atomic category S might have a feature attached to it (described between two brackets after the category symbol) which distinguishes types of sentences such as declarative S[dcl] or wh-question S[wq].,21,22
735,14315601,"In the following description of each method, e is the escape probability and p(φ) is the conditional probability for symbol φ , given a context.",21,22
736,14315601,c(φ) is the number of times the context was followed by the symbol φ .,13,14
737,14315601,Method A works by allocating a count of one to the escape symbol.,12,13
738,14315601,"e = 1 n + 1 (9) p(φ) = c(φ) n + 1 (10) Method B makes assumption that the first occurrence of a particular symbol in a particular context may be taken as evidence of a novel symbol appearing in the context, and therefore does not contribute towards the estimate of the probability of the symbol which it occurred.",31,32
739,14315601,"e = 1 n + 1 (9) p(φ) = c(φ) n + 1 (10) Method B makes assumption that the first occurrence of a particular symbol in a particular context may be taken as evidence of a novel symbol appearing in the context, and therefore does not contribute towards the estimate of the probability of the symbol which it occurred.",44,45
740,14315601,"e = 1 n + 1 (9) p(φ) = c(φ) n + 1 (10) Method B makes assumption that the first occurrence of a particular symbol in a particular context may be taken as evidence of a novel symbol appearing in the context, and therefore does not contribute towards the estimate of the probability of the symbol which it occurred.",63,64
741,14315601,"e = t n (11) p(φ) = c(φ) − 1 n (12) Method C (Moffat, 1990 ) is similar to Method B, with the distinction that the first observation of a particular symbol in a particular symbol in a particular context also contributes to the probability estimate of the symbol itself.",41,42
742,14315601,"e = t n (11) p(φ) = c(φ) − 1 n (12) Method C (Moffat, 1990 ) is similar to Method B, with the distinction that the first observation of a particular symbol in a particular symbol in a particular context also contributes to the probability estimate of the symbol itself.",45,46
743,14315601,"e = t n (11) p(φ) = c(φ) − 1 n (12) Method C (Moffat, 1990 ) is similar to Method B, with the distinction that the first observation of a particular symbol in a particular symbol in a particular context also contributes to the probability estimate of the symbol itself.",58,59
744,14315601,"e = t n + t (13) p(φ) = c(φ) n + t (14) Method D (Howard, 1993) is minor modification to method B. Whenever a novel event occurs, rather than adding one to the symbol, half is added instead.",45,46
745,14315601,"Suppose the character following dealornodeal is o. Since the order-2 context is al and the upcoming symbol o has already seen in this context, the order-2 model is used to encode the symbol.",16,17
746,14315601,"Suppose the character following dealornodeal is o. Since the order-2 context is al and the upcoming symbol o has already seen in this context, the order-2 model is used to encode the symbol.",33,34
747,14315601,We chose a trie structure to store the symbol and count.,8,9
748,198909640,This continues until the end-of-sequence symbol is obtained.,9,10
749,2854427,"By span, we denote the number of tokens that are allowed to be covered by a non-terminal symbol (usually ""X"") in the source language side of an SCFG rule.",20,21
750,2854427,"In approaches which do not use linguistic syntactic labels (such as ITG (Wu, 1997) or Hiero, where only the start symbol S and the non-terminal X are used), the maximal span size allowed in implementations is often between 10 and 15 tokens, because using wider spans has (in experiments done in the past) resulted in decreased translation quality (e.g., (Chiang, 2007) ).",25,26
751,5456786,"In Figure 2 (c), we can see that each symbol or punctuation mark is separated as a single token.",12,13
752,5456786,tags can be kept as a whole symbol during tokenisation.,7,8
753,36425898,"This is because for each lexical entry there is only one possible pronunciation, and therefore in the lexicon transducer at any state there is at most one transition with a given output symbol.",33,34
754,8443926,"symbol, we identified all tokens which had a period symbol flanked by alphabetic characters.",0,1
755,8443926,"symbol, we identified all tokens which had a period symbol flanked by alphabetic characters.",10,11
756,237558718,"In the corpora, numerical quantities are replaced by a single category symbol.",12,13
757,235899058,"A language id symbol "" LID "" is used as the initial token to predict the sentence.",4,5
758,235899058,Speech recognition task is treated as the same as the speech translation task but with the source speech language id symbol.,21,22
759,18700572,The correspondence in the target language string of the substitution node is called a substitution symbol.,15,16
760,18700572,"The substitution symbol can represent a single word, or phrase that can be expanded by other matching-tree.",2,3
761,18700572,"Examples of TSC substitution node, its corresponding substitution symbol will be replaced by the translation candidate of the TSC whose root node corresponds to this substitution node.",9,10
762,18700572,"If the target language string contains the substitution symbol, then the substitution symbol is replaced with the translation of the corresponding substitution node.",8,9
763,18700572,"If the target language string contains the substitution symbol, then the substitution symbol is replaced with the translation of the corresponding substitution node.",13,14
764,38833319,"The Hierarchical Phrase-based Model The hierarchical phrase-based model is based on a weighted SCFG, which consists of the following rewrite rules: X → α, γ, ∼ (1) where X is a non-terminal symbol, α/γ is a string consisting of source/target terminals and nonterminals.",44,45
765,541464,"Similarly structural rules, abbreviated Struct, are rules containing at least one such nonterminal symbol.",15,16
766,237940508,"Dropping the Unobserved Latent Variable Building on observations from equation 3, we question the usefulness of training both latent variables for conditional generation when semi-supervised learning only aims for an improvement on the inference of the partially-observed latent variable y. For the case of language generation, the sequence of discrete symbols in each sample is often modeled by an autoregressive distribution p θ (x|y, z) = i p θ (x i |y, z, x <i ) where x i is the i th symbol in the sequence, and x <i are the symbols preceding x i .",96,97
767,236486287,"x n , with x 0 a dummy root symbol, and otherwise x i the i th word, and n the number of words.",9,10
768,11410088,Conjunctions of predicates for the source or target are denoted with a + symbol.,13,14
769,2141094,"Consider the word ""#dog#"", where # is a word boundary symbol.",15,16
770,17679323,"There are 9 different non-terminal symbols in the tree, of which Query is the start symbol.",18,19
771,17679323,"More formally we define a Context-Free Grammar (CFG) for NL query as a 4-tuple G=(N, T, S, R) where • N is a set of non-terminals; • T is a set of terminals; • S∈N is a special non-terminal called start symbol, • R is a set of rules {A→β} where A is nonterminal and β is a string of symbols from the infinite set of strings of (N ∪T ) * .",57,58
772,17679323,"The sequence of =⇒ used to derive w from S is called a derivation of w. Here =⇒ * is defined as the reflexive transitive closure of =⇒. We can then formally define the language L(G) generated by the grammar G as the set of strings composed of terminal symbols that can be drawn from the designated start symbol S. L = {w|w is in T * and S=⇒ * w} Given the above definitions, parsing a string w means to find all (if any) the derivations of w from S. Our grammar composes a constraint ordering on queries.",62,63
773,6343829,"The entity mention is a subsequence of consecutive words in the question, where the relation pattern is the question where the mention is substituted by a special symbol.",28,29
774,6343829,"If an exact match was found, then the pattern would be derived by replacing the mention in the question with the special symbol.",23,24
775,6648997,"A reduced double-array trie The standard implementation of a double-array trie stores an (integer) index with a key at a child node (value node) traversed by a terminal symbol '\0' (or an alphabet not included in a key, e.g., '#') from the node reached after reading the entire key (Yoshinaga and Kitsuregawa, 2009; Yasuhara et al.,",36,37
776,16213906,"Introduction Hashtags, which are usually prefixed with the symbol # in microblogging services, represent the relevance of a tweet to a particular group, or a particular topic (Kwak et al.,",9,10
777,14326448,The symbol † is used to represent that the difference in F 1 -score from the best-performing system is statistically significant (p < 0.01).,1,2
778,14326448,The symbol † indicates that the F 1 -score is significantly lower than that achieved by the proposed system (p < 0.01).,1,2
779,12854977,is the separate symbol.,3,4
780,225067232,"As described in § 2.1, when training the language models, a special control symbol </s> is inserted at every sentence boundary.",15,16
781,16637008,Another common error is that the % symbol is made compatible with a wide variety of common nouns in the financial domain.,7,8
782,153313581,"Suppose we have K different datasets D i , then we can define K different styles, and we denote each style by the symbol s (i) .",24,25
783,241583423,"We split each corpus into sentences with a sentence tokenizer 7 , and normalize repetitions of symbols 8 to a single symbol (e.g., from +++ to +).",21,22
784,1994218,"Each span corresponds to a word position and contains several arcs, each of which represents an alternative word (could be the empty symbol , ) at that position.",24,25
785,820218,"By assuming that the length of sentence is n, we will have recursion layer l ∈ [1, ⌈log n 2 ⌉+1], where symbol ⌈q⌉ indicates the minimal integer q * ≥ q. At each recursion layer l, the activation of the j- th (j ∈ [0, 2 ⌈log n 2 ⌉−l )) hidden node h (l) j ∈ R d is computed as h (l) j = { zN ⊙ ĥl j + zL ⊙ h l−1 2j + zR ⊙ h l−1 2j+1 , l > 1, corresponding word embedding, l = 1, (2) where z N , z L and z R ∈ R d are update gates for new activation ĥl j , left child node h l−1 2j and right child node h l−1 2j+1 respectively, and ⊙ indicates element-wise multiplication.",27,28
786,5816303,"Unless otherwise specified, the character dictionary is extracted from the training set and unknown characters are mapped to a special symbol that is not used elsewhere.",21,22
787,6764076,"The activation of the hidden state h t at time-step t is computed as a function f of the current input symbol x t and the previous hidden state h t−1 h t = { 0 t = 0 f (h t−1 , x t ) otherwise (1) It is common to use the state-to-state transition function f as the composition of an element-wise nonlinearity with an affine transformation of both x t and h t−1 .",23,24
788,7735409,The symbol indicates a sentence boundary.,1,2
789,2307240,Each forest node consists of a grammar symbol and target language boundary words used to track n-grams.,7,8
790,8886886,"Trying to explain why treating documents as symbol sequences and using string kernels led to such good results the authors suppose that: ""the [string] kernel is performing something similar to stemming, hence providing semantic links between words that the word kernel must view as distinct"".",7,8
791,8886886,"Definition 1 Let S 1 , S 2 ∈ Σ * be two strings with symbols (n-grams) from the alphabet Σ. Local Rank Distance between S 1 and S 2 is defined as: ∆ LRD (S 1 , S 2 ) = ∆ lef t + ∆ right = xs∈S 1 min xs∈S 2 {|pos S 1 (x s ) − pos S 2 (x s )|, m} + + ys∈S 2 min ys∈S 1 {|pos S 1 (y s ) − pos S 2 (y s )|, m}, where x s and y s are occurrences of symbol s ∈ Σ in strings S 1 and S 2 , pos S (x s ) represents the position (or the index) of the occurrence x s of symbol s ∈ Σ in string S, and m ≥ 1 is the maximum offset.",117,118
792,8886886,"Definition 1 Let S 1 , S 2 ∈ Σ * be two strings with symbols (n-grams) from the alphabet Σ. Local Rank Distance between S 1 and S 2 is defined as: ∆ LRD (S 1 , S 2 ) = ∆ lef t + ∆ right = xs∈S 1 min xs∈S 2 {|pos S 1 (x s ) − pos S 2 (x s )|, m} + + ys∈S 2 min ys∈S 1 {|pos S 1 (y s ) − pos S 2 (y s )|, m}, where x s and y s are occurrences of symbol s ∈ Σ in strings S 1 and S 2 , pos S (x s ) represents the position (or the index) of the occurrence x s of symbol s ∈ Σ in string S, and m ≥ 1 is the maximum offset.",149,150
793,8886886,"A string may contain multiple occurrences of a symbol s ∈ Σ. LRD matches each occurrence x s of symbol s ∈ Σ from a string, with the nearest occurrence of symbol s in the other string.",8,9
794,8886886,"A string may contain multiple occurrences of a symbol s ∈ Σ. LRD matches each occurrence x s of symbol s ∈ Σ from a string, with the nearest occurrence of symbol s in the other string.",19,20
795,8886886,"A string may contain multiple occurrences of a symbol s ∈ Σ. LRD matches each occurrence x s of symbol s ∈ Σ from a string, with the nearest occurrence of symbol s in the other string.",32,33
796,8886886,"A symbol can be defined either as a single character, or as a sequence of characters (n-grams).",1,2
797,8886886,"In other words, ∆ lef t considers every symbol from s 1 , while ∆ right considers every symbol from s 2 .",9,10
798,8886886,"In other words, ∆ lef t considers every symbol from s 1 , while ∆ right considers every symbol from s 2 .",19,20
799,5444502,"One is to decide what the right-hand terminals or non-terminals should be, and the other is to decide the left-hand symbol (LHS).",27,28
800,5444502,"Given an utterance U = t 0 t 1 …t n as input string of some linguistic token, e.g. part-of-speech tags, the unsupervised grammar induction looks for the substring with maximum description length decrease, i.e. maximum DLG, at each iteration and then replaces the n-gram (bi-gram or the tri-gram in our work) with a random symbol in the whole corpus, at the same time, output the learned rules in this iteration.",72,73
801,5444502,The only difference between them is the number of the right-hand symbol and the syntactic category of the left-hand symbol.,13,14
802,5444502,The only difference between them is the number of the right-hand symbol and the syntactic category of the left-hand symbol.,23,24
803,5154493,"To recognize ENEs in a news article, Mazii inputs each sentence of the article into the AL+ ENER API (sentence boundary detection in Japanese is very simple because Japanese language has a special symbol for sentence boundary mark).",35,36
804,212725611,"Sta n z a parses each sentence for its syntactic structure, where each word in the sentence is assigned a syntactic head that is either another word in the sentence, or in the case of the root word, an artificial root symbol.",44,45
805,14926846,"As well as long sound symbol substitution, some hiragana characters such as 'あ'(a), 'い'(i), 'う'(u), 'え'(e), 'お'(o), 'わ'(wa), and 'か'(ka) are substituted by their lowercases: 'ぁ,' 'ぃ,' 'ぅ,' 'ぇ,' 'ぉ,' 'ゎ,' and 'ヵ.'",5,6
806,14926846,"These rules substitute a long sound symbol with one of the vowel characters: 'あ,' 'い,' 'う,' 'え,' and 'お,' that minimizes the difference in pronunciation.",6,7
807,53099899,"13) Here, w is a character index in the output vocabulary, y 0 a special start-of-sequence symbol in the vocabulary, and h dec 0 the concatenation of the last hidden states of each direction of the encoder.",23,24
808,53099899,"During training, all embeddings are randomly replaced with a <drop> symbol with p = .33.",13,14
809,11914496,"The symbol ""a"" in the second frameset represents a type of syntactic alternation.",1,2
810,7786830,"To distinguish the argument in focus, we use a special symbol to indicate the corresponding frame item.",11,12
811,3047293,"As a preprocessing step, we replace hapax legomena (defined as words that appear once in our unlabeled training data) with the special symbol *UNKNOWN*, and do the same for words in the labeled test sets that never appeared in any of our unlabeled training text.",25,26
812,17554801,"The feature matrix F and the label vector l defined in Section 4.1 are used, but with instances from MI removed (we use ñ to denote the number of remaining training instances, and the same symbol F for feature matrix).",38,39
813,227905355,"The reply on the bottom includes a quote, which generally starts with a symbol "">"".",14,15
814,227905355,"From the dataset, we extract replies that contain quotes, which start with the symbol "">"".",15,16
815,8674440,"FreeParser uses a domain-independent architecture to automatically identify sentences relevant to each new database symbol, which it uses to supplement its manually-annotated training data from the training domain.",16,17
816,8674440,Every test example has a predicate symbol that has never been observed before in training.,6,7
817,231933962,"For instance, the theoretical insights underlying a number of certification approaches are intimately tied to symbol substitution (Jia et al.,",16,17
818,231933962,"An LSTM cell is a function, denoted LSTM, that takes as input a symbol x i and the previous hidden state and cell state, and outputs the hidden state and the cell state at the current time step.",15,16
819,231933962,It takes an interval of symbol embeddings and an interval of states.,5,6
820,231933962,"We rewrite Eq 13 by explicitly applying the transformations defining the perturbation space S, thus deriving our final equations: where a = j−s k +1 and b = j. Notation f k,:l (x a:b ) collects the first l symbols for each z in f k (x a:b ), i.e., G S i,j contains (1) strings whose suffix is perturbed by T k = (ϕ k , f k ) and the last symbol of z is the lth symbol of the output of T k (the first line of Eq 14), and (2) strings whose suffix (the last character) is not perturbed by any transformations (the second line of Eq 14).",88,89
821,231933962,"We rewrite Eq 13 by explicitly applying the transformations defining the perturbation space S, thus deriving our final equations: where a = j−s k +1 and b = j. Notation f k,:l (x a:b ) collects the first l symbols for each z in f k (x a:b ), i.e., G S i,j contains (1) strings whose suffix is perturbed by T k = (ϕ k , f k ) and the last symbol of z is the lth symbol of the output of T k (the first line of Eq 14), and (2) strings whose suffix (the last character) is not perturbed by any transformations (the second line of Eq 14).",94,95
822,13599554,"D j correct=Y D j correct=N D i correct=Y a b D i correct=N c d Table 2: D i & D j Contingency table The correlation coefficient (ρ) is defined as: ρ ij = ad − bc (a + b)(c + d)(a + c)(b + d) 5 The symbol †indicates a statistically significant result using Evaluation and Discussion Tables 3, 4 , 5, 6 show the classification results for W ebKB 4 , W ebKB 6 , LingSpam, Reuter, and 20N ewsgroups respectively.",63,64
823,227905430,"In an instance x, y , the source part x is a linearized sequence representing a sub-graph, and the target part y is a label indicating an entity, a relation or a boolean symbol.",38,39
824,227905430,"The source part of a negative instance can be generated by replacing a random component in the tuple with a random symbol of the same type, to satisfy the condition that the changed tuple is not equal to or inside of a triple in the KG.",21,22
825,227905430,We add a specific symbol indicating the learning view at the beginning of the source part of the instance.,4,5
826,237513833,"q ij = 1{ŷ u i = ŷu j }, (8) where the symbol q ij denotes whether s u i and s u j belong to the same cluster.",15,16
827,10738010,"Due to space constraints, we did not reported the complete set of results of the competition, however, these results can be seen at the task 10 description Table 3 : Results obtained at the Task 10 of the Semeval competition for the Spanish language (NOTE: The * symbol denotes a system that used Wikipedia to build its model for the Wikipedia test dataset) Cross-Level Semantic Similarity This task aims to find semantic similarity between a pair of texts of different length written in English language, actually each text belong to a different level of representation of language (para-graph, sentence, phrase, word, and sense).",52,53
828,227905400,"Formally, given a KG G and a piece of text T , it assigns each mention m ∈ T with an entity e ∈ G indicating that m refers to e, or with the symbol φ indicating that there is no corresponding entity.",36,37
829,248780282,The decision space is large even in the simplest situation that each node consists of consecutive words (a span in the sentence without any symbol or placeholder outside the sentence).,25,26
830,248780282,"If a continuous sequence of relation symbols exists in the graph, one can merge them into one new relational symbol.",20,21
831,248780282,"If some nodes have prefix/suffix symbols, one can add a boolean attribute to the node indicating that this node has a prefix/suffix symbol.",27,28
832,248780282,"If a node is a sequence of placeholders, one can design a new relational symbol to replace the sequence of placeholders.",15,16
833,248780282,"When processing the Parataxis, these arguments are chained by label ""parataxis"", and the n is omitted since the order is naturally embedded in the chain; • Relational Symbols Nested Up to Two Layers Nested Relational symbol is the most complex situation for GPG.",40,41
834,248780282,"In OIA, only one symbol Parataxis can be nested.",5,6
835,16777049,"Because there are many extremely long sentences in the original PD corpus, we first split them into normal sentences according to the full-stop punctuation symbol.",27,28
836,2822831,"1 (c) the symbol ε denotes a null word, which is inserted by the alignment normalization algorithm described in section 3.4.",5,6
837,5453533,"Sentences with Dependency Tags For training, we first parse (Klein and Manning, 2002) the English sentences and feed sequences with dependency tags as follows ( I like ( red apple ) ) to genCNN in training, where 1) each paired parentheses contain a subtree, and 2) the symbol "" "" indicates that the word next to it is the dependency head in the corresponding subtree.",55,56
838,235293847,SUPPORTED [Moscovium] Moscovium is a superheavy synthetic element with symbol Mc and atomic number 115.,11,12
839,220047695,"input sentence with the greedy search strategy by replacing each word, one at a time, with a special ""unknown"" symbol (<unk>), and examining the changes in unlabeled attachment score (UAS) like (Yang et al.,",23,24
840,220047695,"Output: an adversarial example x of x. Algorithm: 1: κ = γ •n (the maximum number of words to be modified) 2: for each word xi except those listed in the constraint (iii) 3: xi = replace xi with a special symbol ""<unk>"" in x; 4: calculate the unlabeled attachment score of f (xi).",50,51
841,15908485,The symbol in the Word column means that words in a query are handled as necessary terms.,1,2
842,15908485,The symbol in the Word column means that words in a query are handled as optional terms.,1,2
843,1131623,"Here, the symbol f t (s [p], p, ART) refers to the condence score of the article classier at position p, and s [p] is the word at position p of s .",3,4
844,2686191,"Without confusion, we adopt the symbol β(i, j) to denote the count of binary tree for span i, j : β(i, j) =          1 i = j j−1 k=i β(i, k) • β(k + 1, j) i < j (2) β(1, |e|) is the count of binary trees of target sentence e. We also need to calculate the count of binary tree fragments that cover the nodes outside span i, j .",6,7
845,2686191,"We also adopt the symbol α(i, j) here: α(i, j) =                      1 i = 1, j = |e| |e| k=j+1 α(i, k) • β(k + 1, |e|) + i−1 k=1 α(k, j) • β(k, j − 1) else (3) For simplicity we omit some conditions in above formulas.",4,5
846,235166497,"After preprocessing (i.e., lowercasing, replacing all the digits with the symbol digit and removing the duplicated data), the final KP20k dataset contains 509,818 samples for training, 20,000 for validation, and 20,000 for testing.",13,14
847,6866394,"We use w to denote the parameter vector of the ME model, and f (i, j, r) to denote the feature vector for the assumption that the word pair i and j has a dependency relationship r. The symbol r indicates the supposed classification result, where r = + means we suppose it as a dependency edge and r = − means the contrary.",43,44
848,13527410,"The candidate constituent set of this production consists the head of the production itself, and a set of incomplete constituents, { l, r, p • nt * |c 1 • lb ≤ l ≤ c h • lb∧ c h • rb ≤ r ≤ c |p| • rb∧ (l < c h • lb ∨ r > c h • rb)} where the symbol * indicates an incomplete non-terminal.",71,72
849,235432084,"However, if the number symbol v 5 = 20%, the target expression for the same problem would be ""v 4 /(v 2 − v 3 ) * (1 + v 5 )"".",5,6
850,235432084,"It can also generate a number symbol in V c , and use it to copy a number from the problem X. The final distribution is the combination of the generated probability and copy probability: H v = {h v k } K k=1 , p c = σ(W z [s t : c t : r t ] + W v H v ), P c (y t ) = softmax(f ([s t : c t : r t : H v ])), P g (y t ) = softmax(f ([s t : c t : r t ])), P(y t |y <t ,X) = p c P c (y t ) + (1−p c )P g (y t ). (",6,7
851,13728350,"Better eos translation As a special symbol marking the end of sentence, target side eos has a critical impact on controlling the length of the generated translation.",6,7
852,216869436,We mask the contiguity loss such that the end of sentence symbol is not considered during this procedure.,11,12
853,8315726,"HPB Translation Model: an Overview In HPB models (Chiang, 2007) , synchronous rules take the form X → γ, α, ∼ , where X is the nonterminal symbol, γ and α are strings of lexical items and non-terminals in the source and target side, respectively, and ∼ indicates the one-to-one correspondence between non-terminals in γ and α.",33,34
854,61982581,"Figure 1 (b) shows an example of converted meaning representation, where each token is in the format of A@B where A is the symbol while B is either s indicating that the symbol is a string or a number indicating the symbol's arity (constants, including strings, are treated as zeroargument functions).",26,27
855,61982581,"Figure 1 (b) shows an example of converted meaning representation, where each token is in the format of A@B where A is the symbol while B is either s indicating that the symbol is a string or a number indicating the symbol's arity (constants, including strings, are treated as zeroargument functions).",35,36
856,61982581,"Figure 1 (b) shows an example of converted meaning representation, where each token is in the format of A@B where A is the symbol while B is either s indicating that the symbol is a string or a number indicating the symbol's arity (constants, including strings, are treated as zeroargument functions).",44,45
857,61982581,This can be easily done by examining each symbol's arity.,8,9
858,61982581,"Enriched SCFG In hierarchical phrase-based (HPB) translation models, synchronous rules take the form X → γ, α, ∼ , where X is the non-terminal symbol, γ and α are strings of lexical items and non-terminals in the source and target side respectively, and ∼ indicates the one-to-one cor-respondence between non-terminals in γ and α.",33,34
859,61982581,"From an aligned phrase pair <state that border, state@1 next to 2@1> in Figure 2 (a), for example, we can get a synchronous rule X → state X 1 , state@1 X 1 , where we use boxed indices to indicate which nonterminal occurrences are linked by ∼. The fact that SCFGs in HPB models contain only one type of non-terminal symbol 2 is responsible for ill-formed translation (e.g., answer@1 state@1).",70,71
860,61982581,"Given a word sequence e i j from position i to position j in MRL , we enrich the non-terminal symbol X to reflect the internal structure of the word sequence of e i j .",22,23
861,61982581,We use symbol C to label word sequences which are complete.,2,3
862,1987772,"The # symbol, called a hashtag, is usually used to mark keywords or topics in a microblog.",2,3
863,523485,"HPB Translation Model In HPB models, synchronous rules take the form X → γ, α, ∼ , where X is the non-terminal symbol, γ and α are strings of lexical items and nonterminals in the source and target side, respectively, and ∼ indicates the one-to-one correspondence between non-terminals in γ and α.",27,28
864,5971144,"Due to lack of linguistic knowledge, Chiang's HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.",17,18
865,5971144,"In addition, once a 1 Another non-terminal symbol S is used in glue rules.",10,11
866,5971144,"Instead of collapsing all non-terminals in the source language into a single symbol X as in Chiang (2007) , given a word sequence f i j from position i to position j, we first find heads and then concatenate the POS tags of these heads as f i j 's non-terminal symbol.",14,15
867,5971144,"Instead of collapsing all non-terminals in the source language into a single symbol X as in Chiang (2007) , given a word sequence f i j from position i to position j, we first find heads and then concatenate the POS tags of these heads as f i j 's non-terminal symbol.",58,59
868,5971144,"The full rule table size (including HD-HRs and NRRs) of our HD-HPB model is ˜1.5 times that of Chiang's, largely due to refining the non-terminal symbol X in Chiang's model into head-informed ones in our model.",35,36
869,3230806,"In addition to the limited number of characters in the content, microblogs also contain a form of metadata tag (hashtag), which is a string of characters preceded by the symbol (#).",33,34
870,1605812,"However, due to lack of linguistic knowledge, Chiang's HPB model contains only one type of non-terminal symbol X, often making it difficult to select the most appropriate translation rules.",21,22
871,1605812,"By contrast, our HD-HPB model refines the nonterminal symbol X with syntactic head information and provides flexible reordering rules, including swap, which can mix freely with hierarchical translation rules for better interleaving of translation and reordering in translation derivations.",11,12
872,1605812,"Instead of collapsing all non-terminals in the source language into a single symbol X as in Chiang (2007) , ideally non-terminals should capture important information of the word sequences they cover to be able to properly discriminate between similar and different word sequences during translation.",14,15
873,1605812,"Given a word sequence f i j from position i to position j, we refine the non-terminal symbol X to reflect some of the internal syntactic structure of the word sequence covered by X. A correct translation rule selection therefore not only maps terminals into terminals, but is both constrained and guided by syntactic information in the non-terminals.",20,21
874,1605812,"As a compromise, given a word sequence f i j , we first find heads and then concatenate the POS tags of these heads as f i j 's non-terminal symbol.",33,34
875,1605812,"The full rule table size (including HD-HRs and NRRs) of our HD-HPB model is about 1.5 times that of Chiang's, largely due to refining the non-terminal symbol X in Chiang's model into head-informed in our model.",36,37
876,1605812,Our model maintains the strengths of Chiang's HPB model while at the same time it addresses the over-generation problem caused by using a uniform non-terminal symbol.,30,31
877,1605812,"For example, for a non-terminal symbol VV, we believe it will benefit translation if we use fine-grained dependency labels (subject, object etc.)",8,9
878,12444004,"Compared to a baseline that replaces all rare words with an unknown word symbol, our best variable-length encoding strategy improves WMT English-French translation performance by up to 1.7 BLEU.",13,14
879,12444004,All of them eliminate the need to replace rare words with the unknown word symbol.,14,15
880,12444004,"Nonetheless, compared to a baseline system that replaces all rare words with an unknown word symbol, our encoding approach improves English-French news translation by up to 1.7 BLEU.",16,17
881,12444004,"As a result, neural machine translation systems only consider the top 30K to 100K most frequent words in a training corpus, replacing the other words with an unknown word symbol.",33,34
882,12444004,"An encoding can be represented as a tree in which each leaf corresponds to an element of V, each node contains a symbol from W, and the encoding of any leaf is its path from the root.",23,24
883,12444004,"As stricter constraints are imposed on the encoding, the encoded corpus length increases and the number of elements of V that can be represented using a single symbol decreases.",28,29
884,12444004,The variants we consider are designed to prevent specific forms of symbol sharing across encodings.,11,12
885,12444004,"In our experiments with V ≈ 2 • 10 6 , W = 3 • 10 4 , and frequencies drawn from the WMT corpus, all words in V are encoded as either a single symbol or two symbols of W. We denote the single-symbol words (which have the highest frequency) as common, and we call the other words rare.",36,37
886,12444004,"In our experiments with V ≈ 2 • 10 6 , W = 3 • 10 4 , and frequencies drawn from the WMT corpus, all words in V are encoded as either a single symbol or two symbols of W. We denote the single-symbol words (which have the highest frequency) as common, and we call the other words rare.",47,48
887,12444004,"Rare words are represented by two words, and the first is always a pseudo-word symbol introduced into W of the form sX for an integer X. Repeat-Symbol.",17,18
888,12444004,"Instead, each rare word is encoded as a two-symbol sequence of the form ""sX sY,"" where X and Y are integers that may be the same or different.",11,12
889,12444004,"In this scheme, common words and rare words do not share symbols, and each symbol can immediately be identified as common, the first of a rare encoding pair, or the second of a rare encoding pair.",16,17
890,12444004,"To do so, for each new symbol (non-terminal node in our encoding tree), we have all W symbols under it in that branch.",7,8
891,12444004,"For the remaining rare words, we encoded them using a distinct symbol whose form matched the one prescribed for each encoding scheme.",12,13
892,12444004,"Thus, rare words of similar frequency in the training corpus tended to have encodings with the same first symbol.",19,20
893,12444004,"Conclusion and Future Work We described a novel approach for encoding the source and target text based on Huffman coding schemes, eliminating the use of the unknown word symbol.",29,30
894,12444004,"An important continuation of our work would be to develop heuristics for effectively grouping ""similar"" words in the source and target text, so that they tend to have encodings that share a symbol.",35,36
895,195063921,"Specifically, it splits every symbol into two, and then re-merge some new subcategories which cause little or less loss in likelihood incurred when removing it.",5,6
896,16745211,"In addition to the limited number of characters in the content, microblogs also contain a form of metadata tag (hashtag), which is a string of characters preceded by the symbol (#).",33,34
897,15940597,"For example, the symbol NP might be split into the subsymbol NPˆS in subject position and the subsymbol NPˆVP in object position.",4,5
898,18543034,A hashtag is a string of characters preceded by the symbol #.,10,11
899,237485343,"After preprocessing (i.e., lowercasing, replacing all the digits with the symbol digit and removing the duplicated data), the final KP20k dataset contains 509,818 samples for training, 20,000 for validation and 20,000 for testing.",13,14
900,53082905,"Encoding Layer The input of natural language inference task is a pair of sentences x and y. Since each word in a sentence is a symbol that can not be directly processed by neural networks, we need first map each word to a d dimensional embedding vector.",25,26
901,218930,"For example consider a grammar with only two ID rules given in ( 5 ) and consider S as the start symbol: (5) S → B, c B → d, e It can be easily verified that dec is a sentence of the language but dce is not.",21,22
902,218930,"N is a set of non-terminals; • T is a set of terminals; • S ∈ N is a special non-terminal called start symbol, • R is a set of rules {A i → X j } where A i is a non-terminal and X j is a set of terminals and non-terminals.",29,30
903,218930,"The algorithm succeeds if there is a constituent in the chart that corresponds to the start symbol and covers all the words in the query, i.e. there is a constituent of the form (S, {1,2,….n}) in the chart.",16,17
904,36396995,"We present a new approach to the design of deep networks for natural language processing (NLP), based on the general technique of Tensor Product Representations (TPRs) for encoding and processing symbol structures in distributed neural networks.",35,36
905,36396995,TPRs are a general method for generating vector-space embeddings of complex symbol structures.,13,14
906,36396995,"Prior work has proved that TPRs enable powerful symbol processing to be carried out using neural network computation (Smolensky, 2012) .",8,9
907,36396995,Then Equation 1 becomes v Jay saw Kay = Jr SUBJ + Kr OBJ + sr VERB (2) Note that the set of matrices (or the set of tensors with any fixed number of indices) is a vector space; thus Jay saw Kay → v Jay saw Kay is a vector-space embedding of the symbol structures constituting sentences.,61,62
908,36396995,"Just as an outer product -the tensor product -can be used to bind the vector embedding a filler Jay to the vector embedding a role SUBJ, J ⊗ r SUBJ or Jr SUBJ , so an inner product can be used to take the vector embedding a structure and unbind a role contained within that structure, yielding the symbol that fills the role.",60,61
909,36396995,"x 0 is initialized as the one-hot vector corresponding to a ""start-of-sentence"" symbol.",20,21
910,36396995,"Conclusion Tensor Product Representation (TPR) (Smolensky, 1990 ) is a general technique for constructing vector embeddings of complex symbol structures in such a way that powerful symbolic functions can be computed using hand-designed neural network computation.",22,23
911,8665208,This forces the Viterbi path to go through t. We only tag the final boundary symbol for goal hypotheses.,15,16
912,18297290,"Therefore, a variety of techniques have been developed to enrich and generalize the original grammar, ranging from lexicalization to symbol annotation.",21,22
913,18297290,"Then, some manual and automatic symbol splitting methods are presented, which get comparable performance with lexicalized parsers (Klein and Manning, 2003; Matsuzaki et al.,",6,7
914,18297290,"That is, it splits every symbol into two, and then re-merges some new subcategories based on the likelihood computation.",6,7
915,18297290,"Splitting In each splitting stage, the previous syntactic symbol is split into two subcategories, and the EM algorithm is adopted to learn probability of the rules for these latent annotations to maximize the likelihood of trees in the training data.",9,10
916,18297290,"Finally, each symbol generates a series of new subcategories in a hierarchical fashion.",3,4
917,18297290,The symbol merging stage is introduced to alleviate this defect.,1,2
918,18297290,"This approach splits symbols only where needed, and it is implemented by splitting each symbol first and then measure the loss in likelihood incurred when removing this subcategory.",15,16
919,9694800,We define phrasal TTS templates as those with more than one symbol (word or non-terminal) in their LHS.,11,12
920,15016194,"P u(•) is a boolean function that checks whether a character is a punctuation symbol (returns 1 for a punctuation, 0 for not).",15,16
921,20298653,"We feed the predicted word as input at the next timestep back into the decoder until we predict the end symbol, END, after which we stop decoding.",20,21
922,798995,The symbol denotes a set cross-product.,1,2
923,2116604,"All the words in the sentence are sequentially generated using the RNN, until the end-of-the-sentence symbol is generated.",22,23
924,16749512,"This context-as-symbol augmentation of the grammar is similar in character to augmenting symbols with lexical items to score language models during hierarchical decoding (Chiang, 2007) .",5,6
925,16749512,"That is, ITG inference is organized around search states associated with a grammar symbol and a bispan; augmenting grammar symbols also augments this state space.",14,15
926,13951982,"We show how thoughtful binarization can further increase parsing speed, and we present a new coarse-to-fine scheme that uses rule subsets rather than symbol clustering to build a coarse grammar projection.",28,29
927,13951982,"Each n-ary rule consists of a root symbol, a sequence of lexical items and non-terminals on the source-side, and a fragment of a syntax tree on the target side.",9,10
928,13951982,Aligned non-terminals share a grammar symbol derived from a target-side monolingual grammar.,7,8
929,13951982,"The grammar includes 149 grammatical symbols, an augmentation of the Penn Treebank symbol set.",13,14
930,13951982,X n with the intermediate symbol X 1 +. .,5,6
931,13951982,"This symbol set must at least include the left-hand side of every rule in the grammar (lexical and non-lexical), including the intermediate The number of non-terminal symbols introduced to the grammar through LNF binarization depends upon the policy for binarizing type sequences.",1,2
932,13951982,To ensure that a symbol sequence X 1 . . .,4,5
933,13951982,We introduce an indicator variable T Y for each symbol Y ∈ V + to indicate that Y is used in the grammar.,9,10
934,13951982,Y can be either a base non-terminal symbol X i or an intermediate symbol X 1:n .,9,10
935,13951982,Y can be either a base non-terminal symbol X i or an intermediate symbol X 1:n .,15,16
936,13951982,Let R be the set of symbol sequences on the right-hand side of all non-lexical rules.,6,7
937,13951982,"Equation 3 does not require that a symbol represent the entire right-hand side of each non-lexical rule, but does ensure that each right-hand side sequence can be built from two subsequence symbols.",7,8
938,13951982,"Because greedy requires very little time to compute and generates symbol counts that are close to optimal when both can be computed, we use it for our remaining experiments.",10,11
939,13951982,"The dynamic program is organized into spans S ij and computes the Viterbi score w(i, j, X) for each edge S ij [X], the weight of the maximum parse over words i+1 to j, rooted at symbol X. For each S ij , computation proceeds in three phases: binary, lexical, and unary.",43,44
940,13951982,"While this definition is recursive, we allow only one unary rule application per symbol X at each span to prevent infinite derivations.",14,15
941,13951982,"We track the following statistics as we parse: min END (i, X), max END (i, X): The minimum and maximum position k for which symbol X was successfully built over S ik .",32,33
942,13951982,"min START (j, X), max START (j, X): The minimum and maximum position k for which symbol X was successfully built over S kj .",23,24
943,13951982,"We collapse all intermediate non-terminal grammar symbols (e.g., NP) to a single coarse symbol X, while pre-terminal symbols (e.g., NN) are hand-clustered into 7 classes (nouns, verbals, adjectives, punctuation, etc.).",18,19
944,13951982,"We then project the rules of the original grammar into this simplified symbol set, weighting each rule of the coarse grammar by the maximum weight of any rule that mapped onto it.",12,13
945,13951982,We use a voting scheme to select the symbol subset.,8,9
946,13951982,We select the top n u symbol sequences as the set R of right-hand sides.,6,7
947,13951982,"Finally, we augment the symbol set of C with intermediate symbols that can construct all sequences in R, using only binary rules.",5,6
948,13951982,"Table 2 shows that both methods of constructing a coarse grammar are effective in pruning, but selecting symbol subsets outperformed the more typical clustering approach, reducing parsing time by an additional factor of 2.",18,19
949,13951982,Coarse-to-fine parsing included a coarse pruning pass using a symbol subset grammar.,13,14
950,13951982,"By pruning the chart with max marginals from the coarse symbol subset grammar from Section 5, we were able to rerank with beams of length 200, leading to a 0.8 BLEU increase and a 31% reduction in total decoding time.",10,11
951,13951982,"Our grammar normal form, CKY improvements, and symbol subset coarse-to-fine procedure reduced parsing time for large transducer grammars by 81%.",9,10
952,1664713,Replace all reported speech verbs with a symbol.,7,8
953,3541819,"The nodes are states in the decoding process that include the span (i, j) of the sentence to be translated, the grammar symbol s over that span, and the left and right context words of the translation relevant for computing n-gram language model scores.",26,27
954,3541819,"The grammar rules of Hiero all share a single non-terminal symbol X, and have at most two non-terminals and six total items (non-terminals and lexical items), for example: my X 2 's X 1 → X 1 de mi X 2 We extracted the grammar from training data using standard parameters.",12,13
955,1319915,"The resulting grammar, which does not handle null alignments, consists of a symbol N to represent a bitext cell produced by a normal rule and I for a cell formed by an inverted rule; alignment terminals can be either N or I. In order to ensure unique derivations, we stipulate that a N cell can be constructed only from a sequence of smaller inverted cells I. Binarizing the rule N → I 2+ introduces the intermediary symbol N (see Figure 2 Wu (1997) and Zens and Ney (2003) ).",14,15
956,1319915,"The resulting grammar, which does not handle null alignments, consists of a symbol N to represent a bitext cell produced by a normal rule and I for a cell formed by an inverted rule; alignment terminals can be either N or I. In order to ensure unique derivations, we stipulate that a N cell can be constructed only from a sequence of smaller inverted cells I. Binarizing the rule N → I 2+ introduces the intermediary symbol N (see Figure 2 Wu (1997) and Zens and Ney (2003) ).",81,82
957,1319915,The N 00 symbol is constructed by one or more 'complete' inverted cells I 11 terminated by a no-null I 00 .,3,4
958,1319915,"N 00 transitions to the N 10 symbol and accepts any number of e, • English terminal alignments.",7,8
959,5237589,"Each forest node P ij compactly encodes all parse derivations rooted by grammar symbol P and spanning the source sentence from positions i to j. Each derivation of P ij is rooted by a rule with non-terminals that each anchor to some child node C (t) k , where the symbol C (t) is the tth child in the source side of the rule, and i ≤ k < ≤ j. We build this forest with a CKY-style algorithm.",13,14
960,5237589,"Each forest node P ij compactly encodes all parse derivations rooted by grammar symbol P and spanning the source sentence from positions i to j. Each derivation of P ij is rooted by a rule with non-terminals that each anchor to some child node C (t) k , where the symbol C (t) is the tth child in the source side of the rule, and i ≤ k < ≤ j. We build this forest with a CKY-style algorithm.",54,55
961,5237589,"For each span (i, j) from small to large, and each symbol P , we iterate over all ways of building a node P ij , first considering all grammar rules with parent symbol P and then, for each rule, considering all ways of anchoring its non-terminals to existing forest nodes.",15,16
962,5237589,"For each span (i, j) from small to large, and each symbol P , we iterate over all ways of building a node P ij , first considering all grammar rules with parent symbol P and then, for each rule, considering all ways of anchoring its non-terminals to existing forest nodes.",37,38
963,5237589,We construct a new symbol B and two new rules: [[PRP1 NN2] VBD3] PP4 PRP1 VBD3 NN2 PP4 S !,4,5
964,5237589,"For the new symbol B, we introduce a new forest node B that does not correspond to any particular span of the source sentence.",3,4
965,1379461,"p clc1 = max l −k≤j−1,j≤k log p(w t+j |w t ) (8) For example, if we indicate v as a function that maps one part of a loanword w to its n-dimensional vector representation, then v(""jinping"") − v(""shi"") + v(""zEmin"") ≈ v(""jyang"") (9) The symbol ≈ means its right hand side must be the nearest neighbor of the value of the left hand side.",60,61
966,5213476,"In order to make the input character sequence long enough to fill up the receptive fields of multiple CNN layers, we pad each predicate or entity using three padding symbols P , a special start symbol, and a special end symbol.",36,37
967,5213476,"In order to make the input character sequence long enough to fill up the receptive fields of multiple CNN layers, we pad each predicate or entity using three padding symbols P , a special start symbol, and a special end symbol.",42,43
968,119887,"Definition 3.1 (Lexicalized Tree Adjoining Grammar (LTAG)) Lexicalized Tree Adjoining Grammar £ 2 is a quintuplet ¡ 32 ¢¡ 3( 7 ¤£ ¤¥ § where and 2 ¦¡ are a finite set of terminal symbols and a finite set of nonterminal symbols respectively, ( is a distinguished nonterminal symbol called the start symbol, and £ and ¥ are a finite set of initial trees and a finite set of auxiliary trees respectively.",56,57
969,119887,"Definition 3.1 (Lexicalized Tree Adjoining Grammar (LTAG)) Lexicalized Tree Adjoining Grammar £ 2 is a quintuplet ¡ 32 ¢¡ 3( 7 ¤£ ¤¥ § where and 2 ¦¡ are a finite set of terminal symbols and a finite set of nonterminal symbols respectively, ( is a distinguished nonterminal symbol called the start symbol, and £ and ¥ are a finite set of initial trees and a finite set of auxiliary trees respectively.",60,61
970,119887,"The symbol of one leaf node in an auxiliary tree ¥ is identical to that of its root node, and is specially marked for a foot node.",1,2
971,119887,"Finally, we give the definition of strong equivalence between two given grammars In what follows, we assume that structural descriptions of LTAG are derivation trees in which the root node of § #i is labeled by the start symbol S in the definition 3.2.",41,42
972,119887,"Let § ( be a subtree except that a node labeled by 2 ¦¡ © ( is added to its root node, and let § be a supertree, except that the symbol of y is relabeled by the symbol c2 ¦¡ and by marking it for substitution as shown in Figure 1 .",34,35
973,119887,"Let § ( be a subtree except that a node labeled by 2 ¦¡ © ( is added to its root node, and let § be a supertree, except that the symbol of y is relabeled by the symbol c2 ¦¡ and by marking it for substitution as shown in Figure 1 .",41,42
974,119887,"Since we assume that a derivation tree is rooted by an elementary tree whose symbol of the root node is ( , every occurrence of § h in B C must always accompany with § h and vice versa.",14,15
975,119887,"8 is denoted as follows: 8 a ¡ #"" %$ ¡ #"" E '& '( & H ¤ § ¡ )"" "" '& "" ( "" ¤H 4"" § § where Q ¦£ ¥ , "" %$ is the symbol of the mother node of the anchor in § 8 , and "" q © 2 ¦¡ 5 '& q © 2 ¢¡ 5 ( q % 10 32 54 76 ¢8 6 %9 A@ )B 8 S H q % DC S are values of Sym, Leaf, Dir, Foot?",50,51
976,119887,"We define the origination of the feature in 8 as © R 4 § 8 , which indicates that the value of the feature originates from the symbol of a node with address in § A8 .",27,28
977,119887,"Since the substitution and adjunction rules preserve the order of the elements in the Arg feature of 8 , substitution rules always unify the symbol of the substitution node with the symbol of the root node of § #8 c , which represents the same constraint as the one on which substitution imposes.",24,25
978,119887,"Since the substitution and adjunction rules preserve the order of the elements in the Arg feature of 8 , substitution rules always unify the symbol of the substitution node with the symbol of the root node of § #8 c , which represents the same constraint as the one on which substitution imposes.",31,32
979,15155369,"The null inflectional category does not interact with the rest of the derivation, much like a punctuation symbol.",18,19
980,9084229,An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ¥).,13,14
981,9084229,"In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).",13,14
982,9084229,Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4 ).,24,25
983,9084229,A feature Sym represents the non-terminal symbol of the mother node.,8,9
984,9084229,"represent the leaf node; the non-terminal symbol, the direction (on which side of the trunk node the leaf node is), and the type (whether a foot node or a substitution node), respectively.",9,10
985,9084229,"The above algorithm gives the conversion of LTAG, and it can be easily extended to handle an FB-LTAG grammar by merely storing a feature structure of each node into the Sym feature and Leaf feature together with the non-terminal symbol.",44,45
986,2180008,"m n ∈ R e , with m 0 reserved for the ROOT symbol.",13,14
987,18309765,"For instance, one of our constructions maps the question to a pattern by replacing the entity mention with a generic symbol <e> and then compares it with a candidate chain, such as ""who first voiced meg on <e>"" vs. cast-actor.",21,22
988,18309765,"For example, the bag of letter-trigrams of the word ""who"" are #-w-h, w-h-o, h-o-# after adding the word boundary symbol #.",36,37
989,18309765,PatChain compares the pattern (replacing the topic entity with an entity symbol) and the predicate sequence.,12,13
990,7739040,"Trying to explain why treating documents as symbol sequences and using string kernels led to such good results the authors suppose that: ""the [string] kernel is performing something similar to stemming, hence providing semantic links between words that the word kernel must view as distinct"".",7,8
991,7739040,"One possible remedy to this problem would be to replace all proper names with a special string or symbol, solution adopted by others (Baroni and Bernardini, 2006) as well.",18,19
992,11754890,"For example, word cat is hashed as the bag of letter trigram {#-c-a, c-a-t, a-t-#}, where # is a boundary symbol.",36,37
993,207979868,"Single-encoder models take the preceding sentence(s) as the contexts in addition to the source sentence and concatenate them with a special symbol <CONC> (Tiedemann and Scherrer, 2017) .",24,25
994,8297449,"The atom can be a Chinese character, punctuation, symbol string, etc.",10,11
995,17456595,"2 ), the com-bination of each slot of semantic frame represents the user intention symbol.",17,18
996,17456595,"In this example, the state symbol is ""request+search_loc+[loc_name]"".",6,7
997,12563149,"The symbol true is a goal that always succeeds, and we omit, for brevity, the problem specific definition of pickLabel(Y), which would consist of rules for each possible label y i (relation types), i.e. pickLabel(y 1 ) ← true, . . . ,",1,2
998,3106120,"In this example, the state symbol is 'request×search loc×loc name'.",6,7
999,1239119,"To model the morpheme's connectability to one another, besides the morpheme dictionary, the separate morpheme-connectivity table encodes all the connectable pairs of morpheme groups using the morpheme's tag and morphotactic adjacency symbol patterns.",37,38
1000,1239119,"So the lexical probability e,-(td for unknown morphemes can be estimated using the frequency of syllable tri-gram products according to the following formula: m = ele2...e~ (11) (eilei-2, el-t) i=3 Pr(#1e,,-t, en) Pr(tlm) Pr(t) ~ Prt(ell#'#)Prt(e21#'el) n [I Prt (12) Prt ( eilei-2, ei-t ) .~ .ft(eilei-2, ei-1) +.h(eilei-) + fft(e{) ( 13 ) where ~m' is a morpheme, 'e' is a syllable, 't' is a POS tag, '#"" is a morpheme boundary symbol, and ft(eilei-~., el-l) is a frequency data for tag ~t' with cooccurrence syllables el-2, ei-1, ei.",112,113
1001,8690887,"For character-level features, a Chinese character may take on 3 different roles: word, morpheme, or alphabet symbol, where the extracted features are organized according to these roles.",22,23
1002,17919559,Non-contiguous n-grams are those intermediate grams that are replaced with a special symbol like '*'.,16,17
1003,52820513,"The POS tags are hierarchically organized symbols that were iteratively refined from the eight major grammatical categories of Korean: nominal, predicate, modifier, particle, ending, affix, special symbol, and interjection.",33,34
1004,52820513,"For a given morpheme, the acronym of a path name in the symbol hierarchy up to a certain level is assigned as a POS tag.",13,14
1005,52820513,"Besides the morpheme dictionary, to model morphemes' connectability to one another the system uses an independent morpheme connectivity table that encodes all the connectable pairs of morpheme groups using the morphemes' tags and morphotactic adjacency symbol patterns.",38,39
1006,52820513,"Nominals that have a D-ha as a morphotactic adjacency symbol can be connected with predicate particles, and they play the role of a verb or adjective.",11,12
1007,52820513,Eo is a morphotactic adjacency symbol for vowel harmony when connecting with endings.,5,6
1008,52820513,">"" is a special symbol for adjacent direction ("">""= right connection; ""<""= left connection).",5,6
1009,52820513,"The lexical probability Pr (t i |m i ) Pr (t i ) for unknown morphemes can be estimated using the frequency of syllable trigram products according to the formula in ( 11 )-( 13 ) (Nagata 1994) + f t (e i ) ( 13 ) where m is a morpheme, e is a syllable, t is a POS tag, ""#"" is a morpheme boundary symbol, and f t (e i | e i−2 , e i−1 ) is a frequency datum for tag t with co-occurrence syllables e i−2 , e i−1 , and e i .",78,79
1010,3038406,"is translated into "" ¸¹ º » ¼ j %½ ¾ j % ¿ À jp ef sf (%who auxiliary-verb %person preposition Yahoo Korea symbol)"".",30,31
1011,10826301,"System Configuration To handle the two major problems of previous NLIDB systems, i.e., 1) the discordance between attribute vocabulary and linguistic processing 2 Part-of-speech tag for a proper noun 3 ""@"" is the start symbol for the semantic tags.",43,44
1012,10826301,"4 ""%"" is the start symbol for the user-defined semantic classes.",7,8
1013,10826301,"In a lexico-semantic pattern, each semantic tag follows a ""@"" symbol.",15,16
1014,1575573,"For a particular database DB, we refer to its components as DB.S and DB.R. For a set of databases D, define the set of referents as S D = ( DB∈D DB.S) ∪ {OOD}, where OOD is a special symbol indicating something that is ""out of database"", or not found in any of the databases in D. Given a corpus C, a set of mentions M that occur in C, and a set of databases D, the Open-DB NED task is to produce a function f : M → S D , which identifies an appropriate target symbol from one of the databases in D, or determines that the mention is OOD.",45,46
1015,1575573,"For a particular database DB, we refer to its components as DB.S and DB.R. For a set of databases D, define the set of referents as S D = ( DB∈D DB.S) ∪ {OOD}, where OOD is a special symbol indicating something that is ""out of database"", or not found in any of the databases in D. Given a corpus C, a set of mentions M that occur in C, and a set of databases D, the Open-DB NED task is to produce a function f : M → S D , which identifies an appropriate target symbol from one of the databases in D, or determines that the mention is OOD.",111,112
1016,1575573,"Finally, we will assume the existence of a function µ(s, t) which indicates whether the text t is a valid surface form of database symbol s. Our experiments in Section 7.3 explore several possible simple definitions for this function.",27,28
1017,1575573,"k i } Define function f sim i,j (m, id): count ← 0 Identify the tuple t ∈ r i containing id val ← t j Identify the set of similar tuples T : attributes of the entity id appear near m. For example, if id is 5 in the movie relation in Figure 1 , the feature function for attribute year would count how often 2010 matches the text surrounding mention m. Defining precisely whether a database symbol ""matches"" a word or phrase is a subtle issue; we explore several possibilities in Section 7.3.",88,89
1018,1575573,"As before, the set of possible referents includes the symbol OOD, key values from the sports database, and Wikipedia articles, and a given mention may be labeled with both a sports entity and a Wikipedia article, if appropriate.",10,11
1019,1575573,"We experiment with two different matching strategies between a symbol s and text t, exact matching and partial matching.",9,10
1020,2308679,"We replace hapax legomena in the unlabeled data with the special symbol *UNKNOWN*, and also do the same for word types in the labeled test sets that never appear in our unlabeled training texts.",11,12
1021,201739018,"Given a sentence x ∈ X , we denote x \u:v as a modified version of x where its fragment from position u to v are masked, 0 < u < v < m and m is the number of tokens of sentence x. We denote k = v − u + 1 as the number of tokens being masked from position u to v. We replace each masked token by a special symbol [M], and the length of the masked sentence is not changed.",76,77
1022,9841638,"We also get rid of punctuation and the corresponding tags, change all words to lowercase and change all numbers into a single symbol.",23,24
1023,3118156,"However, to speed up training during our experiments and, in some cases, to avoid running out of memory, we replaced words appearing twice or fewer times in the data with the special symbol * UNKNOWN * .",36,37
1024,3118156,"In addition, all numbers were replaced with another special symbol.",10,11
1025,3118156,"For the LSA model, we had to use a more drastic cutoff to fit the singular value decomposition computation into memory: we replaced words appearing 10 times or fewer with the * UNKNOWN * symbol.",36,37
1026,226262289,"Given the input subtoken sequence, a certain portion of sub-tokens are replaced by a special symbol [M].",18,19
1027,2265838,"We High-Coverage Freebase PCCG Grammar and Lexicon Figure 1 : We reduce the task of learning a largescale semantic parser to a combination of 1) a standard supervised algorithm for learning semantic parsers; 2) our MATCHER algorithm for finding correspondences between words and database symbols; and 3) our LEXTENDER algorithm for integrating (word, database symbol) matches into a semantic parsing lexicon.",63,64
1028,36720539,"This means, twitter user names, e.g. @username, are stripped from their '@' symbol, but the user names themselves are preserved.",17,18
1029,202540640,"1 In order to distinguish the edge direction, we add a direction symbol to each label with ↑ for climbing up along the path, and ↓ for going down.",13,14
1030,202777662,Making similar perturbations to text for adversarial evaluation is more challenging due to the discrete symbol space: modifying a single token may change an instance's semantics or make it ungrammatical.,15,16
1031,235187330,"The original input article is the concatenation of several document paragraphs by the ""||"" symbol containing 1600 tokens in maximum.",16,17
1032,44071286,"The backbone of the decoder is an attention based GRU RNN, which generates a word at each time step and repeats the process until generating the end-of-sentence symbol.",32,33
1033,235376968,"The encoder side takes an input sequence of symbol representations (x 1 , ..., x n ) and outputs a sequence of continuous representations z = (z 1 , ..., z n ).",8,9
1034,52011663,The (−) symbol indicates that the respective feature type is excluded.,4,5
1035,218674345,"x im } be the subset of tokens in x selected to be masked out, and x denote the masked sequence with tokens in x m replaced by a [MASK] symbol.",34,35
1036,218674345,"| real Column Type | 2005 Cell Value (1) The linearization of a row is then formed by concatenating the above string encodings of all the cells, separated by the [SEP] symbol.",36,37
1037,218674345,"TABERT also outputs an optional fixedlength table representation T using the representation of the prefixed [CLS] symbol, which is useful for parsers that operate on multiple DB tables.",18,19
1038,218674345,"i |T | i=1 (3) where each entry T i , X i , C i in M consists of T i , the representation of table T i given by the output vector of the prefixed [CLS] symbol, the tablespecific representations of utterance tokens X i = {x 1 , x 2 , . . .},",42,43
1039,7926547,"As a span cube denotes a deduction of the source span, it is represented by a source sequence of symbol γ, which is a mixture of words and nonterminals.",20,21
1040,9205274,We define w 0 to be a special symbol denoting the root of the tree.,8,9
1041,1954146,"Using an example from GeoQuery, given an input function of the form answer(population(city(cityid('seattle', 'wa')))) we produce a ""decorated"" translation input of the form answer 1 population 1 city 1 cityid 2 seattle 0 wa 0 where each subscript indicates the symbol's arity (constants, including strings, are treated as zeroargument functions).",51,52
1042,1954146,"Rules for the phrase-based model consist of pairs of aligned source and target sequences, while hierarchical rules are SCFG productions containing at most two instances of a single nonterminal symbol.",32,33
1043,2951864,"f root = sym,word,pos,le Figure 3 shows example features: f root is the feature for the root node, in which the phrase symbol is S and the surface form, part-of-speech, and lexical entry of the lexical head are ""saw"", VBD, and a transitive verb, respectively.",30,31
1044,9203411,The Y symbol denotes that a random variable is the parent of all Y random variables.,2,3
1045,221655744,"We use a small grammar G = (N, Σ + , R, S), where N = {S, join} is the set of non-terminals, Σ + is the set of terminals (defined in §2), R is a set of four rules detailed in Figure 2 , and S is a special start symbol.",66,67
1046,222291650,"Then, at every decoding step where the next prediction symbol's ""gold"" alignment is to question tokens at indices I, we add the term − log i∈I p i to the objective, pushing the model to put attention probability mass on the aligned tokens.",10,11
1047,204901567,3 We treat the [CLS] symbol as a special case.,7,8
1048,204901567,"Following this observation, we do not use any position and segment embeddings for the [CLS] symbol.",18,19
1049,1701627,"An example of a pattern is: GPE(E2) POS (PERSON)+(E1) This pattern indicates that the word(s) with the tag GPE in the sentence represents the second en-tity (Entity 2) in the relation, while the word(s) tagged PERSON represents the first entity (Entity 1) in this relation, the ""+"" symbol means that the (PERSON) entity is repetitive (i.e. may consist of several tokens).",63,64
1050,202541036,"However, using such interval bounds is unsuitable for our situation of perturbations consisting of a small number of symbol substitutions.",19,20
1051,202541036,"It is worth noting that symbol substitution is general enough to encompass other threat models such as lexical entailment perturbations (Glockner et al.,",5,6
1052,53113638,"Traditionally, text generation models are trained by going over a gold sequence of symbols (characters or words) from left-to-right, and maximizing the probability of the next symbol given the history, namely, a language modeling (LM) objective.",34,35
1053,12777818,"Answer is C EOR C EOS "", where "" EOS "" is the end of sentence symbol and "" EOR "" is the end of rationale symbol.",17,18
1054,12777818,"Answer is C EOR C EOS "", where "" EOS "" is the end of sentence symbol and "" EOR "" is the end of rationale symbol.",27,28
1055,12777818,This process repeats until the model generates the end-of-sentence symbol.,13,14
1056,51873108,"During each REDUCE action, the topmost stack elements that belong to the new constituent are popped from the stack and then composed by the composition function; the composed symbol is then pushed back into the stack.",30,31
1057,51873108,"6 , with a single element The on the stack, the action NT SW(NP) adds the open nonterminal symbol NP to become the topmost stack element, but after applying the swap operator the stack now contains (NP | The (step 2).",20,21
1058,201668205,"State-of-the-art semantic parsers rely on autoregressive decoding, emitting one symbol at a time.",16,17
1059,2611129,"The list of features used in our experiments is: 1{w is the first word in the mention}; 1{w ends with a period}; 1{w is the last word in the men-tion}; 1{w is a Roman numeral}; 1{w starts with an upper-case letter}; 1{w is an Arabic number}; 1{w ∈ {mr, mrs, ms, miss, dr, mdm} }; 1{w contains ≥ 1 punctuation symbol}; 1{w ∈ {jr, sr}}; 1{w ∈ {is, in, of, for}}; 1{w is a person entity}; 1{w is an organization entity}.",85,86
1060,15737225,"In (A) of Table 1 , ""ˆ"" is a symbol representing the beginning of a sentence.",13,14
1061,15136978,"We restricted our patterns to the most frequent 3.9 million patterns of the form ""X-[case particle] Y-[case particle] predicate"" such as ""X-ga Y-ni aru"" (""X is in Y"") which do not contain any negation, number, symbol or punctuation character.",51,52
1062,8192267,"We distinguish this matched word from the other words by replacing it with QW, a special symbol representing a word in the question.",17,18
1063,14551797,"On the other hand we can move from state zero, which corresponds to edit distance zero, to state one, which corresponds to edit distance one, with three different ways: • input is mapped to a different output (input is consumed and a different symbol is emitted) which corresponds to a substitution, • input is mapped to an epsilon (input is consumed and no output emitted) which corresponds to a deletion, and • an epsilon is mapped to an output (output is emitted without consuming any input) which corresponds to an insertion.",49,50
1064,14551797,"Once we reach state 1, the only possible transitions are those that consume a symbol and emit the same symbol again and hence allowing only one edit operation to take place.",15,16
1065,14551797,"Once we reach state 1, the only possible transitions are those that consume a symbol and emit the same symbol again and hence allowing only one edit operation to take place.",20,21
1066,14551797,Adding symbol confusion matrices Adding a symbol confusion matrix can help reduce the number of candidate corrections.,1,2
1067,14551797,Adding symbol confusion matrices Adding a symbol confusion matrix can help reduce the number of candidate corrections.,6,7
1068,14551797,The confusion matrix determines for each symbol a set of symbols that may have substituted it in the garbled word.,6,7
1069,14551797,"For any symbol x, we add an arc x : y between states zero, and one in the transducer where y ∈ Conf usion M atrix(x) rather than for all symbols y in the vocabulary.",2,3
1070,672106,"Each hypernode corresponds to translation hypotheses with identical decoding states, which usually include the span of the words being translated, the grammar symbol for that span and the left and right boundary words of hypotheses for computing language model (LM) scores.",24,25
1071,15262897,"2013) , but differs in the following ways: (1) entity information is represented on a separate embedding, but its positional information remains as symbol entity ; (2) when the vectors are combined, we use the average of each index to normalize features.",28,29
1072,15262897,"Then, the two entity # Entries Accuracy MP pairs 291,585 89% QP pairs 4,764 98% Table 2 : Statistics of NLE-KB pairs mentions are replaced with the symbol entity (who play entity in entity ?).",32,33
1073,10172694,"We map Chinese measure words having non-null translations to a unified symbol {NULL} as mentioned in Section 2.4, indicating that we need not generate these kind of measure words since they can be translated from English.",13,14
1074,51882793,"At the very beginning of the measurement, the similarity between every feature vector in O g and that in O Ǧ is first calculated by the matrix-matrix multiplication O g O ǧ : ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ o g,1 o ǧ,1 ... o g,1 o ǧ,t ... o g,1 o ǧ,l ... ... ... ... ... o g,1 o ǧ,t ... o g,t o ǧ,t ... o g,t o ǧ,l ... ... ... ... ... o g,1 o ǧ,l ... o g,l o ǧ,t ... o g,l o ǧ,l ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ where, the symbol denotes the transpose operation; l is the sentence length which is defined to be uniform for all sentences (l=80), and if it is larger than the real ones, padding is used; o g,i o ǧ,j denotes the scalar product between the feature vectors o g,i and o ǧ,j .",127,128
1075,7432093,"the name pair; the symbol ' ' indicates a ""NUL-L"" alignment.",5,6
1076,2945574,"The words that out of the vocabulary are represented by the ""UNK"" symbol.",14,15
1077,2945574,"We attempt to extract a dictionary to replace the ""UNK"" symbol in target sentence.",12,13
1078,2945574,"Once a ""UNK"" symbol is generated, we locate the corresponding source word and translate it with the dictionary.",5,6
1079,2945574,"We locate the source word that generates the ""UNK"" symbol in target sentence according to the word alignment vector and translate it with the dictionary.",11,12
1080,2813562,Unknown words (those not recognized by the dictionary) are replaced by an OOV symbol.,15,16
1081,2813562,"Rather than attempt to recover the morpheme sequence for an OOV word, in this paper we try only for the tag sequence, replacing all of an OOV's morphemes with the OOV symbol.",34,35
1082,5927863,"If P 2 yields subtree C, and at that point patterns P 3 and P 5 can be applied, this yields subtree D and exact location F (which is expressed by the termination symbol $), respectively.",36,37
1083,5927863,"Figure 6 : Pseudocode for FindPCs in the case of conjunction powerset as the non-terminals (adding a few more details like the start symbol) and production rules such as {P2} → P2 {P3, P5}.",26,27
1084,5927863,"Let pset(P ) be the set of all subsets of patterns which include the pattern P , i.e., pset(P ) = {ν ∪ {P } | ν ∈ powerset(Π)} • Let T = { P pset(P ) | P ∈ Π} {$} be the set of terminals, where $ is a special symbol 4 . •",61,62
1085,5927863,Let N = {S} powerset(Π) be the set of nonterminals with S being the start symbol. •,18,19
1086,5927863,"These are the terminal transitions, they emit a terminal pattern and the symbol $. {",13,14
1087,211296766,"Specifically, the set of symbols (i.e. symbol vocabulary) is initialised with the set of characters and each word is represented as a sequence of characters, plus a special end-of-word symbol.",8,9
1088,211296766,"Specifically, the set of symbols (i.e. symbol vocabulary) is initialised with the set of characters and each word is represented as a sequence of characters, plus a special end-of-word symbol.",37,38
1089,211296766,"Next, BPE iteratively counts all symbol pairs and replaces each occurrence of the most frequent pair ('A', 'B') with a new symbol 'AB'.",6,7
1090,211296766,"Next, BPE iteratively counts all symbol pairs and replaces each occurrence of the most frequent pair ('A', 'B') with a new symbol 'AB'.",29,30
1091,211296766,"Each merge operation produces a new symbol, which represents a character n-gram.",6,7
1092,211296766,Frequent character n-grams (or whole words) are eventually merged into a single symbol.,16,17
1093,211296766,"Because of this bottom-up nature of BPE, it does not require a shortlist and the final symbol vocabulary size is equal to the size of the initial vocabulary, plus the number of merge operations.",19,20
1094,251980383,"For NTCIR-10 and ASPEC, we replaced words of frequency less than 3 with the [UNK] symbol and excluded them from the vocabularies.",18,19
1095,251980383,The beam search was terminated when an end-of-sentence [EOS] symbol was generated.,15,16
1096,11982955,"Inside variables I w i (s, t) store the probability P (X i ⇒ * w s,t | X i ), that is the probability of producing the sub-sequence w s,t = w s , ..., w t of the string w 1,T = w 1 , ..., w T from the non-terminal symbol X i , given the non-terminal symbol X i , in any number of steps (⇒ * is the closure of the production symbol ⇒).",68,69
1097,11982955,"Inside variables I w i (s, t) store the probability P (X i ⇒ * w s,t | X i ), that is the probability of producing the sub-sequence w s,t = w s , ..., w t of the string w 1,T = w 1 , ..., w T from the non-terminal symbol X i , given the non-terminal symbol X i , in any number of steps (⇒ * is the closure of the production symbol ⇒).",77,78
1098,11982955,"Inside variables I w i (s, t) store the probability P (X i ⇒ * w s,t | X i ), that is the probability of producing the sub-sequence w s,t = w s , ..., w t of the string w 1,T = w 1 , ..., w T from the non-terminal symbol X i , given the non-terminal symbol X i , in any number of steps (⇒ * is the closure of the production symbol ⇒).",95,96
1099,11982955,"Outside variables O w i (s, t) store the probability P (S ⇒ * w 0,s , X i , w t,T | S), that is the probability of producing the subsequence w 0 , ..., w s , X i , w t , ..., w T from the root symbol S, given the root symbol S. Probability re-estimation consists in computing the quantities P (X i ⇒ X j , X k ), P (X i ⇒ w) and P (X i ) in terms of Inside and Outside variables.",61,62
1100,11982955,"Outside variables O w i (s, t) store the probability P (S ⇒ * w 0,s , X i , w t,T | S), that is the probability of producing the subsequence w 0 , ..., w s , X i , w t , ..., w T from the root symbol S, given the root symbol S. Probability re-estimation consists in computing the quantities P (X i ⇒ X j , X k ), P (X i ⇒ w) and P (X i ) in terms of Inside and Outside variables.",67,68
1101,9161437,"They were as follows: 1 In Myanmar language, there is no question mark, exclamation mark, colon and semicolon, usually only two punctuation symbols are used, and the little section symbol (as comma in English) ""၊""",35,36
1102,9161437,"and section symbol (as full stop in English) ""။"".",2,3
1103,202773799,"For simplification, we use a special symbol, [S], to concatenate all the entity information together.",7,8
1104,202773799,Unstructured models concatenate different source inputs into a long sequence with a special separation symbol [SEL] .,14,15
1105,13257146,"Besides full matching, we introduce a symbol of ""^"" for sub-phrase, such as e for a sub-phrase of e i and e for a sub-phrase of e , to allow partial matching.",7,8
1106,2888924,"Formally, an SCFG G is defined as the tuple (N, E, F, R, S), where N is the finite set of non-terminals with S ∈ N the start symbol, F and E are finite sets of words for the source and target language and R is a finite set of rewrite rules.",38,39
1107,2888924,"Beginning from the start symbol S, an initial phrase-span structure is constructed monotonically using a simple 'glue gram-mar': S →S 1 X 2 / S 1 X 2 S →X 1 / X 1 The true power of the system lies in expanding these initial phrase-spans with a set of hierarchical translation rules, which allow conditioning reordering decisions based on lexical context.",4,5
1108,447315,"To handle this problem, the probability of a latent state t x generating a rare word w is forced to be proportional to the emission probability of word w given the surface tag t. This is achieved by mapping all words with frequency less than threshold 3 λ to the unk symbol, and for each latent state t x of a POS tag t, accumulating the word tag statistics of these rare words to c r (t x , unk) = w:c(w)<λ c(t x , w), and then redistributing them among the rare words to estimate their emission 2 The parallel version is able to train our largest grammar on a 8-core machine within a week, while the non-parallel version is not able to finish even after 3 weeks.",52,53
1109,235097508,"Each of our MNMT models in the tables is denoted with a symbol ""+"", which indicates that a MNMT model is constructed with the baseline and one of our cross-modality learning models.",12,13
1110,235097508,"We label the former with a symbol of "" "" in Tables 3 and 4 to ease the comparison.",6,7
1111,17255410,"A case schema contains three main description slots; a) roles of cases associated with the semantic symbol, b) transformation rules of schemata, c) choice of German syntactic realization schemata.",18,19
1112,17255410,"Though English-like terms are used for semantic symbols, the choice of a German word associated with each semantic symbol and its syntactic structure very differ from the English corresponding one.",21,22
1113,17255410,"The original Japanese text does not contain an explicit word that coreponds to the semantic symbol ""USE.ACT"", that is infered by tile analyzer.",15,16
1114,17255410,"Especially in our approach, a semantic symbol generally corresponds to an upper concept, under which an appropriate German term is registered as a specialization.",7,8
1115,17255410,"The reason why the German ideom ""unter Verwendung yon"" was frequently used can be attributed to the semantic symbol ""USE.ACT"", often infered (about I0%) by the analysis system. (",20,21
1116,1560072,"For deeper levels, the system includes subtrees corresponding to the successive application of rules on non-terminals symbols until either a terminal symbol is reached or the given depth is reached.",24,25
1117,220045943,The symbol -denotes both the results and codes are not reported by these papers. .,1,2
1118,9055166,"An example of a TGrep2 search pattern for the progressive verb tense is the following: ""VP < /^VB/ < (VP < VBG)"" Searching for this pattern returns sentences in which a verb phrase (VP) dominates an auxiliary verb (whose symbol begins with VB) as well as another verb phrase, which in turn dominates a verb in gerund form (VBG).",47,48
1119,10252606,"An HMM uses probability matrices Π, A, and B for the initial state, state transitions, and symbol emissions respectively (Manning and Schütze, 1999) .",20,21
1120,5387945,"While there is some freedom in this choice and a traditional CNF transformation often introduces a binarized variant of each constituent, we find no appreciable benefit from having more than one binary symbol.",33,34
1121,6937668,"Each input symbol is split into two new symbols, both with a new unique symbol label, and the grammar is updated to include a copy of each original rule for each such refinement, with a small amount of random noise added to the probability of each production to break ties.",2,3
1122,6937668,"Each input symbol is split into two new symbols, both with a new unique symbol label, and the grammar is updated to include a copy of each original rule for each such refinement, with a small amount of random noise added to the probability of each production to break ties.",15,16
1123,16412985,Each cluster has a meta-tag symbol (as can be seen in Table 2) and all occurrences of members of a cluster were substituted by the cluster's meta-tag.,7,8
1124,4121123,"Defining the input/output An initial crucial decision in building a disambiguator for a Semitic text concerns the ""tokenization"" of the input sentence: what constitutes a terminal (i.e., input) symbol.",36,37
1125,4121123,"In this case a nonterminal a that is assigned to a word w consists of a sequence of POS tags, each assigned to a mor-pheme of w, delimited with a special segmentation symbol.",36,37
1126,13941080,"Probabilistic Context-Free Grammars: As usual, a PCFG is a five-tuple (V N , V T , S, R, P ), with S ∈ V N being the start symbol and R ∈ V N × (V N ∪ V T ) + is the set of productions.",38,39
1127,13941080,"Furthermore, we define the Expected Labeled Recall rate (ER LR ): ER LR (T g |w n 1 ) def = i,X,j ∈Tg g(i, X, j) N (w n 1 ) (4) Given a PCFG µ with a start symbol S, Goodman observes that, under a common assumption 2 , g(i, X, j) is approximately equal to the probability of the set of all PCFG derivations of w n 1 that pass through constituent i, X, j , i.e. P µ ({S + ⇒ w i−1 1 Xw n j }|w n 1 ).",52,53
1128,13941080,"In figure 1 , we exhibit a toy corpus of correct parse-trees, where S and A are the non-terminals and x the terminal symbol.",28,29
1129,229679923,"2020) to use [<sep>, quesiton,</sep>, paragraph,</sep>] as the input, where <sep> is a special token in front of two segmants and </sep> is a special symbol to split two kinds of data types.",39,40
1130,11162815,"If the special class null is represented by the symbol φ, then for every set of such arguments, the following linear equality represents this constraint.",9,10
1131,10324322,"s m , where s i (1 ≤ i ≤ m) is the non-terminal symbol associated with the ith node being traversed in the path, with s 1 and s m being the non-terminal symbol associated with the nodes spanning NP k and NP j , respectively.",18,19
1132,10324322,"s m , where s i (1 ≤ i ≤ m) is the non-terminal symbol associated with the ith node being traversed in the path, with s 1 and s m being the non-terminal symbol associated with the nodes spanning NP k and NP j , respectively.",41,42
1133,661,"Let the set of subtrees associated with the PA-SSFs, which the learning algorithm outputs, be the set of elementary-trees of a TSG; the TSG has the same start-symbol, terminal and non-terminal symbols as the CFG underlying the tree-bank.",36,37
1134,9111381,"We encode a first-order decision as α cs , a binary variable indicating that constituent c is aligned with the logical symbol s. A second-order decision β cs,dt , is encoded as a binary variable indicating that the symbol t (associated with constituent d) is an argument of a function s (associated with constituent c).",23,24
1135,9111381,"We encode a first-order decision as α cs , a binary variable indicating that constituent c is aligned with the logical symbol s. A second-order decision β cs,dt , is encoded as a binary variable indicating that the symbol t (associated with constituent d) is an argument of a function s (associated with constituent c).",44,45
1136,9111381,First-order decision features Φ 1 Determining if a logical symbol is aligned with a specific constituent depends mostly on lexical information.,11,12
1137,9111381,1990) and add features which measure the lexical similarity between a constituent and a logical symbol's surface forms (as defined by the lexicon).,16,17
1138,1101955,"The STSG has start-symbol S, two terminals represented by T and F , nonterminals which include (beside S) all Ck, for mula does not contain repetition of conjuncts.",5,6
1139,2969247,"If the special class null is represented by the symbol φ, then for every set of such arguments, the following linear equality represents this constraint.",9,10
1140,10048734,"In addition, some other features are extracted from the target phrase, including: symbol explanation icap the first character of a word is capitalized acap all characters of a word are capitalized incap some characters of a word are capitalized suffix the suffix of a word is ""ing"", ""ment"", etc.",15,16
1141,638874,First a fertility is drawn for each English word (including the NULL symbol).,13,14
1142,7626777,"Finally, Inanimate uses thing, object, space, place, symbol, food, structure, sound, measure, and unit.",12,13
1143,234335834,¬ means the negation operation upon a specific logical symbol and → acts as a conditional relationship between two logical symbols. (,9,10
1144,234335834,"If any negative word (e.g., ""not"", ""unable"") is in or immediately before a logical symbol α, we add the negation connective ¬ before α as a new symbol ¬ α.",22,23
1145,234335834,"If any negative word (e.g., ""not"", ""unable"") is in or immediately before a logical symbol α, we add the negation connective ¬ before α as a new symbol ¬ α.",36,37
1146,234335834,"The delete, reverse or negate operations are respectively to delete a logical expression in the context, reverse the conditional order of a logical expression and negate a logical symbol in a logical expression.",30,31
1147,234335834,"We re- Extending the Implicit Logical Expressions  port the recall of logical symbol and logical expression identification as 65.9% and 48.9%, respectively.",13,14
1148,234335834,"Algorithm 1 Logic Identification Algorithm Input: A sentence in the context or an option t to be parsed, a set of logical negative keywords N and a set of logical conditional indicators C. Output: A logical expressions set S parsed from the input t. 1: Initializing S := {} 2: Extracting constituents from the input t. if ∃ n i ∈ N is in or immediately before the logical symbol a then 6: Adding the negation connective ¬ before a as ¬a.",76,77
1149,234335834,7: Replacing the original symbol with the negative one as a := ¬a.,5,6
1150,234335834,"We regard literally similar constituents with an overlap rate over 60% as the same symbol if they also have consistent degree modifiers, such as ""only"", ""most"", ""least"", etc.",15,16
1151,5145599,"2006) : Richer, Linguistic Trees: Our grammars have a Penn Treebank-style linguistic tree on the English (target) side, and a hierarchical structure using only a single non-terminal symbol (X) on the source side.",37,38
1152,5145599,"Starting with an initial symbol pair representing a rule with a single substitution site, 2 TOP↓, X↓ , a tree pair can be generated by the following steps: 1.",4,5
1153,7591666,"The nonterminal (NT) M is first predicted from the parent P , then -in order -L 1 through L m (stopping symbol '#') and R 1 through R n (again '#'), as shown in Equation 1 .",24,25
1154,43964183,"HIERO makes very strong independence assumptions, since it uses only one label ""X"" apart from the glue symbol GOAL, allowing any HIERO rule to substitute to any other rule.",20,21
1155,43964183,"The ""X"" symbol in the rule is the symbol that will be substituted to HIERO (non-glue) rules.",4,5
1156,43964183,"The ""X"" symbol in the rule is the symbol that will be substituted to HIERO (non-glue) rules.",10,11
1157,43964183,"The GOAL label, occurring on the left-hand side of the rule and as the first nonterminal on the right-hand side, is also known as the start symbol.",32,33
1158,1576593,After pre-processing we replace every word that occurs less than 5 times with an UNK symbol.,17,18
1159,248512524,"Specifically, MWP randomly replaces words in input sentences with a special symbol and requires models to recover the masked words in the input.",12,13
1160,248512524,We use random symbols instead of a fixed symbol because we found that it gives better performance in our preliminary experiments.,8,9
1161,241033035,"DeltaLM + Zcode (Direct) denotes the strategy that we choose the direct translation for all translation directions, where the target language symbol is prefixed to the input sentence to indicate the translation direction.",24,25
1162,10312772,"This means that invalid trigram sequences (e.g., two or more trigrams containing the symbol 'end') could appear in the output compression.",15,16
1163,8905391,"Besides, it filters out those nodes which are punctuations or with ""symbol"" dependency relations.",13,14
1164,997446,"Toward Bilingual Syntax-aware Translation Generation As we could see in section 4.1, there is only one kind of non-terminal symbol ""X"" in the target side.",24,25
1165,997446,"However, in this rule set with the only symbol ""X"", it may be merged with upper structure as a ""VP"" (verb phrase) instead, because there is no way to favor one over another.",9,10
1166,222141016,"Specifically, we feed the target-side sentence to the noising model C and append the corresponding language ID symbol; the model then attempts to reconstruct the original sentence.",20,21
1167,5667590,"We frame the inference problem as an Integer Linear Programming (ILP) problem (Equation ( 2 )) in which the first-order decisions are governed by α cs , a binary decision variable indicating that constituent c is aligned with logical symbol s. And β cs,dt capture the second-order decisions indicating the symbol t (associated with constituent d) is an argument to function s (associated with con-stituent c).",45,46
1168,5667590,"We frame the inference problem as an Integer Linear Programming (ILP) problem (Equation ( 2 )) in which the first-order decisions are governed by α cs , a binary decision variable indicating that constituent c is aligned with logical symbol s. And β cs,dt capture the second-order decisions indicating the symbol t (associated with constituent d) is an argument to function s (associated with con-stituent c).",60,61
1169,5667590,The set of constraints are: • A given constituent can be associated with exactly one logical symbol. •,17,18
1170,5667590,First-order decision features Φ 1 Determining if a logical symbol is aligned with a specific constituent depends mostly on lexical information.,11,12
1171,5667590,We add features which measure the lexical similarity between a constituent and a logical symbol's surface forms (as defined by the lexicon).,14,15
1172,5667590,We also prune the search space by limiting the number of logical symbol candidates per word (on average 13 logical symbols per word).,12,13
1173,247619149,"Finally, Decoder(•) finishes prediction when outputting the end symbol <eos>, then we convert the predicted SEL expression into the extracted information record.",10,11
1174,199379793,"Thus, by the end of the algorithm most frequent word segments would have been joined into a single symbol.",19,20
1175,5930290,"For example, we might specify that we are interested in trees which use the symbol NP and then list several examples of prototypical NPs (determiner noun, pronouns, etc.,",15,16
1176,5930290,"That is, an NP in one of these grammars may correspond to the target symbol VP, or may not correspond well to any target symbol.",15,16
1177,5930290,"That is, an NP in one of these grammars may correspond to the target symbol VP, or may not correspond well to any target symbol.",26,27
1178,1756650,"Question Pattern-based Translation A question pattern QP includes a pattern string QP pattern , which is composed of words and a slot 3 For simplicity, a cleaned entity dictionary dumped from the entire KB is used to detect entity mentions in Q. symbol [Slot] , and a KB predicate QP predicate , which denotes the meaning expressed by the context words in QP pattern .",45,46
1179,1756650,"Question patterns are collected as follows: First, 5W queries, which begin with What, Where, Who, When, or Which, are selected from a large scale query log of a commercial search engine; Then, a cleaned entity dictionary is used to annotate each query by replacing all entity mentions it contains with the symbol From experiments (Table 3 in Section 4.3) we can see that, question pattern based question translation can achieve high end-to-end accuracy.",61,62
1180,1756650,"If a question matches any one of these patterns, then sub-questions are generated by collecting the paths between n 0 and each n i (i > 0) in the pattern, where each n denotes a complete subtree with a noun, number, or question word as its root node, the symbol * above prep * denotes this preposition can be skipped in matching.",58,59
1181,2024558,"Joint Translation Model Our model is based on a probabilistic Synchronous CFG (Wu, 1997; Chiang, 2005) language over string pairs, which are generated beginning from a start symbol S and recursively expanding pairs of linked non-terminals across the two strings using the grammar's rule set.",33,34
1182,2024558,"We utilise probabilistic SCFGs, where each rule is assigned a conditional probability of expanding the left-hand side symbol with the rule's right-hand side.",20,21
1183,2024558,"The probability of a derivation D of tuple e, f beginning from start symbol S is equal to the product of the probabilities of the rules used to recursively generate it.",14,15
1184,2247967,"Assuming a vocabulary V and grammar symbol set G, the state space size is up to |V | 2(n−1) |G|, which is immense for a large vocabulary when n > 1.",6,7
1185,1600804,This is done until only one symbol remains in this stack.,6,7
1186,214794966,"For each batch, we randomly sample 15% of the words and replace them with (i) a special symbol [MASK], (ii) a random token or (iii) keep them unchanged with probability 80%, 10% and 10%, respectively.",21,22
1187,214794966,3) Replace 10% of the input words in X with the [MASK] symbol. (,16,17
1188,214794966,"We use a special symbol [MASK] to replace 80% of the masked tokens, keep 10% unchanged, and random replace 10% of the masked tokens.",4,5
1189,202734468,GEN(w) Push the terminal symbol w ∈ Σ onto the top of the stack and the end of the buffer.,5,6
1190,202734468,The embeddings of the subtrees on the stack are computed recursively using a bidirectional LSTM that reads the embeddings of the nonterminal symbol and its children.,22,23
1191,231815627,"Here, we adopt two independent neural networks initialized from pre-trained LMs for the two encoders E q (•) and E p (•) separately, and take the representations at the first token (e.g., [CLS] symbol in BERT) as the output for encoding.",45,46
1192,216144451,"generate a sequence until a blank symbol "" "" is produced.",6,7
1193,229923118,"When the model learns CAMLM, the model can only predict the MASK token based on the sentence of its corresponding parallel sentence and the MASK symbol of this sentence, which provides the position and language information.",26,27
1194,225062100,"2) Comprehensive N-gram Prediction We propose to simultaneously predict n-grams in fine-grained and coarse-grained manners corresponding to single mask symbol [M], which helps to extract comprehensive n-gram semantics, y ,y { } 1 x 2 x 3 x 5 x 4 x 6 x 1 2 3 4 5 + + 2 + 2 + 4 + + + + ×L K,V Q K,V Q K,V Q K,V Q K,V Q 2 y 4 y Multi-Head Self-Attention as shown in Figure 1 (c).",28,29
1195,189762527,"Representing relations with language While the automatic metrics point to insignificant differences when comparing models with symbol relations and those with natural language relations (Table 6 ), examples can provide qualitative insights into the benefits of representing relations as language.",16,17
1196,189762527,"The model that uses symbol relation embeddings only manages to produce the relation ""dove SymbolOf submarine"", which seems to relate ""submarine"" to a more nautical (and unrelated) word sense of ""dove"".",4,5
1197,627008,"The other kind is lexicalized features, such as those in Equations 2 and 3, but involving functions words (like the Japanese characters ""wa"", ""ga"", ""ni"", ""de"") or special characters (such as numeral symbol and punctuation).",48,49
1198,199452955,"To differentiate between user utterance and agent utterance, we add symbol [U] before each user utterance and [A] before each agent utterance.",11,12
1199,12929928,"Let Σ be a set of finite symbols, and Σ n be a set of possible (symbol) sequences whose sizes are n or less that are constructed by symbols in Σ. The meaning of ""size"" in this paper is the number of symbols in the sub-structure.",18,19
1200,12929928,"Specifically, PrefixSpan algorithm evaluates uw, where uw represents a concatenation of a sequence u and a symbol w, using the following three conditions.",18,19
1201,12929928,The values of each symbol represent χ 2 (u) and χ 2 (u) that can be calculated from the number of x u and y u .,4,5
1202,12929928,The TRIE structure in the figure represents the statistically significant sub-sequences that can be shown in a path from ⊥ to the symbol.,24,25
1203,12929928,"We introduce a special symbol Λ to represent an ""empty sequence"", and define Λw = w and |Λw| = 1.",4,5
1204,5450302,"We assume a one-hot vector for a special start symbol, such as ""⟨S⟩"", when i < 1.",11,12
1205,647664,This design preserves the feature whereby each HMM emits a single symbol from a single state (or transition).,11,12
1206,5640129,"6 's particle is denoted ZP, and CP stands for 7 's next word that is 7 's particle or a punctuation symbol.",23,24
1207,8888654,"A substitution symbol corresponds to a substitution node in t. In Figure 2 (b), which is derived from Figure 2 (a), the strings in the angle brackets indicate the substitution symbols.",2,3
1208,8888654,"For each TSC in the forest, if the target language string contains the substitution symbols, then the substitution symbol is substituted by the translation that is obtained from the child TSC.",20,21
1209,2128816,We annotate each parent node of the swapped edge with # symbol.,11,12
1210,2128816,"For example, a nonterminal symbol PP # (with) shows that a noun phrase ""a/an telescope"" and a word ""with"" are inverted.",5,6
1211,2128816,"Our monolingual ITG G is a tuple G = (V, T, P, I, S) where V is a set of nonterminals, T is a set of terminals, P is a set of production rules, I is a set of nonterminals on which ""the"" ""a/an"" or ""no articles"" must be determined, and S is the start symbol.",73,74
1212,21731209,"At each step, the discriminator scores are recomputed for all candidates, with the exception of the entailment score, which is only recomputed for hypotheses which end with a sentence terminating symbol.",33,34
1213,52138320,Experiments on Constituency Parsing Choe and Charniak (2016) achieved high F1 scores on the Penn Treebank constituency parsing task by transforming candidate trees into a symbol sequence (S-expression) and reranking them based on the perplexity obtained by a neural language model.,27,28
1214,8745565,"In this case for all stress variants of the word we collect prefixes of length 1 through the length of the word, and similarly for suffixes, except that for the stress symbol we treat that together with the vowel it marks as a single symbol.",33,34
1215,8745565,"In this case for all stress variants of the word we collect prefixes of length 1 through the length of the word, and similarly for suffixes, except that for the stress symbol we treat that together with the vowel it marks as a single symbol.",46,47
1216,6910651,"For HMMs, we consider a first order HMM defined in the following equation: p(x, y|θ) = S+1 s=1 θy s−1 ,ys θy s,xs , where θ y s−1 ,ys and θ ys,xs represent the transition probability between states y s−1 and y s and the symbol emission probability of the s-th position of the corresponding input sequence, respectively, where θ y S+1 ,x S+1 = 1.",55,56
1217,6910651,"Again, the second term, which indicates the expectation of transition probabilities and symbol emission probabilities, can be rewritten in the form of a forward-backward algorithm in the same manner as γ i , where the only difference is that V D i,s is substituted by V G j,s in Equation ( 7 ).",14,15
1218,6910651,"Thus, each HMM maintains to consist of a non-overlapping feature set since each feature type only generates one symbol per state.",21,22
1219,216868683,On it was a round symbol with geometric shapes glowing with dark red embers and a dull tendril of smoke rising up from it .,5,6
1220,216868683,On it was a round symbol with geometric shapes glowing with dark red embers and a dull tendril of smoke rising up from it .,5,6
1221,1994584,"In all experiments, we add a START symbol at the beginning of each sentence.",8,9
1222,15401237,The symbol R in the sub-path column shows that more than one node skip occurred there.,1,2
1223,7962030,"For the style features, we add a START symbol at the beginning of each sentence.",9,10
1224,8066499,"The decoder can optionally choose to output an unknown word symbol, in which case the predicted attention is used to copy a token directly from the input sequence into the output sequence.",10,11
1225,215827766,"Both strategies result in disentangled messages, where each primitive symbol (or sym-bol+position pair) univocally refers to a distinct primitive meaning independently of context.",10,11
1226,215827766,"Next, the message is generated symbol-by-symbol by sampling from a Categorical distribution over the vocabulary c voc , parameterized by a linear mapping from Sender's hidden state.",6,7
1227,215827766,"Next, the message is generated symbol-by-symbol by sampling from a Categorical distribution over the vocabulary c voc , parameterized by a linear mapping from Sender's hidden state.",10,11
1228,215827766,"Consider a language with a symbol r referring to input element color:red and another symbol l referring to weight:light, where r and l can be juxtaposed (possibly, in accordance with the syntactic rules of the language) to refer to the input set {color:red, weight:light}.",5,6
1229,215827766,"Consider a language with a symbol r referring to input element color:red and another symbol l referring to weight:light, where r and l can be juxtaposed (possibly, in accordance with the syntactic rules of the language) to refer to the input set {color:red, weight:light}.",16,17
1230,215827766,"On the other hand, a language where both r and l refer to these two input elements, but only when used together, whereas other symbol combinations would refer to color:red and weight:light in other contexts, is intuitively not compositional.",27,28
1231,215827766,"In our simple environment, it might be the case that the first symbol is used to discriminate among values of an attribute, and the second to discriminate among values of another attribute.",13,14
1232,215827766,Let's denote s j the jth symbol of a message and a j 1 the attribute that has the highest mutual information with s j : a j 1 = arg max a I(s j ; a).,7,8
1233,215827766,"The latter maintains the requirement for symbols to univocally refer to distinct meanings, but captures the intuition of a permutation-invariant language, where only symbol counts are informative.",27,28
1234,215827766,"Denoting by n j a counter of the j-th symbol in a message, bosdis is given by: bosdis = 1/c voc cvoc j=1 I(n j ; a j 1 ) − I(n j ; a j 2 ) H(n j ) (2) In all experiments, the proposed measures topsim, posdis and bosdis are calculated on the train set.",11,12
1235,215827766,"Look first at the top block, where the trained Receiver of the relevant run is fed messages with the symbol in one original position preserved, the others shuffled.",20,21
1236,215827766,"Indeed, the results in the bottom block of the table (one symbol shuffled while the others stay in their original position) confirm that pos1 carries useful complementary information: when fixing the latter and either one of the other positions, we achieve 100% accuracy for the relevant attribute (att2 for pos1+pos2 and att1 for pos1+pos3), respectively.",13,14
1237,215827766,"We define the cue validity of s p (symbol in position p) w.r.t an attribute a as CV (s p , a) = max ā P (ā|s p ), where ā iterates over all possible values of a. CV (s pos1 , att2) is significantly higher in those (train/test) messages where CV (s pos2 , att2) is below average.",9,10
1238,215827766,"This suggests that, for large |I|, the emergence of a perfectly nonambiguous compositional languages, where each message symbol denotes only one attribute value and each value attribute is denoted by only one message symbol, is impossible.",20,21
1239,215827766,"This suggests that, for large |I|, the emergence of a perfectly nonambiguous compositional languages, where each message symbol denotes only one attribute value and each value attribute is denoted by only one message symbol, is impossible.",36,37
1240,215827766,Both languages come from the training configuration with 2 100-valued input attributes and 3 100-symbol positions.,18,19
1241,5564363,"Data preprocessing For the CoNLL dataset, we lowercase all tokens and remove any token that only contains a punctuation symbol unless it is in an entity mention.",20,21
1242,17984918,"Formally, the set of these hyperedges can be defined as a 3-tuple E=<T, N, P>, where T is a set of the terminal word symbol in target language, N is a set of the non-terminal symbol including three symbols N={S, X 1 , X 2 }, P is a set of production rules including two types: Lexical rule: X w, w D Non-terminal rule : S X 1 X 2 , X X 1 X 2 where D is a dictionary including null word (ε ) for normalization in system combination, start symbol (G R) and single word.",31,32
1243,17984918,"Formally, the set of these hyperedges can be defined as a 3-tuple E=<T, N, P>, where T is a set of the terminal word symbol in target language, N is a set of the non-terminal symbol including three symbols N={S, X 1 , X 2 }, P is a set of production rules including two types: Lexical rule: X w, w D Non-terminal rule : S X 1 X 2 , X X 1 X 2 where D is a dictionary including null word (ε ) for normalization in system combination, start symbol (G R) and single word.",45,46
1244,17984918,"Formally, the set of these hyperedges can be defined as a 3-tuple E=<T, N, P>, where T is a set of the terminal word symbol in target language, N is a set of the non-terminal symbol including three symbols N={S, X 1 , X 2 }, P is a set of production rules including two types: Lexical rule: X w, w D Non-terminal rule : S X 1 X 2 , X X 1 X 2 where D is a dictionary including null word (ε ) for normalization in system combination, start symbol (G R) and single word.",111,112
1245,1403704,"At each tree node, we generate a virtual nonterminal symbol by concatenating the source span it dominates.",10,11
1246,7556893,Syntactic measures Let V be a finite vocabulary and ξ be the null symbol.,13,14
1247,9818013,"x (j) i−1 , g (j) , E (j) ; θ), where x (j) 1 is the start symbol.",28,29
1248,21688828,"In fact, we can often use a symbol effectively with no prior data.",8,9
1249,21688828,"For example, a language user that has never have encountered the symbol Socrates before may nonetheless be able to leverage their syntactic, semantic and inferential skills to conclude that Socrates is mortal contradicts Socrates is not mortal.",12,13
1250,21688828,Marcus' experiment essentially requires extrapolating what has been learned about one set of symbols to a new symbol in a systematic way.,18,19
1251,14906667,Training is based on the EM algorithm along with 6 cycles of splitting each symbol into two and remerging the 50% of sub-symbols carrying the least information.,14,15
1252,237260023,"Encoder-Decoder first (ST5-EncDec first): the first decoder output when the input text is fed into the encoder and the standard ""start"" symbol is fed as the only decoder input.",29,30
1253,13344783,"Each cell ij of the CKY matrix corresponds to G ij , a subsequence of G starting at position i and ending at position j. If a cell in the CKY matrix is labeled with a nonterminal symbol s, it means that the corresponding tree of G ij has s as its root.",37,38
1254,13344783,"ijs = 1 iff cell ij of the matrix is assigned (3) parse tree symbol s Todo:[Rename symbols to tags throughout the paper] Where i 2 [0, N) indexes CKY matrix diagonals and j 2 [0, N i) indexes elements of diagonal i. In order to model rule selection at each CKY step, we define variables, which correspond to a PCFG rule used at the given cell ij of CKY matrix: ijkr = 1 iff ijh = ikp (4) = (k+1)jq = 1, Where r = h pq 2 R and k 2 [i, j).",16,17
1255,13344783,"α ijk = 1 iff α ik = α j(k+1) = 1 (2) Variables for Tree Structure: Variables β encode the parse structure: β ijs = 1 iff the phrase sequence G ij (3) maps to the nonterminal symbol s ∈ N T Where i ∈ [0, N ) and j ∈ [i, N ) index rows and columns of the CKY-style matrix in Figure 3 .",45,46
1256,13344783,"ijs = 1 iff cell ij of the matrix is parse tree symbol s Todo:[Rename symbols to tags throug per] Where i 2 [0, N) indexes CKY matr and j 2 [0, N i) indexes elements of In order to model rule selection at eac we define variables, which correspond rule used at the given cell ij of CKY m ijkr = 1 iff ijh = ikp = (k+1)jq = Where r = h pq 2 R and k 2 [i corresponds to the choice of children fo cell.",12,13
1257,13344783,"ijs = 1 iff cell ij of the matrix is assigned (3) parse tree symbol s Todo:[Rename symbols to tags throughout the paper] Where i 2 [0, N) indexes CKY matrix diagonals and j 2 [0, N i) indexes elements of diagonal i. In order to model rule selection at each CKY step, we define variables, which correspond to a PCFG rule used at the given cell ij of CKY matrix: ijkr = 1 iff ijh = ikp (4) = (k+1)jq = 1, Where r = h pq 2 R and k 2 [i, j).",16,17
1258,13344783,"ijs = 1 iff cell ij of the matrix is assigned parse tree symbol s Todo:[Rename symbols to tags throughout the per] Where i 2 [0, N) indexes CKY matrix diago and j 2 [0, N i) indexes elements of diagona In order to model rule selection at each CKY s we define variables, which correspond to a PC rule used at the given cell ij of CKY matrix: ijkr = 1 iff ijh = ikp = (k+1)jq = 1, Where r = h pq 2 R and k 2 [i, j).",13,14
1259,13344783,"ee Congruence Constraints: To ensure that ch CKY cell has at most one symbol we require 8 ij , X s2S ijs  1 (10) e also require that 8 i,j>i,h , ijh = j 1 X k=i X r2R h ijkr (11) here R h = {r 2 R : r = h !",14,15
1260,13344783,This constraint rbids instantiations where a nonterminal symbol h selected for cell ij without selecting a correspondg PCFG rule.,7,8
1261,13344783,"Tree Congruence Constraints: To ensure that each CKY cell has at most one symbol we require 8 ij , X s2S ijs  1 (10) We also require that 8 i,j>i,h , ijh = j 1 X k=i X r2R h ijkr (11) Where R h = {r 2 R : r = h !",14,15
1262,13344783,This constraint forbids instantiations where a nonterminal symbol h is selected for cell ij without selecting a corresponding PCFG rule.,7,8
1263,13344783,"Tree Congruence Constraints: To ensure that each CKY cell has at most one symbol we require 8 ij , X s2S ijs  1 (10) We also require that 8 i,j>i,h , ijh = j 1 X k=i X r2R h ijkr (11) Where R h = {r 2 R : r = h !",14,15
1264,13344783,This constraint forbids instantiations where a nonterminal symbol h is selected for cell ij without selecting a corresponding PCFG rule.,7,8
1265,13344783,"Tree Congruence Constraints: To ensure that each CKY cell has at most one symbol we require 8 ij , X s2S ijs  1 (10) We also require that 8 i,j>i,h , ijh = j 1 X k=i X r2R h ijkr ( 11 ) Where R h = {r 2 R : r = h !",14,15
1266,13344783,This constraint forbids instantiations where a nonterminal symbol h is selected for cell ij without selecting a corresponding PCFG rule.,7,8
1267,13344783,"Tree Congruence Constraints: To ensure that each CKY cell has at most one symbol we require ∀ ij , s∈N T β ijs ≤ 1 (10) We also require that ∀ i,j>i,h , β ijh = j−1 k=i r∈R h β ijkr (11) Where R h = {r ∈ R : r = h → pq}.",14,15
1268,13344783,This constraint forbids instantiations where a nonterminal symbol h is selected for cell ij without selecting a corresponding PCFG rule.,7,8
1269,13344783,"We define a matrix of scores D[i, j, h] (Equation 17), where h is one of the nonterminal symbols being considered for a cell indexed by i, j, i.e. a candidate for the root symbol of a branch ŷ[i : j].",42,43
1270,46602,"2006) create an xRS rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multiheaded syntactic structure; and one sibling xRS rule that explains how the pseudo symbol can be combined with other genuine non-terminals for acquiring the genuine parse trees.",17,18
1271,46602,"2006) create an xRS rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multiheaded syntactic structure; and one sibling xRS rule that explains how the pseudo symbol can be combined with other genuine non-terminals for acquiring the genuine parse trees.",39,40
1272,9808585,All words were converted to lowercase and numbers were replaced with the symbol num .,12,13
1273,7100691,"The less successful books considered in Table 9 had an Amazon seller's rank beyond 200k (higher rank indicating less commercial success) except Dan Brown's The lost symbol, which we included mainly because of negative critiques it had attracted from media despite its commercial success.",30,31
1274,1939935,"They argue that, without grounding words to bodily actions and perceptions in the environment, we can never get past defining a symbol by simply pointing to covariation of amodal symbolic patterns (Harnad, 1990) .",23,24
1275,1939935,"Integrating textual and perceptual information Louwerse (2011) , contributing to the debate on symbol grounding in cognitive science, theorizes the interdependency account, which suggests a convergence of symbolic theories (such as distributional semantics) and perceptual theories of meaning, but lacks of a concrete way to harvest perceptual information computationally.",15,16
1276,1939935,"Coming back to the discussion of symbol grounding at the beginning of the paper, we consider this (very!)",6,7
1277,15744099,"The higher probability of our binary bITG rule VP→[VP NP], where the square brackets denote the same ordering (straight) of the two right-hand-side constituents in both languages when expanding the left-hand-side symbol, indicates a similar VO construct exists in English (SVO language) and Chinese (SVO language).",43,44
1278,15744099,"On the contrary, the different VO construct in English and Japanese (SOV language) is modeled through the high inverted probability of the binary bITG rule VP→<VP NP> where the pointed brackets denote that we expand the left-hand-side symbol into two right-hand-side symbols in reverse orientation in two languages.",45,46
1279,15744099,"The substring pair (positive role, 積極 作 用) associated with linguistic symbol NP will be assigned a probability.",14,15
1280,15744099,"e f = ( ) 1 1 , m n e e f f Parsing Algorithm //Initial Step For 1 ,1 i m j n ≤ ≤ ≤ ≤ (1) ( ) ( ) ( ) 2 2 2 , 1, , 1, P P 1 P 1 i i j i i j e t i i j j f t e f λ λ λ δ − − = → × Φ = × Φ = (2) For every in i L t G E → ∈ (3) ( ) ( ) ( ) 2 2 2 , 1, , 1, P P 1 P 1 i j i j e L i i j j f L e f λ λ λ δ − − = → × Φ = × Φ = For 1 , 0 i m j n ≤ ≤ ≤ ≤ (4) ( ) ( ) 2 2 , 1, , , P P 0 i i i i e t i i j j t e λ λ δ ε − = → × Φ = (5) For every in i L t G E → ∈ (6) ( ) ( ) 2 2 , 1, , , P P 0 i i e L i i j j L e λ λ δ ε − = → × Φ = For 0 ,1 , syntactic labels on end i m j n L E ≤ ≤ ≤ ≤ ∈ (7) ( ) ( ) 2 2 , , , 1, P P 0 j j L i i j j f L f λ λ δ ε − = → × Φ = //Recurrent Step For any possible (s,t,u,v) //1 , ,1 , s t m u v n ≤ ≤ ≤ ≤ For any possible grammatical label p If ( t s ≥ and v u ≥ ) and not ( t s = and v u = ) (8) [ ] ( ) ( ) 1 1 , syntax labels on , , , , P max q r E s s t u u v q s s u u r s t u v p s t u v q s s u v r s t u u p q r p q r λ λ δ δ δ δ δ ∈ ′ ≤ ≤ ′ ≤ ≤ ′ ′ ′ ′ ′ ′ ′ ′ → × × = → × × ⎧ ⎫ ⎪ ⎪ ⎨ ⎬ ⎪ ⎪ ⎩ ⎭ //for backtracking [ ] ( ) ( ) 1 1 , syntax labels on , , , , , , , , , , , , , , , , , , , , P , arg max attached with a syntactic symbol p on E side, is constructed.",502,503
1281,15744099,"In Step (8), s' divides the substring + , with q as a possible grammatical symbol of the first part and r as a possible symbol of the second, while u' divides the substring 1 u v f f + into 1 ' u u f f + and ' 1 u v f f + .",18,19
1282,15744099,"In Step (8), s' divides the substring + , with q as a possible grammatical symbol of the first part and r as a possible symbol of the second, while u' divides the substring 1 u v f f + into 1 ' u u f f + and ' 1 u v f f + .",28,29
1283,769547,"c #x ), where w is a word, p is its POS tag, and a ""#"" symbol denotes the number of elements in each variable.",22,23
1284,15012802,"Intuitively, with rules like those shown in Figure 2 learned from a parallel corpus and a monolingual treebank, we should be able to extend a CYK-style parser to derive bilingual parse tree as shown in Figure 3 , where the symbol indicates word order of the subtrees in the target language is inverted.",44,45
1285,15012802,"However, only a lesser version, bracketing transduction grammar (BTG) with three structural labels A,B,C and a start symbol S, was experimented to perform bilingual parsing.",25,26
1286,15012802,"and backtrack by using following three steps, where S is the start symbol.",13,14
1287,15012802,"where * is the wildcard symbol and , e f are words in language , E F respectively.",5,6
1288,685945,"8 When the hypernym extraction failed, a special hypernym symbol, e.g., ""UNK"", was used.",10,11
1289,8530309,"The symbol $0 designates the changing of the pivot word in the construct pattern while $9 and $1 are the words proceeding and following the pivot word, respectively.",1,2
1290,18893263,"Σ is a finite set of source language terminal symbols, ∆ is a finite set of target language terminal symbols, V is a set of nonterminal symbols, with a designated start symbol S, and R is a set of synchronous rewrite rules.",34,35
1291,18893263,"Then, for every position in s, we decide whether it will contain a terminal or nonterminal symbol by repeated, independent draws from a Bernoulli distribution.",18,19
1292,18893263,"Since we believe that shorter rules should be relatively more likely to contain terminal symbols than longer rules, we define the probability of a terminal symbol to be φ |s| where 0 < φ < 1 is a hyperparameter.",26,27
1293,18893263,"We then determine whether each position in t is a terminal or nonterminal symbol by drawing uniformly from the bag of # NT (s) source nonterminals and |t| − # NT (s) terminal indicators, without replacement.",13,14
1294,18893263,"At this point we have created a rule template which indicates how large the rule is, whether each position contains a terminal or nonterminal symbol, and the reordering of the source nonterminals a. To conclude the process we must select the terminal types from the source and target vocabularies.",25,26
1295,1167524,"The symbol ""#get"" in the action part indicates that the value can be obtained by accessing the phrase rules or the dictionary to complete the structure recursively.",1,2
1296,1167524,"The symbol ""optional:"" indicates that the attribute-value pair behind it is optional.",1,2
1297,216868277,"Hence, we: • Propose Base-InflecTion Encoding (BITE), which uses morphological information to help the data-driven tokenizer use its vocabulary efficiently and generate robust symbol 3 sequences.",32,33
1298,216868277,Propose metrics like symbol complexity to operationalize and evaluate the vocabulary efficiency of an encoding scheme.,3,4
1299,216868277,"models represented each word as a single symbol in the vocabulary (Bengio et al.,",7,8
1300,216868277,2011) and uncommon words were represented by an unknown symbol.,10,11
1301,216868277,2016) were proposed to encode out-of-vocabulary (OOV) words by segmenting them into subwords and encoding each subword as a separate symbol.,27,28
1302,216868277,A benefit of this approach is that the neural network can now generate orthographically appropriate inflected forms by generating the base form and the corresponding inflection symbol.,26,27
1303,216868277,"Ablation Study To tease apart the effects of BITE's two components (lemmatization and inflection symbol) on task performance, we ablate the extra grammatical information from the encoding by replacing all inflection symbols with a dummy symbol (BITE abl ).",16,17
1304,216868277,"Ablation Study To tease apart the effects of BITE's two components (lemmatization and inflection symbol) on task performance, we ablate the extra grammatical information from the encoding by replacing all inflection symbols with a dummy symbol (BITE abl ).",39,40
1305,216868277,We discuss this in greater detail in Appendix B.2 and also report the pseudo log-likelihoods and per-symbol pPPLs in the spirit of transparency and reproducibility.,20,21
1306,216868277,We term this the symbol complexity.,4,5
1307,216868277,"For simplicity, we define the penalty of each extra unknown symbol to be double that of a symbol in the vocabulary.",11,12
1308,216868277,"For simplicity, we define the penalty of each extra unknown symbol to be double that of a symbol in the vocabulary.",18,19
1309,216868277,"1 To measure the symbol complexities of our vocabularies, we use WordNet's single-word lemmas (Miller, 1995) as our ""corpus"" (N = 83118).",4,5
1310,216868277,"4 , we see that training data-driven tokenizers with BITE produces vocabularies with lower symbol complexities.",16,17
1311,216868277,"It is logical that a symbol that maximizes a language model's likelihood on the training data is also semantically ""denser"", hence prioritizing such symbols produces efficient vocabularies.",5,6
1312,216868277,"Hence, the improved robustness shown in §4.1 can be directly attributed to the separation of each content word's base forms from its inflection and keeping it consistent as the inflection varies, hence mitigating any significant symbol-level changes.",39,40
1313,216868277,"Hence, we guide the data-driven tokenizer by incorporating linguistic information to learn a more efficient vocabulary and generate symbol sequences that increase the network's robustness to inflectional variation.",21,22
1314,216868277,"As a first step, we propose to evaluate an encoding scheme's efficacy by measuring its vocabulary coverage and symbol complexity (which may have interesting connections to informationtheoretic limits (Ziv and Lempel, 1978) ).",20,21
1315,216868277,We have already shown that Base-Inflection Encoding helps a data-driven tokenizer use its limited vocabulary more efficiently by reducing its symbol complexity when the combination is trained from scratch.,24,25
1316,216868277,We conduct two ablations to investigate the effects of lemmatization and inflection symbols on the models' pseudo perplexities: the first simply lemmatizes the input before encoding it with Word-Piece (WordPiece+LEMM) and the second replaces every inflection symbol generated by BITE with a dummy symbol (WordPiece+BITE abl ).,42,43
1317,216868277,We conduct two ablations to investigate the effects of lemmatization and inflection symbols on the models' pseudo perplexities: the first simply lemmatizes the input before encoding it with Word-Piece (WordPiece+LEMM) and the second replaces every inflection symbol generated by BITE with a dummy symbol (WordPiece+BITE abl ).,49,50
1318,216868277,"A reviewer pointed out that per subword symbol perplexities are not directly comparable across different subword segmentations/vocabularies, but per word perplexities are (Mielke, 2019; Salazar et al.,",7,8
1319,216868277,"However, using the same denominator would unfairly penalize models using BITE since it inevitably increases the symbol sequence length, which affects the predicted log-likelihoods.",17,18
1320,216868277,"S N ) = N i=1 (|S i | − u i ) + λu i (2) where N is the total number of word types in the evaluation corpus, S i is the sequence of symbols obtained from encoding the ith base form, u i is the number of unknown symbols in S i , and λ is the weight of the unknown symbol penalty.",69,70
1321,236478088,"Rather than replacing characters with an indiscriminate symbol ""[MASK]"", we mask characters with pinyin or similar pronounced characters.",7,8
1322,236478088,"This is caused by a prediction problem of BERT that for most of the time, BERT corrects the first character to be the period symbol (""。"").",25,26
1323,236478088,"Therefore, a lot of sentences incorrectly predicts the beginning character to be the period symbol.",15,16
1324,8989479,The user can distinguish modifiers from simple words in the prediction window because the former are suffixed with a special symbol (for example an underscore).,20,21
1325,215754246,"Following multiple transformer encoder layers, we represent each sentence s i by the output representation of the [CLS] symbol preceding s i .",21,22
1326,1426279,"For example, the tokens 10 thousand dollars in the raw training text may be replaced with a placeholder class symbol.",20,21
1327,3523412,"|r|, initial state 0, final state |r|, and the following transi-tions (a matches any symbol): For 0 ≤ i < |r|: i r i+1 :1 − −−− → i + 1 match i :0 − − → i + 1 deletion i :0 − − → i + 1 substitution For 0 ≤ i ≤ |r|: i :−1 − −−− → i insertion This automaton can be intersected with a typical stack-based phrase-based decoder lattice (Koehn, 2004a) or CKY-style shared forest (Chiang, 2007) in much the same way that a language model can, yielding a polynomial-time algorithm for extracting the best-scoring translation from a lattice or forest (Wagner, 1974) .",20,21
1328,182486653,We introduce the query symbol in the next section. (,4,5
1329,182486653,"before a word or part of speech symbol match nothing or the word/pos that follows.(e.g., """,7,8
1330,6056834,"A derivation creates a tree by starting with the root symbol and rewriting (substituting) it with an elementary tree, then continuing to rewrite frontier non-terminals with elementary trees until there are no remaining frontier non-terminals.",10,11
1331,6056834,"Specifically, we define the distribution of elementary tree e with root non-terminal symbol c as G c |α c , P 0 ∼ DP(α c , P 0 (•|c)) e|c ∼ G c where P 0 (•|c) (the base distribution) is a distribution over the infinite space of trees rooted with c, and α c (the concentration parameter) controls the model's tendency towards either reusing elementary trees or creating novel ones as each training instance is encountered (and consequently, the tendency to infer larger or smaller sets of elementary trees from the observed data).",15,16
1332,12186549,"For example, in comparison to Yu (2005) 's model, our approach (1) induces distributed representations for words, based on linguistic and visual context, and (2) operates entirely on distributed representations through similarity measures without positing a categorical level on which to learn wordsymbol/category-symbol associations.",56,57
1333,5893838,"Finally, a set of PoS symbols is defined to support queries that need more precision than the symbol "" "".",18,19
1334,195767544,"Notable features of EGG include: (a) Primitives for implementing single-symbol or variable-length communication (with vanilla RNNs (Elman, 1990) , GRUs (Cho et al.,",14,15
1335,195767544,"In turn, messages can be either single-symbol or multisymbol variable-length sequences.",9,10
1336,195767544,"Similarly, one can switch between one-symbol communication and variable-length messages with little changes in the code.",8,9
1337,195767544,"In a typical scenario, the communication method (single or multiple symbol messages) will be implemented by EGG-provided wrappers.",12,13
1338,195767544,"Suppose that Sender produces a distribution over the vocabulary, with ith symbol having probability p i = S(i s ).",12,13
1339,195767544,We treat y as a relaxed symbol representation.,6,7
1340,195767544,"In the case of single-symbol communication, the embedding of y is passed to Receiver.",6,7
1341,195767544,"In case of variable-length messages, the embedding is also fed into a RNN cell to generate the next symbol in the message.",21,22
1342,195767544,"In the case of one-symbol communication, Sender's output is passed through a softmax layer and its output is interpreted as the probabilities of sending a particular symbol.",6,7
1343,195767544,"In the case of one-symbol communication, Sender's output is passed through a softmax layer and its output is interpreted as the probabilities of sending a particular symbol.",30,31
1344,195767544,"This cell is then ""unrolled"" to generate a message, until the end-of-sequence symbol (eos) is produced or maximum length is reached.",19,20
1345,195767544,"Receiver's input is an embedding of the message: either the embedding of the single-symbol message, or the last hidden state of the RNN cell that corresponds to the eos symbol.",17,18
1346,195767544,"Receiver's input is an embedding of the message: either the embedding of the single-symbol message, or the last hidden state of the RNN cell that corresponds to the eos symbol.",34,35
1347,195767544,"To do this, we enumerate all possible 100 two-symbol messages x, y and input them to Receiver.",11,12
1348,195767544,"The eos symbol is fixed to be 0, hence if the first symbol is 0 then the second symbol is ignored (top row of Figure 5 ).",2,3
1349,195767544,"The eos symbol is fixed to be 0, hence if the first symbol is 0 then the second symbol is ignored (top row of Figure 5 ).",13,14
1350,195767544,"The eos symbol is fixed to be 0, hence if the first symbol is 0 then the second symbol is ignored (top row of Figure 5 ).",19,20
1351,195767544,"Note that the first symbol x tends to denote digit identity: x ∈ {2, 4, 7, 8, 9}.",4,5
1352,195767544,"In contrast, the second symbol y is either ignored (x ∈ {4, 8}) or specifies the style of the produced digit (x ∈ {3, 7}).",5,6
1353,195767544,"The second symbol has the most striking effect with x = 7, where y encodes the rotation angle of the digit 1.",2,3
1354,18884529,"The chunk labels typically are represented using B-I-O symbols followed by phrase type, with each symbol denoting Beginning, Inside, and Outside of the phrase.",20,21
1355,167217880,"Currently, for ease of analysis, messages are single discrete symbols selected from a vocabulary of size 10, but extension to symbol sequences is trivial (although it would of course complicate the analysis).",23,24
1356,167217880,"In the current paper, we limited ourselves to one-symbol messages, facilitating analysis but greatly reducing the spectrum of potentially emergent linguistic phenomena to study.",11,12
1357,248227526,"x n ), the input sequence can be constructed by adding specific tokens x = ([CLS], x 1 , x 2 , ..., x n , [SEP]), where [CLS] denotes the special symbol for representing the whole sequence, and [SEP] can be used for separating non-consecutive token sequences (Devlin et al.,",44,45
1358,2991153,The symbol 'H/X' denotes a predicate-object relation; 'X/H' denotes a modifier-head relation; 'H/H' denotes a conjunctive head-head relation; and 'X/X' denotes an independent relation.,1,2
1359,2991153,Table 1 shows the feature types and symbol notations.,7,8
1360,2991153,"The context feature is neighboring PoSs of Vt and N: the symbols of t -2 and t -1 represent its left PoSs, and the symbol t 3 and t 4 represent its right PoSs.",26,27
1361,2991153,"The symbol P1(%) is the 10-fold cross validation accuracy of the training data, and the symbol P2(%) is the accuracy of the test data.",1,2
1362,2991153,"The symbol P1(%) is the 10-fold cross validation accuracy of the training data, and the symbol P2(%) is the accuracy of the test data.",19,20
1363,215238861,"Given a set of atomic input elements (for example, a set of independent attribute values), each atomic symbol should refer to one and only one input element, independently of the other symbols it co-occurs with.",21,22
1364,215238861,We will refer to the lack of symbol interdependence in denoting distinct input elements as naïve compositionality.,7,8
1365,215238861,"2017) deem non-compositional those languages that either use single symbols to refer to ensembles of input elements, or where the meaning of a symbol depends on the context in which it occurs.",27,28
1366,215238861,"Havrylov and Titov (2017) looked for symbol-position combina-tions that encode a single concept in an image, as a sign of a compositional behavior.",8,9
1367,215238861,"While lang-identity is naïvely compositional (one symbol encodes one attribute only), lang-entangled is not: each symbol of an utterance encodes equal amount of information about both attributes and both symbols are equally needed for decoding each attribute.",9,10
1368,215238861,"While lang-identity is naïvely compositional (one symbol encodes one attribute only), lang-entangled is not: each symbol of an utterance encodes equal amount of information about both attributes and both symbols are equally needed for decoding each attribute.",23,24
1369,215238861,This languagetask pair mirrors the lang-identity/task-identity pair: each symbol encodes one output value.,15,16
1370,215238861,Each symbol of lang-rotated carries equal amounts of information about both coordinates of i. Task Receiver has to recover the original (nonrotated) coordinates i of a point.,1,2
1371,4460159,"For LM training, we included the 50K most frequent words in each corpus in the vocabulary, replacing the other tokens with the UNK symbol.",26,27
1372,52145734,"Earlier studies have focused on the agents' symbol usage, rather than on their representation of visual input.",8,9
1373,52145734,"However, while most studies present some analysis of the agents' symbol usage, they pay little or no attention to the representation of the visual input that the agents develop as part of their evolving interaction.",12,13
1374,52145734,"The Sender always sees the target in the left position, and it must pick one discrete symbol from a fixed vocabulary to send to the Receiver.",17,18
1375,52145734,"The Receiver sees the images in random order, together with the sent symbol, and it tries to guess which image is the target.",13,14
1376,52145734,"Lazaridou and colleagues present preliminary evidence suggesting that, indeed, agents are now developing conceptual symbol meanings.",16,17
1377,52145734,"The Sender takes image representations as input, it projects them into its own representational space, compares them, and finally outputs a probability distribution over vocabulary symbols, from which a single discrete symbol is then sampled.",35,36
1378,52145734,"The Receiver takes as input the target and distractor input image representations in random order, as well as the symbol produced by the sender (as a vocabulary-sized one-hot vector).",20,21
1379,52145734,"It embeds the images and the symbol into its own representational space, where it performs a symbol-to-image comparison, producing a probability distribution over the two images, one of which is selected by sampling from this distribution.",6,7
1380,52145734,"It embeds the images and the symbol into its own representational space, where it performs a symbol-to-image comparison, producing a probability distribution over the two images, one of which is selected by sampling from this distribution.",17,18
1381,52145734,"We then compare, pair by pair, whether the highest probability symbol changes when the target is swapped.",12,13
1382,52145734,"In both versions of the game, for more than 99% of the pairs, the symbol with highest probability changes when the target is swapped.",17,18
1383,1734281,"The former rewrites a nonterminal symbol as a string of two or three nonterminals along with an alignment, specifying the corresponding ordering of the child trees in the source and target language.",5,6
1384,1734281,"Each rule in the grammar, r i , is generated from its root symbol, z i , by first choosing a rule type t i ∈ {TERM, NON-TERM} from a Bernoulli distribution, r i ∼ Bernoulli(γ).",14,15
1385,52273813,"x T ) from a finite phrase-structure grammar with start symbol C: C → S and S | S after S | S S → V twice | V thrice | V V → D [1] opposite D [2] | D [1] around D [2] | D | U D → U left | U right | turn left | turn right U → walk | look | run | jump For each command, the corresponding target action sequence Y = (y 1 , . . . ,",12,13
1386,52273813,"In the context of analyzing RNNs, Rodriguez and Wiles (1998) found that simple RNNs can develop a symbol-sensitive counting strategy for accepting a simple (palindrome) context-free language.",20,21
1387,222272178,The system revises the sentence by replacing the symbol with an appropriate expression regarding its context.,8,9
1388,17546171,Two characters differing from each other by at most one symbol in Cangjie code were considered as strongly similar and were retained.,10,11
1389,219302896,"The symbol ""．""",1,2
1390,1392255,"Hence, it is a member of the cross-product a k ∈ E ∪{ǫ}×F ∪{ǫ}\{(ǫ, ǫ)}, where ǫ is the null character symbol.",28,29
1391,1545841,A set of PoS symbols (shown in Table 2 ) is defined to support queries that need more precision than the symbol *.,22,23
1392,589284,The transformation process is very simple involving changing the proposition to a wild part of speech symbol.,16,17
1393,219303844,Two characters which differ from each other by at most one symbol in Cangjie code are considered as strongly similar and are retained.,11,12
1394,204744108,"As shown in Figure 2 , with a special mask symbol [M] as input, it keeps collecting information from the context and target response (red lines).",10,11
1395,5743807,"Let , % = 〈ℎ % ' , ℎ % ( , … , ℎ % ) 〉 where ℎ % * denotes the phonetic symbol of ! .",26,27
1396,5743807,Let ℎ + * denotes the phonetic symbol of .,7,8
1397,14848792,"Finally, the expansion of the nonterminal symbol in the translation templates is flexible.",7,8
1398,14848792,"a translation template is comprised of three parts: a non-terminal symbol (Na) on the left-hand side, a source language template ([Na 1 ]業) in the middle, and a target language template ([Na 1 ] industry) on the righthand side.",13,14
1399,14848792,"Definition of Translation Template Based on the symbols used by Chiang (2005) , we define a translation template as follows: > →< , ,α γ X (1) where X is a left-hand side non-terminal symbol, which is usually a part-of-speech that constrains the part-of-speech of the target unknown word; γ is a translation template of the source language, and may contain terminal and non-terminal symbols; α is a translation template of the target language, and may also contain terminal and nonterminal symbols; and ~ is a one-to-one correspondence between non-terminal occurrences in γ and non-terminal occurrences in α .",45,46
1400,14848792,"Na  < [Nv 1 ]業, [Nv 1 ] industry> Na < [Nv 1 ]業, [Nv 1 ] business > Figure Then, the non-terminal symbol of each rule is expanded with the translation equivalents of the in-vocabulary word "" 出 口 "" (export) and the following translation candidates are generated by the matched translation templates (shown as Figure 3 ).",36,37
1401,236486232,Hashtags are split up and the preceding hash symbol is removed.,8,9
1402,204800442,The workers were allowed to insert the special symbol < * > in places where they could not think of a good expression for that position in their answer Y cand ja→en .,8,9
1403,1318875,"A token may be a word, punctuation symbol, or special NULL token indicating a deletion in the alignment.",8,9
1404,563931,"If a token falls outside the sentence, it is replaced by the empty token symbol nil.",15,16
1405,3666937,Beam search is ended until the endof-sentence eos symbol is generated.,10,11
1406,17101961,The symbol represents the element-wise product and ⊗ is the tensor product.,1,2
1407,5601748,"In the above productions, each nonterminal symbol stands for a pair of matched strings.",7,8
1408,5601748,which means that a symbol x in language L 1 is translated by the symbol y in language L 2 .,4,5
1409,5601748,which means that a symbol x in language L 1 is translated by the symbol y in language L 2 .,14,15
1410,5601748,"The x, y may be a null symbol e, which means there may be no counterpart string in the other language.",8,9
1411,5601748,SBTG employs only one nonterminal symbol A that can be used recursively.,5,6
1412,218973987,"The basic idea is to take a word and its transliteration, such as the Tamil version of the English word ""temple"" presented earlier (ெட பி temple) and use expectation-maximization to derive a character-bycharacter alignment, such as the following: ட:t ெ◌:e ம:m ◌்:_ ப:p ◌ி:_ ள:l ◌்:e Each symbol consists of an input side and an output side, separated by a colon, where the underscore character ( _ ) represents the empty string.",76,77
1413,61492637,"The symbol grounding problem Since DSMs represent the meaning of a symbol (a word) in terms of a set of other symbols (the words or other linguistic contexts it co-occurs with), they are subject to the lack-of-grounding criticism traditionally vented against symbolic models (Harnad 1990 ; from a philosophical perspective, the obvious reference is to the Chinese Room thought experiment of Searle 1980 ).",1,2
1414,61492637,"The symbol grounding problem Since DSMs represent the meaning of a symbol (a word) in terms of a set of other symbols (the words or other linguistic contexts it co-occurs with), they are subject to the lack-of-grounding criticism traditionally vented against symbolic models (Harnad 1990 ; from a philosophical perspective, the obvious reference is to the Chinese Room thought experiment of Searle 1980 ).",11,12
1415,61492637,"With research continuing in this direction, DSMs might be the first symbolic semantic models (or even more generally the first fully implemented large-scale computational semantic models) to truly address the symbol grounding problem.",35,36
1416,61492637,Meaning and reference The symbol grounding challenge raised by philosophers and cognitive scientists pertains to the perceptual underpinnings of our generic knowledge of concepts (you need to have seen a dog to truly grasp the meaning of the word dog).,4,5
1417,61492637,"To help reading the proof terms, we use the @ symbol to indicate the application of a function to an argument (f @a).",11,12
1418,1193152,which ignores punctuation and the root symbol.,6,7
1419,196196445,"+ + + + + + E [CLS] E𝑤 ' E𝑤′ ) E Q E Q E P E 0 E 1 E N+M+1 E [234] 567 𝑤 8 + + + E𝑤 8 E Q E N 𝑤′ ' + + + E𝑤′ ' E P E N+2 C T 1 T' M T N T' 1 ... Logistic regression layer Correct Incorrect E = ' 567 E = > 567 E [4?@] 567 E =A B 567 E = C ' 567 + E [234] DEF5 ... ... E = ' DEF5 E = > DEF5 E [4?@] DEF5 E =A B DEF5 E = C ' DEF5 [SEP] + + + E [SEP] E P E N+M+2 + T[SEP] E [4?@] 567 E [4?@] DEF5 Transformer encoder E [CLS] E 1 E' M E N E [SEP] ... ... E' 1 ... E [SEP] Figure 3 : Architecture of BERT: E represents input embedding and T i represents contextual representation of token i. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. for separating questions/passages). (",204,205
1420,7190919,"The nonterminal symbol has no specific function, such as a noun phrase.",2,3
1421,3097291,"Each cell in the CYK grid is specified by a non-terminal symbol and position: (N, x, y), spanning s x+y−1 x on the source sentence s 1 ...s J .",13,14
1422,10712436,"2.1, and annotated it with the following notations: the beginning symbol of a spatial expression #l, the end symbol #r, the spatial expression notation SE, and the sequence number of the spatial expression.",12,13
1423,10712436,"2.1, and annotated it with the following notations: the beginning symbol of a spatial expression #l, the end symbol #r, the spatial expression notation SE, and the sequence number of the spatial expression.",22,23
1424,151775,"3) Replace the tokens for E's instances with the symbol ""E"" and the type-III token containing the translation F with the symbol ""F"".",11,12
1425,151775,"3) Replace the tokens for E's instances with the symbol ""E"" and the type-III token containing the translation F with the symbol ""F"".",27,28
1426,151775,"The surface patterns P learned in the training phase are applied to match E in the tokenized summaries, to extract a token that matches the F symbol in the pattern. (",27,28
1427,17024370,"In the absence of linguistic knowledge, the system models linguistic structure using an SCFG that contains only one type of nonterminal symbol 1 .",22,23
1428,17024370,"In this paper, we pursue a different approach X → γ, α, ∼ (1) where X is the nonterminal symbol and γ and α are strings that contain the combination of lexical items and nonterminals in the source and target languages, respectively.",24,25
1429,17024370,"The ∼ symbol indicates that nonterminals in γ and α are synchronized through co-indexation; i.e., nonterminals with the same index are aligned.",2,3
1430,17024370,"Using precede symbol (≺) to indicate the rst operand immediately dominates the second operand in the hierarchical structure, the topological orderings in Fig.",2,3
1431,17161141,5 ARPAbet symbol will be used for representing phonemes.,2,3
1432,17161141,"Note that f S,GS is a symbol for indicating both f S and f GS .",8,9
1433,17161141,"f P,GP is a symbol for indicating both f P and f GP .",6,7
1434,17161141,$ is a symbol for representing the start of words.,3,4
1435,2785745,"We model the grammar as a set of distributions, G c , over the productions for each non-terminal symbol, c. We adopt a non-parametric Bayesian approach by treating each G c as a random variable with a Dirichlet process (DP) prior, r|c ∼ G c G c |α c , P 0 ∼ DP(α c , P 0 (•|c)) , where P 0 (•|c) (the base distribution) is a distribution over the infinite space of trees rooted with c, and α c (the concentration parameter) controls the model's tendency towards either reusing existing rules or creating novel ones as each training instance is encountered (and consequently, the tendency to infer larger or smaller grammars).",21,22
1436,2785745,"the TOP symbol, while the GHKM grammar instead relies on the rule (TOP S 1 ), 1 to produce the same fragment.",2,3
1437,2785745,"In a number of the remaining rules the commas were lexicalised, or S rules were extended to include the TOP symbol.",21,22
1438,11561366,"So in the case where a node and both of its children are aligned, we commit 8 rules into the grammar, as follows: LHS → RHS1 RHS2 LHS+a → RHS1 RHS2 LHS → RHS1+b RHS2 LHS+a → RHS1+b RHS2 LHS → RHS1 RHS2+c LHS+a → RHS1 RHS2+c LHS → RHS1+b RHS+c LHS+a → RHS1+b RHS2+c A category label which ends in a '+' symbol followed by a Goodman index is fragment-internal and all other nodes are either fragment roots or → N=N VP+2 0.5 S=S → N=N VP+2 0.5 S=S → N=N+3 VP+2 0.5 S=S → N=N+4 VP+2 0.5 S=S+1 → N=N VP+2 0.5 S=S+1 → N=N VP+2 0.5 S=S+1 → N=N+3 VP+2 0.5 S=S+1 → N=N+4 VP+2 0.5 N=N → John 0.5 N=N → Mary 0.5 N=N+3 → John 1 N=N+4 → Mary 1 VP+2 → V+4 N=N 0.5 VP+2 → V+5 PP+3 1 VP+2 → V+4 N=N+5 0.5 V+5 → plaît 1 V+4 → likes 1 PP+3 → P+6 N=N 0.5 N=N → Mary 0.5 PP+3 → P+6 N=N+7 0.5 N=N+5 → Mary 1 P+6 → à 1 N=N → John 0.5 N=N+7 → John 1 Figure 2 : A parallel tree and its corresponding Goodman reduction.",68,69
1439,763080,The following examples are two ITG productions: C -> [A B] C -> <A B> Each nonterminal symbol stands for a pair of matched strings.,24,25
1440,763080,There are also lexical productions of the following form in ITG: A -> x/y This means that a symbol x in language L 1 is translated by the symbol y in language L 2 .,22,23
1441,763080,There are also lexical productions of the following form in ITG: A -> x/y This means that a symbol x in language L 1 is translated by the symbol y in language L 2 .,32,33
1442,763080,"x or y may be a null symbol e, which means there may be no counterpart string on other side of the bitext.",7,8
1443,763080,A Statistical BTG (SBTG) grammar is as follows: j b i b j i b a a v e A e u A v u A AA A AA A ej ie ij / ; / ; / ; ]; [ →  →  →  > < →  →  SBTG employs only one nonterminal symbol A that can be used recursively.,62,63
1444,12307380,"Each MR production consists of a semantic category (""QUERY""), a function symbol (""answer"") which can be optionally omitted, as well as an argument list which possibly contains  Now we give a brief overview of the hybrid tree framework and the LNLZ08 system that was presented in Lu et al. (",16,17
1445,12307380,"In the table, m refers to the MR production, the symbol w denotes an NL word sequence and is optional if it appears inside [].",12,13
1446,12307380,The symbol Y and Z refer to the first and second semantic category under the MR production m respectively. #,1,2
1447,7671334,"The correlation coefficient between variable X and Y is defined as Y Xs s Y X COV Y X r ) , ( ) , ( = (7) where COV(X,Y) is the covariance defined by ∑ − − − = ) )( ( 1 1 ) , ( Y Y X X n Y X COV i i (8) The symbol meanings are as follows: sX: sample standard deviation of variable X sY: sample standard deviation of variable Y n: sample size Xi (Yi) : the ith component of variable X (Y) X ( Y ): the sample mean of variable X (Y) From its definition, we know that the correlation coefficient is scale-independent and 1 1 ≤ ≤ − r .",67,68
1448,390966,"On the RHS of the gapping rules, a diamond symbol ( ) indicates a gap, while on the LHS, it emits a superscripted symbol X to indicate a gapped phrase (plain Xs without superscripts are thus contiguous phrases).",10,11
1449,390966,"On the RHS of the gapping rules, a diamond symbol ( ) indicates a gap, while on the LHS, it emits a superscripted symbol X to indicate a gapped phrase (plain Xs without superscripts are thus contiguous phrases).",26,27
1450,390966,"When scoring entries, we treat gapped entries as contiguous phrases by ignoring the gap symbol and rely on the orientation model to penalize such entries.",15,16
1451,5741058,All words were converted to lowercase and numbers were replaced with the symbol num .,12,13
1452,189928395,"As shown in Figure 1 , the task-specific operation f θo consists of m o layers of CNNs that map the shared representations {h s 1 , ..., h s n } to {h o 1 , ..., h o n }, an attention layer att o , and a decoding layer dec o , where o ∈ {ds, dd} is the task symbol.",73,74
1453,249394694,"Note that y J denotes the special end-of-sentence symbol ⟨eos⟩. The encoder first maps a source sentence x into a sequence of word embeddings e(x) = e(x 1 ), ..., e(x I ), where e(x) ∈ R d×I , and d is the embedding dimension.",12,13
1454,249394694,"The word embeddings are then encoded to the corresponding hidden representations h. Similarly, the decoder maps a shifted copy of the target sentence y, i.e., ⟨bos⟩, y 1 , ..., y J−1 , into a sequence of word embeddings e(y) = e(⟨bos⟩), e(y 1 ), ..., e(y J−1 ), where ⟨bos⟩ denotes a special beginning-of-sentence symbol, and e(y) ∈ R d×J .",70,71
1455,6871864,A derivation creates a tree by recursive substitution starting with the root symbol and finishing when there are no remaining frontier nonterminals.,12,13
1456,6871864,"We define the distribution over elementary trees e with root nonterminal symbol c as G c |α c , P 0 ∼ DP(α c , P 0 (•|c)) e|c ∼ G c where P 0 (•|c) (the base distribution) is a distribution over the infinite space of trees rooted with c, and α c (the concentration parameter) controls the model's tendency towards either reusing elementary trees or creating novel ones as each training instance is encountered.",11,12
1457,219309499,"The symbol ',' in the rules denotes an 'or' relation.",1,2
1458,16391184,An edge's output label may contain mixtures of terminal symbol yields and positions indicating where a child node's yield should be substituted.,10,11
1459,1933463,"That is, for each symbol A x in the fine grammar, there is some symbol A in the coarse grammar.",5,6
1460,1933463,"That is, for each symbol A x in the fine grammar, there is some symbol A in the coarse grammar.",16,17
1461,1933463,"Here, the rules of the grammar are clustered by their coarse parent symbol.",13,14
1462,1933463,"We then have multiple work queues, with parse items only being enqueued if the span (i, j) allows that symbol in its pruning mask.",23,24
1463,1933463,"Instead, we take advantage of the partitioned structure of the grammar and organize our computation around the coarse symbol set.",19,20
1464,1933463,"Spreading symbols across clusters may be inefficient: if a parse item licenses a given symbol, we will have to enqueue that item to any queue that has the symbol in its signature, no matter how many other symbols are in that cluster.",15,16
1465,1933463,"Spreading symbols across clusters may be inefficient: if a parse item licenses a given symbol, we will have to enqueue that item to any queue that has the symbol in its signature, no matter how many other symbols are in that cluster.",30,31
1466,1933463,We use a very simple method: we cluster the rules in the grammar by coarse parent symbol.,17,18
1467,16138979,"Here a translation skeleton is a target string where all segments of non-skeleton translation are generalized to a symbol X. E.g., in Figure 1 , the trans-lation skeleton is 'the cost X has been further reduced X .',",20,21
1468,3107882,"For example, to PPs usually attach to verbs and of PPs usually attach to nouns, but a context-free PP symbol can equally well attach to either.",23,24
1469,3107882,"In past work that has used tree-structured CRFs in this way, increased accuracy partially came from decorating trees T with additional annotations, giving a tree T over a more complex symbol set.",34,35
1470,202780331,"x n , 0 Goal S 0 •, ∅, 0 Generate(x) S,•x i |B,n S|x i •,B,n Open(N) S|S 0 •,B,n S|•N |S 0 •,B,n+1 Close S|•N i |...|S 0 •,B,n S|N •,B,n−1 Each Open action is parametrized by a nonterminal symbol, thus there are as many nonterminals in the set A of actions as there are nonterminal symbols.",59,60
1471,18110049,"The techniques we introduce include grammar compilation, recursive symbol blocking, and cache-sharing.",9,10
1472,18110049,"Maximize use of registers for symbol scores, and minimize use of shared memory (in fact we will not use it at all).",5,6
1473,18110049,"The two-dimensional work arrays must be stored in ""symbol-major"" order for this to work.",11,12
1474,18110049,"That is, the parent VP for one work item is stored next to the parent VP for the next work item, while the VP symbol for the first work item is stored on the next ""row"" of the work array.",26,27
1475,18110049,"The reason the workspace cells are stored in ""symbol-major"" order is to maximize coalesced access: each thread in the SMX accesses the same symbol for a different work item in parallel, and those work items are in consecutive memory locations.",9,10
1476,18110049,"The reason the workspace cells are stored in ""symbol-major"" order is to maximize coalesced access: each thread in the SMX accesses the same symbol for a different work item in parallel, and those work items are in consecutive memory locations.",28,29
1477,18110049,"That is, for a given span, the NP symbol is next to the same span's VP symbol (for example).",10,11
1478,18110049,"That is, for a given span, the NP symbol is next to the same span's VP symbol (for example).",19,20
1479,18110049,This order accelerates both symbol loading and Viterbi search later on.,4,5
1480,18110049,"At the heart of our approach is the use of grammar compilation and symbol/rule blocking, described next.",13,14
1481,18110049,"Grammar Compilation Each rule in a probabilistic context-free grammar can be evaluated with an update of the form: S ij,m = k=1...j; n,p∈Q S ik,n S (k+1)j,p c mnp (2) where S ij,m is the score for symbol m as a generator of the span of words from position i to j in the input sentence, c mnp is the probability that symbol m generates the binary symbol pair n, p, and Q is the set of symbols.",56,57
1482,18110049,"Grammar Compilation Each rule in a probabilistic context-free grammar can be evaluated with an update of the form: S ij,m = k=1...j; n,p∈Q S ik,n S (k+1)j,p c mnp (2) where S ij,m is the score for symbol m as a generator of the span of words from position i to j in the input sentence, c mnp is the probability that symbol m generates the binary symbol pair n, p, and Q is the set of symbols.",82,83
1483,18110049,"Grammar Compilation Each rule in a probabilistic context-free grammar can be evaluated with an update of the form: S ij,m = k=1...j; n,p∈Q S ik,n S (k+1)j,p c mnp (2) where S ij,m is the score for symbol m as a generator of the span of words from position i to j in the input sentence, c mnp is the probability that symbol m generates the binary symbol pair n, p, and Q is the set of symbols.",87,88
1484,18110049,"The scores will be stored in a CKY chart indexed by the span ij and the symbol m. To evaluate (2) as fast as possible, we want to use register variables which are limited in number.",16,17
1485,18110049,Note that we show here the sum-product code for computing inner/outer symbol probabilities.,15,16
1486,18110049,"Loads are performed as late as possible, that is, a load instruction will immediately precede the first use of a symbol: float R031 = right[tid+65 * stride]; P001 += L001 * R031 * 1.338202e-001f; where tid is the thread ID plus an offset, and stride is the row dimension of the workspace (typically 8192), and right is the main memory array of right symbol scores.",22,23
1487,18110049,"Loads are performed as late as possible, that is, a load instruction will immediately precede the first use of a symbol: float R031 = right[tid+65 * stride]; P001 += L001 * R031 * 1.338202e-001f; where tid is the thread ID plus an offset, and stride is the row dimension of the workspace (typically 8192), and right is the main memory array of right symbol scores.",74,75
1488,18110049,"In hindsight, this is obvious because the symbols in this grammar are splits of base symbols, and so splits of the parent symbol will be involved in rules with each pair of L,R splits.",24,25
1489,18110049,i.e. the compiler converts P008 += L041 * R008 * 6.200769e-001f; P009 += L041 * R008 * 6.201930e-001f; P010 += L041 * R008 * 6.202160e-001f; into float LRtmp = L041 * R008; P008 += LRtmp * 6.200769e-001f; P009 += LRtmp * 6.201930e-001f; P010 += LRtmp * 6.202160e-001f; and inspection of the resulting assembly code shows that each rule is compiled into a single fused multiply-add of LRtmp and a value from constant memory into the P symbol register.,91,92
1490,18110049,"The re-use rate is determined by the number of rules in which a particular symbol occurs, which is actually very high (more than 1000 on average).",16,17
1491,18110049,"Each symbol in a major cube will be loaded just once from main memory, but loaded into (up to) 4 different SMXes through the L2 cache.",1,2
1492,18110049,"Each symbol would need to be loaded 55 2 = 3025 times, there would be almost no symbol re-use.",1,2
1493,18110049,"Each symbol would need to be loaded 55 2 = 3025 times, there would be almost no symbol re-use.",18,19
1494,18110049,"Instead, we use a rule partitioning scheme that creates as small a symbol footprint as possible in each cube.",13,14
1495,18110049,Before describing the spectral method we mention an optimization that drops the symbol count by 2.,12,13
1496,18110049,XT and TX kernels (with one terminal and one non-terminal symbol) are called only O(s 2 ) times since one of L or R must be at the base of the chart.,13,14
1497,18110049,Spectral Partitioning We explored a number of partitioning schemes for both symbol and rule partitioning.,11,12
1498,18110049,In the end we settled on a spectral symbol partitioning scheme.,8,9
1499,18110049,Each symbol is a node in the graph to be partitioned.,1,2
1500,18110049,"In the end the vector for a particular P symbol is a = (a 1 , 0.1 * a 2 , 0.1 * a 3 ) where a 1 is a vector whose elements are indexed by L, R pairs and whose values represent the number of rules involving both those symbols (and the parent symbol P), a 2 encodes L symbols and counts the number of rules containing that L symbol and P, and a 3 encodes the R symbols and counts rules containing that R symbol and P. This feature vector produces a high similarity between P symbols that exactly share many L,R pairs and lower similarity for shared L and R. A spectral clustering/partitioning algorithm approximately minimizes the total edge weight of graph cuts.",9,10
1501,18110049,"In the end the vector for a particular P symbol is a = (a 1 , 0.1 * a 2 , 0.1 * a 3 ) where a 1 is a vector whose elements are indexed by L, R pairs and whose values represent the number of rules involving both those symbols (and the parent symbol P), a 2 encodes L symbols and counts the number of rules containing that L symbol and P, and a 3 encodes the R symbols and counts rules containing that R symbol and P. This feature vector produces a high similarity between P symbols that exactly share many L,R pairs and lower similarity for shared L and R. A spectral clustering/partitioning algorithm approximately minimizes the total edge weight of graph cuts.",58,59
1502,18110049,"In the end the vector for a particular P symbol is a = (a 1 , 0.1 * a 2 , 0.1 * a 3 ) where a 1 is a vector whose elements are indexed by L, R pairs and whose values represent the number of rules involving both those symbols (and the parent symbol P), a 2 encodes L symbols and counts the number of rules containing that L symbol and P, and a 3 encodes the R symbols and counts rules containing that R symbol and P. This feature vector produces a high similarity between P symbols that exactly share many L,R pairs and lower similarity for shared L and R. A spectral clustering/partitioning algorithm approximately minimizes the total edge weight of graph cuts.",76,77
1503,18110049,"In the end the vector for a particular P symbol is a = (a 1 , 0.1 * a 2 , 0.1 * a 3 ) where a 1 is a vector whose elements are indexed by L, R pairs and whose values represent the number of rules involving both those symbols (and the parent symbol P), a 2 encodes L symbols and counts the number of rules containing that L symbol and P, and a 3 encodes the R symbols and counts rules containing that R symbol and P. This feature vector produces a high similarity between P symbols that exactly share many L,R pairs and lower similarity for shared L and R. A spectral clustering/partitioning algorithm approximately minimizes the total edge weight of graph cuts.",93,94
1504,18110049,"Since many symbols are involved, this typically does not happen to an in-dividual symbol, but this heuristic is successful at making the individual symbol or LR pair distributions across the cuts as unbalanced as possible.",16,17
1505,18110049,"Since many symbols are involved, this typically does not happen to an in-dividual symbol, but this heuristic is successful at making the individual symbol or LR pair distributions across the cuts as unbalanced as possible.",27,28
1506,18110049,i.e. one side of the cut has very few instances of a given symbol.,13,14
1507,18110049,"The number of instances of a symbol is an upper bound on the number of subcells in which than symbol occurs, and therefore on the number of times it needs to be loaded from memory.",6,7
1508,18110049,"The number of instances of a symbol is an upper bound on the number of subcells in which than symbol occurs, and therefore on the number of times it needs to be loaded from memory.",19,20
1509,18110049,"Repeating this operation recursively to produce a 3d cell decomposition also concentrates each symbol in relatively few cells, and so tends to reduce the total register count per cell.",13,14
1510,18110049,"In a bit more detail, from the vectors a above we construct a matrix A whose columns are the feature vectors for each P symbol.",25,26
1511,18110049,"Partitioning is applied in order P, L, R to generate the major cubes of the rule/symbol partition, and then again to generate minor cubes.",19,20
1512,18110049,"Most importantly the reload rate (the mean number of major cells containing a given symbol, or the mean number of times a symbol needs to be reloaded from main memory) drops to about 6 (vs. 3000 for naive partitioning).",15,16
1513,18110049,"Most importantly the reload rate (the mean number of major cells containing a given symbol, or the mean number of times a symbol needs to be reloaded from main memory) drops to about 6 (vs. 3000 for naive partitioning).",24,25
1514,18110049,"Each symbol is used on average 343, 000/501 ≈ 6000 times overall by the XX ker-nel.",1,2
1515,18110049,"Dropping the reload factor to 6 means that for every 1 main memory load of a symbol, there are approximately 1000 register or L2 cache reuses.",16,17
1516,18110049,The symbol for each node can obviously be stored with the height.,1,2
1517,18110049,"For unaries, we require exactly one unary rule per node, with the possibility that it is the identity rule, and so we store two nodes: one for the ""pre-unary"" symbol, and one for the ""post-unary."" (",37,38
1518,18110049,"First, the symbol of the root is recorded.",3,4
1519,18110049,Each thread block includes a number of threads which are used to rapidly (in partly parallel fashion) iterate through rulesets and symbol vectors for the BestBinary and BestUnary operations using coalesced memory accesses.,23,24
1520,18110049,"Then the thread block parallel-iterates through the rules for the current parent symbol, which will be in a contiguous block of memory since the rules are sorted by parent symbol, and again are coalesced.",14,15
1521,18110049,"Then the thread block parallel-iterates through the rules for the current parent symbol, which will be in a contiguous block of memory since the rules are sorted by parent symbol, and again are coalesced.",32,33
1522,18110049,"The thread block therefore needs storage for all the L, R symbol scores and in addition working storage proportional to the number of threads (to hold the best child symbol and its score from each thread).",12,13
1523,18110049,"The thread block therefore needs storage for all the L, R symbol scores and in addition working storage proportional to the number of threads (to hold the best child symbol and its score from each thread).",31,32
1524,18110049,"However, since symbol variables correspond almost one-to-one with registers (modulo lifetime overlap and reuse, which our code generator is slightly better at than the compiler), there is no reason for our code generator not to generate assembly code directly.",3,4
1525,2388321,"Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the naïve approach.",19,20
1526,2388321,"When multi-part annotations are used in the same grammar, systems have generally multiplied these annotations together, in the sense that an NP that was definite, possessive, and VP-dominated would have a single unstructured PCFG symbol that encoded all three facts.",42,43
1527,2388321,"In addition, modulo backoff or smoothing, that unstructured symbol would often have rewrite parameters entirely distinct from, say, the indefinite but otherwise similar variant of the symbol (Klein and Manning, 2003) .",10,11
1528,2388321,"In addition, modulo backoff or smoothing, that unstructured symbol would often have rewrite parameters entirely distinct from, say, the indefinite but otherwise similar variant of the symbol (Klein and Manning, 2003) .",30,31
1529,2388321,"This parser starts from a grammar with labels annotated with sibling and parent information, and then adds specific annotations, such as whether an NP is possessive or whether a symbol rewrites as a unary.",31,32
1530,2388321,"Here, each symbol is given a latent annotation, referred to as a substate.",3,4
1531,2388321,"Here, i A j is a symbol representing building the base symbol A over the span [i, j] .",7,8
1532,2388321,"Here, i A j is a symbol representing building the base symbol A over the span [i, j] .",12,13
1533,2388321,"First, we compute the expected number of times the rule i A j → i B k k C j occurs, and then then we locally normalize for each symbol i A j .",31,32
1534,2388321,"We examined development performance for training and inference on a small product of two parsers, each with two latent states per symbol.",22,23
1535,7042984,"Thus, we extend the conventional transition actions illustrated in Table 1 to a new set of transition actions for the parsing, denoted by 𝐴 ̂: 𝐴 ̂= {𝐿𝑒𝑓𝑡𝐴𝑟𝑐, 𝑅𝑒𝑑𝑢𝑐𝑒} ∪ {𝑆ℎ𝑖𝑓𝑡(𝑠)|𝑠 ∈ 𝑄} ∪ {𝑅𝑖𝑔ℎ𝑡𝐴𝑟𝑐(𝑠)|𝑠 ∈ 𝑄} where Q is the set of punctuation symbols to be predicted, 𝑠 is a punctuation symbol belonging to Q, Shift(s) is an action that attaches s to the current word on the basis of original Shift action in parsing, RightArc(s) attaches 𝑠 to the current word on the basis of original RightArc action.",63,64
1536,18731469,"For convenience of reference, we repeat the features in Table 1 , where the symbol S i represents the i th item from the top of the stack S and the symbol Q i denotes the i th item from the front end of the queue Q. The symbol w represents the lexical head for an item; c represents the label for an item; and t denotes POS of a lexical head.",15,16
1537,18731469,"For convenience of reference, we repeat the features in Table 1 , where the symbol S i represents the i th item from the top of the stack S and the symbol Q i denotes the i th item from the front end of the queue Q. The symbol w represents the lexical head for an item; c represents the label for an item; and t denotes POS of a lexical head.",32,33
1538,18731469,"For convenience of reference, we repeat the features in Table 1 , where the symbol S i represents the i th item from the top of the stack S and the symbol Q i denotes the i th item from the front end of the queue Q. The symbol w represents the lexical head for an item; c represents the label for an item; and t denotes POS of a lexical head.",49,50
1539,18731469,"Mistakes in cases 4-5 are caused by wrong choices of labels where the symbols X, X 1 , and X 2 refer to treebank phrase labels and the symbol X * denotes a temporary label which is introduced when a constituent with label X is binarized.",31,32
1540,18731469,Here the symbol w represents a word and the symbol t represents a POS tag.,2,3
1541,18731469,Here the symbol w represents a word and the symbol t represents a POS tag.,9,10
1542,18731469,"Specifically, the symbol L specifies w 1 to be the head and the symbol R designates w 3 to be the head.",3,4
1543,18731469,"Specifically, the symbol L specifies w 1 to be the head and the symbol R designates w 3 to be the head.",14,15
1544,41550072,"In decoding, we maintain a stack to track the parsing configuration, and our model terminates once the Word-RNN predicts a special ending symbol EOS and all the words in the stack have been reduced.",26,27
1545,12448591,"S 1 , S 1 ⇒ S 2 X 3 , S 2 X 3 ⇒ X 4 X 3 , X 4 X 3 ⇒ NP-NP 5 VBD 6 X 3 , NP-NP 5 VBD 6 X 3 ⇒ NP 7 for NP-NP 8 VBD 6 X 3 , NP-NP 8 de NP 7 VBD 6 X 3 ⇒ A request for NP-NP 8 VBD 6 X 3 , NP-NP 8 de shenqing VBD 6 X 3 ⇒ A request for a purchase of shares VBD 6 X 3 , goumai gufen de shenqing VBD 6 X 3 ⇒ A request for a purchase of shares was X 3 , goumai gufen de shenqing bei X 3 ⇒ A request for a purchase of shares was made, goumai gufen de shenqing bei dijiao In these rules, the left-hand nonterminal symbol X can not match any nonterminal symbol on the righthand side.",156,157
1546,12448591,"S 1 , S 1 ⇒ S 2 X 3 , S 2 X 3 ⇒ X 4 X 3 , X 4 X 3 ⇒ NP-NP 5 VBD 6 X 3 , NP-NP 5 VBD 6 X 3 ⇒ NP 7 for NP-NP 8 VBD 6 X 3 , NP-NP 8 de NP 7 VBD 6 X 3 ⇒ A request for NP-NP 8 VBD 6 X 3 , NP-NP 8 de shenqing VBD 6 X 3 ⇒ A request for a purchase of shares VBD 6 X 3 , goumai gufen de shenqing VBD 6 X 3 ⇒ A request for a purchase of shares was X 3 , goumai gufen de shenqing bei X 3 ⇒ A request for a purchase of shares was made, goumai gufen de shenqing bei dijiao In these rules, the left-hand nonterminal symbol X can not match any nonterminal symbol on the righthand side.",163,164
1547,808347,"Each of the text and hypothesis is a semantic dependency graph; n(h) is the set of nodes (words) and e(h) is the set of edges (grammatical relations) in a hypothesis h. An alignment a : n(h) → n(t) ∪ {null} maps each hypothesis word to a text word or to a null symbol, much like an IBM-style machine translation model.",63,64
1548,6039651,"or ending with "".org"", "".com"" are converted to a ""#URL"" symbol • Repeated punctuations such as ""!!!!""",18,19
1549,202780116,"We first introduce three symbols A, B and w. The symbol A refers to a placeholder for the left sub-tree (rooted by the left child node), and similarly B is a placeholder for the right sub-tree.",11,12
1550,202780116,The symbol w refers to a contiguous sequence of (1 or more) words.,1,2
1551,2037025,"There are two non-terminals in the grammar except the start symbol S: Y and Z. The general derivation rules are defined as follows: a) Derivations from non-terminal to nonterminals are restricted to binary branching forms; b) Any non-terminals that derives a list of terminals, or any combination of two non-terminals, if the resulting source string won't cause any cross-bracketing problems in the source parse tree (it exactly corresponds to a linguistic phrase in binary parse trees), are reduced to Y; c) Otherwise, they are reduced to Z. Table 1 shows a complete list of derivation rules in our synchronous context grammar.",12,13
1552,10044222,These solutions share the same property that a specific empty symbol on the target language side is posited and any source word is allowed to translate into .,10,11
1553,10044222,This symbol is invisible in every module of the decoder except the translation model.,1,2
1554,10361562,"Baseline Features Our baseline features are adopted from Zhang and Clark (2009) , and are shown in Table 1 Here s i represents the i th item on the top of the stack S and q i denotes the i th item in the front end of the queue Q. The symbol w denotes the lexical head of an item; the symbol c denotes the constituent label of an item; the symbol t is the POS of a lexical head.",53,54
1555,10361562,"Baseline Features Our baseline features are adopted from Zhang and Clark (2009) , and are shown in Table 1 Here s i represents the i th item on the top of the stack S and q i denotes the i th item in the front end of the queue Q. The symbol w denotes the lexical head of an item; the symbol c denotes the constituent label of an item; the symbol t is the POS of a lexical head.",64,65
1556,10361562,"Baseline Features Our baseline features are adopted from Zhang and Clark (2009) , and are shown in Table 1 Here s i represents the i th item on the top of the stack S and q i denotes the i th item in the front end of the queue Q. The symbol w denotes the lexical head of an item; the symbol c denotes the constituent label of an item; the symbol t is the POS of a lexical head.",75,76
1557,10361562,"2 From the dependency trees, we extract bigram lexical dependencies w 1 , w 2 , L/R where the symbol L (R) means that w 1 (w 2 ) is the head of w 2 (w 1 ).",22,23
1558,10361562,"Here the symbol s i denotes a stack item, q i denotes a queue item, w represents a word, and t represents a POS tag.",2,3
1559,53080607,"Initially, S 1 is empty and B 1 contains the whole input sentence and a end-of-sentence symbol at the end.",21,22
1560,52916493,"2) To conveniently determine the ending of our transition process, we add an auxiliary symbol $ to each sentence.",16,17
1561,52916493,The last action is to shift the symbol $.,7,8
1562,52916493,"If one of the elements in the stack is temporary, say X*, which means it is not finished, then last terminal symbol $ cannot be shifted until it is reduced to X.",25,26
1563,2076914,"We then extract from the obtained snippets the patterns of the form 'Prefix t 1 Infix t 2 Postfix', where Prefix and Postfix should contain either a punctuation symbol or 1-3 words.",31,32
1564,5337047,An atomic formula or atom is a predicate symbol applied to a tuple of terms.,8,9
1565,2302492,"We also remove tweets marked as retweets (using the RT sign, a standard Twitter symbol to indicate that this tweet was written by a different user).",16,17
1566,16088818,"Optionally, the embeddings for the query symbol representations and question words are initialised and/or fine-tuned during training, as discussed in §3.4.",7,8
1567,5563288,It is also useful for our purposes to view this quantity as the sum of the probabilities of all parses of the sentence which have S as the start symbol.,29,30
1568,5563288,"Intuitively, the algorithm is given a nonterminal symbol, such as S or NP, and a span of words, and has to decide (a) what rule to apply to expand the non-terminal, and (b) where to split the span of words, so that each non-terminal resulting from applying the rule has an associated word span, and the process can repeat.",8,9
1569,218538467,"The Single-Encoder Approach The input of the single-encoder system is the concatenation of the context sentences and the current sentence, with a special symbol inserted to distinguish them (Tiedemann and Scherrer, 2017; Agrawal et al.,",28,29
1570,18324297,"2 Approximate inference for PCFGs Definitions A PCFG is a tuple (T, N, S, R, θ), where T , N , R and θ are the finite sets of terminals, nonterminals, rules and parameters respectively, and S ∈ N is the start symbol.",52,53
1571,18324297,"2005) , where each nonterminal symbol is augmented with hidden variables (or subtypes).",6,7
1572,14434979,len(v) − 1)C(v) can be seen as the number of characters reduced by replacing v with a nonterminal symbol.,20,21
1573,14434979,"Once v is obtained, we replace all occurrences of v with a new non-terminal symbol.",17,18
1574,222208874,We also use label-specific beginningof-sentence (BOS) tokens as the initial symbol fed to the decoder RNN.,16,17
1575,222141642,"So, we create new datasets: for any given sentence, we randomly replace a token x with an unknown word symbol ""UNK"" with probability α.",22,23
1576,207847493,"Then, the energy term is: E TLM (y) = − T +1 t=1 log y t y t (8) where y 0 is the start-of-sequence symbol and y T +1 is the end-of-sequence symbol.",34,35
1577,207847493,"Then, the energy term is: E TLM (y) = − T +1 t=1 log y t y t (8) where y 0 is the start-of-sequence symbol and y T +1 is the end-of-sequence symbol.",46,47
1578,189999040,"Character-by-character generation The first component model, p char (s t | h t ), generates s t by sampling a sequence of characters from a LSTM language model over Σ and a two extra special symbols, an end-of-word symbol /W / ∈ Σ and the end-of-sequence symbol /S discussed above.",50,51
1579,189999040,"Character-by-character generation The first component model, p char (s t | h t ), generates s t by sampling a sequence of characters from a LSTM language model over Σ and a two extra special symbols, an end-of-word symbol /W / ∈ Σ and the end-of-sequence symbol /S discussed above.",62,63
1580,189999040,The end-of-sequence symbol can never be generated in the initial position.,6,7
1581,44161048,We extract these nested entities based on symbol * appeared 'lex' attribute which  is an connection indicator of the separated texts in discontinuous entities.,7,8
1582,44161048,"Unfortunately, there are some inconsistent cases such as ""c-fos and c-jun transcripts"" where symbol * should be in the 'lex' attribute as the discontinuous entity ""c-fos transcript"" is connected by ""c-fos"" and ""transcript"" while ""c-jun transcript"" is connected by ""c-jun"" and ""transcript"".",20,21
1583,11473733,"We then picked up words that only appear once in the training dataset and replaced 1% of them with a symbol word ""UNK"" randomly.",21,22
1584,91184639,"Convolutional Neural Networks CNNs are frequently used in NLP to extract features based on symbol subsequences, whether words or characters (Collobert et al.,",14,15
1585,91184639,CNNs use filters that are applied to symbol sequences and are typically followed by some sort of pooling operation.,7,8
1586,9873374,"Patterns and Sentences Assume we have a global vocabulary of atomic symbols, containing the reserved substitution symbol ∇. Define a pattern as a sequence of symbols.",17,18
1587,9873374,"∀0 ≤ i ≤ k : [β i , γ i ] ∈ spans(τ )} We will represent a tree as a spanmap, defined as a function that maps spans to symbol sequences.",34,35
1588,11610652,"Learning the trigram model The learning of the language model is based on counts of the corpus, assigning a special symbol, ""u/k"" (unknown) for all words that do not appear in the corpus.",21,22
1589,11610652,"The value of the symbol ""u/k"" was observed to be significant.",4,5
1590,18403563,"In other words, a non-terminal symbol in an SRCG (CFG) derivation can dominate a subset (substring) of terminals in an input string.",8,9
1591,18403563,"Formalism An SRCG G is a tuple (N, T, V, P, S), with finite sets of non-terminals (N ), terminals (T ) and variables (V ), with a start symbol S ∈ N .",43,44
1592,18403563,"By definition, the start symbol has arity 1.",5,6
1593,18403563,"A string w is within the language defined by a particular SRCG iff the start symbol S, instantiated with the exhaustive range (0, w n ), derives .",15,16
1594,18403563,"A random string in the language of the grammar can then be obtained through a generative procedure that begins with the start symbol S and iteratively expands it until deriving : At each step for some current symbol A, a rewrite rule r is sampled randomly from P A in accordance with the distribution over rules and used to expand A. This procedure terminates when no further expansions are possible.",22,23
1595,18403563,"A random string in the language of the grammar can then be obtained through a generative procedure that begins with the start symbol S and iteratively expands it until deriving : At each step for some current symbol A, a rewrite rule r is sampled randomly from P A in accordance with the distribution over rules and used to expand A. This procedure terminates when no further expansions are possible.",37,38
1596,18403563,"Data sets Our models are unsupervised and therefore learn from raw text, but their evaluation requires annotated data as a gold-standard and these were derived 6 as follows: Arabic (MSA) We created the dataset BW by synthesising 50k morphotactically correct word types from the morpheme lexicons and consistency rules supplied with the Buckwalter Arabic Morphological 5 Including the arity as part of the non-terminal symbol names forms part of our convention here to ensure that the grammar contains no cycles, a situation which would complicate inference under PYSRCAG.",72,73
1597,29626007,Singleton tokens are replaced with an unknown word symbol with probability 0.5 during training.,8,9
1598,15271865,"For the semantic tuples in Table 2 , our modified RNN generates the target sentence word by word, until meets the end symbol character: ""I want a white 4G cellphone with a big screen."".",23,24
1599,2162684,Every constituent (except of the start symbol) is given a unique label.,7,8
1600,2162684,"For example, in the WSJ Penn Treebank noun phrases are annotated with the symbol NP, but there is no distinction between subject and object NPs.",14,15
1601,2162684,"Two sentence level constituents are usually used: one for the root symbol at the top (which was not counted), and one real symbol (in WSJ10 it is usually, but not always, S), which was counted.",12,13
1602,2162684,"Two sentence level constituents are usually used: one for the root symbol at the top (which was not counted), and one real symbol (in WSJ10 it is usually, but not always, S), which was counted.",26,27
1603,11171811,We add a special start and stop symbol to the path before computing the bigram features.,7,8
1604,55383367,"classifier symbol beats the balanced model trained on extended data, CL +b M E , using the same feature set, F All .",1,2
1605,1458921,We will use the same symbol but we will use the prefix notation.,5,6
1606,6879435,"c z ], plus a sentinel symbol $ marking either the left or the right boundary of the word, depending on the direction of the model.",7,8
1607,209387614,"Another important idea that underlies the current approach is the notion of subword, whose members include anything from a single letter or symbol to a full-fledged word.",23,24
1608,245855667,"For more noisy corpora (e.g. ParaCrawl), we added hard filtering rules on special symbol, digital number, word length, punctuation number, HTML tags.",16,17
1609,2939658,"A special symbol (↑) indicates that the next node in the R1 A B Z W R2 X Y B Z W B Z W weight: w 1 weight: w 2 weight: w 3 R1, A, ↑, B, Z, ↑, W R2, X, ↑, Y, ↑, B, Z, ↑, W B, Z, ↑, W R1 w 1 A ↑ W X R2 w 2 Z Y B w 3 1,1 1,2 2,1 1,1 1,1 1,- 1,1 1,3 3,1 1,2 1 2 Figure 2: Fragment indexing.",2,3
1610,2939658,"Overall, about 2/3 of the fragments contain at least some terminal symbol (i.e. words), generally a preposition or an adverb.",12,13
1611,10480989,"We construct the predictive distribution by attending to the words in the source sentences: h c j =f com (h c j−1 , c j−1 ) (2) u j (i) =w T 3 tanh(W 1 h c j +W 2 h e i ) (3) q φ (c j |c 1:j−1 , s) = softmax(u j ) (4) where c 0 is the start symbol for each compressed sentence and h c 0 is initialised by the source sentence vector of h e |s| .",81,82
1612,10480989,We also introduce a start symbol s 0 for the reconstructed sentence and h d 0 is initialised by the last state output ĥc |c| .,5,6
1613,233240744,"In practice, for each transcript line, if a colon symbol appears in the first 8 tokens and there exists at least one character name in front of the colon symbol, we will count it as a character utterance.",11,12
1614,233240744,"In practice, for each transcript line, if a colon symbol appears in the first 8 tokens and there exists at least one character name in front of the colon symbol, we will count it as a character utterance.",31,32
1615,16863853,"It should be able to enforce constraints in the search space that would prevent incompatible phrases to be adjacent in the translation, e.g. if the last translated symbol is an NC( or NC+ we would like to restrict the search to microtag phrases beginning with NC+ or NC) (intra-chunk consistency).",28,29
1616,2340513,"As we consider syntactic parse trees, each node with its children is associated with a grammar production rule, where the symbol on the left-hand side corresponds to the parent and the symbols on the right-hand side are associated with the children.",22,23
1617,803811,"n; (iii) a designated start symbol ROOT ; and (iv) a set of rules, {ρ = N i → ζ j }, where ζ j is a sequence of terminals and nonterminals.",8,9
1618,220061061,We insert a mask symbol in the blank position and compute the probability of the distractor or correct answer at that position.,4,5
1619,248780434,"When visualizing the prediction in Table 3 , we exclude the non-ASCII symbol prediction from the top word list of all models.",14,15
1620,11625207,"One such technique was introduced by the organizers of the IWSLT evaluation campaign, who suggested duplicating the ending punctuation symbol to the beginning of each sentence before training the language model 1 .",20,21
1621,11625207,Another drawback associated with such an approach is that the method encodes strong dependency assumptions between the punctuation symbol to be inserted and its surrounding words.,18,19
1622,11625207,"That is, we assume each word can be associated with an event, which tells us which punctuation symbol (possibly NONE) should be inserted after the word.",19,20
1623,11625207,The tag NONE means no punctuation symbol is inserted after the current word.,6,7
1624,11625207,Any other tag refers to inserting the corresponding punctuation symbol.,9,10
1625,11625207,"The word layer tags are responsible for inserting a punctuation symbol (including NONE) after each word, while the sentence layer tags are used for annotating sentence boundaries and identifying the sentence type (declarative, question, or exclamatory).",10,11
1626,11625207,"which can be used to guide the prediction of the punctuation symbol at each word, hence improving the performance at the word layer.",11,12
1627,11625207,"Specifically, these experiments can be divided into two categories: with or without duplicating the ending punctuation symbol to the start of a sentence before training.",18,19
1628,11625207,This setting can be used to assess the impact of the proximity between the punctuation symbol and the indicative words for the prediction task.,15,16
1629,11625207,"Specifically, for English, duplicating the ending punctuation symbol to the start of a sentence before training is shown to be very helpful in improving the overall prediction performance.",9,10
1630,11625207,"Thus, duplicating the ending punctuation symbol to the start of a sentence so that it is near these indicative words helps to improve the prediction accuracy.",6,7
1631,11625207,"Thus, retaining the position of the ending punctu-ation symbol before training yields better performance.",11,12
1632,14852353,"Let e(t) be the embedding of decoder symbol t. The RNN state at the next time-step is computed as d j+1 = W (5) e(t j ) + W (6) h a j s j+1 = RN N (d j+1 , s j ).",8,9
1633,14852353,Singletons in the encoder input are replaced with an unknown word symbol with probability 0.5 for each iteration.,11,12
1634,148051,"2011) , in addition to real words, each vocabulary contains a special unknown word symbol unk to handle unseen words; two sentence boundary symbols s and /s , which are filled into surrounding window when necessary; furthermore, to handle null alignment, we must also include a special null symbol null .",16,17
1635,148051,"2011) , in addition to real words, each vocabulary contains a special unknown word symbol unk to handle unseen words; two sentence boundary symbols s and /s , which are filled into surrounding window when necessary; furthermore, to handle null alignment, we must also include a special null symbol null .",54,55
1636,53092914,"We first replace the target mention with a special symbol which has a manually assigned constant embedding, and then feed the sequence into the LSTM.",9,10
1637,53092914,"In this case, we also replace the target mention with a special symbol which has a manually assigned constant embedding vector, we feed it into the LSTM, and use the last hidden vector h as the context representation.",13,14
1638,52891302,"Given an input text sequence with T tokens x = {x 1 , x 2 , ..., x T }, find a corresponding output sequence y = {y 1 , y 2 , ..., y T } where each output symbol y i is one of N possible output labels.",45,46
1639,1670126,"For example, the features in syntactic MT systems can be defined as the generation probabilities conditioned on the root symbol of the tree-fragment.",20,21
1640,1670126,"More formally, we use the following estimates for these probabilities: P phr (t r | s r ) = r :ϕ(s r )=ϕ(sr)∧t r =tr c(r ) r :ϕ(s r )=ϕ(sr) c(r ) P phr (s r | t r ) = r :ϕ(s r )=ϕ(sr)∧t r =tr c(r ) r :t r =tr c(r ) P(r | root(r)) = c(r) r :root(r )=root(r) c(r ) P(r | s r ) = c(r) r :s r =sr c(r ) P(r | t r ) = c(r) r :t r =tr c(r ) where c(r) is the count of r, and root(•) and ϕ(•) are functions that return the source root symbol for a tree-to-string rule and the sequence of leaf nodes for a tree-fragment respectively.",145,146
1641,235097319,"Once our model produces the specific symbol ""[m]"" which is designed to indicate the termination of the right part generation, we will start predicting the left part of the sentence ""Chatting with people ,"".",6,7
1642,235097319,"in front of the left part and removing the additional symbol ""[m]"".",10,11
1643,235097319,"Furthermore, when it predicts the symbol ""[m]"", we start predicting the left part of the sentence (y 1 , . . . ,",6,7
1644,235097319,We explore the distribution of the positions of symbol [m] .,8,9
1645,20138082,"Each encoder may be configured differently, such as by the number of hidden units and the embedding dimension for the source symbol.",22,23
1646,20138082,"We augment the parent non-terminal's information p when computing the decoder state z t , as follows: z t = Ψ dec (z t−1 , E y [ỹ t−1 ], p) (2) where Ψ dec is the LSTM function and ỹt−1 is the previous target symbol.",55,56
1647,20138082,"2015) computes a timedependent context vector c t (as defined later in Section 3.1 and 3.2), which is subsequently used for computing the probability distribution over the next symbol, as follows: zt = tanh(Uz t + Vc t ) (3) p(y t |y <t , X) ∝ exp(Wz t ) (4) where U, V, and W are weight matrices.",32,33
1648,14386356,"A derivation creates a tree by starting with the root symbol and rewriting (substituting) it with an elementary tree, then continuing to rewrite frontier non-terminals with elementary trees until there are no remaining frontier non-terminals.",10,11
1649,14386356,"P lcfg (e|c) = f ∈F(e) s fc i∈I(e) (1 − s ic ) × A(lex-cfg-rules(e|c)) α|c ∼ A c A c |a lcfg c , b lcfg c , P cfg ∼ PYP(a lcfg c , b lcfg c , P cfg (•|c)), where I(e) are the set of internal nodes in e excluding the root, F (e) are the set of frontier non-terminal nodes, and c i is the non-terminal symbol for node i and s c is the probability of stopping expanding a node labelled c. The function lex-cfg-rules(e|c) returns the CFG rules internal to e, each of the form c → α; each CFG rule is drawn from the backoff distribution, A c .",96,97
1650,233296655,"To minimize the effect of entities during retrieval, we use a named entity tagger 1 to detect spans of entities and mask them with [BLANK] symbol with a probability p mask , during training.",28,29
1651,233296655,The entity linker has tagged the bison as a university symbol (m.0c5s26) rather than the Bison football team (m.0c41_v).,10,11
1652,11674974,"Moreover, for the terminal symbols, we will use their dominant POS tags (instead of the symbol itself).",18,19
1653,199367455,"At each state, the probability of the next target symbol is updated by a softmax function.",10,11
1654,3135059,"To avoid very rare features, we only consider the 250 most freqent terminal symbol (English words) in the English of Set1 and map all other terminal symbols into a single class.",14,15
1655,14333426,"For example, we can append the special beginning-of-sentence symbol s and end-of-sentence symbol /s to all sentences to increase their lengths, allowing the relaxed hybrid trees to be constructed for certain sentence-semantics pairs with short sentences.",13,14
1656,14333426,"For example, we can append the special beginning-of-sentence symbol s and end-of-sentence symbol /s to all sentences to increase their lengths, allowing the relaxed hybrid trees to be constructed for certain sentence-semantics pairs with short sentences.",21,22
1657,7697403,"Each semantic representation consists of se-mantic units as its tree nodes, where each semantic unit is of the following form: m a ≡ τ a : p α (τ b * ) (1) Here m a is used to denote a complete semantic unit, which consists of its semantic type τ a , its function symbol p α , as well as an argument list τ b * (we assume there are at most two arguments for each semantic unit).",63,64
1658,7697403,"w j Here the symbol ⊗ means extract and compute, a process that involves 1) extraction of additional features when the two structures on the right-hand side are put together (for example, the local bigram feature ""w i w i+1 "" can be extracted in the above case), and 2) computation of the score for the new structure when the two structures from both sides of ⊗ are combined, based on the scores of these structures and newly extracted features.",4,5
1659,220446125,"We adopt a basic strategy to accelerate beam search: the search ends when any candidate predicts the EOS symbol, and there are no candidates with higher scores.",19,20
1660,13287001,"Each semantic representation consists of semantic units as its tree nodes, where each semantic unit is of the following form: m a ≡ τ a : p α (τ b * ) (1) Here m a is used to denote a complete semantic unit, which consists of its semantic type τ a , its function symbol p α , as well as a list of types for argument semantic units τ b * (here * means 0, 1, or 2; we assume there are at most two arguments for each semantic unit).",61,62
1661,4895452,"Preliminaries Assume we have a global vocabulary of symbols, containing the reserved substitution symbol ♦.",14,15
1662,4895452,"sentence called the pattern 1 , X is a symbol called the postcondition, π is a k-length sentence called the preconditions, Γ is a function (called the carry function) that maps a k-length list of carries to a carry, and c is a function (called the cost function) that maps a k-length list of carries to a real number.",9,10
1663,4895452,Assume that a zero-rank sentence s does not contain the same symbol more than once.,13,14
1664,4895452,"Hence, under the assumption that the input sentence s does not contain the same symbol more than once, then there are at most |s| scope(s * ) • |C| k chart updates involving a rule with pattern s * .",15,16
1665,4895452,The above lemma can also be relaxed to assume only that there is a constant upper bound on the multiplicity of a symbol in the input sentence.,22,23
1666,4895452,"CNF LNF Scope 3 All Grammars Derivation Cost problem is the same for both R and R. 7 CNF Binarization A rule r is CNF if its pattern is ""♦ ♦"" or ""x"", where x is any non-substitution symbol.",45,46
1667,4895452,"To answer this question, it is useful to distinguish between lexical rules (i.e. rules whose patterns contain at least one non-substitution symbol) and non-lexical rules.",25,26
1668,14220069,The symbol () denotes optional.,1,2
1669,5527031,We denote such a combination of word embedding set A with word embedding set B using the symbol (+).,17,18
1670,13830525,"For a given λ-expression e, our algorithm finds either two expressions h and f such that (h f ) ≡ e, or three expressions h, f , and g such that ((h f ) g) ≡ e, where the symbol ≡ is interpreted as α-equivalent after reductions 1 (Barendregt, 1985) .",49,50
1671,13830525,"For better readability, we introduce the symbol ¡ as an alternative notation for functional application.",7,8
1672,13830525,"The symbol ∼ denotes the one-to-one correspondence between nonterminal occurrences (i.e., in this case types of λ-expressions) in both p λ and h w .",1,2
1673,7177285,"Bidirectional Leaf-Node Encoding As discussed in Section 1, the unidirectional recurrent neural network reads an input sequence in order, from the first symbol to the last.",26,27
1674,220047192,"In Figure 3 , One special case is that when the depth equals 1, the template only has one symbol ""S"".",20,21
1675,15902640,"h enc t ) Finally, the initial state of the decoder LSTM is set to be h ctx t and the decoder LSTM reads a vector representation of the start symbol v S and generates the next word w t+1 character by character.",31,32
1676,15902640,"To predict the j-th character in w t , the decoder  LSTM reads vector representations of the previous characters in the word, conditioned on the context vector h ctx t and a start symbol.",37,38
1677,15902640,"2017) use a clever trick to estimate the probability, λ t of drawing from the LM by augmenting their (closed) vocabulary with a special symbol indicating that a copy should be used.",28,29
1678,52156390,"In the tree-structured semantic representations as illustrated in Figure 1 , each tree node is a semantic unit of the following form: m i ≡ τ α : p α (τ * β ) where m i denotes the complete semantic unit, which consists of semantic type τ α , function symbol p α and an argument list of semantic types τ * β (here * denotes that there can be 0, 1, or 2 semantic types in the argument list.",56,57
1679,4875809,"Finally, the decoder state d t is computed as d t = LSTM([ỹ t−1 ; c t−1 ], d t−1 ) , where ỹt−1 is the embedding vector corresponding to the previous output symbol y t−1 .",35,36
1680,4875809,The decoder stops upon producing the end-of-sentence symbol.,11,12
1681,4891276,Define a labeling scheme as a set of symbols including a special symbol null (this will designate that a given span is unlabeled).,12,13
1682,4891276,"Define a model variable of L as a symbol of the form S ij or L k ij , for positive integers i, j, k, such that j ≥ i and k ≤ m. The domain of model variable S ij is {true, f alse} (these variables indicate whether a given span is a tree constituent).",8,9
1683,4893310,Define a labeling scheme as a set of symbols including a special symbol null (this will desig- nate that a given span is unlabeled).,12,13
1684,4893310,"Define a model variable of L as a symbol of the form S ij or L k ij , for positive integers i, j, k, such that i ≤ j and k ≤ m. Model variables of the form S ij indicate whether span (i, j) is a tree constituent, hence the domain of S ij is {true, f alse}.",8,9
1685,1557806,"We define a symbol tree over an alphabet ∆ as a rooted, directed tree, the nodes of which are each labeled with a symbol of ∆. We want to capture the process by which a symbol tree over the target language is derived from a string of source symbols.",3,4
1686,1557806,"We define a symbol tree over an alphabet ∆ as a rooted, directed tree, the nodes of which are each labeled with a symbol of ∆. We want to capture the process by which a symbol tree over the target language is derived from a string of source symbols.",25,26
1687,1557806,"We define a symbol tree over an alphabet ∆ as a rooted, directed tree, the nodes of which are each labeled with a symbol of ∆. We want to capture the process by which a symbol tree over the target language is derived from a string of source symbols.",37,38
1688,1557806,Let us refer to the symbol tree that we want to derive as the target tree.,5,6
1689,1557806,"Furthermore, we define a derivation string as an ordered sequence of elements, each of which is either a source symbol or a target subtree.",21,22
1690,1557806,"In it, we begin with a source symbol ""ne"", followed by a target subtree rooted at V B, followed by another source symbol ""pas.""",8,9
1691,1557806,"In it, we begin with a source symbol ""ne"", followed by a target subtree rooted at V B, followed by another source symbol ""pas.""",27,28
1692,1557806,"The input to the rule are the roots of the elements of the derivation string that are replaced (where we define the root of a symbol to be simply the symbol itself), whereas the output of the rule is a symbol tree, except that some of the leaves are labeled with variables instead of symbols from the target alphabet.",26,27
1693,1557806,"The input to the rule are the roots of the elements of the derivation string that are replaced (where we define the root of a symbol to be simply the symbol itself), whereas the output of the rule is a symbol tree, except that some of the leaves are labeled with variables instead of symbols from the target alphabet.",31,32
1694,1557806,"The input to the rule are the roots of the elements of the derivation string that are replaced (where we define the root of a symbol to be simply the symbol itself), whereas the output of the rule is a symbol tree, except that some of the leaves are labeled with variables instead of symbols from the target alphabet.",43,44
1695,4954706,"In our experiments, we limit the vocabulary to various sizes v, always keeping the most frequent v words and replacing the rest with an unknown word symbol.",28,29
1696,587061,We could not use the bigram word2vec model because of the frequent occurrence of the placeholder symbol.,16,17
1697,43304,"As with the tagger, we tune over the decision to keep type embeddings fixed or update them during learning, again using i n j n ∆ = 1 ∆ = 2 3 ≤ ∆ ≤ 5 6 ≤ ∆ ≤ 10 ∆ ≥ 11 i < j i > j x j is wall symbol Table 4 : Dependency pair features for arc with child x i and parent x j in an n-word sentence and where ∆ = |i − j|.",56,57
1698,43304,"The final feature is 1 if x j is the wall symbol ($), indicating a root attachment for x i .",11,12
1699,9900131,"Decoder: The decoder uses another recurrent neural network to generate a corresponding target sequence Y = (y 1 , y 2 , ... , y T ) based on the encoded sequence of hidden state h. At each time i, the conditional probability of target symbol y i is computed by z i = RNN([y i−1 ; c i ], z i−1 ) (3) p(y i |y <i , h) = softmax(g(y i−1 , z i , c i )) (4) where g is a non-linear function, z i is the i th hidden state of the decoder, and it is calculated conditional on the previous hidden state z i−1 , previous target symbol y i−1 and source context vector c i .",48,49
1700,9900131,"Decoder: The decoder uses another recurrent neural network to generate a corresponding target sequence Y = (y 1 , y 2 , ... , y T ) based on the encoded sequence of hidden state h. At each time i, the conditional probability of target symbol y i is computed by z i = RNN([y i−1 ; c i ], z i−1 ) (3) p(y i |y <i , h) = softmax(g(y i−1 , z i , c i )) (4) where g is a non-linear function, z i is the i th hidden state of the decoder, and it is calculated conditional on the previous hidden state z i−1 , previous target symbol y i−1 and source context vector c i .",128,129
1701,9900131,"To introduce these constraints, the conditional probability of each target symbol y i can be rewritten as p(y i |y <i , h) = exp (g i ) * I(y i ) k exp (g k ) * I(y k ) (9) where g i is the ith element of g(y i−1 , z i , c i ).",11,12
1702,27246259,"More specifically, we provide evidence that the hidden state vectors represent atomic formulas Φ[c] where Φ is a semantic property (predicate) and c is a constant symbol entity identifier.",30,31
1703,27246259,"2016) , use bidirectional LSTMs or GRUs to construct a contextual embedding h t of each position t in the passage and also an embedding h q of the question q. They then select an answer c using a criterion similar to argmax c t < h t , h q > < h t , e(c) > (1) where e(c) is the vector embedding of the constant symbol (entity identifier) c. In practice the innerproduct < h t , h q > is normalized over t using a softmax to yield attention weights α t over t and (1) becomes argmax c < e(c), t α t h t > . (",73,74
1704,27246259,"We argue that for aggregation readers, roughly defined by (2), the hidden state h t of the passage at position (or word) t can be viewed as a vector concatenation h t = [s(Φ t ), s(c t )] where Φ t is a property (or statement or predicate) being stated of a particular constant symbol c t .",66,67
1705,27246259,"Furthermore, the question can be interpreted as having the form Ψ[x] where the problem is to find a constant symbol c such that the passage implies Ψ[c].",21,22
1706,27246259,3) The first inner product in (3) is interpreted as measuring the extent to which Φ t [x] implies Ψ[x] for any x. The second inner product is interpreted as restricting t to positions talking about the constant symbol c. Note that the posited decomposition of h t is not explicit in (2) but instead must emerge during training.,44,45
1707,27246259,Explicit reference readers avoid (2) and instead use argmax c t∈R(c) α t (4) where R(c) is the subset of the positions where the constant symbol (entity identifier) c occurs.,31,32
1708,27246259,We have experimented with two-sparse constant symbol embeddings where the number of embedding vectors in dimension d is 2d(d − 1) (d choose 2 times the four ways of setting the signs of the non-zero coordinates).,8,9
1709,102491590,"A regular uppercase symbol, e.g., S x , represents a lexical sequence.",3,4
1710,102491590,"A regular lowercase symbol, e.g., x i or y, represents a token.",3,4
1711,3101974,Adding the root symbol at the end of the sentence rather than at the front gives very similar parsing performance.,3,4
1712,3101974,"Punctua-tion is removed, numbers and symbols are mapped to a single symbol and the vocabulary is limited to 10, 000 words.",14,15
1713,16732516,"For example, the generalization path ap-ples→fruits↝vegetarians→people→organisms is 75% edge-accurate (i.e., 3/4 edges are correct as indicated by the symbol →), but it can lead to the wrong inference that apples are vegetarians and, in turn, people and organisms.",28,29
1714,227231587,"The final vocabulary size is 800K. The out-of-vocabulary words are simply replaced by a ""UNK"" symbol.",21,22
1715,4971762,"Each word has exactly one parent, and $ is a special ""wall"" symbol that is located at position 0 in the sentence and acts as parent to words that have no other parent in the sentence.",15,16
1716,4971762,"Each word has exactly one parent and $ is a special ""wall"" symbol that serves as the parent of all root words in the tree (i.e., those with no other parent).",14,15
1717,4971762,"n} dependency tree on source words x x x, where τ x x x (i) is the index of the parent of word x x x i (0 is the wall symbol $) τ φ : {1, . . . ,",36,37
1718,4971762,"n } dependency tree on target phrases φ, where τ φ (i) is the index of the parent of phrase φ i (0 is the wall symbol $) h h h = h h h , h h h vector of feature functions; h h h holds the Moses feature functions and h h h holds the QPD feature functions θ θ θ = θ θ θ , θ θ θ vector of feature weights for h h h k such that 1 ≤ j ≤ k ≤ m. The number of words in phrase φ is denoted |φ|.",30,31
1719,4971762,"If a phrase contains a root word in τ y y y , we extract a phrase dependency with the wall symbol as its head.",21,22
1720,9820235,"w[x/X] denotes substitution of the string x for the symbol X in the string w. The function p(•) provides the LM probability for all complete m-grams in a string, while the function q(•) elides symbols whose m-grams have been accounted for by p(•).",12,13
1721,9820235,"Applying an m-gram LM in the Decoder Integrating an LM into chart parsing requires two functions p(•) and q(•) (see Figure 1 ) that operate on strings over T T ∪ { }, where is a special ""placeholder"" symbol for an elided part of a targetlanguage string.",46,47
1722,1528374,"During training, the MEMM log-likelihood for a tagged tweet x, y is the sum over the observed token tags y t , each conditional on the tweet being tagged and the observed previous tag (with a start symbol before the first token in x), (x, y, β) = |x| t=1 log p(y t | y t−1 , x, t; β).",42,43
1723,1528374,"The symbol U+E12F, which represents a picture of a bag of money, is grouped with the words cash and money.",1,2
1724,8045822,"Following an inorder traversal of this MR tree, we can equivalently represent it with the following list of meaning representation productions (MR productions): (0) QUERY : answer (NUM) (1) NUM : count (STATE) (2) STATE : exclude (STATE 1 STATE 2 ) (3) STATE : state (all) (4) STATE : loc 1 (RIVER) (5) RIVER : river (all) Each such MR production consists of three components: a semantic category, a function symbol which can be omitted (considered empty), and a list of arguments.",99,100
1725,8045822,"Take production (1) for example: it has a semantic category ""NUM"", a function symbol ""count"", and a child semantic category ""STATE"" as its only argument.",19,20
1726,8045822,"Production (5) has ""RIVER"" as its semantic category, ""river"" as the function symbol, and ""all"" is a constant.",19,20
1727,8045822,"We denote a single NL word as w, a contiguous sequence of NL words as w, and a complete NL sentence as w. In the MR structure, we denote a semantic category as M. We denote a single MR production as m a , or M a : p α (M b , M c ), where M a is the semantic category for this production, p α is the function symbol, and M b , M c are the child semantic categories.",77,78
1728,8045822,"Given a semantic category M a , we first pick an MR production m a that has the form M a : p α (M b , M c ), which gives us the function symbol p α as well as the child semantic categories M b and M c .",37,38
1729,8045822,"The symbol w refers to a contiguous sequence of NL words, and anything inside [] can be optionally omitted.",1,2
1730,8045822,"t θ(t|m j , Λ) =1 for all j, where t is a NL word, the ""END"" symbol, or a semantic category.",23,24
1731,8045822,"Λ is the context associated with m j and t. These parameters model the emission of NL words, the ""END"" symbol, and child semantic categories from an MR production.",23,24
1732,8045822,"The symbol h is used to denote a hybrid sequence, and the function Parent(h) gives the unique MR substructure-NL subsequence pair which can be decomposed as h. Parent(n v ) returns the set of all possible hybrid sequences under which the pair n v can be generated.",1,2
1733,8045822,We assume half of the time words can be generated from the production's function symbol alone if it is not empty.,15,16
1734,8045822,"Mathematically, assuming m a with function symbol p a , for a NL word or semantic category t, we have: θ(t|m a , Λ) = θ e (t|m a , Λ) If p a is empty θ e (t|m a , Λ) + θ e (t|p a , Λ) /2 otherwise where θ e models the generation of t from an MR production or its function symbol, together with the context Λ. A Dynamic Programming Algorithm for Inside-Outside Computation Though the inside-outside approach already employs packed representations for dynamic programming, a naive implementation of the inference algorithm will still require O(n 6 m) time for 1 EM iteration, where n and m are the length of the NL sentence and the size of the MR structure respectively.",7,8
1735,8045822,"Mathematically, assuming m a with function symbol p a , for a NL word or semantic category t, we have: θ(t|m a , Λ) = θ e (t|m a , Λ) If p a is empty θ e (t|m a , Λ) + θ e (t|p a , Λ) /2 otherwise where θ e models the generation of t from an MR production or its function symbol, together with the context Λ. A Dynamic Programming Algorithm for Inside-Outside Computation Though the inside-outside approach already employs packed representations for dynamic programming, a naive implementation of the inference algorithm will still require O(n 6 m) time for 1 EM iteration, where n and m are the length of the NL sentence and the size of the MR structure respectively.",76,77
1736,1559377,"Letting y denote the tag sequence for a sentence e with m tokens, the singleexample log-likelihood is: log y p(stop | y m ) m j=1 p(y j | y j−1 ) • p(e j | y j ) (3) where y 0 is a designated ""start"" symbol.",55,56
1737,233210259,"However, the second entity ""PCP"" could be misclassified as a PRODUCT entity if a model relies more on the context ""begin trading with"" but ignores the hidden information that ""PCP"" is the symbol of ""Precision Castparts Corp."".",39,40
1738,1152008,"Rather than drawing a discrete symbol in Σ x from a softmax, we draw a distribution over symbols from a logistic-normal distribution at each time step.",5,6
1739,2657328,"For example, the probabilistic CKY algorithm (run on sentence w 1 w 2 ...w n ) is written as C X,i−1,i = p X→w i (1) C X,i,k = max Y,Z∈N;j∈{i+1,...,k−1} p X→Y Z × C Y,i,j × C Z,j,k goal = C S,0,n where N is the nonterminal set and S ∈ N is the start symbol.",81,82
1740,2657328,"Letting L ⊆ Σ * be the proof language on some symbol set Σ, this semiring is defined on the set R ≥0 × L with 0 element 0, and 1 element 1, .",11,12
1741,221865540,"The training objective is calculated as: L(θ; X ) = X=( XG 1 || XG 2 ),X∈X log P ( XG 2 | XG 1 ; θ) We do not make constraints that the split point must be the position right after a full-stop symbol, which ensures full sentences for each segment.",52,53
1742,221865540,We highlight the high-est ROUGE scores in Table 4 using bold font and use the symbol * to indicate the models that perform significantly different from STEP.,17,18
1743,226262263,"For each column, we concatenate the column name with its table name, separated by a dedicated symbol [#], as its canonical string representation.",18,19
1744,5192882,"We trained a classifier which, for every word in the parser's output, had to decide whether an empty node should be added as a new dependent of the word, and what its symbol ('*', '*U*' or '0' in WSJ), POS tag (always -NONE-in WSJ) and the label of the new dependency (e.g., 'S NP-SBJ' for NP PRO and 'VP SBAR' for empty complementizers) should be.",36,37
1745,14609577,"Notation and preliminaries A probabilistic context-free grammar (PCFG) G = (V, T, S † , P, ρ) consists of a set of nonterminal variables V ; a set of terminal items (words) T ; a special start non-terminal S † ∈ V ; a set of rule productions P of the form A → α for A ∈ V , α ∈ (V ∪ T ) * ; and a function ρ that assigns probabilities to each rule in P such that for any given non-terminal symbol X ∈ V , α ρ(X → α) = 1.",102,103
1746,14609577,"For a leftmost derivation S † * ⇒ G α, where α ∈ (V ∪ T ) * , the sequence of derivation steps that yield α can be represented as a tree, with the start symbol S † at the root, and the ""yield"" sequence α at the leaves of the tree.",39,40
1747,14609577,"As with language modeling, it is important to model the end of the string as well, usually with an explicit end symbol, e.g., </s>.",23,24
1748,235097530,2017) to encode inputs and decode tokens one by one until a special end-ofsequence symbol is encountered.,17,18
1749,44118631,"Let sentence w 0:n be a sequence of words, where w 0 is always the designated root symbol ROOT and w n the end-of-sentence symbol EOS.",20,21
1750,44118631,"Let sentence w 0:n be a sequence of words, where w 0 is always the designated root symbol ROOT and w n the end-of-sentence symbol EOS.",31,32
1751,44118631,The end-of-sentence symbol is generated implicitly when ROOT is reduced from the stack.,6,7
1752,8505251,"Here are some example tweets upon querying the keyword ""$aapl"" (which is the stock symbol for Apple Inc.) in Twitter: 1. """,18,19
1753,1458424,"Otherwise, the token is separated from the form by the symbol "" / "".",11,12
1754,9879243,"The 'V' across the line between purchase order and the 'sent-by' role indicates that a purchase order cannot exist without being related in this way to a customer, but the absence of such a symbol at the opposite end of the relationship line shows that a customer can exist without having any (current) purchase orders.",42,43
1755,9879243,"For each symbol added, the change is recorded in the session database, and its internal representation is passed to the language generation component, which produces an English description of the effect of the changes.",2,3
1756,9879243,"Suppose for example, that the graphics window contains the first drawing shown in Appendix A. The user then adds the V symbol to produce the next drawing shown.",22,23
1757,219308430,"u k ∈ U is a (possibly empty) path, and A some non-terminal symbol from N .",18,19
1758,250390924,"Table 6 shows the most frequently misclassified tokens, which are the English and Dutch definite determiner and the Chinese symbol for Sri Lanka.",20,21
1759,7769307,"One implication, we think, is that a semantic or pragmatic symbol at the tactical level should be considered by its very definition a shorthand for a semantic or pragmatic equivalence relation: a way of capturing what semantic or pragmatic characteristics certain surface expressions have in common.",12,13
1760,7769307,"And so a tactical generator's task with respect to such a symbol, stated a bit more carefully, is to choose among expressions which have been previously judged semantically or pragmatically equivalent in context and grouped under that symbol for just that reason. (",12,13
1761,7769307,"And so a tactical generator's task with respect to such a symbol, stated a bit more carefully, is to choose among expressions which have been previously judged semantically or pragmatically equivalent in context and grouped under that symbol for just that reason. (",40,41
1762,7769307,Assuming that the purpose of tactical semantic and pragmatic symbols is precisely to capture paraphrases --to symbolize what is semantically and pragmatically invariant in a range of surface expressions --it is natural that a symbol set is to be tested by generating purported paraphrases from it.,34,35
1763,7769307,We will refer to the symbol yielding the first grouping as CONCESSION.,5,6
1764,7769307,"An analysis routine may in such eases need a wide range of knowledge to determine which tactical symbol gives a better interpretation, or what more specific symbol could have been used instead.",17,18
1765,7769307,"An analysis routine may in such eases need a wide range of knowledge to determine which tactical symbol gives a better interpretation, or what more specific symbol could have been used instead.",27,28
1766,3009300,"Following 4], we de ne a PDA as a tuple (V T ; V S ; ; $ 0 ; $ f ) where V T is a nite set of terminal symbols, V S is a nite set of stack symbols, $ 0 2 V S is the initial stack symbol, $ f 2 V S is the nal stack symbol and is a nite set of SWAP, PUSH and POP transitions.",56,57
1767,3009300,"Following 4], we de ne a PDA as a tuple (V T ; V S ; ; $ 0 ; $ f ) where V T is a nite set of terminal symbols, V S is a nite set of stack symbols, $ 0 2 V S is the initial stack symbol, $ f 2 V S is the nal stack symbol and is a nite set of SWAP, PUSH and POP transitions.",67,68
1768,3009300,"3 Bidirectional Linear Indexed Automata Linear Indexed Automata (LIA) 1, 11] are an extension of push-down automata in which each stack symbol has been associated with a list of indices.",27,28
1769,3009300,We de ne Bidirectional Right-oriented Linear Indexed Automata (BR{LIA) as an extension of BPDA in which each stack symbol has been associated with a list of indices.,22,23
1770,3009300,"Formally, a BR-LIA is a tuple (V T ; V S ; V I ; BR LIA ; $ 0 ; $ f ) where V T is a nite set of terminal symbols, V S is a nite set of stack symbols, V I is a nite set of indices, BR LIA is a nite set of transitions, $ 0 2 V S is the initial stack symbol, and $ f 2 V S is the nal stack symbol.",76,77
1771,3009300,"Formally, a BR-LIA is a tuple (V T ; V S ; V I ; BR LIA ; $ 0 ; $ f ) where V T is a nite set of terminal symbols, V S is a nite set of stack symbols, V I is a nite set of indices, BR LIA is a nite set of transitions, $ 0 2 V S is the initial stack symbol, and $ f 2 V S is the nal stack symbol.",88,89
1772,3009300,"In the case that C is associated with an empty list of indices, we let p be the dummy index , D = E the dummy stack symbol and r = s = t = u the dummy input position .",28,29
1773,997009,"The steps D Scan CYK and D CYK are used to start the bottom-up parsing process by recognizing a terminal symbol for the input string, or none if we are using a tree with an epsilon node.",22,23
1774,997009,"To emulate this functionality in our parsing schema-based system, we have used its extensibility mechanism to define a function Selectstree(a,T) that returns true if the terminal symbol a selects the tree T. The implementation of this function is a Java method that looks for this information in XTAG's syntactic database.",32,33
1775,18425142,"The symbol endGroup, which is inserted after the group, will be used in the next layer when gluing the phonetic parts and the word endings.",1,2
1776,18425142,"An instance of a purely phonetic sign value: {val: tsig=<P17>, tphon=nmh, tsem=<>, vtype=<phon>} The notation <> stands for the empty string and <P17> for the single symbol P17, whereas nmh is a string of three symbols.",42,43
1777,18425142,A class of symbol is a finite set of symbols which has a name.,3,4
1778,4075637,"Here are some examples of class definitions: class short_v is a, e, i, u; class long_v is aa, ee, ii, uu; class vow is a, e, i, u, long_v; A symbol may belong to several classes.",44,45
1779,4075637,The sharp symbol is an auxiliary symbol used to make sure that the 0 on both sides is the same occurrence of this symbol.,2,3
1780,4075637,The sharp symbol is an auxiliary symbol used to make sure that the 0 on both sides is the same occurrence of this symbol.,6,7
1781,4075637,The sharp symbol is an auxiliary symbol used to make sure that the 0 on both sides is the same occurrence of this symbol.,23,24
1782,14940451,"At initiation, the stack contains only the root symbol (Σ = [ROOT]), the buffer contains the tokens of the sentence (β = [w 1 , ..., w n ]) and the set of arcs is empty (A = ∅).",9,10
1783,14940451,"When the buffer is empty, the stack contains only the root symbol and A contains a parse tree, the configuration is completed.",12,13
1784,12435083,"It results in the following labelling of the RDs in RD-list: RDs that cart overcome all negative effects are labelled with the symbol Jail] (the only RDs that may be labelled in this manner axe Assertions and Negations); RDs that cannot convey an intended proposition by themselves are labelled with the symbol [-] ; RDs that can convey an intended proposition, but cannot overcome any negative effects are labelled with [none]; and the remaining RDs are labelled with the negative RDs they can overcome.",25,26
1785,12435083,"It results in the following labelling of the RDs in RD-list: RDs that cart overcome all negative effects are labelled with the symbol Jail] (the only RDs that may be labelled in this manner axe Assertions and Negations); RDs that cannot convey an intended proposition by themselves are labelled with the symbol [-] ; RDs that can convey an intended proposition, but cannot overcome any negative effects are labelled with [none]; and the remaining RDs are labelled with the negative RDs they can overcome.",59,60
1786,5495116,"A constraint is associatted to at parse tree: it is the conjunction of all the constr~dnts of the labels and the oqualities between the tUllle of wtriables from the non-termilml ,if the loft-hand side of a label and the tlq)le of the relewmt symbol of tim right> hand side of tim l~dml of its p~trent.",51,52
1787,5495116,"hfformally, it is a machine using three data structures: a stack containing at each level a stack symbol and its tuple of variables; a representation of the terminal string that distinguishes those that have already been used and those that are still to be read; finally a constraint.",19,20
1788,5495116,We add some restrictions to these transitions: the only clmnge allowed for the string is that at most one more terminal is read; only the top of the stack is accessible and at most one symbol can be added or removed from it at once.,37,38
1789,5495116,"For example, the built-in parsing method of Prolog for DCGs is ol~t.ained by combining tim solver for ])CGs, the top-down strategy, 0 symbol of look-ahead a.nd a backtracking interpreter (and other modules not mentioned in Iigure 1 because they do not change the algorithm, but a.t most its implenmntation).",33,34
1790,9974904,"Conventional symbol: especially those are confusable to a foreign tourist, e.g., some symbols are not international.",1,2
1791,9974904,"It first preprocesses the selected region, binarizes the image to get text or symbol, and feeds the binary image into the sign recognizer.",14,15
1792,707080,"Whenever a symbol of one side does not have an actual counterpart in the other string, a special symbol 0 is inserted at the relevant position in order to fulfill the same-length constraint.",2,3
1793,707080,"Whenever a symbol of one side does not have an actual counterpart in the other string, a special symbol 0 is inserted at the relevant position in order to fulfill the same-length constraint.",19,20
1794,707080,"For such a string to be in lexicographic normal form, a dependent symbol must appear in the loop between b and a. Operations and closure properties Recognizable trace languages are closed under intersection and union.",13,14
1795,707080,Partitioned relations and trace languages It is possible to convert a partitioned relation into a trace language as follows: • represent the partition boundaries using a symbol ω not in Σ. • distinguish the symbols according to the component (tape) of the n-tuple they belong to.,27,28
1796,707080,"Partition-based formalism are especially adapted to express relations between different representation such as feature structures and affixes, with respect to two-level morphology which imposes an artificial symbol-to-symbol mapping.",31,32
1797,707080,"Partition-based formalism are especially adapted to express relations between different representation such as feature structures and affixes, with respect to two-level morphology which imposes an artificial symbol-to-symbol mapping.",35,36
1798,707080,"If the operands belong to disjoint monoids (which do not share any symbol), then the join is a Cartesian product.",13,14
1799,707080,The second partitioning is a symbol-to-symbol correspondence similar to the one used in standard two-level morphology.,5,6
1800,707080,The second partitioning is a symbol-to-symbol correspondence similar to the one used in standard two-level morphology.,9,10
1801,707080,These rules may be read: a symbol I 4 is realized as i (resp.,7,8
1802,707080,A different symbol is used for each kind of partitioning.,2,3
1803,707080,"Distinguishing symbols from different tapes in order to ensure that µ(x) is a singleton for each x ∈ Σ. Symbols of Σ are therefore pairs with the symbol appearing in the rule as first component and the tape identifier, a number, as second component.",28,29
1804,707080,"The order used by our system orders symbol with respect to tapes: symbols of the first tape are smaller than the symbols of tape 2, and so on.",7,8
1805,10070203,"In fact, it is the relations among the concepts and their fields that distinguish them, not their symbol names, and it is the mappings that determine what lexical items are used to express them (though some mappings use the concept name as a default lexical item when none is specified.)",19,20
1806,53234787,"We define feature vectors of frozen columns as h (<k) i−1 = [h (1) i−1 ; h (2) i−1 ; ...; h (k−1) i−1 ] of dimensional- ity n (<k) i−1 , where the symbol ; denotes concatenating.",48,49
1807,86021,"As a convenction, we decided to describe internal arguments with the symbol '/', while a '//' denotes a verbal adjunct.",12,13
1808,1622819,"Judges are also persons, but the node [JUDGE: {,}] is marked as plural by the symbol {,} and is therefore unlikely to be indicated as the Pfix Goncourt.",21,22
1809,232021659,"The tokens are marked up with parts of speech, potential lemma forms, and (optionally) a small set of semantic classes (indicating whether the token is punctuation, abbreviation, acronym, number, year, currency, or some kind of symbol).",46,47
1810,248780389,"As a result, we propose ChipSong, which is an assisted lyric generation system built based on a Transformerbased autoregressive language model architecture, and generates controlled lyric paragraphs fit for musical short-video display purpose, by designing 1) a novel Begin-Internal-End (BIE) word-granularity embedding sequence with its guided attention mechanism for word-level length format control, and an explicit symbol set for sentence-level length format control; 2) an open-ended trigger word mechanism to guide specific lyric contents generation; 3) a paradigm of reverse order training and shielding decoding for rhyme control.",74,75
1811,248780389,The control character is placed after the sentence separator [SEP ] and before the beginning of the sentence to learn the correspondence between the control symbol and the sentence length.,26,27
1812,231643130,"The relation wn30:classifiedByRegion was created from the ;r pointer symbol in Princeton WordNet data distribution, documented in wninput(5wn).",12,13
1813,2516915,A filler can be any symbol or a Lisp function call.,5,6
1814,10197337,Help associations (arrow with a symbol on top) support ~ with extra material.,6,7
1815,202586770,We decide to explicitely indicate this deep nature by introducing deep-syntactic features on dependencies with the @ symbol.,19,20
1816,202586770,"7 Extensions used in UD which are unknown in SUD are just copied but with the symbol @. For instance, case:loc used in different Chinese UD gives us comp:obj@loc in Chinese SUD.",16,17
1817,220445564,"Bullet of an item segment can be a roman number, an English letter or a special symbol present at its start.",17,18
1818,220446119,We infer the symbol by identifying multiple left-aligned lines introduced by the same single-character token.,3,4
1819,220446119,"Indeed, the end of list items are computed while computing paragraph structures: a list item ends when the next list item starts (i.e., same bullet symbol, same indentation) or when less indented text objects starts.",29,30
1820,21697471,"Evaluation of Event POS Tags Table 3 shows the total amount of tokens for each of the following POS tags: verb, noun, adjective, adverb, preposition, number, particle, determiner and symbol, as well as the total amount and percentage of tokens annotated as events for each POS tag.",37,38
1821,219307936,"5 perform the following evaluations: (1) enclosing node is a noun, (2) lemma of enclosing node is one of the three listed movement-related verbs, (3) the bound node is a noun, (4) the parent node of a bound token has at least 5 children in total, (5) the part-of-speech tag of the second-to-last word in the bound sentence does not contain the symbol N (depending on the tagset, this will exclude nouns, proper nouns, conjunctions, past participle verbs and other tags at that position in the sentence).",87,88
1822,220445766,It also includes symbol characters like '(' and '1.',3,4
1823,13182059,The init symbol $ 0 and final symbol $1 I I I I I I I I I I • il ••. J. ' • N B D 0 Figure 4: Application of Rule 1 Space complexity of the tabular technique for BU 2-SA is obviously O(n 4 ) as at most 4 indices are stored in buXCF items.,2,3
1824,13182059,The init symbol $ 0 and final symbol $1 I I I I I I I I I I • il ••. J. ' • N B D 0 Figure 4: Application of Rule 1 Space complexity of the tabular technique for BU 2-SA is obviously O(n 4 ) as at most 4 indices are stored in buXCF items.,7,8
1825,15656232,"7 These complexities remain polynomial when dealing with DATALOG features (no symbol of functions) and may be exponentia1 vü1erv.:..,~. Time complexity is directly related to the number of tried applications and created objects if objects can be accessed and added in constant time.",12,13
1826,8481156,"The first letter of each of the names is prefixed with a begin symbol, B, and the last letter is suffixed with an end symbol E. 2.",13,14
1827,8481156,"The first letter of each of the names is prefixed with a begin symbol, B, and the last letter is suffixed with an end symbol E. 2.",26,27
1828,30963769,We use the symbol '/' to separate different case slots of the same predicate which share arguments.,3,4
1829,7498596,"As an additional advantage, the algorithm does not need to require the restriction that every auxiliary tree must have at least one terminal symbol in its frontier (Vijay-Shanker and Joshi, 1985) .",24,25
1830,6751938,"Arguments are represented as an argument name followed by the ""="" symbol followed by a value and/or subargument (s) .",13,14
1831,15050799,"Alternation rules are expressed in finite state using XFST replace rules of the general form: (29 a -> b || L _ R This means that the string a is replaced with the string b when a occurs between the left context L and the right context R. When no context is specified the replacement operates globally, and the special symbol '.#.'",64,65
1832,15050799,"The multi-character symbol ""^ss^"" stands for stem start and ""^se^"" for stem end.",4,5
1833,14554661,"Finite-state control can also be eliminated from EPDA, obtaining a new definition that considers a EPDA as a tuple (VT, Vs, e, $0, $ f) where VT is a finite Set of terminal symbols, Vs is a finite set of stack symbols, $ 0 E Vs is the initial stack symbol, $ / E Vs is the final stack symbol and e is a finite set of six types of transition:  An instantaneous co11figuratio11 is a pair (Y, w ) , where Y represents the contents of the automaton stack and w is the part of the input string that is yet to be read.",62,63
1834,14554661,"Finite-state control can also be eliminated from EPDA, obtaining a new definition that considers a EPDA as a tuple (VT, Vs, e, $0, $ f) where VT is a finite Set of terminal symbols, Vs is a finite set of stack symbols, $ 0 E Vs is the initial stack symbol, $ / E Vs is the final stack symbol and e is a finite set of six types of transition:  An instantaneous co11figuratio11 is a pair (Y, w ) , where Y represents the contents of the automaton stack and w is the part of the input string that is yet to be read.",72,73
1835,14554661,"Given a EPDA, the equivalent L-LIA is obtained by means of a simple change in the notations: if we consider the top element of a stack as a stack symbol, and the rest of the stack as the indices !",33,34
1836,235313683,"The grammar G m,L has starting symbol S, other non-terminals A k , B k , A l k , A r k , B k for all k ∈ {1, 2, ..., m}, and terminals a i for all i ∈ {1, 2, ..., m + 1 + L }, c j for all j ∈ {1, 2, ..., m}.",8,9
1837,235313683,"Formally, a PCFG (Chomsky, 1956 ) is a 5-tuple G = (Σ, N, S, R, Π) in which Σ is the set of terminals, N is the set of non-terminals, S ∈ N is the start symbol, R is the set of production rules of the form r = (r L → r R ), where r L ∈ N , r R is of the form B 1 B 2 ...B m , m ∈ Z + , and ∀i ∈ {1, 2, ..., m}, B i ∈ (Σ ∪ N ).",51,52
1838,235313683,Let w 0 = S be the sentence start symbol.,9,10
1839,16870912,"We say that V £ 4} 5 P QP QP 5 k} |b ~© is an instantiation of the predicate V £ XW 5 P QP QP 5 YW cb d© iff } ~`C q n $5 g ut wv $t e and each symbol (terminal or variable) of W `, g t xv %t e is instantiated to a range in q n s.t.",50,51
1840,11328110,"This hypothesis about lexical knowledge leads to the conclusion (section 5) that there are differences between lexical knowledge and expert-domain knowledge, in that the objective of the former is the use of a word as a symbol for a conceptualization of an object, and the objective of the latter is to have knowledge on (all and any imaginable) properties of the object itself.",41,42
1841,4311707,"As noted previously, a comment which is sarcastic often spawns a chain of subsequent comments which are all sarcastic, but which lack the ""/s"" symbol.",28,29
1842,2935389,"Their general form is: A j because S j (we use here the term 'because' which is more vague than the implication symbol used in formal argumentation, because natural language is not so radical).",26,27
1843,219310042,"Linear indexed grammars associate a stack of indices with each non-terminal symbol, with the restriction that the indices stack of the head non-terminal of each pro duction (the fa ther) can be inherited by at most one body non-terminal (the dependent child) while the other stacks must have a bounded stack size .",13,14
1844,219310042,"Linear Indexed Grammars A linear indexed grammar is a tuple (Vr, V N , Vi, P, S), where Vr is a finite set of terminals, V N a finite set of non-terminals, Vi is a finite set of indices, SE V N is the start symbol and Pisa finite set of productions.",56,57
1845,219310042,A CYK-like algorithm generalized for linear indexed grammar with productions manipulating more than one symbol at the top of the indices stacks is described in [17] .,16,17
1846,219310042,"Vijay-Shanker and Weir [18] try to solve this problem by defining a non-deterministic finite state automaton that determines if a given LIGed forest symbol (A, i, j)[a:] derives a string of terminals.",29,30
1847,219310042,"In fact, a v' i symbol is equivalent to a dotted production with the dot just before the non-terminal Ai + I or with the dot at the end of the right-hand side in the case of v' m • It is interesting to remark that the set of non-terminals is a subset of the set of items for CYK like and bottom-up Earley-like algorithms, and Earley-like algorithms without the VPP.",7,8
1848,2861172,"Similarly, a different grammar morphosyntactic symbol set can also be externally defined.",6,7
1849,2861172,"The FST plugin therefore incorporates logic in its reInit() method which scans an FST file (itself generated by an FST compiler typically running in the background), and determinesby deferring to a symbol compiler-what new annotation types and attribute features need to be dynamically configured and incrementally added to the model.",36,37
1850,6768034,"The types of linguistic data used for the attribute collection included part of speech information, orthography (upper case, lower case, initial upper case letter, mixture of upper and lower case), token kind (word, symbol, punctuation or number), sentence boundary, the presence of certain known names and keywords from the gazetteer lists provided by the ANNIE system.",42,43
1851,25845573,"The non-terminal <ref> denotes a discourse referent, and <sym n > an n-place predicate symbol.",22,23
1852,1938499,"AMTG-2 removes the rigid limitation on the password format and accepts 3-8-symbol alphanumeric, case-sensitive input while generating two lines of purported political satire (see McDonough 2000) .",15,16
1853,7771097,"For an SLM, a terminal node indicates that the most likely successor state is the special end-of-sentence symbol.",22,23
1854,10002136,"For example, the word 吃饭 'to dine' can be analyzed as a verb [ ] V made of a verbal and a nominal element [V N] V 吃 'to eat' and 饭 'rice', where character form classes are denoted by the symbols inside the bracket while the word classes/POS tags are denoted by the subscript symbol of the bracket.",67,68
1855,9600282,"As already said, the default mediator for | returns an ordered system → de£nition {command} * variables de£nition → module ""="" regexpr newline module → a fully quali£ed Java class name regexpr → var | ""("" regexpr "")"" | regexpr ""+"" regexpr | ""("" ""|"" {regexpr} + "")"" | ""("" ""*"" regexpr "")"" newline → the newline character command → mediator | threaded mediator → ""Mediator ="" med newline med → a fully quali£ed Java class name threaded → ""Threaded ="" {""yes"" | ""no""} newline variables → {vareq newline} + vareq → var ""="" module [initexpr] var → a lowercase symbol initexpr → ""("" string {"","" string} * "")"" string → a Java string Figure 1 : The EBNF for the syntax of SDL.",141,142
1856,15252837,"Every possible permutation of the child nodes for each frequent production A → B1 B 2 …B n , where B i is any terminal or nonterminal symbol, is considered a transformation.",28,29
1857,18785212,"The center panel can be further divided into two sub-panels: the top one shows the information about the Knowledge Base resources related to the linked named entities present in the tweets (image, textual description, type as symbol and the classification confidence score), and the bottom one provides the list of the recognized named entities for which it does not exist a correspondence in the KB, i.e. NIL entities.",42,43
1858,215786544,"There are two different types of rules that the model applies during decoding: 1) If the current rule generates a non-terminal symbol, then ApplyRule[r] is executed, which applies a production rule to the current tree.",25,26
1859,215786544,"2) If the next symbol is a terminal, then GenToken[v] is applied, which selects the token from a vocabulary.",5,6
1860,248780055,"In Section 2.1, we present symbol-based systems that go beyond needsbased, functional communication supporting the expression of personal thoughts in the context of social closeness and sharing information (cf.",6,7
1861,248780055,"Technical solutions for symbol-based AAC are increasingly available on mainstream devices like smartphones and tablets (Ascari, 2018) , ranging from simple concatenation of symbols for needs-based, functional communication (see, e.g., the popular free apps SymboTalk 3 and LetMeTalk 4 for German) to complex (commercial) systems (cf.",3,4
1862,14319538,The DLG from extracting all occurrences of x i x i+1 ...x j (also denoted as x i..j ) from a corpus X= x 1 x 2 ...x n as a word is defined as DLG(x i..j ) = L(X) − L(X[r → x i..j ] ⊕ x i..j ) (4) where X[r → x i..j ] represents the resultant corpus from replacing all instances of x i..j with a new symbol r throughout X and ⊕ denotes the concatenation of two substrings.,89,90
1863,243864634,"We experiment with two types of label representation settings: Sparse, which represents each frame (and the empty symbol) as a one-hot vector, while Embedding defines dense embeddings for frames.",20,21
1864,243864634,The empty symbol is predicted if the cosine similarity to the best frame is below a threshold t f .,2,3
1865,46611064,"Eventually, the decision to (nondeterministically) perform that set of actions is taken on next stack symbol.",18,19
1866,46611064,"algorithm Generator of nondeterministic ""DR(0) parsing tables with right-hand side check input Reduced context-free grammar G output St := fp j _ p; A; X ] 2 Ig if 8A : fp j _ p; A; X ] 2 Ig 2 fSt; ;g then StTbl (qI; X) := St else % state transitions upon stack symbol I 0 := (I; X); add I 0 to StAut StTbl (qI; X) := fgo-st q I 0 g end Figure 3 : Compilation algorithm for G""DR(0) parsing S 0 1 !",72,73
1867,46611064,"In state q 2 , it is clear that, beyond stack symbol S, actions 3 and 4 will follow exactly the same language of stack su xes 4 on the left of left-hand side S, so we can decide here to perform both actions and to stop construction.",12,13
1868,46611064,"6 ) a node of the GSS is a tuple symbol, set of predecessor nodes, set of right-hand sides].",10,11
1869,46611064,Stack tops will be merged if they correspond to the same symbol and have exactly the same predecessors Pred in the GSS.,11,12
1870,46611064,"Although the notion of predecessor set is more natural when using a GSS, our construction allows to replace it by a single index pointing algorithm Generalized ""-skip discriminating-reverse parser input ""DR(0) parsing table StTbl and input string `za output Forest of non-"" deriving parsing trees rooted at reduced, or rejection on erroneous input begin shifttops := ; repeat % parsing actions on next input symbol read (a); 0 := NewNode(a); Pred( 0) := shifttops currtops := f 0g; shifttops := ;; reduced := ; repeat % GSS explorations while reductions are done reductions := ; curr := f t; t; q0] j t 2 currtopsg repeat % check next symbol next := ; for ; t; q] 2 curr do at := StTbl (q; Symb ( )) if at = fgo-st q 0 g then for 0 2 Pred ( ) do add 0 ; t; q 0 ] to next else if 0 2 at then add t to shifttops for g 2 at f0g do add g; ; t] to reductions curr := next; until curr = ; currtops := ; % compute new tops from reductions and shared forest for g; ; t] 2 reductions and A g !",74,75
1871,46611064,"Although the notion of predecessor set is more natural when using a GSS, our construction allows to replace it by a single index pointing algorithm Generalized ""-skip discriminating-reverse parser input ""DR(0) parsing table StTbl and input string `za output Forest of non-"" deriving parsing trees rooted at reduced, or rejection on erroneous input begin shifttops := ; repeat % parsing actions on next input symbol read (a); 0 := NewNode(a); Pred( 0) := shifttops currtops := f 0g; shifttops := ;; reduced := ; repeat % GSS explorations while reductions are done reductions := ; curr := f t; t; q0] j t 2 currtopsg repeat % check next symbol next := ; for ; t; q] 2 curr do at := StTbl (q; Symb ( )) if at = fgo-st q 0 g then for 0 2 Pred ( ) do add 0 ; t; q 0 ] to next else if 0 2 at then add t to shifttops for g 2 at f0g do add g; ; t] to reductions curr := next; until curr = ; currtops := ; % compute new tops from reductions and shared forest for g; ; t] 2 reductions and A g !",139,140
1872,46611064,"Single derivations from a node are represented by its corresponding set SD containing production-number, symbol-node correspondence, handle-nodes] triples.",17,18
1873,46611064,"9 In the end, the di erent parses of the input text can be easily and e ciently recovered from the single derivations starting from the top node, whose symbol is S 0 if the input is a legal sentence.",31,32
1874,46611064,"Exploration is restarted from q 0 using the new topmost symbol x, which indicates to reduce using production S 2 !",10,11
1875,9519654,"The segmentation is indicated with hyphens, while the hash symbol (#) represents the word boundary.",10,11
1876,15757164,"This is followed by an editable window giving the modernised word form, lemma, PoS and potential gloss; if the corpus contains distinct annotations for the word form, they are all shown, separated by a pipe symbol.",40,41
1877,235293802,"When generating a list of items of the same type, instead of emitting a special action RE-DUCE as the symbol of termination (Yin and Neubig, 2017) , we enumerate all possible number of occurrences in the training set (see the constructors for type select and from in Figure 8 ).",22,23
1878,2656876,"The syntax of CLLS is given by: ::= XJ(Xl,...,X,) (]J""ES) I X<*Y I A(x)=Y I ante(X)=Y I X/X'~Y/Y' [ ~ A~' The semantics of CLLS is given in terms of first order structures L, obtained from underlying tree structures, by adding relations eL for each CLLS relation symbol ¢ E {~*, A(.)= "", ante(.)=., ./.~-/-, :@, :lam, :vat,...}.",65,66
1879,15791630,"Vowels, unless word-initial, are ignored, as are the letters H and W. If the word is longer than the four symbol sequence, the remaining letters are ignored.",25,26
1880,5806560,"Nominal predicates derived from verbs denote events, but there are other, non-eventive predicates in NomBank (e.g., the partitive predicate indicated by the ""%"" symbol).",31,32
1881,219301054,"The syntax of CLLS is given by: (p ::= X:f(X1,...,Xn) (fnEP,,) [ X<*Y I A(x)=Y I ante(X)=Y I x/x'~y/z' I 9~A9 J The semantics of CLLS is given in terms of first order structures L, obtained fl'om underlying tree structures, by adding relations ~L for each CLLS relation symbol ~ E {,~*, A(.)=., ante(.)=.,-/.,-~./., :@, :lain, war,...}. :",70,71
1882,2437616,"The statistical language modeling problem for the sequence of words W = Wl,...,wn where wn is a special end of sentence symbol can then be rephrased as n p(W) = rIp(w~lwl,..., w,_x) i=1 We will for most applications probably never be able to find enough data to estimate p as presented above.",23,24
1883,2437616,"Given a bigram model p and a test text wl,..., w,~ the perplexity PP is defined as PP = 2-~ ~=1 logP(w,lw,_~) where we make usage of a special ""start-of-sentence"" symbol as w0.",42,43
1884,2437616,"Inspired by (Lauer, 1995) , we have very recently extended this technique so that the algorithm has the option of, instead of replacing a sequence of two units by a new symbol, replacing it by either the left or right component of that sequence.",35,36
1885,2749752,"In Table 2 , the symbol † is used to highlight the relatively major polarity of each relation.",5,6
1886,2749752,"The symbol ‡ is marked when the polarity is the majority (i.e., with a frequency greater than 50% The distribution of the discourse relations versus (p 1 , p 2 ), the sentiment polarity transitions between two clauses, is shown in Table 4 .",1,2
1887,2749752,"The relatively major sentiment polarity transition of each discourser maker is labeled with the symbol †. The symbol ‡ is marked when the sentiment polarity is the majority, i.e., its ratio is greater than 50%.",14,15
1888,2749752,"The relatively major sentiment polarity transition of each discourser maker is labeled with the symbol †. The symbol ‡ is marked when the sentiment polarity is the majority, i.e., its ratio is greater than 50%.",17,18
1889,226262245,"To avoid this problem, we replace all the mentions of an entity in a document by a special blank symbol [BLANK] with probability α following Soares et al. (",20,21
1890,51887970,We pad short sequences with a special symbol P AD .,7,8
1891,51887970,Texts are lowercased and numbers are replaced by the special symbol %.,10,11
1892,177694,"The following figure shows the gist of the feature architecture of a linguistic sign: (1)               sign PHON list(phon-symbol) SYNSEM       LOC     CAT   HEAD head SPR list(synsem) COMPS list(arg)   CONT INDEX index     NONLOC nonloc       RELATIONS list(rels) C-CONT c-cont               At the highest level, features whose values can be constrained by selecting heads are collected under the SYNSEM attribute.",36,37
1893,203692282,"Pauses, which we transcribe with the '#' symbol, are stops or interruptions in the speech flow of the speaker.",10,11
1894,211005745,"An agent may insert an anchor symbol (e.g. "">>Agent[01]"") at the beginning of its talk, in order to specify which agent to speak to.",6,7
1895,248798649,Notice that each token c i in the flattened HTML codes c can be a raw text word or tag symbol such as <div> while the user query q is a word sequence of plain text.,20,21
1896,69975561,"Although human-robot dialogue systems often leverage a similar architecture to that of the spoken dialogue systems described above, humanrobot dialogue introduces the challenge of physically situated dialogue and the necessity for symbol and action grounding, which generally incorporate computer vision.",34,35
1897,250390632,"The difference between its accuracy on 'train' and 'train100' is due to lexical issues: we found that when trained on 'train', the AM parser typically predicts the correct delexicalized formulas and then inserts an incorrect but related constant or predicate symbol.",48,49
1898,237353087,"IsCurrency: True if some word in the phrase contains a currency symbol, e.g., $23.",12,13
1899,237353087,Context: Oxygen is a chemical element with symbol O and atomic number 8.,8,9
1900,222125204,"Given the i-th input text sequence x 1 i = (x 1 i,1 , x 1 i,2 , • • • , x 1 i,L ) in style s 1 , the LSTM autoencoder for style s 1 can be formulated as: p( x1 i |x 1 i ; enc 1 , dec 1 ) = L t=1 p(x 1 i,t |z 1 i , x1 i,<t ; enc 1 , dec 1 ) (1) where x1 i = (x 1 i,1 , • • • , x1 i,L ) is the reconstructed sequence with the same length L as x 1 i , z 1 i = enc 1 (x 1 i ) is the learned latent representation from the encoder enc 1 , x1 i,<t are the tokens generated before x1 i,t and we start the decoder by a start-of-sentence symbol ""<bos>"" which is x1 i,<1 .",163,164
1901,10761261,"w 0 is taken to be the invisible ""wall"" symbol at the left edge of the sentence; it has a single child (|{i : τ (i) = 0}| = 1).",11,12
1902,218900918,The symbol • denotes module composition.,1,2
1903,24216565,"sequence: p(s|w) = N i=1 p(s i |h i ) = N i=1 softmax(W o • h i ) T δ s i (2) where the matrix W o (output layer) consists of the vector representations of each slot tag, the symbol δ d is a Kronecker delta with a dimension for each slot tag, and the softmax function is used to estimate the probability distribution over all possible plain slots.",48,49
1904,226278354,"The model is trained to generate a sequence of pairs: (property, value) separated with a special symbol.",20,21
1905,244464082,"A paragraph is defined as a sequence of characters ending with closing punctuation marks, i.e., a full stop, exclamation mark, question mark, followed by a new line symbol.",32,33
1906,244464082,"If colons and semicolons are followed by a new line symbol, the new line is removed so that such phrases comprise a part of a larger paragraph.",10,11
1907,244464082,"After the new line symbol is deleted, if a hyphenated word is found in the library, it is left untouched.",4,5
1908,7561702,"These include simple acronyms, in which the first letter of every word from the long form is represented in the short form, as well as more complex cases such as: inner letter matches, missing short form characters, and specific substitutions (such as of a chemical element and its symbol).",54,55
1909,7561702,"Note that in some cases, the term within the parenthesis is parsed, e.g., in the following text, ELISA is extracted from the parenthesis, by removing the text beyond the ';' symbol: "". . .",37,38
1910,7561702,"The edit operations used in our model, E = E d ∪E m ∪E s , is shown in Table 3 and includes: E d , deletions of characters or words from the long form, or of single characters from the short form; E m , matches of a full of partial word from the long form to a character in the short form; and E s , word substitutions in which a word from the long form is replaced by a symbol in the short form.",87,88
1911,7561702,"As an example, we have added a number of substitution operations (see Table 3 ), including an operation for the commonly used convention of replacing a chemical element name (e.g., Sodium) with its symbol (Na).",39,40
1912,675987,"Hence, we linked the words with the = symbol.",9,10
1913,244464119,"Typically, this is done for quotation symbol styles, English contracted forms, and Russian ë.",7,8
1914,2493136,"For example, section 9 is a gene named after the functional unit to which it belongs, and abbreviated by the symbol 9.",22,23
1915,14249992,"Implementation Identification of mathematical expressions within word-tokenized text is performed using simple indicators: single character tokens (with the characters ) and standing for power set and set complement respectively), mathematical symbol unicodes, and new-line characters.",36,37
1916,226262287,"The special symbol ⊥ indicates when a particular slot is not filled, e.g., a noncause clause (b=O) has no related emotion, thus it always associates with the symbol ⊥. For example, While the total number of tags in Y is N t = 2 * (n − 1) + 1 + 1, which relies on the size of text X, resulting in the inconsistency during the training stage.",2,3
1917,226262287,"The special symbol ⊥ indicates when a particular slot is not filled, e.g., a noncause clause (b=O) has no related emotion, thus it always associates with the symbol ⊥. For example, While the total number of tags in Y is N t = 2 * (n − 1) + 1 + 1, which relies on the size of text X, resulting in the inconsistency during the training stage.",34,35
1918,11131833,"DEF contains up to four types of primitives: basic independent primitives (基本独立义元), other independent primitives (其他独立义元), relation primitives (关系义元), and symbol primitives (符号义元), where basic independent primitives and other independent primitives are used to indicate the basic concept, and the other types are used to indicate syntactical relationships.",30,31
1919,11131833,"The symbol primitives ""@ComeToWorld|问世"" and ""$congratdulate|祝 贺"" provide more specific, distinguishing features to indicate syntactical relationships.",1,2
1920,7342576,"The resulting constraint @(@(s,c),j) = C (j) presents an equation between a term with a constant @ as its (""rigid"") head symbol and a term with a context variable C as its (""flexible"") head symbol.",30,31
1921,7342576,"The resulting constraint @(@(s,c),j) = C (j) presents an equation between a term with a constant @ as its (""rigid"") head symbol and a term with a context variable C as its (""flexible"") head symbol.",47,48
1922,7342576,Projection produces a clash of two rigid head symbols @ and j. Imitation presents two possibilities for locating the argument j of the context variable C as a subtree of the two arguments of the rigid head symbol @. Both alternatives lead to new rigid-flexible situations.,37,38
1923,7342576,We assume an infinite set of HOL-variables ranged over by x and y. The signature of context constraints contains a unary function symbol lamx and a constant var.,24,25
1924,7342576,"per HOL-variable x. Futhermore, we assume a binary function symbol @ that we write in left associative infix notation and constants like john, language, etc.",12,13
1925,7342576,For example the tree (many@language)@(lamx((spoken_by@john)@varx)) represents the HOL formula (=poke by(j Note that the function symbol @ represents the application in HOL and the function symbols lamx the abstraction over x in HOL.,19,20
1926,15058567,The binary predicate symbol '= q '.,3,4
1927,15058567,"This also verifies Niehren and Thater's (2003) assumption that EP-conjunctions are ""syntactic sugar"" which can be resolved in a preprocessing step: EP-conjunctions can be resolved by exhaustively applying the following rule which adds new literals to make the implicit conjunction explicit: h : E 1 (h 1 ,... ,h n ), h : E 2 (h 1 ,... ,h m ) ⇒ h : 'E 1 &E 2 '(h 1 ,... ,h n , h 1 ,... ,h m ), where E(h 1 ,... ,h n ) stands for an EP with argument handles h 1 ,... ,h n , and where 'E 1 &E 2 ' is a complex function symbol.",144,145
1928,12693287,"We write E for the empty path and uv for the concatenation of two paths u and v. A tree T consists of a finite set of nodes u E DT , each of which is labeled by a symbol LT (u) E E. Each node u has a sequence of children ul, , un E DT where n = ar(Ly (u)) is the arity of the label of u. A single node E, the root of -r, is not the child of any other node.",39,40
1929,2037006,"Let u, v, vi, E V. The dominance relationship u<* t v holds if there is a path from u to v in E and the labeling relationship u: ft (vi , ,v"") holds iff u is labeled by the n-ary symbol f and has the children v , , vn in this order; that is, and LE((lt,Vi) Lv (u) = f, ar(f) = n, {(u,v 1), ,(u,v"")} C E, ) = i for all 1 < i < n. Let c be a dominance constraint and Var((p) be the set of variables of c. A pair of a tree structure glit and a variable assignment a: Var((p) 14, satisfies ( if it satisfies each literal in the obvious way.",53,54
1930,2037006,"Formally, a formula with n holes is a complex function symbol of arity n as above.",11,12
1931,2037006,"Its syntax-semantics interface produces dominance constraints describing formulas of higher-order logic; the symbol @ stands for functional application, and abstraction and variables are written as 'lam,' and `var x '.",17,18
1932,18620783,"For instance, ""、"" is the pause symbol, which is used somewhat like the comma but only when separating items in a list.",9,10
1933,14552960,Keyword transducers replace a set of keywords specific to a category with a symbol for that category.,13,14
1934,14552960,This extra step simplifies the syntactic pattern transducers that look for the category symbol in their pattern.,13,14
1935,14552960,Keyword transducers also add the category symbol to a list when they match; this list is used for disambiguation.,6,7
1936,14552960,"Syntactic pattern transducers likewise match, putting a category symbol on a separate disambiguation list.",9,10
1937,14552960,"In the disambiguation routine, both lists are consulted, and the first category symbol found on both lists determines the classification of the utterance.",14,15
1938,2015467,The symbol ≤ for the outscopes relation.,1,2
1939,5598476,"A tree consists of a finite set of nodes Y , each of which is labeled by a symbol T V u .",18,19
1940,7418292,"Constants -function symbols of arity 0 -are ranged over by a, b. We assume that Σ contains at least one constant and one symbol of arity at least 2.",24,25
1941,7418292,"A (finite) constructor tree τ is a pair (T, L) consisting of a tree T = (V, E), a node labeling L : V → Σ, and an edge labeling L : E → N, such that for each node u ∈ V and each 1 ≤ k ≤ ar(L(u)), there is exactly one edge (u, v) ∈ E with L((u, v)) = k. 1 We draw 1 The symbol L is overloaded to serve both as a node and an edge labeling.",89,90
1942,7418292,"The tree structure M τ of a constructor tree τ is a first-order structure with domain V τ which provides the dominance relation ¡ * τ and a labeling relation for each function symbol f ∈ Σ. Let u, v, v 1 , . . .",35,36
1943,7418292,"v n ) holds iff u is labeled by the n-ary symbol f and has the children v 1 , . . . ,",13,14
1944,227230600,"In case (a), RNN-based mult-source method just translates an irrelevant symbol rather than a meaningful sentence in English, while SIN-RNN captures the main idea of source sentences.",17,18
1945,215814127,"Note that we can define arbitrary new formats not restricted to the ones pre-defined in the corpus, thus |C| → ∞. Format token c i denotes a place-holder symbol of C which need to be translated into a real word token.",33,34
1946,215814127,Note that we align the position symbol indices in a descending order.,6,7
1947,215814127,"Segment Symbols: S = {s 0 , s 0 , s 0 , s 0 , s 0 , /s s 1 , s 1 , s 1 , s 1 , s 1 , s 1 , s 1 , /s , eos } (5) where s i is the symbol index for sentence i. The purpose is to enhance the interactions between different sentences in different positions by defining the sentence index features.",55,56
1948,215814127,"Specifically, we randomly (say 20%) select partial of the original content and keep them not changed when building the format symbols C. For example, we will get a new symbol set C for the example sentences: C = {c0, c0, c0, love, c1, /s bends, c0, c0, c0, c0, remove, c1, /s , eos } where ""love"", ""bends"" and ""remove"" are kept in the format C .",34,35
1949,6241225,"The end of the stack sequence is then determined by a special end symbol, which can only be emitted within the T frames once all mandatory stacks have been visited.",13,14
1950,6241225,"The probability of the resulting utterance is thus computed over all frames up to the end symbol, which determines the length L of S * and R * .",16,17
1951,6241225,"We thus divide pre-terminal concepts in the semantic stacks into two types: (a) enumerable attributes whose values are associated with distinct semantic stacks in our model (e.g., inform(pricerange(cheap))), and (b) non-enumerable attributes whose values are replaced by a generic symbol before training in both the utterance and the semantic stack (e.g., inform(name(X)).",54,55
1952,6241225,"2007) , which results in 202 unique dialogue acts after replacing nonenumerable values by a generic symbol.",17,18
1953,14875383,"The end of the stack sequence is then determined by a special end symbol, which can only be emitted within the T frames once all mandatory stacks have been visited.",13,14
1954,14875383,"The probability of the resulting utterance is thus computed over all frames up to the end symbol, which determines the length L of S * and R * .",16,17
1955,14875383,"As a result, the probability that inform(pricerange(cheap)) is followed by inform(type(restaurant)) will be high even if inform(type(restaurant)) was not observed, as long as inform(pricerange(cheap)) is frequently followed by the tail symbol inform(type(SOMETHING)) in the training data.",40,41
1956,14875383,the probability that the head symbol restaurant governs inform(type(SOMETHING)) in the second factor.,5,6
1957,14875383,"We thus divide pre-terminal concepts in the semantic stacks into two types: (a) enumerable attributes whose values are associated with distinct semantic stacks in our model (e.g., inform(pricerange(cheap))), and (b) non-enumerable attributes whose values are replaced by a generic symbol before training in both the utterance and the semantic stack (e.g., inform(name(X)).",54,55
1958,14875383,"For example, a trigram phrase model with a vocabulary of size V requires searching over V symbols times V 2 paths leading to that symbol at every time step.",25,26
1959,14875383,"At the realization stage, the is a realization phrase is associated with a high probability, given an inform stack following a restaurant name and a sentence start symbol, while the phrase food following is a Y is allowed because the unseen context gets dropped by the back-off strategy.",29,30
1960,14875383,"The resulting data set consists of 1,646 unique dialogue acts after replacing nonenumerable values by a generic symbol.",17,18
1961,14875383,"Because BAGEL relies on the prediction of an end symbol, we extend O&R's model with an end symbol determining when to end the utterance.",9,10
1962,14875383,"Because BAGEL relies on the prediction of an end symbol, we extend O&R's model with an end symbol determining when to end the utterance.",19,20
1963,21703354,trademark symbol McRib[ R ] The trademark symbol usually appears right after the brand name.,1,2
1964,21703354,trademark symbol McRib[ R ] The trademark symbol usually appears right after the brand name.,8,9
1965,21703354,"This set can be subdivided into seven categories: company name, brand name, series, model, trademark symbol, type and feature.",20,21
1966,9625371,the mapping itself must be accessible to some form of symbol querying by the NL/MM interface.,10,11
1967,5349301,"So, in this line of discussion, it is possible to think of the pronoun it in Figure 1 as a deictic pronuon which refers to the world, although in an indirect manner through a symbol of a different modality.",37,38
1968,51918810,"In the proposed modeling, we use both symbol sequences by converting them into continuous vectors.",8,9
1969,235390604,"At the first time step, we use the special symbol ""SOS"" as y 0 and the initial hidden state h * 0 = h a .",10,11
1970,227231160,"To create the lexicon, we took the gene symbol, alias and description information for each gene identifier matched the following query on NCBI 1 : "" 'Homo sapiens'[porgn] AND alive[prop]"".",9,10
1971,227231064,"One of the most complex tasks in ad analysis is symbol interpretation, a much harder problem than, for instance, object detection.",10,11
1972,227231064,Both factors make symbol interpretation difficult.,3,4
1973,227231064,"They introduced several annotation tasks: topic detection, sentiment detection, symbol recognition, strategy analysis, slogan annotation, and question answering for texts related to the ads' messages and motivation.",12,13
1974,227231064,"In media studies, certain content that stands for a conceptual symbol is called a ""signifier"" (in the case of this dataset, signifiers are marked with bounding boxes on the image), and the symbols themselves are the ""signified"" (Williamson, 1978) .",11,12
1975,227231064,"2017) , we treat symbol prediction as a multi-label classification problem, using the two provided label sets with 221 and 53 labels.",5,6
1976,227231064,We have also tried an approach that uses the location of signifiers (symbols) on input images to improve symbol recognition.,20,21
1977,227231064,"Objects were detected in both training and validation sets, and we retained only those objects from the training set that intersect with given symbols with IoU (Intersection over Union) over 0.6 (fixed threshold); the objects were cropped and put in correspondence with symbol labels.",48,49
1978,227231064,"We solved multilabel classification for symbol recognition by feeding the images of these objects into pre-trained CNN feature extractors, namely MobileNets v1/v2 and EfficientNets B0/B3 (Tan and Le, 2019) , with a single dense layer on top of them for classification.",5,6
1979,227231064,"The final decision for a test image was made as follows: (1) detect objects using the same Faster R-CNN; (2) extract their visual features with pre-trained CNNs; (3) classify the features with the shallow network; (4) unite its outputs (scores or predictions of symbol posterior probabilities) with non-maximal suppression, return only symbols with scores exceeding a given threshold.",60,61
1980,227231064,"Conclusion We have presented a novel approach to symbol classification on multimodal advertisement data, improving upon state of the art results already with pure image-based approaches and showing further improvements with text-based methods.",8,9
1981,226283717,"We used regex in order to transform dates written with dashes to dates written with slashes, • some articles use comma and others use dot as a thousand symbol separator.",29,30
1982,226283717,"We used regex in order transform all thousand symbol separators to dots, • in the articles, some countries are written by their acronym instead of by their entire name (eg.",8,9
1983,2507563,"Let a context-free grammar G be a quadruple < N, T, R, S > where N and T are finite disjoint sets of nonterminal symbols and terminal symbols, respectively, R is a set of rules of the form A -+ a (A is a nonterminal and a a possibly empty string of nonterminal or terminal symbols), S is a speciM nontermin~l, called start symbol.",76,77
1984,2507563,"An ordered directed graph marked according to grammar ~ is a triple < V,E,m > so that V is a finite set of vertices or nodes, E a finite set of edges e of the form (vl, (v2,... ,vn) ) (vi C V,n > 2, e starts at vl, vl is the predecessor of v2,..., vn), m is the marking function which associates with each vertex a terrainai or nonterminai symbol or the special symbol e. m is restricted so that the vertices on each edge are marked with the,symbols of a rule in 6, the empty string being represented by the additional symbol ~. A parse tree is an ordered directed acyclic graph (DAG) satisfying the following constraints.",92,93
1985,2507563,"An ordered directed graph marked according to grammar ~ is a triple < V,E,m > so that V is a finite set of vertices or nodes, E a finite set of edges e of the form (vl, (v2,... ,vn) ) (vi C V,n > 2, e starts at vl, vl is the predecessor of v2,..., vn), m is the marking function which associates with each vertex a terrainai or nonterminai symbol or the special symbol e. m is restricted so that the vertices on each edge are marked with the,symbols of a rule in 6, the empty string being represented by the additional symbol ~. A parse tree is an ordered directed acyclic graph (DAG) satisfying the following constraints.",96,97
1986,2507563,"An ordered directed graph marked according to grammar ~ is a triple < V,E,m > so that V is a finite set of vertices or nodes, E a finite set of edges e of the form (vl, (v2,... ,vn) ) (vi C V,n > 2, e starts at vl, vl is the predecessor of v2,..., vn), m is the marking function which associates with each vertex a terrainai or nonterminai symbol or the special symbol e. m is restricted so that the vertices on each edge are marked with the,symbols of a rule in 6, the empty string being represented by the additional symbol ~. A parse tree is an ordered directed acyclic graph (DAG) satisfying the following constraints.",128,129
1987,2507563,The root is marked with the start symbol.,7,8
1988,2507563,"A packed shared forest for an input string a obeys the further constraint that, there must be at most one vertex for each grammar symbol and substring of a. Thus, if a consists of n words, there will be at most k * n 2 vertices in the parse forest for it (k being constant).",25,26
1989,2507563,"aThc Prolog symbol leq represents the UDRS subordination relation <. ], [",2,3
1990,17453203,"A second alternative is to allow the LVSCR system to enter into the transcription a symbol meaning ""something outside the vocabulary"".",15,16
1991,27352487,"Furthermore, all the Internet and Twitterspecific tokens that, according to UD specifications, should be classified as SYM (symbol) were further specified based on the token type.",21,22
1992,27352487,The @ symbol that characterizes the so-called mentions and replies is used to call out usernames in tweets.,2,3
1993,27352487,13 http://universaldependencies.org/u/ dep/discourse.html 14 https://support.twitter.com/articles/ 464314 (3) @ChiaZe93 io non sono d' accordo ( @ChiaZe93 I disagree ) vocative Hashtags are key words or phrases preceded by the # symbol.,34,35
1994,110013556,"The information captured for each token is: physical string, token symbol, token type.",12,13
1995,110013556,"symbol id, part of speech, string case type, and character start and end positions.",0,1
1996,15575166,"CFG is formalised by 4-tuple: G = (T, N, S, R) where T is set of terminals (lexicon) shown in Figure 3 , N is a set of non-terminals (usually POS tags), S is a start symbol (one of non-terminals).",51,52
1997,1183321,"When we need to refer to the ith symbol of the word w, we will denote this by w [i] , preserving ordinary subscripts for the enumeration of words.",8,9
1998,12921807,"A context-free grammar (CFG) is a tuple G = (N, Σ, R, S), where N is a finite set of nonterminal symbols, Σ is a finite set of terminal symbols disjoint from N , S ∈ N is the start symbol and R is a finite set of rules.",51,52
1999,12921807,"Without loss of generality, we assume that in G the start symbol S is never used in the right-hand side of a rule.",12,13
2000,12921807,"From our assumptions on the start symbol S, we have that S only appears at the root of the trees in T (G).",6,7
2001,12921807,"For each A ∈ N , quantity out G (A) is the sum of the probabilities of all trees generated by G, having root labeled by S and having a yield composed of terminal symbols with an unexpanded occurrence of nonterminal A. Again, we assume that symbol S does not appear in any of the right-hand sides of the rules in R. This means that S only appears at the root of the trees in T (G).",50,51
2002,12921807,"We have already observed in Section 3 that, under our assumption on the start symbol S, we have E p T f (S, t) = 1. (",15,16
2003,16060349,"A Stochastic Context-Free Grammar is a quadruple (Σ, N, S, R), where Σ and N are respectively the terminal and nonterminal alphabets, with N ∩ Σ = φ; S ∈ N is the start symbol; and R is the set of rewriting probabilistic rules.",44,45
2004,16060349,"It is assumed, without loss of generality, that R i contains only one rule rewriting the start symbol S i , that is S i → ξ.",19,20
2005,16060349,"The initial symbol X init is defined as [S i → •ξ], while the final symbol is X final = [S i → ξ•].",2,3
2006,16060349,"The initial symbol X init is defined as [S i → •ξ], while the final symbol is X final = [S i → ξ•].",18,19
2007,16060349,"Segments can only have the form αX a =⇒ + βY , where at least one of α and β is null, X is the start symbol or results from a pop or a scan and Y is the end symbol or prepares a scan or a push ([ A i → α i • bβ i ] with b ∈ Σ ∪ N i ).",28,29
2008,16060349,"Segments can only have the form αX a =⇒ + βY , where at least one of α and β is null, X is the start symbol or results from a pop or a scan and Y is the end symbol or prepares a scan or a push ([ A i → α i • bβ i ] with b ∈ Σ ∪ N i ).",42,43
2009,16060349,"More in detail, the search is conducted by considering a particular stack symbol Q and gives as a result new items which have this symbol in the part of the stack which precedes the * position, if such items are consistent with the analysis.",13,14
2010,16060349,"More in detail, the search is conducted by considering a particular stack symbol Q and gives as a result new items which have this symbol in the part of the stack which precedes the * position, if such items are consistent with the analysis.",25,26
2011,16060349,"The choice of the action depends on whether the dot is at the end of the right-hand side, or whether the symbol following the dot is a terminal or a nonterminal.",24,25
2012,16060349,"Initialization: Pr γ (S → •ξ | * X init , * X init ) = Pr(S → ξ) = 1 Scanning: Pr γ (A → µa • ν σa | * α, * γδ) = β Pr γ (A → µ • aν σ | * α, * γβ) Pr(β a =⇒ + δ) Pr γ (A → µa • ν σa | * γα, * δ) = β Pr γ (A → µ • aν σ |γ * α, γ * β) Pr(γβ a =⇒ + δ) Prediction: Pr γ (C → •ξ | * X, * X) = Pr(C → ξ) if Pr γ (A → µ • Bν σ | * α, * βX) > 0 and R(B * ⇒ C) > 0 Completion: Pr γ (A → µB • ν σ | * α, * γδ) = = β,C→ξ σ1, σ2 : σ1σ2 = σ Pr γ (A → µ • Bν σ 1 | * α, * γβ)R(B * ⇒ C)Pr γ (C → ξ • σ 2 | * β, * δ) Pr γ (A → µB • ν σ | * γα, * δ) = = β,C→ξ σ1, σ2 : σ1σ2 = σ Pr γ (A → µ • Bν σ 1 |γ * α, γ * β)R(B * ⇒ C)Pr γ (C → ξ • σ 2 | * γβ, * δ) If the symbol following the dot is a terminal, in addition to the scanning operation, also backward search is launched to look for stack symbols that can allow further stack operations.",285,286
2013,16060349,"If, on the other hand, the symbol immediately following the dot is a nonterminal, then it is necessary to look in the completed analyses list to check which analyses can be used.",8,9
2014,2475802,"This information will be recorded as a correction attribute on token annotations and possibly on name annotations: { symbol: string, constituents: sequence of (parse or token or name)}; Invoking Annotators Each TIPSTER system will be provided with a number of ""annotators"" procedures for generating annotations.",19,20
2015,33850483,"If your product uses icons to indicate information storage media, it is very important to be aware that a symbol like the filing cabinet is not transferable to just any target locale.",20,21
2016,59588992,"is • defined as a quadruple G 8 = (N, :E, P, S) , where N is a finite set of nontermi nal symbols, :E is a finite set of terminal symbols disjoint from N, P is a finite set of productions of the form H -+""et, H EN, a E (:E U N)*, and S EN is a special symbol called start symbol.",75,76
2017,59588992,"is • defined as a quadruple G 8 = (N, :E, P, S) , where N is a finite set of nontermi nal symbols, :E is a finite set of terminal symbols disjoint from N, P is a finite set of productions of the form H -+""et, H EN, a E (:E U N)*, and S EN is a special symbol called start symbol.",78,79
2018,59588992,cl by G 8 starting from symbol H .,6,7
2019,59588992,The framework introduced in this paper can also be used to predict words adjacent to an already rec ognized string and to compute the probability that the first (last) word x1 (x m ) of a gap is a certain symbol a E I;.,43,44
2020,37915543,"2015) and a in-home emoticons lexicon, (3) normalizing hashtags by removing the # symbol and the underscores that connect words in composite hashtags, and (4) normalizing letter repetitions (elongations).",19,20
2021,1637866,We use i = 0 for the root symbol.,8,9
2022,6520568,"2015) , and prepared our own emoticons lexicon), (3) normalizing hashtags by removing the ""#"" symbol and the underscores that are used to separate words in composite hashtags, and (4) normalizing word elongations (letter repetitions).",22,23
2023,3069760,"The slash (""/"") symbol stands for part of speech attribute, ""at"" (""@"") stands for the lexeme attribute while square brackets (""[,]"") indicate a list of values that are accepted for a match.",7,8
2024,237353486,Each symbol in the top bar chart represents an information source for propaganda detection.,1,2
2025,6265951,"Subtrees and Subset Trees In our study, we consider syntactic parse trees, consequently, each node with its children is associated with a grammar production rule, where the symbol at left-hand side corresponds to the parent node and the symbols at right-hand side are associated with its children.",31,32
2026,5190289,"makes prohibited) an Actant Slot (ASlot) of symbol s. The set of Primitive Unit Types (PUTs) is denoted T and defined as the disjoint union of T D , the range of γ γ γ, γ γ γ 1 and γ γ γ 0 , plus the prime universal PUT and the prime absurd PUT ⊥. T is then pre-ordered by a relation which is computed from the set of asserted PUTs comparisons C A ⊆ T 2 .",10,11
2027,5190289,"Thus as one goes down the hierarchy of unit types, an ASlot with symbol s is introduced by the radix {γ γ γ(s)} and first defines an optional ASlot for any unit type t ∩ more specific than {γ γ γ(s)}, as long as t ∩ is not more specific than the obligat {γ γ γ 1 (s)} (resp.",14,15
2028,5190289,"Finally, to each CSymbol is assigned a signature that specifies the type of units that are linked through a relation having this symbol.",23,24
2029,240635028,"This is worse than useless unless accompanied by a completely detailed description of how the thing was put together, what every code symbol means; everything about it so someone can reproduce the results.",23,24
2030,809755,"A difficulty is also presented by cases where no explicit conjunction is stated in the text, nor is there a punctuation symbol (such as a comma) which would serve its role.",22,23
2031,1995357,"basic parser: this produces two scores, a binary flag indicating whether any bracketing inside the target sentence is correct, and one indicating if the sentence ends with an end of sentence symbol (period, colon, semi-colon, question/exclamation/quotation mark, comma, apostrophe, close parenthese) • out-of-vocabulary: this generates two scores, the number of out-of-vocabulary words in the sentence, and the same one but normalized by the length of the sentence.",34,35
2032,17976898,"However, the introduction of Paiwan, in which there is a distinction between palatalized and non-palatalized sounds, forced us to change our writing policy though, as c is the standard IPA symbol used to represent a palatal stop.",36,37
2033,17976898,Each IPA symbol is automatically converted into its corresponding numeric reference entity throughout a document.,2,3
2034,17976898,"In Microsoft Word (e.g., 2000/XP), there are several ways to insert a Unicode IPA symbol.",20,21
2035,17976898,"Initially, that symbol was not used in our glosses because in the languages that we were annotating (Rukai, Tsou, Atayal and Saisiyat), two infixes barely co-occur simultaneously.",3,4
2036,211069634,"In this work, we introduce the Blank Language Model (BLM), which uses a special "" "" symbol to control where tokens can be placed.",20,21
2037,211069634,"Each blank represents a nonterminal symbol or the start symbol, In the first stage, an index is chosen among all current blank positions.",5,6
2038,211069634,"Each blank represents a nonterminal symbol or the start symbol, In the first stage, an index is chosen among all current blank positions.",9,10
2039,211069634,indicates that the preceding symbol is optional.,4,5
2040,158652,"Second, we include per-symbol parameters θ x,y which, in principle, are capable of specifying any emission distribution on their own.",6,7
2041,158652,"In our work, we map proper nouns to nouns and map symbol marks 3 and interjections to a catch-all tag X because it is hard and unnecessary to disambiguate them in a low-resource learning scenario.",12,13
2042,5509327,"For this purpose, we use the CMU Dictionary of word pronunciations, 3 which is based on the ARPAbet symbol set and consists of about 130K word-to-phoneme pairs.",20,21
2043,1994530,"n}, i = j} Here (i, j) represents a dependency with head w i and modifier w j (i = 0 corresponds to the root symbol in the parse).",32,33
2044,1994530,"Each symbol in the grammar takes the form A(h) where A ∈ N is a non-terminal, and h ∈ {1 . . .",1,2
2045,1994530,"There are two possible parts of speech, A and B, and an additional non-terminal symbol X. The sentence is of length 3, w 1 w 2 w 3 .",18,19
2046,15816722,"Since Vietnamese uses a phonetic alphabet, it is never the case that a given segment-tone can be expressed by more than one written symbol.",26,27
2047,15816722,"Without an additional indexing logographic symbol, they may have separate entries for syllable and tone, since the two elements are presented separately in the writing (prosodic components of Vietnamese words [tonality] are represented by diacritical marks above vowel symbols in Vietnamese writing.",5,6
2048,15816722,"More specifically, the strategy used by native Chinese readers for holding words in short-term memory was acoustically oriented, in that they immediately associated the symbol with a particular sound, while the non-native Chinese readers rely more on graphic processing.",28,29
2049,8854945,"In addition, each lexical node is encoded as a word lemma, and has a suffix which is composed by a special :: symbol and the first letter of the POS-tag of the word.",25,26
2050,144306,"A Linear Indexed Grammar is a 5-tuple (V, T, I, P, S), where V is the set of variables, T the set of terminals, I the set of indices, S in V is the start symbol, and P is a finite set of productions of the form, where A, B ∈ V , α, γ ∈ (V ∪ T ) * , i ∈ I: a. A[..] → α B[..] γ b. A[i..] → α B[..] γ c. A[..] → αB[i..] γ Example 1 L(G wcw ) = {wcw |w ∈ {a, b} * }, G ww = ({S, R}, {a, b}, {i, j}, S, P ) and P is: 1.S[..] → aS[i..] 2.S[..] → bS[j..] 3.S[..] → cR[..] 4.R[i..] → R[..]a 5.R[j..] → R[..]b 6.",47,48
2051,144306,"Definition 1 A GIG is a 6-tuple G = (N, T, I, S, #, P ) where N, T, I are finite pairwise disjoint sets and 1) N are non-terminals 2) T are terminals 3) I a set of stack indices 4) S ∈ N is the start symbol 5) # is the start stack symbol (not in I,N ,T ) and 6) P is a finite set of productions, having the following form, 5 where x ∈ I, y ∈ {I ∪ #}, A ∈ N , α, β ∈ (N ∪ T ) * and a ∈ T .",64,65
2052,144306,"Definition 1 A GIG is a 6-tuple G = (N, T, I, S, #, P ) where N, T, I are finite pairwise disjoint sets and 1) N are non-terminals 2) T are terminals 3) I a set of stack indices 4) S ∈ N is the start symbol 5) # is the start stack symbol (not in I,N ,T ) and 6) P is a finite set of productions, having the following form, 5 where x ∈ I, y ∈ {I ∪ #}, A ∈ N , α, β ∈ (N ∪ T ) * and a ∈ T .",72,73
2053,199379835,In Table 2 the symbol x-y means that all the n-grams features from x to y of the corresponding column are taken into account in the classification.,4,5
2054,204913961,1 fundamentally expresses an assumption that all the relevant information in the context about the symbol w i is concentrated in exactly one other symbol t i -corresponding to the dependency grammar postulate described above.,15,16
2055,204913961,1 fundamentally expresses an assumption that all the relevant information in the context about the symbol w i is concentrated in exactly one other symbol t i -corresponding to the dependency grammar postulate described above.,24,25
2056,204913961,"More formally, let p L be a conditional probability distribution with support over symbols w i , called words, and a special sentinel symbol which marks the end of a sentence.",25,26
2057,204913961,"The distribution p L generates symbols conditional on a sequence of previous words w <i , called a context, also generated by p L , and starting with a special beginning-of-sentence symbol called root.",37,38
2058,204913961,"3 We hold the distribution p L to be a fixed target, and we are interested in finding the assignment of heads t that minimizes the expected per-symbol KL-divergence between the dependency approximation p t of L and the true distribution p L : D KL (p L (w i |w <i )||p t (w i |t i )) = E log p L (w i |w <i ) p t (w i |t i ) . (",30,31
2059,15704866,Term Expansion The documents are preprocessed by tokenization of punctuation and mapping all numbers to a generic symbol.,17,18
2060,15704866,"The vocabulary of the models is restricted to those words that occur at least 3 times in the training data; all others are mapped to a generic ""unknown word"" symbol.",32,33
2061,10014168,6) | * ##ceive| V ↔ | * ##ception| Ns The '#' signs in the above representations stand for letters that must be instantiated but are not specified; the ' * ' symbol stands for a letter that is not specified and that may or may not be instantiated.,40,41
2062,16007011,Table 3 shows the count of the semantic relations with symbol codes.,10,11
2063,7923167,"Another interesting treatment, proposed in this article, is the auto-correction of hamoza in the ASR system output in order to rectify the orthography confusion of this symbol above or below Alif.",30,31
2064,7923167,Another issue in Arabic concerns the omission of the symbol hamoza which is pronounced but often not written.,9,10
2065,7923167,"Auto-correction of hamoza The hamoza symbol is widely used in Arabic; by analyzing the errors of our ASR system on the Dev corpus, we noticed that one of the main errors concerns the presence or not of this symbol above or below Alif.",7,8
2066,7923167,"Auto-correction of hamoza The hamoza symbol is widely used in Arabic; by analyzing the errors of our ASR system on the Dev corpus, we noticed that one of the main errors concerns the presence or not of this symbol above or below Alif.",42,43
2067,1810460,"We combine the tables on the left side and right side of  Later the recordsets that have the relation symbol as ""\""(denoted ""derived from,"" refer to Table 2 ) are extracted and these derived adjectives and adverbs are further assigned with the same domain as the nouns they are derived from.",20,21
2068,573746,"w i+2 ) The other features are intra-lingual features: each word is assigned its average mutual information with the other words in the sentence; interlingual features: each word in target sentence is assigned its average mutual information with the words in source sentence; IBM1 features: contrary to IBM1 based baseline features which take into account the number of translations, we use the probability values in the translation table between source and target words; basic parser (correction of bracketing, presence of end-of-sentence symbol); number and ratio of out-of-vocabulary words in source and target sentences.",96,97
2069,14768095,"Additional work on RBMT focused on issues revealed through manual inspection of its performance on the QTLeap corpus (see also section 3): • Separate menu items: The rule-based system was observed to be incapable of handling menu items properly, mostly when they were separated by the "">"" symbol, as they often ended up as compounds.",55,56
2070,35616700,All numerical tokens were replaced with zero and tokens appearing less than 10 times were replaced with a special symbol.,19,20
2071,18061240,"For example, in the case of ¹⁰http://finereader.abbyy.com/ Surgut Khanty, the close central rounded vowel represented with the IPA symbol /ʉ/ is marked with /ü/ by Steinitz and with /u/ by Márta Csepregi.",20,21
2072,196189944,"A General TRL Scheme As noted in § 2, PBLM is similar to an LSTM language model, but instead of predicting the next word at each position, it predicts the next unigram or bigram if these are pivots and a special NONE symbol otherwise.",45,46
2073,18182164,"Son bođii guokte jagi ovdal Case=Acc Case=Gen She came two year ago @num< @<advl @subj> @<advl (a) Genitive complement Sii addet stipeandda golmma studentii Case=Acc Case=Ill They gave stipend three to student @>n @<advl @subj> @<obj (b) Attributive numeral Throughout the article we use the convention of prefixing an @ symbol to dependency labels using the Giellatekno scheme, labels without the @ are Universal dependency labels.",65,66
2074,1707814,We generate data so that every sequence is split into three parts with the same size and emit one meaningful symbol in first and second parts of a sequence.,20,21
2075,4763722,"In the first step 'customization of surface form', the tokenization module was modified to tokenize and/or chunk very complex symbol words, a chemical formula, a mathematical formula, programming codes, and so on.",22,23
2076,248512647,"We add a special symbol to the generated sentences to help the models differentiate between synthetic and authentic source language data (Caswell et al.,",4,5
2077,19028093,"In the first step 'customization of surface form', the tokenization module was modified to tokenize and/or chunk very complex symbol words, a chemical formula, a mathematical formula, programming codes, and so on.",22,23
2078,10397022,"In the first phase for customization of surface form analysis, the tokenization module is modified to tokenize and/or chunk very complex symbol words, a chemical formula, a mathematical formula, programming codes, and so on.",22,23
2079,16805396,A Korean verb pattern is linked to its corresponding Chinese verb pattern by the symbol `>'.,14,15
2080,52346770,"For an n-words input text w 1 , ..., w n we embed each symbol as d dimensional vector, resulting in word vectors w 1 , ..., w n ∈ R d .",17,18
2081,2952144,"In proving P G P L we will use a simple problem where every example seen in training or test data is one of the following two tagged sentences: x 1 x 2 x 3 = a b c, d 1 d 2 d 3 = A B C x 1 x 2 x 3 = a b e, d 1 d 2 d 3 = A D E (7) Note that the input x 2 = b is ambiguous: it can take tags B or D. This ambiguity is resolved when the next input symbol, c or e, is observed.",101,102
2082,2952144,"The ambiguity at input symbol b is naturally resolved when the next symbol (c or e) is observed, but the locally normalized model is not able to revise its prediction.",4,5
2083,2952144,"The ambiguity at input symbol b is naturally resolved when the next symbol (c or e) is observed, but the locally normalized model is not able to revise its prediction.",12,13
2084,2952144,"7 ) by allowing scores ρ(d 1:i−1 , d i , x 1:i+1 ) that take into account the input symbol x i+1 .",24,25
2085,53083422,"In these models, the decoding is still sequential, because the probability of emitting a symbol is conditioned on the previously decoded symbols.",16,17
2086,53083422,"Finally, the decoder states are labeled either with an output token or a null symbol.",15,16
2087,53083422,The algorithm computes and stores partial log-probabilities sums for all prefixes and suffixes of the output symbol sequence using dynamic programming.,18,19
2088,12771016,Attentive S2S Learning The attention mechanism in S2S learning allows an RNN decoder to directly access information about the input each time before it emits a symbol.,26,27
2089,233296723,"The input symbols are represented by learned, possibly contextual embeddings (yellow and blue boxes in Figure 1 ) which are used to compute a representation of symbol pairs with a small feed-forward network.",28,29
2090,233296723,"The symbol pair representation is used to estimate the probabilities of insert, delete and substitute operations (blue, red and green arrows in Figure 1 ).",1,2
2091,233296723,"We represent the symbol-pair contexts as a function of the respective symbol representations (small gray rectangles in Figure 1 ) as a function of repspective symbol representation c i,j = f (h s i , h t j ) depending on the task.",3,4
2092,233296723,"We represent the symbol-pair contexts as a function of the respective symbol representations (small gray rectangles in Figure 1 ) as a function of repspective symbol representation c i,j = f (h s i , h t j ) depending on the task.",13,14
2093,233296723,"We represent the symbol-pair contexts as a function of the respective symbol representations (small gray rectangles in Figure 1 ) as a function of repspective symbol representation c i,j = f (h s i , h t j ) depending on the task.",28,29
2094,233296723,"The degree of contextualization spans from static symbol embeddings with the same strong interpretability as statistical models, to Transformers with richly contextualized representations, which, however, makes our model more similar to standard black-box models.",7,8
2095,233296723,"The symbol-pair context c i,j is computed as LN ReLU Linear(h s i ⊕ h t j ) ∈ R d , (4) where LN stands for layer normalization and ⊕ means concatenation.",1,2
2096,233296723,"To better contextualize the generation, we add attention to the symbol-pair representation c i,j : LN ReLU Linear(h s i ⊕ h t j ) ⊕ Att h t j , h s (5) of dimension 2d, where Att(q, v) is a multihead attention with queries q and keys and values v. While generating the string left-to-right, the only way a symbol can be generated is either by inserting it or by substituting a source symbol.",11,12
2097,233296723,"To better contextualize the generation, we add attention to the symbol-pair representation c i,j : LN ReLU Linear(h s i ⊕ h t j ) ⊕ Att h t j , h s (5) of dimension 2d, where Att(q, v) is a multihead attention with queries q and keys and values v. While generating the string left-to-right, the only way a symbol can be generated is either by inserting it or by substituting a source symbol.",76,77
2098,233296723,"To better contextualize the generation, we add attention to the symbol-pair representation c i,j : LN ReLU Linear(h s i ⊕ h t j ) ⊕ Att h t j , h s (5) of dimension 2d, where Att(q, v) is a multihead attention with queries q and keys and values v. While generating the string left-to-right, the only way a symbol can be generated is either by inserting it or by substituting a source symbol.",90,91
2099,233296723,"Therefore, we estimate the probability of inserting symbol t j+1 given a target prefix t :j from the probabilities of inserting a symbol after t j or substituting any s i by t j+1 (i.e., averaging over a row in Figure 1 ): P (t j+1 | t:j , s) = |S| j=1 α i,j P ins (t j+1 |c i,j ) + |S| j=2 α i,j P subs (s i , t j+1 |c i,j ). (",8,9
2100,233296723,"Therefore, we estimate the probability of inserting symbol t j+1 given a target prefix t :j from the probabilities of inserting a symbol after t j or substituting any s i by t j+1 (i.e., averaging over a row in Figure 1 ): P (t j+1 | t:j , s) = |S| j=1 α i,j P ins (t j+1 |c i,j ) + |S| j=2 α i,j P subs (s i , t j+1 |c i,j ). (",24,25
2101,233296723,"Probablity P del is unkown at this point because computing it would be computed based on state c i,j+1 which is impossible without what the (j + 1)-th target symbol is, where logits for P ins and P subs use c i,j and c i−1,j .",32,33
2102,233296723,"7) At inference time, we decide the next symbol tj based on Pi,j .",10,11
2103,233296723,"Knowing the symbol allows computing the P i,j distribution and values α •,j that are used in the next step of inference.",2,3
2104,233296723,The interpretable context-free (unigram) encoder uses symbol embeddings summed with learned position embeddings.,10,11
2105,233296723,"2019) , we use the representation of the first technical symbol as an input to a linear classifier.",11,12
2106,233296723,Models that use static symbol embeddings as the input perform worse than the black-box S2S models in both tasks.,4,5
2107,233296723,Local contextualization with CNN improves the performance over static symbol embeddings.,9,10
2108,233296723,"Our model is stronger because, in the WFST, the output symbol generation only depends on the contextualized source symbol embedding, disregarding the string generated so far.",12,13
2109,233296723,"Our model is stronger because, in the WFST, the output symbol generation only depends on the contextualized source symbol embedding, disregarding the string generated so far.",20,21
2110,233296723,Several neural sequence-matching methods utilize a scoring function similar to symbol-pair representation.,12,13
2111,233296723,The symbol embeddings are summed with learnable position embeddings before the convolution.,1,2
2112,233296723,"We include all main loss functions with weight 1.0, i.e., for string-pair matching: the EM loss, non-matching negative log-likelihood and binary cross-entropy; for string transduction: the EM loss and next symbol negative log-likelihood.",44,45
2113,233307206,"For instance, the following function detects entities of type MONEY by searching for numbers preceded by a currency symbol like $ or e: def money_detector(doc): """"""Searches for occurrences of MONEY entities in text"""""" for tok in doc[1:]: if (tok.text[0] .isdigit() and tok.nbor(-1).is_currency): yield tok.i-1, tok.i+1, ""MONEY"" skweak also provides functionalities to easily construct heuristics based on linguistic constraints (such as POS patterns or dependency relations) or the presence of neighbouring words within a given context window.",19,20
2114,216867841,"The neural NER models are provided in two versions: one that directly outputs the raw model predictions, and one that runs a shallow postprocessing step on the model predictions to correct known recognition errors (for instance, ensuring that a numeric amount that is either preceded or followed by a currency symbol is always classified as an entity of type MONEY).",54,55
2115,15879713,Each grammar symbol spans all the input to its left until another grammar is encountered.,2,3
2116,31211,"4) V → kicked The symbol ↑ is a metavariable representing the fstructure of the parent of a node in the c-structure, and ↓ the f-structure of that node itself.",6,7
2117,31211,"The first implication says that if (↑SUBJ) σ is available (i.e., if we have already built the semantic projection for the verb's subject-in our example, Bradshaw), it will be consumed and will saturate the first variable of the lambda expression, to produce the new premise that follows the first symbol, leaving us with λY .kick(Bradshaw,Y ) : (↑OBJ) σ ↑ σ .",60,61
2118,14251173,"The symbol ""//"" before ""flea bag"" represents a fused element: the keyword hotel does not compose with the value of the LF function to form a collocation.",1,2
2119,9707388,"With NPRs, a word's pronunciation is represented according to the ordinary spelling rules of English, without attempting to represent each sound with a unique symbol.",27,28
2120,7658355,A symbol sym is a lin -el and refers to a daughter of the node the linearization rule is associated with.,1,2
2121,7980682,"In traditional Japanese, numbers up to 104 are formed by using the kanji digits in conjunction with the kanji symbols for the various powers of ten up to 1000, e.g., 6542 would be written (6)( 1000)(5)(100) (4)(10) (2), with each number in parentheses replaced by the appropriate kanji symbol.",60,61
2122,7980682,"In the current phase, we have chosen a lexicalized context-free grammar, which has the property that the probability of choosing a particular production rule in the grammar is dependent on headwords associated with each non-terminal symbol.",41,42
2123,7844724,"To increase the robustness of the matching, we add a simplified predicate symbol which contains only the lemma and part-of-speech.",13,14
2124,7844724,This makes matching possible in cases where the ERG has given different predicate symbol interpretations of the same word in text and hypothesis.,13,14
2125,218973755,"For each pair of NPs in a bi-sentence, we obtain a BERT sentence embedding by concatenating the two sentences (with a sentence boundary symbol in-between) before using them as input to the pretrained BERT model and extracting weights by summing up its last 4 hidden layers.",27,28
2126,17781445,"The tags are meant to provide the following information: • The multi-character symbol ˆssˆstands for stem start, and ˆseˆfor stem end. •",15,16
2127,218517099,"Then, the first symbol of the input sequence 6 is fed into a sequence of dense feedforward layers D to obtain a final output score, i.e., y = D(h 0 ).",4,5
2128,218517099,Some models also use a second embedding layer to represent which sequence each symbol comes from.,13,14
2129,218517099,"6 Before being processed by a transformer model, sequences are typically prefixed by a start symbol, such as ""[CLS]"" or ""<s>"".",16,17
2130,2021512,Clause boundary manifested by the punctuation symbol. (,6,7
2131,3155867,"Also, since the ids that correspond to a symbol are unique to each machine, we added a post-processing phase that replaces the ids with the ""canonical"" ones in the converter FSA.",9,10
2132,3155867,Table 2 shows that factoring the symbol conversion out from the individual machines resulted in huge savings: the running time of the Hungarian setup improved by 70% to 0.32 second; the Breton one by 40% to 1.55 seconds.,6,7
2133,3155867,"As the job of token matching has been delegated to the symbol automaton (see section 4.6), we no longer maintain separate tries for all individual FSAs.",11,12
2134,249605730,"In other words, we designate a special symbol (0), which appears only in a limited number of contexts in training.",8,9
2135,249605730,We then separately evaluate how the models perform on unseen examples with ordinary symbols (IND test set) and on unseen examples with the special symbol (OOD test set).,26,27
2136,249605730,"In two of the metrics, namely positional disentangle-ment and topographic similarity, we achieve higher scores than the original architecture, while in bagof-symbol disentanglement the situation is reversed.",28,29
2137,249605730,"Manual inspection of the agents' communication showed that most of the time agents successfully reconstruct the non-zero symbols, which means that most of the errors are caused by wrongly reconstructing the zero symbol.",36,37
2138,249605730,"These errors are also systematic, meaning that, given the position in the string, the zero symbol is replaced by the same symbol regardless of its neighbor.",18,19
2139,249605730,"These errors are also systematic, meaning that, given the position in the string, the zero symbol is replaced by the same symbol regardless of its neighbor.",24,25
2140,249605730,"One might object that seeing just one or two examples of a given symbol in the training data is too little for ANNs to learn its embedding and reliably map it close to other 'similar' symbols in the semantic space (Lake and Baroni, 2018; Loula et al.,",13,14
2141,249605730,"A Appendix A.1 Architecture Both the sender and the receiver are implemented by the following encoder-decoder architecture: The encoder produces two embeddings (size 500) for each input symbol, one syntactic and one semantic.",32,33
2142,249605730,The autoregressive decoder embeds the last produced symbol (or the start-of-sequence symbol) (size 500) and transforms it with another GRU layer (size 500).,7,8
2143,249605730,The autoregressive decoder embeds the last produced symbol (or the start-of-sequence symbol) (size 500) and transforms it with another GRU layer (size 500).,16,17
2144,249605730,This vector (size 500) is added to the query and transformed by a linear layer to the output symbol logits.,20,21
2145,41531604,"2006b) and (Varga and Simon, 2007) We preprocess our corpora by replacing the numerical strings with a common symbol.",22,23
2146,5566192,"Note that finite state transducer-based systems perform essentially the same type of optimizations, eliminating symbol redundancy when two symbols behave the same in every rule, and eliminating state redundancy when two states have the exact same continuations.",17,18
2147,227231090,"Technically the entailment task may be considered ill-defined for highly polysemous words such as letter, since the question of whether letter entails mail hinges on the choice between the definitions 'a written or printed communication' and 'a symbol in an alphabet', only one of which entails any sense of mail.",43,44
2148,227231090,"An example in the dev dataset is letter → mail which is labelled as entailment but simply isn't if we choose the definition ""A symbol in an alphabet."".",26,27
2149,17592422,"This does not hold as a generalization in the contemporary language, and consonant gradation is thus in transducers for Finnic and Saami languages encoded as triggered by a special symbol ˆWG inserted in the lexc continuation lexicon rather than by referring to the closed syllable, like here for k : v (the left and right vowel contexts are specific in order not to overlap with the k : j and k : 0 gradation contexts.",30,31
2150,17592422,"Back harmony"" Vx:Vy <=> BackVowel: NonFront:* _ ; where Vx in ( %^A %^O %^U ) Vy in ( a o u ) matched ; Gemination as insertion of consonant As seen in the previous section, we treat consonant gradation morphologically (weak grade is triggered by an explicit symbol ˆWG rather than by a phonological context), but vowel harmony we treat phonologically.",60,61
2151,17592422,"Consonant gemination for a (alternative approach)"" %^RC:Cx <=> Cx _ ... %^CNSLEN: ; In the lexicon file, this alternative approach would have been implemented as follows: The gemination place-holder would have been added to all stems containing a single root consonant, and all suffixes introducing a long second syllable would have been enriched with the trigger symbol as well: LEXICON Nouns sana:san^RCa n_21 ; ... LEXICON n_21 +Par:^CNSLEN%>^V K ; ! %",71,72
2152,203837062,"Unlike words or subwords that bear some meaning, a character in many languages simply represents an orthographical symbol, and the relationship between that symbol and its meaning are arbitrary from the linguistic point of view.",18,19
2153,203837062,"Unlike words or subwords that bear some meaning, a character in many languages simply represents an orthographical symbol, and the relationship between that symbol and its meaning are arbitrary from the linguistic point of view.",25,26
2154,246210255,Attribute Masker The Attribute Masker identifies the attribute words (words responsible for bias in a text) and masks these words with a special [MASK] symbol.,28,29
2155,203902359,A word which has not seen in the vocabulary of the input text (called unknown word) are presented by the unk symbol in NMT systems.,23,24
2156,203902359,"Similarly to BPE approach, we also employ a pair of the special symbol @ for separating affixes from the word.",13,14
2157,218977355,"However, since the grapheme-to-phoneme conversion tool we developed could convert all the text to phonemes, we considered this to be a better option, because it reduced the symbol set (some of the French phonemes also exist in Occitan) and so we could expect better results.",34,35
2158,17901227,"The data is transliterated into Latin based ASCII characters, mostly with single-symbol equivalents of the Arabic phonemes and by replacing diacritics and glyphs which represent short vowels in Arabic orthography with appropriate Roman letters.",14,15
2159,1557137,l symbol that can be at the root of a substitutable ( adjoina.,1,2
2160,1557137,"Bod formalizes DOP in terms of a stochastic tree-su bstitution grammar, which consists of a finite set of elementary trees, each with an associated probability such that the probabilities of all the trees with the same non-terminal symbol sum to 1, with an operation of substitution to combine the trees.",43,44
2161,218977351,"There are 15 categories, encoded in a positional tag, which is a string of 15 characters; every position encodes one morphological category using one character symbol.",28,29
2162,15562208,"l grammar consists of the fo llowing elements: • A set V of v.rords, called the vocabulary, which contains a distinguished symbol ROOT.",24,25
2163,15562208,Add lal left children and l/31 right children to the leaf x. Label the left children and their attached edges with the symbol pairs in a (from right to left ); similarly label the right children and their attached edges with the symbol pairs in /3 (from left to right).,22,23
2164,15562208,Add lal left children and l/31 right children to the leaf x. Label the left children and their attached edges with the symbol pairs in a (from right to left ); similarly label the right children and their attached edges with the symbol pairs in /3 (from left to right).,44,45
2165,15562208,"A dependency parse tree is a dependency tree generated from the grammar by starting with a single node, labeled with the special symbol ROOT E V, and repeatedly expanding leaves until every node has been expanded exactly once.",23,24
2166,15562208,"Thus the ith symbol of O is a confusion set of possibilities for the ith word of the input, e.g., {bank 1 , bank 2 , bank 3 }.",3,4
2167,2530161,"Third step: DISF tier processing As for the coding of prominences, the experts use the symbol '1' to indicate the disfluencies clearly identified and '?'",17,18
2168,250390915,"2, U (b) ∈ R (d+1)×d and b (b) ∈ R. The symbol ⊕ denotes vector concatenation and MLP in Eq.",18,19
2169,18774233,"Actually, the features we used in our first pass were Name, Continuation, and two new features especially suit able for a fast first pass, the length of the constituent and the terminal symbol following the constit uent .",36,37
2170,10918665,Their model overcomes the drawback of context insensitivity of PCFGs by estimating the probability of each LR parsing action according to its left (i.e. LR parse state) and right context (i.e. next input symbol) .,36,37
2171,10918665,"Further, for each coupling of a state s and input symbol l E La(s), the table specifies a set of possible parsing actions: Act(s, l) � A. Each action a E A is either a shift action or reduce action.",11,12
2172,10918665,"The third factor P(WIT) is the distribution of lexical derivations from T, where each terminal symbol of T is assumed to be a part of speech symbol.",17,18
2173,10918665,"The third factor P(WIT) is the distribution of lexical derivations from T, where each terminal symbol of T is assumed to be a part of speech symbol.",28,29
2174,10918665,Most statistical parsing frameworks estimate this distribution by assuming that the probability of the i-th word w i E H l depends only on its corresponding terminal symbol (i.e. part of speech) l i .,29,30
2175,10918665,"a.,, O"" Q :::::::: 0"" 1 :::::::: • • • � O"" n -1 ===} O"" n (4) where O""i is the i-th stack, whose stack-top stat e is denoted by top(O"" i ), and li E La(top(O""i_i)) and ai E Act(top(O"" i _i), li) are, respectively, an input symbol and a parsing action chosen at O""il • It can be proven from the LR parsing algorithm that, given an input symbol l i +l E La( top( O""i )) and an action a i +l E Act( top(O"" i ), l i +I ), the next (derived) stack next(O""i,a i +i) (= O""i+i) can always be uniquely det ermined as follows: • If the current action a i +I is a shift action for an input symbol li+l, then the parser consumes li+I, pu shing l i +I onto the stack, and then pushes the next state Si+I, which is uniquely specified by the LR table, onto the stack. •",90,91
2176,10918665,"a.,, O"" Q :::::::: 0"" 1 :::::::: • • • � O"" n -1 ===} O"" n (4) where O""i is the i-th stack, whose stack-top stat e is denoted by top(O"" i ), and li E La(top(O""i_i)) and ai E Act(top(O"" i _i), li) are, respectively, an input symbol and a parsing action chosen at O""il • It can be proven from the LR parsing algorithm that, given an input symbol l i +l E La( top( O""i )) and an action a i +l E Act( top(O"" i ), l i +I ), the next (derived) stack next(O""i,a i +i) (= O""i+i) can always be uniquely det ermined as follows: • If the current action a i +I is a shift action for an input symbol li+l, then the parser consumes li+I, pu shing l i +I onto the stack, and then pushes the next state Si+I, which is uniquely specified by the LR table, onto the stack. •",113,114
2177,10918665,"a.,, O"" Q :::::::: 0"" 1 :::::::: • • • � O"" n -1 ===} O"" n (4) where O""i is the i-th stack, whose stack-top stat e is denoted by top(O"" i ), and li E La(top(O""i_i)) and ai E Act(top(O"" i _i), li) are, respectively, an input symbol and a parsing action chosen at O""il • It can be proven from the LR parsing algorithm that, given an input symbol l i +l E La( top( O""i )) and an action a i +l E Act( top(O"" i ), l i +I ), the next (derived) stack next(O""i,a i +i) (= O""i+i) can always be uniquely det ermined as follows: • If the current action a i +I is a shift action for an input symbol li+l, then the parser consumes li+I, pu shing l i +I onto the stack, and then pushes the next state Si+I, which is uniquely specified by the LR table, onto the stack. •",188,189
2178,10918665,"We say stack transition sequence T is complete if l n = $ , a n = accept, and O"" n = final, where final is a dummy symbol denoting the stack when parsing is complet ed .",31,32
2179,10918665,VVe assume that only the current stack-top state Si-l = top((T i -i) has any effect on the probability of the next input symbol k This means that: Case 3.,28,29
2180,10918665,"Unlike Case 2, the input symbol does not get consumed for reduce actions, and thus the next input symbol l i is always identical to li-l ; namely, l i can be deterministically predicted.",6,7
2181,10918665,"Unlike Case 2, the input symbol does not get consumed for reduce actions, and thus the next input symbol l i is always identical to li-l ; namely, l i can be deterministically predicted.",20,21
2182,10918665,"i = 1: 125 (9) Next, we estimate the second term P(ai l(Ti-l, l i ) relying on the analogous assumption that only the current stack-top state Si-I and input symbol ii have any effect on the probability of the next action ai: (14) where L P(als, l) = 1 (15) aEAct(s,l) Finally, as mentioned above, given the current stack (T i-l and action a i , the next stack (T i can be uniquely determined: (16) As shown in equations ( 11 ) and ( 13 ), the probability P(l i l(T i _ i) should be estimated differently depending on whether the previous action ai-l is a shift action or a reduce action.",41,42
2183,10918665,"First, as in equation ( 14 ), the probabilistic distribution of each parsing action depends on both its left context (i.e. LR parse state) and right context (i.e. input symbol).",34,35
2184,10918665,"P(l i ls1_ 1 ) in equation ( 11 ) is a model that predicts the next terminal symbol ii for the current left context si -I E S 5 • In this case of s i -I E S s , since s i -I uniquely specifies the previous terminal symbol Li -I, P(li l s i -d = P(l i lsi-i,l i -d, which is a slightly more context-sensitive version of the bigram model of terminal symbols P(l i l l i _i).",19,20
2185,10918665,"P(l i ls1_ 1 ) in equation ( 11 ) is a model that predicts the next terminal symbol ii for the current left context si -I E S 5 • In this case of s i -I E S s , since s i -I uniquely specifies the previous terminal symbol Li -I, P(li l s i -d = P(l i lsi-i,l i -d, which is a slightly more context-sensitive version of the bigram model of terminal symbols P(l i l l i _i).",52,53
2186,10918665,"Without this treatment, probability P(li lsil ) in equation (1 1) could be incorrectly duplicated for a sin gle terminal symbol, which would make it diffi cult to give probabilistically well-founded semantics to th e overall score.",23,24
2187,10918665,"After shifting the left-most x, which leads the process to state 1, the model predicts the next input symbol as either u or v, and chooses the reduce action in each case, reaching state 4.",22,23
2188,10918665,"In state 4, however, B&C model repredicts the next input symbol, despite it already having been determined in state 1.",12,13
2189,10918665,"In our model, on the other hand, the probabilities in state 4, which is of class S r , are normalized for each input symbol, and thus the prediction of the input symbol u ( or v) is not duplicated.",27,28
2190,10918665,"In our model, on the other hand, the probabilities in state 4, which is of class S r , are normalized for each input symbol, and thus the prediction of the input symbol u ( or v) is not duplicated.",36,37
2191,10918665,"Grammar G2: (1) S ---+ u X (2) S ---+ v X (3) X ---+ x Let us compute again the probability of each tree for the two models: m m, ( m ) 2 PB&c(tree(a)) = --• 1 • --• 1 = -- m + n m+n m+n n n ( n ) 2 PB&c(tree(b)) = --• 1 • --• 1 = -- m + n m+n m+n m m P PGLR(tree(a)) = --• 1 • 1 • 1 = -- m + n m+n n n P PGLR (tree(b)) = --• 1 • 1 • 1 = -- m + n m+n (32) (33) (34) (35) In B&C model, the probability assigned to the reduce action in state 3 with the next input symbol being $ is subdivided according to whether the state reached aft er the stack-pop operation is state 1 or 2 (see Table 2 ).",152,153
2192,10918665,"Table 3 shows that both B&C mod�l and our model correctly prefer the shift action in state 5 with the next input symbol being x. For input sentence W 2 , on the other hand, the left br anching tree (d) is preferred.",24,25
2193,10918665,"According to Table 3 , st ate 6 with the next input symbol being x correctly prefers the reduce action, which derives the left-branching tree ( d).",12,13
2194,10918665,"Furthermore, although not explicitly demonstrated in the above example, it should also be noted that the GLR-based models are sensitive to the next input symbol as shown in ( 14) in section 2.",28,29
2195,250311076,"In this sense, we can be much more confident that the Sparse Factored Attention model has actually learned the symbol grounding and the meaning of the words as they relate to cell attributes in the environment.",20,21
2196,3892356,"In our toy example, data points with the same shape symbol form a positive pair, while points with different symbols are negative pairs.",11,12
2197,214623124,"If we were to use an autoregressive insertion model, the natural way to model it would be to run the decoder until it decides to stop by producing a special stop symbol.",32,33
2198,214623124,"Since by design we opted for using a non-autoregressive model, to represent variable-length insertions we use a PAD symbol to pad all insertions to a fixed-length 5 sequence of MASK tokens.",23,24
2199,1509192,3] report 64% (70% correct) accuracy using CI models with a 39-phone symbol set.,19,20
2200,15499511,"Online help information is provided for all commands, and a variety of command switches may be set, such as turning on and off the debugger and execution timer and setting the start symbol.",34,35
2201,6437918,"As an ex-~r~ole, the following structure is created :for the sente/%ce The man had seen many_~i[~LS_ from his window, where the symbol $e denotes an empty value, and "" "" denotes a place-holder for features: (6) [[c,_] , Se, [ Se, Se, [[i,_] , [[n,_] System Operation Following the strategy :in (i), the GH9 reads in a sentence (assumed to be grammatically correct), analyzes the ~orphology of each word, and applies the phrase structure rule (5) recursively to build the S-str~Icture.",27,28
2202,5787926,The * symbol stands for one of the two operators: addition (plus) and multiplication (mult).,2,3
2203,21702682,"We also subdivide exact synonyms into more specific categories: synonym (carbon dioxide  carbonic acid gas), symbol (carbon dioxide  CO2), abbreviation (greenhouse gas  GHG); variant (microorganism  micro-organism); feminine (Fr.",20,21
2204,12154117,"The question mark is exchanged for a special symbol (""qst"" QDL), cf.",8,9
2205,97528,The two-level rules are symbol to symbol rules which apply to the lexicon in parallel.,6,7
2206,97528,The two-level rules are symbol to symbol rules which apply to the lexicon in parallel.,8,9
2207,97528,> n || _ [ [t o m a] | [k a m a] | ....| [v u r a]] The second problem involving complex consonants was solved by representing each complex by a multicharacter symbol that is not used in the lexicon.,43,44
2208,97528,The finite state developer can solve morphological problems using the most appropriate approach depending on whether what is being replaced is a symbol or a string.,22,23
2209,9066011,"In steps 5 and 6 which follow, there are formal rules involving symbol processing which insert the relevant Arabic preposition between the nouns.",13,14
2210,9066011,"It is by means of the symbol processing that the system decides between inserting, suppressing, or keeping the article.",6,7
2211,60611657,The statement w ----+ * A means that the string w can be reduced to the symbol A. Observe that we do not distinguish a start symbol from which sentences are derived.,16,17
2212,60611657,The statement w ----+ * A means that the string w can be reduced to the symbol A. Observe that we do not distinguish a start symbol from which sentences are derived.,26,27
2213,60611657,"Finally, the rules (S) and (C) finalize the recognition of a predicted and recognized token or nonterminal-witnessed by the second premise-by shifting the • over the predicted symbol.",36,37
2214,60611657,The function goto computes the set of items that results from a state by shifting the dot in the items over a symbol X. The schema defines three deduction rules.,22,23
2215,60611657,"LLR = { [a • ,6 ➔ A] I a,6 ➔ A E P(Q) } I = { [ <P, i, j] I <P � LLR } The closure of a set of items <P is the smallest set of items containing <P and closed under prediction, i.e., <P � closure( <P) [a • B,6 ➔ A] , 1 ➔ B E P(Q) [ • 1 ➔ B] E closure( <P) Given a symbol X the goto function maps a set of items to the closure of the set obtained by shifting all items with X. goto(X, <P) = closure( {[a.X • ,6 ➔ A] I [a • X ,6 ➔ A] E <P}) Given these functions an LR parser is defined 1 by the following deduction rules. [ {[",97,98
2216,60611657,"goto(, ➔ B, <P) = closure( {[aB • ,6 ➔ A] I [a • B,6 ➔ A] E <P /\ [a[, ➔ B]/J ➔A ] � Q}) □ Note that the goto function has to be parameterized with the production that is recognized instead of with just the symbol. (",67,68
2217,60611657,"The SLR algorithm is based on the observation that a reduction is only useful if the next symbol in the string can follow the symbol that is recognized by the reduction, i.e., the right hand-side of the production that is reduced.",17,18
2218,60611657,"The SLR algorithm is based on the observation that a reduction is only useful if the next symbol in the string can follow the symbol that is recognized by the reduction, i.e., the right hand-side of the production that is reduced.",24,25
2219,60611657,"The function first(a, \JI) yields the set of symbols that can start a phrase derived from a string of symbols a followed by a symbol from the set \JI.",26,27
2220,60611657,"The expression follow(B, \JI) denotes the set of symbols that can follow symbol B in a ph rase that is followed by a symbol from the set \JI.",14,15
2221,60611657,"The expression follow(B, \JI) denotes the set of symbols that can follow symbol B in a ph rase that is followed by a symbol from the set \JI.",25,26
2222,60611657,The reduce rule now only applies if a production has been recognized and the next symbol in the string can follmv the right-hand side of the production.,15,16
2223,60611657,"The reduce rule of the schema in Definition 6.1 is restricted by requiring that the next symbol in the string is an element of the follow set of B. [a, i1,;, i1,; [,:r,.[a•B.6➔A] h ""] [,:r,.[",16,17
2224,60611657,The following parsing schema optimizes the SLR(l) parsing schema by defining the follow set for a production instead of for a symbol and adapting the reduce rule accordingly.,22,23
2225,6847055,"5 Every tree was given a new root symbol 'ROOT', attached by a unary branch to the root in the tree bank.",8,9
2226,61101472,"Prelhn inaries A context-free grammar G is a 4-tuple ( E, N, P, S) , where E and N are two finite disjoint sets of terminal and nonterminal symbols, respectively, S' E N is the start symbol, and P is a. finite set of rules.",47,48
2227,61101472,"The process is initiated at the start symbol, and from there the process descends the grammar in all ways until terminals are encountered, and then transitions are created labelled with those terminals.",7,8
2228,61101472,"If S E N i , then augment the grammar with new non terminal st and rule st ---+ S and choose 5t to be the new start symbol of the grammar.",29,30
2229,219308344,This symbol represents the decoration of all derivations starting from A and ending with x l . •,1,2
2230,219308344,"i • The index j is assumed fixed, there is i , J one such symbol for each pair ( A, i): hence <P A (A i , j ) = <P A (A '7 x ] i , j ] ) I: r=A ---twoA1 w 1 • • •w p -1 A p w p s.t.",16,17
2231,219308344,"Level 0 <S'---t•S$, 0, 0, () > <S---t•NPVP, 0, 0, ()> <N P---t•DetN, 0, 0, () > <Det---t•the , 0, 0, 0 > Level 1 <Det --dhe•, <N P----tDet 0 • 1 •N, <N----t•man, <N----t•apple, <N----t •men, <N----t•app les, Recognition symbol: cp(Det 0 • 1 ) Level 2 <N----tman•, <N P----tDet 0 • 1 N 1 • 2 •, <S----tNP 0 • 2 •VP, <VP----t•VNP, <V----teeats, <V----t•sings, <V----t•eat, <V----t•sing, Recognition symbols: 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, { (S, Det N[Z] VP[Z]) } > ()o(cp(Det 0 • 1 )) > .o> o> o> o> <$(Det 0 • 1 ----tthe) { (Det 0 • 1 , the) } { (S, the man V P[sg] ) } > (cp(Det 0 • 1 ))o(cp(N 1 • 2 )) > ()o(cp(N po,2 )) > o> o> o> o> o> cp(N 1 , 2 ) = <$(N 1 , 2 ----tman) { (N 1 • 2 [sg] , man) } cp(N po , 2 ) = <$ (N po , 2 ----t Det o , 1 N 1 , 2 ) x'H.<$(Det o , 1 ) x'H.<$(N 1 , 2 ) = { (N P 0 • 2 [X 1 ], Det 0 • 1 N 1 • 2 [X 1 ]) } x'H.{ (Det 0 • 1 , the) } X1-f.4> (N 1 • 2 ) = { (N P 0 • 2 [X 1 ], the N 1 • 2 [X 1 ]) } x'H.{ (N 1 • 2 [sg] , man) } = { (N P 0 • 2 [sg] , the man) } As a consequence, the 2 last items disappear while computing their prediction decoration: the unification of s g and pl fails.",77,78
2232,219308344,"Level 3 Recognition symbol: Level 4 Recognition symbol: <V----teats•, 2, 3, { (S, the man eatN P   One remarks that the last substitution is the identity.",3,4
2233,219308344,"Level 3 Recognition symbol: Level 4 Recognition symbol: <V----teats•, 2, 3, { (S, the man eatN P   One remarks that the last substitution is the identity.",8,9
2234,53082203,"We use this symbol to denote that we sum the gram-matrices for the distinct standalone kernels, and use the resulting kernel matrix as input to SVMs.",3,4
2235,219300157,"Every symbol of RFODG belongs to terminals or other symbols ( nonterminals) , to deletable or nondeletable symbols and to positive or negative symbols.",1,2
2236,219300157,"For the purpose of this paper it is possible to describe this notion informally as a smallest subtree of a DR-tree, which contains a particular mf-symbol and from which leads an mf-insensitive edge ( such an edge was created by the application of an rnf-insensitive rule) .",30,31
2237,52010945,"We also replace numbers in the dataset with a special symbol, <NUM> and out-of-vocabulary (OOV) words with <UNK>.",10,11
2238,6492147,Zero subjects are presented by the symbol Ø and in the English translations the subjects which are elided in Spanish are marked with parentheses.,6,7
2239,5888627,"The symbol "" ' "" is used to mark the accent of the Bulgarian words, and the symbol "" | "" is used to separate the variable part of the word from the main part.",1,2
2240,5888627,"The symbol "" ' "" is used to mark the accent of the Bulgarian words, and the symbol "" | "" is used to separate the variable part of the word from the main part.",18,19
2241,18799285,"Morphological analysis disambiguates the morphological features as far as possible based on syntax and context, and the information is further stored in the database as a positional tag in the form of a string of symbols where each morphological feature represented by a given symbol has a fixed slot (for positional tags, see also Hajič, 2004) .",45,46
2242,7327599,Finally a page number or the quotation marks symbol must be recognized.,8,9
2243,9978848,"This LF expresses the sense 'set of X', where X is an argument that is usually filled by nouns, such as grape, or flower, as shown in ( 1 ): (1) Mult(grape) = bunch of Mult(flower) = bouquet of, bunch of This LF can correspond to some lexical units that are not related syntagmatically (as examples above) but paradigmatically (in these cases, the value of the LF is preceded by the symbol //).",86,87
2244,235097205,"To aid this exploration, this paper introduces the concept of the hyperlexicon, the network consisting of all possible words for a given symbol set and their neighbourhood relations.",24,25
2245,235097205,5 Pairs of identical adjacent symbols act as a single symbol for the purposes of counting possible deletion sites.,10,11
2246,235097205,"For this word, a deletion at position 2 is equivalent to a deletion at position 3; the sequence bb can be essentially treated as a single symbol for the purposes of counting deletion sites.)",28,29
2247,16886141,we replaced each entity by its refering symbol.,7,8
2248,216079514,"γ(i, j) = d(T i , H j )+ min{γ(i − 1, j − 1), γ(i − 1, j), γ(i, j − 1)} (2) The decision to select or not a detected sentence is then taken according a detection threshold on the aligned symbol score (phonems) of each identified call.",57,58
2249,13230702,Words with a low frequency (<10) are substituted with a low frequency word symbol.,16,17
2250,2830333,"The output of the recognition process may be (1) poorly segmented or not segmented at all; (2) containing underspecified symbols (where the recognition process can only indicate that the symbol belongs to a specific group), e.g. shape codes; (3) containing incorrectly identified symbols.",35,36
2251,2830333,The framework uses customized morpho-syntactic and syntactic analysis where the lexicons and their alphabets correspond to the symbol set acquired from the recognition process.,19,20
2252,2830333,A post-processing framework must not simply perform a symbol-to-symbol conversion.,10,11
2253,2830333,A post-processing framework must not simply perform a symbol-to-symbol conversion.,14,15
2254,2830333,"This method uses a unified error model with flexible symbol mapping, facilitating the use of any linguistic module with traditional orthographic lexicons-for any recognition process (OCR, handwriting, speech recognition), even for highly inflectional languages.",9,10
2255,2830333,"It comprises of complex symbol codes in a normalized format, where the codes closely correspond to the signals recognized by the particular recognition process.",4,5
2256,2830333,"2 Unlike in two-level-morphology, the framework must provide n-to-m character (symbol) mapping, where n ≠ m. Mapping between speech and text chunks of different length makes the system able to offer, for example, consonant sequences instead of affricates usually represented by single characters: me{t s ,t s ˘} metsz, metssz ('engraves,slits,cuts' in affirmative and imperative) With continuous speech recognition (and, though less frequently, continuous handwriting recognition) it is even possible that a written segment boundary-such as the end of a word or a sentence-occurs within a symbol.",20,21
2257,2830333,"2 Unlike in two-level-morphology, the framework must provide n-to-m character (symbol) mapping, where n ≠ m. Mapping between speech and text chunks of different length makes the system able to offer, for example, consonant sequences instead of affricates usually represented by single characters: me{t s ,t s ˘} metsz, metssz ('engraves,slits,cuts' in affirmative and imperative) With continuous speech recognition (and, though less frequently, continuous handwriting recognition) it is even possible that a written segment boundary-such as the end of a word or a sentence-occurs within a symbol.",120,121
2258,2830333,"2 The basics: symbol mapping Atomic segments of input sequences are assumed to consist of (underspecified) symbols (phonemes/phoneme complexes, characters/character complexes).",4,5
2259,2830333,"ke˘t y n y u˘l két nyúl ('two rabbits') As the above method has some obvious disadvantages, we decided to separate the symbol mapping from the linguistic processes.",27,28
2260,2830333,Note that this input symbol is the result of a mapping from the output of the recognition module.),4,5
2261,2830333,"The input sequence is processed symbol by symbol, and when the segmenter encounters a potential segment boundary, registers it and checks if the phonetic processor saw any pause, stress or other sign of segmentation at that particular position in the original speech signal.",5,6
2262,2830333,"The input sequence is processed symbol by symbol, and when the segmenter encounters a potential segment boundary, registers it and checks if the phonetic processor saw any pause, stress or other sign of segmentation at that particular position in the original speech signal.",7,8
2263,2830333,"Our proposed solution is a post-processing phase performed on the output of the recognition source, where morpho-lexical and syntactic models validate (either accept or reject) different orthographical candidates derived from a single recognized symbol sequence.",40,41
2264,2830333,"Its main advantage is the ease of application of any linguistic module, thanks to the separate symbol mapping process and the open architecture.",17,18
2265,218956812,"The DLPC has, in general, more structured information such as synonyms (preceded by ≈), examples (shown in italics), cross-reference to lexical units that preferentially co-occur are represented by the symbol +, usage labelling, among other relevant features.",41,42
2266,17105124,"γ(i, j) = d(Ti, Hj)+ min{γ(i − 1, j − 1), γ(i − 1, j), γ(i, j − 1)} (1) The decision to select or not a detected sentence is then taken according a detection threshold on the aligned symbol score (phonems) of each identified call.",53,54
2267,1667185,"TaskActions These features represent the action type (function symbol) of each action a k in i t,j = A 1 : a 1 , A 2 : a 2 , ..., A n : a n , as a string.",9,10
2268,7203123,"Once NP boundaries are identified, subsequences corresponding to NPs can be substi-tuted with an NP symbol.",18,19
2269,7203123,"The more features constrained for a symbol, the more specific the pattern is.",6,7
2270,7203123,"Bearing in mind that all grammar items have a source and the target component -because the target 'tree' is being built simultaneously with the parse forest -, and there can be different constraints on each constituent symbol within an item, we provide an example for a rule or pattern below: *NX=approach+to:12 EN.NX[ct=COUNT] = N(lex=""approach"") + PPOBJ(lex=""to"") HU.NX = PPOBJ[case=GEN] + N[lex=""megközelítés""] ; example: This is a really nice approach to religion.",39,40
2271,7203123,"Within a rule or pattern, a symbol is specified by one compulsory syntactic or morpho-syntactic label and, optionally, one or more feature constraints.",7,8
2272,248780472,"System x, corresponding to the setup described in §3, does not predict punctuation, nor does it predict the symbol @ (which indicates Chinese loanwords), whereas system y predicts these two elements.",22,23
2273,248780472,These last two systems predict punctuation and the @ symbol for loanwords.,9,10
2274,473993,"The asterisk symbol can be used to substitute any LE-item, e.g., LE[SP(NP, * ,PP), DP( * , * ,V)].",2,3
2275,3169212,"A PCFG (N, W, R, S, θ) consists of a start symbol S, N and W disjoints sets of nonterminals and terminal symbols respectively.",17,18
2276,3169212,"Finally, prosody was introduced here by way of a discrete symbol, forcing us to make a binary decision.",11,12
2277,235097362,"We use D i , k i , T i , T s to symbol the encoded counterparts, where T s is the encoded representation for all topic entities in the pre-defined set.",14,15
2278,1126190,"When an NDA is mapped onto this network, pairs of NDA-state q and input-symbol x, such that 6(q, x) y£ {~, are mapped onto activity patterns.",18,19
2279,1126190,"An NDA state is rap resented by all activity patterns that represent a pair containing that state, and input patterns are represented by a component-wise OR over all activity patterns containing that input symbol.",36,37
2280,15010667,"For instance, the character ""猿"" (ape) often relates to sadness, anxiety and distress, while the character ""荷"" (lotus) is the symbol of beauty, love and rectitude.",31,32
2281,219303680,"In the case of generative approaches, this amounts to finding a derivation from a distinguished symbol to this input.",16,17
2282,2439516,We here consider the end of the paragraph (denoted by the symbol 0 and the class identilier EOP) as an lulchoring point for coherence computation.,12,13
2283,11213361,"Label' is an atomic symbol which labels the arc, and 'destination' is a pointer to the node which is the destination of the arc.",5,6
2284,8683986,The reader should note that each proposition in a topic environment has at least one symbol identical to the name of the topic.,15,16
2285,235097245,Sentence endings are marked with an <EOS> symbol added to the last token.,9,10
2286,227392,"In derivations, the rewriting of symbol strings involves unification (Robinson 1965) , instead of simple replacement.",6,7
2287,227392,"When such a rule is used in rewriting, the gaps appearing in the lefthand side may match arbitrary strings of grammar symbols, and then the left-hand side is replaced by the right-hand side followed by the symbol strings matched by the gaps (in the same order).",42,43
2288,227392,"For example, the XG rule a,b...c...d --> e,f is really a rule schema a,b,X,c,Y,d--> e,f,X,Y where X and Y stand for arbitrary grammar symbol strings.",50,51
2289,227392,"Elements (like 'trace' above) on the left-hand sides of XG rules following the initial symbol are in effect put on the extraposition list during parsing, and can be taken off when they are required later by the right-hand side of another rule.",20,21
2290,227392,"Then the modifier structure of the phrase is a term of the form syn (NT,Sem,Mods) where NT is the leading symbol (a non-terminal) in A and where Mods is the list of modifier structures of the subphrascs analyzed with the right-hand side B of the rule.",26,27
2291,227392,"For parsing a sentence with respect to the grammar in Figure 1 , one would use sent([MST],[ ]) as start symbol (with MST unknown) and the parse would bind MST to the modifier structure tree of the sentence.",24,25
2292,227392,"A node in such a list is of the form x(Context,Type,Symbol,Ext) where Context is either 'gap' or 'nogap', Type is either 'terminal' or 'nonterminal', Symbol is a grammar symbol, and Ext is the remainder of the list.",45,46
2293,227392,"The ""left-hand side remainder"" in a grammar rule (the part after the leading symbol) is converted to an extraposition list in a straightforward way, with one node for each symbol in the remainder.",18,19
2294,227392,"The ""left-hand side remainder"" in a grammar rule (the part after the leading symbol) is converted to an extraposition list in a straightforward way, with one node for each symbol in the remainder.",36,37
2295,227392,"The Context says whether the symbol has a gap preceding it, and the remaining fields of an 'x' node have the obvious meaning.",5,6
2296,17533290,For null values the hash symbol (#) is specified.,5,6
2297,9013266,"It can take any of the forms: r(tl ..... tn) where r is an n-ary predicate symbol corresponding to a distributive, collective or respective relation.",20,21
2298,9013266,"If a terminal symbol ""a"" labels an arc leading to point ""x"", the starting point is named ""a.x"" (where .... is a binary operator in infix notation).",3,4
2299,9013266,"A rule such as S --> General Rewriting Rules When the left-hand side of a rule contains more than one symbol, a single arrow is not enough to depict it: we need another path between the extreme points.",23,24
2300,9013266," where A is a non-terminal symbol, x is a string of terminals and y is any sequence of grammar symbols.",8,9
2301,9013266,"Its application leaves a symbol ""Inv"" as a marker.",4,5
2302,1280719,Thereby the context provided by the neighbors of a symbol and the previous tags are used as evidence to decide its tag.,9,10
2303,52013386,The diamond shapes that contain a plus symbol indicate concurrent streams of action; the diamond shapes containing a cross represent decision points.,7,8
2304,28959070,"Compositional representations (third column in the Appendix table) encode each phonetic symbol with a basic sign which can be further specified through diacritics (conveying information e.g., about stress or nasality of vowels).",13,14
2305,18603314,"The first one has been proposed by Raison and Pope (1971) and uses a string composed by one or two characters (Lm, L or Lc depending on the symbol, respectively metric, phonetic or compound) followed by a number, for example: L2.",32,33
2306,18603314,"The second one, used in the GORILA collection (Godart and Olivier, 1976) and on John G. Younger's website, consists of a string composed by one or two characters (AB if the symbol is shared by Linear A and Linear B, A if the symbol is only used in Linear A) followed by a number and eventually other alphabetical characters (due to addenda and corrigenda to earlier versions), for example: AB03.",38,39
2307,18603314,"The second one, used in the GORILA collection (Godart and Olivier, 1976) and on John G. Younger's website, consists of a string composed by one or two characters (AB if the symbol is shared by Linear A and Linear B, A if the symbol is only used in Linear A) followed by a number and eventually other alphabetical characters (due to addenda and corrigenda to earlier versions), for example: AB03.",51,52
2308,18603314,We provide an example of different transcriptions for the same symbol in Table 4 .,10,11
2309,196184038,"This procedure is repeated until either the maximum output sequence length is reached, or a special < ST OP > symbol is produced.",21,22
2310,12111719,Rules must have at least one terminal symbol.,7,8
2311,16644191,"This means that all atomic elements in a phrase pattern have three feature structures; two for the concatenation of two adjacent symbols, and one that describes the global ('phrase-wide') behavior of the symbol in question.",40,41
2312,16644191,Thus a single higher-level symbol can be generated from the phrase pattern that inherits features from the lower levels.,6,7
2313,16763629,A baseline experiment was conducted where the ticker symbol of the affiliation of the quote maker was retrieved from Yahoo!,8,9
2314,10572593,"This entry has two different senses (marked by the sense number, as usual), and three of these senses end with a list of synonyms, after the congruency symbol ( ∼ =), in small capitals.",32,33
2315,232021829,"Par exemple la question : ""What is the democratic party symbol?""",11,12
2316,3253153,"The symbol '∅' represents the untuned versions of our metrics, i.e., applying a uniform linear combination of the individual metrics.",1,2
2317,3253153,The symbol '+' stands for concatenation of datasets.,1,2
2318,18075251,"In addition, at each segment boundary (including the document end) we insert an artificial segment end symbol.",19,20
2319,323987,A generic non-terminal symbol serves as a placeholder that marks the gaps.,5,6
2320,13485889,"Soft Syntactic Labels A possibility to enhance the hierarchical model is to extend the set of non-terminals from the original generic symbol to a richer, syntax-oriented set.",23,24
2321,13485889,"During decoding, we compute two additional quantities for each derivation d. The first one is denoted by p h (Y |d) (h for ""head"") and reflects the probability that the derivation d under consideration of the additional non-terminal symbols has Y as its starting symbol.",53,54
2322,13485889,"An initial rule S → X ∼0 , X ∼0 (6) is engrafted, as well as a special glue rule that the system can use for serial concatenation of phrases as in monotonic phrase-based translation (Chiang, 2005) : S → S ∼0 X ∼1 , S ∼0 X ∼1 (7) S denotes the start symbol of the grammar, the X symbol is a generic non-terminal which is used on all left-hand sides of the rules that are extracted from the training corpus and as a placeholder for the gaps within the right-hand side of hierarchical rules.",64,65
2323,13485889,"An initial rule S → X ∼0 , X ∼0 (6) is engrafted, as well as a special glue rule that the system can use for serial concatenation of phrases as in monotonic phrase-based translation (Chiang, 2005) : S → S ∼0 X ∼1 , S ∼0 X ∼1 (7) S denotes the start symbol of the grammar, the X symbol is a generic non-terminal which is used on all left-hand sides of the rules that are extracted from the training corpus and as a placeholder for the gaps within the right-hand side of hierarchical rules.",71,72
2324,5272196,"Lexicon entries contain the equivalent of a single kind of form construction rule: <fi)rm> : <stem>/{ supersede I augment} The interaction of lexical information with the word's p~tradigm is as fi)llows: • If <form> correspends to a lexical stem nile in the paradigm (i.e., one whose right-hand-side is the special symbol LEX), then this form provides the stem fi)r that rule. •",68,69
2325,7671968,"LP2 is repeated below; the symbol '<<' is the obliqueness ordering carried over into phonology (Pollard & Sag, 1987, 174) .",6,7
2326,8223327,"Note that, by the encoding algorithm described in the previous subsection, the precedence vector of a symbol A that must precede a symbol B always be included in B's precedence vector.",18,19
2327,8223327,"Note that, by the encoding algorithm described in the previous subsection, the precedence vector of a symbol A that must precede a symbol B always be included in B's precedence vector.",24,25
2328,8223327,We can then use the algorithm to check the legality of next input symbol with respect to all preceded symbols easily by checking with the resultant string's precedence vector only.,13,14
2329,712385,The first description is treated by extending type symbol lattices to include complement type symbols.,8,9
2330,712385,A feature structure is either atomic or complex: an atomic FS is denoted by an atomic symbol; a complex FS consists of a set of feature-value pairs each of which describes an aspect of an object.,17,18
2331,712385,A TFS consists of a type symbol from a lattice and a set of rearm:e-value pairs.,6,7
2332,712385,"In this paper, both these type structures and the tyl)e symbol lattice on which term strnctures are delined are e×tcuded to treat negative descril)tions.",11,12
2333,712385,"Nega tions of type symbols are treated by extending type symbol lattices, aud negations of feature cxistmmes attd feature-address disagreements are treated by extending term structures.",10,11
2334,712385,"x~ H) In the above example, the tag symbol, X1 shows that features in and out must take the same value.",10,11
2335,712385,"We may ob taiu unexpected results Sllch ll.~ il ~l'FS with an tlnex peeled type symbol, a TFS with an unexpected lea tare value agreement and so on. [",15,16
2336,712385,"Thus, descriptions of TFSs are chLssitied into descriptions of TFSs having: (1) a certain type symbol (or having a subtype syn,hol of a certain type symbol), (2) a feature, and (3) two feature-address vahtes that agree.",19,20
2337,712385,"Thus, descriptions of TFSs are chLssitied into descriptions of TFSs having: (1) a certain type symbol (or having a subtype syn,hol of a certain type symbol), (2) a feature, and (3) two feature-address vahtes that agree.",32,33
2338,712385,"Negative counterparts of these descriptions are ebLssified into deseriptions of TFSs: (1') not having a certain tyl)c symbol (or having a type symbol which is not subsunmd by a certain type symhol), (2') not having a certain feature, and (3') having two thature-addrcss values that do not agree.",21,22
2339,712385,"Negative counterparts of these descriptions are ebLssified into deseriptions of TFSs: (1') not having a certain tyl)c symbol (or having a type symbol which is not subsunmd by a certain type symhol), (2') not having a certain feature, and (3') having two thature-addrcss values that do not agree.",27,28
2340,712385,"Then, type symbol htttiees are extended to inch,de complement type symbols as suggested in [1] .",3,4
2341,712385,"That is, a type symbol in 7-denotes a set of objects in an interpretation.",5,6
2342,712385,"The partial order <~-denotes the subsumption relation between these sets; for any type symbols a, b, and c, A feature symbol denotes a function from a subset of U to U. A feature path is a finite string of feature symbols and denotes the function obtained by tile composition of the functions that tile feature symbols denote.",25,26
2343,712385,A term domain is represented by a rooted directed graph within which each arc has a feature symbol as its label.,17,18
2344,712385,"Ait-Kaci's term structure, the basis of the C-type, is defined by assigning a type symbol and a tag symbol to each feature address as follows.",21,22
2345,712385,"Ait-Kaci's term structure, the basis of the C-type, is defined by assigning a type symbol and a tag symbol to each feature address as follows.",25,26
2346,712385,"Definition 4 A term is a triple (A, r, v) where A is a term domain on .T, r is a type symbol function fi'om 2-* to T such that r(f* -A) = {T}, and v is a tag symbol 5ruction front A to Y. Given a tag symbol fimction v, Addr.",27,28
2347,712385,"Definition 4 A term is a triple (A, r, v) where A is a term domain on .T, r is a type symbol function fi'om 2-* to T such that r(f* -A) = {T}, and v is a tag symbol 5ruction front A to Y. Given a tag symbol fimction v, Addr.",50,51
2348,712385,"Definition 4 A term is a triple (A, r, v) where A is a term domain on .T, r is a type symbol function fi'om 2-* to T such that r(f* -A) = {T}, and v is a tag symbol 5ruction front A to Y. Given a tag symbol fimction v, Addr.",59,60
2349,712385,"Definition 5 An augmented term is a quintuple (A,r,o,¢,X) where A is a term domain on 5 v, r is a type symbol timer(on from ~'* to T such that r(2-* -A) = {T}, v is a tag symbol function front A to V, ¢ is an inhibited feature filnction front 5 r* to 2 ~ such that ¢(p) is finite for any p (5 A and ~(~'* -A) = {0}, and X is a disagreement tag symbol function from J'* to 2 v such that X(P) is finite for any p (5 A and X(f'* -A) _-{0}, 2 The inhibited feature fimction ¢ specifies which features cannot exist at a given address.",29,30
2350,712385,"Definition 5 An augmented term is a quintuple (A,r,o,¢,X) where A is a term domain on 5 v, r is a type symbol timer(on from ~'* to T such that r(2-* -A) = {T}, v is a tag symbol function front A to V, ¢ is an inhibited feature filnction front 5 r* to 2 ~ such that ¢(p) is finite for any p (5 A and ~(~'* -A) = {0}, and X is a disagreement tag symbol function from J'* to 2 v such that X(P) is finite for any p (5 A and X(f'* -A) _-{0}, 2 The inhibited feature fimction ¢ specifies which features cannot exist at a given address.",52,53
2351,712385,"Definition 5 An augmented term is a quintuple (A,r,o,¢,X) where A is a term domain on 5 v, r is a type symbol timer(on from ~'* to T such that r(2-* -A) = {T}, v is a tag symbol function front A to V, ¢ is an inhibited feature filnction front 5 r* to 2 ~ such that ¢(p) is finite for any p (5 A and ~(~'* -A) = {0}, and X is a disagreement tag symbol function from J'* to 2 v such that X(P) is finite for any p (5 A and X(f'* -A) _-{0}, 2 The inhibited feature fimction ¢ specifies which features cannot exist at a given address.",101,102
2352,712385,"There is thus inconsistency if there is an address p in A such that ¢(p)n{fe2-lp.f(sA} # O. ( 6 ) The disagreement tag symbol fimction X specifies, for a given address, substructures with which its argument disagrees.",26,27
2353,712385,"9(1) For a term t = (A, r, v, ¢, X), a type symbol a (similarly, a tag symbol or a term t') is said to occnr in t if there is a feature address p in ,X such that r(p) = a (similarly, v(p) = X or X (5 X(P), or lip = t').",21,22
2354,712385,"9(1) For a term t = (A, r, v, ¢, X), a type symbol a (similarly, a tag symbol or a term t') is said to occnr in t if there is a feature address p in ,X such that r(p) = a (similarly, v(p) = X or X (5 X(P), or lip = t').",28,29
2355,712385,"A matrix represents a set of feature-value pairs preceded by a tag symbol, followed by a set of iuhibited features and followed by a set of disagreement tag symbols.",14,15
2356,712385,"The set WY5 r of well-formed terms includes many terms that llave tile same type syml)ol function, tile same coreferenee relations, the same inhibited feature function, and the same disagreelnent address fllllClion but different tag symbol fiUlCtions.",40,41
2357,712385,"Type Symbol Lattice Extension to Include Complement Type Symbols q_¥eating a negative desGil)tion of a given type sylnbol, say a, requires a type symbol I) such that b has only information that unification of it with a yiekls inconsistency, or such that aVT h = -V and aAT b = ±. Such a symbol is called a complement type symbol of a and written as a ~. If a given type symbol lattice (7-; _<7"") is a Boolean lattice, that is, a comI)lcmented 3 distributive lattice, we do not need to do anything.",25,26
2358,712385,"Type Symbol Lattice Extension to Include Complement Type Symbols q_¥eating a negative desGil)tion of a given type sylnbol, say a, requires a type symbol I) such that b has only information that unification of it with a yiekls inconsistency, or such that aVT h = -V and aAT b = ±. Such a symbol is called a complement type symbol of a and written as a ~. If a given type symbol lattice (7-; _<7"") is a Boolean lattice, that is, a comI)lcmented 3 distributive lattice, we do not need to do anything.",57,58
2359,712385,"Type Symbol Lattice Extension to Include Complement Type Symbols q_¥eating a negative desGil)tion of a given type sylnbol, say a, requires a type symbol I) such that b has only information that unification of it with a yiekls inconsistency, or such that aVT h = -V and aAT b = ±. Such a symbol is called a complement type symbol of a and written as a ~. If a given type symbol lattice (7-; _<7"") is a Boolean lattice, that is, a comI)lcmented 3 distributive lattice, we do not need to do anything.",63,64
2360,712385,"Type Symbol Lattice Extension to Include Complement Type Symbols q_¥eating a negative desGil)tion of a given type sylnbol, say a, requires a type symbol I) such that b has only information that unification of it with a yiekls inconsistency, or such that aVT h = -V and aAT b = ±. Such a symbol is called a complement type symbol of a and written as a ~. If a given type symbol lattice (7-; _<7"") is a Boolean lattice, that is, a comI)lcmented 3 distributive lattice, we do not need to do anything.",75,76
2361,712385,"For a finite type symbol lattice T, for example, a Boolean lattice T ~ can he constructed a.s follows.",4,5
2362,712385,"A node structure consists of live fields: lsymbol for a type symbol, arcs for a set of feature-vafile pairs, ifeatures for a set of inhibited features, dnodes for a set of disagreement nodes i.e., disagreement K-classes, and forward.",12,13
2363,712385,A disagreement tag symbol fnnetion is implemented using dnodes.,3,4
2364,712385,is also shown that the first kind call be treated by exteuditlg type symbol lattices t() include complement type synd)ols.,13,14
2365,2747187,SThe usual symbol for the initial nonterminal is s; we prefer to use al for reasons of notational coherence.,2,3
2366,480741,"In this notation, the training set is placed on the left hand side of symbol ""-"", while the test set is on the right hand side.",15,16
2367,480741,"For instance, A-B means that the training set is corpus A and the test set is corpus B. The symbol ""+"" stands for set union.",22,23
2368,15906130,As the ultimate generalization we can allow any tree in T. The top is a nonterminal and the leaves (:an be any symbol in V; a tree may or may not be part of a parse for w. Initially we start with elementary trees that correspond to the productions in our grammar.,23,24
2369,15906130,Marking a leaf is denoted by underlining the terminal symbol.,9,10
2370,15906130,"From the definition of g' it follows that a,~r • E iff complete(T) and the leftmost unmarked symbol of yield(a) is root(T).",19,20
2371,13126576,"1See Goeser (1990) for a more formal discussion of (i) * initialization is the special predictor case D(0, S) where 6' is a start symbol.",31,32
2372,13126576,Clause 4 expects a start synd)ol on the basis of left adjunction to a SET indexed symbol.,16,17
2373,13126576,"¢en * ( S, ,o ""'~ A,~g ) = 1 iff D ( i, A,, ) for a S E Sseti,,,l 4.~ The Completer The completer component integrates the predictor relation and the substring generation function and has two rules for rightside and ~see Appendix A for a complete formal characteri-t~ation of the RPSG chart parser leftside mljunction under a set-indexed symbol.",74,75
2374,13126576,"Given that the conditions in the if-clause (and the lookahead condition, see below) yield, tlte completer adds new items to the chart 9 Clansc I of the RPSG completer, is, up to the generation function instead of derivation, equivalent to the completer of the basis vari-t~nt: Given a rightslde passive item, it adds a new item both for a matching active item and for the prediction of an appropriate rules's LtlS symbol.",87,88
2375,13126576,Clause 2 does riglitsld-ndjnnclion of a start symbol item to a passive SET indexed item. ],9,10
2376,875918,"In this notation, the training set is placed at the left hand side of symbol ""-"", while the test set is at the right hand side.",15,16
2377,875918,"For instance, A-B means that the training set is corpus A and the test set is corpus B. The symbol ""+"" stands for set union, therefore A+B-B means that the training set is A union B and the test set is B. When comparing the performance of two algorithms, two different statistical tests of significance have been apphed depending on the case.",22,23
2378,6658384,0 serves here as the epsilon symbol.,6,7
2379,2105675,"The unification algorithm is based oil nondestructive graph unification [Wroblewski, 87] , which we extend to treat negation, loop, type symbol subsumption relationships, and disjunctiou.",25,26
2380,2105675,"when it finds an inactive edge whose starting and ending vertex are the left-most and right-most vertex of the chart, respectively, and whose label is the start symbol of the granunar.",33,34
2381,11285074,"2014) use a combination of lexicon-based sentiment analysis and LDA topics extracted from Twitter posts containing a stock's ticker symbol, on which the stock's price is regressed using a VAR model.",23,24
2382,1342209,"In this example, the symbol ""=="" will be used for defining the type hierarchy.",5,6
2383,310507,"Following the standard description in the compilers literature (see, e.g., [ASU86] ), Johnson-Laird adopts the definition of a top-down parser as one that operates by recursive descent: it begins with the start symbol of the grammar and successively rewrites tile leftmost nonterminal until it reaches a terminal symbol or symbols that can be matched against the input.",43,44
2384,310507,"Following the standard description in the compilers literature (see, e.g., [ASU86] ), Johnson-Laird adopts the definition of a top-down parser as one that operates by recursive descent: it begins with the start symbol of the grammar and successively rewrites tile leftmost nonterminal until it reaches a terminal symbol or symbols that can be matched against the input.",58,59
2385,310507,"The first symbol in such a pair represents tile top-down prediction of a node, and tile second a node that has been encountered bottom-up.",2,3
2386,310507,"Finally, rule 9 pops a symbol off the top of the stack if we have predicted a constituent X top-down and then succeeded in finding it bottom-up.",6,7
2387,310507,"These are, in fact, precisely the two VPs in the symbol [VP VP].",12,13
2388,310507,"Since the operation of the automaton prevents the symbol from being popped until the bottoIn-up view has been completed, it is clear that this automaton implements an arc-standard strategy rather than an arc-eager one.",8,9
2389,310507,"As discussed toward the end of the previous section, ""attachment"" of a node X to its pareut occurs when the symbol IX X], representing the top-down and bottom-up views of that node, is removed from the stack.",23,24
2390,310507,"In order to attach the node (i.e., enumerate the arc) eagerly, we should pop the symbol as soon as it is introduced.",19,20
2391,1996821,"Inputs to the Algorithm In order to construct a reference to a particular em tity, tile host system must provide: -a symbol corresponding to the intended referent; and • a list of symbols correspondiug to the members of the contrast set (i.e., the other entities in focus, besides the intended referent).",23,24
2392,227231145,"If there is no chunk type in L that matches the current S, the first symbol s becomes the selected chunk token.",16,17
2393,227231145,"There are two types of new chunks in the results: unknown symbols s / ∈ L and concatenations of known chunks (c i , c i+1 ) (with c i ∈ L and c i+1 ∈ L) that occur consecutively in S. L starts empty, learns the symbol chunks, then the smallest chunks construct larger chunks and the larger chunks construct even larger chunks.",52,53
2394,227231145,"Here, #s denotes the number of unique symbols s in L (either as a single-symbol chunk or as part of a larger chunk); Freq(s i ) and P (s i ) are the occurrence count and ratio of s i in L; #u denotes the number of unique units u in the corpus; Freq(u j ) and P (u j ) are the occurrence count and ratio of u j in the corpus.",19,20
2395,1609192,"We also make explicit the type, symbol, value and date of conditions when these are applicable (e.g., predicates, relations, named entities, time expressions, cardinal expressions, or anaphoric conditions).",7,8
2396,14653442,"It is common practice to include two additional rules to the set of hierarchical rules S → S ∼0 X ∼1 , S ∼0 X ∼1 (5) S → X ∼0 , X ∼0 (6) where S is the initial symbol in the grammar.",44,45
2397,14653442,Rule (6) allows the substitution of the initial symbol in the grammar with the generic non-terminal that allows the translation process to be carried out.,10,11
2398,13168996,In Figure 1 volving the root symbol of a grammar.,6,7
2399,227230647,"They are able to extract low-level features in order to recognize concepts (e.g. cat), but such representations are shallow and fall short from symbol grounding (meaning).",28,29
2400,248182532,"We included a ""Start"" and ""End"" symbol in our dependency paths for the baselines in order for all paths to be one trigram at a minimum.",10,11
2401,8937198,"The resulting frames are represented as a list of syntactic slots concatenated with the symbol ""#"".",14,15
2402,8130930,"We found 35,971 genes with associated 'gene symbols' (e.g. Tl is the gene symbol for Toll) and 48,434 synonyms; therefore, each gene has an average of 2.3 alternate naming forms, including the gene symbol.",16,17
2403,8130930,"We found 35,971 genes with associated 'gene symbols' (e.g. Tl is the gene symbol for Toll) and 48,434 synonyms; therefore, each gene has an average of 2.3 alternate naming forms, including the gene symbol.",40,41
2404,8130930,"For example, the gene symbol of the gene takeout is ""to"", and the symbol for the gene wee is ""we"".",5,6
2405,8130930,"For example, the gene symbol of the gene takeout is ""to"", and the symbol for the gene wee is ""we"".",17,18
2406,8753872,"3 For the MALINE row in Table 1 , the ALINE documentation explains the notation as follows: ""every phonetic symbol is represented by a single lowercase letter followed by zero or more uppercase letters.",21,22
2407,8753872,The initial lowercase letter is the base letter most similar to the sound represented by the phonetic symbol.,17,18
2408,9781962,The union symbol is used as we concatenate time intervals.,2,3
2409,7808406,"In the following definition Ca, denotes the set of productions for Ai and l~(Ai) denotes the number of right-hand side occurrences of nonterminal A~. Definition 2.1 A weakly restricted stochastic grammat"" Gzv is a pair"" (Cc, A), where Cc = (VN, ~,},, P, X) is a conlex#free flrammar and A is a set of functions A = {p~lA~ C VN} where, if j E 1 ...t~(Ai) and k E 1...[CA,I, pi(j,k) = Pij~"" G [0, 1] The set of productions P contains ezacily one produclion for start symbol S. In words, Plj~ stands for the probability that the k-th production with left~hand side Ai is used for rewriting the j-th right-hand side-occurrence of nonterminal Ai.",124,125
2410,7808406,"By definition it is required t, hat the grammar has one production for start, symbol Z: Z -+ £'.",16,17
2411,7808406,",,)~i, j) estimates the likelihood of de-riving otl~iAp(q,,)j~t)lw I [roln the start symbol S. The msideq~robability for st, ring w and nontermin~d occurrellce Ap(q.,,)",24,25
2412,60668848,"The head of a pro duction p is the r(p)-t h symbol of the right-hand side; an £-production has head c. In a practical notation, we give a head gram mar as a set of productions with the heads un derlined.",11,12
2413,6552619,"An input symbol is a <Word, Tag> pair.",2,3
2414,6552619,"An output symbol is an integer ranging from 0 to 3, specifying to which of two output segments an input symbol is assigned (0 = neither segment, 3 = both segments, 1 and 2 to be interpreted in the obvious way).",2,3
2415,6552619,"An output symbol is an integer ranging from 0 to 3, specifying to which of two output segments an input symbol is assigned (0 = neither segment, 3 = both segments, 1 and 2 to be interpreted in the obvious way).",21,22
2416,198342019,It is regarded by many as the best example of Mughal architecture and a symbol of India's rich history.,14,15
2417,198342019,The Taj Mahal is regarded by many as the best example of Mughal architecture and a symbol of India's rich history.,16,17
2418,198342019,Article: Taj Mahal Source Sentence: It is regarded by many as the best example of Mughal architecture and a symbol of India's rich history.,21,22
2419,198342019,Article: Hydrogen Source Sentence: Hydrogen is a chemical element with symbol H and atomic number 1.,12,13
2420,198342019,H&S System's Yes/no Question: Is hydrogen a chemical element with symbol H and atomic number 1?,14,15
2421,198342019,Retrieved Sentence: Fermium (symbol Fm) is a chemical element.,5,6
2422,8792578,"In LexIt, arguments and adjuncts are treated alike, so that a SCF represents an unordered pattern of syntactic dependencies headed by the target predicate, and it is labeled by concatenating its atomic slots names with the symbol ""#"".",39,40
2423,53653639,"e) Formulas are formed of variables, constants (each of specific type), and the symbol λ.",18,19
2424,53653639,The symbol ⇐⇒ is used since the syntactic variability properties are true only when both elements co-occur at the same time.,1,2
2425,15395374,"9 L yield is defined as follows: • L yield (τ ) = σ; • for n > 0, L yield (X n ) = λx 1 • • • x n .x 1 + • • • + x n ; • for n = 0, X 0 : τ represents a terminal symbol and L yield (X 0 ) = X. Then, the derivation tree, the derived tree, and the yield of Fig.",60,61
2426,891343,The symbol '-' before tile name of a rule marks it as being an 'ignore' rule.,1,2
2427,17768293,"To swell the gourd, and plump the hazel shells Here, a natural analysis is as follows: We use the symbol ' to denote marked (ictic) syllables, andto denote unmarked ones (non-ictic).",22,23
2428,159564,"2003) , these PSCFG approaches augment each contiguous (in source and target words) phrase pair with a left-hand-side symbol (like the VP in the example above), and perform a generalization procedure to form rules that include nonterminal symbols.",25,26
2429,159564,"In the spirit of isolating the additional benefit of syntactic categories, the SAMT system used here also generates a purely hierarchical (single generic nonterminal symbol) variant for each syntax-augmented rule.",26,27
2430,4413183,"In the case of the closed categories, our goal is to mark numerical entities along with the property they refer to and the unit or symbol which is used for it.",26,27
2431,4413183,"For example, in the sentence Hegazkinak 2000 km/h-ko abiaduran mugi daitezke ""The airplanes can fly at 2000 km/h"", 2000 is labeled with a couple of tags: the symbol of measurement is ""km/h"" and the associated property is ""speed"".",38,39
2432,4413183,"It must be underlined that in general other systems do not classify these open categories, and in the case of percents they only tag the number followed by the percent symbol, but not the common noun that the number refers to.",31,32
2433,4413183,The SUBTYPE is credited when a unit or symbol that expresses a property is marked correctly.,8,9
2434,4413183,"So when we detect a numerical entity associated with a property, TYPE is credited if the property is assigned correctly, SUBTYPE is credited if the unit or symbol is marked correctly and TEXT is credited if the boundaries of the numerical entity are identified properly.",29,30
2435,4413183,It is remarkable that other systems only classify simple percent structures like 20% that is a number followed by a percent symbol (%).,22,23
2436,13873204,"This information thus includes the attribute 'Verb Type', with a value v-ditr being the expression explicitly entered behind the symbol ':=', and a value for 'Semantic and Functional Properties', which is not explicitly entered in (3), but induced from the Verb Type v-ditr in the conversion list illustrated in TABLE 1, here the by item (iii) in that list.",24,25
2437,10769891,"The interchangeability is done as a prelude to rhyme checking, whereby phonemes in certain groups, such as p, are replaced by an abstract symbol denoting the group (e.g. PTK).",26,27
2438,204783569,"It is an important observation, though, that the meaning of each remaining symbol at this point has changed.",14,15
2439,218973756,"Incidentally, our annotation choices are compatible: false starts are marked directly with a specific symbol ($ in our case, the suffix -INA in their case), repetition and revisions are labelled (respectively, PARA for us, REP and REV for them).",16,17
2440,18866953,"Mechanically speaking, in going from template to grammatical type, one simply replaces each hyphen or underline in the template label by a type unification symbol.",26,27
2441,2523524,"The symbol "".""",1,2
2442,2523524,"signifies a fatal violation, the one that is responsible for a candidate's nonoptimality, whereas the symbol or indicates the optimal output. (",18,19
2443,5994124,"Formulas are built out of {<, >, , , N, X, V, lex} where the symbol lex stands for the word the formula has been assigned to.",22,23
2444,5994124,"We indicate this value by means of the symbol C i : C i = F words ( W i , T i ) + F types ( W i , T i ) 2 .",8,9
2445,227230636,"We mark each relation of this phenomenon in the ant attribute of the ANA element, using a separating symbol.",19,20
2446,7273065,"In the list above, for each feeling term, following the weight of the relation, a symbol in brackets indicates the majority polarity (which accounts for over 50% of votes) or the absence of a dominant polarity.",18,19
2447,613436,"Quantum Language Models In this section we describe our approach to build QLM that can compute probabilities for the occurrence of a sequence w = (w 1 , w 2 , ..., w n ) of length n, composed using N different symbols, the vocabulary containing all the words in the model, i.e. for every symbol w in the sequence w ∈ {0, ..., N − 1}.",60,61
2448,613436,"We define a set of orthogonal N -dimensional vectors {e w : w ∈ {0, ..., N −1}}, spanning the complex space H = C N ; to measure the probability of a symbol w, collapsing the state over the space spanned by e w , we use the projector Π w = e w e † w .",40,41
2449,613436,"Note that all the words in the vocabulary have been encoded as numbers corresponding to the N dimensions of the vector space H. Our method is sequential, from QMT point of view, in the sense that we use a quantum system that produces a single symbol upon measurement.",47,48
2450,613436,"To solve this issue, our approach is to avoid the complete collapse of the state after each symbol measurement using a common technique in QMT: we introduce an ancillary system described by a fictitious D-dimensional Hilbert space, H ancilla = C D , and we couple the original system to the ancillary system.",18,19
2451,613436,The advantage of using this method is that the time evolution for the coupled system creates nontrivial correlations between the two entangled systems such that measuring and collapsing the symbol state keeps some information about the whole sequence stored in the ancillary part of the state.,29,30
2452,613436,"This information is then reshuffled into the symbol state via time evolution, resulting in a 'memory effect' that takes the whole sequence of symbols into account, thereby extending the idea behind the Ngrams approach.",7,8
2453,613436,"Following this idea, we can represent every symbol in our system with a specific pdimensional vector trained using one of the available techniques w → (α 1 (w), ..., α p (w)) or fixed randomly.",8,9
2454,613436,We defined ρ s 0 as a diagonal N × N matrix containing the classical Maximum Likelihood probability Estimation to have a specific symbol at the first sequence position: ρ s 0 = 1 |S| w∈S Π w 1 where S is again the set of all sequences in the training set and w 1 is the first word in each sequence w. With regard to the ancilla system we do not know anything about it and thus we have to define ρ a 0 as the D × D diagonal matrix ρ a 0 = I D Tr(I D ) .,23,24
2455,613436,"Note that only the value of first symbol in the sequence, w 1 , enters in the expression.",7,8
2456,835116,Also introduced is a symbol which matches any non-empty essential phrase sequence.,4,5
2457,835116,The symbol `?',1,2
2458,835116,The symbol `I' indicates a correct segmentation point.,1,2
2459,1759487,"A dependency grammar is a six-tuple <W, C, S, D, I, H>, where W is a finite set of words; C is a set of syntactic categories; S is a non-empty set of root categories (CD_ S); D is the set of dependency relations (e.g. SUB J, OBJ, XCOMP, P-OBJ, PRED2); I is a finite set of symbols (among which the special symbol 0), called u-indices; H is a set of dependency rules of the form x:X (<rlYlUl'Cl> ... <ri.lYi.lUi.l'Ci_l> # <a'i+IYi+lUi+l'Ci+l> ... <rmYmum'Cm>) 1) xe W, is the head of the rule; 2) Xe C, is its syntactic category; 3) an element <rj Y-u-q:j> is a d-quadruple • J J (which describes a dependent); the sequence of d-quads, including the special symbol # (representing the linear position of the head), is called the d-quad sequence.",90,91
2460,1759487,"A dependency grammar is a six-tuple <W, C, S, D, I, H>, where W is a finite set of words; C is a set of syntactic categories; S is a non-empty set of root categories (CD_ S); D is the set of dependency relations (e.g. SUB J, OBJ, XCOMP, P-OBJ, PRED2); I is a finite set of symbols (among which the special symbol 0), called u-indices; H is a set of dependency rules of the form x:X (<rlYlUl'Cl> ... <ri.lYi.lUi.l'Ci_l> # <a'i+IYi+lUi+l'Ci+l> ... <rmYmum'Cm>) 1) xe W, is the head of the rule; 2) Xe C, is its syntactic category; 3) an element <rj Y-u-q:j> is a d-quadruple • J J (which describes a dependent); the sequence of d-quads, including the special symbol # (representing the linear position of the head), is called the d-quad sequence.",189,190
2461,1759487,A dependency rule (as well as a u-triple) with u-indices all annotated is said to be instantiated: I + refers to the set of annotated u-indices (including the special symbol 0).,39,40
2462,1759487,"A derivation for the sentence ""Beans I know John likes"" is the following (at each step, the leftmost derivation object is underlined, and the derivation relation to be applied marks the symbol =~): The dependency tree corresponding to this derivation is in fig.",36,37
2463,1759487,"A PATrERN is an abstraction over a dependency rule, where the head can (possibly) reduce to the syntactic category (from x:X to X), and some subsequences of d-quads can be (possibly) replaced by some variable symbol.",46,47
2464,1759487,"that uiE I. (2) • To introduce and keep apart the u-indices for a subtree and the ones for a single node in the derivation process, we must modify the word objects in 4-tuples consisting of a word w (E W) or the trace symbol 8 (~W) and three annotated indices r I, St and v. Given a grammar G, the set of word objects of Gis Wx(G)={q,lax v / r i , St,v E I +, xE W u { 8} }.",53,54
2465,1759487,"is a special symbol that specifies that the complex index concerns the single node linked by the dependency relation r. (4) To constrain the attachment of the trace nodes of gapped dependents to the trace node of the head, we generalize the complex form of the utriple in <q.u, r, x:X> where u, r, and X are as before, and qeIu{ !}.",3,4
2466,11474571,"Here hair and law are homonyms in Chinese, and sky, the symbol of justice in Chinese culture, is blocked by the umbrella, therefore ""a lawless society"".",13,14
2467,14492097,"second) symbol on the righthand side of the rule is considered dominant, and the other dependent.",2,3
2468,14492097,"If a rule has only one symbol on its right-hand side, we consider the symbol to be dominant.",6,7
2469,14492097,"If a rule has only one symbol on its right-hand side, we consider the symbol to be dominant.",17,18
2470,14492097,"A rule is applied (for a reduction) in the following way: The dependent symbol is deleted (if there is one on the right-hand side of the rule) , and the dominant one is rewritten (replaced) ~, the symbol standing on the left-hand side of the rule.",16,17
2471,14492097,"A rule is applied (for a reduction) in the following way: The dependent symbol is deleted (if there is one on the right-hand side of the rule) , and the dominant one is rewritten (replaced) ~, the symbol standing on the left-hand side of the rule.",47,48
2472,14492097,"Put informally', a DR-tree (created by a FODG G) is a finite tree with a root and with the following two types of edges: a) vertical: these edges correspond to the rewriting of the dominant symbol by"" the symbol which is on the left-hand side of the rule (of G) used.",44,45
2473,14492097,"Put informally', a DR-tree (created by a FODG G) is a finite tree with a root and with the following two types of edges: a) vertical: these edges correspond to the rewriting of the dominant symbol by"" the symbol which is on the left-hand side of the rule (of G) used.",48,49
2474,14492097,The vertical edge leads {is oriented) from the node containing the original dominant symbol to the node containing the symbol from the lefthand side of the rule used.,15,16
2475,14492097,The vertical edge leads {is oriented) from the node containing the original dominant symbol to the node containing the symbol from the lefthand side of the rule used.,21,22
2476,14492097,b) oblique: these edges correspond to the deletion of a dependent symbol.,13,14
2477,14492097,"Any, such edge is oriented from the node with the dependent deleted symbol to the node containing the symbol from th{ left-hand side of the rule used.",13,14
2478,14492097,"Any, such edge is oriented from the node with the dependent deleted symbol to the node containing the symbol from th{ left-hand side of the rule used.",19,20
2479,14492097,In the sequel the symbol :Vat means the set of natural numbers (without zero).,4,5
2480,14492097,"The A is called symbol of I r. the number i is called hori:o,tal indcz"" of U. j is called vertical index, c is called domination index.",4,5
2481,14492097,The horizontal index expresses the correspondence of U with the i-th input symbol.,14,15
2482,14492097,"Then there is exactly, one node Ul of the form [B, i, j-1, ij] in Tr, such that the pair {U], U) creates a (vertical) edge of Tr, and there is a rule in G with A on its left-hand side, and with B in the role of the dominant symbol of its right-hand side.",67,68
2483,14492097,In such a case we say that the string w is parsed into Tr by G. The symbol L(G) represents the set of strings (sentences) parsed into some DR-tree from TN(G).,17,18
2484,14492097,"The dependency tree dT(Tr) contracted from Tr is defined as follows: The set of nodes of dT(Tr) is the set of 3-tuples [ai.i, k(i)] (note that ai is the i-th symbol of u:).",42,43
2485,14492097,"The symbol dTN(G) denotes the union of all dTN(u,, G) for u"" E L(G).",1,2
2486,14492097,When i is the symbol .. it means that no limitation is imposed on the corresponding value of the measure A'g.,4,5
2487,14492097,The symbol CF + denotes the set.,1,2
2488,5487759,"Def is a noun, Num the value of Number (Sg or P1) and Def that of Definiteness (+ or-), then (i) rewrite + (-) into Artdf (Arfin) and place this symbol to the left of the nodes subordinated to node x (including x itself); recall that 'subordinated' is the transitive closure of 'depending'; Artdf will be transduced into the phonemic and phonetic (or graphemic) shapes of the, and Artin into a, an, or (with plural) into zero, (ii) rewrite PI (Sg) into -s (0) (this certainly requires to take into account also several specific sets of exceptions before Rule M2 is applied, cf.",42,43
2489,1173587,This attention vector is used to generate the next output symbol w t and to compute the next state of the decoder s t+1 .,10,11
2490,1173587,"This means that to delete a word, there must always be a corresponding DEL symbol.",15,16
2491,1173587,There are much more KEEP symbols in the training data as any other symbol (see tables 2 and 5).,13,14
2492,10206811,"A dependency grammar is a five-tuple <W,C,S,D, H>, where W is a finite set of words of a natural language; C is a finite set of syntactic categories; S is a non-empty set of categories (S _C C) that can act as head of a sentence; D is the set of dependency relations, for instance SUB J, OBJ, XCOMP, P-OB3, PRED; H is a set of dependency rules of the form z:X (<raYl> ... <ri-l~-l> # <ri+l~+l> ... <rmYrn>) 1) z E W, is the head of the rule; 2) X E C, is its syntactic category; 3) an dement <rjYj> is a d-pair (which describes a dependent); the sequence of d-pairs, ineluding the special symbol # (representing the linear position of the head), is called the d-pair sequence.",173,174
2493,10206811,"In order to compute the sets of dependency relations that the subcategorization frame includes, we form the cartesian product of the various Depe(d): Carte = I]aeD Depq(d) and we evaluate the union of each member of Carte; each of them is extended by including the special symbol #: DepSet, = {m I m = (U.es, sec°.t.s) U {#}} where the union is a mukiset union, preserving duplications.",50,51
2494,130265,"Consider figure 6 , where each symbol denotes an example in S, with symbols ""x"" belonging to X and symbols ""e"" belonging to T. The curved lines delimit the semantic vicinities (extents) of the two ""e""s, i.e. sense 1 and sense 2, respectively 1.",6,7
2495,14859347,"Anentry kind indicator (symbol), is followed by an open list of field names(symbols) and values (strings) pairs: (hi, vi)*.",4,5
2496,14859347,The ordering of the pairs in the list is relevant and several pairswith the same symbol can be contiguous.,15,16
2497,11734261,"We also collapsed sequences of NNPs, possibly interleaved by the symbol ""&"", e.g., George Bush:NNP and Procter & Gamble:NNP.",11,12
2498,3090702,"Adaptation to dependency treebanks We apply a similar approach to dependency treebanks: internal nodes (LHS of the rules) are now the head category, and children nodes (RHS) are the list of dependents, respecting their projection order, plus an extra node with the symbol ""*"" indicating the head projection.",50,51
2499,3090702,A link on each LHS symbol loads the corresponding information on the main part of the windows.,5,6
2500,15263800,"It is marked by the symbol * in the result tables, and it is not taken into account in the rankings.",5,6
2501,15263800,"The "" * "" symbol denotes runs that include task organizers.",4,5
2502,14509543,Strings with zero instances of symbol 2 (main stress); any utterance must contain a stressed syllable in order to be pronounceable. •,5,6
2503,1252249,"They estimate this probability using the stack-top state s i−1 , next input symbol l i and next action a i .",15,16
2504,11644074,"They estimate this probability using the stack-top state s i−1 , next input symbol l i and next action a i .",15,16
2505,14474596,"Thus, anyone can use any lexical item not only to refer to some extra-linguistic state of affairs, but to refer to the lexical item, qua symbol.",30,31
2506,14474596,"This symbol, of course, has many representations: phonetic, phonemic, syntactic, or orthographic, as well as more exotic representations such as Morse code, ASCII code, spectrographic, and so on, but it also has many variant forms within each of those representation (e.g., handwriting, neon lights, type), as in examples (8)--( 16 ). (",1,2
2507,13896830,"The lexica of the affixes is loaded (1-6) and then any prefix (the ""*"" symbol indicates 0 or more times) followed by one unique suffix is identified.",21,22
2508,13896830,"The ""+"" symbol is used for splitting the term.",4,5
2509,14577262,Here is an excerpt from a text file that contains qualitative-adjective bases with Umlaut: {alt}:{1lt} On the left is the lemma (alt 'old') that will appear in the analysis output and on the right is the abstract form that contains the abstract symbol 1 for a lowercase a which is subject to Umlaut alternations under certain conditions.,51,52
2510,8835914,"In the type SP r , segmentation points are pre-inserted between symbol and Chinese characters.",13,14
2511,13154442,"for ""any symbol"" and '0' or '[]' and '[..]' for deletion and epenthesis rules, respectively, require attention. '?'",3,4
2512,8593587,"9) #: V, # 4 <(SUBJ) (OBJ) (XCOMP)>' (OBJ PRED) P. ,' GENERIC XCOMP SUBJ) = OBJ) (t XCOMP OBJ) (t SUBJ) The symbol indicates extension and it is sometimes referred to as overwriting or unification by default inheritance [7] .",44,45
2513,8821705,"Stochastic Machine Translation In machine translation, the objective is to map a source symbol sequence Ws = wx,...,WNs (wi E Ls) into a target sequence WT = xl,..., XNT (Xi E LT).",14,15
2514,8821705,"The probability P(Ws, WT) = P(R) is computed in the same way as n-gram model: where wl E LsUe, zi E LTUe, e is the empty string and wi_zi is the symbol pair (colons are the delimiters) drawn from the source and target language.",39,40
2515,218973969,"During character checking, we regularly encounter inconsistencies such as occurrences of both a combined character and a sequence of base character plus diacritic for what appears as the same symbol to the human eye (e.g. U+0101 vs. U+0041 U+0304 for <ā>).",30,31
2516,16538528,"Upon seeing the eos symbol, the final time step initializes a target blue RNN.",4,5
2517,216050683,"Eventuality descriptions such as run or love are of type v → t, which is abbreviated to E (see Pustejovsky (1995) ), while the same symbol E is also used as as a symbol for a variable ranging over a set of eventualities or instances of an eventuality.",30,31
2518,216050683,"Eventuality descriptions such as run or love are of type v → t, which is abbreviated to E (see Pustejovsky (1995) ), while the same symbol E is also used as as a symbol for a variable ranging over a set of eventualities or instances of an eventuality.",38,39
2519,216050683,"b. Annotation(id=a5) event(e1, w2-3, pred:run, tense:past) c. Semantic form: σ(e1 e ) α := {e 1 :e}[run(e 1 ) t ∧ past(e 1 ) t ] α where "":="" is a meta-symbol standing for ""is"".",53,54
2520,7466076,"Upon seeing the <eos> symbol, the final time step initialises a target RNN.",6,7
2521,7466076,Translation is finished when the decoder predicts the <eos> symbol.,11,12
2522,12713328,"An object and an expertise field are represented by an atomic symbol, and an attribute of an object is represented by a fimction syml)ol.",11,12
2523,21698095,CTC can be implemented with a softmax output layer using an additional unit for the blank label ∅. The symbol ∅ corresponds to no output and is used to estimate the probability of outputting no label at a given time.,19,20
2524,21698095,"The output layer is a 41-dimensional softmax layer with the units, corresponding to 39 context-independent phones, 1 noise model and 1 blank symbol.",28,29
2525,219310243,"We can distinguish two sections: S (for the classic start symbol), that describes the structure (i.e. the ""right-hand side"" of the rule); and restrictions, where we specify the morphosyntactic restrictions applicable to the elements of the structure.",12,13
2526,5153552,Merging multiple letters representing a single phoneme into one symbol.,9,10
2527,201706805,Apply symbol grounding Symbol grounding helps to connect abstract representations of meaning with objects in the real world or to unambiguous descriptions of concepts or entities.,1,2
2528,201706805,"Other interesting applications for symbol grounding are GPS coordinates for toponyms (Leidner, 2008) , visualisation of concepts or actions (Navigli and Ponzetto, 2012) , or creating timelines (Bamman and Smith, 2014) .",4,5
2529,3533173,"Basic conditions are defined as follows: • If W is a symbol denoting a WordNet concept and x is a term, then W(x) is a basic condition; • If V is a symbol denoting a thematic role and x and y are terms, then V(x,y) is a basic condition;  The first two structures are basic DRSs while the last one is a segmented DRS. •",12,13
2530,3533173,"Basic conditions are defined as follows: • If W is a symbol denoting a WordNet concept and x is a term, then W(x) is a basic condition; • If V is a symbol denoting a thematic role and x and y are terms, then V(x,y) is a basic condition;  The first two structures are basic DRSs while the last one is a segmented DRS. •",36,37
2531,201710741,"In JPC zh-ja task, zh part is segmented by a character and ja part is segmented by a word except for alpha-numeric and symbol character sequences.",28,29
2532,201710741,Alpha-numeric and symbol character sequences are segmented by a character.,4,5
2533,7996125,"For example, male is the symbol of the pronouns he and himself, europe of the adjective European, and 14 : 00 for the time expression 2 pm.",6,7
2534,7996125,A symbol together with a CCG category and a semtag are sufficient to determine the lexical semantics of a token (see Figure 2 ).,1,2
2535,7996125,"Notice that the employed symbols are not as radical and verbalized as the concepts in AMRs, e.g., the symbol of opinion is opinion rather than opine.",20,21
2536,7996125,"First, using deep forms as symbols often makes it difficult to recover the original and semantically related forms, e.g., if opinion had the symbol opine, then it would be difficult to recover opinion and its semantic relation with idea.",26,27
2537,5315979,man(x) (2) Formal semantics of a content word usually involves a symbol corresponding the lemma.,14,15
2538,5315979,"In particular, in the PMB, Boxer (Bos, 2008 (Bos, , 2015) ) interprets a sem-tag as a mapping from CCG categories (augmented with thematic roles) to a formal semantic schema which is further specified by a token-related predicate/constant symbol and thematic roles (if any).",53,54
2539,3012890,"The concept of substitutability generally applies to central part of the induction procedure itself, i.e. substitutable elements (e.g. substrings, words, structures) are assumed to be of the same type (represented e.g. with the same symbol).",40,41
2540,3012890,"Grammar construction is performed by replacement of morphemes with a symbol, if they have equal signatures.",10,11
2541,12637467,Each computing agent in the network corresponds to an occurfence of a non-terminal or terminal symbol appearing in the grammar rules.,17,18
2542,12637467,"Suppose we have a context fi-ee grammar rule such as: VP --> V NP (1) In bottom-up parsing, a usual interpretation of this kind of rule is: In a substring of an input string, if its first half portion can he reduced to a category (terminal/non-terminal symbol) V and subsequently, if its second half portion can be reduced to a category VP, then the whole substring can be reduced to a category VP.",62,63
2543,12637467,"r VP tl t2 Figure 1 : Instead, we will take a radically different approach, ill which * more than one, actually, a number of computing agents are allowed to work concurrently, each performing a rather simple task, • for each occurrence of a non-terminal or terminal symbol in grammar rules, a computing agent is assumed, • such a computing agent receives data (messages), manipulates and stores data in its local memory, and also can send data (messages) asynchronously to other computing agents that correspond to non-terminal or terminal symbols, and • data to be passed around among such computing agents are partial parse trees.",55,56
2544,12637467,Suppose that the computing agent which acts for the V symbol in Rule (1) has received a (token that represents a) partial parse tree tl.,10,11
2545,12637467,Also suppose that the computing agent which acts for the NP symbol in Rule (1) has received a partial parse tree t2.,11,12
2546,12637467,"If the terminal symbol which is the right boundary of tl is, in the original input string, adjacent to the terminal symbol which is the left boundary of t2, then tl and t2 can be put together and they can form a larger partial parse tree which corresponds to the VP symbol in Rule (1).",3,4
2547,12637467,"If the terminal symbol which is the right boundary of tl is, in the original input string, adjacent to the terminal symbol which is the left boundary of t2, then tl and t2 can be put together and they can form a larger partial parse tree which corresponds to the VP symbol in Rule (1).",23,24
2548,12637467,"If the terminal symbol which is the right boundary of tl is, in the original input string, adjacent to the terminal symbol which is the left boundary of t2, then tl and t2 can be put together and they can form a larger partial parse tree which corresponds to the VP symbol in Rule (1).",54,55
2549,12637467,"In our scheme, it is natural that the computing agent acting for the NP symbol does the boundary checking because, in many simple cases, the NP agent often receives t2 after the V agent receives tl (due to the left-to-right nature of on-llne processing).",15,16
2550,12637467,"When the adjacency test succeeds, the NP agent concatenates tl and t2 and sends them to the computing agent acting for the non-terminal symbol VP in Rule (1).",26,27
2551,12637467,"The VP agent constructs, out of tl and t2, a partial parse tree with the root-node tag being the non-terminal symbol 'VP.'",26,27
2552,12637467,This newly constructed partial parse tree is then distribuled by tile VP agent to all the computing agents each of which acts for an occurrence of symbol VP in the right-hand side of a rule.,26,27
2553,12637467,"It is the matter of course that every single computing agent acting for a non-terminal or terminal symbol can work independently, in parallel and asynchronously.",19,20
2554,12637467,A Set of Rules as a Netwol'k of Computing Agents It should be clear from the previous subsection that a set of context-free grammar rules (even a singleton grammar) is represented as a network of computing agents each of which acts for an occurrence of a non-terminal or terminal symbol in a grammar rule.,55,56
2555,12637467,"More precisely, the correspondence between the set of computing agents and the set of occurrences of symbols in the set of grammar rules is oneto-one; for each occurrence of a symbol in a rule, there is one distinct computing agent.",34,35
2556,12637467,s --> NP vP (2) s --> s PP (3) NP --> DET N (4) PP --> PREP NP (5) A white box corresponds to the computing agent acting for a symbol in the right-hand side of a grammar rule and a dark box corresponds to the computing agent acting for the non-terminal symbol in the left-hand side of a grammar rule.,43,44
2557,12637467,s --> NP vP (2) s --> s PP (3) NP --> DET N (4) PP --> PREP NP (5) A white box corresponds to the computing agent acting for a symbol in the right-hand side of a grammar rule and a dark box corresponds to the computing agent acting for the non-terminal symbol in the left-hand side of a grammar rule.,69,70
2558,12637467,"This means that a partial parse tree constructed by the computing agent acting for the left symbol NP in Rule (4) is distributed to the three computing agents acting for tile three occurrences of symbol NP ill Rules (1), (2), and (5).",16,17
2559,12637467,"This means that a partial parse tree constructed by the computing agent acting for the left symbol NP in Rule (4) is distributed to the three computing agents acting for tile three occurrences of symbol NP ill Rules (1), (2), and (5).",36,37
2560,12637467,"Three Types of Computing Agents 1As the reader might have already noticed, there are three types of computing agents: Type-1 corresponds to the left symbol in a grammar rule, 'type-2 corresponds to the left-corner (i.e. left-most) right symbol, and Type-3 corresponds to other right symbols. (",26,27
2561,12637467,"Three Types of Computing Agents 1As the reader might have already noticed, there are three types of computing agents: Type-1 corresponds to the left symbol in a grammar rule, 'type-2 corresponds to the left-corner (i.e. left-most) right symbol, and Type-3 corresponds to other right symbols. (",47,48
2562,12637467,"If a grammar rule has more than two right symbols, each of tlle right symbols except the left-corner symbol is represented as a Type-3 agent.)",21,22
2563,12637467,"L-It t : Figure 2 : A Type-1 computing agent A1 receives a concatenation of parse trees from the Type-3 agent acting for the rightmost right symbol (e.g., NP for the case of Rule (1)) and constructs a new parse tree with its root node being the non-terminal symbol that A1 acts for and distributes it to all the Type-2 or Type73 agents acting for the occurrences of the same non-terminal symbol (e.g., 'NP' in the above case).",28,29
2564,12637467,"L-It t : Figure 2 : A Type-1 computing agent A1 receives a concatenation of parse trees from the Type-3 agent acting for the rightmost right symbol (e.g., NP for the case of Rule (1)) and constructs a new parse tree with its root node being the non-terminal symbol that A1 acts for and distributes it to all the Type-2 or Type73 agents acting for the occurrences of the same non-terminal symbol (e.g., 'NP' in the above case).",57,58
2565,12637467,"L-It t : Figure 2 : A Type-1 computing agent A1 receives a concatenation of parse trees from the Type-3 agent acting for the rightmost right symbol (e.g., NP for the case of Rule (1)) and constructs a new parse tree with its root node being the non-terminal symbol that A1 acts for and distributes it to all the Type-2 or Type73 agents acting for the occurrences of the same non-terminal symbol (e.g., 'NP' in the above case).",82,83
2566,12637467,"A Type-2 computing agent A2 receives a partial parse tree from some computing agent that is acting for the occurrence of the same symbol as A2 acts for, and simply passes it to the computing agent acting for the symbol occurrence which is right-adjacent to the symbol occurrence that A2 is acting for.",23,24
2567,12637467,"A Type-2 computing agent A2 receives a partial parse tree from some computing agent that is acting for the occurrence of the same symbol as A2 acts for, and simply passes it to the computing agent acting for the symbol occurrence which is right-adjacent to the symbol occurrence that A2 is acting for.",40,41
2568,12637467,"A Type-2 computing agent A2 receives a partial parse tree from some computing agent that is acting for the occurrence of the same symbol as A2 acts for, and simply passes it to the computing agent acting for the symbol occurrence which is right-adjacent to the symbol occurrence that A2 is acting for.",49,50
2569,12637467,"In the case where a grammar rule has just one right symbol as in NP --> N, (6) a Type-2 agent acting for N sends a partial parse tree to the 'type-1 agent acting for NP.",11,12
2570,12637467,A Type-3 computing agent has two kinds of sources of parse trees to receive: one from Type-1 agents and the other from the Type-2 or Typeo3 agent acting for its leftadjacent symbol occurrence.,32,33
2571,12637467,"In the case of Rule (1), the Type-3 agent acting for NP receives partial parse trees from Type-1 agents acting for occurrences of symbol NP in other rules and also from the Type-2 agent acting for V in Rule (1).",26,27
2572,12637467,"If such a parse tree t2 has already arrived at A3, then A3 concatenates tl and t2 and passes them to the computing agent acting ibr the symbol occurrence which is right-adjacent to the symbol occurrence A3 acting for.",28,29
2573,12637467,"If such a parse tree t2 has already arrived at A3, then A3 concatenates tl and t2 and passes them to the computing agent acting ibr the symbol occurrence which is right-adjacent to the symbol occurrence A3 acting for.",37,38
2574,12637467,"In the case where no right-adjacent symbol exits in the grammar rule, (which means that the symbol occurrence A3 is acting for is the right-most right symbol in the glamrnar rule), A3 sends the concatenated trees to the Type--1 computing agent acting for the left symbol of the grammar rule.",8,9
2575,12637467,"In the case where no right-adjacent symbol exits in the grammar rule, (which means that the symbol occurrence A3 is acting for is the right-most right symbol in the glamrnar rule), A3 sends the concatenated trees to the Type--1 computing agent acting for the left symbol of the grammar rule.",20,21
2576,12637467,"In the case where no right-adjacent symbol exits in the grammar rule, (which means that the symbol occurrence A3 is acting for is the right-most right symbol in the glamrnar rule), A3 sends the concatenated trees to the Type--1 computing agent acting for the left symbol of the grammar rule.",32,33
2577,12637467,"In the case where no right-adjacent symbol exits in the grammar rule, (which means that the symbol occurrence A3 is acting for is the right-most right symbol in the glamrnar rule), A3 sends the concatenated trees to the Type--1 computing agent acting for the left symbol of the grammar rule.",53,54
2578,12637467,We do not have to make a special treatment for grammar rules such as: NP --> NP and NP (7) where a lower case symbol 'and' is a terminal symbol.,28,29
2579,12637467,We do not have to make a special treatment for grammar rules such as: NP --> NP and NP (7) where a lower case symbol 'and' is a terminal symbol.,35,36
2580,12637467,"Thus, in our parsing scheme, the grammatical categories of each word in the whole vocabulary in use are described by grammar rules with a single right symbol.",28,29
2581,12637467,"Notice that there can be more than one such computing agent for each word, due to multiplicity of grammatical category and the multiple occurrences of the same symbol in grammar rules.",28,29
2582,12637467,"In fact, a single word (terminal symbol) is also the simplest case of parse tree.)",8,9
2583,12637467,"This agent constructs a data form of the parse tree for NP, which looks like: (2 4 (NP (DET a) (N girl))) This data form will be distributed among the Type-2 and Type-3 computing agents acting for symbol NP in the network. (",47,48
2584,12637467,Suppose a Type-1 computing agent Npl is acting for an occurrence of a nonterminal symbol NP.,14,15
2585,12637467,"Instead of letting Npl distribute the parse trees it constructs to Type-2 or Type-3 agents acting for occurrences of the symbol NP, we can let Npl send the parse trees to the semantics processing agent which checks the semantic validity of the parse trees in tim pipe-lining manner.",20,21
2586,12637467,The complete separation of the semantic processing phase from the syntactic processing phase in usual natural language processing systems corresponds to the placing semantic processing agents only after the Type-1 computing agents that act for tile non-terminal symbol S that stands for correct sentences.,39,40
2587,12637467,"Since the node set of the network has one-to-one correspondence to the set of symbol occurrences in a given set of grammar rules, the nmnber of computing agents can be very large if the grammar is complex.",18,19
2588,12637467,"Since the main task of a Type-1 agent (acting for the left symbol of a grammar rule) is just to distribute a constructed parse tree, this task Can be performed by the Type-3 agent which acts for the rightmost right symbol of the grammar rule.",13,14
2589,12637467,"Since the main task of a Type-1 agent (acting for the left symbol of a grammar rule) is just to distribute a constructed parse tree, this task Can be performed by the Type-3 agent which acts for the rightmost right symbol of the grammar rule.",43,44
2590,5871781,Every figure (digit) in the corpus has been changed into a special symbol.,14,15
2591,13553861,"As a first observation, we notice that a few frequent trigger words are almost always associated to incorrect event predictions, such as the trigger words 'ubiquitous' and 'radiation', or a punctuation symbol.",38,39
2592,28721147,Explanation The fact that two attributes in an f-structure have the same value can be expressed in a compact way by combining the two attributes with '&' as the label of the dominance symbol '>'.,37,38
2593,6466946,A gene symbol disambiguation algorithm then links these canonical forms to gene families and gene identifiers (Section 3.2).,2,3
2594,6466946,Section 3 describes how we tackle gene symbol ambiguity across and within species.,7,8
2595,6466946,"Furthermore, BANNER frequently tags noun phrases such as human Esr-1 gene rather than only the minimal symbol Esr-1.",17,18
2596,6466946,"In a first step towards canonicalization of the entities, a mapping table was assembled containing common contexts in which a gene symbol appears and where the full noun phrase can be reduced to that embedded symbol for the sake of information retrieval (Table 1 ).",22,23
2597,6466946,"In a first step towards canonicalization of the entities, a mapping table was assembled containing common contexts in which a gene symbol appears and where the full noun phrase can be reduced to that embedded symbol for the sake of information retrieval (Table 1 ).",36,37
2598,6466946,"If the string can be matched 6 to a known symbol in Entrez Gene, stop the algorithm 3.",10,11
2599,6466946,The first step towards gene symbol disambiguation involves collecting all possible synonyms for each gene family from either Ensembl or Homolo-Gene.,5,6
2600,6466946,"records whether the symbol is registered as an official or default gene symbol, as the gene description, an abbreviation, or a synonym.",3,4
2601,6466946,"records whether the symbol is registered as an official or default gene symbol, as the gene description, an abbreviation, or a synonym.",12,13
2602,6466946,"In a subsequent step, the ambiguity is reduced by applying the following set of rules, relying on a priority list imposed on the type of the symbol, ensuring we choose an official or default symbol over a description or synonym.",28,29
2603,6466946,"In a subsequent step, the ambiguity is reduced by applying the following set of rules, relying on a priority list imposed on the type of the symbol, ensuring we choose an official or default symbol over a description or synonym.",37,38
2604,6466946,"If one family has the most (or all) hits for a certain symbol and these hits refer to a symbol type having priority over other possibilities, this family is uniquely assigned to that symbol.",14,15
2605,6466946,"If one family has the most (or all) hits for a certain symbol and these hits refer to a symbol type having priority over other possibilities, this family is uniquely assigned to that symbol.",21,22
2606,6466946,"If one family has the most (or all) hits for a certain symbol and these hits refer to a symbol type having priority over other possibilities, this family is uniquely assigned to that symbol.",36,37
2607,6466946,"If a conflict exists between one family having the highest linkage count for a certain symbol, and another family linking that symbol to a higher priority type, the latter is chosen.",15,16
2608,6466946,"If a conflict exists between one family having the highest linkage count for a certain symbol, and another family linking that symbol to a higher priority type, the latter is chosen.",22,23
2609,6466946,"If two families have equal counts and type priorities for a certain symbol, this symbol can not be unambiguously resolved and is removed from further processing.",12,13
2610,6466946,"If two families have equal counts and type priorities for a certain symbol, this symbol can not be unambiguously resolved and is removed from further processing.",15,16
2611,6466946,"If the ambiguity is still not resolved, all families with only one hit for a certain symbol are removed, and steps 1-3 repeated.",17,18
2612,6466946,"This, however, does not take symbol synonymy into account.",7,8
2613,6466946,"We evaluate its impact using manually tagged entities from the publicly available BioNLP'09 Shared Task (ST) training set, which specifically aims at identifying entities that are likely to match gene and protein symbol databases (Kim et al.,",35,36
2614,6466946,Evaluation of homology-based disambiguation The symbol to gene family disambiguation algorithm succesfully resolves almost all gene symbols in HomoloGene or Ensembl (Section 3.2).,7,8
2615,6466946,"First, we propose an algorithm for stripping affixes in entity occurrences tagged by the BAN-NER named entity recognizer, addressing the problem of such entities often including wider context which prevents direct matching against gene symbol databases.",38,39
2616,6466946,"Second, we propose an algorithm which assigns to the vast majority of gene symbols found in Ho-moloGene and Ensembl a single unique gene family, resolving the present intra-organism ambiguity based on symbol occurrence statistics and symbol type information.",37,38
2617,6466946,"Second, we propose an algorithm which assigns to the vast majority of gene symbols found in Ho-moloGene and Ensembl a single unique gene family, resolving the present intra-organism ambiguity based on symbol occurrence statistics and symbol type information.",41,42
2618,6466946,"All relevant data, namely all original events and entities together with their canonical forms, the generalizations of events based on canonical entity forms and gene families, as well as the gene symbol to unique family mapping are made publicly available as records in a MySQL database.",34,35
2619,16842624,"If we assume that n has a different function when used with another relationship, then we can consider that r and n make a new symbol, called a. So that we can express the 5−tuple P v,r 1 ,n 1 ,r 2 ,n 2 ( ) as P v,a 1 ,a 2 ( ) .",26,27
2620,7030044,"From the GRN data, we can retrieve a symbol-to-type mapping, recording whether a specific symbol referred to e.g. a gene, protein or operon in a certain article.",9,10
2621,7030044,"From the GRN data, we can retrieve a symbol-to-type mapping, recording whether a specific symbol referred to e.g. a gene, protein or operon in a certain article.",20,21
2622,7030044,"We believe this shift in semantics is caused by the fact that a promoter binding is usually extracted as a binding event by the TEES classifier, while it can semantically be seen as a Transcription event, especially in those cases where the Theme is a protein name, and the Cause a gene symbol (Table 2 ).",55,56
2623,7030044,"We found that 23% could be attributed to a missing or incompatible BANNER entity, 59% to a false negative TEES prediction, 15% to a wrong GRN event type and 3% to incorrectly mapping the gene symbol to the standardized GRN format.",41,42
2624,11627586,Such non-causal relations between a noun phrase and its embedded gene symbol are referred to as entity relations.,13,14
2625,11627586,"Finally, the content of the gene symbol was also used as lexical information.",7,8
2626,11627586,"All lexical information in the feature vectors has undergone generalization by blinding the gene symbol with ""protx"" and all other co-occurring gene symbols with ""exprotx"".",14,15
2627,11627586,"The final feature vectors, representing sentences with exactly one tagged gene symbol, are classified using an SVM with a radial basis function as kernel.",12,13
2628,11627586,"Entity detection Once a sentence with a gene symbol is classified as containing a certain type of entity relation, it is necessary to find the exact domain term that is related to that gene symbol.",8,9
2629,11627586,"Entity detection Once a sentence with a gene symbol is classified as containing a certain type of entity relation, it is necessary to find the exact domain term that is related to that gene symbol.",35,36
2630,11627586,"To this end, we have designed a pattern matching algorithm that searches within a given window (number of tokens) around the gene symbol.",25,26
2631,13320571,"Lexical Feature of One Word Let sw be a one word lexical feature, where w is the word and s is an arbitrary symbol.",24,25
2632,13320571,"s 0 e d , commonly used by all words, across the feature template s 0 w. As another example, in the case of features combining a word and its POS tag, such as s 0 t VBD w saw , we treat s 0 t VBD as a formal symbol and replace the feature as the following: s 0 t VBD e saw := 0.6s 0 t VBD e 1 + . . .",53,54
2633,9631585,"A frequent case in biomedical literature involves use of the slash symbol (""/"") to state synonyms.",11,12
2634,9631585,The slash symbol is ambiguous as it is used also to indicate dimerized proteins.,2,3
2635,2055349,"The character pattern designated (shaped) is similar to a shape feature, but the consecutive character types are reduced to one symbol, for example, ""ULLLL"" (shape) is represented with ""UL"" (shaped) in the example of Table 1 ).",23,24
2636,17420851,We replaced all the rare words (occurring less than four times) with the special symbol OOV (implying out of vocabulary) to facilitate the calculation of the SP of unseen words appearing in the test set.,16,17
2637,17420851,"CSPD-X is the same as CSPD except that the coreferent arguments are masked; namely, the task is to discriminate a masked context-attached tuple (e.g. ⟨Mary, eat, ⟨X, delicious⟩ subj ⟩, where X denotes the special symbol for masked arguments) from its masked pseudo-negative counterpart.",46,47
2638,371,"Consider Figure 7 , where each symbol denotes an example in a given corpus, with symbols x as unsupervised examples and symbols e as supervised examples.",6,7
2639,9837420,"The six cause types, and potential means of avoiding them, are as follows: (a) Errors in keyword extraction In some instances, unsuitable keywords such as ""なんち ゃら (watchamacallit)"", ""どさくさ (mess)"", and ""○○"" (a symbol used to mean ""a certain"", as in ""a certain person"") were extracted as misinformation keywords.",53,54
2640,14963062,"D(W i ) contains the corresponding observed word w i and a special symbol ω that represents other possibilities, i.e., D(W i ) = {w i , ω}.",13,14
2641,14001491,"Finally, we apply a text mining technique to extract frequent symbol patterns among a set of the symbol sequences.",11,12
2642,14001491,"Finally, we apply a text mining technique to extract frequent symbol patterns among a set of the symbol sequences.",18,19
2643,14001491,"For instance, a fixation of an argument candidate to the left of the target predicate is denoted by the symbol 'LA'.",20,21
2644,14001491,"E ' In step 3, highly frequent patterns of symbols are extracted from the set of symbol sequences (Pei et al.,",17,18
2645,12804140,"The following figure shows the basic formal setup of a linguistic sign: (1)             sign PHON list(phon-symbol) SYNSEM         LOC       CAT   HEAD head SPR list(synsem) COMPS list(synsem)   CONT INDEX index RELS list(rels)       NONLOC nonloc                     At the highest level, there is a separation between the phonological and the syntactic and semantic properties of the sign.",32,33
2646,227231820,"1(a) ), which are extracted for every input token and include a special starter symbol called [CLS] token (Devlin et al.,",16,17
2647,174798168,"As the vocabulary size has a large effect on the distribution of BPE symbol lengths (Figure 4 , also see §3.2) and model quality, we determine this hyper-parameter empirically (Table 4).",13,14
2648,9524198,"In general, grammar input is first tokenized by a standard tokenizer that separates the input string into single tokens and replaces the white spaces with a special token boundary symbol.",30,31
2649,11580500,"Actions and mouse cursor positions are recorded at intervals of 10 msec, and are abstracted into (1) a time span labeled with an action symbol (""move"", ""rotate"" or ""flip"") and its target piece number (1-7), and (2) a time span labeled with a piece number which is under the mouse cursor during that span.",27,28
2650,231709707,"In semantic composition, sym is the place where the symbol (lemma) for a lexical item appears.",10,11
2651,226226646,"Here, all-but-verb masking S all\verb tended to achieve the best results, and all-but-symbol masking S all\symbol was on par or slightly better than S all .",22,23
2652,226226646,"Specifically, we consider two aspects: (i) how many [MASK] tokens to fill simultaneously and (ii) how to determine a word symbol in each time step.",28,29
2653,226226646,"For (ii) how to determine a word symbol in each time step, we compare the following two methods: • SAMPLE: Following Wang and Cho (2019) , we determine the output symbol by sampling a word from the probability distribution over vocabulary that is computed by pretrained MLM. •",9,10
2654,226226646,"For (ii) how to determine a word symbol in each time step, we compare the following two methods: • SAMPLE: Following Wang and Cho (2019) , we determine the output symbol by sampling a word from the probability distribution over vocabulary that is computed by pretrained MLM. •",37,38
2655,226226646,"ARGMAX: From the probability distribution over the vocabulary, we determine the output symbol by choosing the token with the highest probability.",14,15
2656,9707349,"apple {apple.n.01}++ orange {orange.n.01}++ oranges {orange.n.01}++ banana {banana.n.02}+= That is, the symbol '++' denotes an outbound link, whereas '+=' indicates a bidirectional link.",16,17
2657,14309920,"The top-level category is one of the parameters of a configuration, and the EUREKA CONFIGURATION specifies that FIELD instead of the STANDARD ROOT is the start-symbol of the grammar.",30,31
2658,21697629,"We apply BPE 1 to all Wikipedias 2 of sufficient size with various o and pre-train embeddings for the resulting BPE symbol using GloVe (Pennington et al.,",23,24
2659,14054312,We mark such words in the codesw tier as a intraword switch and use the symbol § following Çetinoglu (2016) .,15,16
2660,202779863,"The attention distribution is updated as follows (the symbol means element-wise product): a r t = r t a t (5) Local Variance Loss As discussed in section 1, the attention model putting most of attention weight on just a few parts of the input tends to achieve good performance.",9,10
2661,29363368,"The definition of fnuctiorml uncertainty given by Kaplan and Zaenen (in press ) is essentially as follows: (27) lf a is a regular expression, then (fa) = v holds |land only if ((fa) Surf(a, a))= v for some symbol a, where Suff(a, a) is the set of suffix strings y such that ay ( u. We will not discuss functional uncertainty further in this paper, except to show how it fits into out"" model for sets To achieve the proper interaction between sets and regular expressions, we merge ( 27 ) witb ( 16 ): (28) (so) :: v := IT(fia),forallfiEs = I I ((fi ai) Surf(a| u)), for all fi < s Alh)wiug difCcrent a i to be chosen tbr each fi provides the variation needed lot (23).",49,50
2662,8844517,"This has the following interpretation: / (1) (f s)= e holds if and only if f is an f-structure, s is a symbol, and the pair <s;v> E f. An f-structure is a hierarchical finite function from symbols to either symbols, semantic forms, f-structures, or sets of f-structures, and a parenthetic expression thus denotes the value that a thnetion takes for"" a particular symbol.",29,30
2663,8844517,"This has the following interpretation: / (1) (f s)= e holds if and only if f is an f-structure, s is a symbol, and the pair <s;v> E f. An f-structure is a hierarchical finite function from symbols to either symbols, semantic forms, f-structures, or sets of f-structures, and a parenthetic expression thus denotes the value that a thnetion takes for"" a particular symbol.",84,85
2664,8844517,"This notation is straightforwardly extended to allow for strings of symbols, as illustrated in expressions such as ( I"" co,~w (re,l) above, lfx=sy is a string composedofan irfitial symbol s followed by a (possibly empty) suffix stringy, then (2) (fxI~((fs)y) (f~) =-/', where c is the empty string.",37,38
2665,8844517,Suppose u is a (possibly infinite) set of symbol strings.,10,11
2666,8844517,"Then Kaplan and Zaenen say that (3) (f(r)= v holds if and only if ((fs) Suff(s,a))= v for some symbol .s, where Suff(s,a) is the set of suffix strings y such that sy 6 a. Thus, an equation with a string-set argnment holds if it wouhl hold for a string in the set that results fl'om a sequence of left-to-right symbol choices.",27,28
2667,8844517,"Then Kaplan and Zaenen say that (3) (f(r)= v holds if and only if ((fs) Suff(s,a))= v for some symbol .s, where Suff(s,a) is the set of suffix strings y such that sy 6 a. Thus, an equation with a string-set argnment holds if it wouhl hold for a string in the set that results fl'om a sequence of left-to-right symbol choices.",79,80
2668,8844517,"Strings x and y arbitrarily chosen frmn a and 13, respectively, might be related in any of three significant ways: Either (a) x is a prefix ofy (y is xy' for some string y'), (b) y is a prefix ofx (x is yx'), or (c) x and y are identical up to some point and then diverge (x is zsxx' and y is zsyy' with symbol Sx distinct from Sy).",86,87
2669,8844517,"The disjunction contains one branch for each symbol in the uneertainty's First set that is an initial attribute in one of the other equations, ohm a single branch tbr all of the residual inithd symbols: (12) (fa)=u iff (fslSuffix(a,S(qa,st))) 5(q(, , st~) ))::::v V (l'n--{s b...s,d~:*) = v The statement of the generic Free a/gm'ithm (10) is simplified by considering specific attributes as trivial regular languages, buL this suggests that COlnplex finite-state machinery would be roquh'ed to process them.",7,8
2670,231705323,"Bag-of-Audio-Words To represent the mood feature of a short music excerpt, we use a discrete symbol called an audioword (Liu et al.,",22,23
2671,33175460,"In general, any monadic predicate symbol of classical syntax can be used as a class expression.",6,7
2672,33175460,"For example, if like is a binary relation symbol and woman is a class symbol, then one can construct the class expressions (like (some woman)) and (like (every woman)).",9,10
2673,33175460,"For example, if like is a binary relation symbol and woman is a class symbol, then one can construct the class expressions (like (some woman)) and (like (every woman)).",15,16
2674,33175460,"In addition to these two, we have introduced the notion of a function symbol that we incorporated into the syntax and so as not to complicate the logic, we specified the result of applying a function to a class expression to be again a class expression.",14,15
2675,33175460,"10) (some C exists) (some C C) (11) (some C W ) (some C exists) (12) (some C W ) (some W C) (13) (every C W ),(every W Z) (every C Z) F ::= c a constant symbol | (some s t) | s a predicate symbol | (every s t) | (R(some s)) | (Q 3 * s t) | (R(every s)) | negation of F | x a variable symbol | Bool.",64,65
2676,33175460,"10) (some C exists) (some C C) (11) (some C W ) (some C exists) (12) (some C W ) (some W C) (13) (every C W ),(every W Z) (every C Z) F ::= c a constant symbol | (some s t) | s a predicate symbol | (every s t) | (R(some s)) | (Q 3 * s t) | (R(every s)) | negation of F | x a variable symbol | Bool.",75,76
2677,33175460,"10) (some C exists) (some C C) (11) (some C W ) (some C exists) (12) (some C W ) (some W C) (13) (every C W ),(every W Z) (every C Z) F ::= c a constant symbol | (some s t) | s a predicate symbol | (every s t) | (R(some s)) | (Q 3 * s t) | (R(every s)) | negation of F | x a variable symbol | Bool.",110,111
2678,6049209,"The format of the tags is a string of symbols (letters, digits or hyphens) where for each value there is one single symbol that denotes it.",25,26
2679,6049209,The first symbol is a capital letter denoting the POS category.,2,3
2680,6049209,"In case the hyphen or hyphens come last in the tag string, that is, no symbol follows them, they are omitted.",17,18
2681,234337605,Y(x) denotes the set of all possible label sequences given x. y 0 is defined to be a special start symbol.,21,22
2682,7327540,"Starting from the root, each state defines the next possible state whilst emitting (or consuming) a certain symbol.",20,21
2683,7327540,"The first rule is /N/ deletion, where the nasal symbol /^N/ is omitted if it is preceded by /me/ or /pe/ and followed by either of the phonemes /l/, /m/, /n/, /r/, /y/, /w/, /t/, /s/, /p/, or /k/. The next four rules are the phonetic stem changes, which apply to stem words preceded by the morpheme /me^N/ or /pe^N/, and whose initial phoneme is one of /t/, /p/, /r/, /s/, or /k/. More specifically, the phoneme /t/ will transform into /n/, /p/ will transform into /m/, /s/ will transform into /ny/, and finally the phoneme /k/ will transform into /ng/. All morphophonemic rules that implement the various processes are composed into one large rule, from which the morphophonemic transducer network can be constructed.",10,11
2684,1484762,"Using their approach, all words with a frequency less than a threshold τ are mapped to symbol rare 1 , and their emission probability P θ (w|t x ) is set in proportion to their co-occurrences with the surface POS tag: P θ (w|t x ) = c t,w w :c •,w <τ c t,w P θ (rare|t x ) 1 τ is tuned on the development set.",17,18
2685,46049123,"service :== serviceId | serviceBinding method :== symbol serviceId :== symbol serviceBinding :== ""bind("" serviceId bindingInfo+ "")"" bindingInfo :== "","" invocationId "":"" service invocationId :== symbol arg :== ""'"" symbol ""'"" symbol :== LETTER+ Using this language, we can describe the above service binding as below.",11,12
2686,46049123,"service :== serviceId | serviceBinding method :== symbol serviceId :== symbol serviceBinding :== ""bind("" serviceId bindingInfo+ "")"" bindingInfo :== "","" invocationId "":"" service invocationId :== symbol arg :== ""'"" symbol ""'"" symbol :== LETTER+ Using this language, we can describe the above service binding as below.",16,17
2687,46049123,"service :== serviceId | serviceBinding method :== symbol serviceId :== symbol serviceBinding :== ""bind("" serviceId bindingInfo+ "")"" bindingInfo :== "","" invocationId "":"" service invocationId :== symbol arg :== ""'"" symbol ""'"" symbol :== LETTER+ Using this language, we can describe the above service binding as below.",46,47
2688,46049123,"service :== serviceId | serviceBinding method :== symbol serviceId :== symbol serviceBinding :== ""bind("" serviceId bindingInfo+ "")"" bindingInfo :== "","" invocationId "":"" service invocationId :== symbol arg :== ""'"" symbol ""'"" symbol :== LETTER+ Using this language, we can describe the above service binding as below.",54,55
2689,46049123,"service :== serviceId | serviceBinding method :== symbol serviceId :== symbol serviceBinding :== ""bind("" serviceId bindingInfo+ "")"" bindingInfo :== "","" invocationId "":"" service invocationId :== symbol arg :== ""'"" symbol ""'"" symbol :== LETTER+ Using this language, we can describe the above service binding as below.",58,59
2690,8988829,"In particular, the hierarchical model (Chiang, 2007) studied in this paper explores hierarchical structures of natural language and utilize only a unified nonterminal symbol X in the grammar, X → γ, α, ∼ where ∼ is the one-to-one correspondence between X's in γ and α, and it can be indicated by underscripted co-indexes.",27,28
2691,8988829,"We then abstract the trees nodes with two symbol, X for phrases, and B for non-phrases, and call the result the decomposition tree of the source side phrases.",8,9
2692,8988829,"Inducing Latent Syntactic Categories If we designate a unique symbol S as the new root of the syntactic decomposition forests introduced in the previous section, it can be shown that these forests can be generated by a probabilistic contextfree grammar G = (V, Σ, S, R, φ), where • V = {S, X, B} is the set of nonterminals, • Σ is the set of terminals comprising treebank categories plus the CR tag (the crossing category), 7 The intermediate binarization nodes are also labeled as either X or B based on whether they exactly cover a phrase or not. •",9,10
2693,8988829,"S ∈ V is the unique start symbol, • R is the union of the set of production rules each rewriting a nonterminal to a sequence of nonterminals and the set of emission rules each generating a terminal from a nonterminal, • and φ assigns a probability score to each rule r ∈ R. Such a grammar can be derived from the set of syntactic decomposition forests extracted from a source-side parsed parallel corpus, with rule probability scores estimated as the relative frequencies of the production and emission rules.",7,8
2694,11566764,"For example, a simplified ToBI encoding scheme uses the symbol 4 for major intonational breaks, p for hesitation, and 1 for all other breaks (Dreyer and Shafran, 2007) .",10,11
2695,11566764,"Each word input into the parser has an associated break index represented by the symbol 1, 4, or p enclosed in asterisks indicating the break after the word.",14,15
2696,2183720,"To handle this problem, we map all words with frequency less than threshold 3 λ to symbol unk and for each latent tag accumulate the word tag statistics of these rare words to c r (a x , unk) = w:c(w)<λ c(a x , w).",17,18
2697,8074746,Searching a given tile (which includes a bracket symbol) in this tree yields the positive count for the tile.,9,10
2698,14332516,The first problem is mainly seen in symbol (abbreviated) types.,7,8
2699,14332516,A merge of each database entry was done using the 'official symbol' or ORF name and link data provided by each entry and the protein-sequence data entry.,12,13
2700,14332516,"For example, in H. sapiens, HUGO, Locuslink, GDB, and GenAtlas were registered in this order, using the merged entry for the same 'official symbol '.",30,31
2701,14332516,"LocusLink's 'preferred symbol', which is not yet administered by HUGO, was also used.",4,5
2702,14332516,"The corresponding 'official symbol' was searched using a partial match of registered names, and finally was checked manually.",4,5
2703,14332516,"6) For symbol-type names (less than seven characters), the initial of the organism is added to the spelt-out type.",3,4
2704,14332516,"Only when the symbol name has a symbol (abbreviation)-full name pairs, and the full name is not the corresponding gene name or contains a word that is not a component of the synonyms, the hit-ID is discarded.",3,4
2705,14332516,"Only when the symbol name has a symbol (abbreviation)-full name pairs, and the full name is not the corresponding gene name or contains a word that is not a component of the synonyms, the hit-ID is discarded.",7,8
2706,1206711,"The second approach is based on the predictability strategy, which assumes that speech should be segmented at locations where some measure of the uncertainty about the next symbol (phoneme or syllable for instance) is high (Harris, 1955; Gammon, 1969; Saffran et al.,",28,29
2707,5204434,"The context window size was set to C = 10 and if the length of a context extends beyond the sentence length, we used a padding symbol in-place of a word.",27,28
2708,62625908,Searching a given tile (which includes a bracket symbol) in this tree yields the positive count for the tile.,9,10
2709,298559,"Using the results of step 1, Vein expressions are computed top-down for each node in the tree, using the following functions: − mark (x), which returns each symbol in a string of symbols x marked with parentheses.",35,36
2710,1339781,"A symbol-level translation mapping T would then include a correspondence between the slots in the two languages, as in T (bites, muerde) .",1,2
2711,1339781,"As suggested by Edelman (1999) , this principle can be used to compute an optimal translation mapping between two disjoint symbol systems, such as the vocabularies of two languages, as long as they refer to same conceptual network, which constitutes the common underlying metric space (Goldstone and Rogosky, 2002) .",22,23
2712,1339781,"Specifically, we seek an optimal mapping T using as inter-symbol distances in the two domains -language (A) and language (B) -the probabilities of symbol co-occurrence, P (a j 1 , a j 2 ) and P (b k 1 , b k 2 ), where the symbols stand for the various constructions.",12,13
2713,1339781,"Specifically, we seek an optimal mapping T using as inter-symbol distances in the two domains -language (A) and language (B) -the probabilities of symbol co-occurrence, P (a j 1 , a j 2 ) and P (b k 1 , b k 2 ), where the symbols stand for the various constructions.",30,31
2714,1712903,"This is in line with Mann and Yarowsky (2003) 's modification, consisting in replacing all numbers in the patterns with the symbol ####. •",24,25
2715,17597829,"In this example, equivalence class #75 is not extended to subsume the subject position, because that position appears in a different context (e.g., immediately to the right of the symbol BEGIN).",34,35
2716,44159724,"From those, we use 20,000 of the most frequent words whose syllable counts are equal to or less than 10, and converted others to a special symbol unknown .",28,29
2717,44159724,"For each time step t, the model outputs a single word or boundary symbol taking a pair of the previously generated word w t−1 and the musical feature vector n t for the current word position which includes context window-based features that we describe in the following section.",14,15
2718,44159724,w 0 = B is a symbol denoting the beginning of lyrics.,6,7
2719,241583663,"In the figures shown below, a node symbol p corresponds to a premise and a symbol c to a conclusion.",8,9
2720,241583663,"In the figures shown below, a node symbol p corresponds to a premise and a symbol c to a conclusion.",16,17
2721,15861335,"In context-free grammars augmented with a unification formalism, packing based on the CF symbol equality has been complemented by subsumption-or disjunction-based packing of the associated feature structures (Moore and Alshawi, 1992; Maxwell and Kaplan, 1995) .",16,17
2722,5661541,"Following a typical practice in seq2seq, we use the top N d most frequent words in a dialog corpus as the seq2seq vocabulary (henceforth, V d ) and convert the other infrequent words into a spe-cial symbol UNK.",41,42
2723,233240929,"Scan the phrase from the beginning, and keep eliminating words until a word other than a symbol appears.",17,18
2724,23265360,"For a path sequence, we skipped a middle part of intermediate tokens and inserted a special symbol in the center of the sequence if the token length exceeded 15.",17,18
2725,248182536,GEN(x) introduces a terminal symbol x that is encoded as a vector onto the top of the stack.,5,6
2726,248182536,"This action generates a terminal symbol ""x"". •",5,6
2727,15976919,"If a context or a mention extends beyond the sentence length, a padding symbol is used in-place of a word.",14,15
2728,235742850,"For the reentrancies of the same head and dependent on different labels in the EUD graph, we combined these arcs into one and concatenate the labels of these arcs with a special symbol '+' representing the combination of two arcs.",33,34
2729,9118831,"We may write i as j H w x § 2w I , where w is a dummy symbol at the dummy string position 9 .",18,19
2730,9118831,"For the purposes of predicting output symbols, a series of consecu-tive target symbols and reorder operators following a source symbol in the training sentences are treated as a single symbol by the bigram model, and only those may be output after that source symbol.",22,23
2731,9118831,"For the purposes of predicting output symbols, a series of consecu-tive target symbols and reorder operators following a source symbol in the training sentences are treated as a single symbol by the bigram model, and only those may be output after that source symbol.",32,33
2732,9118831,"For the purposes of predicting output symbols, a series of consecu-tive target symbols and reorder operators following a source symbol in the training sentences are treated as a single symbol by the bigram model, and only those may be output after that source symbol.",47,48
2733,9118831,It also ensures that a bounded number of output symbols per input symbol are produced.,12,13
2734,9368851,"The simplest realization of a heuristic function, denoted as h T (j), takes into account only the translation probability p(f |e): h T (j) = max e p(f j |e) This heuristic function can be refined by introducing also the fertility probabilities (symbol F) of a target word e: h T F (j) = = max max e =e 0 ,φ p(f j |e) φ p(φ|e), p(f |e 0 ) Thereby, a coupling between the translation and fertility probabilities is achieved.",51,52
2735,236460049,"The gold annotation for Q is (k1 = 2×10−9tok1 = 1×10−9cm3s−1) and ME is average base reaction rate, where we return ""2×10"" as a Q with ""average"" as ME, ""1×10"" as Q with ""reaction"" as ME and ""9"" (split by symbol ""−"") as seperate quantities with ME as ""reaction"", incurring false positive errors.",55,56
2736,34147474,"Most importantly, most of the linguistic properties that must be considered for text processing are not emergent properties of the texts at all but crucially depend on l'arbitraire du signe, the arbitrary relation between a symbol and what it symbolizes.",37,38
2737,17679593,"The symbol ""+"" in the figure indicates that the preceding/subsequent word/POS tag is immediately adjacent to the erroneous position, while the symbol "">"" indicates that the preceding/subsequent word/POS tag is not immediately adjacent to the erroneous position.",1,2
2738,17679593,"The symbol ""+"" in the figure indicates that the preceding/subsequent word/POS tag is immediately adjacent to the erroneous position, while the symbol "">"" indicates that the preceding/subsequent word/POS tag is not immediately adjacent to the erroneous position.",28,29
2739,5516564,"7  The Loco C model assumes a random generation process (of an output derivation, given an input one) which begins with the starting symbol of the output grammar as the ""current sentential form"" and then, while the current sentential form contains a non-terminal, iteratively performs the following sequence of two random choices: in Choice 1, one of the rules in the input derivation is chosen; in Choice 2, the non-terminal in the current sentential form is rewritten using a randomly chosen rule of the output grammar.",27,28
2740,5739793,"We stop adding to the French multiword once we have found all the French words in the compounddefining set, or if we encounter a punctuation symbol, or if we encounter three or more consecutive words not in the set.",26,27
2741,227230370,"As data cleaning we first removed blank lines and redundant whitespaces, then replaced whitespace inside English and code-mixing parts with the hyphen symbol.",25,26
2742,2930536,"For example, when we want to exam-Figure 9 : Relationship between research organizations and research areas in journal papers (The name of each research organization is given a ""■"" symbol.)",35,36
2743,2865581,"We take care to mark which non-terminal is the foot, using a * symbol.",16,17
2744,2865581,"For example, the non-binary rule VP → V NP PP SBAR would be converted to the structure [VP [@VP [@VP V NP] PP] SBAR] where @VP is a new symbol in the grammar.",38,39
2745,218974440,UNK] symbol.,2,3
2746,21239563,"X>Y"" / ""X=>""Y""} means that the social position of the person corresponding to symbol X {is higher than / is higher than or equal to} that of the person corresponding to symbol Y. {""(X)(Y)""/ ""(X)(Y)(Z)""} means that {X and Y / X and Y and Z} are the out-group, {""(X,Y)"" / ""(X,Y,Z)""} means that {X and Y / X and Y and Z} are the in-group and there is no restriction as to the relative social position between them, where X/Y/Z are S (speaker)"", ""L (listener)"", ""A (person A)"", or ""B (person B)"" and are different.",19,20
2747,21239563,"X>Y"" / ""X=>""Y""} means that the social position of the person corresponding to symbol X {is higher than / is higher than or equal to} that of the person corresponding to symbol Y. {""(X)(Y)""/ ""(X)(Y)(Z)""} means that {X and Y / X and Y and Z} are the out-group, {""(X,Y)"" / ""(X,Y,Z)""} means that {X and Y / X and Y and Z} are the in-group and there is no restriction as to the relative social position between them, where X/Y/Z are S (speaker)"", ""L (listener)"", ""A (person A)"", or ""B (person B)"" and are different.",39,40
2748,15967245,"In particular, formally syntax-based models explore hierarchical structures of natural language and utilize only a unified nonterminal symbol X in the grammar, X → γ, α, ∼ , (1) where ∼ is the one-to-one correspondence between X's in γ and α, which is indicated by underscripted co-indices on both sides.",20,21
2749,15967245,"3) That is, X is also our sentence start symbol.",11,12
2750,15967245,"4 as: P (D) ∝ P LM (e) λ LM × X→<γ,α>∈D ( i φ i (X →< γ, α >) λ i )L(X →< γ, * >) λ L , (5) where L(•) is a feature function defined over a production but only depending on one side of the rules and asterisk denotes arbitrary symbol sequences on the other side consistent with our grammars 2 .",73,74
2751,15967245,"Model Syntactic Variations Each abstract rule is generalized from a set of original relevant phrase pairs by grouping an appropriate set of sub-phrases into a nonterminal symbol, with each sub-phrase linked to a tree list.",28,29
2752,15967245,"Therefore, the joined tree lists form a forest for this nonterminal symbol in the rule.",12,13
2753,39984436,"According to Markov stochastic process, the probability of symbol string S = W 1 W 2 • • • W n can be calculated by the initial probability distribution and the transfer probability as follows: P(S) = P(W 1 ) • (P(W k |W k−1 k−n+1 )) (2) where P(W 1 ) can be considered as an initial probability distribution and P(W k |W k−1 k−n+1 ) can be re- garded as a state transition probability.",9,10
2754,227230497,"According to Shannon's coding theorem, the optimal code length for a symbol s is − log p s , where p s is the occurrence probability of s."" From observation, we hypothesized a relationship between the number of segments and the likelihood of a subword sequence; therefore, we evaluated the bilingual subword segmentation method that selects subword sequence pairs on the basis of the likelihood obtained by the unigram language model rather than on the number of segments in the sentence.",13,14
2755,51872303,"Compared with other treelinearization methods, our method combines several different kinds of information within one symbol, retaining the parent-child information, and incorporating the confidence of the parser in the sequence.",16,17
2756,51872303,"First, the input sequence of the encoder consists of two parts: the symbol sequence and the score sequence.",14,15
2757,51872303,"Second, each symbol in the symbol sequence consists of several parts (words and constituent labels), which are combined by certain operators ( c , ⊗, ⊕, or ).",3,4
2758,51872303,"Second, each symbol in the symbol sequence consists of several parts (words and constituent labels), which are combined by certain operators ( c , ⊗, ⊕, or ).",6,7
2759,51872303,"l T , ξ T ), where l i denotes the i-th symbol and ξ i its score.",15,16
2760,51872303,"Then, the sequence is fed into the score layer and the symbol layer.",12,13
2761,51872303,"The score and symbol layers receive the sequence and output the score sequence ξ = (ξ 0 , . . . ,",3,4
2762,51872303,"ξ T ) and symbol sequence l = (l 0 , . . . ,",4,5
2763,51872303,Any item l ∈ l in the symbol layer has the form l = o 0 x 1 o 1 . . .,7,8
2764,51872303,"m) is a word or a constituent label, m is the total number of words and constituent labels in a symbol, o 0 is "" c "" or empty, and each o k (k = 1, . . . ,",22,23
2765,51872303,"For the ""No score"" configurations, we force the input score sequence to be a sequence of 1.0 with the same length as the input symbol sequence, so that neither the embedding layer nor the attention layer are affected by the score sequence.",27,28
2766,227231569,Note that in BPE segmentation there is no explicit word boundary and a symbol may form either of the beginning/inside/end/whole of a word.,13,14
2767,227231569,"For example, we plot the values for 12 primary POS categories -NN, NNP, NNS, JJ, PRP, CC, RB, VB, VBD, VBN, VBZ, SYM (noun, proper noun, plural noun, adjective, pronoun, conjunction, adverb, base verb, verb past form, verb past participle, verb third person singular and symbol).",68,69
2768,2724633,"For NTCIR-10 and ASPEC, we replaced words with frequencies less than 3 with the [UNK] symbol and excluded them from the vocabularies.",18,19
2769,2724633,The beam search was terminated when an end-ofsentence [EOS] symbol was generated.,13,14
2770,2529916,A new agent can be added to the system by introducing one or more axiom that advertise its capabilities and introducing a link between a symbol in the theory and the agent.,25,26
2771,2529916,It should be understood that a function symbol such as feature does not stand for a program that can be computed; it is a notation for speaking about a place.,7,8
2772,2529916,"For instance, canada is a symbol that denotes the actual country Canada, a region on Earth.",6,7
2773,2529916,"For latitudes and longitudes represented in compass notation, there is a function symbol lat-long-compass( ?",13,14
2774,2529916,"Similarly, for the signed notation, there is the function symbol lat-long-sign-string( ?",11,12
2775,2529916,The Procedural Attachment Mechanism The procedural attachment mechanism allows an agent that is attached to a symbol in the theory to be executed while the proof is in progress.,16,17
2776,2529916,"Assume that the symbol plus is attached to an agent, an ordinary program, that performs numerical addition.",3,4
2777,2529916,"The Alexandria Digital Library gazetteer is used for several purposes, and the gazetteer is procedurally attached to more than one symbol in the theory.",21,22
2778,2529916,"For instance, the symbol place-to-lat-long invokes the gazetteer simply to find the bounding box corresponding to some place whose name is ?",4,5
2779,10756596,Replacing numbers with a special symbol We replaced each occurrence of numbers in the corpus with a special symbol and trained an SMT system.,5,6
2780,10756596,Replacing numbers with a special symbol We replaced each occurrence of numbers in the corpus with a special symbol and trained an SMT system.,18,19
2781,10756596,"That is, all numbers in the corpus were replaced with the same special symbol.",14,15
2782,10756596,"If an input sentence for the SMT system had a num-ber, the number was replaced with the special symbol before inputing it to the SMT system.",21,22
2783,10756596,The special symbol in the output sentence was replaced with the translation of the number in the input sentence.,2,3
2784,1695909,"A full phoneme set contains 33 phoneme symbols in total, which consists of 10 vowels (including diphthongs), 22 consonants, and one silent symbol It consists of vowels, i.e., /a/ (like ""a"" in ""father""), /i/ (like ""ee"" in ""screen""), /u/ (like ""oo"" in ""soon""), /e/ (like ""e"" in ""bed""), /e2/ (a schwa sound, like ""e"" in ""learn""), /o/ (like ""o"" in ""boss""), and four diphthongs, /ay/, /aw/, /oy/ and /ey/. The articulatory pattern for Indonesian consonants can be seen in Table 1 .",27,28
2785,282739,"This is, of course, a serious weakness for use in text processing, where one wants at least to work with some subset of Unicode, or even worse, to treat each tokenized word as an alphabet symbol.",40,41
2786,203593443,"Our dictionary contains words that are common in all of the three corpora, while the rest of the words that don't exist in this dictionary are replaced with 〈unk〉 symbol.",33,34
2787,203593443,"Intuitively, seeing more evidence of this symbol during training would improve our estimate for the 〈unk〉.",7,8
2788,203593443,"In contrast, the effect the 〈unk〉 symbol has on the corpora generated with the NS(p=0.9) decoding scheme is minimal for two reasons: First, the vocabulary size for the generated corpora, for all values of C is close to the original corpus (the corpus we used to train the C -VAE LSTM ).",9,10
2789,203593443,"As a result, minimum replacement of the words with the 〈unk〉 symbol is required, making the experiment to be more reflective of the quality of the generated text.",14,15
2790,13573743,"Written in Java, it takes two input files, one as a collection of PSG rules and the other containing a poetic line where each component character is tagged with a partof-speech symbol.",35,36
2791,14634517,"A prefix tree, also called trie, is an ordered tree data structure used to store an associative array where the keys are symbol sequences.",24,25
2792,1479529,"Range SA[sp j−1 , ep j−1 ] matching α[j − 1, m − 1] with c def = α[j − 1] can be expressed as sp j−1 = C[c] + RANK(BWT, sp j , c) ep j−1 = C[c + 1] + RANK(BWT, ep j + 1, c) − 1 where C[c] refers to the starting position of all suffixes prefixed by c in SA and RANK(BWT, sp j , c) determines the number of occurrences of symbol c in BWT[0, sp j ].",90,91
2793,1479529,Recall that BWT [i]   corresponds to the symbol preceding the suffix start- ing at SA[i].,10,11
2794,1479529,"This method enumerates the children of the node (line 3) and calculates either the frequency of each child (line 7) or the modified count N 1+ ( • α x), for each child u where x is the first symbol on the edge vu (line 5).",46,47
2795,1479529,"This check could be done by repeatedly calling edge(v, k) to find the k th symbol on the given edge to check for sentinels, however this is a slow operation as it requires multiple backward search calls.",17,18
2796,6013945,"Therefore an alternative approach is adopted: every untyped symbol is followed by a symbol type indicator, which specifies the type of its predecessor.",9,10
2797,6013945,"Therefore an alternative approach is adopted: every untyped symbol is followed by a symbol type indicator, which specifies the type of its predecessor.",14,15
2798,6013945,"At a minimum, there need to be two symbol type indicators: one for input symbols and one for output symbols.",9,10
2799,6013945,"The particular choice of symbols used here is arbitrary and can easily be changed, Extended alphabet symbols thus consist of a sequence of two symbols, where the second symbol determines the type of the first symbol.",30,31
2800,6013945,"The particular choice of symbols used here is arbitrary and can easily be changed, Extended alphabet symbols thus consist of a sequence of two symbols, where the second symbol determines the type of the first symbol.",37,38
2801,6013945,"With the preceding conventions, for example, the sequence [z a] (= [z input]) represents 'z' as an input symbol, and [< x] (= [< marker]) represents the angle bracket '<' used as a marker symbol.",27,28
2802,6013945,"With the preceding conventions, for example, the sequence [z a] (= [z input]) represents 'z' as an input symbol, and [< x] (= [< marker]) represents the angle bracket '<' used as a marker symbol.",51,52
2803,6013945,"Generally, these tools are designed to hide the symbol type indicators from the user.",9,10
2804,6013945,"For general use, one should replace this with Σ, which is the Foma symbol for an open alphabet.",15,16
2805,6013945,This is composed with a transducer that eliminates the symbol type indicators. •,9,10
2806,6013945,"5 What is needed is a graphical interface that shows the types of symbols in a concise way without using sequences of two symbols, and without using the symbol type indicator.",29,30
2807,6013945,"In all cases, the symbol type indicator is removed.",5,6
2808,6013945,So 'a%:' is the two-character symbol consisting of an 'a' followed by a colon.,10,11
2809,6013945,Assertions and Boolean Tests An important invariant of the typed approach is that every sequence is of even length and every symbol in an even position is a symbol type indicator.,21,22
2810,6013945,Assertions and Boolean Tests An important invariant of the typed approach is that every sequence is of even length and every symbol in an even position is a symbol type indicator.,28,29
2811,6013945,"Here, an input symbol [Σ, input] = [Σ, a] is ""marked up"" as [Σ, identity] = [Σ, c].",4,5
2812,10014954,"In these tables, the symbol ""Ó bÓ "" indicates that the difference in Ã between the two systems is statistically significant at the 1% level, based on ""number Ô Õ W cAE Ç WÈ y gÉ GÊ 3Ë ¦Ì W is the probability that a randomly chosen pair of words a distance of Ö words apart is inconsistently classified; that is, for one of the segmentations the pair lies in the same segment, while for the other the pair spans a segment boundary"" (Beeferman et al.,",5,6
2813,225428,"The suffix tree (Weiner, 1973) of T is the compact labeled tree of n + 1 leaves where the root to leaf paths correspond to all suffixes of T $, where $ is a terminating symbol not in Σ. The path-label of each node v corresponds to the concatenation of edge labels from the root node to v. The node depth of v corresponds to the number of ancestors in the tree, whereas the string depth corresponds to the length of the path-label.",39,40
2814,225428,"Here, RANK(T bwt , i, c) counts the number of times symbol c occurs in T bwt [0 . . .",14,15
2815,225428,"Thus, we compute RANK(T bwt , l i , c), the number of times symbol c occurs before l i and RANK(T bwt , r i + 1, c), the number of occurrences of c in T bwt [0, r i ].",17,18
2816,225428,"To determine SA[l i−1 , r i−1 ], we additionally store the starting positions C s of all suffixes for each symbol s in Σ at a negligible cost of σ log n bits.",22,23
2817,225428,"2 If locations of matches are required, ad-2 However, if code-words for each symbol are chosen based on their Huffman-codes the size of the wavelet tree ditional space is needed to access SA[i] or the inverse suffix array SA −1 [SA[i]] = i. In the simplest scheme, both values are periodically sampled using a given sample rate SAS (e.g. 32) such that SA[i] mod SAS = 0.",17,18
2818,225428,"Algorithm 1 shows how this is computed, with lines 7 and 8 enumerating s ∈ F (α) using the edge labels of the children of v. For each symbol, line 9 searches for an extended pattern incorporating the new symbol s in the reverse CSA (part of the reverse CST), by refining the existing match v R using a single backward search operation after which we can compute N 1+ ( • αs).",31,32
2819,225428,"Algorithm 1 shows how this is computed, with lines 7 and 8 enumerating s ∈ F (α) using the edge labels of the children of v. For each symbol, line 9 searches for an extended pattern incorporating the new symbol s in the reverse CSA (part of the reverse CST), by refining the existing match v R using a single backward search operation after which we can compute N 1+ ( • αs).",43,44
2820,225428,"Calling string-depth is constant time for internal nodes, but O(SAS log σ) for leaf nodes; fortunately we 7 Backward search in the reverse tree corresponds to searching for the reversed pattern appended with one symbol.",39,40
2821,225428,"Equivalently, the iteration can be considered in reverse, starting from unigram estimates and successively growing to large mgrams, in each stage adding a single new symbol to left of the pattern.",28,29
2822,226262326,sop is the special startof-piece symbol. ',7,8
2823,226262326,"In each iteration, we take the output sequence from the last iteration and mask a subset of tokens with low confidence scores by a special mask symbol.",27,28
2824,226262326,Then this size of tokens are randomly picked from the target sequence and replaced with the mask symbol.,17,18
2825,19806003,Each sentence pair is also given a field symbol.,8,9
2826,19806003,The field symbol is a single letter of A-Z and show the scientific field for each document 4 where the sentence pair is extracted.,2,3
2827,13342457,If a surface form template was specified as ِ r F 2 r M 0َ r L 2 (AiF2t~aM0aL2Y) and it was to be combined with the affix rule r L 2 ْ ُ َ (L2yotumaA) then SG simply needs to align the affix rule with the surface form template using the place holder symbol in the affix rule and replace appropriately as in Table 1 .,60,61
2828,3006124,"If a word never occurs or only occurs once in training corpus, we replace it with a special symbol <unk>.",19,20
2829,62202373,The symbol s in the figure represents the sample size and k represents the SVD dimension.,1,2
2830,3138536,"However, as mentioned before because IPA requires special fonts, which are not readily available for a few of the sounds, we have used an ASCII symbol that resembled the relevant IPA symbol.",28,29
2831,3138536,"However, as mentioned before because IPA requires special fonts, which are not readily available for a few of the sounds, we have used an ASCII symbol that resembled the relevant IPA symbol.",34,35
2832,3138536,"In order to assign a symbol to each letter of the alphabet, the corresponding letter representing the sound of that letter was chosen.",5,6
2833,3138536,"In order to ensure a one-to-one representation between the orthography and USCPers, these letters were each assigned a symbol, as presented on Table7.",23,24
2834,3138536,"USCPers, therefore, provides us with a way to capture each letter of the alphabet with one and only one ASCII symbol, creating a comparable system to USCPron for the orthography.",22,23
2835,203693213,"In early modern times, when encryption became frequently used in Europe, ciphers were typically based on transposition, where the plaintext characters are reordered in a systematic way, or substitution of plaintext characters to transform each character in the plaintext to another symbol from existing alphabets, digits, special symbols, or a mixture of these (Bauer, 2007) .",45,46
2836,203693213,"Ciphertexts contain symbol sequences with spaces, or without any space to hide word boundaries.",2,3
2837,203693213,"Each type of entity to be encrypted might be encoded by one symbol only (unigragh), two symbols (digraph), three symbols (trigraph), and so on.",12,13
2838,203693213,"Each letter in the alphabet has at least one corresponding ciphertext symbol, represented as a two-digit number (digraph), and the vowels and double consonants have one additional graphical sign (unigraph) .",11,12
2839,203693213,"The transcription is carried out symbol by symbol and row by row keeping line breaks, spaces, punctuation marks, dots, underlined symbols, and cleartext words, phrases, sentences, paragraphs, as shown in the original image.",5,6
2840,203693213,"The transcription is carried out symbol by symbol and row by row keeping line breaks, spaces, punctuation marks, dots, underlined symbols, and cleartext words, phrases, sentences, paragraphs, as shown in the original image.",7,8
2841,19004933,"The symbol ← is used in place of assignments, while → denotes unification (as in languages such as Prolog).",1,2
2842,33700538,"One is based on the universal PoS tagset, 10 which consists of 17 main PoS categories: adjective, adposition, adverb, auxiliary, coordinating conjunction, determiner, interjection, noun, numeral, particle, pronoun, proper noun, punctuation, subordinating conjunction, symbol, verb and others with their morphological features.",49,50
2843,17703143,"In this example, the symbol represents empty alignments, meaning insertions or deletions.",5,6
2844,17703143,The symbol in the source word meðr denotes the insertion of u in the target word meður.,1,2
2845,17703143,"Likewise, the symbol in the target word galda denotes the deletion of i as compared to the source word giallda.",3,4
2846,248406169,Every pattern is denoted as the right side symbol p i .,8,9
2847,335939,"Characters are classified according radicals, and their meanings cluster around the basic concept of the semantic symbol.",17,18
2848,335939,Our analysis and comparative studies of the semantic symbol ontologies for the four hoofed-mammals show that they share similar conceptual structures strongly motivated by their functions in human society.,8,9
2849,335939,"2.1.Radical Search There are two searching methods for the semantic symbol ontology: (i) Search on SUMO concepts classification Choose certain SUMO concept, then this concept and its lower SUMO concept will show up on the interface. (",10,11
2850,335939,"Figure2: The classification of Hanzi semantic symbols The link of related semantic symbol Under ""telic"" and ""participating,"" we add a column for ""related semantic symbol"" to show and link the related deriving concepts.",13,14
2851,335939,"Figure2: The classification of Hanzi semantic symbols The link of related semantic symbol Under ""telic"" and ""participating,"" we add a column for ""related semantic symbol"" to show and link the related deriving concepts.",31,32
2852,335939,"Our ontology system links "" 羌""with its related semantic symbol ""人"" to offer cross-referencing in oreder to build a more realistic ontology of the conceptual convention.",9,10
2853,195325865,"The ""Intra-Sentential Attention"" layer only allows to attend to the previous tokens in the current sentence, i.e. the intra-sentential attention mask activates the tokens between the most recent sentence boundary marker and the current symbol.",41,42
2854,14649268,The symbol * represents the elementwise multiplication.,1,2
2855,2104026,"The semanticphonetic (xing2sheng1) characters, representing over 90 percent of Chinese character, are formed by combining a semantic symbol and a phonetic symbol. (",21,22
2856,2104026,"The semanticphonetic (xing2sheng1) characters, representing over 90 percent of Chinese character, are formed by combining a semantic symbol and a phonetic symbol. (",25,26
2857,2442180,The proposed approach removes the need to replace rare words with the unknown word symbol.,14,15
2858,14952042,PoS tags) and preserves the arc-level alignment between each input and output symbol (eg.,15,16
2859,14952042,"Graphically, in our running example A r is: 1 2 3 a/[(0, 2), (1, 1)] b/[(0, 1), (3, 1)] c/[(0, 5), (1, −1), (2, 1), (5, 1)] and the output is B r : 0, 1), (3, 1)] a/[(0, 2), (1, 1)] (1, 0) (2, 0) (3, 0) (2, [(1, −1), (2, 1)]) b/[( c/[(0, 5), (5, 1)] a/[(0, 2), (2, 1)] Reversing B r yields an acceptor B (still in the sparse tuple vector semiring) which has the same topology as our goal T and can be trivially mapped to T in linear time: each arc takes the tropical weight via ᾱ and has only one topological feature which points to the arc in T containing the required output symbol.",205,206
2860,15727309,"Typically this is done by adding a stack alphabet and labeling each transition with a stack operation (a stack symbol to be pushed onto, popped, or read from the stack) in addition to the usual input and output labels (Aho and Ullman 1972; Berstel 1979 ) and weight (Kuich and Salomaa 1986; Petre and Salomaa 2009) .",20,21
2861,15727309,"Our equivalent representation allows a transition to be labeled by a stack operation or a regular input/output symbol, but not both.",19,20
2862,15727309,Stack operations are represented by pairs of open and close parentheses (pushing a symbol on and popping it from the stack).,14,15
2863,15727309,"SHORTESTDISTANCE(T) for each q ∈ Q and a ∈ Π do B[q, a] ← ∅ for each q ∈ Q do d[q, q] ← ∞ GETDISTANCE(T, I) ⊲ I is the unique initial state return d[I, f ] ⊲ f is the unique final state RELAX(s, q, w, S ) if d[s, q] > w then ⊲ if w is a better estimate of the distance from s to q d[s, q] ← w ⊲ update d[s, q] if q ∈ S then ⊲ enqueue q in S if needed ENQUEUE(S, q) GETDISTANCE(T,s) for each q ∈ Q do d[s, q] ← ∞ d[s, s] ← 0 S s ← {s} while S s = ∅ do q ← HEAD(S s ) DEQUEUE(S s ) for each e ∈ E[q] do ⊲ E(q) is the set of transitions leaving state q if i[e] ∈ Σ ∪ {ǫ} then ⊲ i[e] is a regular symbol RELAX(s, n[e], d[s, q] + w[e], S s ) elseif i[e] ∈ Π then ⊲ i[e] is a close parenthesis B[s, i[e]] ← B[s, i[e]] ∪ {e} elseif i[e] ∈ Π then ⊲ i[e] is an open parenthesis if d[n[e], n[e]] = ∞ then ⊲ n[e] is the destination state of transition e GETDISTANCE(T, n[e]) for each e ′ ∈ B[n[e], i[e]] do w ← d[s, q] + w[e] + d[n[e], p[e ′ ]] + w[e ′ ] RELAX(s, n[e ′ ], w, S s ) Figure 9 PDT shortest distance algorithm.",187,188
2864,15727309,e 1 has symbol i[e 1 ] = ( 1 and destination state n[e 1 ] = 5 5.,3,4
2865,15727309,Dyer (2010a) presents a more practical intersection algorithm that avoids creating rules that are inaccessible from the start symbol.,20,21
2866,9222739,"Typ- ically this is done by adding a stack alphabet and labeling each transition with a stack operation (a stack symbol to be pushed onto, popped or read from the stack) in additon to the usual input label (Aho and Ullman, 1972; Berstel, 1979) and weight (Kuich and Salomaa, 1986; Petre and Salomaa, 2009) .",21,22
2867,9222739,Our equivalent representation allows a transition to be labeled by a stack operation or a regular input symbol but not both.,17,18
2868,9222739,Stack operations are represented by pairs of open and close parentheses (pushing a symbol on and popping it from the stack).,14,15
2869,2613012,"Each cell in the CYK grid is specified by a nonterminal symbol and position in the CYK grid: (N, x, y), which spans s x+y−1 x on the source sentence.",11,12
2870,2613012,"L(N, x, y, r, i) = A(α i ) if α i ∈ T L(N , x , y ) otherwise (2) where A(t), t ∈ T returns a single-arc acceptor which accepts only the symbol t. The lattice L(N, x, y) is then built as the union of lattices corresponding to the rules in R(N, x, y): L(N, x, y) = r∈R(N,x,y) L(N, x, y, r) ( 3 ) Lattice union and concatenation are performed using the ⊕ and ⊗ WFST operations, respectively, as described by Allauzen et al. (",46,47
2871,2613012,"For alignment, Equations ( 1 ) and ( 2 ) are redefined as L(N, x, y, r) = A T (r, ) i=1..|α r | L(N, x, y, r, i) ( 5 ) L(N, x, y, r, i) = A T ( , α i ) if α i ∈ T L(N , x , y ) otherwise (6) where A T (r, t), R r ∈ R, t ∈ T returns a single-arc transducer which accepts the symbol r in the input language (rule indices) and the symbol t in the output language (target words).",106,107
2872,2613012,"For alignment, Equations ( 1 ) and ( 2 ) are redefined as L(N, x, y, r) = A T (r, ) i=1..|α r | L(N, x, y, r, i) ( 5 ) L(N, x, y, r, i) = A T ( , α i ) if α i ∈ T L(N , x , y ) otherwise (6) where A T (r, t), R r ∈ R, t ∈ T returns a single-arc transducer which accepts the symbol r in the input language (rule indices) and the symbol t in the output language (target words).",118,119
2873,14667871,"The rule type can be obtained by replacing every sequence of terminals by a single symbol 'w', thus ignoring the identity of the words, but capturing its generalized structure and the kind of reordering it encodes (this was defined as rule pattern in Iglesias et al. (",15,16
2874,15442819,"Each cell in the CYK grid is specified by a non-terminal symbol and position: (N, x, y), spanning s x+y−1 x on the source sentence s 1 ...s J .",13,14
2875,13987185,"For example, in counting paths that contain u 1 , Ψ L n retains the first occurrence of u 1 and maps every other symbol to ǫ.",25,26
2876,13987185,Multiple counting is avoided by counting only the last occurrence of each symbol u on a path.,12,13
2877,13987185,The modification simply requires keeping track of which symbol u is encountered along each path to a final state.,8,9
2878,13987185,"We take the view that the full complexity of that approach is not needed here, since only one symbol is introduced per path and per exit state.",19,20
2879,13987185,"The forward algorithm does not readily yield the per-symbol probabilities, although an arc weight vector indexed by symbols could be used to correctly aggregate the required statistics (Riley et al.,",10,11
2880,13987185,It is easier to count paths by the final occurrence of a symbol than by the first.,12,13
2881,13987185,"For machine translation lattices, conflating the values of p(u|E) and c(u|E) for higherorder n-grams might not be a serious problem, but in other scenarios -especially where symbol sequences are repeated multiple times on the same path -it may be a poor approximation.",32,33
2882,11411390,"For each feature we give, apart from its identifier, the languages it is appropriate for and the full name of its attribute, while its value is encoded as the content of the feature, as a symbol with the full name of its value, e.g., <f id=""N4.a"" se-lect=""cs hu sl"" name=""Case""><sym value=""accusative""/> In the corpus, both libraries are stored in a dedicated corpus element, together with the TEI header.",39,40
2883,62276728,"Each cell in the CYK grid is specified by a non-terminal symbol and position in the CYK grid: (N, x, y), which spans s x+y−1 x on the source sentence.",13,14
2884,62276728,"Taken together, L(N, x, y, r) = i=1..|α r | L(N, x, y, r, i) (1) L(N, x, y, r, i) = A(α i ) if α i ∈ T L(N ′ , x ′ , y ′ ) else (2) where A(t), t ∈ T returns a single-arc acceptor which accepts only the symbol t. The lattice L(N, x, y) is then built as the union of lattices corresponding to the rules in R(N, x, y): L(N, x, y) = r∈R(N,x,y) L(N, x, y, r) (3) Lattice union and concatenation are performed using the ⊕ and ⊗ WFST operations respectively, as described by Allauzen et al.(2007) .",79,80
2885,2870777,"Given a rule set, we define source patterns and target patterns by replacing every sequence of non-terminals by a single symbol 'w' (indicating word, i.e. terminal string, w ∈ T + ).",23,24
2886,209063939,"The encoder maps an input sequence of symbol representations (i.e., a source sentence) X = (x 1 , x 2 , . . . ,",7,8
2887,209063939,"Figure 4 shows an example of subword-level dependency relations, where ""@@"" is a subword segmentation symbol. """,20,21
2888,16633800,"Of note, we separate the preceding symbol in mentions and hashtags (the @ or # characters) as a distinct token, but still include this in entity spans.",7,8
2889,1435098,"The symbol N (e) denotes the unigram count of a word e and N (f, e) denotes the count of the event that the target language word e is aligned to the source language word f .",1,2
2890,1435098,The $ symbol is the sentence boundary marker.,2,3
2891,9970925,"and What we did by using this kind of knowledge was to pre-process the text by replacing each piece of English text with a symbol ""E"" and replacing every Arabic number with another symbol ""N"".",26,27
2892,9970925,"and What we did by using this kind of knowledge was to pre-process the text by replacing each piece of English text with a symbol ""E"" and replacing every Arabic number with another symbol ""N"".",37,38
2893,8119798,"Word boundaries are represented by the symbol #, utterance boundaries by $, following Brent (1999a) .",6,7
2894,234777742,"Often highly visual, this technique often utilizes symbols (for example, the swastikas used in Nazi Germany, originally a symbol for health and prosperity) superimposed over other visual images.",22,23
2895,234777742,"Figure 27 shows an example, where the Transfer technique makes use of a communist symbol (namely, hammer and sickle) on top of the pictures of two targeted politicians, with the aim of depicting them in a negative way.",15,16
2896,234777742,"The technique is further reinforced by the use of the red color (which is also a symbol of Communism), and by the two instances of Name Calling (""Moscow Mitch"" and ""Moscow's bitch""), which make a connection to Moscow (which in turn was the capital of the former Communist block).",17,18
2897,201706782,We inserted a special separator symbol between each pair of spliced sentences both on the source and the target side.,5,6
2898,201706782,"Finally, in the (L) mode we additionally add a special symbol to represent a line break (in rare cases, two breaks) within a subtitle.",13,14
2899,237581474,"Hard Assignment This approach aligns each output symbol y k to the input that has the highest attention weight, i.e., to x t k , where t k = arg max t α t,k .",7,8
2900,204904179,The decoder continuously generates symbols until the end-of-sentence symbol is produced.,12,13
2901,204904179,"First, the sentences must start with start-of-sentence symbol and end-of-sentence symbol.",12,13
2902,204904179,"First, the sentences must start with start-of-sentence symbol and end-of-sentence symbol.",19,20
2903,204904179,"Translation occurs primarily at the word level, and the system uses the character-level model when an unknown symbol is predicted. [",20,21
2904,246904989,The decoder continuously generates symbols until the end-of-sentence symbol is produced.,12,13
2905,246904989,"First, the sentences must start with start-of-sentence symbol and end-of-sentence symbol.",12,13
2906,246904989,"First, the sentences must start with start-of-sentence symbol and end-of-sentence symbol.",19,20
2907,246904989,"Translation occurs primarily at the word level, and the system uses the character-level model when an unknown symbol is predicted. [",20,21
2908,226283976,In (8) a question is asked about a number format and the information about the specific last symbol is unlikely to be a part of a KG. (,18,19
2909,236087687,"The subtitle block remains on screen until the next <eob> symbol is generated, therefore the first subtitle is substituted by the next one.",12,13
2910,219177513,"The symbol <eol> corresponds to a line break inside a subtitle block, while the symbol <eob> to a subtitle block break (the next subtitle comes on a different screen), as seen in the following example from the MuST-Cinema test set: This kind of harassment keeps women <eol> from accessing the internet -<eob> essentially, knowledge.",1,2
2911,219177513,"The symbol <eol> corresponds to a line break inside a subtitle block, while the symbol <eob> to a subtitle block break (the next subtitle comes on a different screen), as seen in the following example from the MuST-Cinema test set: This kind of harassment keeps women <eol> from accessing the internet -<eob> essentially, knowledge.",17,18
2912,219177513,"For BLEU, an incorrect break symbol would account for an extra n-gram error in the score computation, while BLEU-nob allows us to evaluate only the translation quality without taking into account the subtitle segmentation.",6,7
2913,219177513,One question quickly arising is how the system can determine whether to insert a subtitle break symbol <eob> (which means that the next subtitle will follow on a new screen) or a line break symbol <eol> (which means that the next line of the subtitle will appear on the same screen).,16,17
2914,219177513,One question quickly arising is how the system can determine whether to insert a subtitle break symbol <eob> (which means that the next subtitle will follow on a new screen) or a line break symbol <eol> (which means that the next line of the subtitle will appear on the same screen).,38,39
2915,219177513,"Coming back to the example in Section 3.1, depending on the choice of break symbols (except for the last symbol which should always be an <eob>), there are two possible renderings of the subtitle: 10 00:00:31,066 --> 00:00:34,390 This kind of harassment keeps women from accessing the internet --11 00:00:34,414 --> 00:00:36,191 essentially, knowledge.",21,22
2916,219177513,"In this case, how can the MT system determine which type of break symbol to insert?",14,15
2917,219177513,"Therefore, we expect that the end of a subtitle block, which in our setting is signalled by the break symbol <eob>, should correspond to the end of a speech act, a pause or a terminal juncture.",21,22
2918,219177513,"On the other hand, line breaks inside the same subtitle block, which in our work correspond to the break symbol <eol>, have a different role.",21,22
2919,219177513,"If the hypothesis above holds, the choice of whether to insert an <eob> or an <eol> symbol is defined by prosodic properties and not solely by reaching the maximum length of 42 characters.",21,22
2920,219177513,"To achieve this, we perform forced alignment of the transcript against the audio and subtract the end time of each word from the start time of the next word: pause w1w2 = start time w2 − end time w1 (1) Then we separate the pauses in 3 groups: i) pauses corresponding to positions where <eob> is present, ii) pauses corresponding to positions where <eol> is present and iii) pauses after which there is no break symbol (None).",88,89
2921,219177513,For French the accuracy is 89% for the Cascade and 93% for the E2E. For German the accuracy is 85% for the Cascade and 88% for the E2E. This difference in accuracy suggests that the E2E is aided by the acoustic information and specifically by the pause duration in determining the correct break symbol.,57,58
2922,219177513,"The E2E inserts the first break symbol in the same position as in the source and reference (entreprise), while the Cascade inserts it at a later position (expliquer), resulting in a subtitle of 46 characters, which is above the 42-character length limit.",6,7
2923,211296855,We replaced the double spaces with the <eol> symbol.,10,11
2924,211296855,The top portions in the figure show the number of sentences where the <eol> symbol is present.,16,17
2925,211296855,"In this work, instead of attempting to recreate MuST-C from the Amara website (which would require a new pipeline and weeks for downloading and processing the dumps), we re-annotate the existing data with a model that learns to insert the <eol> symbol (Section 6.).",51,52
2926,222291004,We drop the symbol m that denotes a particular training example for simplicity.,3,4
2927,236486221,⋅ The † symbol indicates an end-to-end submission exploiting pre-trained models (not all parameters are jointly trained).,3,4
2928,236486221,⋅ The † symbol indicates an end-to-end submission exploiting pre-trained models (not all parameters are jointly trained).,3,4
2929,15477873,These formalisms allow various kinds of agreement by generalizing the notion of a grammar symbol to include features and variables.,14,15
2930,15477873,"We demonstrate the feasibility of a completely symbol-based approach to speech processing by achieving the same performance with layers of stochastic regular grammars as our best FSA-based system (Picone et al, 1988) .",7,8
2931,15477873,"The layers allow expansion of more than one symbol in a rule as in CFGs, but without recursive ability.",8,9
2932,15477873,"In stochastic chart parsing, the same symbol may be needed for several different explanations of the speech signal, but only the most likely representative actually becomes hypothesized (tl).",7,8
2933,15477873,"This leads to a situation where a lower probability explanation of the symbol may not only survive where it otherwise would have been pruned (tj), but the subsequent hypotheses using this symbol may actually give the more probable explanation (tz).",12,13
2934,15477873,"This leads to a situation where a lower probability explanation of the symbol may not only survive where it otherwise would have been pruned (tj), but the subsequent hypotheses using this symbol may actually give the more probable explanation (tz).",34,35
2935,15477873,"Furthermore, since the chart parser expands only the most likely symbol, the less likely hypotheses cause no additional computation during evaluation of the symbol.",11,12
2936,15477873,"Furthermore, since the chart parser expands only the most likely symbol, the less likely hypotheses cause no additional computation during evaluation of the symbol.",25,26
2937,15477873,"Although multiple hypotheses of the same symbol at the same time occur in the HMM grammar, the hypotheses at this level correspond to reference vectors of one frame in duration and the FSA-based system evaluates these only once.",6,7
2938,221586175,This shows that the probability mass is concentrated in one single symbol in the vast majority of cases.,11,12
2939,4073255,The left-hand side of rule 13 includes the symbol ++.,10,11
2940,4073255,"This symbol tells KATR that even if another, seemingly better rule matches a query, this rule should take precedence if it matches.",1,2
2941,4073255,"We choose to use the disambiguator ++ in Rule 13 instead; in the terminology of (Stump, 2001) , the ++ symbol identifies rules that apply in ""expanded mode"".",26,27
2942,3937260,"The fact that invisible tweet entropy increases 5 Usernames can be up to 15 characters (plus a space and an symbol per mention); even if each username is only 7 characters, five mentions use almost one-third of the character limit.",21,22
2943,198167944,This process continues until the decoder generates the end-of-sentence symbol.,13,14
2944,203353939,"The system exploits BERT training schedule with streams A and B: the encoder receives as input both the source and the MT output separated by the special symbol ""[SEP]"", assigning to the first ""A"" segment embeddings and to the latter ""B"" segment embeddings.",28,29
2945,2493033,"To be included in the training set the tweet must contain at least 1 token that is not a punctuation symbol, emoji or special token 2 .",20,21
2946,53222270,"Notice that the multilingual architecture leverages the target forcing symbol both as input to the encoder to build its states, and as initial input to the decoder to trigger the first target word.",9,10
2947,348565,An online bespoke symbol management has been created to hold the lexical entries alongside specifically designed symbols which are then accepted via a voting system using a series of criteria.,3,4
2948,348565,"Background Much has been written by speech and language therapists about the necessity for core vocabularies that have been adapted to suit symbol users who need to enhance their language skills [3] , [4] , [5] and [6] .",22,23
2949,348565,More recently concept coding [8] with the idea of mapping different symbol vocabularies along with a focus on psychosocial and environmental factors [9] to improve outcomes have been added to the mix.,13,14
2950,348565,"The authors go on to point out the diglossia [two variations of a language in different social situations] nature of Arabic which means there is a 'phonological distance [in grapheme-to-phoneme mapping] that has a negative impact on the acquisition of basic literacy skills in young Arabic children…"" Words or word phrases (referents) may also be presented above or below a corresponding symbol, with changing forms depending on grammatical status, gender and/or number plus many letters will change their shape depending on their position within a word.",74,75
2951,348565,"To this end not only has research concentrated on word frequency lists and collating an AAC user core vocabulary, but also instigating a voting system for symbol acceptance, so that words or multiword/word phrases are represented by symbols that are suitable culturally, linguistically and for the settings in which they will be used.",27,28
2952,348565,"A concrete noun, even if it is considered part of a fringe vocabulary, is a much easier concept to illustrate with a symbol and may be seen as one of the early building blocks to language acquisition.",24,25
2953,348565,It should also be noted that therapists may choose nouns rather than pronouns for the purpose of symbol transparency.,17,18
2954,348565,"There will always be the need to improve outcomes by collecting more lists from AAC users in the future to improve the balance between words used for symbol communication and those based on frequency of use, although the latter informs vocabulary development By using this system the combined AAC word lists from the Doha schools and clinics making up 'List a' once translated into English, could be compared to the Prenke Romich 100 Frequently Used Core Words [20] , [21] (as Lists b).",27,28
2955,348565,"Some centres in Doha were providing specifically designed symbols for the Arabic culture, environment, social and personalised linguistic needs but there were no adapted symbol sets that were freely available for sharing.",26,27
2956,348565,Building symbol acceptance system As part of the online management system a simple voting set up was created using the filters developed for batches of symbols.,1,2
2957,348565,The four criteria are listed with a free text box for comments: • Feelings about the symbol as a whole • Represents the word or phrase • Color contrast • Cultural sensitivity Figure 3 Voting system with criteria for acceptance on a scale of 1-5 where 5 is completely acceptable Results from voting sessions The initial batch of symbols had 63 voters logging into the Symbol Manager resulting in 2341 votes for 65 symbols.,17,18
2958,348565,"Discussion about the Symbol Management system The initial development of the Symbol Management system was purely for the team to upload lexical entries and symbols with a set of filter systems based on parts of speech, gender, number and symbol descriptions.",41,42
2959,348565,Those therapists working in the Doha area were very willing to express their opinions about symbol suitability and the links with the corresponding word lists collected.,15,16
2960,348565,"It was noted that there was a general understanding that the lexical entries in Modern Standard Arabic and those entries in Qatari colloquial Arabic may share the same symbol for similar meaning words or multiword phrases but there may need to be additional symbols and / or changes in symbol labels to represent different parts of speech, gender and number and to take into account the bilingual nature of the dictionary to aid those who were not fluent Arabic speakers.",28,29
2961,348565,"It was noted that there was a general understanding that the lexical entries in Modern Standard Arabic and those entries in Qatari colloquial Arabic may share the same symbol for similar meaning words or multiword phrases but there may need to be additional symbols and / or changes in symbol labels to represent different parts of speech, gender and number and to take into account the bilingual nature of the dictionary to aid those who were not fluent Arabic speakers.",49,50
2962,348565,Conclusion The core vocabulary and symbol management systems have provided the research team with quick and easy ways to analyse data as well as provide a platform for user participation.,5,6
2963,348565,"Having a selection of MSA and Qatari core and fringe vocabularies has been essential for ongoing symbol development, but there is still a need to continually update the collection of local vocabularies to ensure that colloquial as well as written language is captured.",16,17
2964,232765112,FORM: Word form or punctuation symbol.,6,7
2965,22006749,The decoding phase is initialised with a conventional delimiter symbol and terminates when the same symbol is output.,9,10
2966,22006749,The decoding phase is initialised with a conventional delimiter symbol and terminates when the same symbol is output.,15,16
2967,53245252,The search algorithm initiates with a start symbol (<s>) and expands the title hypotheses position-by-position (given the pre-computed bi-gram counts).,7,8
2968,53245252,"If the candidate token of the new hypothesis is the end-of-sentence (EOS -</s>) symbol, the new hypothesis is not created.",20,21
2969,218487229,$ is the end-of-segment symbol and is removed when applying the π operator.,8,9
2970,52987803,"The deep output is a densely-connected nonlinear function, which takes as input the concatenation of the LSTM output, the attention output and the current symbol (character) embedding, and outputs a tensor of size 512.",28,29
2971,52987803,"Label smoothing [16] smooths the cross-entropy cost function by giving a weight of 0.9 to the probability of the correct symbol, and 0.1 to the sum of the probabilities of the other symbols.",24,25
2972,236459862,"Often highly visual, this technique often uses symbols (e.g., the swastikas used in Nazi Germany, originally a symbol for health and prosperity) superimposed over other visual images.",21,22
2973,46890017,"In the preprocessing step, we lowercased the tweets and removed URLs, digit, time patterns, special characters, single character, username started with the @ symbol.",29,30
2974,1373479,"Note that the "" "" symbol indicates a concatenation operation in line 10, where the current f orm is placed onto the end of the lexemes array.",5,6
2975,235829504,A break symbol is considered as another token contributing to the score.,2,3
2976,245855936,They used several corpus pre-processing steps such as special symbol filtering and filtering based on segment length.,11,12
2977,237277943,We use cosine (the * symbol in Table 7 ) and the exponential learning rate scheduler (the ‡ symbol in Table 7 ) with a decay rate set to 0.05.,6,7
2978,237277943,We use cosine (the * symbol in Table 7 ) and the exponential learning rate scheduler (the ‡ symbol in Table 7 ) with a decay rate set to 0.05.,20,21
2979,195854926,The stem is marked by the symbol # on either side. •,6,7
2980,237485075,The * symbol indicates statistically significant improvements over the baseline.,2,3
2981,237485075,"The * symbol indicates statistically significant improvements over the baseline computed with bootstrap resampling (Koehn, 2004) with 10,000 samples, 1,000 sample size and 95% significance level.",2,3
2982,231749867,"In particular, for each time step, the CTC produces a probability distribution over the possible target labels augmented with a dedicated <blank> symbol representing the absence of a target value.",26,27
2983,10934368,The stem is marked by the symbol # on either side. •,6,7
2984,11784184,In LF we represent all words as predicates with predicate symbol the corresponding base form of the word and one argument.,10,11
2985,11784184,"For thematic roles we also add predicates with predicate symbol ""theta"" and three arguments.",9,10
2986,220058279,"The discrete symbol token sequence, which is obtained as the intermediate representation in the joint model, is used as an input to an independent text-based MT model, whose outputs are ensembled with the joint model.",2,3
2987,15618774,"For merged words, we place a '#' symbol where a split should happen, this fix aggregate through all the other annotated features.",10,11
2988,15618774,"In the case where there is a split, we place the '#' symbol at the end of the first split part to indicate the merging position.",15,16
2989,218974541,"We also add a copy of each hashtag to the tweet, with the # symbol being removed from the copy and all _ characters in the copy being replace with a single space (i.e, ""#great_day"" becomes ""#great_day great day"").",15,16
2990,3883482,2016) add the approach of setting the output language using a symbol in the input.,12,13
2991,17176892,Each edge has associated an input symbol and an output string.,6,7
2992,17176892,The condition of determinism implies that no two distinct edges departing from a given state have the same input symbol.,19,20
2993,17176892,"Every time an input symbol is accepted, the string associated to the corresponding edge is output and a new state is reached.",4,5
2994,17176892,"The parameters considered for E are: -A probability of insertion for each symbol of Σ. -A probability of deletion for each symbol of Σ. -A probability of substitution for each symbol pair of Σ × Σ. These parameters are estimated using a corpus D which contains pairs consisting in a sentence x C from the input language and a possibly erroneous version x E The reestimation procedure of the parameters of E is done by repeating the following steps for each sentence: -The sentence x E is parsed using Viterbi ECP, obtaining a sentence x L .",13,14
2995,17176892,"The parameters considered for E are: -A probability of insertion for each symbol of Σ. -A probability of deletion for each symbol of Σ. -A probability of substitution for each symbol pair of Σ × Σ. These parameters are estimated using a corpus D which contains pairs consisting in a sentence x C from the input language and a possibly erroneous version x E The reestimation procedure of the parameters of E is done by repeating the following steps for each sentence: -The sentence x E is parsed using Viterbi ECP, obtaining a sentence x L .",22,23
2996,17176892,"The parameters considered for E are: -A probability of insertion for each symbol of Σ. -A probability of deletion for each symbol of Σ. -A probability of substitution for each symbol pair of Σ × Σ. These parameters are estimated using a corpus D which contains pairs consisting in a sentence x C from the input language and a possibly erroneous version x E The reestimation procedure of the parameters of E is done by repeating the following steps for each sentence: -The sentence x E is parsed using Viterbi ECP, obtaining a sentence x L .",31,32
2997,17176892,"To account for this, the error model is smoothed by assigning a small probability to the generation, by means of an insertion or a substitution, of a special symbol representing out-of-vocabulary words.",31,32
2998,232335496,The results show the strong performance of the Czert-B 12 The results in Table 10 with the † symbol.,20,21
2999,16772171,Each edge of the network has an input symbol and an output string associated to it.,8,9
3000,16772171,"Every time an input symbol is accepted, the corresponding string is output and a new state is reached.",4,5
3001,218974360,"goodnight, good morning) are usually used as nouns (""ΚΝΟ"" (Greek)) instead of interjections (""ΕΦ""), b. the most common tag is ""Ρ"" (Greek), (verb), which was assigned to a total of 4,149 tokens, c. the least common tag is ""ΣΥΜ"" (mathematical or other symbol), which was assigned to a total of 43 tokens, and d. the most frequent token, with 3,692 appearances, is the codeword ""ΣΗΣΤ"" (punctuation mark).",67,68
3002,247363308,"When the <eos> (i.e end of sentence) symbol is seen, the final time step initializes the decoder RNN (blue colored).",11,12
3003,247363308,"Then, the prediction is fed back to the decoder RNN to predict the next word until the <eos> symbol is generated [7] .",21,22
3004,53220272,"When the <eos> (i.e end of sentence) symbol is seen, the final time step initializes the decoder RNN (blue _____________________________________________________________ Proceedings of the 14th International Workshop on Spoken Language Translation Tokyo, Japan, December 14th-15th, 2017 color).",11,12
3005,53220272,"Then, the prediction is fed back to the encoder RNN to predict the next word, until the <eos> symbol is generated [19] .",22,23
3006,236459941,The input to the first layer i.e. h 0 <t are the embeddings of all previous transitions a <t concatenated with a start symbol.,25,26
3007,236459941,p(y t:t+n | y <t ) = softmax(E Y * • h m <t ) y t:t+n (7) Here E Y * is the vocabulary of all transition ngrams excluding words found in the train corpus plus a blank symbol.,47,48
3008,2171500,"The first word is generated by initializing the output sequence with the start symbol <s>, which is mapped to an embedding, and further encoded into a (recursive) hid-den state, which also depends on a linear combination of the bidirectional hidden states of the encoder.",13,14
3009,2171500,The process continues until the sentence boundary symbol (<s>) is emitted.,7,8
3010,2171500,This algorithm iteratively merges the most frequent pair of symbols (in this case character or sequence of characters) into a single symbol.,23,24
3011,38694283,"For identifying mentions, two binary features, is-start-with-capital-letter and is-all-uppercase, were extracted together with the following: • word shape-1, a length 6 one-hot vector containing the following six binary flags: upper case, lower case, digit, '@' symbol, '#' symbol, and other characters, • word shape-2, a length 39 one-hot vector consisting of the 26 letters of the English alphabet converted to lower case, together with the ten digits, the two symbols '@' and '#', and one spot for other characters, and • a word2vec pre-trained vector of length 150, Tweets were collected from the W-NUT 2016 shared task, 4 the 2016 NEEL challenge, 5 and the W-NUT 2017 workshop datasets to build the word2vec model (Mikolov et al.,",60,61
3012,38694283,"For identifying mentions, two binary features, is-start-with-capital-letter and is-all-uppercase, were extracted together with the following: • word shape-1, a length 6 one-hot vector containing the following six binary flags: upper case, lower case, digit, '@' symbol, '#' symbol, and other characters, • word shape-2, a length 39 one-hot vector consisting of the 26 letters of the English alphabet converted to lower case, together with the ten digits, the two symbols '@' and '#', and one spot for other characters, and • a word2vec pre-trained vector of length 150, Tweets were collected from the W-NUT 2016 shared task, 4 the 2016 NEEL challenge, 5 and the W-NUT 2017 workshop datasets to build the word2vec model (Mikolov et al.,",65,66
3013,34587934,"After initial digitisation and correction, tables of the consonant inventories were created, one language inventory per row, with consonants ordered in equal length rows and a dummy symbol for missing items in specific inventories.",30,31
3014,34587934,"∪ If the rows consistently represent the values of ordered attributes and if they are of equal length (for example when missing items are given an explicit dummy symbol), the rows can be interpreted as sequences.",29,30
3015,67855753,"At each timestep the model can take three different actions: NT, which introduces a nonterminal symbol-such as a VP or NP-onto the stack; SHIFT, which places a terminal symbol onto the top of the stack, or REDUCE.",17,18
3016,67855753,"At each timestep the model can take three different actions: NT, which introduces a nonterminal symbol-such as a VP or NP-onto the stack; SHIFT, which places a terminal symbol onto the top of the stack, or REDUCE.",36,37
3017,247130574,"The model is organized as a stack of encoder-decoder networks that works in an auto-regressive way, using the previously generated symbol as input for the next prediction.",25,26
3018,28044836,"Bidirectional LSTM Tagger The most straightforward way of modeling WSD as formulated in Section 3 is that of considering a sequence labeling architecture that tags each symbol x i ∈ V in the input sequence with a label y j ∈ O. Even though the formulation is rather general, previous contributions (Melamud et al.,",26,27
3019,28044836,"In the sequence-to-sequence framework, a variable-length sequence of input symbols x is represented as a sequence of vectors x = x 1 , ..., x T by converting each symbol x i ∈ x into a real-valued vector x i via an embedding layer, and then fed to an encoder, which generates a fixed-dimensional vector representation of the sequence.",37,38
3020,28044836,"3 A decoder is then trained to predict the next output symbol y t given the encoded input vector c and all the previously predicted output symbols y 1 , ..., y t−1 .",11,12
3021,28044836,"2014; Vinyals and Le, 2015) , we condition each output symbol y t on c, allowing the decoder to peek into the input at every step, as in Cho et al. (",13,14
3022,229923220,"The dataset was anonymized by replacing usernames, indicated by the '@' symbol.",14,15
3023,14429450,"Below, the | symbol denotes the cons (concatenation) operator.",4,5
3024,218974489,"Separated punctuation is attached to a ""#"" symbol marking the direction for concatenation once the sentence is put back together.",9,10
3025,4119594,"When such an adverb appears in an input sentence (it may appear as an unidentified element in a sentence for analysis) we give it the symbol of a noun phrase (NP), with a special index number: NP8 or NP9.",27,28
3026,4119594,"However, this symbol seems superfluous and we have preferred to mark this element as NP, with a preposition preceding it (in our system, a preposition may also be "" "").",3,4
3027,6885924,"The symbol * indicates that the best system beats the other systems with statistical significance, with p < 0.05 and according to a bootstrap resampling test (Koehn, 2004) .",1,2
3028,6885924,"We trained the English delexicalized system on the MPQA corpus, using the same test documents The symbol * indicates that the best system beats the other systems with statistical significance, with p < 0.05 and according to a bootstrap resampling test (Koehn, 2004) .",17,18
3029,4121989,"The system takes into account the cases where there are no prefixes or suffixes and denotes either of them with the symbol ""#"".",21,22
3030,4121989,"All are normalized to the symbol ¨ ""A"".",5,6
3031,3906753,"We use the term ""recursion"" to refer to situations in which multiple embeddings require the use of an unbounded symbol memory to keep track of unfinished dependencies.",21,22
3032,3906753,"1 We focus here on the case of centerembedding recursion, which can be generated by a context free grammar (one symbol on the left of each rule, finitely many symbols on the right) or a pushdown automaton (stack memory + finite state controller) but not by a finite state device (Hopcroft and Ullman, 1979) .",22,23
3033,3906753,"Here, we take advantage of recent formal results indicating how recurrent neural networks can encode abstract recursive structure (Moore, 1998; Pollack, 1987; Siegelmann, 1999; Tabor, 2000) An essential insight is that the network can use a spatial recursive structure, a fractal, to encode the temporal recursive structure of a symbol sequence.",61,62
3034,3906753,"2011) focused on counting recursion languages (only a single stack symbol is required to track the recursive dependencies), we provide evidence here for mirror recursion learning by a few participants (multiple stack symbols required).",12,13
3035,3906753,"Following standard terminology, we call the trials in which boxes 1 and 4 change colors ""push"" trials (because in a natural implementation of the grammar with a pushdown automaton, the automaton pushes a symbol onto the stack at these trials).",38,39
3036,3906753,"When two of the same symbol occurred in a row (e.g., 1 1 2 2 5), we shifted the shade of the color of the repeated element so that participants would notice the change.",5,6
3037,3906753,"The task of the network was to predict, on its output layer, what symbol would occur next at each point.",15,16
3038,3906753,"Fractal Encoding of Recursive Structure in Neural Ensembles In the past several decades, a number of researchers (Moore, 1998; Pollack, 1987; Siegelmann, 1996; Siegelmann and Sontag, 1994; Tabor, 2000) have developed devices for symbol processing which compute on finite-dimensional complete metric spaces (distance is defined, no points are ""missing""- (Bryant, 1985) ), like the neural networks considered here.",45,46
3039,3906753,"A common strategy in all of these proposals is the use of spatially recursive sets-i.e., fractals-to encode the temporal recursive structure in symbol sequences.",27,28
3040,3906753,"F is a finite list of functions f i : H → H, P is a partition of the metric space, Σ is a finite symbol alphabet, IM is an Input Map-that is, a function from symbols in Σ and compartments in P to functions in F .",27,28
3041,3906753,"If, when the last symbol has been presented, the system is in the region F R ⊆ H, then the DA accepts the string.",5,6
3042,3906753,"A good way of understanding the principle underlying this mechanism is to note that a pushdown automaton (PDA) (Hopcroft and Ullman, 1979) for processing this language must employ a stack alphabet with one symbol for tracking""1"" and another for tracking ""4"". (",38,39
3043,3906753,"In particular, for each cluster corresponding to a DA 1/PDA 1 state with more than one symbol on the stack in PDA 1, we considered all the clusters with one-fewer symbols on the stack.",19,20
3044,3906753,We asked if the nearest cluster with one fewer symbols on the stack corresponded to the nearest one-fewer stack symbol point in DA 1.,21,22
3045,1669441,A special empty symbol is used for the POS when the position goes beyond the end or the beginning of the current sentence.,3,4
3046,1669441,"As for POS, a special empty symbol stands for words beyond the end or the beginning of the sentence and similarly to neighboring words features, words in collocations are given under their lemma form.",7,8
3047,4064082,"Note that the symbol ""#"" in the figure serves to ""anchor"" the initial state of the HMM and facilitate computation.",3,4
3048,5395686,"In erasure noise, a symbol in the context is probabilistically erased and replaced with a special symbol E with probability e. In deletion noise, a symbol is erased from the sequence completely, leaving no trace.",5,6
3049,5395686,"In erasure noise, a symbol in the context is probabilistically erased and replaced with a special symbol E with probability e. In deletion noise, a symbol is erased from the sequence completely, leaving no trace.",17,18
3050,5395686,"In erasure noise, a symbol in the context is probabilistically erased and replaced with a special symbol E with probability e. In deletion noise, a symbol is erased from the sequence completely, leaving no trace.",27,28
3051,5395686,"We apply deletion noise with by-symbol deletion probability d. So for example, given a prefix NCNCNVV, the prefix can be corrupted to NCNNVV with probability proportional to d, representing one deletion.",7,8
3052,5395686,"Supposing that the noisy representation of context V is the result of running the veridical context w 1:i−1 through progressive erasure noise, we can see V as a sequence of values v 1:i−1 , where each v i is equal to either w i or the erasure symbol E. Rewriting pmi(w i ; V ) as pmi(w i ; v 1:i−1 ), we can decompose it into interaction informations as follows: pmi(wi; v1:i−1) = i−1 n=1 I∈( 1:i−1 n ) i(wi; vI 1 ; ...; vI n ). (",52,53
3053,4551282,"If an OH gains a new insight after reading a comment, he/she replies to that comment with a ∆ symbol and specifies the reasons behind his/her view change.",22,23
3054,11453394,"This demonstrates that, as expected, the online algorithms do not take 11 Results marked with the "" * "" symbol are NOT statistically significant compared to the corresponding batch model.",21,22
3055,3262752,The symbol ◊ has been used for the nodes that are anchored to a lexical item. -,1,2
3056,10071979,"-Presence of retweet symbol ""RT"" and position in the tweet (start, end and middle). """,3,4
3057,7287507,We replace language model input symbols and sentence model output symbols with the empty symbol ǫ and use the V-expectation semiring of Eisner (2002) to annotate noise model arcs with initial parameter values.,14,15
3058,248780135,"Given a sentence, we randomly replace spans of the sentence with [BLANK] symbol and append the original contents of the blanks in order separated by [FILLER] symbol.",15,16
3059,248780135,"Given a sentence, we randomly replace spans of the sentence with [BLANK] symbol and append the original contents of the blanks in order separated by [FILLER] symbol.",31,32
3060,3925481,"One feature denotes whether the symbol represents a space ([+SPACE]) or a letter ([−SPACE]), an important distinction because spaces indicate word boundaries.",5,6
3061,3925481,The other feature attached to each symbol specifies whether the character in the text that the symbol was emitted from was being fixated ([+FIX]) or not ([−FIX]).,6,7
3062,3925481,The other feature attached to each symbol specifies whether the character in the text that the symbol was emitted from was being fixated ([+FIX]) or not ([−FIX]).,16,17
3063,3925481,"This visual input string is generated by a process of moving a marker from the beginning to the end of the perceptual span, generally inserting a symbol into the visual input string for each character it moves across (EMISSION).",27,28
3064,3925481,"To provide only noisy information about word length, however, this process is not always one of EMISSION, but sometimes it inserts a symbol into the visual input string that does not correspond to a character in the text (INSERTION), and at other times it fails to insert a symbol for a character in the text (SKIPPING).",25,26
3065,3925481,"To provide only noisy information about word length, however, this process is not always one of EMISSION, but sometimes it inserts a symbol into the visual input string that does not correspond to a character in the text (INSERTION), and at other times it fails to insert a symbol for a character in the text (SKIPPING).",54,55
3066,3925481,"Otherwise, a decision is made about whether to emit a symbol into the visual input string from the character at the marker's current position (EMISSION) or whether to skip outputting a symbol for that character (SKIPPING).",11,12
3067,3925481,"Otherwise, a decision is made about whether to emit a symbol into the visual input string from the character at the marker's current position (EMISSION) or whether to skip outputting a symbol for that character (SKIPPING).",35,36
3068,3925481,"A [−SPACE] symbol (produced through EMIS-SION or INSERTION) contains noisy information about the identity of the letter that generated it, obtained via sampling.",4,5
3069,3925481,"Given this representation, a [−SPACE] symbol contains a sample from a 26-dimensional Gaussian with a mean equal to the letter's true identity and a diagonal covariance matrix Σ(ε) = λ (ε) −1 I, where λ (ε) is the visual acuity at eccentricity ε.",8,9
3070,44161904,"Color-related features: Dealing with four emojis with the heart symbol in different colors, we decided to use another NRC Lexicon -on Word-Colour Associations (Mohammad, 2011) .",12,13
3071,203736437,"Henceforth, we denote embedding concatenation with the symbol and model ensembling with ⊕. For example, U2H U2M is a model trained on the concatenation of U2H and U2M embeddings, while U2H ⊕ BERT represents the average predictions of two models, one trained on U2H embeddings and one on BERT.",8,9
3072,203736437,"As mentioned in Section 3, the symbol ' ' denotes a single model trained on the concatenation of the features, while the symbol '⊕' denotes an averaging of individual models trained on each feature separately.",7,8
3073,203736437,"As mentioned in Section 3, the symbol ' ' denotes a single model trained on the concatenation of the features, while the symbol '⊕' denotes an averaging of individual models trained on each feature separately.",24,25
3074,52010508,"As for the local measure, normalized log probability of the full stop and question mark is calculated as LogP n (QM/F S) = log( pm( ) pu( ) ) where p m ( ) is the probability of the symbol, , at a given position given by the model; p u ( ) is the unigram probability of the .",47,48
3075,8926220,"For the sake of simplicity, the magnitude of r values are presented using colors where as '' symbol represent the corresponding r is not significant.",18,19
3076,8926220,"q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Ang Fru Emp (a) Non-overlapping segments q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Ang Fru Emp 3 : Correlation analysis at the non-overlapping segment, and overlapping segments, where '' symbol represents that the corresponding r is not significant.",744,745
3077,226283759,Hashtags are preprocessed by removing the '#' symbol and those with multiple words are split based on uppercase letters.,9,10
3078,226283759,Twitter user handles and the symbol 'RT' which indicates re-tweet are removed.,5,6
3079,196213305,"Phonological feature vectors are obtained from Epitran using Panphon's database of International Phonetic Alphabet (IPA) symbol features (Mortensen et al.,",18,19
3080,196213305,"Each phonetic symbol is represented by a fixed-length vector of integers between -1 and 1 representing the presence (+1), absence (0), or lack (-1) of 21 phonological features.",2,3
3081,233365128,The valence score ϑ(t) C helps determine the importance of a given word/symbol t in a given class C while considering its presence or absence in other classes.,15,16
3082,53223761,"If a meal is in excess with regard to the ideal value, we add a + symbol to the category (e.g. C + 2 ) to denote the deviation.",17,18
3083,53223761,"In contrast, if a meal is lacking, we add a − symbol to the category (e.g. C − 1 ).",13,14
3084,10752052,"Computers allow to directly target this goal by efficiently providing symbol manipulation; Redefinition of the role of the reader: thanks to digital printing, in the new Italian and English editions of Balestrini's novel Tristano (2015-16 ) (Balestrini, 2015; Balestrini, 2016) , each copy is different from any other, thus reaching his original purpose, i.e. to escape ""the rigid determinism of the mechanical Gutenberg printing process"" (Balestrini, 2015) .",10,11
3085,231709547,"A template predicate node represents a predicate symbol or a term, while a slot value node represents an atom mentioned in utterances.",7,8
3086,226227504,"As each latent variable value can be represented by a symbol, the module for P r(z|h k , x, r) is realized by a SEQ2SEQ model.",10,11
3087,226227504,"It takes as input a token sequence consisting of a latent variable value h k , a word sequence x, and a relation symbol r, and predicts a word sequence representing the next event z. We enrich the input vocabulary of the chosen SEQ2SEQ model with the symbols of h k and r which are mapped to the corresponding latent embeddings and relation embeddings during forward propagation.",24,25
3088,6622975,"To include the rules of the form ÿ© º³ , where ¨ and ³ ¡ , in the set ¦ ¤ ©¨, we make use of a special symbol $ which is not in the terminals nor in the non-terminals.",31,32
3089,6622975,"In order to avoid null values, the unseen events were labeled with a special ""unknown"" symbol which did not appear in the vocabulary, so that the probabilitie of the unseen envent were positive for all the categories.",18,19
3090,448226,Unknown words in the test set are represented by a special symbol.,11,12
3091,226283728,For each sentence we implement and then delete all but one of these backspace actions leaving only the first errorfully pressed key (the first f in the fff) and a single backspace symbol.,34,35
3092,17768039,"Basic Concepts -rid Notation Given an alphabet X, X* is the free monoid of strings over X. The symbol A represents the empty string, first letters (a, b, c, ...) represent individual symbols of the alphabets and last letters (z, y, x, ...) represent strings of the free monoids.",20,21
3093,17768039,Each arc connects two states and it is associated to an input symbol and an output substring (that may be empty).,12,13
3094,17768039,"Each time the procedure finds an arc whose input symbol is a category label, it expands this arc by the adequate cSST producing a new model.",9,10
3095,17809731,Informal Illustration Let us consider the following example contextfree grammar: NP → Adj NP | N Adj → big | angry N → dog (The vertical bar separates alternative right-hand sides for the same left-hand side nonterminal symbol.),43,44
3096,17809731,"Let G be an OP-SCFG as above, with S 1 the start symbol of G 1 and S 2 the start symbol of G 2 .",15,16
3097,17809731,"Let G be an OP-SCFG as above, with S 1 the start symbol of G 1 and S 2 the start symbol of G 2 .",24,25
3098,17809731,"For technical reasons, we need to augment the grammar with a new start symbol, as shown in (10).",14,15
3099,17809731,"This manipulates items of the form A → α • For given CFG G, with set N of nonterminals, set Σ of terminals, and start symbol S ∈ N , the SCFG G fuzzy has one synchronous rule: A → ( A X 1 • • • X m ) A , A(s (l) 0 , s (r) 0 ) → y (l) Y 1 • • • Y m y (r) (5) for each rule A → X 1 • • • X m in G, for each choice of the pair: (y (l) , y (r) ) ∈ {(( A , ) A ), ((, ))} ∪ {[ A , [, ε} × {] A , ], ε} (6) and for each choice of pairs: (s (l) i , s (r) i ) ∈ {(( B , ) B ) | B ∈ N } ∪ {((, ))} ∪ ({[ B | B ∈ N } ∪ {[, ε}) × ({] B | B ∈ N } ∪ {], ε}) for each i (1 ≤ i ≤ m) such that X i ∈ N , and (s (l) i , s (r) i ) = (ε, ε) for each i such that X i ∈ Σ, and: Y i = X i (s (l) i , s (r) i ) if X i ∈ N X i if X i ∈ Σ (7) under the following constraints: m = 1 ∧ (s (l) 1 , s (r) m ) ∈ {(( A , ) A ), ((, ))} → (y (l) = ε ∨ y (r) = ε) ∧ (s (l) 1 = [ A ∨ s (l) 1 = [) → y (l) = ε ∧ (s (r) m = ] A ∨ s (r) m = ]) → y (r) = ε (8) and: (s l) , s (r) ) otherwise, where: (l) 0 , s (r) 0 ) =                                (y (l) , y (r) ) if (y (l) , y (r) ) ∈ {(( A , ) A ), ((, ))} (s (l) 1 , s (r) m ) if m = 1 ∧ y (l) = y (r) = ε ∧ (s (l) 1 , s (r) m ) ∈ {(( B , ) B ) | B ∈ N \ {A}} (s ( s (l) =    y (l) if y (l) ∈ {[ A , [} s (l) 1 if y (l) = ε ∧ s (l) 1 ∈ {[ B | B ∈ N \ {A}} ε otherwise s (r) =    y (r) if y (r) ∈ {] A , ]} s (r) m if y (r) = ε ∧ s (r) m ∈ {] B | B ∈ N \ {A}} ε otherwise (9) G fuzzy further has one synchronous rule: S † → S, S † → S(s (l) , s (r) ) (10) for each choice of the pair: (s (l) , s (r) ) ∈ {(( B , ) B ) | B ∈ N } ∪ {((, ))} ∪ ({[ B | B ∈ N } ∪ {[, ε}) × ({] B | B ∈ N } ∪ {], ε}) where S † is a new symbol.",28,29
3100,17809731,"This manipulates items of the form A → α • For given CFG G, with set N of nonterminals, set Σ of terminals, and start symbol S ∈ N , the SCFG G fuzzy has one synchronous rule: A → ( A X 1 • • • X m ) A , A(s (l) 0 , s (r) 0 ) → y (l) Y 1 • • • Y m y (r) (5) for each rule A → X 1 • • • X m in G, for each choice of the pair: (y (l) , y (r) ) ∈ {(( A , ) A ), ((, ))} ∪ {[ A , [, ε} × {] A , ], ε} (6) and for each choice of pairs: (s (l) i , s (r) i ) ∈ {(( B , ) B ) | B ∈ N } ∪ {((, ))} ∪ ({[ B | B ∈ N } ∪ {[, ε}) × ({] B | B ∈ N } ∪ {], ε}) for each i (1 ≤ i ≤ m) such that X i ∈ N , and (s (l) i , s (r) i ) = (ε, ε) for each i such that X i ∈ Σ, and: Y i = X i (s (l) i , s (r) i ) if X i ∈ N X i if X i ∈ Σ (7) under the following constraints: m = 1 ∧ (s (l) 1 , s (r) m ) ∈ {(( A , ) A ), ((, ))} → (y (l) = ε ∨ y (r) = ε) ∧ (s (l) 1 = [ A ∨ s (l) 1 = [) → y (l) = ε ∧ (s (r) m = ] A ∨ s (r) m = ]) → y (r) = ε (8) and: (s l) , s (r) ) otherwise, where: (l) 0 , s (r) 0 ) =                                (y (l) , y (r) ) if (y (l) , y (r) ) ∈ {(( A , ) A ), ((, ))} (s (l) 1 , s (r) m ) if m = 1 ∧ y (l) = y (r) = ε ∧ (s (l) 1 , s (r) m ) ∈ {(( B , ) B ) | B ∈ N \ {A}} (s ( s (l) =    y (l) if y (l) ∈ {[ A , [} s (l) 1 if y (l) = ε ∧ s (l) 1 ∈ {[ B | B ∈ N \ {A}} ε otherwise s (r) =    y (r) if y (r) ∈ {] A , ]} s (r) m if y (r) = ε ∧ s (r) m ∈ {] B | B ∈ N \ {A}} ε otherwise (9) G fuzzy further has one synchronous rule: S † → S, S † → S(s (l) , s (r) ) (10) for each choice of the pair: (s (l) , s (r) ) ∈ {(( B , ) B ) | B ∈ N } ∪ {((, ))} ∪ ({[ B | B ∈ N } ∪ {[, ε}) × ({] B | B ∈ N } ∪ {], ε}) where S † is a new symbol.",811,812
3101,17525858,"12) Broad Sound Groups (BSGs) -a vector of length 8 that represents the number of times each BSG occurs in word w, e.g., the word ""problem"" has 2 vowels, 2 stops, 2 liquids, and 1 nasal; and (13) Phonemes -a vector of length 39 that represents the number of times a phonetic symbol appears in w's phonetic transcription.",65,66
3102,5729140,"It allows us to obtain the most probable parse tree that simultaneously analyzes two strings, X = x 1 ... The parsing algorithm is based on the definition of: δ ijkl (A) = Pr(A * ⇒ x i+1 • • • x j /y k+1 • • • y l ) as the maximum probability of any parsing tree that simultaneously generates the substrings x i+1 • • • x j and y k+1 • • • y l from the non-terminal symbol A .",88,89
3103,218974028,"2012) (used by spaCy), and it lists the following 17 categories: adjective (ADJ), adposition (ADP), adverb (ADV), auxiliary (AUX), coordinating conjunction (CCONJ), determiner (DET), interjection (INTJ), noun (N), numerical (NUM), particle (PART), pronoun (PRON), proper noun (PROPN), punctuation (PUNCT), subordinating conjunction (SCONJ), symbol (SYM), verb (VERB) and other (X).",89,90
3104,218974028,"Each line contains the following ten fields, separated with tabulator character: 1) word index (integer starting from 1 for each new sentence), 2) word form or punctuation symbol, 3) lemma or stem of word form, 4) Universal Part-of-Speech tag, 5) language-specific tag with nPOS introduced in Table 1 (or underscore if not available), 6) list of morphological features from the universal feature inventory or from a defined language-specific extension (or underscore if not available), 7) head of the current word, which is either a value of ID or zero, 8) Universal dependency relation to the HEAD, 9) enhanced dependency graph in the form of a list of head-deprel pairs, and 10) any other annotation.",34,35
3105,3227110,"This system works in the 0-initiM suffix symbol 0, a vowel letter a, or a vowel letter 9 by context; and (2) the arctfiphoneme I for the proper treatment of predicativc postposition 'ol' /i/, which can be changed into either 0 or a vowel letter i by context.",9,10
3106,10211215,"However, it is pronounced as a short vowel /e/. Sometimes, for disambiguation purposes it is preferred to explicitly mark its presence by a written symbol (the diacritic Kasre) after some words in order to facilitate the correct pronunciation.",26,27
3107,5556636,"They used a separate mark-up token for hyphened compounds, where the hyphen was split into a separate token, and marked by a symbol.",26,27
3108,5556636,"2007) marked morphs with a symbol and merged all marked words with the next word for translation between Finnish, Swedish, and Danish, without showing any improvements over an unmarked baseline.",6,7
3109,5556636,"El-Kahlout and Oflazer (2006) used a similar symbol-based merging strategy for translation from English into Turkish, but with the addition of morphographemic rules.",11,12
3110,5556636,Should compound modifiers be marked with a special symbol?,8,9
3111,5556636,"In Example (4b), called the marked scheme, compound modifiers are not normalized, but they are marked with the symbol ""#"".",23,24
3112,5556636,"Table 3 gives an overview of the possible bigram combinations, using the three-symbol tagset, plus sentence beginning and end markers, and their judgment as good, bad, or neutral.",15,16
3113,5556636,The symbol-based method is inspired by work on morphology merging (El-Kahlout and Oflazer 2006; Virpioja et al.,1,2
3114,5556636,It merges words that are marked with a symbol with the next word in the marked scheme.,8,9
3115,5556636,"In the sepmarked scheme, when a standard symbol is found, the words on both sides of it are merged.",8,9
3116,5556636,"In all experiments merging was performed using the POS-match heuristic (see Section 7.2.1), except for the sepmarked scheme that used the SPOS-tagset that does not allow this type of POS-match, and for which the symbol merging algorithm (see Section 7.2.2) was used.",44,45
3117,5556636,"We also extended the list-and symbol-based methods by a restriction that the head of the compound should have a compounding part of speech, that is, a noun, adjective, or verb.",7,8
3118,5556636,"In these experiments we used the unmarked representation scheme, which means that compound modifiers were normalized, but not marked with any symbol, and we used the EPOS-tagset.",23,24
3119,5556636,"The merging methods based on lists all give significantly worse results than the baseline on all metrics, except when combined with the symbol method.",23,24
3120,5556636,"The compound-list+ method is the best method of the systems that only uses list information; it is significantly better than word-list and compound-list on all metrics, but still significantly worse than the baseline, symbol, and POS-match systems.",42,43
3121,5556636,"There are no significant differences between the baseline and the systems with symbol or POS-match merging, but the trend on most metrics is that the POS-match systems have a slightly higher score than the other methods.",12,13
3122,5556636,These two error types are often prohibited by the head-pos restrictions and by the symbol and POS-match algorithms.,16,17
3123,5556636,"With the EPOS-model there are very small differences in number of merges between the methods based on symbol and POS-match, although this difference is much larger without the EPOS-model.",19,20
3124,5556636,"For instance, the difference is 68 merges between symbol and POS-match with the EPOS-model, whereas without the EPOS-model the difference is 277.",9,10
3125,5556636,"On the automatic metrics, the POS-match and symbol methods gave about the same results as the baseline; we believe there are other strengths to this method as compared to the baseline system, however, such as the production of a higher number of compounds.",10,11
3126,5556636,"We chose two heuristic strategies, the POS-match strategy, which gave the overall best results, and compound-list+, which gave the best results of the word-list based merging strategies without any POS or symbol information.",41,42
3127,12704877,"lished but had to develop slowly from ml artificial, refercnce language-dependent symbol system into m~ autotiomous hmguage by being used in a community (cf.",14,15
3128,12704877,"text, where people are u~d to defining symbol systems which they call ""languages"".",10,11
3129,12704877,"Machine translation, by con trast, is concerned with translatin G texts between thunau languages, which hem a sem,'mtic point of view --even if die lmlguage may be simplified or the text pre-edited--are in-hermNy more complicated than artificial symbol systems.",46,47
3130,12704877,"Interestingly enough, this translatability criterion is the property by which human language is distinguished from artificial symbol systems by one of the classics of linguistics, Louis Hjelmslev (1963: 101) .",17,18
3131,12704877,"According to him, a human language (his term is dagligsprog) is a language into which all other communication systems (human languages and artificial symbol systems) can be translated.",27,28
3132,12704877,"If the pivot of a machine translation system is a language (rather than an artificial symbol system), this removes the problems of spelling out semantic dements and relations.",16,17
3133,12704877,"Conch~slon An inte.uediate language for high-quality machine translation needs to he a full-fledged human language, due to the inherent lack of expressiveness that is an inevitable characteristic of artificial symbol systems.",36,37
3134,13612096,A constituent c A ij is defined by the nonterminal symbol A (either a syntactic label or a POS tag) and its span ij (the starting and ending indexes which delimit the part of the input sentence encompassed by the constituent).,10,11
3135,21693348,"We will denote aligned chunks with S(align.chunk) ↔ T (align.chunk); • Filtering the chunks to those in which a source part of a chunk matches a term from a list of domain terms in a source language: S(align.chunk) ∼ S(term.list), where symbol ∼ denotes the relation ""match"" (that is for our experiment defined in Subsection 4.5.); •",49,50
3136,243865635,"1  In discourse segmentation, we can consider joint probability P (x, e) for a sequence with inserting a symbol, [EDU], at an EDU boundary (Figure 3 (a) ).",23,24
3137,243865635,"2019) , we used the symbol [SEP] to represent [EDU] and symbol [unused#] starting from 0 to represent parsing labels such as ""(N"" and ""(Attribution"".",6,7
3138,243865635,"2019) , we used the symbol [SEP] to represent [EDU] and symbol [unused#] starting from 0 to represent parsing labels such as ""(N"" and ""(Attribution"".",16,17
3139,243865635,"We used symbol ""====="" in vocab to represent the symbol [EDU] .",2,3
3140,243865635,"We used symbol ""====="" in vocab to represent the symbol [EDU] .",15,16
3141,243865635,"Because the vocab of GPT-2 has no available symbol for representing an unseen symbol, we added <pad> and our relation symbols to the vocab of GPT-2 and resized the pre-trained word embeddings.",8,9
3142,243865635,"Because the vocab of GPT-2 has no available symbol for representing an unseen symbol, we added <pad> and our relation symbols to the vocab of GPT-2 and resized the pre-trained word embeddings.",13,14
3143,1997622,The primary issue in this study was to analyse to what extent the concepts in the main AAC symbol databases mirror the vocabulary needed to produce and understand everyday Swedish language.,18,19
3144,1997622,Another goal was to investigate the possibility of extending the AAC symbol databases by combining separate basic words from the vocabulary into compounds.,11,12
3145,1997622,The AAC symbol coverage of these database entries was then assessed by addressing an existing AAC symbol database.,2,3
3146,1997622,The AAC symbol coverage of these database entries was then assessed by addressing an existing AAC symbol database.,16,17
3147,1997622,Some of the symbol systems have a visual structure that supports different parts of speech.,3,4
3148,1997622,"In order to analyse to what extent the AAC symbols really supported this basic vocabulary, an additional comparison was made, focusing on the intersection of words with and without symbol coverage in the two sets.",31,32
3149,1997622,"Verbs Adjusted frequency of the 44 verbs not represented in the symbol database ranged between 14.97 and 1.38, implying a moderate dispersion and frequency.",11,12
3150,1997622,"Nouns We found that 24 % of the noun lemmas in LBL and SBV lacked symbol coverage, and that there was a wide range in adjusted frequency, varying from 232.84 down to 1.06.",15,16
3151,1997622,"Adjectives and adverbs For adjectives, the proportion of lemmas without symbol coverage was as high as 29 %, while 40 % of the adverbs lacked symbol support.",11,12
3152,1997622,"Adjectives and adverbs For adjectives, the proportion of lemmas without symbol coverage was as high as 29 %, while 40 % of the adverbs lacked symbol support.",27,28
3153,1997622,"Augmenting the AAC lexicon The next step was to investigate to what extent SALDO could be of assistance when augmenting As the concepts in SALDO are related by the mother-child relation, we could get the necessary lexical-semantic associations for further analysis of probable candidates for symbol representation.",50,51
3154,1997622,"As was stated earlier, a rather high proportion of noun lemmas missing in the symbol database were characterized as abstract nouns.",15,16
3155,1997622,"From our core vocabulary database we get that the only existing entry is identified as kapitel 1/1, i.e. Figure 4 : Overall ratio of LäSBarT adverbs, presence in SBV and symbol coverage lemma identifier 1 and lexeme identifier 1.",32,33
3156,1997622,"Concerning ompound nouns, which made up the largest portion of lemmas occurring only in LBL and not in SBV, (66 % of the 14,856 noun lemmas), decomposition into simplex words made it possible to achieve information enough for further elaboration into symbol representations.",46,47
3157,1997622,"It is not present in the symbol vocabulary, but we find it directly by a look-up in SALDO with the semantic descriptors kontor 'office' and främst 'major', both with symbol coverage in the database.",6,7
3158,1997622,"It is not present in the symbol vocabulary, but we find it directly by a look-up in SALDO with the semantic descriptors kontor 'office' and främst 'major', both with symbol coverage in the database.",37,38
3159,1997622,"Since we already have the symbol illustrating the most common concept for affär in the primary corpus material, we use that.",5,6
3160,1997622,"There is, however, no symbol in the database for ägare.",6,7
3161,1997622,"As mentioned earlier, the few verbs not existing in the symbol database were generally either hapax, or particle verbs.",11,12
3162,1997622,"Even if we regard the hapax words in LBL as peripheral in the easy-to-read genre, the fact that they exist in the SBV make them candidates for further analysis and inclusion into an augmented symbol lexicon.",39,40
3163,1997622,"It also turned out the the symbol coverage of these entries in the AAC language studied was impressively high for verbs (95 %), lower for nouns (76 %) and adjectives (71 %), and considerably lower for adverbs (60 %).",6,7
3164,1997622,"The fact that the nouns to a higher degree lack symbol support, was compensated for by the circumstance that a relatively high amount of entries could be found in or derived by information in a semantic lexicon.",10,11
3165,1997622,"Given that the results in this study are based on only one of several symbol languages, we would like to extend the research also to these, at first hand Bliss and more of the pictorial systems, such as PCS.",14,15
3166,15086160,By being able to use a picture-or symbol-based communication board the children are given an exciting opportunity to explore language; to play and in the same time learn to use a method for alternative and augmentative communication.,9,10
3167,15086160,"The robot executes the commands that was expressed by the communication board; e.g., if the child points at the symbol for ""draw a figure"", and the symbol with a flower, the utterance might be ""draw a flower, please"", which the robot then performs.",21,22
3168,15086160,"The robot executes the commands that was expressed by the communication board; e.g., if the child points at the symbol for ""draw a figure"", and the symbol with a flower, the utterance might be ""draw a flower, please"", which the robot then performs.",31,32
3169,15086160,"E.g., if the child only points at the symbol for ""draw a figure"", the robot does not get enough information.",9,10
3170,18003146,"7 in show each symbol , following things represent .",4,5
3171,18003146,Here each symbol show and Table 1 in Fig.,2,3
3172,6767256,"The element rl' is a conjunction of such b~sic constraints, or a schema symbol.",17,18
3173,28636463,"The Improved Encoding The prior encoding can be improved through weak, reduced bracketing that packs adjacent closing or opening brackets into a single symbol.",24,25
3174,204770248,The vertices are separated with a bullet symbol ( • ) instead of curly brackets ({}).,7,8
3175,195068926,"If w is a string and σ is the ith symbol in w then w i = σ , so abcd 2 = b. The set of prefixes of w, Pref(w), is {p ∈ Σ * | (∃s ∈ Σ * )[w = ps]}, the set of suffixes of w, Suff(w), is {s ∈ Σ * | (∃p ∈ Σ * )[w = ps]}, the set of substrings, Substr(w), is {u ∈ Σ * | (∃l, r ∈ Σ * )[w = lur]}, and the set of subsequences, Subseq(w) = u 1 u 2 • • • u n |∃v 0 • v 1 • • • v n ∈ Σ * [w = v 0 u 1 v 1 • • • u n v n ] Elements of Finite Model Theory Model theory, combined with logic, provides a powerful way to study and understand mathematical objects with structures (Enderton, 2001) .",10,11
3176,234742697,"Using a clear name and symbol, each badge signposts students' takeaways and how it fits within the top level learning journey (see Figure 2 ).",5,6
3177,11865790,These expressions define symbol sets used in Phonemic and Tonemic tiers in Table 1 .,3,4
3178,11865790,"In its definition, a hidden symbol, Beyond is used as a temporary TBU for tones that are left over.",6,7
3179,11865790,"The markup is done, again, using the same marker symbol, ""<>"", as before.",11,12
3180,5365321,"In this format, the base forms are defined over the alphabet of orthographical symbols Ω whereas the morphological symbols and syntactic categories are multi-character symbols that belong, respectively, to the alphabets Π and Γ. In addition, there is a token boundary symbol #.",47,48
3181,5365321,The percent symbol (%) stands for the unspecified part of the lexical base form.,2,3
3182,734605,"1999)) , function symbols start with upper case letters, form symbols with lower case letters, and both are combined in a combined colon -separated symbol (text) or function -over-form sy mbol (graphics).",28,29
3183,734605,"Recently, a reduced symbol set for propedeutic use and schools, ""VISL light"", was agreed upon, and the Danish X-and-O-system adapted to match the function categories used in VISL light.",4,5
3184,14652960,The Use of CR Constraints Let Σ be the (pair symbol) alphabet over which all the words are defined.,11,12
3185,14652960,"For example, a phonological constraint (Koskenniemi, 1983 ) such as p:m ⇒ Σ * n:m Σ * , (1) specifies a formal language L ⊆ Σ * where the (pair) symbol p:m may occur only when immediately preceded by the symbol n:m. The left hand side (p:m) describes the constrained pattern while the right hand side (Σ * n:m Σ * ) specifies the contexts to which the pattern occurrences are restricted.",41,42
3186,14652960,"For example, a phonological constraint (Koskenniemi, 1983 ) such as p:m ⇒ Σ * n:m Σ * , (1) specifies a formal language L ⊆ Σ * where the (pair) symbol p:m may occur only when immediately preceded by the symbol n:m. The left hand side (p:m) describes the constrained pattern while the right hand side (Σ * n:m Σ * ) specifies the contexts to which the pattern occurrences are restricted.",53,54
3187,14652960,This is done by indexing every atomic symbol in α by the context pairs λ i ρ i .,7,8
3188,14652960,"Thus, each symbol in α is divided into k different variants, α 1 , ..., α k .",3,4
3189,14652960,The markers are often cheaper than the set-based encoding of FO variables as they extend the alphabet only by one new symbol per variable.,23,24
3190,10657167,1 The technique expresses that every symbol in the restricted strings must have separately a valid context.,6,7
3191,10657167,"10) Rearranging the Contexts The right hand side of (10) can be viewed in such a way that the contexts are arranged according to the symbol that follows the marker: [Σ * g(S ) Σ * M ⇒ ∪ a∈S ∪ n a i=1 L a,i g(a) R a,i ] (11) The Underline Operator For notational convenience, introduce an underline operator X = g(X).",28,29
3192,10657167,"Lines marked with %3 compute correct prefixes Σ * \h((Σ * ⋄S )\(L a,i ⋄a)) by testing that every symbol a∈S is preceded by a diamond.",25,26
3193,12767487,A constituent c A ij is a span de-fined by a nonterminal symbol (or syntactic tag) A that covers the substring x ij .,14,15
3194,233864791,"c M +1 ) where x 0 , c 0 , c M +1 denote start, separation, and end symbol, respectively.",21,22
3195,52056110,We keep running this process until the gen-erative module generates an ending symbol.,14,15
3196,1830624,"When a flag diacritic is encountered as a transition label during a lookup, it is interpreted (Table 1 ) and the symbol does not consume any input.",23,24
3197,1830624,"In these rules, the dots symbol . . .",6,7
3198,1830624,"In a finite-state lexicon formalism, this ambiguity class could correspond to a user-defined constant symbol that denotes a regular expression [@1.E@i | @2.E@e].",19,20
3199,1830624,"For example, one should avoid the eager expansion of the dots symbol '. . . '",12,13
3200,1830624,"when compiling the regular expressions in the grammar G. To compute the original semantics of the intersection G ∩ N i , one can compute the image of the regular relation (Id(G) • T 1 ) • (T 2 • Id(N i )) where the intermediately composed relation T 1 substitutes the occurrences of the symbol . . .",59,60
3201,1830624,and the relation T 2 substitutes the occurrences of the symbol . . .,10,11
3202,6758620,A solution would be to assign each square bracket an unsaturated subcategorization frame with a symbol that indicates a state in a special subcategorization automaton.,15,16
3203,17255632,The term 'npp' denotes personal pronoun; 'jxt' denotes topicalized auxiliary particle; 'ncn' denotes non-predicative common noun; 'jco' denotes objective case particle; 'pvg' denotes general verb; 'ef' denotes final ending; and 'sf' denotes full stop symbol.,56,57
3204,17255632,"The symbol '+' is not a part-of-speech, but rather a delimiter between words within a word phrase.",1,2
3205,250391043,Abstract symbol.,1,2
3206,250391043,"Sometimes an abstract symbol is used if it is widespread and more comprehensible (e.g., traffic signs).",3,4
3207,250391043,"Case studies A semantic criterion, namely the inherent conceptual content of the event, is used to group meanings and relative icons Modality Initially, the symbol of a traffic policeman in the position of giving instruction was proposed to express the modal verb must (Figure 2 (a)).",27,28
3208,250391043,"Payuk and Zakrimal (2020) defined the magnifying glass symbol as ""finding and searching without any character or letter.""",10,11
3209,5285027,"The same string with the upper-case initial, Po, can represent four different proper names: the Italian river Po, the French commune Pau (on the northern edge of the Pyrenees), part of a personal name as in Edgar Allan Poe, and finally the chemical symbol for Polonium 1 .",53,54
3210,9665964,"Any symbol a E E, as well as any symbol set {al, a2, , arn }, al, a2.... , e E, are valid FSIG regular expressions.",1,2
3211,9665964,"Any symbol a E E, as well as any symbol set {al, a2, , arn }, al, a2.... , e E, are valid FSIG regular expressions.",10,11
3212,36939595,"In case the predicted nodes are headed, the terminal symbol (i.e. the lexical anchor) is checked for redundancy, when it is the current word.",10,11
3213,36939595,"Proceedings of TAG+6 Here is the definition of the fringe in the left context ¡ ¤£ ¦¥ : at the beginning, the derivation process postulates a left context given by the start symbol; at the step i, we take the path leading from the preterminal node for the word ¨¡ ¤£ ¦¥ to the root: -if there are no nodes marked for substitution, all the nodes on this path are marked for adjunction; -if there are nodes marked for substitution, take the lowest common ancestor of ¨¡ ¤£ ¦¥ and the leftmost node marked for substitution; all the nodes on the two paths to the common ancestor are marked for adjunction.",34,35
3214,16225920,We sorted each emoticon and symbol into one of the six emotions using the explanations written in the websites.,5,6
3215,16225920,"Moreover, combining emoticon and symbol feature particularly improved the classification for happiness increased from 73% to 76.3%.",5,6
3216,5591459,The symbol • refers to the dot product and ReLU(•) is the elementwise rectified linear unit function.,1,2
3217,2598164,"Namely, if a --* bz ... bn is a boolean rule, we may consider it as an abbreviation of the set of rules a* --* b~ ... b~ where a* is an atom of £ below a and b~ is an atom of £ below bi for each i. Likewise, the start symbol abbreviates a set of start symbols ~*, which by familiar tricks can be replaced by a single one denoted by R, which is added artificially.",59,60
3218,2598164,"In this way we can translate G into a cfg O* over the set of atoms of £ plus 0 and the new start symbol R, which generates the same fully labelled trees -ignoring the deviant start symbol.",25,26
3219,2598164,"In this way we can translate G into a cfg O* over the set of atoms of £ plus 0 and the new start symbol R, which generates the same fully labelled trees -ignoring the deviant start symbol.",39,40
3220,2598164,"Next we reduce the unary grammar to an ordinary cfg G ~* in the way described above, with an artificially added start symbol R. This grammar is completely isomorphic to a transition network alias directed graph with single source R and single sink 0.",24,25
3221,201684131,"d j ) , its imme- diate c[ommand]-string is ics(n) = d 1 • • • d i n. The augmented c[ommand]-string acs(n) of n is recursively defined as shown below, where ↑ is a distinguished symbol: Licensing conditions are then formalized as constraints on the shape of permissible c-strings.",39,40
3222,201684131,"δ(q, σ ↑ σ 1 σ 2 • • • σ n−1 )( σ n ))) where q ∈ F and, as previously defined at the beginning of §2.2, the use of σ i with some symbol σ (n) i is a shorthand for σ i (x i 1 , . . . ,",43,44
3223,203693150,A related concept to tails and tail-equivalency is the contribution of a symbol a ∈ Σ relative to a string w ∈ Σ * with respect to a function f .,14,15
3224,203693150,"DSFSTs are also deterministic on the input, such that each state has at most one outgoing transition per input symbol.",20,21
3225,203693150,"Informally, this will be an onward DSFST in which the non-initial and non-final states represent the most recent k − 1 tier symbols written thus far, meaning that this is the only information that will dictate what the DSFST writes upon reading the next input symbol.",51,52
3226,203693150,"represents any symbol that is not s or S relies on certain properties that hold when k = 2, but not necessarily for greater values of k. These are outlined below.",2,3
3227,203693150,"If at any point some symbol gets removed from Θ, the set Keep is immediately emptied.",5,6
3228,10569516,Any word not contained in the lexicon is replaced by a special out-of-vocabulary symbol.,17,18
3229,10569516,"The OOV rate for the LM training set corresponds to the proportion of words replaced by the OOV symbol in the LM training data, i.e., words that were not included in the recognition vocabulary.",18,19
3230,49189287,"To extend the order-1 model to an order-2 model, we transform the unigram label sequence into a bigram label sequence y 0 y 1 , y 1 y 2 , • • • , y T −1 y T , where y 0 is a special START symbol.",48,49
3231,7262042,The beginning symbol must be followed by a sentence or a symbol and must not have any preceding sentences/symbols.,2,3
3232,7262042,The beginning symbol must be followed by a sentence or a symbol and must not have any preceding sentences/symbols.,11,12
3233,7262042,The end symbol must be preceded by a sentence or a symbol and must not have any following sentences/symbols.,2,3
3234,7262042,The end symbol must be preceded by a sentence or a symbol and must not have any following sentences/symbols.,11,12
3235,7262042,"The following equations represent these constraints: ∑ i a 0,i = 1 ∑ i a i,0 = 0 ∑ i a n+1,i = 0 ∑ i a i,n+1 = 1 Each sentence in the summary must be preceded and followed by a sentence/symbol.",47,48
3236,224803588,"Moreover, we see that the test results of the best validation model (red symbol) are always below those of the ensembles' performance.",15,16
3237,15652746,"/%-,5 134/,5 UCB1 = v i + C log N n i n i Syntactic rule S(start symbol) is assigned to the root node.",17,18
3238,14488781,"Moreover, in student answers data, we found that the symbol A (such as in bulb A and node A) typed in lowercase was incorrectly labeled as a determiner 'a' by the POS tagger.",11,12
3239,178954,"In this grammar, a nonterminal symbol corresponds to a certain textual unit such as section, paragraph, and sentence; a terminal corresponds to an internal representation of a certain text fragment, typically sentence or clause.",6,7
3240,178954,"From a start nonterminal symbol, a grammar nondeterministically produces a derivation tree, which represents a concrete text structure enough to produce the corresponding surface string.",4,5
3241,178954,A grammar is augmented; it means that a nonterminal symbol can take a bundle of parameters.,10,11
3242,178954,"During derivation, these parameters can convey any information from a nonterminal symbol to others, and also can control rule application.",12,13
3243,178954,"The input of story generation is a three-tuple: a story grammar, a start nonterminal symbol, and an initial specification (a bundle of parameters) given to the start nonterminal for derivation.",18,19
3244,178954,Each nonterminal symbol knows what parameters are required for derivation.,2,3
3245,7707558,"Evaluating Computational Creativity While the concept of the Turing Test -the behaviouralist assessment of a symbol manipulating system sheerly on the basis of its output -has captured the popular imagination, the field of Computational Creativity has probably since its inception been concerned not only with the evaluation of artefacts produced by machines but also with the perception of the machine itself as a producer. (",15,16
3246,7707558,"For one thing, the model derives its features from an unsupervised traversal of a corpus, so the semantic relationships which it captures are discovered without the human dictated assignment of symbol manipulating rules.",32,33
3247,10258571,"The brackets that enclose the right part of the rule mean that the two non-terminals are expanded in the same order in the input and output lan-guages, whereas the rules with pointed bracketing expand the left symbol into the right symbols in direct order in the input language and in reverse order in the output language.",41,42
3248,10258571,"Let s j i be the phrase that contains the source sentence words from position i to j. Then, we define δ ij (A) = max t Pr(A * = ⇒ s j i /t) (3) as the maximum probability of any parse tree that yields the bilingual string s j i /t from the non-terminal symbol A, where t is any target language sentence.",64,65
3249,10258571,"For all A ∈ N and i, j such that 0 ≤ i < j ≤ |s|, j − i ≥ 1, (4) δ ij (A) = max(δ [] ij (A), δ ij (A), max t Pr(A → s j i /t)) (5) where δ [ ] ij (A) =                max B,C∈N i<I≤j Pr(A → [BC])δ iI (B)δ Ij (C) if j − i > 1 0 otherwise (6) δ ij (A) =                max B,C∈N i<I≤j Pr(A → BC )δ iI (B)δ Ij (C) if j − i > 1 0 otherwise (7) Figure 2 : Algorithm for computing the maximum probability of all the derivations that yield the sentence pair s/t from the non-terminal symbol A, given s. Now we define τ ij (A) = argmax t (Pr(A ⇒ * s j i /t)) (8) as the target language phrase t that maximizes the probability of derivation from the non-terminal symbol A to the bilingual string (s j i /t).",191,192
3250,10258571,"For all A ∈ N and i, j such that 0 ≤ i < j ≤ |s|, j − i ≥ 1, (4) δ ij (A) = max(δ [] ij (A), δ ij (A), max t Pr(A → s j i /t)) (5) where δ [ ] ij (A) =                max B,C∈N i<I≤j Pr(A → [BC])δ iI (B)δ Ij (C) if j − i > 1 0 otherwise (6) δ ij (A) =                max B,C∈N i<I≤j Pr(A → BC )δ iI (B)δ Ij (C) if j − i > 1 0 otherwise (7) Figure 2 : Algorithm for computing the maximum probability of all the derivations that yield the sentence pair s/t from the non-terminal symbol A, given s. Now we define τ ij (A) = argmax t (Pr(A ⇒ * s j i /t)) (8) as the target language phrase t that maximizes the probability of derivation from the non-terminal symbol A to the bilingual string (s j i /t).",236,237
3251,249194,"Given a tree t associated to a string x 1|x| , a constituent c A ij is defined by a nonterminal symbol (or syntactic tag) A that spans the substring x ij .",21,22
3252,6797639,"The rules of the bracketing ITG are as follows: A → ⟨Y /Z⟩, A → [Y /Z], A → f i /e j , A → f i /ϵ, and A → ϵ/e j , where A, Y , and Z are non-terminal symbols, f i and e j are terminal strings, ϵ is a null symbol, ⟨⟩ denotes the inversion of two phrase positions, and [] denotes the reversion of two phrase positions.",67,68
3253,10793689,"2016) , we replaced symbol A/B/C with bulb A/B/C. Similarly, X/Y/Z was replaced by switch X/Y/Z. We used this domain knowledge based on the notes found in student-answers training data description file.",5,6
3254,51986388,"In cases where there was a ""PROPN"" symbol, our method simply replaced it by the correct proper noun in the original fragment.",9,10
3255,207902813,"At each training step, the model can either write a symbol to the output sequence, or move the attention pointer to the next state of the sequence.",11,12
3256,51987182,"However, at each step of the training, the model can either write a symbol to the output sequence, or move the attention pointer to the next state of the sequence.",15,16
3257,15684328,"Definition 2: ""Positive"" and ""Negative"" attribute knowledge If a CFG-rule is allowed to reduce to a non-terminal symbol while a piece of attribute knowledge attached to the rule is satisfied, we say this piece of attribute knowledge in this CFG-rule is ""positive"" (briefly, ""P"").",26,27
3258,13977340,The hash-tags and symbol in the tweets are kept because of the sentiment expressing property.,5,6
3259,2088698," We conducted experiments involving various initialization strategies: scattering boundaries at random throughout the text, starting from entirely unsegmented state, or considering each symbol of the text to be a separate token.",26,27
3260,218974261,"In addition to the formal morphemes, we considered the symbol morphemes.",10,11
3261,15722076,A constituent c A ij is defined by the nonterminal symbol (either a syntactic label or a POS tag) A and its span ij (the starting and ending indexes which delimit the part of the input sentence encompassed by the constituent).,10,11
3262,21716374,And there is a prefixed symbol either '+' or '-' depends on the direction of the relative relationships between temporal entities.,5,6
3263,21716374,"In other words, the relValue should be start with '+' symbol if the meaning of text is later than the reference date, and vice versa.",13,14
3264,21716374,"However, in a special case such as the last example in Table 1 , there is no symbol to prefix because the meaning of text exactly pointed the specific date or time.",18,19
3265,21716374,"Small text nearby the arrow, either '1' or '*' symbol, is a cardinality of the node where '1' means a single child node must be appeared Conclusion In this paper, we supplemented the annotation language to reflect relative temporal information and presented an extended dataset-Korean TimeBank v2.0.",14,15
3266,15667945,"In a notation more familiar to linguists, the result amounts to showing that every r. e. subset of Σ + can be generated by some generative grammar using a symbol vocabulary V = Σ ∪ N in which all rules have the form 'xW → W y' for specified strings x, y ∈ V * and some fixed W ∈ V * .",30,31
3267,15667945,"This amounts to showing that every canonical subset of Σ + can be generated by (what would later be called) a generative grammar using a symbol vocabulary V = Σ ∪ N in which all rules have the form 'W xZ → W yZ' for specified strings x, y ∈ V * and fixed W, Z ∈ V * .",27,28
3268,15667945,"These randomly generate r. e. sets of symbols by expanding an initial axiomatic string, which can be just a single symbol.",21,22
3269,15667945,In the case of pre-1990 work the starting point was apparently a start symbol; in post-1990 'minimalist' work it is a numeration: a randomly chosen multiset of categorized items from the lexicon.,13,14
3270,2556949,"For example, a suitable minimization for very infamous cold war symbol (i.e., the Berlin wall) is cold war symbol, i.e., we consider cold as essential to the meaning of the constituent and very infamous as overly specific.",11,12
3271,2556949,"For example, a suitable minimization for very infamous cold war symbol (i.e., the Berlin wall) is cold war symbol, i.e., we consider cold as essential to the meaning of the constituent and very infamous as overly specific.",22,23
3272,2556949,"In our example, if {cold war} ∈ D, we obtain cold war symbol.",16,17
3273,2556949,"To generate the set of PSS, we enumerate all syntactically valid subconstituents of I. For example, infamous symbol or cold infamous war are syntactically valid, whereas very symbol or very cold war are not.",19,20
3274,2556949,"To generate the set of PSS, we enumerate all syntactically valid subconstituents of I. For example, infamous symbol or cold infamous war are syntactically valid, whereas very symbol or very cold war are not.",30,31
3275,2556949,The set of PSS for very infamous cold war symbol contains 22 entries.,9,10
3276,14399076,"Since transitions of terminal and nonterminal types can occur together at a state, terminal transitions are estimated as follows: ---+ a-ij Gk Et(ik) + Gk Ent(~) (3) For nonterminal transitions: E,,,( q) (4) And for pop transitions, notice that only pop transitions are possible at a pop state: L aq _ Go,(q) Y~k Epop( ik ) (5) For a terminal transition ~ and a word symbol w: Ct ~.,.",89,90
3277,8868251,Each node in the DAG is labelled with a sort symbol (or type) corresponding to the category of the linguistic object.,10,11
3278,208527419,"2019) , symbol ""@"" and ""$"" are used to mark the entity boundary, which indicate the entity positions in a sample and distinguish different relation samples sharing the same sentence.",3,4
3279,208527419,"In our experiments, the entity start symbol is selected to represent an entity typing sample.",7,8
3280,10373325,"Each ele-mentary prediction includes a predicate symbol, a label (or 'handle', prefixed to predicates with a colon in Fig.",8,9
3281,1961985,The chunk weight is transmitted to each phoneme symbol.,8,9
3282,5888548,"We define the verb 1 i is an allomorph of ka and lul is an allomorph of ul 2The symbol ""-"" in the Korean sentence represents the morpheme boundary.",19,20
3283,5401747," # sentences with a time-slot tag value phological analysis error are presented as the symbol ""---"".",17,18
3284,129939336,Recurrent punctuation marks such as .../SE + .../SE in the word are also merged into a single symbol.,19,20
3285,735269,"Methods are applied cumulatively; for example, symbol 'M 1-6' means the version of a treebank to which method 1, 2, 3, 4, 5 and 6 are applied cumulatively.",8,9
3286,2680887,/Yinaertion is the cost of a insertion-error for a nonterminal symbol.,12,13
3287,2680887,"~dele,~ is the cost of a deletion-error for a nonterminal symbol. •",12,13
3288,2680887,is the cost of a mutation-error for a nonterminM symbol.,11,12
3289,2680887,"a <~ ~deletion <= Oeinsertion ~ ~mutation ~deletion ~ ~insergiort where ~ is the error cost of a terminal symbol,/~ is the error cost of a nonterminal symbol. •",29,30
3290,15248370,Lines ending with an @ symbol were concatenated with the next line.,5,6
3291,5293733,"The parsing algorithm is based on the definition of: δ ijkl (A) = Pr(A * ⇒ x i+1 • • • x j /y k+1 • • • y l ), as the probability that the non-terminal symbol A simultaneously generates the substrings x i+1 • • • x j and y k+1 • • • y l .",43,44
3292,5293733,Increasing the number of nonterminal symbols in the SITG Note that the SITG described in Section 4.1.1 was very restricted since only one non-terminal symbol should be modeling the structural relations of both strings.,26,27
3293,5293733,"We generated all the syntactic rules (direct and inverse) that could be generated with a fixed number of non-terminal symbols, except for one non-terminal symbol that only generated lexical rules.",31,32
3294,2275946,"Two such examples are: (choice, probate, executor, chosen, certificate, testator, will) and (numeral, monarchy, monarch, crown, significance, autocracy, symbol, interpretation) .",34,35
3295,203688501,Let us group these parameters under the symbol Θ. Each Θ identifies some stochastic language L Θ ∈ C A .,7,8
3296,203688501,"As mentioned, a single PDFA M defines a class of stochastic languages given by possible parameter values of M. In this case, it is wellknown how to find Θ. Essentially, each transition probability T (q, σ) equals the relative frequency that symbol σ is emitted at a state q (Vidal et al.,",47,48
3297,203688501,"2016) , in which machine learning models competed to best predict the next symbol in a natural and artificial sequences was won by Shibata and Heinz (2016) , who integrated SP-style representations into a neural network.",14,15
3298,203688501,"The Co-emission Probability Given a Prefix It is useful to consider the co-emission probability of the symbol σ given the prefix σ 1 • • • σ i−1 , which we denote Coemit(σ, i).",20,21
3299,203688501,"Then it follows from Definition 1 that the probability that a symbol σ is emitted after the product machine 1≤j≤K M j reads the prefix σ 1 • • • σ i−1 is the following: Coemit(σ, i) = K j=1 T j (q(j, i), σ) σ ∈Σ K j=1 T j (q(j, i), σ + K j=1 F j (q(j, i)) (4) To simplify the notation and analysis, we assume that there is a end marker ∈ Σ which uniquely occurs at the end of words.",11,12
3300,203688501,For example suppose each had exactly one state with self-loop transitions for every symbol in Σ. The co-emission product M 1 M 2 does not uniquely factorize though the above theorem establishes its convexity.,15,16
3301,6398219,Every terminal and non-terminal symbol in the syntactic tree under construction has a set of features.,6,7
3302,6398219,Every symbol that is created and is not eliminated by an overriding pattern is retained even if it does not form part of a correct sentence's syntactic tree.,1,2
3303,202544040,"16) Assuming that ϕ ¨ψ1 , we have: WK MBPϕ D WK ψ 1 ¢ ψ 2 ψ 1 WK MBPϕ F WK ψ 1 ¢ ψ 2 (17) Assuming that ϕ ~ψ1 , we have: SK MBPϕ D SK ψ 1 ¢ ψ 2 2ϕ E ψ 1 WK MBPϕ F WK ψ 1 ¢ ψ 2 ψ 1 The symbol standardly refers to the univalent proposition denoting 1 in all models.",66,67
3304,243865356,"Therefore, we use the symbol E to represent a series of generalized pre-trained models.",5,6
3305,203692828,"The SUBSEQ class is exactly described by subsequetial finite-state transducers (SFSTs), or determinstic finite state machines that output a string for each input symbol and upon ending on a state (Schützenberger, 1977; Mohri, 1997) .",28,29
3306,203692828,"Intuitively, the ISL class is exactly those functions for which, for any symbol in the string, its output is entirely decided by the preceding k − 1 symbols in the input.",14,15
3307,203692828,"1 is exactly one such SFST: its states represent the previous 1 symbol in the input-state 0 represents a preceding a in the input, and state 1 a preceding b. The function is thus ISL 2 .",13,14
3308,203692828,"This is OSL 2 , because (either way) whether or not an input a is output as b depends on the immediately preceding output symbol.",26,27
3309,203692828,1 all transitions with the same input symbol lead to the same state whereas in Fig.,7,8
3310,203692828,"For each symbol γ ∈ Γ, we set γ i (x) = ϕ w 1 (x) ∨ ϕ w 2 (x) ∨ ... ∨ ϕ wn (x), where w 1 , w 2 , ..., w n is the exhaustive set of strings w j = q j σ j for which a transition δ(q j , σ j ) = (v j , r j ), exists where γ = v j (i).",2,3
3311,198189346,"Obligatoriness, since the only factor involved is a single symbol, can be expressed as the complement of an SP stringset, it is co-SP, as well as co-SL.",10,11
3312,198189346,P w σ -is the set of positions in w at which the symbol σ occurs.,13,14
3313,218974053,"The modified CBOW algorithm In the annotated corpus used for training the embedding models, word (and punctuation) tokens are followed by an arbitrary number of special tag tokens marked by a symbol as shown in Figure 1 .",34,35
3314,218973882,"Our experience was that the transformer model implemented in the Marian toolkit often simply quit generating output at the point it encountered an unknown character (hence an unknown token) in the input, in other cases it substituted some other symbol for it.",42,43
3315,8282618,The PCFG is defined as follows: (1) The start symbol is TOP. (,12,13
3316,24527658,During testing we use argmax to select the most probable output for each step j and run each decoder until the first end of sentence (EOS) symbol.,28,29
3317,2780011,"were on sale, the adjective tasty cannot be directly connected with the S node introduced by that; there is an intervening NP symbol that has not yet been predited in the structure.",25,26
3318,2780011,"Given that one step of the derivation process is a combination of a left context and an elementary tree, we have that the rightmost symbol of the left context and the leftmost anchor of the elementary tree (the current input word) must be adjacent in the sentence.",25,26
3319,2780011,"Since no new symbol can intervene between the rightmost symbol of the left context and the leftmost anchor of the elementary tree (the current input word), the elementary tree must be extended in ways that do not alter such linear order of the terminal symbols.",3,4
3320,2780011,"Since no new symbol can intervene between the rightmost symbol of the left context and the leftmost anchor of the elementary tree (the current input word), the elementary tree must be extended in ways that do not alter such linear order of the terminal symbols.",9,10
3321,2780011,"In order to satisfy such a constraint, the elementary tree has to be leftanchored, i.e. the leftmost symbol of the elementary tree, but possibly the foot node in case of a right auxiliary tree, is an anchor.",19,20
3322,5434183,Each edge of the network has associated an input symbol and an output string.,9,10
3323,5434183,Every time an input symbol is accepted the corresponding string is output and a new state is reached.,4,5
3324,231985794,XLM-RoBERTa's vocabulary was trained in a similar fashion but with 250k units and a special start symbol (Unicode lower eights block) instead of continuation symbols.,19,20
3325,231985794,Each word is prefixed with this start symbol before it is tokenized into one or more subword units.,7,8
3326,231985794,We indicate the proportion of words starting with a standalone start symbol along with other tokenization statistics in Table 2 .,11,12
3327,235097667,Note that we concatenated these elements as sequences for each side by a separator symbol <SEP>.,14,15
3328,235097667,"The combinations of Flu, Rel, and QS are represented by the + symbol.",14,15
3329,6919351,"Following (Kracht, 2011) , a grammar consists of a finite signature (F, Ω) of function symbols (Ω : F → N assigns an arity to the symbols), together with an interpretation that interprets each function symbol f as an Ω(f )-ary function on the space of signs, (see also Hodges 2001) .",44,45
3330,6919351,"If these are granted, each function symbol f then gives rise to a pair of functions (f ε , f µ ), where f ε is an Ω(f )-ary function on strings and f µ an Ω(f )-ary function on meanings.",7,8
3331,202677317,We use a special dummy symbol for the intent and DA of the current turn.,5,6
3332,682684,"To overcome this we marked all compound parts but the last, with the symbol '#'.",14,15
3333,10310211,"In our approach, a sequential pattern is defined as a sequence S(s 1 s 2 … s i … s n ) where s i can be a word, a POS tag, or a symbol denoting either a comparator ($C), or the beginning (#start) or the end of a question (#end).",37,38
3334,10310211,"For any given comparative question and its comparator pairs, comparators in the question are replaced with symbol $Cs.",17,18
3335,1456583,The average length of a morph is such that a word corresponds to 2.52 morphs including a word break symbol.,19,20
3336,1456583,"In average, there are 2.37 morphs per word including the word break symbol.",13,14
3337,244008458,is a symbol for element-wise multiplication and σ is a sigmoid activation function for normalizing ranges of gate outputs.,2,3
3338,208163639,These words are usually replaced with a special unknown symbol.,9,10
3339,52012533,"The words that appeared less than five times in the En-Jp training data and those less than twice in the English side of the En-Bg and En-Ro training data were replaced with the special symbol ⟨UNK⟩. In the training, all words were lowercased by lowercase.perl 8 , and long sentences with over 50 words were filtered out.",40,41
3340,15908417,The symbol not stands for negation as failure; not L i means that L i is not known.,1,2
3341,15908417,The symbol ← stands for an implication.,1,2
3342,15908417,The expression on the left-hand side of this symbol is called the head of the rule and may consist of a disjunction (or) of literals.,10,11
3343,2725045,"The symbol adv is replaced by one of three adverbs } #} #~(often), (usually), or (always). [",1,2
3344,2725045,"The symbol ""1"" in the left-hand side of Table 2 indicates that the corresponding annotator tagged to instances, and the ""0"" indicates not tagged.",1,2
3345,2725045,"The symbol ""e 1 ⇒ e 2 "" in Table 6 indicates the case where the head element of e 1 is located nearer to the beginning of the a. the head element of e 2 is an ancestor of the head element of e 1 .",1,2
3346,2725045,"The symbol ""inter-sentential"" indicates the case where two head elements appear in different sentences.",1,2
3347,14861435,The string before the symbol := is the type identifier (noun-lex).,4,5
3348,12190874,"Each tree node contributes to one frequency count of the corresponding CFG rule with the parent's symbol as the LHS, and the symbols of its daughters as the RHS.",17,18
3349,52013644,"The user may enter any number of subordinators or subordinator-matrix adverb pairs, including an orthographic representation and a predicate symbol for each.",22,23
3350,34175343,"This is not always the case, however, and so the 1 An accent on a symbol means that it corresponds in translation to the unaccented symbol.",17,18
3351,34175343,"This is not always the case, however, and so the 1 An accent on a symbol means that it corresponds in translation to the unaccented symbol.",27,28
3352,10580095,The concept and category information is stored in corresponding attribute values; the pipe symbol (|) is used to separate multiple categories.,14,15
3353,2432242,"We rely on the fact that in some 5-10% of cases, company names are marked with a stock market ticker symbol, as in: • Bear Stearns Cos. (",24,25
3354,226262239,All of our codings reinforce NLP inputs by reconstructing the character/symbol sequence of a word in various ways with a new alphabet.,12,13
3355,120651699,"Each constituent of a s-construction is marked with a symbol /I or /O, stating whether the situated instance should be present before and whether it will be modified.",11,12
3356,16280869,"It can be specified that patterns are completed by a dummy symbol if the context of occurrence of the verb does not provide enough linguistic elements to fill the specified window length, for example, at the end of a sentence.",11,12
3357,16280869,"By default, no dummy symbol is used. •",5,6
3358,235097694,"A Lexicon creation details The one-layer LSTM that generated nonce words had a 27-symbol alphabet ('a'-'z' and the start/stop token), embedding dimension 8, and hidden size 128.",17,18
3359,18981789,"Linguistic words may contain one or more syllables and are also separated by the same symbol, ""tsheg"", thus the language is lacking word boundaries.",15,16
3360,53093455,"In this paper, we consider two POS taggers: a symbol-refined generative HMM tagger (SR-HMM) (Huang et al.,",11,12
3361,18343565,"A vertical bar, the 'pipe' symbol, is used to create a link while labelling it with a different name on the current page.",8,9
3362,249890268,"The following guidelines are provided for example purposes only: a) The chapter name is preserved as the top title, followed by a distinctive symbol, making it easier to distinguish between chapters.",26,27
3363,249890268,"b) Within a chapter, many sub-topics are separated by another unique symbol, such as a double semi-colon ';;'.",15,16
3364,249890268,"2 , for example, (1) Defects and (2) Crystal structure c) If there is a further subtopic within a subtopic, it is separated by a symbol such as ':' followed by some relevant points.",32,33
3365,14256751,Multi-tapping as basic encoding method for short message sending (SMS) on cellular phones uniquely addresses a symbol by a predefined number of button hits in a row.,20,21
3366,14256751,"Nevertheless, the presented system called UKO-II (Harbusch and Kühn, 2003) is adaptive with respect to the number of keys, i.e. the system can be tailored to any number of keys where the symbol distribution is matter of definition.",39,40
3367,15322018,Thus we get two-level segmentation as follows lexicaltape: * 0 0 0 * + i n g + s surface tape: 0 p a r O O i n g 0 s The special character 0 represents the null symbol (i.e. the surface form would be paringswithout the nulls).,43,44
3368,26322548,"Experiments for CCG-grounded analysis were performed using automatically assigned POS-tags that are generated by a symbol-refined HMM tagger (Huang et al.,",19,20
3369,14807899,"Again, bi-grams are a common approximation in order to robustly estimate the Markov Model parameters, the state transitions probabilities P (s j |s i ) and the observation symbol probability distribution P (w m |s j ) in state j. In contrast to speech recognition, the computation of the model parameters can be achieved through maximum likelihood estimation, i.e. by counting event occurrences.",33,34
3370,12483639,The most important properties el' this kind are a) the dil]erentiation between defining and applied occm'rences of variables b) the assessment of the scope of defining occurrences c) the problem of identification: finding the defining occurrences that match the applied occurrences d) the question of type compatibility Conventional compilers contain a symbol table that will serve as a basis for the investigation of the questions raised above.,57,58
3371,12483639,"Elements of the symbol table are built from objects of the types recognized by the language, supplemented with auxiliary information.",3,4
3372,12483639,"During compilation the symbol table undergoes continuous modification, problems related to static semantics are solved on the basis of the actual state of the symbol table.",3,4
3373,12483639,"During compilation the symbol table undergoes continuous modification, problems related to static semantics are solved on the basis of the actual state of the symbol table.",25,26
3374,12483639,On the basis of this specification it is possible to generate procedures for symbol table manipulation.,13,14
3375,12483639,The symbol table can then serve as a tool for deciding type compatibility.,1,2
3376,12483639,"Draw the radius of circle ~C. OBJ -search for the first occurrence of the object with the given type by tracking the symbol table backward, cf.",22,23
3377,236486291,At each iteration the pair of symbols (occurring inside words) with highest frequency is selected and substituted with a new symbol.,22,23
3378,236486291,"Let S be a corpus of words w from a vocabulary V , and let each word be decomposed as a sequence of symbols (initially characters) s from an alphabet Σ. The log-likelihood of S can be written as: L(S, Σ) = s∈Σ C S,Σ (s) log Pr(s) (1) where C S,Σ (s) is the count of symbol s in corpus S, in which words are segments according to Σ, i.e.: C S,Σ (s) = w∈V C S (w)C Σ (s, w) (2) Algorithm 1 initializes Σ with single characters (Σ 0 ).",75,76
3379,236486291,"Thus the probability mass will shift to the probability of the joint symbol, and the probability of the single elements will be greatly reduced.",12,13
3380,14066271,"Figure 1 illustrates some of the results of a run of the labeling algorithm on Python, where the verbs occurred before the symbol '@' are the input verbs and other verbs occurred after '@' are their possible hypernyms.",23,24
3381,3161610,"1 Utterances with ellipsis, disfluencies, false starts, reformulations, repetitions and ungrammaticalities and special characters such as the symbol '*' that indicates uncertainty due to noise in the communication channel.",21,22
3382,23048409,"Experiments for CCG-grounded analysis were performed using automatically assigned POS-tags that are generated by a symbol-refined HMM tagger (Huang et al.,",19,20
3383,218900599,"For instance, the needs and availabilities could be marked on a map, with each type of resource be- ing represented with a different symbol, making it easy to physically locate them.",25,26
3384,237099292,"Seq2seq When the indirect signal is a sequential output, (y 1 , ..., y M ), the downstream supervised learner takes on the form of a recurrent neural network decoder whose initial hidden state is set to the fixed length representation v. The decoder is trained to generate the output sequence by predicting the next symbol y m given the hidden state of the decoder at time m, which is computed by One potential issue with this approach is that the use of the fixed-length vector v is a bottleneck in improving the performance of this encoder-decoder architecture.",59,60
3385,7490669,"In this table, the symbol ""+"" means features of current configuration contains both the baseline features and new features for semi-supervised or transductive learning.",5,6
3386,186206312,"Preprocessing Preprocessing is done on the tweets by removing twitter handles starting with ""@"" or words that had any special symbol.",22,23
3387,186206456,"Preprocessing is done on the sentences, and Twitter handles starting with ""@"" or words that have any special symbol are removed.",21,22
3388,14928306,"Another source of errors was detecting some numeric quantities due to special symbols such as dashes, percentages, or the euro symbol, which were incorrectly interpreted as a string type (3.3% and 1.6%).",22,23
3389,4936344,"2016) , since the s symbol can easily be included as the first word of a constraint.",6,7
3390,227231121,"Only when a special symbol signifying stopping generation is predicted, we move to generate function words to the next content word.",4,5
3391,17272416,"This prediction is repeated recurrently until the end-of-sentence symbol (""nil"" in Algorithm 1) is predicted as the next symbol.",12,13
3392,17272416,"This prediction is repeated recurrently until the end-of-sentence symbol (""nil"" in Algorithm 1) is predicted as the next symbol.",26,27
3393,9001123,"Equation (1) shows the smoothed lexicon probability: P ′ (X → w) ≡ (1 − λ)P (X → w) + λP (X → w unk ), (1) where X is any pre-terminal (part-of-speech) symbol in the grammar, w is any word, and w unk is the unknown word.",53,54
3394,2163392,"Being able to relate abstract symbols to observations with physical properties in the real world is known as the physical symbol grounding problem (Vogt, 2002) ; which is recognised as being one of the main challenges for human-robot interaction and constitutes the focus of this paper.",20,21
3395,2163392,2013) tackle learning symbol grounding in language commands combined with gesture input in a table-top scenario.,4,5
3396,2163392,"However, all these approaches depend on having predefined specifications of different concepts in the environment: they either assume a pre-annotated semantic map with respect to which they ground the linguistic input or have an offline trained symbol classifier that decides whether a detected object can be labelled with a specific symbol; e.g. colour and shape in (Matuszek et al.,",40,41
3397,2163392,"However, all these approaches depend on having predefined specifications of different concepts in the environment: they either assume a pre-annotated semantic map with respect to which they ground the linguistic input or have an offline trained symbol classifier that decides whether a detected object can be labelled with a specific symbol; e.g. colour and shape in (Matuszek et al.,",54,55
3398,2163392,"Thus in order to deploy such a system, one should have access to an already trained classifier for every anticipated symbol, prior to any user interaction.",21,22
3399,2163392,"Our work builds on this method: it uses it to achieve natural language symbol grounding, as a by-product of user interaction in a task-oriented scenario.",14,15
3400,2163392,"It learns symbol grounding by exploiting the concept of intersective modification (Morzycki, 2013) -i.e.,",2,3
3401,2163392,an object can be labelled with more than one symbol.,9,10
3402,2163392,"On top of this, we learn classifiers for performing symbol grounding.",10,11
3403,2163392,Each symbol has a probabilistic model which is fit to a subset of the extracted (visual) features.,1,2
3404,2163392,"When a new instruction is received, the classifier for each symbol makes a decision regarding the object in the world (and their respective features) to which the symbol may be grounded.",11,12
3405,2163392,"When a new instruction is received, the classifier for each symbol makes a decision regarding the object in the world (and their respective features) to which the symbol may be grounded.",30,31
3406,2163392,"Input to the system are natural language instructions, together with eye-tracking fixations and a camera view of the world from above (a) Natural language instructions are deterministically parsed to an abstract plan language (b) Using the abstract plan, a set of labelled image patches is produced from the eye-tracking and video data (c) Observable predefined features are extracted from the image patches (d) Each symbol is grounded to a subset of observable features (e) the human participant is simultaneously teaching the robot how to execute a task and what properties the surrounding objects must have for that execution to be successful.",78,79
3407,2163392,"It consists of an end-to-end process, from raw linguistic and video inputs on the one hand to learned meanings of symbols that in turn are conceptually grouped: i.e., a symbol can correspond either to an object in the real world, or to a property of an object.",36,37
3408,2163392,"The abstract form we use is a list of tuples with the format (action target location) (Figure 2b ), where action corresponds to an element from a predefined set A, target corresponds to a list of terms that describe an object in the world and location corresponds to a single symbol denoting a physical location in the environment.",55,56
3409,2163392,for a blue cube the location would be the symbol blue-cube-location.,9,10
3410,2163392,"For example, the symbol on-left-of-cube is produced for location for the bottom sentence in Figure 3 .",4,5
3411,2163392,"In this way, the result of parsing is a sequence of abstract instructions-i.e., an abstract plantogether with a symbol set S, containing all symbols which are part of any target entry.",22,23
3412,2163392,"Model and Location Inference In order to solve the problem of symbol grounding, inference is performed using a generative probabilistic model, which is shown in Figure 4 .",11,12
3413,2163392,"For each symbol s we group the extracted features from each image labelled with s resulting in S lists of M s tuples with F entries in each tuple, where M s is the number of images being labelled with s; see Figure 2 (d, left ).",2,3
3414,2163392,"Symbol Meaning Learning For each symbol s ∈ S and each feature f ∈ F we fit a 1-D Normal distribution resulting in a new list of tuples with size F -s j : [(µ s j f 1 , σ s j f 1 ), . . . , (",5,6
3415,2163392,"µ s j f F , σ s j f F )] for the j th symbol.",17,18
3416,2163392,the label of an image can be associated with the wrong symbol-we process our distributions to refit them to data that falls within two standard deviations from the means of the original distributions.,11,12
3417,2163392,We are then seeking observed features f that are invariant with respect to each token use of a specific symbol s within the user instructions so far-i.e. their distributions are 'narrow' and with variance below a predefined threshold σ thresh (see Figure 5 ).,19,20
3418,2163392,"In the last step, we construct a set of the invariant features from the discovered narrow distributions for a given symbol l -(F s invar ) -and say that this set characterizes the symbol.",21,22
3419,2163392,"In the last step, we construct a set of the invariant features from the discovered narrow distributions for a given symbol l -(F s invar ) -and say that this set characterizes the symbol.",34,35
3420,2163392,The parameters for the symbol are the concatenation of the means of the features from (F l invar ) into a mean vector and the concatenation of the variances into a diagonal covariance matrix.,4,5
3421,2163392,"Overall, 48 objects were presented to the algorithm where the features for each object would fall into one of the following categories: • Previously seen features (Figure 6 (left)) • Previously unseen features, close to the features of the training data (Figure 6 (middle)) • Previously unseen features, not close to the features of the training data (Figure 6 (right)) Experimental Set up Inference over new images is performed by thresholding the probability density function (PDF) values from the model parameters for each symbol.",101,102
3422,2163392,"Thus we go over each concept group and if there are symbols yielding PDF values above a predefined threshold, we assign the new image the symbol from that group with the highest PDF.",26,27
3423,2163392,"For instance, whenever the robot observes a new object it can either label it with a symbol or deem it as unknown for a particular concept group.",17,18
3424,2163392,"Whenever a symbol is assigned, the feature model for that symbol is updated, taking into account the new data point.",2,3
3425,2163392,"Whenever a symbol is assigned, the feature model for that symbol is updated, taking into account the new data point.",11,12
3426,2163392,"If on the other hand the symbol  is unknown, the system can prompt the human for new linguistic input which together with its feature model is added to the knowledge base and allows for its future recognition.",6,7
3427,2163392,"For example, if the robot observes a new hue of blue it would update its parameters for blue to account for that; whereas if it observes a new colour (e.g. green) it would ask the human for the unknown symbol and would record it for future reference.",43,44
3428,2163392,"Refinements to our model would be necessary in order to represent more complex symbol relations, e.g. in a hierarchical fashion (Sun et al.,",13,14
3429,7663785,The non-terminal set of a standard hierarchical grammar comprises two symbols which are shared by source and target: the initial symbol S and one generic non-terminal symbol X .,23,24
3430,7663785,The non-terminal set of a standard hierarchical grammar comprises two symbols which are shared by source and target: the initial symbol S and one generic non-terminal symbol X .,31,32
3431,7663785,"During decoding, a maximum length constraint of ten is applied to all non-terminals except the initial symbol S .",19,20
3432,59766847,"However, these methods are inadequate because of the information loss due to signal-symbol conversion.",15,16
3433,59766847,"H M M -L R M e th o d Basic M echanism In standard LR parsing, the next parser action (shift, reduce, accept or error) is determined using the current parser state and next input symbol to check the LR parsing table.",41,42
3434,17934901,"-word shape: the shape of each character in the word (capital letter, small letter, digit, punctuation, other symbol) -word type: the type of the word (uppercase, digit, symbol, combination ) -Named entity: the IOB annotation for the named entity extracted from the review using Senna (Collobert, 2011).",23,24
3435,17934901,"-word shape: the shape of each character in the word (capital letter, small letter, digit, punctuation, other symbol) -word type: the type of the word (uppercase, digit, symbol, combination ) -Named entity: the IOB annotation for the named entity extracted from the review using Senna (Collobert, 2011).",38,39
3436,17934901,"-if the initial letter is uppercase, if all letters are uppercase, All letters lowercase, All letters digit, Contains a uppercase letter, Contains a lowercase letter, Contains a digit, Contains a alphabet letter, Contains a symbol.",42,43
3437,16170214,"First, we add a deletion rule which allows us to delete any arbitrary word w with any head symbol X, formally: X → ⟨w X, X⟩. (3) This rule allows our grammar an option of ignoring words that it does not know what to do with.",19,20
3438,16170214,"In an n-SCFG, the elementary structures are rewrite rules of n − 1 target sides: where X is a non-terminal symbol, γ 1 is the source side string of terminal and non-terminal symbols, and γ 2 , ...γ n are the target side strings.",26,27
3439,16170214,Terminal Rule: Equal to one if there is no nonterminal symbol in the rule.,11,12
3440,17342618,"First, we extracted every CHAT symbol in the transcript files and stored them according to their frequencies and positions in each sentence.",6,7
3441,17342618,"For example, having the CHAT symbol [//] at a specific position within a sentence implies that the patient was retracing a verbal error that precedes that position and at the same time attempting to make correction, while the CHAT symbol [/] shows the patient making immediate word repetition (MacWhinney, 2000) .",6,7
3442,17342618,"For example, having the CHAT symbol [//] at a specific position within a sentence implies that the patient was retracing a verbal error that precedes that position and at the same time attempting to make correction, while the CHAT symbol [/] shows the patient making immediate word repetition (MacWhinney, 2000) .",43,44
3443,17342618,"Each utterance is identified to start from the beginning of a verbal communication to the next verbal pause length, such as punctuation or a CHAT symbol that represents a specific break in communication (Marini et al.,",26,27
3444,53621182,"16 The Open American Corpus has been tagged 17 Tags such as CD (Cardinal Number), LS (List item marker), POS (Possessive ending), SYM (symbol) and TO (to) are not useful in the sense that they do not add any information to the word they describe; they can be automatically added with simple SED-type replacements such as s/\([0-9]*\)/\1_CD/. audienceless, autodialed, balancingly, bioremediation, barklike, etc.",33,34
3445,15373046,"The second is that we perform normalization of events to reduce their variability, namely removing all function words, replacing proper nouns with special symbol, and lemmatizing words.",25,26
3446,51715039,"We label the last token (which is typically a punctuation symbol) with the ""O"" label in PE and CRC.",11,12
3447,2467720,"In this way, the model will distinguish translation rules extracted in contexts in which the English symbol string ""[X1] record [X2]"" behaves as a verbal phrase, from contexts in which the same string acts as nominal phrase.",17,18
3448,2467720,"In SCFGs, the elementary structures used in translation are synchronous rewrite rules with aligned pairs of source and target symbols on the right-hand side: X → ⟨ s, t ⟩ (1) where X is the head symbol of the rewrite rule, and s and t are both strings of terminals and nonterminals on the source and target side respectively.",43,44
3449,2467720,"Hierarchical rules are composed of initial head symbol S, and synchronous rules containing terminals and single kind of non-terminals X. 2 Hierarchical rules are extracted using the same phrase extraction procedure used in phrase-based translation (Koehn et al.,",7,8
3450,2467720,"To address this problem, it is desirable to be able to distinguish pivotside phrases that have different syntactic roles or meanings, even if the symbol strings are exactly equivalent.",26,27
3451,2467720,"Synchronous rules of T P S and T P T take the form of X → ⟨p, s⟩ and X → ⟨ p, t ⟩ respectively, where p is a symbol string that expresses pivot-side parse subtree (S-expression), s and t express source and target symbol strings.",33,34
3452,2467720,"Synchronous rules of T P S and T P T take the form of X → ⟨p, s⟩ and X → ⟨ p, t ⟩ respectively, where p is a symbol string that expresses pivot-side parse subtree (S-expression), s and t express source and target symbol strings.",55,56
3453,2467720,"Therefore we introduce a heuristic estimation method as follows: ψ( pT | pS) = w( pS, pT ) ∑ p∈T P T w( pS, p) • max p∈T P T w( pS, p) (21) ψ( pS| pT ) w( pS, pT ) ∑ p∈T SP w(p, pT ) • max p∈T SP w(p, pT ) (22) w( pS, pT ) =    0 (f lat( pS) ̸ = f lat( pT )) exp (−d ( pS, pT )) (otherwise) (23) d( pS, pT ) = T reeEditDistance( pS, pT ) (24) where f lat(p) returns the symbol string of p keeping non-terminals, and T reeEditDistance( pS , pT ) is minimum cost of a sequence of operations (contract an edge, uncontract an edge, modify the label of an edge) needed to transform pS into pT (Klein, 1998) .",139,140
3454,3221856,"We also have a special symbol ⊥ that indicates when a particular slot is not filled: e.g., a nonargumentative unit (b = O) has neither component type, nor relation, nor relation type.",5,6
3455,3221856,"The label of this link is O, the symbol for non-argumentative units. •",9,10
3456,53584863,The system allows representing a consonant and a vowel together as a symbol.,12,13
3457,53584863,Each symbol represents a CV syllable.,1,2
3458,13861819,"Given that the result of a die roll is otherwise associated with randomness, it is somewhat surprising that it would catch on as a symbol for summarizing reviews -something one would typically hope to construe as a well-informed and deliberate judgment of merit and quite the opposite of chance or luck.",25,26
3459,233004564,The process is repeated until a specified end-symbol token is drawn at the K th step.,9,10
3460,5400629,We also reserved a symbol for single token (S) keyphrase aspirants.,4,5
3461,9737200,We use special ST ART symbol at t = 1.,5,6
3462,6177202,This would require the development of a symbol set that is sufficiently expressive while not being overly ambiguous.,7,8
3463,13226696,"The probability distribution for the next target symbol is computed by p(y t = k|ỹ <t , X) ∝ exp(W s z t + b t ).",7,8
3464,13226696,"The vocabulary size is limited to 50,000 excluding the special boundary symbol and the unknown word tag.",11,12
3465,13226696,The beginning and end of each passage are also padded with a special boundary symbol.,14,15
3466,24337225,"There are three types of scoring functions based upon the cardinality of the slot fillers: (1) single, (2) enumerated and (3) range, s An example of an ordered domain with single fillers is that of TEMPERATURE: (make-frame TEMPERATURE (instance-of (value field)) (database-in (value z)) (element-type (value symbol)) (domain-type (value ordered)) (cardinality (value single)) (elements (value cold cool tepid lukewarm warm hot scalding))) 6l_nt'orrnal feedback thus far has indicated that these values are geared to having more emphasis placed on the records that contain easier fields and less on the harder ones, thus not rewarding those who perform well on the harder fields.",75,76
3467,12161321,"Taking into account the Polish declension paradigm, we also added a basic metric based on the longest common prefix, calculated as follows: CP δ (s, t) = ((|lcp(s, t)| + δ)  /(|s| • |t|), where lcp(s, t) denotes the longest common prefix for s and t. The symbol δ is a parameter for favoring certain suffix pairs in s (t).",62,63
3468,11398491,"Suppose we are trying to match TEMPERATURE slots in an ILT frame and a GL entry: ILT: (temperature (value cool) ) GL: (temperature (value lukewarm)) The penalty assigned to a mismatch depends on two variables: • D: the distance between the fillers in the ordered set of values (make-frame TEMPERATURE (instance-of (value slot)) (element-type (value symbol)) (domain-type (value ordered)) (cardinality (value single)) (elements (value (cold cool tepid lukewarm warm hot scalding)))) The quality of the match depends, of course, on the distance between the two fillers; however, it is important to consider the size of the filler domain as well.",80,81
3469,227126558,"For example, for the word ""смайлик"" (emoji) the predicted candidates are completely unsuitable (person, device, flatterer, hypocrite, visual materials) in comparison with the fastText prediction and correct hypernyms (graphical sign, image, symbol).",45,46
3470,227126558,"Extracted nearest neighbours can be semantically related words but not necessary cohyponyms: • delist (WordNet); expected senses: get rid of; predicted senses: remove, delete; • хэштег (hashtag, RuWordNet); expected senses: отличительный знак, пометка (tag, label); predicted senses: символ, короткий текст (symbol, short text).",62,63
3471,7126458,"The general form of the auxiliary symbols is direction:parent[head]next|previous where direction is either L, M, or R, parent is the symbol on the left hand side of the rule, head is the head on the right hand side of the rule, next is the symbol which will be generated next, and previous is the symbol that was generated before.",25,26
3472,7126458,"The general form of the auxiliary symbols is direction:parent[head]next|previous where direction is either L, M, or R, parent is the symbol on the left hand side of the rule, head is the head on the right hand side of the rule, next is the symbol which will be generated next, and previous is the symbol that was generated before.",51,52
3473,7126458,"The general form of the auxiliary symbols is direction:parent[head]next|previous where direction is either L, M, or R, parent is the symbol on the left hand side of the rule, head is the head on the right hand side of the rule, next is the symbol which will be generated next, and previous is the symbol that was generated before.",62,63
3474,7126458,"Each rule contains information about the parent, the head, and (usually) three child symbols (which may include an imaginary boundary symbol).",25,26
3475,7126458,"The Berkeley parser takes an automated approach, in which each constituent symbol is split into subsymbols applying an expectation-maximization method.",12,13
3476,7126458,"The combination of the two radically different approaches (linguistically motivated grammar and automatic symbol splitting) is a rather promising area of research for improving parsing accuracy, which we plan to address in future work.",14,15
3477,51875130,"Given a sequence of words in a sentence: ..., w i−2 , w i−1 , w i , w i+1 , w i+2 , ... and the current word in consideration is w i , we used the following features: -The word w k itself -If w k is upper case -Shape and Short shape (where same consecutive characters in the shape are compressed to a single character) of w k -If w k contains any special symbol like: • If w i is ,#,$,-,,,etc.",82,83
3478,51871806,They also use language independent phones by modeling them jointly and merging them on the basis of the associated IPA symbol.,20,21
3479,2654931,"In order to differentiate question q and answer a sentences, we insert a special symbol, <S>, after the question sequence.",15,16
3480,51877436,"Due to the nature of RNNs, the network assigns one language variant per input symbol, and thus per character -but the task is to predict a tag for each word.",15,16
3481,18254573,"where pM S (c) indicates the prevalence of class c in set S as estimated via method M (the ""hat"" symbol indicates estimation).",25,26
3482,51880483,"Named entity token identification Only the tweets containing named entities were extracted from the data from the previous codeswitched workshop, and a CRF based model was built using these tweets with different features (local context, suffix, prefix, all-upper-case, startswith-upper-case, and hash symbol) and applied to the current shared task's training, development and test data to get named entity information for each token.",56,57
3483,12816813,"news to inform about important events concerning companies, e.g., to support trading (i.e., buy or sell) the corresponding symbol on the next day, or managing a portfolio.",23,24
3484,12816813,"Initially, a query consists exclusively of the ""symbol"", i.e., the abbreviation of the name of a company as it is listed on the stock market.",9,10
3485,12816813,"First, the company symbol is expanded with terms relevant for the company, either directly -e.g.,",4,5
3486,12816813,We detail our symbol expansion algorithm in Section 3.,3,4
3487,12816813,"Query Expansion In company-oriented summarization query expansion is crucial because, by default, our query contains only the symbol, that is the abbreviation of the name of the company.",21,22
3488,12816813,"Unfortunately, existing query expansion techniques which utilize such knowledge sources as WordNet or Wikipedia are not useful for symbol expansion.",19,20
3489,12816813,"For every company symbol in our collection, we download its business summary, split it into tokens, remove all words but nouns and verbs which we then lemmatize.",3,4
3490,12816813,"Table 1 shows that this approach succeeds in expanding the symbol with terms directly related to the company, e.g., ipod for Apple, but also with more general information like the industry or the company operates in, e.g., software and computer for Apple.",10,11
3491,10032144,Number Replacement (NMR) Number replacement is a particular approach that transforms all numbers into a dummy symbol.,18,19
3492,155630917,"27 hitsuyou = ""necessary"" 28 The symbol 100 %* stands for the matching of 100 % of the two translation outputs with synonymy information.",8,9
3493,227231492,"Among the 41 nonliteral translations predicted as literal, the error ratio for each non-literal translation technique is as fol- 11 The symbol '@@' is automatically added by the tool fastBPE after BPE tokenization.",24,25
3494,15141967,"constrained unification of structures, only true if a feature is present in both structures, but may not be added (symbol ""=c""), 3.",22,23
3495,15141967,"obligatory presence of a feature (symbol ""~""), 4.",6,7
3496,15141967,obligatory absence of a feature (symbol tilde).,6,7
3497,15141967,"obligatory difference between two values (symbol ""#""), 2.",6,7
3498,15141967,"disjunction of obligatory differences (a sequence of obligatory differences separated by the symbol ""1"") (this can also be viewed as the negation of a conjonction of obligatory presences) 3.",13,14
3499,236477449,"We investigate if, given a simple symbol masking strategy, self-attention models are capable of learning nested structures and generalise over their depth.",7,8
3500,236477449,"In the model each input symbol is associated with a vector embedding of size K. A sequence of opening and closing brackets is represented by a matrix of size (N, K).",5,6
3501,236477449,Then a softmax classifier is employed to predict the symbol at the current position.,9,10
3502,236477449,"Hence, we use a masking strategy to train and test the model (otherwise it could simply use the current symbol for prediction).",21,22
3503,236477449,"Looking at the behaviour of the attention heads we note that the first layer in the (2,8) and (4,4) models focuses its attention on the previous symbol.",30,31
3504,236477449,"This analysis is compatible with the (8,2) model using a symbol counting method.",12,13
3505,236477449,"In summary, the (4,4) model appears to first look at the previous symbol in the sequence.",15,16
3506,236477449,"First it looks at the previous symbol, then all around the sequence.",6,7
3507,236477449,"2020) investigated self-attention models using Dyck languages, and claimed that self-attention models with a starting symbol are able to generalise to longer sequences and deeper structures without learning recursion, as competitive LSTM models do.",21,22
3508,236477449,"When a single symbol is masked, predicting the kind of missing parenthesis can be done by subtracting the number of closing parentheses by the number of opening parentheses for each type, and predict the type which exhibits a discrepancy.",3,4
3509,18001033,"As a heuristic for the IT domain, terms that occur separated by a > symbol are also collapsed.",15,16
3510,40250300,"We did not use expressions where the English expression was a noun, a proper noun, a symbol, or adjective that was modified by adding the Japanese suffixes ""na"" (for an adjective) and ""suru"" (for a verb).",18,19
3511,8764788,"It is worth noting that the first element of an expanded contraction is marked with a symbol (+) indicating that, originally, that token occurred as part of a contraction: 4 um, dois →|um|,|dois| da →|de+|a| viu-o →|viu|-o| In what concerns Portuguese, the non-trivial aspects of tokenization are found in the handling of ambiguous strings that, depending on their POS tag, may or may not be considered a contraction.",16,17
3512,17567471,"During training, we introduce an empty morpheme symbol to Chinese, in case some English morphemes (e.g., morphological suffixes) do not correspond to any Chinese character.",8,9
3513,17567471,The appearances of the symbol are removed from the decoding outputs.,4,5
3514,15034416,Figure 1 : Example of creating a confusion network from monotone one-to-one word alignments (denoted with symbol |).,21,22
3515,15034416,The symbol $ denotes a null alignment or an ε-arc in the corresponding part of the confusion network.,1,2
3516,15034416,"The alignment is shown with the | symbol, and the words of the primary hypothesis are to the right of this symbol.",7,8
3517,15034416,"The alignment is shown with the | symbol, and the words of the primary hypothesis are to the right of this symbol.",22,23
3518,15034416,"The symbol $ denotes a null alignment or an ε-arc in the corresponding part of the confusion network, which is shown at the bottom of the figure.",1,2
3519,648535,"Confusion networks A confusion network (CN) is a word graph where each edge is labeled with exactly zero or one symbol, and each path from the start node to the end node visits each node of the graph in canonical order.",22,23
3520,648535,"Within this paper, we represent a CN by a list of lists of words {w i,j }, where each w i,j corresponds to a symbol on an edge between nodes i and i + 1.",31,32
3521,648535,"Here, the x i and x i symbols will correspond to the variable with the same name or their negate, respectively, whereas will serve as an ""isolator symbol"", to avoid unwanted bigram matches or mismatches between separate parts of the constructed CN or sentences.",31,32
3522,648535,"CN-4BLEU-DECIDE It is straightforward to modify the construction above to create an equivalent CN-4BLEU-DECIDE problem instead: Replace each occurrence of the isolating symbol in A, B, C, R by three consecutive isolating symbols .",27,28
3523,817666,"All these words are also inflected, but are too short to retain stable parts longer than one symbol.",18,19
3524,16690392,"In the first (base) level, the chunker identifies two base phrases, (NP Estimated volume) and (QP 2.4 million), and replaces each phrase with its non-terminal symbol and head 1 .",36,37
3525,16690392,"In addition to the features listed in Table 1 , the chunker looks into the daughters of the current non-terminal symbol and use them as features.",22,23
3526,16690392,It also uses the words and POS tags around the edges of the region covered by the current non-terminal symbol.,21,22
3527,16690392,s represents a non-terminal symbol.,6,7
3528,16690392,h represents a head percolated from the bottom for each symbol.,10,11
3529,16690392,most common symbol and consist of 55% of all phrases.,2,3
3530,16690392,"For example, we could naturally incorporate the restriction that every chunk has to contain at least one symbol that has just been created in the previous level 3 .",18,19
3531,8884550,"Markov Logic Formulation Each discourse segment s is modeled with a constant symbol c ∈ C. The set C, therefore, models the discourse segments in the text under consideration and comprises the set of constants of the Markov logic network.",12,13
3532,15726899,"The source sentence can be generated by the grammar if the start symbol of the grammar S is found in top cell (J, 1), i.e. S ⇒ * f J 1 .",12,13
3533,15726899,"In the hierarchical approach of (Chiang, 2005) , the set of non-terminals consists of a start symbol S and a generic non-terminal X .",21,22
3534,15726899,"The target sentence is generated by the new grammar and the forced derivation procedure is successful, if the start symbol S J 1 is found in cell (I, 1).",20,21
3535,30947813,"Given the inputs R and N , we define a goal item to be an item [S, R, N ] where S is the start symbol of the grammar and N ⊇ N .",28,29
3536,6456424,"The function Function Return value x ctype δ (a, x, t) x a t(x) +δ is {U (uppercase), L (lowercase), D (digit), W (whitespace), S (symbol) } letter x position δ (a, x, t) x a t(x) +δ is at the {H (head), T (tail), S (syllable head), I (inner), W (whitespace) } of the word x char δ (a, x, t) The lower-cased letter of x a t(x) +δ x word δ (a, x, t) The lower-cased word (offset position δ) containing the letter xa t(x) x pos δ (a, x, t) The part-of-speech code of the word (offset position δ) containing the letter xa t(x) y ctype(a, y, t) ya t(y) is {N (NIL) U (uppercase), L (lowercase), D (digit), S (symbol) } letter y position(a, y, t) ya t(y) is at the {N (NIL) H (head), T (tail), I (inner)} of the word y char(a, y, t) The lower-cased letter of ya t(y) a state(a, y, t)  x diff wd measures the distance of two words. {",45,46
3537,6456424,"The function Function Return value x ctype δ (a, x, t) x a t(x) +δ is {U (uppercase), L (lowercase), D (digit), W (whitespace), S (symbol) } letter x position δ (a, x, t) x a t(x) +δ is at the {H (head), T (tail), S (syllable head), I (inner), W (whitespace) } of the word x char δ (a, x, t) The lower-cased letter of x a t(x) +δ x word δ (a, x, t) The lower-cased word (offset position δ) containing the letter xa t(x) x pos δ (a, x, t) The part-of-speech code of the word (offset position δ) containing the letter xa t(x) y ctype(a, y, t) ya t(y) is {N (NIL) U (uppercase), L (lowercase), D (digit), S (symbol) } letter y position(a, y, t) ya t(y) is at the {N (NIL) H (head), T (tail), I (inner)} of the word y char(a, y, t) The lower-cased letter of ya t(y) a state(a, y, t)  x diff wd measures the distance of two words. {",210,211
3538,5898104,Each of the source and target phrases is a sequence of words and gaps (indicated by the symbol ); each gap acts as a placeholder for exactly one unspecified word.,18,19
3539,1361096,"The symbol ""X"" indicates a ""don""t care condition"", meaning that this value does not have any impact on the result.",1,2
3540,215513848,Solving the visual symbol grounding problem has long been a goal of artificial intelligence.,3,4
3541,14406864,"However, the similarity among the languages is often obscured by the attitude of the speakers since language is considered as a symbol of identity (Lanza and Woldemariam, 2008; Smith, 2008) .",22,23
3542,248780394,"A variable is usually defined as a symbol standing as a referent for a set consisting of at least two elements (Philipp, 1992) .",7,8
3543,248780394,"Given their nature of abstract and dynamic referents, two aspects make the representation of variables particularly challenging in the field of NLP: (i) The meaning of a variable is exclusively determined by its context (Schoenfeld and Arcavi, 1988) , a variable carries no meaning when considered in isolation, behaving unlike any word in English; (ii) The same variable symbol (e.g., the variable x) can be reused in an unlimited number of sentences and expressions, possibly assuming different meanings and referring to different sets of values while keeping the same name.",69,70
3544,248780394,A variable with the symbol x can refer to completely different sets depending on its context.,4,5
3545,248780394,"Therefore, when representing a variable, its surface form (or symbol) should not interfere with its representation; the semantics of the variable should be guided exclusively by the defined type.",12,13
3546,248780394,"In order to obtain a representation that can better distinguish from symbol and abstraction of the variables, we generate a new representation E (v i −) ∈ R N ×D for each variable in the sentence, where the vectors representing the variables are all replaced by zeroes.",11,12
3547,6854952,This is because it predicts every output symbol depending on the whole input sequence.,7,8
3548,6854952,The forward sub-layer receives the encoded input words staring from the sentence start symbol up to the last word before the sentence end symbol (sequence < s > w 1 w 2 w 3 in Fig.,15,16
3549,6854952,The forward sub-layer receives the encoded input words staring from the sentence start symbol up to the last word before the sentence end symbol (sequence < s > w 1 w 2 w 3 in Fig.,25,26
3550,6854952,The forward hidden states are used to predict words starting from the first word after the sentence start symbol up to the sentence end symbol (sequence w 1 w 2 w 3 </s> in Fig.,18,19
3551,6854952,The forward hidden states are used to predict words starting from the first word after the sentence start symbol up to the sentence end symbol (sequence w 1 w 2 w 3 </s> in Fig.,24,25
3552,6854952,Any out-of-vocabulary word is mapped to a special unk symbol.,13,14
3553,2484166,Each symbol in this index labelling a meaning description structure (see below) is associated with one or (in case of synonymy) more dictionary entries.,1,2
3554,2484166,"While expanding such a pattern, a '<' or '>'marked constituent is imported into the syntagmatic sequence from the left or from the right of the syntagmatic symbol, thus changing the initial ordering.",32,33
3555,14800663,"It consists of: (1) a root symbol, which is a type constructor and denotes a class of entities, (2) attribute ~labels, which are record field symbols.",9,10
3556,14800663,"The root symbol is person; id, born and father are three Sub-V-terms which have either constants or type s as values.",2,3
3557,14800663,"It can even be replaced by a completive sentence (the semantic interpretation remaining the same) as exemplified in (6)(b): (6)(b) Marie considdre que l'eau est froide (""Mary considers that the water is cold"") We have then empirical evidence to analyse l'eaufroide in (6) as a clause, a ""small clause"" using an usual label in the literature (since the categorial status of the small clause is irrelevant for our purposes, we will only use the symbol ""SC"" to refer to this constituent, assuming the small clauses analysis proposed by Stowell (1981) and Stowell (1983) ).",91,92
3558,4311046,This corresponds to replacing words from unambigu-ous translation pairs with a unique dummy symbol.,15,16
3559,15161994,The system marked with the * symbol is the recommended system.,6,7
3560,12416656,It is also possible to have the grammar reject European Portuguese (using type brazilianportuguese) or to ignore variation completely by not constraining this feature in the start symbol.,29,30
3561,7156590,"We also merged close expressions such as B-B or I-B, because different temporal expressions are generally divided at least by a symbol or a punctuation character (e.g. ""Wednesday/B morning/B"" is converted into ""Wednesday/B morning/I"").",26,27
3562,1499756,We use a special symbol for the beginning and end of sentence (or word) for bigrams and trigrams.,4,5
3563,5871571,"Deletions and insertions are less worrying in the rank case rather than in the Hamming case: if one incorrectly moves a symbol by, say, one position, the Hamming distance loses any track of it, but rank distance does not, and the mistake is quite light.",22,23
3564,5871571,We denote the set of elements in the list τ with the same symbol as the list.,13,14
3565,13745905,"Typical stocktwits consist of references to company stock symbols (so-called cashtags -a stock symbol preceded by ""$"", e.g. ""$AAPL"" for the company Apple Inc.), a short supporting text or references to a link or pictures (typically containing charts showing stock values analysis). (",16,17
3566,13745905,Each statement (instance) is annotated with the following information: • Cashtag (subtask1) / Company (subtask2): A stock company symbol (for microblogs) or reference to a company (for news/headlines) to which a sentiment score is assigned. •,25,26
3567,11644625,Document Meta-Information Every document in XML file format has embedded metainformation: Symbol Each UN document has a unique symbol 6 which is common for all language versions.,21,22
3568,11644625,"Publication date The original publication date for a document by symbol, which applies to all language versions.",10,11
3569,233189570,The context surrounding the variables is more important than the symbol itself.,10,11
3570,233189570,"There is still a gap in how to handle variable typing in latent models, considering its meaning instead of its lexi-cal symbol.",24,25
3571,220044873,2019) uses fastText to produce formula embeddings for symbol layout trees (SLTs) and operator trees (OPTs).,9,10
3572,1765384,The Composition Process A category symbol like np in the rule above also stands for the entry node of its associated feature structure.,5,6
3573,29154146,"In addition to be filled with entity annotations, recursive slots have subordinate slots (e. g., INVESTIGATION-METHOD in Figure 5 , marked with a drop-down symbol on the left).",31,32
3574,29154146,"In such cases, a new slot value can be added by clicking on the + symbol (e. g., OBSERVATION or TESTFORFUNCTION in Fig.",16,17
3575,216869018,"Most tokenization strategies will use the tree structure of an equation to define the target tokens and can range from considering the full equation (Krstovski and Blei, 2018) as a single token or decomposing its component expressions or at the individual symbol-level (Gao et al.,",44,45
3576,9298882,"Based on the assumption that collective identities are constructed by means of discursive appropriation of particular entities of the real world, we apply another shallow lexico-syntactic pattern in order to detect such entities that are recurrently used in appropriation contexts: unser ... <NOUN> our ... <NOUN> In this pattern, all morphological variants of the lexical cue are considered (e. g., unsere, unseren), as indicated by the symbol.",80,81
3577,244087332,"Whatever the case, the way in which we mask the terms in this work is called Distorted View with Single Asterisks and consists in replacing w with a single asterisk or a single # symbol if the term is a word or a number, respectively.",35,36
3578,235097471,"It is well established that eating with others (Dunbar, 2017) and eating the same food as the others is a major symbol of social integration (Harris, 1971; Young et al.,",24,25
3579,233189651,"Discontinuous connectives are indicated with a ""+"" symbol between their parts.",9,10
3580,2597987,"For example, the answer below ends on three dots instead of a full-stop and a white-space is missing before the > symbol.",26,27
3581,17101280,"This year's work on RBMT focuses on issues revealed through manual inspection of its performance on the development set: • Separate menu items: The rule-based system was observed to be incapable of handling menu items properly, mostly when they were separated by the "">"" symbol, as they often ended up as compounds.",52,53
3582,21693383,The decoding process iterates until EOS symbol is produced.,6,7
3583,2784061,The symbol ¢ denotes a zero pronoun.,1,2
3584,32925047,"The Sanskrit subunit receives the lemmata of the full sentence, where the target lemma is replaced by the UNK symbol.",20,21
3585,6315707,"Fourteen features were used in the experiment including, common surnames, first names, job titles, numeric tokens, alphabet tokens, punctuation symbol, and common characters in front or behind personal names.",25,26
3586,5404402,Note that all of them are based on symbol manJ pu]atien paradigm.,8,9
3587,5404402,"Recently a connectionist model ( called CM hereafter ) approach has been noticed in many area of cognitive science including hatura], language recognition° This approach has some advantages that the symbol manipulation approaches do not have.",33,34
3588,5404402,"Since the connectionist model is a parallel system without any central, controller I and an activation level of each unit and a connectlon strength between units may be presented as continuous values] it alludes much more flexible approaches than symbol manipulation approaches do.",41,42
3589,5404402,"Parser based on conneetionist model Here we omit the technical details of the CM /MeCle].land&Rumelhart 1986/, but we must make clear that we stand for the so called ""localist"" view in which one symbol corresponds to one unit.",36,37
3590,3503845,"The value can be an integer, a string, a symbol, another frame, or a set of frames.",11,12
3591,5356454,"We suspect that this is primarily due to the preponderance of short symbol clarification sentences such as ""What does EA mean?""",12,13
3592,3544473,"A semi-content word would also be acceptable as in, ""Please explain the letter Q."" The system allowed the unknown word ""letter"" to come under the same terminal node as the word ""symbol.""",39,40
3593,52000720,"Twitter is a popular domain choice for sarcasm researchers because tweets are readily-available and may be freely downloaded, and moreover many tweets are self-labeled by Twitter users for various attributes using hashtags, or keywords prefaced with the ""#"" symbol.",46,47
3594,17048295,"The next step, now that x is generated, is to compute the partial outer representation for the second dependent (see Figure 5-b ) ō2 = f (W hi i h + W ho o h + W dr(x) i x + b o ) where W dr(x) is a n × n real matrix specific for the dependency relation of x with h. Next y is generated (using the softmax function above), and the partial outer representation for the third dependent (see Figure 5-c ) is computed: ō3 = f (W hi i h + W ho o h + 1 2 W dr(x) i x + W dr(y) i y + b o ) Since the third dependent is the End-of-Children symbol (EOC), the process of generating dependents for h stops.",144,145
3595,236460225,The rest of symbol keeps the same as Equation 3 .,3,4
3596,6015182,"For instance, in (3), we have a symbol SY functioning as a verb heading an S, and a foreign multiword UOM functioning as a noun heading an NP. (",11,12
3597,218973975,"2 ): • average number of sentences • average number of words • frequency of the number of unique words • frequency of complex words (more than 5 characters) • average of the number of characters in a word • frequency of the number of verbs • frequency of the number of auxiliary verbs • frequency of the number of adjectives • frequency of the number of superlative adjectives • frequency of the number of superlative relative adjectives • frequency the number of comparative adjectives • frequency of the number of nouns • frequency of the number of conjunctions • frequency of the number of adverbs • frequency of articles • frequency of indefinite articles • frequency of definite articles • frequency of indefinite articles prepositions • frequency of pronouns • frequency of numbers • frequency of special characters • frequency of emoji • frequency of unigrams • frequency of bigrams • frequency of trigrams • frequency of offensive words • frequency of punctuation • frequency of commas • frequency of colon • frequency of semi-comma • frequency of exclamation mark • frequency of question mark • frequency of quotes • frequency of upper-case words • frequency of words starting with upper case • frequency of stretched words • frequency of the first singular person pronouns • frequency of the first plural person pronouns • frequency of the second singular person pronouns • frequency of the second plural person pronouns • frequency of the third singular person pronouns related to male • frequency of the third singular person pronouns related to female • frequency of the third plural person pronouns related to male • frequency of the third plural person pronouns related to female • frequency of the # symbol • frequency of the @ symbol • frequency of proper nouns To validate the hypothesis that a stylistic profile can help to detect misogynous contents from the not misogynous ones, we trained several machine learning models both on the traditional feature vector (Eq.",295,296
3598,218973975,"2 ): • average number of sentences • average number of words • frequency of the number of unique words • frequency of complex words (more than 5 characters) • average of the number of characters in a word • frequency of the number of verbs • frequency of the number of auxiliary verbs • frequency of the number of adjectives • frequency of the number of superlative adjectives • frequency of the number of superlative relative adjectives • frequency the number of comparative adjectives • frequency of the number of nouns • frequency of the number of conjunctions • frequency of the number of adverbs • frequency of articles • frequency of indefinite articles • frequency of definite articles • frequency of indefinite articles prepositions • frequency of pronouns • frequency of numbers • frequency of special characters • frequency of emoji • frequency of unigrams • frequency of bigrams • frequency of trigrams • frequency of offensive words • frequency of punctuation • frequency of commas • frequency of colon • frequency of semi-comma • frequency of exclamation mark • frequency of question mark • frequency of quotes • frequency of upper-case words • frequency of words starting with upper case • frequency of stretched words • frequency of the first singular person pronouns • frequency of the first plural person pronouns • frequency of the second singular person pronouns • frequency of the second plural person pronouns • frequency of the third singular person pronouns related to male • frequency of the third singular person pronouns related to female • frequency of the third plural person pronouns related to male • frequency of the third plural person pronouns related to female • frequency of the # symbol • frequency of the @ symbol • frequency of proper nouns To validate the hypothesis that a stylistic profile can help to detect misogynous contents from the not misogynous ones, we trained several machine learning models both on the traditional feature vector (Eq.",301,302
3599,218973975,"In particular, when referring to a target by using a mention (denote by the @ symbol), the stylometric features are not able to capture the gender-related to a given noun and therefore is biased by the bad words typically related to women.",17,18
3600,234742115,"Contexts and Monotonicity Contexts Informally, we treat a natural language context as a sentence with a ""gap"", represented by a variable symbol.",25,26
3601,234742115,A binary relation symbol .,3,4
3602,227231762,"Mention Contextual Embedding: Since a mention could consist of more than one token , the average of each token's pre-trained contextual embedding is taken as the representation of the mention, which is represented by the symbol x n .",40,41
3603,227231762,The weighted matrix is denoted by the symbol C n .,7,8
3604,5814013,"The current system outputs all the definition sentences for a particular word, and the symbol ""|"" is used to separate definition sentences with different meanings.",15,16
3605,218974315,"Finally, introductory and concluding sentences can have varying status between the Bibles: they may be off-set typographically, unambiguously part of the main text, or reside somewhere in between The '©' symbol marks those translations whose original edition is known (or strongly suspected) to still be under copyright.",38,39
3606,203612092,"To test whether pronoun resolution errors can be avoided by introducing the context of the previous source sentence, we trained a variant of the adapted model in which we joined two subsequent short German sentences from the same document with a special separator symbol, whenever the second sentence contained a pronoun and the total number of words in the joined sentence did not exceed 50.",44,45
3607,203612092,"At translation time, we did the joining on the source side only, and then evaluated only the part of the MT output after the generated separator symbol.",28,29
3608,505,"The window size used by the algorithm will also dynamically change depending on the information present in the context for the disambiguation of a particular focus symbol (see Schfitze et al.,",26,27
3609,17078659,"Using the same mechanism, we also pass OOV words to the target side ""as is"" (without using any special unknown word symbol).",25,26
3610,10672840,"Our model assigns probability to a production rule of the form r = P → C 1 • • • C d in a constituent tree T , conditioned on a context h consisting of previously generated treelets, 3 where P is the parent symbol of a rule r and C d 1 = C 1 • • • C d are its children.",45,46
3611,10672840,p(r) = p(C d 1 |h) The probability of a constituent tree T is given by the following equation: p(T ) = ∏ r∈T p(r) The context h differs depending on whether C d 1 is a terminal symbol or a sequence of non-terminal symbols.,42,43
3612,10672840,"Terminal When C d 1 is a terminal symbol w, p(C d 1 |h) = p(w|P, R, r ′ , w −1 , w −2 ) where P is the POS tag of w, R is the right sibling of P , r ′ is the production rule which yields P and its siblings, and w −2 and w −1 are the two words preceding w. Non-Terminal When C d 1 is a sequence of non- terminal symbols, p(C d 1 |h) = p(C d 1 |P, P ′ , r ′ ) where P is the parent symbol of C d 1 , P ′ is the parent symbol of P .",8,9
3613,10672840,"Terminal When C d 1 is a terminal symbol w, p(C d 1 |h) = p(w|P, R, r ′ , w −1 , w −2 ) where P is the POS tag of w, R is the right sibling of P , r ′ is the production rule which yields P and its siblings, and w −2 and w −1 are the two words preceding w. Non-Terminal When C d 1 is a sequence of non- terminal symbols, p(C d 1 |h) = p(C d 1 |P, P ′ , r ′ ) where P is the parent symbol of C d 1 , P ′ is the parent symbol of P .",109,110
3614,10672840,"Terminal When C d 1 is a terminal symbol w, p(C d 1 |h) = p(w|P, R, r ′ , w −1 , w −2 ) where P is the POS tag of w, R is the right sibling of P , r ′ is the production rule which yields P and its siblings, and w −2 and w −1 are the two words preceding w. Non-Terminal When C d 1 is a sequence of non- terminal symbols, p(C d 1 |h) = p(C d 1 |P, P ′ , r ′ ) where P is the parent symbol of C d 1 , P ′ is the parent symbol of P .",120,121
3615,10672840,Parent Annotation They annotated all VPs and children of the ROOT node with their parent symbol.,15,16
3616,10672840,We kept only the bottom-most symbol of the unary rule chain.,7,8
3617,10672840,"To encode the relationship between the target word and another node in the NP, we append a symbol which reflects the direction of tree traversal to the label: 'p' for going up (parent) and 'c' for going down (child).",18,19
3618,8555434,We prefix each suffix with a special symbol and treat them as separate tokens.,7,8
3619,27531097,"The morphologically segmented corpus is created by identifying prefixes, suffixes, as well as the definite article, detaching them from the rest of the word concerned, and marking them with symbol "" "".",33,34
3620,27531097,"The morpheme boundaries are marked in the parser output with symbol "" "".",10,11
3621,16384032,A symbol root for the root node.,1,2
3622,5948170,The symbol ## denotes concatenation of phrase pairs.,1,2
3623,10520176,"In the patterns, the symbol ""|"" represents ""OR"".",5,6
3624,15215411,9}; any other symbol is discarded (cf.,5,6
3625,1613589,"Actually, the linguistic ""segment"", corresponding to a sentence, is much longer: the beginning of the sentence, then an in-line mathematical relation that can function as a verbal phrase or as a proper noun, and the end of the sentence, made of a ""where"" dominating 3 bullet items, each of them giving the definition of some symbol.",69,70
3626,53246405,"Furthermore, we replace all hash symbols (#) because we use them as separation symbol.",16,17
3627,53246405,"After tokenization, we search for and replace any escaped characters with the corresponding symbol and squeeze repeating whitespaces.",14,15
3628,53246405,"If data is used to train the count-based models or if we apply the count-based models on a sentence pair, numbers are replaced by a category symbol.",31,32
3629,243907790,"-Removing the asterisk symbol that marks unattested etymons (e.g., the Latin etymon *conquerire of a cuceri, meaning to conquer).",3,4
3630,30433582,"If a very large number of links for a synset and presentation direction (top/down, left/right), exceeds a threshold, then the rest is hidden in the green circle symbol and can be 'taken out' by user clicking it.",36,37
3631,51986755,"In this regard, the conventional computational stance on grounded language learning embraces a view of the relationship between language and the world as a symbol grounding problem, by which abstract symbols susceptible to formal operations are somehow associated with perceptions and propositions: the hard work is done by a complex and philosophically opaque process of transforming signals into symbols, with the sense that computation by way of deep nets in some sense stands in for an inscrutable mind-brain gestalt.",25,26
3632,51986755,"2018) propose a symbol ungrounding problem: by this account, language begins as a semiotic structure with the representational scheme of a nascent language learner iconically and indexically aligned to embodied and embedded experiences of the world.",4,5
3633,61097914,"Standard stochastic grammars use generative probabilistic models, focussing on rewriting probabilities conditioned by the rewritten symbol.",16,17
3634,11789245,"6 Example of Structure Pattern (Unit Sentence) The argument with the prefix symbol * can match any nanlber of elements, and the argument with the prefix symbol > can match a single element.",14,15
3635,11789245,"6 Example of Structure Pattern (Unit Sentence) The argument with the prefix symbol * can match any nanlber of elements, and the argument with the prefix symbol > can match a single element.",29,30
3636,192760,The symbol + indicates a combination of models.,1,2
3637,49476105,"Because of the nature of RNNs, the network will assign one language variant per input symbol, and thus per character -even though the tags are logically associated word-by-word.",16,17
3638,237572356,The two are separated by a special symbol <sep>.,7,8
3639,9161685,"This is done by implementing a mechanism which collects the results stored in the chart and gather them together under a ""dummy"" sentence symbol.",25,26
3640,7210127,"For simplicity, we will use IT to refer to Chang and Su's method and AV to refer to our method, where the symbol IT implies iterative and AV implies accessor variety.",25,26
3641,16456504,The special symbol # represents a word boundary.,2,3
3642,2927894,"Besides, if the suggested word is not in the target vocabulary, it is associated to the UNK (unknown) symbol, and a new entry to this word is added to the vocabulary.",22,23
3643,1993224,"Similarly, researchers might be interested in whether symbols taken from a Christian symbol repository or from a Jewish symbol repository instead of the symbol used (the meaning) or what the symbol means (the reference).",13,14
3644,1993224,"Similarly, researchers might be interested in whether symbols taken from a Christian symbol repository or from a Jewish symbol repository instead of the symbol used (the meaning) or what the symbol means (the reference).",19,20
3645,1993224,"Similarly, researchers might be interested in whether symbols taken from a Christian symbol repository or from a Jewish symbol repository instead of the symbol used (the meaning) or what the symbol means (the reference).",24,25
3646,1993224,"Similarly, researchers might be interested in whether symbols taken from a Christian symbol repository or from a Jewish symbol repository instead of the symbol used (the meaning) or what the symbol means (the reference).",33,34
3647,14991420,"The transformation of a parallel corpus into a corpus of single sentences is performed with the help of statistical alignments: each word (or substring) is joined with its translation in the output sentence, creating an extended symbol.",40,41
3648,90262188,"Data preprocessing stopped here for the release version (no symbol filtering), given that many researchers want to filter text in their own way.",10,11
3649,9236828,"The decoding of the predicted bit string to a class symbol appears to be a form of voting over class boundaries (Kong and Dietterich, 1995) , and is able to reduce both bias and variance of the classifier.",10,11
3650,51912795,"We preprocessed all datasets to remove punctuation, perform Unicode normalization, replace digits that do not require normalization with a dummy symbol, and lowercase all tokens.",22,23
3651,6020574,"Note that we have overloaded the symbol φ to apply to either a structure y or its parts r. The Markov assumption for factoring labels lets us use the Viterbi algorithm (much like a Hidden Markov Model) in order to find y = argmax y (w φ(x, y )) = argmax y ( n j=1 w φ(x, y j ) + n−1 j=1 w φ(x, y j → y j+1 )).",6,7
3652,6020574,"F 1 the token in lower case, with digits replaced by the symbol #.",13,14
3653,12036979,The symbol • represents the elementwise multiplication of two vectors.,1,2
3654,12036979,"Update for Z p(z ij |π j , x i , S, φ i ) ∝ p(x i |z ij , s j , φ i )p(z ij |π j ) We use q(z ij ) to approximate the posterior: q(z ij ) ∝ exp{E[ln(p(x i |z ij , z −j i , S, φ i )) + ln(p(z ij |π))]} ∝ exp{E[ln(π j )]} * exp{E[− 1 2σ 2 x −j i − s j z ij φ ij T x −j i − s j z ij φ ij ]} ∝ exp{ln(π j )} * exp{− φ 2 ij * z 2 ij * s T j s j − 2φ ij * z ij * s j T * x −j i 2σ 2 } (7) where x −j i = x i − S −j (φ −j i • z −j i ) , and the symbol ¯indicates the expectation value.",166,167
3655,12036979,"Rate-Distortion: The distortion function d(x, x) is a measure of the cost of representing the symbol x to a new symbol x; and the rate can indicate how much compression can be achieved.",20,21
3656,12036979,"Rate-Distortion: The distortion function d(x, x) is a measure of the cost of representing the symbol x to a new symbol x; and the rate can indicate how much compression can be achieved.",25,26
3657,202757910,"At timestep 0, the input word is BOS (""begin of sentence"" symbol), and the hidden state is the output from G 1 .",15,16
3658,202757910,"All the words in sp are generated sequentially using the LSTM, based on previously generated words, until the end-of-sentence symbol is generated.",25,26
3659,14512739,"Lexical units are then permanently removed from the language model by keeping only the first projection (the input side) of the composition: LM = FirstProjection( L • LM w ) (13) In this model, special characters, like punctuations or symbols, are represented by their categories (light, medium and strong punctuations, question mark, symbol, etc.),",65,66
3660,16057026,"Just one category in the right-hand side of each rule is called head-child, which is annotated with a special symbol * as shown in Fig.",24,25
3661,16057026,"The partial parse tree is called a term, which is denoted by [α] X where X is a category and α is a word, a special symbol ?",30,31
3662,227905580,"In Table 22 , we highlight with a special symbol whenever the ranking in one of the metrics differs from the toptobottom sorting of the scores.",9,10
3663,12590155,"This way, we simulate a multilingual text and our algorithm has to learn to identify language boundaries without relying on any particular symbol.",23,24
3664,12316022,"Starting from right, each position in a token represents: word, lemma and tag, separated by the ""|"" symbol.",23,24
3665,12316022,"The tag and morphological features are delimited with the ""+"" symbol.",12,13
3666,12316022,"The symbol ""-"" indicates that there is no tag in the given tagset that would correspond to the Sajjad's one.",1,2
3667,220059964,Their system iteratively refines sequences of symbol pairs in different alphabets.,6,7
3668,220059964,"Finally, n-gram probabilities of symbol pairs are learned.",7,8
3669,220059964,"All possible symbol pairs are generated for each symbol, and the best sequences are selected in a beam search.",2,3
3670,220059964,"All possible symbol pairs are generated for each symbol, and the best sequences are selected in a beam search.",8,9
3671,17291408,PPM encodes all the symbols (characters or words) of a training data within their context where a context of each symbol is a sequence of preceding symbols of different lengths.,22,23
3672,17291408,19 PPM is a simple method which does not require feature selection as it considers the entire text as a single string and computes the probability distribution for each symbol using a blending mechanism.,29,30
3673,17291408,"We implemented a simple version of the PPM method as explained in (Moffat, 1990; Bobicev, 2015) where we used the context of 5 characters for each symbol and the benchmark escape method called C. Hence, we implemented the PPMC5 version of PPM.",31,32
3674,4834807,"Thus, we can generate captions starting from the special symbol BOS with Beam Search.",10,11
3675,650232,"For conciseness, we report the average of the distance between to standard BLEU value and the empirical upper and lower bound after the ""±"" symbol.",27,28
3676,650232,CNG02 (791 unique seen): Tag for punctuation is refined: the lemma of the punctuation symbol is taken into account; previous models disregarded e.g. the distributional differences between a comma and a question mark.,17,18
3677,211082874,"To reduce the vocabulary size (important for abstractive text summarizers), we further replaced number patterns with the # symbol in each of them.",21,22
3678,7689518,"The symbol "" "" is the placeholder of the English word ""which"" in the English context.",1,2
3679,1667755,"We obtained twelve subsets of the test corpus, labeled with the symbol t followed by the sentence length, to which the stack-based decoders were applied.",12,13
3680,16778241,"Moreover, this system solves the problem of large computation times, by implementing feature inheritance through copying rather than unification, and by drastically reducing the number of candidates through the pruning of the features kept on each non-terminal symbol.",42,43
3681,16778241,"In simple cases, the condition is directly applied to a symbol in the pattern.",11,12
3682,16778241,One is that the set of features each non-terminal symbol can have is limited according to a feature definition table as seen in Figure 3 .,11,12
3683,16778241,"With this limitation, every non-terminal symbol has only necessary features, which simplifies parsing trees.",8,9
3684,10497905,"For instance, already edited words could be highlighted in green or a special symbol could be used to display the last position of the caret.",14,15
3685,235097519,"2019 ) who utilized a special symbol ""[SEP]"" to label all entities and output their corresponding representations.",6,7
3686,235097519,"By using the special symbol and self-attention mechanism, TSMSA is capable of capturing the information of the specific aspect.",4,5
3687,235097519,"In summary, our main contributions are as follows: • We propose a target-specified sequence labeling method with multi-head self-attention mechanism to perform TOWE, which generates target-specific context representations for different targets in the same review with the special symbol and multi-head self-attention.",49,50
3688,235097519,"The BIO tagging scheme (Ramshaw and Marcus, 1995) and a special symbol ""[SEP]"" are applied to this task, where each word w i in the sentence s is tagged as y i ∈ {B, I, O, [SEP]} (B: Beginning, I: Inside, O: Others, [SEP]: the tag of an aspect).",14,15
3689,235097519,"As aforementioned, we first use a special symbol ""[SEP]"" to label each aspect.",8,9
3690,235097519,"Inference Process For TOWE, a sentence with a given aspect (i.e., target) is first processed into target-specified mode (""[SEP] Aspect [SEP]"") with the special symbol ""[SEP]"" and then passed into TSMSA, the outputs of which are the target-oriented opinion terms.",38,39
3691,235097519,"Finally, IOG is a state-ofthe-art baseline method for TOWE and the performance of TSMSA(Base) trained by the same word embedding is similar to IOG, which indicates the effectiveness in capturing the representation of a specific aspect with the symbol ""[SEP]"".",45,46
3692,235097519,"The reason may be that the encoder of BERT without fine-tuning cannot capture the information of the specific aspect with the symbol ""[SEP]"".",24,25
3693,235097519,"In addition, ""great"" and ""reasonable"" focus on the special symbol ""[SEP]"" and the specific aspect ""food"", as shown in Figure 3 (a) .",14,15
3694,235097519,"In our methods, the encoder is capable of capturing the information of the specific aspect which is labeled by a special symbol ""[SEP]"".",22,23
3695,235436397,"In each sentence, we use a special symbol [CLS] in the beginning, a symbol [SEP] between the question and the candidate answer, a symbol [SEP] in the end.",8,9
3696,235436397,"In each sentence, we use a special symbol [CLS] in the beginning, a symbol [SEP] between the question and the candidate answer, a symbol [SEP] in the end.",17,18
3697,235436397,"In each sentence, we use a special symbol [CLS] in the beginning, a symbol [SEP] between the question and the candidate answer, a symbol [SEP] in the end.",30,31
3698,179424,"Regarding feature function h 1 and according to (8), we need to maintain the following data: c k,1 and c k,2 given any order k, N 1+ (•), and c X (•) (see section 4.1 for the meaning of each symbol).",51,52
3699,18875090,"The rows labelled with (GT, AD, KN, and SD) show the results for the phrase-based model estimators presented Table 2 : KSMR results for the three Xerox corpora (for both direct and inverse translation directions separated by the symbol ""/"") for different smoothing techniques.",46,47
3700,18875090,"Table 3 : KSMR results for the three Xerox corpora (for both direct and inverse translation directions separated by the symbol ""/"") for all possible combinations of the probability distributions for the f 3 and f 5 feature functions when using two different smoothing techniques In Table 4 the IMT results for the three considered corpora (for both translation directions) are shown.",21,22
3701,218974257,"In the US government, the Privacy Act protects information about individuals that is ""retrieved by personal identifiers such as a name, social security number, or other identifying number or symbol"" (Health and Human Services, 2019).",33,34
3702,248780398,"In our work with Pomak we had the benefit of the electronic lexicon Rodopsky 3 , which contains approximately 61.500 lemmas that correspond to about 3.5 x 10 6 unique forms (i.e., combinations of a lexical token and a PoS symbol) annotated for lemma, PoS and morphological features (Figure 1 ).",42,43
3703,6108710,A similar situation can be constructed for the induction of translation grammars from alignments: i) each symbol in the target language is a possible translation of every symbol in the source language and ii) the translation grammar corresponds to the set of alignments alone.,18,19
3704,6108710,A similar situation can be constructed for the induction of translation grammars from alignments: i) each symbol in the target language is a possible translation of every symbol in the source language and ii) the translation grammar corresponds to the set of alignments alone.,29,30
3705,17079655,"Ultimately, upon complete analysis of a parse tree, a nested semantic frame is produced -a structure with a name, a type, and a set of [key-value] pairs, where the value could be a string, a symbol, a list of values, a number, or another semantic frame.",45,46
3706,12979384,"The linguistic attributes (features) used to predict the output tag and the output tag itself is extracted from the training data files train.conllu and train.parsemetsv combined and transformed into the following form (example for French): Steffi Steffi PROPN _ rend rendre VERB LVC visite visite NOUN CONT à à ADP _ Monica Monica PROPN _ Our model cannot take into account the numbering of MWEs in case more of them are present in one sentence, and we delete the numbers leaving only the name of MWE tags and substituting the continuation of the MWE with the symbol CONT.",102,103
3707,16597131,"Example 1 demonstrates an entry from Ruslan dictionary for the verb vystačit -'to be enough', the explanatory notes are given further: (1) VYSTAC3==R(5,PRP,?(N(D),S(I,G)),39,CHVATIT6): -VYSTAC3 presents a stem of the verb vystačit -'be enough', -R denotes a root of a tree, -5 is a symbol for a verb and PRP is a conjugation pattern of the Czech verb, -N(D),S(I,G)) is a valency frame that we will further describe in detail, -39 is a Russian declination pattern, -CHVATIT6 is the Russian translation of a lexeme, coded in Latin We transformed the entry from the original Ruslan format: lowercased the entries, transferred Ruslan encoding of letters with diacritics (coded in numbers) into common letters, converted Latin into Cyrillic letters for a Russian word.",54,55
3708,245838269,"Figure 1 shows a query with constraints on relations, on words, on word order (<< symbol) and with negative constraints (without part).",19,20
3709,245838269,"For example, in French corpora, the numeral and the symbol are linked by a relation nummod and the symbol is linked to the noun to which it is attached by a relation nmod.",11,12
3710,245838269,"For example, in French corpora, the numeral and the symbol are linked by a relation nummod and the symbol is linked to the noun to which it is attached by a relation nmod.",20,21
3711,1801774,It is used only as a special symbol for slot expansion (Sec.,7,8
3712,384520,"The set of inputs include all 256 possible bytes, a special Generate Output (GO) symbol, and a special DROP symbol used for regularization, which we will discuss below.",17,18
3713,384520,"The set of inputs include all 256 possible bytes, a special Generate Output (GO) symbol, and a special DROP symbol used for regularization, which we will discuss below.",23,24
3714,384520,"The set of outputs include all possible span start positions (byte 0..k), all possible span lengths (0..k), all span labels (PER, LOC, ORG, MISC for the NER task), as well as a special STOP symbol.",50,51
3715,384520,We were able to further improve generalization with a technique we are calling byte-dropout: We randomly replace some fraction of the input bytes in each segment with a special DROP symbol (without changing the corresponding span annotations).,33,34
3716,162183964,"   We define the ""confidence"" of a head as the average of its maximum attention weight excluding the end of sentence symbol, 2 where average is taken over tokens in a set of sentences used for evaluation (development set).",23,24
3717,218470517,"E-BERT-concat combines entity IDs and wordpieces by string concatenation, with the slash symbol as separator (Schick and Schütze, 2019) .",17,18
3718,1245593,A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word.,33,34
3719,1245593,"Despite these advantages, conventional NMT systems are incapable of translating rare words because they have a fixed modest-sized vocabulary 1 which forces them to use the unk symbol to represent the large number of out-of-vocabulary (OOV) words, as illustrated in Figure 1 .",30,31
3720,1245593,"The decoder is initialized with this vector and generates a translation, one word at a time, until it emits the end-of-sentence symbol <eos>.",27,28
3721,1245593,"Like the PosAll model, we use the symbol unkpos ∅ for unknown target words that do not have an alignment.",8,9
3722,219299786,"The variable L ex presses the irrelevance of the (non-)lexicality of the functor symbol: no matter what value the functor has for lex, the range type will have the value -for the attribute lex.",14,15
3723,248780261,"Internally, 40k BPE operations are used, jointly learned on the source and target data, and the white-space sentence word separator symbol is used as a suffix to ease the decoding.",25,26
3724,16163710,CFG is a formal grammar in which every production rule is of the form V -> w where V is a non-terminal symbol and w is a string consisting of terminals and/or non-terminals.,25,26
3725,16163710,"In CFG, the left-hand side of a production rule may only be formed by a single non-terminal symbol.",22,23
3726,16163710,"Multiple candidates that appear in parsing process are prioritized generally in this order: 1: One that has less patterns with more than one terminal symbol in the whole tree 2: One with less nodes in the whole tree Therefore, registered sentences and expressions are prioritized.",26,27
3727,16163710,"With this limitation, every non-terminal symbol has only necessary features, which simplifies parsing trees.",8,9
3728,14646431,"Note that we refer to patterns with the symbol w, as they are the words in our topic models.",8,9
3729,1992250,"The RNN is fed with input words X i (one at a time), until we feed a special symbol ""GO"".",21,22
3730,1992250,"During the first pass over the input, the network is expected to learn a compact, distributed representation of the input sentence, which will allow it to start generating the right predictions when the second pass starts, after the ""GO"" symbol is read.",45,46
3731,1992250,"The output layer is a SoftMax classifier that predicts, after the ""GO"" symbol is read, one of the following three Figure 3 : Architecture of the network used for sentence compression.",15,16
3732,18570189,Phonological transcriptions will be presented between /…/ but we will use the HSB consonant forms when possible to minimize confusion from different symbol sets.,24,25
3733,15360834,"If it was, the hash symbol was removed from the term and so were any underscores.",6,7
3734,420797,"VN is a finite set of syntactic categories, VT is a finite set of terminals, VS is a finite set of stack symbols having the form 2 d c , 3 d c , /c, or \c, S ∈ VN is the start symbol, and P is a finite set of productions, having the form A[] → a A[• • l] → A1[] . . .",47,48
3735,420797,The notation for stacks uses [• • l] to denote an arbitrary stack whose top symbol is l. The linearity of LIG comes from the fact that in each production there is only one daughter that share the stack features with its mother.,17,18
3736,18965351,"Since paired modalities are either eliminated or preserved and no modalities are left on the start symbol, it guarantees that there is eventually no modality in derivation.",16,17
3737,18965351,2) s ∈ V A is the designated symbol called 'start symbol.' (,9,10
3738,18965351,2) s ∈ V A is the designated symbol called 'start symbol.' (,13,14
3739,18965351,3) R : V T → P(F) is a function assigning to each terminal symbol a set of formulae from F. The set of all strings generated from G is denoted as L(G).,16,17
3740,28588014,"Below, we consider two sets of edges: N is the set of new edges, mostly argument of verbs (drawn in blue and above words in our figures) and A the set of edges impacted by an alternation (namely with a canonical function different from the final grammatical function and labeled with the '@' symbol in figures).",61,62
3741,218869883,"2015) , who proposed to predict the delete-or-keep choice for each output symbol.",17,18
3742,14675140,"In sentence 2, the trace is indicated with a symbol.",10,11
3743,16357664,"Each word is assigned with at least one syntactic category, denoted by an argument symbol (such as np and num) or a functional symbol X/Y and X\Y that require Y from the right and the left respectively to form X. The basic concept is to find the core of the combination and replace the grammatical modifier and complement with set of categories based on the same concept of the rule of fraction cancellation as follow: Upon applying to Thai, we have modified argument set and designed eight arguments shown in Table 1 .",15,16
3744,16357664,"Each word is assigned with at least one syntactic category, denoted by an argument symbol (such as np and num) or a functional symbol X/Y and X\Y that require Y from the right and the left respectively to form X. The basic concept is to find the core of the combination and replace the grammatical modifier and complement with set of categories based on the same concept of the rule of fraction cancellation as follow: Upon applying to Thai, we have modified argument set and designed eight arguments shown in Table 1 .",26,27
3745,16357664,"For ""X"" category, it is used for punctuation or symbol which takes the same categories from the left or right sides and produces the taken category.",12,13
3746,16357664,"In details, this symbol signifies plurality while it is after noun but it intensifies a degree of meaning while it is placed after adjective.",4,5
3747,61951283,"The symbol ""≀"" indicates where the average is out of sequence compared to the main Pearson average.",1,2
3748,981267,We arbitrarily choose the root form of a synonym to act as the conceptual symbol.,14,15
3749,15609850,This correlation is rendered by the numerical index attached to the Conv symbol: Conv 3214 (sell) = buy: the j-th position in the index is occupied by i if the j-th argument of the output corresponds to the i-th actant of the input.,12,13
3750,15609850,The argument structure of the derivative is described by means of an index attached to the Adv symbol.,17,18
3751,15609850,We showed above (in 2.3) how the correlation between the arguments of the conversives can be stated by means of the numerical index attached to the symbol of the Conv Lexical Function.,28,29
3752,7643172,"Besides desktop environments in Asian languages, IMs are also essential in any language for ambiguous keyboards that have more than one character or letter assigned to each key, resulting in some uncertainty about the intended symbol when a key is pressed.",37,38
3753,21691258,The thicker an edge the heavier the weight and the more the communication was used via the @ symbol.,18,19
3754,15049828,"Basically, the Mandarin Phonetic Alphabet (MPA, also called zhu-in-fu-hao) and Pinyin (han-yu-pin-yin) are the most widely known phonetic symbol sets to transcribe Mandarin Chinese, but both of these systems are designed for Mandarin.",36,37
3755,18876473,"We use the symbol ""%"" to represent uncertain or unspecified part of a word.",3,4
3756,18876473,"We use the symbol ""#"" to represent a wildcard word which can match with any single word.",3,4
3757,18876473,"We use the symbol ""*"" to represent a subsequence.",3,4
3758,18876473,"The aligned sequences allow gaps represented by the symbol ""-"" between elements.",8,9
3759,3960255,"For example, according to 7 In the inference rule, $q is a meta symbol to ask the inference engine to generate a unique identifier for the newly derived quantity fact. (",16,17
3760,3960255,"Every MWP in the datasets can be associated with a solution expression template, such as ""⊡ + ⊡"" or ""⊡ − ⊡"", where the symbol ⊡ represents a slot to hold a quantity.",30,31
3761,3960255,"Here the ellipse symbol ""…"" denotes unchanged text. (",3,4
3762,2313543,"2008) is used with monotonic setting; we set the source (kanji) and target (kana) phrase length limits to 1 and 4, and prohibit alignments to a null symbol in News-1/2, the OOV rate in the table is the OOV word rate based on the KyTea's output.",34,35
3763,7208683,The feature value is also not defined (ND) if the token itself is a punctuation symbol or contains any special symbol or digit.,17,18
3764,7208683,The feature value is also not defined (ND) if the token itself is a punctuation symbol or contains any special symbol or digit.,22,23
3765,7208683,"CntDgtSpl: Token consists of digit and special symbol such as $, # etc.",8,9
3766,790774,The underscore symbol (_) means there was originally no white space; it is used merely to make the translation in English more readable.,2,3
3767,12357946,"For example, <Person name> Sonia Gandhi </Person name>, <Location name> Kolkata </Location name> and <Organization name> Jadavpur University </Organization name> For each tag T inserted in the training corpus, the algorithm generates a lexical pattern p using a context window of maximum width 6 (excluding the tagged NE) around the left and the right tags, e.g., p = [l-3l-2 l-1 <T> ...</T> l+1 l+2 l+3], where, l±i are the context of p. Any of l±i may be a punctuation symbol.",107,108
3768,12357946,The feature value is also not defined (ND) if the token itself is a punctuation symbol or contains any special symbol or digit.,17,18
3769,12357946,The feature value is also not defined (ND) if the token itself is a punctuation symbol or contains any special symbol or digit.,22,23
3770,12357946,"CntDgtSpl: Token consists of digit and special symbol such as $, # etc.",8,9
3771,10969391,"Finally, to take advantage of the fact that TAL is an order-sorted logic, one-place relations that are identical to sorts defined in the current background theory are compiled into a symbol table and removed from the formula in Figure 5f .",36,37
3772,29985857,"First track involves finding a sentiment score towards a given 'cashtag' (stock symbol preceded by a $, e.g. $AAPL for Apple Inc.) in microblog messages while the second track involves finding a sentiment score towards a given company name in the news headlines.",15,16
3773,7819714,In the future we want to investigate the usefulness of our method for other Unicode symbol embeddings.,15,16
3774,7410354,"2007 ]: 1) ""one-to-one"" -Each grapheme relates with only one phoneme (segments with one symbol only).",23,24
3775,7410354,A null symbol ('_') is used to deal with the cases in which a grapheme can originate more than one phoneme (the insertion of phonemes) or the cases where more than one grapheme originate only one phoneme (the deletion of phonemes).,2,3
3776,7410354,It handles insertions and deletions of phonemes without using any special symbol.,11,12
3777,7410354,"As in [Andrade and Viana 1985] , our proposal considered to mark the V stressed (with the symbol ' "" ') and did not require the identification of the syllabic unit.",20,21
3778,7410354,This pre-processing module attributes a special symbol to all stressed vowels generating a univocal graphoneme.,8,9
3779,34033348,"The symbol ""only with feature x"" shows the error rate of using one single feature, while ""with all but feature x"" shows the error rate of using all features except the specified feature.",1,2
3780,23327123,"In addition to the description of using the symbol system to convert language expertise and heuristic knowledge into a knowledge base to cope with a frame-based corpus and a tone sandhi processor, the procedure of connecting the inference engine and the knowledge 1 base to make allotone selection was also discussed.",8,9
3781,23327123,The experiment data of the study also reveals an important clue: the marking of the symbol system makes a higher contribution rate to the tone sandhi accuracy than the rule inference.,16,17
3782,18716013,"and at the same time it also indicates the functional aspects (like symbol of love, friendship, peace etc.).",13,14
3783,210063663,Each language L has an associated alphabet -a set of characters -A(L) which includes the special symbol ∅ 1 .,17,18
3784,210063663,"Figure 4 shows the entropy values of 6 BG characters е, 󰑫, а, щ, и, 󰑱, and the special symbol ∅ for RU readers and the entropy val-ues of 5 RU characters о, е, 󰑱, у, л for BG readers (on the right).",25,26
3785,198229768,"Since we do not want to rely on lexical information to identify the human post-editors, only the 50 most frequent words were kept (most containing punctuation symbols and stop-words), with the remaining ones converted to a special unknown symbol (UNK).",46,47
3786,53051208,The first symbol of a word is always an open bracket.,2,3
3787,53051208,Baseline for Perplexity A prediction system for the next symbol emitted by the generator will not be able to perform arbitrarily well due to the random nature of the process.,9,10
3788,53051208,"In order to get a baseline for the performance, we consider the perplexity per symbol PP of the probability distribution of the generated Dyck languages.",15,16
3789,53051208,"with length |w| it is defined as PP := 2 − 1 |w| |w| i=1 log 2 P (w i |w 1 ,...,w i−1 ) , (5) where P (w i |w 1 , ..., w i−1 ) is the probability of the i-th symbol under the model, given the previous i − 1 symbols.",55,56
3790,53051208,In the edge cases it behaves just like expected: p = 0 means that there is no randomness at all and there is just one possible next symbol.,28,29
3791,53051208,"In order to get a better feeling for the capability of the networks, we consider a second task: Given a Dyck word without the last closing bracket, the RNN has to predict the most likely candidate for this missing symbol.",42,43
3792,2354075,"Rule 3 enforces that, the letter 'a' with a vowel symbol becomes 'w' and a vowel, while Rule 4 enforces that, the characters, 'c/d/j/\/W' with a vowel symbol becomes chillu 1 , '±/²/³/°/®' and a vowel.",13,14
3793,2354075,"Rule 3 enforces that, the letter 'a' with a vowel symbol becomes 'w' and a vowel, while Rule 4 enforces that, the characters, 'c/d/j/\/W' with a vowel symbol becomes chillu 1 , '±/²/³/°/®' and a vowel.",41,42
3794,2354075,"R Rule Example 1 (CSC)Vs = (CSC)S + V en•oÈ = en•m + CÈ word(Noun)+no(verb) 2 (b/e)Vs = V u]SobnWm = u]So + BWm fear(Noun) + is(verb) 3 aVs = w +V ]WaoÈ = ]Ww + CÈ money(Noun) + no(verb) 4 (c/d/j/\/W)Vs = (±/²/³/°/®) + V ®IjodnWm = ®Ijo² + BWm above(Loc)+is(verb) 5 CVs = (CS) + V BtWÁm = BWm + FÁm is(verb)+that(quotative) 6 just split Ae°eè = Ae°+ eè He(Noun)+Came(verb) C=Consonant, V =Vowel, Vs=Vowel symbol, S=Schwa The hybrid approach utilises the phonological changes (Klein et al.,",117,118
3795,3512605,The usage counts of each symbol from the alphabets are necessary to use the Shannon-optimal codes during transmission of the rules.,5,6
3796,14784553,"One possibility is to extend the unknown symbol and improve the obtained estimates via class n-gram models (Trmal et al.,",7,8
3797,2938205,"Finally, the lack of a common format stands as a barrier to the deployment of a truly 'open source' page and symbol set that could be used across formats and developed independently of hardware manufactures.",24,25
3798,21840890,"There are cases in which multiple graphemes represent a single phoneme, as in the word the in English: th e D @ There are cases in which a single grapheme represents multiple phonemes, such as syllabaries, in which each symbol represents a syllable.",43,44
3799,21840890,"Consequently, g2p is harder than simply replacing each grapheme symbol with a corresponding phoneme symbol.",10,11
3800,21840890,"Consequently, g2p is harder than simply replacing each grapheme symbol with a corresponding phoneme symbol.",15,16
3801,21840890,"Sometimes this cleaning algorithm works well: in the German examples in Table 3 , the raw German symbols /X/ and /ç/ are both converted to /x/. This is useful because the /X/ in Ansbach and the /ç/ in Kaninchen are instances of the same phoneme, so their phonemic representations should use the same symbol.",55,56
3802,17517691,"Users often include Twitter usernames in their tweets in order to direct their messages, using the @ symbol before the username (e.g. @radut), therefore a regex replaces all words that start with the @ symbol.",18,19
3803,17517691,"Users often include Twitter usernames in their tweets in order to direct their messages, using the @ symbol before the username (e.g. @radut), therefore a regex replaces all words that start with the @ symbol.",38,39
3804,244051230,"Each language L has an associated alphabet -a set of characters -A(L) which includes the special symbol ∅. We use w ∈ L to denote a word in language L and c i ∈ w to denote the i-th character in word w. Similarly, we will use IPA(w) to denote the symbolic phonetic representation of w, with A IPA (L) being the phonetic alphabet of L and s i ∈ IPA(w) denoting the i-th sound in word w. Moreover, let V(IPA(w)) denote the set of all vowels in w and C(IPA(w)) the set of all consonants in w, respectively.",17,18
3805,1080791,"In this toy grammar, for instance, a noun phrase placeholder can be rewritten to a determiner followed by a noun symbol with probability 0.5.",22,23
3806,6965855,"In Figure 2 , the numbers before the slash symbol indicate the type of IG, and the symbol S and NS indicate ""Selected"" and ""Not-Selected"" IGs, respectively.",9,10
3807,6965855,"In Figure 2 , the numbers before the slash symbol indicate the type of IG, and the symbol S and NS indicate ""Selected"" and ""Not-Selected"" IGs, respectively.",18,19
3808,229365784,"We follow an approach similar to Bandyopadhyay (2019 Bandyopadhyay ( , 2020)) , where we factor the data at wordlevel to include the root word (lemma) and the part of speech (PoS) of each word along with the word itself, each component separated with a pipe (|) symbol.",57,58
3809,34492191,"Besides, the "" "" symbol denotes combinations of modifiers in order.",5,6
3810,14082751,"Features representing agglutinated morphemes are likewise agglutinated in the POS tags, separated by a ""+"" symbol.",18,19
3811,6098470,"Hierarchical Models (Chiang, 2007; Chiang, 2005) build on Phrase-Based Models by relaxing the constraint that phrases must be contiguous sequences of words and allow a short phrase (or phrases) nested within a longer phrase to be replaced by a non-terminal symbol forming a new hierarchical phrase.",51,52
3812,6098470,"For example, a trigram deep syntax language model conditions the probability of each word on the sequence of words consisting of the head of the head of the word followed by the head of the word as follows: p(d e ) = l i=1 P (w i |w m(m(i)) , w m(i) ) (3) In addition, similar to string-based language modeling, we add a start symbol, <s>, at the root of the structure and end symbols, </s>, at the leaves to include the probability of a word being the head of the sentence and the probability of words occurring as leaf nodes in the structure.",77,78
3813,6098470,"In traditional language modeling, the special start symbol is added at the beginning of a sentence so that the probability of the first word appearing as the first word of a sentence can be included when estimating the probability.",8,9
3814,6098470,"With similar motivation, we add a start symbol to the deep syntactic representation so that the probability of the head of the sentence occurring as the head of a sentence can be included.",8,9
3815,2241526,"So, since a writer can start with any symbol and can position it precisely, it is very unlikely that two writers will produce the same spelling for any sign.",9,10
3816,15663921,The median time is 8.55s (• symbol).,7,8
3817,250390593,"2003) to determine style-characteristic words and it includes a Tagger to insert a special symbol '[TAG]' into the input sentences, that will be filled by target-stylecharacteristic phrases.",17,18
3818,250390593,"In particular, for a source sentence with k words, x A = (w 1 , ..., w k ), we replace each of them with a special symbol [MASK] and input the masked sentence to the classifier to compute the probability that the classifier classifies this sentence to the target style.",32,33
3819,250390593,We inspect the following three aspects: • Is the special symbol [MASK] necessary? •,11,12
3820,18727028,"Drawing on ideas in developmental cognition which indicate that infants are aware of conceptual distinctions well before they come to language (Mandler, 2007) , our goal in this paper is to investigate the present-day limits of what we call Uninformed symbol grounding for morphologically rich languages.",45,46
3821,18727028,"Note that punctuation symbol, non recognised foreign characters and illegal Hindi characters are rejected ('Reject' state).",3,4
3822,18727028,"Here we have attempted to learn lexical associations with perceptual data, in an Uninformed symbol grounding approach.",15,16
3823,52008944,"2009) , we filtered the extracted relation triples and retained only those expressing a percentage change in the following format: Here the numerical value of percentage change could be written using either the symbol % or the word percent.",35,36
3824,15603978,The possible values of the features are separated by |(pipe) symbol.,11,12
3825,2745326,"The name and alias are both lowercased first, and the @ symbol removed from the alias.",12,13
3826,8505536,"The feature listed for the Chunk tagging is as follows: F= { W i-m , … ,W i-1, W i, W i+1, …, W i+n, SW i-m , …, SW i-1 , SW i, SW i+1 , … , SW i-n , number of acceptable standard suffixes, number of acceptable standard prefixes, acceptable suffixes present in the word, acceptable prefixes present in the word, word length, word frequency, digit feature, symbol feature} The details of the set of features that have been applied for Chunking in Manipuri text are as follows: 1.",92,93
3827,8505536,"This happens with the following feature set: F= {W i-2 , W i-1, W i, W i+1, SW i-1 , SW i, SW i+1 , number of acceptable standard suffixes, number of acceptable standard prefixes, acceptable suffixes present in the word, acceptable prefixes present in the word, word length, word frequency, digit feature, symbol feature} The experimental result of the above feature combination shows the best result, which gives the Recall (R) of 71.43%, Precision (P) of 83.11% and F-measure (F) of 77.67%.",66,67
3828,8505536,"In the repeat of the experiment with the additional feature of POS tagging, the new feature combination becomes as follows: F= {POS, W i-1, W i, W i+1, SW i-1 , SW i, SW i+1 , upto one acceptable prefixes present in the word, upto four acceptable suffixes present in the word, word length, word frequency, number of acceptable standard suffixes, number of acceptable standard prefixes, digit feature, symbol feature} Notation Meaning W[-i,+j] Words spanning from the i th left position to the j th right position SW[-i, +j] Stem words spanning from the i th left to the j th right positions P[i] The i is the number of acceptable prefixes considered S[i] The i is the number of acceptable suffixes considered  CHUNKING RESULT WITH POS AND RMWE CONCLUSION So far, the SVM based chunking work on Manipuri is not reported.",83,84
3829,18533804,"The first symbol (i.e., VERB) gives the part of speech.",2,3
3830,235097583,"Note that some techniques are not used in certain tasks, specified by the dash symbol on the table.",15,16
3831,7862859,"In transliteration, double consonants in English are mapped with Punjabi gemination symbol 'Addak'.",12,13
3832,5825251,An annotated PCFG is then extracted where each non-terminal symbol in the grammar has been augmented with LFG f-equations: NP[↑OBJ=↓] → DT[↑SPEC=↓] NN [↑=↓] .,11,12
3833,1176769,"For reasons of simplicity, we simulate unseen target words by discarding the vector kernel features indicating the lexical unit of the target word and replacing the label of the corresponding leaf node in the tree kernels by a generic symbol.",40,41
3834,51873081,symbol signals the presences of inquiries.,0,1
3835,18611199,"The Schwa Phoneme An issue of the phoneme mapping is that the Edinburgh set contains the schwa phoneme (denoted by the symbol '@'), which cannot be mapped to any phonemes in the PHONICS set.",22,23
3836,11591301,A pattern is a generalized word sequence in which content words are replaced by a generic CW symbol.,17,18
3837,367308,"They are easy to detect although Indonesian and English radix point are different, where Indonesian uses the comma symbol to separate the integer from the fraction while English uses the dot symbol, e.g. a thousand is 1.000,0 in Indonesian and 1,000.0 in English.",19,20
3838,367308,"They are easy to detect although Indonesian and English radix point are different, where Indonesian uses the comma symbol to separate the integer from the fraction while English uses the dot symbol, e.g. a thousand is 1.000,0 in Indonesian and 1,000.0 in English.",32,33
3839,1754099,"To compute this, we use the well-known minimum edit distance, in which the distance between two strings is the minimal sum of costs of operations (symbol insertion, deletion, and substitution) that transform one string into another.",30,31
3840,215238440,We needed to concatenate the synonyms with another symbol 3 .,8,9
3841,14475949,"It can be defined inducdwdy as follows: • if 3`i a variable or a constant symbol then To give an example, under the direct interpretation the f-structure associated with most representatives supported two candidates is interpreted as an underspecified semantic representation in terms of the supervaluation over the two generalized quantifier representations most (repr, Ax.",16,17
3842,14475949,"if ffi E nf-s, a val-iable or a constant symbol then The result establishes isomorphic subsets of the QLF and LFG formalisms.",14,15
3843,5993225,"2 We introduce the special symbol K for ""no relation"", i.e., no relation holds between two words.",5,6
3844,7667070,"French uses the Roman alphabet with a few additions: accented vowels (éàèùâêîôûëïü), the consonant 'ç', and the ligatures 'oe' and 'ae' Inputting these characters requires a special arrangement such as installing an international keyboard, using ALT codes (which uses the ALT key and a three or four digit code), 1 cutting and pasting from an existing text or inserting a symbol from a table.",75,76
3845,7667070,A finite state transducer is used to represent the mapping from a phonetic symbol in Roman script to its orthographic symbol in the target language.,13,14
3846,7667070,A finite state transducer is used to represent the mapping from a phonetic symbol in Roman script to its orthographic symbol in the target language.,20,21
3847,210494566,"We define the vocabulary size (length of each one-hot vector sent from the speaker) as |V |, and fix the number of words sent to be w. Developing a compositional language To simulate a simplified form of human language on this task, we programatically generate a perfectly compositional language, by assigning each 'concept' (each type of each property) a unique symbol.",71,72
3848,210494566,"By 'unique symbol', we mean that no two concepts are assigned the same word.",3,4
3849,53081004,A symbol with a 'P' inside denotes a pregnancy.,1,2
3850,170078951,"Instead of using multiple encoders to separately encode src and mt, we use BERT pre-training scheme, where the two strings after being concatenated by the [SEP] special symbol are fed to the single encoder.",33,34
3851,7031949,The two subsets do not overlap so as to simulate the case where unseen phonetic symbol strings are converted by the baseline system.,15,16
3852,15736620,"terminal symbol of empty element is indexed, its filler exists in the parse tree.",1,2
3853,15736620,"The final states are in the form of (⟨[• • •] TOP ⟩, n), where TOP is a special symbol for the root of the parse tree and n is the length of the input sentence.",24,25
3854,15736620,"does not yet have a head child, the head-based atomic features are set to a special symbol nil.",19,20
3855,196204438,"If a terminal symbol of an empty element is indexed with a number, its corresponding filler exists in the PTB graph, and is indexed with the same number.",3,4
3856,203557889,"A discourse referent is an expression of type e. If P is an n-place predicate symbol and x 1 , . . .,",17,18
3857,2889786,The symbol with which the arrow is subscripted designates its rule type.,1,2
3858,2889786,The symbol nil designates the operation.,1,2
3859,2889786,The symbol '*' means that the annotated node is introduced by adjoining operation (This node corresponds to the root of the auxiliary tree.).,1,2
3860,218973887,"For instance, the add command adds a new node (with under-specified non-terminal symbol) over each selected node in the current workspace.",18,19
3861,15350399,S is called a start symbol and S ∈ V . #,5,6
3862,15350399,is a special symbol to mark the end of a constituent.,3,4
3863,15350399,The rightmost child of every parent is labeled with this symbol.,10,11
3864,15350399,An allowable chain is a sequence of nonterminal symbols followed by a terminal symbol.,13,14
3865,15350399,"The triple specifies which nonterminal symbol Z is allowed to follow a nonterminal symbol Y under a parent X. For each initial fragment of a sentence, Collins and Roark's incremental parser produces partial parse trees which span all words in the fragment.",5,6
3866,15350399,"The triple specifies which nonterminal symbol Z is allowed to follow a nonterminal symbol Y under a parent X. For each initial fragment of a sentence, Collins and Roark's incremental parser produces partial parse trees which span all words in the fragment.",13,14
3867,15350399,Each auxiliary tree has a leaf called a foot which has the same nonterminal symbol as its root.,14,15
3868,15350399,"An adjoining operation is defined as follows: adjoining An adjoining operation splits a parse tree σ at a nonterminal node η and inserts an auxiliary tree β having the same nonterminal symbol as η, i.e., combines the upper tree of σ with the root of β and the lower tree of σ with the foot of β.",32,33
3869,15350399,We mark each head-child with a special symbol * .,9,10
3870,15350399,"The adjoining probability is approximated as follows: P (β | σ) = P adjoining (β | P, L, H, D) where β is an auxiliary tree or a special symbol nil, the nil means that no auxiliary tree is adjoined.",37,38
3871,202785921,The symbol ⊕ represents concatenation operator.,1,2
3872,2099148,There are many other frequent patterns where the frequency of individual tokens is quite low but at least one member of the trigram has higher frequency: such low frequency tokens are omitted and marked by the (#) symbol.,40,41
3873,2099148,Sometimes more than one low frequency tokens precede or succeed high frequency tokens and they are denoted by the symbol (*) as shown in Table 4 .,19,20
3874,2556,"We used the ChaSen morphological analyzer to segment the Japanese CD-ROM World Encyclopedia (Heibonsha, 1998) into words (we replaced headwords with a common symbol), and then used the CMU-Cambridge toolkit (Clarkson and Rosenfeld, 1997) to model a word-based trigram.",29,30
3875,218973885,"by reference to A golden diamond symbol introduces the etymology field of an entry and typically includes an 'immediate' etymon for the entry (the most recent of its etymons listed in the dictionary) and also in many cases other, earlier 'remote', etymons.",6,7
3876,27706044,"In ( 14 ) S is a global symbol of sentence, i.e. a semion belonging to the category ~. /91 in ( 14 ) is a one-place predicate with a sentence as its argument.",8,9
3877,12808898,"We can describe this form of context-free rule by the following logical formula (here and in the sequel we omit the universal quantifications) : (1:2) At, (xl)^ A,, (x=)^ ...^ A,,, (x,,) = A (x'i'aFx~'aF...-x~'at,,) , where we take A, Ai,, ..., Ain as unary predicate symbols and at1, ..., at, as individual constants; the symbols ""^"" and ""~ "" are the conjunction and implication sign and the function symbol ..... stands for concatenation.",105,106
3878,12808898,"More precisely, for each K-derivation of a terminal string w in a given contextfree grammar in the sense of Chomsky, where K stands for any nonterminal symbol in this grammar, the formal sentence K(w) is in the corresponding first-order theory logically derivable (J. T. WANe, 1973) .",30,31
3879,12808898,"However, it is essential to note, that is this kind of logical formulation of syntactic and semantic rules there is no need of any existence of phrase-marker for the semantical system where the symbol Numeral is a predicate symbol and the symbols "" 0"" and "" 1"" are individual constants.",37,38
3880,12808898,"However, it is essential to note, that is this kind of logical formulation of syntactic and semantic rules there is no need of any existence of phrase-marker for the semantical system where the symbol Numeral is a predicate symbol and the symbols "" 0"" and "" 1"" are individual constants.",42,43
3881,12808898,"In Suppes's explicit formulation, the nodes of the phrase-marker will be numbered, so that the denotation function f is actually defined for pairs (n, s), where n is the number assigned to a node of the phrase-marker and s is a terminal or nonternfinal symbol.",55,56
3882,9497490,"1 , where U, V, W are auxiliary non-terminal symbols, u is a terminal symbol, r is some functor -terminal symbol indicating which of the two non-terminal symbols on the right-hand side of the rule is the dependent one and which is the governing one as well as the type of dependency.",19,20
3883,9497490,"1 , where U, V, W are auxiliary non-terminal symbols, u is a terminal symbol, r is some functor -terminal symbol indicating which of the two non-terminal symbols on the right-hand side of the rule is the dependent one and which is the governing one as well as the type of dependency.",26,27
3884,9497490,"In the written form of the grammar, non-terminal symbols of the grammar are ordered (n q-2)-tuples X, X0, X1, ..., X~ where X is the so-called name-symbol, i.e. a name shared by a certain class of nonterminal symbols, and X0, ..., X,, are indices specifying individual nonterminal symbols of that class.",38,39
3885,9497490,One name-symbol corresponds to the left bracket and one to the right one.,3,4
3886,9497490,The maximum number of indices attached to one non-terminal name-symbol is 15; the maximum number of indices attached to one terminal namesymbol is 30; the maximum number of values of each individual index is 94 and the average number of values per one index is 8.,13,14
3887,9497490,"VERBUM, NP RD 8 = 0,1 $9 = 13 40 0 =-9(20),I0 3 = L8 LS 7----1 8=0,1 $9=13-~ 40 (NP 0 = 9(20),10 2 ----1 3 = L8 RD LS) are non-terminal name-symbols is a functor -terminal name-symbol means that this rule can be used when the value of index 8 with given non-terminal name-symbol is 0 or 1 means that the rule can be used for non-terminal namesymbol VERBUM when index 9 either has the value 13 or is not used a prescribed probability for choice of this alternative (other alternatives are not quoted here for this example) means that in 20 ~o of cases the value 9 is to be chosen, in the rest of the cases the value 10 the value of index 3 with a non-terminal name-symbol NP will equal the value of index 8 of the rewritten non-terminal symbol reference; it means that at this place the whole lefthand side should be repeated (i.e. VERBUM with all its indices).",52,53
3888,9497490,"VERBUM, NP RD 8 = 0,1 $9 = 13 40 0 =-9(20),I0 3 = L8 LS 7----1 8=0,1 $9=13-~ 40 (NP 0 = 9(20),10 2 ----1 3 = L8 RD LS) are non-terminal name-symbols is a functor -terminal name-symbol means that this rule can be used when the value of index 8 with given non-terminal name-symbol is 0 or 1 means that the rule can be used for non-terminal namesymbol VERBUM when index 9 either has the value 13 or is not used a prescribed probability for choice of this alternative (other alternatives are not quoted here for this example) means that in 20 ~o of cases the value 9 is to be chosen, in the rest of the cases the value 10 the value of index 3 with a non-terminal name-symbol NP will equal the value of index 8 of the rewritten non-terminal symbol reference; it means that at this place the whole lefthand side should be repeated (i.e. VERBUM with all its indices).",73,74
3889,9497490,"VERBUM, NP RD 8 = 0,1 $9 = 13 40 0 =-9(20),I0 3 = L8 LS 7----1 8=0,1 $9=13-~ 40 (NP 0 = 9(20),10 2 ----1 3 = L8 RD LS) are non-terminal name-symbols is a functor -terminal name-symbol means that this rule can be used when the value of index 8 with given non-terminal name-symbol is 0 or 1 means that the rule can be used for non-terminal namesymbol VERBUM when index 9 either has the value 13 or is not used a prescribed probability for choice of this alternative (other alternatives are not quoted here for this example) means that in 20 ~o of cases the value 9 is to be chosen, in the rest of the cases the value 10 the value of index 3 with a non-terminal name-symbol NP will equal the value of index 8 of the rewritten non-terminal symbol reference; it means that at this place the whole lefthand side should be repeated (i.e. VERBUM with all its indices).",158,159
3890,9497490,"VERBUM, NP RD 8 = 0,1 $9 = 13 40 0 =-9(20),I0 3 = L8 LS 7----1 8=0,1 $9=13-~ 40 (NP 0 = 9(20),10 2 ----1 3 = L8 RD LS) are non-terminal name-symbols is a functor -terminal name-symbol means that this rule can be used when the value of index 8 with given non-terminal name-symbol is 0 or 1 means that the rule can be used for non-terminal namesymbol VERBUM when index 9 either has the value 13 or is not used a prescribed probability for choice of this alternative (other alternatives are not quoted here for this example) means that in 20 ~o of cases the value 9 is to be chosen, in the rest of the cases the value 10 the value of index 3 with a non-terminal name-symbol NP will equal the value of index 8 of the rewritten non-terminal symbol reference; it means that at this place the whole lefthand side should be repeated (i.e. VERBUM with all its indices).",173,174
3891,9497490,One record will always contain information about all left-hand sides with the same name-symbol and about the corresponding right-hand sides.,17,18
3892,9497490,"The programme reads a corresponding record of the file on the disk storage, using the code of the non-terminal name-symbol as a key.",24,25
3893,431360,"The resultant correspondences contain 944 Japanese and 790 English symbol types, from which we also estimated P(si[ti) and P(ti+l]ti).",9,10
3894,372377,"Words such as sun, sunny, result in a sun in the proper place in a map, rain results in the proper symbol, westerly winds results in an arrow with the proper direction.",24,25
3895,372377,Note that several words may result in the same symbol on the map.,9,10
3896,18232880,"We used the ChaSen morphological analyzer to segment the Japanese CD-ROM World Encyclopedia (Heibonsha, 1998) into words (we replaced headwords with a common symbol), and then used the CMU-Cambridge toolkit (Clarkson and Rosenfeld, 1997) to model a word-based trigram.",29,30
3897,11826760,"For each negated tyt)e ~t, 7""1)£ introduces a new intermediate type symbol I-'tl having the definition ~t and dechu'es it incompatible with t (see Section 3.2.a).",12,13
3898,11826760,"This guarant,ees fast (:oinparabilil,y. We define I;he order <NF on 7>ary normal forms: t~,lpe <N~; neqaled type <NI; conjunction <NI,' dis,-,]'?trtCti01~ <NI"" symbol <NI"" striu9 <NF ~lltll21J(~F. l""ot' the coinl)arisoil of atoms, st;rings, and type names, we use the lexic, ographical order on strings ;rod lbr llitlllt)(!l:S [,h(~ ordering < ou n;ttural IIIIlH[)OI'S. l"",x;unple: a <NI; b <NI; bb <NI; -m(t <NI; c.z A b <NI,' a A -,a <NI; a V b <NJ"" (t V b V c <NI; a V i :1.3.4 Memoizalfioit The memoization t, cchnique describe, d in [10] hw-; 1)een ad;q)ted in order to reuse precomlml,ed resull;s o]' l.ype sinq)li[i<:at,ion.",40,41
3899,11826760,"3.4 Type Exlmnsion and Control Wc noted earlier I, hat types allow us to refer to c(m,--pIex constraints folirougli tim use o[ symbol nantes.",24,25
3900,15337818,"t] X , where t is a terminal symbol and X is a nonterminal symbol. . [",9,10
3901,15337818,"t] X , where t is a terminal symbol and X is a nonterminal symbol. . [",15,16
3902,15337818,substitution The substitution operation replaces a leftmost nonterminal leaf of a partial parse tree σ with an initial tree α having the same nonterminal symbol at its root.,24,25
3903,15337818,"adjunction The adjunction operation splits a partial parse tree σ at a nonterminal node having no nonterminal leaf, and inserts an auxiliary tree β having the same nonterminal symbol at its root.",29,30
3904,15337818,"The decomposition is as follows: • for each node η 1 having no left-sibling, if the parent η p has the same nonterminal symbol as η 1 , split the parse tree at η 1 and η p , and combine the upper tree and the lower tree.",27,28
3905,15337818,"for each node η 2 having only one left-sibling, if the parent η p does not have the same nonterminal symbol as the left-sibling η 1 of η 2 , split the parse tree at η 2 . •",23,24
3906,15337818,Let α be an initial tree and X be the root symbol of α.,11,12
3907,15337818,Let β be a auxiliary tree and X be the root symbol of β.,11,12
3908,15337818,"The probability that β is adjoined is calculated as follows: P (a β ) = C(a β ) C(X) (3) where C(X) is the count of the number of occurrences of symbol X. The probability that adjunction is not applied is calculated as follows: P (nil X ) = 1 − β∈A(X) P (a β ) (4) where nil X means that the adjunction is not applied to a node labeled with X, and A(X) is the set of all auxiliary trees whose root is labeled X. In this PITAG formalism, the probability that elementary trees are combined at each node depends only on the nonterminal symbol of that node 2 .",37,38
3909,15337818,"The probability that β is adjoined is calculated as follows: P (a β ) = C(a β ) C(X) (3) where C(X) is the count of the number of occurrences of symbol X. The probability that adjunction is not applied is calculated as follows: P (nil X ) = 1 − β∈A(X) P (a β ) (4) where nil X means that the adjunction is not applied to a node labeled with X, and A(X) is the set of all auxiliary trees whose root is labeled X. In this PITAG formalism, the probability that elementary trees are combined at each node depends only on the nonterminal symbol of that node 2 .",121,122
3910,2972149,"A code consists of a finite number of code words of fmite length, each representing a source symbol.",18,19
3911,2972149,a) Insert aseparation symbol S at the beginning and end of eacl~ Code word in the code. ..... (,4,5
3912,2972149,lb) Let code word X be of length n. Insert the separation Symbol Xi between i-th and (i + 1)-th symbol of Code word X for 1 ~ i ~ n-1.,24,25
3913,2972149,"For example, after steps (la) and (lb), D = 101 becomes D = S1DIODzlS. (lc) The separation symbol to the right of the code symbol t is called the t-successor of the separation symbol to the left of the same code symbol.",25,26
3914,2972149,"For example, after steps (la) and (lb), D = 101 becomes D = S1DIODzlS. (lc) The separation symbol to the right of the code symbol t is called the t-successor of the separation symbol to the left of the same code symbol.",32,33
3915,2972149,"For example, after steps (la) and (lb), D = 101 becomes D = S1DIODzlS. (lc) The separation symbol to the right of the code symbol t is called the t-successor of the separation symbol to the left of the same code symbol.",43,44
3916,2972149,"For example, after steps (la) and (lb), D = 101 becomes D = S1DIODzlS. (lc) The separation symbol to the right of the code symbol t is called the t-successor of the separation symbol to the left of the same code symbol.",51,52
3917,2972149,2a) Insert a separation symbol Px at the beginning and a separation symbol Qx at the end of each code word X in the code. (,5,6
3918,2972149,2a) Insert a separation symbol Px at the beginning and a separation symbol Qx at the end of each code word X in the code. (,13,14
3919,2972149,"But, if the fourth .symbol received is a 0 we can now decide that the first code word was B, andif the fourth symbol isa 1 we decide that the first code word was D. There may be other types of constraints present on the code.",25,26
3920,248780573,"For simplicity and consistency, we refer to both with the same symbol z j in the following.",12,13
3921,209387737,"In all experiments, the vocabulary size was set to 25,000, and all the OOV words were replaced by ""UNK"" symbol.",23,24
3922,7133552,"For training, we replaced any word that appear less then twice with unknown (unk) symbol.",17,18
3923,11584468,Each word of the corpus has been replaced by a symbol (two figures integer) representing its grammatical group.,10,11
3924,46983454,This study focuses on the use of incomplete multilingual corpora in multi-encoder NMT and mixture of NMT experts and examines a very simple implementation where missing source translations are replaced by a special symbol <NULL>.,35,36
3925,46983454,"In this paper, we examine a simple implementation of multi-source NMT using such an incomplete multilingual corpus that uses a special symbol <NULL> to represent the missing sentences.",24,25
3926,46983454,"The mixture of NMT experts determines an output symbol at each time step t from the final output vector p t , which is the weighted sum of the probability vectors from one-to-one models denoted as follows: p t = m j=1 g j t p j t ( 4 ) where p j t and g j t are the probability vector from j-th model and the corresponding weight at time step t, respectively.",8,9
3927,46983454,"g t is calculated by the gating network as follows: gt = softmax(Wgate tanh(W hid [f 1 t (x ); ...f m t (x )])) (5) where f j t (x) is the input vector to the decoder of the j-th model, typically the embedding vector for the output symbol at the previous time step t-1.",65,66
3928,46983454,"In this work, we employ a very simple modification that helps resolve this issue: replacing each missing input sentence with a special symbol <NULL>.",24,25
3929,46983454,"The special symbol <NULL> can be expected to be basically ignored in multi-encoder NMT, with the decoder choosing word hypotheses using other input sentences.",2,3
3930,46983454,We also employ a special symbol <NULL> in the mixture of NMT experts to deal with missing input sentences in the same way as the multiencoder NMT described above.,5,6
3931,46983454,"Conclusion In this paper, we examined approaches for multisource NMT using incomplete multilingual corpus in which each missing input sentences is replaced by a special symbol <NULL>.",26,27
3932,14282159,"Dog face"" and ""Hot beverage"" are following emojis with positive correlation, while the strong ones with negative correlation are ""Dancer"" , ""Fire"" , ""Baby symbol"" and ""Person raising both hands in celebration"" , all of which have a correlation coefficient higher than 0.5 which is considered to be a strong correlation.",33,34
3933,53116366,"The implementation is simple: missing source translations are replaced with a special symbol NULL as shown in Figure 1 method allows us to use incomplete corpora both at training time and test time, and multi-source NMT with this method was shown to achieve higher translation accuracy.",13,14
3934,53116366,"In the first step, we train a multi-encoder NMT model (Source: English and Slovak, Target: Czech) to get Czech pseudotranslations using the baseline method, which is to replace a missing input sentence with a special symbol NULL .",44,45
3935,53116366,Multi-encoder NMT with back-translation: a multiencoder NMT system using English-to-X NMT to fill up the missing parts in the other source language X. 1 Multi-encoder NMT with NULL : a multi-encoder NMT system using a special symbol NULL to fill up the missing parts in the other source language X [9].,49,50
3936,2955949,"The singulary ones operate on a subtree, called constituent tree, whose initial node has the label St and whose terminal string contains no sentence boundary symbol @. The binary T-rules work on a constituent tree and the kernel tree (called the matrix tree) which dominates it directly.",27,28
3937,2955949,"4 We may assume that a context-free grammar P-~ (V, Y,, R, S) where V-lg = ~, the sentence symbol S in ~, is reduced and has a standard form.",30,31
3938,2955949,Each symbol A i occuring in the sequence is a recursive element in P. Since there are only finitely many elements in • every recursion 4 In E. PAUSE (1972) I also investigate T-grammars that c°ntain generalized transformations of roughly the same form as discussed by N. CHOMSK' Y (1957) .,1,2
3939,2955949,"Let {C1, .... Cs} be the set of these symbols each of which is called a base symbol of P together with the sentence symbol S. Then consider all occurences of base symbols on the right hand side of the rules of P as terminal elements which could not further be expanded by some rule.",20,21
3940,2955949,"Let {C1, .... Cs} be the set of these symbols each of which is called a base symbol of P together with the sentence symbol S. Then consider all occurences of base symbols on the right hand side of the rules of P as terminal elements which could not further be expanded by some rule.",27,28
3941,2955949,"Construct s /~/denotes the cardinality of ~. b) If elements of B are embedded into one another by identifying each time some initial node with some terminal node having as label the same base symbol the resulting tree is always a subtree of a tree ass¢,ciated with some sentence derivation in P. Hence the elements of B are kernel trees in almost the same sense as are those occuring in base trees considered by Petrick.",35,36
3942,2955949,"i[ d f a f ... a f ,, f .I x ]' v i Now all rule sequences of length smaller than or equal to 12 = [~[, (of the form (~_)), starting with the sentence symbol S are (1) S :-.",49,50
3943,2955949,"Now let us consider some derivation in G. It starts with a base tree ~, and we are looking at some kernel tree y that is a subtree of 0c, and some derived tree 13 occurring in the derivation: a) All occurrences of terminal nodes of y in 13, labeled with terminal elements, Or occurrences of nodes in 13, inserted in y or an image of it by T-rules introducing new terminals (or morphemes), are called the rest-nodes of y in ~. b) Each terminal node of y labeled with a base symbol is called a base-node, and all occurrences of such nodes of y in 13 are said to be base-nodes of y in 13.",107,108
3944,2955949,The following graph representing the order in the possible applications of the T-rules of G makes perhaps the periodicity of the derivations more transparent: 10 The symbol Af means almost the same as variables like X in the usual notation of T-rules.,29,30
3945,2955949,"The problems which arise out of the foregoing construction are the following: a) There are cases in which no reversible grammar G' for a given grammar G could be constructed, because, for instance, some auxiliary symbol could not be removed in the course of a sentence derivation.",41,42
3946,8191030,"From the table, there are 6 relations (marked with the symbol '*') that are manually classified as exact mapping.",12,13
3947,21327554,The symbol xx means that the object can be subject if no agent is stated: Alice boiled the eggs.,1,2
3948,10858378,Any other diacritic or symbol is deleted and the texts are lower-cased.,4,5
3949,17912165,"For the tweets only stating the facts without expressing an opinion, we use the word symbol ""NeuW"" to represent them.",16,17
3950,17912165,"The positive opinion words are replaced with the symbol ""PosW"", and the negative opinion words are replaced with the symbol ""NegW"".",8,9
3951,17912165,"The positive opinion words are replaced with the symbol ""PosW"", and the negative opinion words are replaced with the symbol ""NegW"".",22,23
3952,879076,"Dictionary #3: Pronunciation of the syllables Using the CMU pronouncing dictionary (Weide, 1998) and dictionary 2, and knowing all possible APRAbet symbol for all consonant characters, we can program to capture every possible pronunciation of all syllables in the standard dictionary.",27,28
3953,10086161,"Accordingly, we allocate the remaining probability assigned by the lexicon to the unknown word symbol ⟨unk⟩: p l,a (e = ⟨unk⟩|f ) = 1 − ∑ i∈Ve p l,a (e = i|f ). (",15,16
3954,10086161,We replace words of frequency less than a threshold u in both languages with the ⟨unk⟩ symbol and exclude them from our vocabulary.,16,17
3955,9331362,part of speech a symbol (initially S) representing the previous action's part of speech. •,4,5
3956,9331362,deferred effects a list (initially empty) that holds effect functions and the trigger part of speech symbol that indicates when the function will be executed on the belief state.,18,19
3957,10186707,The result is accompanied by a clickable symbol to show the audio and video of that particular speech sequence.,7,8
3958,10186707,"Via results page: Each concordance line has an ""i"" symbol on its very left.",12,13
3959,10186707,"Clicking on this symbol reveals the following information on the informant in question: informant code, sex, age group, country, place, number of words, recording year.",3,4
3960,4709462,"The output symbol distribution over tokens o t ∈ V (where V is the output vocabulary) at step t is given by: p(o t |x, y 1:t−1 , z) = Softmax(g(s rz t )), ( 6 ) where g is a multi-layer feed-forward network that maps s rz t to a vector of size |V |.",2,3
3961,4709462,3-4) and obtain symbol distributions at each step t using Eq.,6,7
3962,4709462,We sample from the distribution and obtain a symbol o t which will be used as the estimated y t and fed to the next steps.,8,9
3963,4709462,"When generating a token at step t, the output token distribution takes the form of a mixture of SHAPED (Mix-SHAPED) model outputs: p(o t |x, y 1:t−1 ) = |D| d=1 p(o t |x, y 1:t−1 , z = d)p(z = d|x), (8) where p(o t |x, y 1:t−1 , z = d) is the output symbol distribution of SHAPED decoder d, evaluated as in Eq.",75,76
3964,4709462,"The joint data likelihood of target sequence y and target domain label z for input sequence x is: p(y, z|x) = p(y|z, x) • p(z|x) (9) Training the Mix-SHAPED model involves minimizing a loss function that combines the negative log-likelihood of the style labels and the negative log-likelihood of the symbol sequences (see the model in Fig 3 ) : At run-time, if the style d of the input is available and d ∈ D, we decode the sequence using Eq.",64,65
3965,4709462,"We apply the attention mechanism on s rz t , using attention weights calculated as: q rz tj = v a tanh(W a h rz j + U a s rz t ), (11) which are normalized to a probability distribution: α rz tj = exp(q rz tj ) Tx i=1 exp(q rz ti ) (12) Context vectors are computed using normalized attention weights: c rz t = Tx j=1 α rz tj h rz j ( 13 ) Given the context vector and the hidden state vectors, the symbol distribution at step t is: p(o t |x, y 1:t , z) = softmax(g([c rz t , s rz t ] )) (14) The attention weights in W a , U a , and v a , as well as the embedding matrix E and vocabulary V are shared by all encoders and decoders.",98,99
3966,4709462,14 to calculate the symbol loss in Eq.,4,5
3967,7472011,One way to capture this notion in the syntax is to enhance the input with a special disjuncture symbol.,18,19
3968,7472011,"This symbol can then be propagated in the grammar, as illustrated in Figure 2 .",1,2
3969,7472011,"Finally, as described in section 2 these tags were augmented with a special prosodic break symbol if the decision tree rated the probability a ToBI 'p' symbol higher than the threshold value of 0.75.",16,17
3970,7472011,"Finally, as described in section 2 these tags were augmented with a special prosodic break symbol if the decision tree rated the probability a ToBI 'p' symbol higher than the threshold value of 0.75.",29,30
3971,14381766,"In effect, the procedure for calculating DLG is to calculate the entropy of the whole treebank, given the encoding method chosen, and then to recalculate its entropy given some subtree which is removed from the treebank and replaced with a symbol that acts as an abbreviation.",43,44
3972,219308397,"Summaries obtained through fuzzy search for the query ""塔莫克魯斯"" In the above examples, each partial matching part is enclosed in a rectangle symbol and the correct transliteration name is underlined.",25,26
3973,2267450,"In the above examples, partial matching part is enclosed in rectangle symbol, e.g., ""克魯斯"", and the correct transliteration name is underlined, e.g., ""湯姆克魯斯"".",12,13
3974,1851512,We also defined a symbol to represent a long vowel.,4,5
3975,1851512,"Here, /:/ is the symbol representing a Japanese long vowel.",5,6
3976,1299971,"These patterns are regular expressions, in which specific morphemes are generalized into parts-of-speech or the special symbol representing the target term.",21,22
3977,13220271,Let w be the next symbol to be matched leaving states s i and s i .,5,6
3978,1234375,"Each row denotes one feature: the left part before the symbol ""&"" is from one of the 6 feature types and the right part denotes a specific relation.",11,12
3979,8516259,"Assuming that when GEN deletes an element (as in the form dish), it marks the deletion (e.g., dish*), then we can implement Max as an acceptor that accepts the deletion symbol with cost 0, 0, 1, 0, 0 per instance.",38,39
3980,8516259,The symbol φ labels backoff transitions.,1,2
3981,8516259,Let w be the next symbol to be matched leaving states s i and s i .,5,6
3982,8516259,"These paths only include tag B for the initial instance of symbol a. However, if g(0, 2) + c(a:A) + g(2, 3) + c(a:A) + g(3, 3) < c(a:B) + g(0, 1) + c(a:A) + g(1, 3), then the tag sequence a:A a:A would have lower (second dimension) cost than a:B a:A, despite having taken a backoff arc.",11,12
3983,9716673,"A WSD specifies many things, including: • the language in question • the alphabet to be used (some languages can be written in either a Roman or a Cyrillic alphabet, for instance) • the character table and entities to be used to represent the alphabet (including the symbol and the binary number, normally stored in a byte, used to represent the symbol).",53,54
3984,9716673,"A WSD specifies many things, including: • the language in question • the alphabet to be used (some languages can be written in either a Roman or a Cyrillic alphabet, for instance) • the character table and entities to be used to represent the alphabet (including the symbol and the binary number, normally stored in a byte, used to represent the symbol).",69,70
3985,9716673,Melby and Gaylord are agreed that a viable solution would be to define TEI-SGML entities for each symbol in ISO 5426 that has not already been assigned to an SGML entity.,19,20
3986,12471523,MaxEnt-based multiclass classifier Our baseline model is a Maximum Entropy (Max-Ent) classifier where each position i from the retelling x gets assigned one of the IO output tags y i corresponding to the set of 25 story elements and a null ('O') symbol.,52,53
3987,3827,"Public messages can be addressed to specific users with the symbol @. According to Honeycutt and Herring (2009) this is used to reply to, to cite or to include someone in a conversation.",10,11
3988,3827,"Messages can be marked and categorized using the ""hashtag"" symbol #, that works as an aggregator of posts having something in common.",11,12
3989,3827,"For each public user, sampled from the public timeline, we collected the nicknames of the related users, who had a conversation with the public users, using the @ symbol.",32,33
3990,18730712,"For the convenience of description, we use the symbol e in the superscript to denote U or H. Each docset is denoted as Model Our h-uHDP model is an extension of a three-level HDP model which naturally incorporates the levels of corpus, docset and document as shown in Fig.",9,10
3991,218973990,FORM: Word form or punctuation symbol.,6,7
3992,17770905,"This involves choosing a label r for each arc, which in a pure dependency structure is an atomic symbol.",19,20
3993,44174238,"We performed minimal preprocessing of the datasets by replacing all words except the top 50,000 most frequent words by an UNK symbol.",21,22
3994,17347947,r m } of dependency types (r 0 is a special symbol for dependents of the root).,12,13
3995,6931165,"Filtering -we remove URL links (e.g. http://example.com), Twitter user names (e.g. 5 http://en.wikipedia.org/wiki/Emoticon#Asian style @alex -with symbol @ indicating a user name), Twitter special words (such as ""RT"" 6 ), and emoticons.",20,21
3996,235390382,"The input is constructed by concatenating the source sentence X and each phrase C i in the constraints C with a separator symbol sep , as follows: [X, sep , C 1 , sep , C 2 , . . . ,",22,23
3997,235390382,"C N , eos ], (3) where eos is the symbol indicating the end of the sentence.",13,14
3998,218973801,"Bahasa Indonesia is a unity language, which was coined by Indonesian nationalists in 1928 and became a symbol of national identity during the struggle for independence in 1945.",18,19
3999,14017397,"it is possible that syntactic phenomena, that have different structural explications, are handled by a common process or sequence of operations A common space of memory is assumed to contain the current hypothesis about the analysis of the parsed segment of the input from the beginning We will refer to such a structured space as Current Global ]lypethesis (CGH) The following set of abstract operations on the space of memory has been defined till now a) an opening and a closing action wsll respectively start and end the storing of the information related to a phrase/clause in a current subspace subsequently merged with the global space The way of storlrtg depends upot't the representation of the output and the corresponding actions are desgined ill accordance Io it b) a t'et)'Jevth,¢ action revolving two partlcipants a symbol that triggers the action {trig,goD and the information to be retrieved (the target ef tile actk)nl will retrieve ent~;e constituents which appear to be possible antecedents fragments of structure, or even simple lexical features The trigger may be a gap a pronoun an ellipsis and any other phenomenon whicil requires the search for an antecedent (,garRet}.",144,145
4000,14017397,"such as, for instance, Suh/aceao"" Retrieving of an antecedent may actually correspond to two different operations depending upon whether the antecedent te be bound linearly proceeds or follows the symbol it is to be bound to.",34,35
4001,225047106,"In this corpus, from the seventeen Universal POS tags listed on the UD website, we have used all except auxiliary (AUX; Akkadian does not have genuine auxiliaries), interjection (INTJ) and symbol (SYM).",38,39
4002,9068564,"Further, the size of a dictionary automaton that is restricted to have a particular symbol in a particular position does not apparently depend on the choice of position.",15,16
4003,9068564,"the automaton e.+ with the dictionary to restrict the first position to have the symbol e, the automaton .e.+ to restrict the second position, and so on.",14,15
4004,15296055,"But, as we can see in the proof of Proposition 7, every contextual language may be generated with a context-free grammar containing only one non-~erminal symbol.",29,30
4005,15296055,"This language is regular, since it is generated by the regular grammar consisting in the rules S ~ ~a, T--->Ua , U--->Ta, --->a, where ~ is the start symbol, La~ is the terminal vocabulary, whereas {S,T,U} is the non-terlainal vocabulary.",32,33
4006,180318706,"Adjective* (Pronoun/Noun) (är/other verbs) som (en/ett) Adjective* Noun (where the symbol ""/"" is meant here to function as a disjunction).",25,26
4007,11096040,"Regarding ""Simple"" case and ""Complex-normal"" case, we decided a symbol of an LCA as follows 3 : arg max XLCA (P (A → B 1 ..X LCA ..B m ) × P (X LCA → C 1 ..C n )) We show the numbers of instances and types of MWEs for each case in our corpus (Section 00-24 of Wall Street Journal in Ontonotes) in Table 1 4 .",16,17
4008,15618468,"does not depend on any symbol of 4 ( ¨iscalledtherootof ² ); ii) for any C8 © u 4 u ª z q m , there is a unique index 8 © u 4 u ª such that 4 f¥ g e depends on 4 f¥ ±d ; iii) « ¬ is an anti-reflexive relation, i.e. ¥ g }© ® ¢ 8 @« ¬ , for any C8 © u 4 u ª .",5,6
4009,15618468,"We if a dependency occurs between X Àa g e¥ ° and a symbol f from 4 c© then ¥ ±© f x8 9 b (respectively ¥ Bf `© e (8 y b 5 h The difference between the above definition of a IDCG and the definition of a structured CG from (Martín-Vide & Pȃun, 1998) consists in the fact that the new symbols inserted by some dependency contextual rule do not attach to some specified (localized) selector symbols, but to some selector symbols having the value specified in the set of new dependencies.",13,14
4010,15618468,"The maximal projection of a symbol 4 f¥ 2 e , C8 © u 4 u ª , of 4 , is the sequence 4 f¥ 2 P } h oh kh 4 f¥ 2 2 , y8 © u 4 u ª , of (not necessarily consecutive) symbols of 4 , such that 5Ó ¸Ô ¨Õ , for any u ÖÔ È vu w , and q m YP i© oh oh oh i© B £ @q m x yq i¯u B« ` .",5,6
4011,4067170,stands for any alpha-bet symbol.,6,7
4012,4067170,"Each of the context markers can be replaced by the special symbol '.#.',",11,12
4013,4067170,Characters with special meaning (such as '+' or '[') can be escaped using the symbol '%'.,20,21
4014,4067170,"For example, the symbol '%+' is a literal plus sign.",4,5
4015,4067170,"For example, nouns are specified for number, and the number feature is expressed as a concatenation of the tag number with the multi-character symbol +singular or +plural.",27,28
4016,4067170,This is a special form of a replace rule which replaces any symbol ('?'),12,13
4017,4067170,"by the multi-character symbol '+feminine', in the context of occurring after '+gender'.",5,6
4018,5667397,"The first context of the first production, (a, A), can be adjoined to occurrences of the symbol b only when these occurrences do not have a symbol a to their right-hand side; in such a case, the Selector ba is present, which is larger than b, thus preventing the use of the first production.",21,22
4019,5667397,"The first context of the first production, (a, A), can be adjoined to occurrences of the symbol b only when these occurrences do not have a symbol a to their right-hand side; in such a case, the Selector ba is present, which is larger than b, thus preventing the use of the first production.",31,32
4020,5667397,"The corresponding formal language consists of words of the form xcx, for x an arbitrarily long word over an alphabet not containing the symbol c (this symbol corresponds to the separator o in the Bambara construction).",24,25
4021,5667397,"The corresponding formal language consists of words of the form xcx, for x an arbitrarily long word over an alphabet not containing the symbol c (this symbol corresponds to the separator o in the Bambara construction).",28,29
4022,5667397,"The idea is the following: h~ -1 is defined on (R U T)*, hence all derivations in G that do not produce words in (R U T)* will be ""lost""; thus, h~ -1 acts like an intersection with the regular language (R U T)*, plus the conversion of each string w E R into the associated symbol bw.",72,73
4023,5667397,"The parentheses [, ] ""kill"" the word u. Productions of types 2 and 3 allow ""living"" symbols o~ to go to the right, across ""dead"" symbols; also b is a ""killer,"" specifically, of the symbol placed immediately to its right.",47,48
4024,5667397,"Of course, this is only a speculation, but it also fits with the general idea of ""natural computation"": for example, nature seems not to use the rewriting operation in the area of genetics, where recombination (crossing over) of chromosomes is the basic evolutionary operation (together with nondeterministic insertion and deletion operations, which, again, are not rewriting) and where no ""nonterminal symbol"" is used.",75,76
4025,5667397,"Take a new symbol, c ~ T, and construct the Chomsky grammar G1 = (NU {S'}, TU {c}, S', P'), where: x is a rule of type 1 in P and c~ E N U r U {c}}.",3,4
4026,5667397,The symbol c is preserved by h~ -1 and it is erased by hi.,1,2
4027,259205,"Its annotation is extracted and the corresponding rule packet is selected, i.e. the one whose rightmost symbol corresponds to the current node category.",17,18
4028,11101999,"Each symbol corresponds to an XML-tag, allowing us to annotate procedural texts,.",1,2
4029,11101999,"Their target is the ""goal"" symbol of the grammar.",7,8
4030,204777591,We inserted a #START# symbol between the current utterance and the thread text as a separator.,6,7
4031,195699881,"An LM-CFTG is a quadruple G = (N, Σ, R, S), where N is a ranked signature of nonterminals of rank at most one, Σ is a ranked signature of terminals, S ∈ N 0 is the start symbol, and R is a finite set of production rules of one of the forms • A → t with A ∈ N 0 and t ∈ T V • A(t) → C[t] with A ∈ N 1 and C ∈ C V , where V = N ∪ Σ. The trees in L(G) ⊆ T Σ are obtained by expanding S with production rules.",48,49
4032,195699881,"let us write for any tree or context t and symbol x, n t x as a shorthand for | yd(t)| x , e t x for the number of xedges in graph(t) and m t x for the maximum length of a string in x * which is also substring of yd(t).",10,11
4033,195699881,"In this case C 2 and C 4 generate only one kind of bar symbol, y, and brackets.",14,15
4034,195699881,"Either left(C 2 ) ∈ X + , right(C 2 ) = B.5 Inductive bounds For any tree or context t and symbol x, let us write n t x as a shorthand for | yd(t)| x , e t x for the number of x-edges generated by t and r t x the length of the rightmost maximal substring of yd(t) consisting in only x-tokens (more formally, r t x = |s| x , where s is the unique substring such that yd(t) = u • s • v where s ∈ x * , if u is non empty its last token is not x, and |v| x = 0).",22,23
4035,28324040,"By the same token, when an unexpected symbol is input half-way through a predictable stretch, a warning of possible error will be given.",8,9
4036,28324040,"It can be anything from the lowest meaning-endowed unit (i.e. morpheme) up to the whole sentence, so long as it has autonomy of explication (: in Chomsky terms, so long as it can be generated as a whole from a non-terminal symbol).",49,50
4037,28324040,"In other words, all syntagmata larger than words/terminal symbols can be assigned a valence of a single word/terminal symbol.",23,24
4038,32495797,"The usefulness of the LOB corpus would be much enhanced if it was grammatically tagged; that is, each word in the corpus would have associated with it a symbol indicating its part of speech.",30,31
4039,32495797,"In Brown a word like ""he'll"" is given the tags for the pronoun ""he"" and the verb ""will"" joined with a special symbol.",29,30
4040,32495797,"However, as can be seen from the appendix, the tag symbol associated with a punctuation mark is the punctuation mark itself, so this trivial tagging operation is performed by the verticalising program. (",12,13
4041,12058805,"For example, if the previous symbol denotes a verb which requires an instrumental theme, only symbols that can function as instruments are presented on the current display.",6,7
4042,12058805,Nondisabled subjects can contribute to the understanding of the cognitive processes underlying the acquisition of symbol and device performance competencies.,15,16
4043,11608317,"X, calculated from a set of observations {x i }; the symbol ""∥"" is used to denote that ""pages under consideration are parallel""; and the symbol ""∦"" is used to denote that ""pages under consideration are not parallel"".",14,15
4044,11608317,"X, calculated from a set of observations {x i }; the symbol ""∥"" is used to denote that ""pages under consideration are parallel""; and the symbol ""∦"" is used to denote that ""pages under consideration are not parallel"".",33,34
4045,8267864,crown is used as a symbol for the monarchy as well as denoting the traditional head ornament worn by the monarch.,5,6
4046,102350997,"Using the current stack representation, the model samples an action (SHIFT or REDUCE): SHIFT generates a terminal symbol, i.e. word, and shifts it onto the stack, 2 REDUCE pops the last two elements off the stack, composes them, and shifts the composed 1 The cardinality of ZT ⊂ {0, 1} 2T −1 is given by the (T − 1)-th Catalan number, |ZT | = (2T −2)!",20,21
4047,102350997,"Subsequent generation depend on z t : • If z t = 0 (SHIFT), the model first generates a terminal symbol via sampling from a categorical distribution whose parameters come from an affine transformation and a softmax, x ∼ softmax(Wh prev + b).",23,24
4048,102350997,The generation process continues until an end-ofsentence symbol is generated.,9,10
4049,233289692,As all currency symbols have their own AGs automatically generated from the data there will always be a 1:1 mapping between a symbol and its normalized form.,22,23
4050,410282,"is translated into ""%who auxiliary-verb %person preposition Yahoo Korea symbol"".",14,15
4051,11029914,"1994) , Shimazu and Takmhima (1996) )and on mapping a predefined symbol to a simple t S means a multi-modal dialogue system and U means a user.",15,16
4052,6477884,"In Table 1, a superscript of a nonterminal symbol identifies a tree, and a subscript identifies a position in the tree.",9,10
4053,16471491,A LHS element that is preceded by the :g symbol indicates a top-level (global) feature.,10,11
4054,236486273,"For our example problem the transducer would have have transitions 0, x, x, 0 with weight 0, which map every symbol to itself, transitions 0, X(, , 0, 0, , X(, 0 and 0, Y (, X(, 0 which allow us to delete, insert, or relable any opening brackets X(, Y ( with weight 1, as well as transitions 0, X), , 0 and 0, , X), 0 with weight 0.",24,25
4055,788527,"The retrieving action will have two participants, a symbol that triggers the action (trig£er) and the infomnation to be retrieved (the target of the action).",9,10
4056,788527,"It is enough to know that its ];asic unit is the attribute-value pair, wk,ere an attribute is a symbol (label) and a value is a symbol or another functional description.",25,26
4057,788527,"It is enough to know that its ];asic unit is the attribute-value pair, wk,ere an attribute is a symbol (label) and a value is a symbol or another functional description.",34,35
4058,7748775,Each cluster represents one distinct language-independent subword symbol.,9,10
4059,7748775,Each symbol in this set is defined by the phone identity and the language identity.,1,2
4060,7748775,"To find the mapping between target triphones and languageindependent source AMs, linguistic knowledge and phonetic symbol notation are the only information we can use.",16,17
4061,7748775,"First, we map each of target monophones to source phone symbols: Any source cluster that contains a phonetic symbol with the same notation as the target phonetic symbol becomes a surrogate symbol for that target phone.",20,21
4062,7748775,"First, we map each of target monophones to source phone symbols: Any source cluster that contains a phonetic symbol with the same notation as the target phonetic symbol becomes a surrogate symbol for that target phone.",29,30
4063,7748775,"First, we map each of target monophones to source phone symbols: Any source cluster that contains a phonetic symbol with the same notation as the target phonetic symbol becomes a surrogate symbol for that target phone.",33,34
4064,7748775,"The same IPA symbol across languages may lie in the same cluster, e.g., /z/ in Cluster 1, or different clusters, e.g., /j/ in Cluster 3 and 4.",3,4
4065,7748775,"   see that on the unknown language Croatian, the derived quasi-language-independent subwords outperform the IPA symbol set in both phone recognition and retrieval using two kinds of queries.",20,21
4066,222132808,"In order to get the sentence-level representations for all individual sequences, we insert an external [CLS] symbol at the start of each sequence, and add a [SEP] symbol at the end of every type of inputs.",21,22
4067,222132808,"In order to get the sentence-level representations for all individual sequences, we insert an external [CLS] symbol at the start of each sequence, and add a [SEP] symbol at the end of every type of inputs.",35,36
4068,13945560,"To effect the reverse operation of recognising the underlying symbol sequence given a spoken utterance, the continuous speech waveform is first converted to a sequence of equally spaced discrete parameter vectors.",9,10
4069,13945560,The role of the recogniser is to effect a mapping between sequences of speech vectors and the wanted underlying symbol sequences.,19,20
4070,229365684,"They do not use any of the tricks (no copy symbol, inline casing, back-translation, etc.)",11,12
4071,229365684,"symbol, inline casing, source-side BPE dropout, and source-side noise.",0,1
4072,2628078,"2-if R is a nucleus-statellite relation symbol, s 1 and s 2 are both RS Trees with contiguous spans (the leftmost leaf in s 2 is textually located right after the rightmost one in s 1 ), and a 1 , a 2 ∈ { N, S ; S , N } then R(t 1 a 1 , t 2 a 2 ) is an RS Tree.",10,11
4073,2628078,"3-if R is a multinuclear relation symbol and s 1 , . . . ,",8,9
4074,2628078,"The set of predicate symbols is as follows: 1) For each relation symbol r in R, L R is a unary predicate of type i, t -i.e.,",14,15
4075,2628078,"For each symbol σ of type u 1 , . . . ,",2,3
4076,2628078,"Intuitively, they mean that x has to be included in the left (for ∈ l ) or right (for ∈ r ) scope of r. For each relation symbol R such as justification or elaboration, the predicate L R takes a relation instance r has argument and states that r is an instance of the rhetorical relation R. Predicates sub, coord and sub −1 apply to a relation instance r, respectively specifying that r's left argument hierarchically dominate its right argument, that both are of equal hierarchical importance, or that the left one is subordinate to the right one.",31,32
4077,2628078,"D i and D l are disjoint sets of individuals for the sorts i and l respectively, and |.| M assigns to each predicate symbol P of type u 1 , . . . ,",25,26
4078,2628078,"If t is an EDU e, return M t = D i = ∅, D l = {e}, where is the interpretation that assigns the empty set to each predicate symbol.",35,36
4079,2628078,"We then identify L(r), the unique relation symbol R such that r ∈ |L R | M .",9,10
4080,2628078,ψ t also specifies for each intermediate node n that the corresponding relation instance r n is labelled with the adequate relation symbol R and relation type (subordinating if N-S . . . ).,22,23
4081,2628078,Assume that we map each node 4 x of g into a unique variable v x ∈ V l and each edge e into a unique variable symbol r e ∈ V i .,27,28
4082,203591420,"Unk: An unknown character is introduced at the beginning (noted UNK.S) or at the end of the sentence (noted UNK.E) before a punctuation symbol if any (this unknown character could be thought as as an unknown emoji, a character in different script, a rare unicode character).",28,29
4083,236486238,"This symbol grounding problem (Harnad, 1990) has been largely studied in the context of mapping language to objects in a situated simple (MacMahon et al.,",1,2
4084,236486238,"If there is no distance in the sentence e.g., ""Send a drone at Jamba Juice"", the model learns to predict, both as start and end position, the final end of sentence symbol, as an indication of absence of distance.",37,38
4085,218977422,"Naturally, some cases were due to the replacement of tokens with the special symbol '@; (i.e. the first limitation).",14,15
4086,7203709,"We could, of course, introduce left and right context for every sound symbol, but this would fragment our data; it is difficult to learn general rules from a handful of examples.",14,15
4087,236411231,"The graphical model uses discourse coherence to jointly learn symbol grounding, domain concepts and valid plans.",9,10
4088,236411231,The former uses a symbolic planner to find a plan given the most likely goal and symbol grounding.,16,17
4089,219176901,"For example, in languages that add lemma information to conj relations, when the coordinating conjunction is a symbol (& or /), most languages just ignore them and keep the bare conj relation.",19,20
4090,227231268,Results marked with a dagger symbol † yield a statistically significant test for a significance level of 0.05.,5,6
4091,229365761,The concatenation of a source sentence and its corresponding target sentence with special symbol tokens is taken as input: [CLS] source [SEP] target [SEP].,13,14
4092,202542455,"As Agent A has no explicit incentive to output the end-of-sentence (EOS) symbol, it tends to keep transmitting the same token repeatedly.",18,19
4093,202542455,"For this reason, we constrain the length of English messages to be no longer than the length of their French source sentence, or shorter if the model outputs the EOS symbol early.",32,33
4094,202542455,"Recall that Agent B is supervised to predict the EOS symbol at the right position, so does not suffer from this issue.",10,11
4095,245838322,"For (5b) and (5c), the lexical semantics of 急救 jíjiù 'to perform first aid' is a bounded stage, adopting the symbol (•^^^•) from MARVS following Huang et al. (",28,29
4096,245131376,"We restrict our grammars to have only a single unique nonterminal symbol, N T .",11,12
4097,245131376,"Formally, let r denote a rule expanded from its parent rule r p 's N T [i] nonterminal (or a special symbol at the root of the derivation tree).",25,26
4098,245131376,"14 To generate a synthetic example (x, y), we use forward sampling: we start from the single N T symbol and sample recursively to expand each nonterminal symbol with a rule, based on p θ (r|r p , i) defined by Eq.",24,25
4099,245131376,"14 To generate a synthetic example (x, y), we use forward sampling: we start from the single N T symbol and sample recursively to expand each nonterminal symbol with a rule, based on p θ (r|r p , i) defined by Eq.",32,33
4100,245131376,"We write SCFG rules as N T → α, β , where N T is a nonterminal symbol, and α and β are strings of nonterminal and terminal symbols.",18,19
4101,10509498,"2015) , the attention mechanism lets the decoder attend more to different source symbols for each target symbol.",18,19
4102,10509498,score() is a feedforward neural network with a single hidden layer that scores how well the source symbol x t and the target symbol y t match.,19,20
4103,10509498,score() is a feedforward neural network with a single hidden layer that scores how well the source symbol x t and the target symbol y t match.,25,26
4104,10509498,"Then, a parametric function out k () returns the conditional probability of the next target symbol being k: p(y t =k|y <t , X) = 1 Z exp out k E y (y t −1 ), s t , c t (2) where Z is again the normalization constant: Z = j exp out j (E y (y t −1 ), s t , c t ) .",17,18
4105,10509498,"Training The entire model can be trained end-toend by minimizing the negative conditional log-likelihood, which is defined as: L = − 1 N N n=1 T (n) Y t=1 log p(y t = y (n) t |y (n) <t , X (n) ), where N is the number of sentence pairs, and X (n)  and y (n) t are the source sentence and the t-th target symbol in the n-th pair, respectively.",90,91
4106,363937,"shtml 6 metaoptimize.com/projects/wordreprs Wikipedia, RCV1, and WSJ with all words lowercased, and digits mapped to a special symbol.",20,21
4107,12061046,1 The special termination symbol # indicates that there is no more symbols to the left/right.,4,5
4108,3429314,"The Mandarin PT has one or two orthographic symbols (or a deletion symbol) aligned to every segment of the English PT; thus for each segment X, its substitution probability mass function (pmf) S X (t) has up to two nonzero entries.",13,14
4109,3429314,"We first aggregate these probabilities over all instances of the same English orthographic symbol, so that S X (t) is the probability that English orthographic symbol X is aligned to Mandarin Pinyin orthographic symbol t. We then build a matrix W whose (i, j)th element, w ij , is the probability that English orthographic symbol i is aligned with Mandarin orthographic symbol j. In order to avoid losing tone information, we define the Mandarin orthography to be composed of Pinyin onsets and tone-annotated rhymes.",13,14
4110,3429314,"We first aggregate these probabilities over all instances of the same English orthographic symbol, so that S X (t) is the probability that English orthographic symbol X is aligned to Mandarin Pinyin orthographic symbol t. We then build a matrix W whose (i, j)th element, w ij , is the probability that English orthographic symbol i is aligned with Mandarin orthographic symbol j. In order to avoid losing tone information, we define the Mandarin orthography to be composed of Pinyin onsets and tone-annotated rhymes.",28,29
4111,3429314,"We first aggregate these probabilities over all instances of the same English orthographic symbol, so that S X (t) is the probability that English orthographic symbol X is aligned to Mandarin Pinyin orthographic symbol t. We then build a matrix W whose (i, j)th element, w ij , is the probability that English orthographic symbol i is aligned with Mandarin orthographic symbol j. In order to avoid losing tone information, we define the Mandarin orthography to be composed of Pinyin onsets and tone-annotated rhymes.",36,37
4112,3429314,"We first aggregate these probabilities over all instances of the same English orthographic symbol, so that S X (t) is the probability that English orthographic symbol X is aligned to Mandarin Pinyin orthographic symbol t. We then build a matrix W whose (i, j)th element, w ij , is the probability that English orthographic symbol i is aligned with Mandarin orthographic symbol j. In order to avoid losing tone information, we define the Mandarin orthography to be composed of Pinyin onsets and tone-annotated rhymes.",60,61
4113,3429314,"We first aggregate these probabilities over all instances of the same English orthographic symbol, so that S X (t) is the probability that English orthographic symbol X is aligned to Mandarin Pinyin orthographic symbol t. We then build a matrix W whose (i, j)th element, w ij , is the probability that English orthographic symbol i is aligned with Mandarin orthographic symbol j. In order to avoid losing tone information, we define the Mandarin orthography to be composed of Pinyin onsets and tone-annotated rhymes.",67,68
4114,3429314,"We also show PERs obtained using an FST union of the English and Mandarin mismatched transcript PTs, and from a majority voting algorithm that outputs a symbol only if the Mandarin and English PTs agree.",27,28
4115,220047809,"When training and evaluating a coherence-agnostic model, this label is set to a special symbol, such as NONE, essentially running the model without coherence information.",17,18
4116,2638525,The output part specifies the tag and compatibility symbol (see below) to be assigned to the allomorph.,8,9
4117,15294488,"In Persian literature, Joseph is the symbol of beauty and Topics Poets Figure 5 : Correlation between (automatically found) topics and poets: the joint probability P (topic, poet) is plotted in dark shades; the darker the shade, the higher the probability.",7,8
4118,1317549,"Words which are clause final (i.e., the last word in a CP or IP) or which precede a punctuation symbol other than a comma (e.g., ';') are followed by a mQor boundary (-//).",22,23
4119,16398052,These two phonetic symbol systems are well-designed in ASCII code and suitable for any learners with common understanding of the English phonetic system.,3,4
4120,10699577,"Each word has one or more senses, represented as a root symbol, which is generally the concatenation of the English token, the ""^"" character, and the PAKTUS lexical category (e .g., """,12,13
4121,2126193,"Some languages do not include all columns; such that the Chinese data does not include LEMMA and FEATURES, these empty columns are shown by the symbol ""-"" in Fig.",27,28
4122,10044759,"The idioms a. This is the case; b. This is not the case; c. This is always the case; d. This is not always the case; can be combined by using the symbol £, thus This is £not £always the case.",36,37
4123,10044759,"The idioms ""treat with S1"" and ""treat with S1 for S2"" can be combined into one by using the symbol /, thus the past participle may be either pre-modified or post-modified or both or not modified at all; c. the agent 01 the action may or may not be given; d. the purpose of the action may or may not be given; e. the combination of any two or three or four of the foregoing possibilities.",23,24
4124,11849431,"2006) for Chinese parsing: they use the firstsense heuristic to append the most general hypernym to the POS of a token, obtaining a semanticallyinformed symbol refinement, and then guide further symbol splits using the HowNet hierarchy.",27,28
4125,11849431,"2006) for Chinese parsing: they use the firstsense heuristic to append the most general hypernym to the POS of a token, obtaining a semanticallyinformed symbol refinement, and then guide further symbol splits using the HowNet hierarchy.",34,35
4126,42198508,"The symbol * as an argument to a function is shorthand for taking the sum of the function over all possible values for that argument ; p denotes the probability of a context relation, where p(w, r, w ′ ) is estimated as f (w, r, w ′ )/f ( * , * , * ) ; and wgt denotes the application of some weight function to a context relation count.",1,2
4127,7456408,"Having obtained these initial grammars, we used each to parse the EmeaFrU unlabeled corpus (with appropriate desinflection and clustering preprocessing for each of the four terminal symbol settings).",28,29
4128,7456408,"Table 2 shows final parsing results on the EMEA test set for each of the four terminal symbol settings, with and without self-training (using 200k parses from EmeaFrU).",17,18
4129,7456408,"However, self-training produces the most pronounced increase in performance (statistically significant improvement over no self-training for each terminal symbol setting), and attenuates the improvement attained by clustering: while clt-er-emea is significantly better than raw or dfl without selftraining, the differences are not significant with self-training.",24,25
4130,804848,"To clearly indicate their double input, we will use the symbol & to separate the conditions from the first and the second arguments (the second is the one immediately following the connector).",11,12
4131,6238238,"Hereafter, if a symbol has subscripts, then missing subscript indicates a set that range over the omitted subscript.",4,5
4132,5637889,"2005) , the Berkeley learning algorithm uses EM to estimate probabilities on symbols that are automatically augmented with latent annotations, a process that can be viewed as symbol splitting.",29,30
4133,5637889,"created, where at each level a symbol appears without track of its original siblings.",7,8
4134,5637889,"Then the Berkeley algorithm performs split/merge/smooth cycles that iteratively rene the binarized grammar: it adds two latent annotations on each symbol, learns probabilities for the rened grammar, merges back 50% of the splits, and smoothes the nal probabilities to prevent overtting.",25,26
4135,19122719,Table 1: Some examples of collocation format LH symbol Japanese term connecting symbol English term RH symbol '、' シェーグレン症候群 '(' Sjogren syndrome ')' '<br>' ラポール ' ' rapport 'と' '[' 代謝 ']](' metabolism ')' '•' アンダーカット '【' undercut '】' '<b>' アンタゴニスト '</b>' antagonist '</a>' '<strong>' イベント '</strong> (<i>' event '</i>' '<font>' 光 '</font></td><td><font>' light '</font>' 2.,9,10
4136,19122719,Table 1: Some examples of collocation format LH symbol Japanese term connecting symbol English term RH symbol '、' シェーグレン症候群 '(' Sjogren syndrome ')' '<br>' ラポール ' ' rapport 'と' '[' 代謝 ']](' metabolism ')' '•' アンダーカット '【' undercut '】' '<b>' アンタゴニスト '</b>' antagonist '</a>' '<strong>' イベント '</strong> (<i>' event '</i>' '<font>' 光 '</font></td><td><font>' light '</font>' 2.,13,14
4137,19122719,Table 1: Some examples of collocation format LH symbol Japanese term connecting symbol English term RH symbol '、' シェーグレン症候群 '(' Sjogren syndrome ')' '<br>' ラポール ' ' rapport 'と' '[' 代謝 ']](' metabolism ')' '•' アンダーカット '【' undercut '】' '<b>' アンタゴニスト '</b>' antagonist '</a>' '<strong>' イベント '</strong> (<i>' event '</i>' '<font>' 光 '</font></td><td><font>' light '</font>' 2.,17,18
4138,19122719,"Collocation format is the patterns of occurrence of the seed term pair, which consists of (a) the connecting symbol sequence, i.e. the character sequence inserted between the seed term pair, and (b) the left-hand (LH) and right-hand (RH) terminating symbols that indicate the starting point of the left-hand term and the ending point of the right-hand term in the term pairs.",21,22
4139,19122719,"For instance, if the system detects the pattern "", JTERM (ETERM)"" on the Web page, it extracts the connecting symbol sequence "" ("", the lefthand terminating symbol "", "", and the right-hand terminating symbol "")"".",25,26
4140,19122719,"For instance, if the system detects the pattern "", JTERM (ETERM)"" on the Web page, it extracts the connecting symbol sequence "" ("", the lefthand terminating symbol "", "", and the right-hand terminating symbol "")"".",34,35
4141,19122719,"For instance, if the system detects the pattern "", JTERM (ETERM)"" on the Web page, it extracts the connecting symbol sequence "" ("", the lefthand terminating symbol "", "", and the right-hand terminating symbol "")"".",45,46
4142,6134239,"A sentence illustrating Gurmukhi is given below: ਪੰ ਜਾਬੀ ਮੇ ਰੀ ਮਾਣ ਜੋ ਗੀ ਮ ਬੋ ਲੀ ਏ. It has 38 consonants, 10 vowels characters, 9 vowel symbols, 2 symbols for nasal sounds and 1 symbol that duplicates the sound of a consonant. (",39,40
4143,6134239,"Non-Aspirated Consonants: In case of nonaspirated consonants, Shahmukhi has more consonants than Gurmukhi, which follows the one symbol for one sound principle.",22,23
4144,1237618,We can simulate its translation process using a stack with a dot indicating which symbol to process next.,14,15
4145,1237618,Note that we bundle all successive terminals in one symbol.,9,10
4146,1237618,"If the symbol after the dot in the rightmost dotted rule is a non-terminal v, this action chooses a viable prefix w of v and generates a new dotted rule for w with the dot at the beginning.",2,3
4147,1237618,"If the right-most dotted rule ends with a dot and it happens to be the starting part of a CFG rule, this action appends one symbol of the remainder of that rule to the stack 4 .",28,29
4148,1237618,"For example: 4 We bundle the successive terminals in one rule into a symbol [ IP] [NN 2 ] grow −→ [ IP] [NN 2 of the vote] From the above definition, we can find that there may be an ambiguity about whether to use a complete action or a grow action.",14,15
4149,1237618,"We calculate the future cost for each node as follows: given a node v, we define its cost function f (v) as f (v) =      1 v is completed lm(v) v is a terminal string max r∈Rv f (r) ∏ π∈rhs(r) f (π) otherwise where V N is the non-terminal set, V T is the terminal set, v, π ∈ V N ∪ V T + , R v is the set of currently applicable rules for v, rhs(r) is the right-hand symbol set of r, lm is the local language model probability, f (r) is calculated using a linear model whose features are bidirectional translation probabilities and lexical probabilities of r. For the translation forest in Figure 4 , if we calculate the future cost of NP with 5 Section 3.7 will describe the binning scheme r 4 , then f (N P ) = f (r 4 ) • f (N N 2 ) • lm(of the vote) = f (r 4 ) • 1 • lm(of the vote) Note that we calculate lm(of the vote) locally and do not take ""the result"" derived from NN 2 as the context.",108,109
4150,572276,1 We use the substitution symbol ↓ to mark predicted structure.,5,6
4151,572276,"As a prediction mark, the substitution symbol can therefore also occur tree-internally.",7,8
4152,207993776,"The node labels can be divided into two classes: (1) surface concepts that are exclusively introduced by lexical entries, whose orthography is the source form of a core part of a concept symbol, and (2) abstract concepts that are used to represent the semantic contribution of grammatical constructions or more specialized lexical entries.",36,37
4153,954984,The foot node must be labeled with a nonterminal symbol that is the same as the label of the root node.,9,10
4154,207984304,We calculate the case-insensitive piece Intersection/Union score after striping punctuation and the SentencePiece white space symbol.,19,20
4155,226239067,"The replacement operation is defined in a very general way, allowing replacement to be constrained by input and output contexts, as in two-level rules but without the restriction of only single-symbol replacements.",36,37
4156,52012630,Each sentence concludes with a special end-of-sentence symbol EOS.,11,12
4157,18091974,"The full definition of nonterminals in Chinese part is: where denotes the nonterminal, indicates the length constraint of the nonterminal (0 means no length constraint), indicates the nonterminal must/must-not contain these words according to the symbol .",44,45
4158,237416552,Then we append to each instance an end-of-sentence token (< /S >) and the corresponding language id symbol (< LID >).,24,25
4159,9707947,Collins (2003) extends the left and right sequences to include a terminating STOP symbol.,15,16
4160,216641784,"While constructing the dialogue context sequence, we insert a tag of [sys] symbol in front of each system utterance, and a tag of [usr] symbol in front of each user utterance.",15,16
4161,216641784,"While constructing the dialogue context sequence, we insert a tag of [sys] symbol in front of each system utterance, and a tag of [usr] symbol in front of each user utterance.",30,31
4162,201687836,"The input of the algorithm is a sequence of 0/1-encodings, which are transformed to symbol embeddings by an embedding layer.",17,18
4163,201687836,"Following modern trends in NLP, we select language modelling as an auxiliary task, predicting not only the morpheme boundary of the current symbol but also the following symbol.",24,25
4164,201687836,"Following modern trends in NLP, we select language modelling as an auxiliary task, predicting not only the morpheme boundary of the current symbol but also the following symbol.",29,30
4165,201687836,"However, this approach fails with basic CNN architecture since the convolutional window observes the next symbol and can easily memorize it.",16,17
4166,201687836,"Therefore we slightly modify our model: instead of using a symmetric window around current symbol, we have two groups of convolutions: the left and right ones.",15,16
4167,201687836,We use symbol embeddings of size 32.,2,3
4168,218974145,"When the STEP operation is predicted, the pointer is moved to the next symbol, so the model implements hard monotonic attention mechanism.",14,15
4169,218974145,"It also uses a pointer to attend current input symbol, but, in contrast to most other models, applies imitation learning to train the decoder.",9,10
4170,218974145,"On inference step, the decoder is simply the recurrent network that conditions on the attended input symbol h i , the previous output action, a t−1 and the global vector f of morphological features (case, gender, etc.).",17,18
4171,218974145,"More precisely, the probability of the symbol c given history h and part-ofspeech t is calculated as p(c|h, T ) = (α + β)p c (c|h, T )+ (1 − α)p(c|h) + (1 − β)p(c|h , t), where h refers to the history without the first word in h, p c denotes the probability calculated using raw counts and coefficients α and β are calculated analogously to Witten-Bell smoothing.",7,8
4172,12171581,"Binary lexical features that indicate whether the word is a: stop word (based on the stop word list for target language), punctuation symbol, proper name or numerical. •",26,27
4173,53064621,"Since, informa-tion in directed edge does not necessarily propagate along its direction, following (Marcheggiani and Titov, 2017) we define an updated edge set E which includes inverse edges (v, u, l −1 uv ) and selfloops (u, u, ) along with the original edge set E, where is a special symbol to denote self-loops.",64,65
4174,53093808,"However, in contrast to machine translation, a symbol of output word is less prone to depend from multiple input symbols, than a translated word from multiple source words.",9,10
4175,53093808,"Consequently, the attention weight is usually concentrated on a single source symbol, being more a pointer than a distributed probability mass.",12,13
4176,53093808,"The key feature of this model is that it predicts not only the output word, but also the alignment between source and target using an additional step symbol which shifts the pointer to the next symbol.",28,29
4177,53093808,"The key feature of this model is that it predicts not only the output word, but also the alignment between source and target using an additional step symbol which shifts the pointer to the next symbol.",36,37
4178,53093808,"On i-th step the decoder takes a concatenation of 3 vectors: x j -the j-th element in the output of the encoder, f = W f eat f -the embedding of the grammatical feature vector and g i = W emb y i−1 -the embedding of previous output symbol.",54,55
4179,53093808,"z i is then passed to a two-layer perceptron with ReLU activation on the intermediate layer and softmax activation on the output layer, which produces the output distribution p i over output letters, formally: z i = max (W p z i + b p , 0), p i = softmax(W o z i + b o ), y i = argmax k p ik If y i is the index of step symbol, we move the pointer to the next input letter.",82,83
4180,53093808,"2017) : since the neural network copies the vast majority of its symbols, the output distribution p i is obtained as a weighted sum of singleton distribution which outputs current input symbol and the preliminary distribution p i specified above.",33,34
4181,53093808,"We choose the simplest possible architecture of the language model, namely, on each step it takes a concatenation of d previous symbol embeddings u i = [g i−d , . . . ,",23,24
4182,53093808,"v i is propagated through a two-layer perceptron to predict the next output symbol analogously to the output layer of the baseline model: u i = max (W LM p u i + b LM p , 0), p LM i = softmax(W LM o u i + b LM o ), y i = argmax k p LM ik The model is trained to predict next output symbol separately from the basic model.",15,16
4183,53093808,"v i is propagated through a two-layer perceptron to predict the next output symbol analogously to the output layer of the baseline model: u i = max (W LM p u i + b LM p , 0), p LM i = softmax(W LM o u i + b LM o ), y i = argmax k p LM ik The model is trained to predict next output symbol separately from the basic model.",75,76
4184,53093808,The language model is conditioned over previously output vectors (excluding step symbol).,12,13
4185,51876439,"1) Here, l(u, v) −1 is the inverse edge label corresponding to label l(u, v), and is a special empty relation symbol for self-loop edges.",28,29
4186,196210175,"We use these datasets in the form provided by Bollmann (2019) , i.e., preprocessed to remove punctuation, perform Unicode normalization, replace digits that do not require normalization with a dummy symbol, and lowercase all tokens.",35,36
4187,218977425,"At post-processing time, words with these symbols to their left are converted into title casing or uppercasing, depending on the symbol generated by the decoder; sequences with a mixed cased symbol are further joined during the post-processing step.",24,25
4188,218977425,"At post-processing time, words with these symbols to their left are converted into title casing or uppercasing, depending on the symbol generated by the decoder; sequences with a mixed cased symbol are further joined during the post-processing step.",35,36
4189,7486840,Each consonant symbol inherits by default the vowel sound [ə] .,2,3
4190,7486840,"In Hindi, SHA (श) and SSA (ष) both represent the sound [ʃ] and have one equivalent symbol in Urdu, i.e. Sheen ‫.)ش(‬ To make distinction between SHA (श) and SSA (ष) in UIT, they are mapped on S and S1 respectively.",23,24
4191,7486840,"Similarly in Urdu, Seh ‫,)ث(‬ Seen ‫)س(‬ and Sad ‫)ص(‬ represent the sound [s] and have one equivalent symbol in Hindi, i.e. SA (स).",21,22
4192,14076822,Any other diacritic or symbol is deleted and the whole text is lower-cased.,4,5
4193,96445784,The symbol ⊕ denotes concatenation.,1,2
4194,241471,"Thus, the unknown word WFST can parse any sequence of characters and generate a unique unk word symbol.",18,19
4195,1085023,"There is also a distinguished root node TOP in each forest, denoting the goal item in parsing, which is simply S 0,m where S is the start symbol and m is the sentence length.",29,30
4196,201070608,"Since the recipe title r is also used for generation, we abuse the symbol x to represent < {d 1 , d 2 , ..., d N }, r > for simplification.",14,15
4197,3054664,The implicitness is indicated with the Ø symbol.,7,8
4198,44143620,"From the training data, we extract a word list of the 50K most frequent head words (nouns, verbs, adjectives and adverbs) and add one OOV symbol 4 .",31,32
4199,235421967,"The definitions of solutions for discrete reasoning introduced in section 4.3 are also expressed in this language except that we use different symbols (e.g., the minus sign ""-"" in our definitions has the same meaning as the symbol ""DIFF"" in their paper).",41,42
4200,53223828,"Here the ∅ symbol indicates the place from where a pronoun appears to have been ""dropped"" from a full sentence.",3,4
4201,9768369,"The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of a word.",6,7
4202,1896970,"The HPB model is formulated by a synchronous context free grammar (SCFG) where gaps are represented by a generic non-terminal symbol X. Rules in the HPB are in the following form: X → γ, α, ∼ , where γ is a string over source terminal symbols and non-terminals, α is a string over target terminal symbols and non-terminals, and ∼ is a one-to-one mapping between non-terminals in γ and α.",24,25
4203,218974499,"Thus, grammatical morphemes may be in all-caps or be prefixed by a special symbol such as '°' in the Yongning Na corpus.",16,17
4204,8808686,"The symbol X in the first position stands for the referent square, while the symbols in the other two positions indicate for each of the other two squares whether it contains a number of shapes within the same range as the referent square (X), within a different range (Y/Z), or whether it does not contain any shapes at all ( ). •",1,2
4205,31006069,"The Amharic writing system is composed of four distinct categories consisting of 276 different symbols; 33 core characters with 7 orders (€, ∫, ‚, ƒ, "", … and †), 4 labiovelars with 5 orders symbol (q, u, k and g), 18 labialized consonants with 1 order (wƒ) and 1 labiodental characters consisting 7 orders (€, ∫, ‚, ƒ, "", … and †).",43,44
4206,31006069,"If the error is detected during the first phase, then the correction proposal phase takes the sentence with error mark and creates (w-n+1) n-grams after adding start ""<s>"" and end ""</s>"" symbol, where w is number of token in sentence and n specifies n-grams.",46,47
4207,14170579,"Both states are updated at each time step in a interweaving fashion, while the output symbol y t is predicted based solely on vectorstate s t (along with c t and y t−1 ).",16,17
4208,14170579,"After that, r t−1 is combined with output symbol y t−1 and c t to update the new vector-state s t = GRU(r t−1 , y t−1 , c t ) (7) The update of vector-state is illustrated in Figure 4 .",9,10
4209,44004989,"The basic principle of RNNs is to iteratively compute a vectorial sequence representation, by applying at each time-step the same trainable func-tion to compute the new network state from the previous state and the last symbol in the sequence.",40,41
4210,227905406,"The symbol ""det(x)"" suggests an operation function, which is used to change a noun phrase into its definite form.",1,2
4211,237421080,Entropy of EOS (ENTEOS): The entropy of the symbol EOS in the sentence as defined in Equation 11 where t = T .,10,11
4212,235377430,And π t ∈ V ∪ {φ} can be either a token in the source vocabulary V or the blank symbol φ.,22,23
4213,222291111,"For example, one could envision future interfaces that reveal definitions of jargon like ""biLM"" or the symbol ""s task "" when a reader hovers over the terms in a reading application (Head et al.,",19,20
4214,222291111,58.7% Incorrect term 27.3% Other: implausible 24.8% Math symbol term 22.7% Other: plausible 11.8% Acronym 3.3% Short name / Synonym 3.5% Acronym and text 1.3% Textual & Formula Def.,12,13
4215,222291111,symbol T in the LISA paper is used for token representation as well as matrix transpose.,0,1
4216,222291111,Potential ideas for improvements of the system include: • Annotation of mathematical definitions: A solution for poor math symbol detection is to annotate math symbols and use them for our training.,20,21
4217,222291111,"To achieve the goal, we proposed a more realistic setup for definition detection task called documentlevel definition detection that requires high recall, mathematical symbol recognition, and documentlevel feature engineering.",25,26
4218,245385814,"2020) unifies decoding in directed and undirected models by a generalized framework, in which the generating process is factorized as the position selection and the symbol replacement, where the first step is achieved by Gibbs sampling or learned adaptive strategies, the second step can be handled by a masked language model pretrained on monolingual corpora and finetuned on the NMT task.",27,28
4219,245385814,can be a mask symbol performing like a placeholder.,4,5
4220,237941017,each belligerent is shown with the same symbol.,7,8
4221,39619593,e) Conversion into the symbol 1 of the words introducing subordinate clauses.,5,6
4222,39619593,f) Conversion into the symbol 4 of every word for which the previous tests have failed.,5,6
4223,39619593,"Ld means that the stroke shall be printed on the right side of the symbol which has just been read, whereas Lg will put the stroke on the left side.",14,15
4224,39619593,"G means that the machine, when arrived at the new state, shall read the symbol to the left of the symbol just read, i.e. the machine shall read from right to left.",16,17
4225,39619593,"G means that the machine, when arrived at the new state, shall read the symbol to the left of the symbol just read, i.e. the machine shall read from right to left.",22,23
4226,220046481,"Graph gluing can be manipulated by an HRG G = (N , T , P, S), where N and T are two finite disjoint alphabets of nonterminal and terminal symbols respectively, S ∈ N is the start symbol, and P is the finite collection of rewriting rules in the form of A → R. The left hand side (LHS) A belongs to N , and the right hand side (RHS) R is a hypergraph fragment over N ∪ T .",42,43
4227,1366476,"For aspiration, in phonetic transcription a simple 'h' following the base consonant symbol is considered adequate (Wells, 1995) .",15,16
4228,19187663,"The symbol "" * "" indicates a significance level of p < 0.05.",1,2
4229,11229482,"For decoding during test time, we simply decode until the end-of-sentence symbol eos occurs, using a beam search with a beam width of 5.",16,17
4230,12685882,Any other diacritic or symbol is deleted.,4,5
4231,10184967,"An alignment Ã is defined as a subset of the Cartesian product of source and target symbol positions: Ã ⊆ {(j, i) : j = 1, . . . ,",16,17
4232,10184967,"Hence, we replace the third symbol with the candidate translations in stack 2 and the first symbol with the candidate translations in stack 7.",6,7
4233,10184967,"Hence, we replace the third symbol with the candidate translations in stack 2 and the first symbol with the candidate translations in stack 7.",17,18
4234,10184967,The reason is that we use only a single non-terminal symbol instead of assigning phrasal categories to the target string.,12,13
4235,15472149,S ∈ N is the start symbol.,6,7
4236,15472149,"Starting from the start symbol S, when a rule (A → R) is applied to an edge e, the edge is replaced by the graph fragment R. Just like in HRG, the ordering of nodes V e in e and external nodes X R in R implies the mapping from V e to X R (Chiang et al.,",4,5
4237,15472149,S ∈ N is the start symbol. •,6,7
4238,15472149,"Non-terminal Symbols In this paper we build a dependency graph-tostring model, so we only use one non-terminal symbol X as in HPB on the target side.",24,25
4239,15472149,"Then the non-terminal symbol for H is defined as the joining of POS tags of its head (Li et al.,",5,6
4240,15472149,"If G j i , e j i is an initial pair, then N (G j i ) → G j i , X → e j i is a rule, where N (G) defines the nonterminal symbol for G. 2.",42,43
4241,5349377,"In order to sisteradjoin between two of these children d i , d i+1 , we recursively sample nonterminals s i,1 , ..., s i,k until we hit a STOP symbol: P a (s i,1 , ..., s i,k , ST OP |C 0 ) (4) = k j=1 P a (s i,j |C j ) • (1 − P C j (ST OP )) • P C k+1 (ST OP ) where C j = d 1 , s 1,1 , ..., d i , s i,1 , ..., s i,j−1 , c is the context for the j'th modifier between these children.",33,34
4242,15224779,"Each elementary tree contains an anchor, a leaf node labeled with a terminal symbol.",14,15
4243,15224779,"At most one other leaf-the foot node-may carry a label of the form A * , where A is a nonterminal symbol.",25,26
4244,15224779,"w n complete if i = n, the prefix tree contains no more substitution nodes, foot nodes, or prediction markers, and the root symbol of the prefix tree is S. The string language of a PLTAG grammar is the set of string yields of its complete prefix trees.",27,28
4245,15224779,"In lieu of a full conversion, we flattened both our PLTAG structures and the original PTB structures such that a node cannot have a child with the same non-terminal symbol.",33,34
4246,15224779,downward (upward) node visits; a stands for a visit to a node labeled with the terminal symbol a; and A ↓ and A * represent visits to substitution and foot nodes.,19,20
4247,15224779,"At any point at which the end of the current fringe (i.e., the next leaf of the prefix tree after the dot) is a terminal symbol a (usually right after a Subst or Adj rule has been applied), we can use the Scan rule to move the dot to just after the downward visit of a. The parse terminates successfully once we derive an item of the form w • f − which contains all words in the input string; that is, an item in which all that remains to do for the depth-first search is to return to the root along the right frontier of the prefix tree.",28,29
4248,237581115,"Sentence boundary is indicated by a special symbol ""[SEN]"".",7,8
4249,237581115,"During decoding, we disable the generation of the endof-sentence symbol for DocInfer until the model outputs the correct number of target translations.",12,13
4250,174802848,"As the length of the ground truth sequence is |y * |, the goal of force decoding is to generate a sequence with |y * | words followed by a special end-of-sentence (EOS) symbol.",40,41
4251,216144667,"For character-level models, we observe that L 0 DROP identifies the inter-word structure in character sequences, and removes 89.2% encodings of the space symbol.",30,31
4252,1920810,"Determiners are merely annotated with a quantifier ""constant"" symbol and no variables or predicates.",10,11
4253,236460298,"<s>"": sentence separator symbol.",7,8
4254,236460298,"The same is applied to the target-side sentences, except for a separator symbol ""<s>"" inserted in-between sentences to distinguish sentence boundaries.",15,16
4255,236460298,"CBD directly translates each chunk, and then recovers sentence-level translation via the separator symbol ""<s>"".",16,17
4256,225075957,"We adopt a simple strategy: one hypothesis is assumed complete once any word in the predictions hits the end-of-sentence symbol (""[/s]"") (line 10).",24,25
4257,237558731,"Specifically, as shown in Figure 1 (left), we replace the classifier indicator CL with the [MASK] symbol of BERT and ask BERT to unmask it.",22,23
4258,237558731,"To do this, we fine-tune BERT on the CCD as a multi-class classification task, where there are 172 classes (i.e., 172 classifier words) in total, and make a prediction with the help of the [CLS] symbol (see Figure 1 (right)).",47,48
4259,6277347,"A PCFG can be defined as a quintuple (N, T, R, S, { θ q } q∈N ), which consists of disjoint finite sets of non-terminal symbols N and terminal symbols T , a finite set of production rules R ⊆ {N → (N ∪ T ) * }, a start symbol S ∈ N , and vectors of probabilistic distributions { θ q } q∈N .",62,63
4260,6277347,"Let t denote a complete derivation, which represents either a tree that expands from a non-terminal q to its leaves, which contain only terminal symbols, or a tree that is composed of a single terminal symbol.",40,41
4261,6277347,"p(o i,j |o −i ; { α} q ) ≈ C −i (u i,j → v i,j ) + α u i,j u i,j →v i,j C −i (u i,j ) + r∈R u i,j α u i,j r (2) where q ∈ N noisy-channel , and C −i (w) denotes the number of times that w is used in the analyses for the corpus, excluding the i th utterance, in which w can be any countable entity such as a rule or a symbol.",113,114
4262,6277347,The new PCFG G is thus a grammar that can be used to parse the terminals v i and generate derivations that are rooted at the start symbol of the AG.,27,28
4263,8789001,"So we have the following translation stack [ IP ][ NP VP], where the dot indicates the next symbol to process in the English word-order.",21,22
4264,8789001,"Since node NP is the next symbol, we then predict with rule r 2 , (r 2 ) NP(Bùshí) → Bush, and add it to the translation stack: [ IP ] [ NP VP ] [ Bush] Since the symbol right after the dot in the top rule is a word, we scan (sc) it, and append it to the current translation, which results in the new translation stack Figure 3 : Simulation of the integraton of an Slm into an incremental tree-to-string decoding.",6,7
4265,8789001,"Since node NP is the next symbol, we then predict with rule r 2 , (r 2 ) NP(Bùshí) → Bush, and add it to the translation stack: [ IP ] [ NP VP ] [ Bush] Since the symbol right after the dot in the top rule is a word, we scan (sc) it, and append it to the current translation, which results in the new translation stack Figure 3 : Simulation of the integraton of an Slm into an incremental tree-to-string decoding.",45,46
4266,8789001,Thus we always do as many sc/co actions as possible immediately after a pr step until the symbol after the dot is another non-terminal.,19,20
4267,5147390,"Binary lexical features that indicate whether the word is a: stop word (based on the stop word list for target language), punctuation symbol, proper name or numerical.",26,27
4268,2045534,"Following the definitions of the previous Sec-tion, given a set of edges L of a lattice for an input sentence C = c 0 ..c n−1 and a PCFG grammar: a 4-tuple N, Σ, P, S , where N is a set of nonterminals, Σ is a set of terminal symbols, P is a set of inference rules, each of which is in the form of X → α : p for X ∈ N , α ∈ (N ∪ Σ) * and p is the probability, and S ∈ N is the start symbol.",110,111
4269,227905387,"Specifically, for the input sentence s, we insert "" "" to the left and right of the chunk where S error occurs, and we add the "" "" symbol to the position where the M error occurs, as shown in Fig.",31,32
4270,13141934,"The subtree button maps to the denotation of the symbol button, i.e. {b 1 , b 2 , b 3 }.",9,10
4271,13141934,"An RTG G = (N, ⌃, S, P ) consists of a finite set N of nonterminal symbols, a ranked signature ⌃, a start symbol S 2 N , and a finite set P of production rules A !",30,31
4272,13141934,"We assume that every terminal symbol r 2 ⌃ occurs in at most one rule, and that the nonterminals of G are pairs A b of a syntactic category A and a semantic index b = ix(A b ).",5,6
4273,13141934,"We ignore the start symbol of G. Instead, we say that given some individual b 2 U and syntactic category A, the set of referring expressions for b is RE G (A, b) = {t 2 L A b (G) | I R (t) = {b}}, i.e. we define an RE as a derivation tree that G can derive from A b and whose relational interpretation is {b}.",4,5
4274,13141934,"Each rule is instantiated for all semantic indices specified in the line above; e.g. the symbol round denotes the set {b 1 , b 3 }, therefore there are rules N b 1 !",16,17
4275,13141934,The values of I R and I S for each symbol are specified below the RTG rule for that symbol.,10,11
4276,13141934,The values of I R and I S for each symbol are specified below the RTG rule for that symbol.,19,20
4277,1607195,"w 4 ]] = ((((([[A]] [[w 1 ]]) [[B]]) [[C]]) [[w 2 ]]) [[w 3 ]]) [[w 4 ]] However, in order to define the combination operation signified here with the symbol, an adjustment to the slot structure of RMRS is required.",67,68
4278,1607195,"An elementary predication :a:R(i) consists of a predicate symbol R, a label , an anchor a, and (optionally) as characteristic variable i an ordinary object language variable (i.e. an individual x , an event e or an underspecified index u).",12,13
4279,1607195,"An argument relation REL(a, v) consists of an argument relation symbol REL from a finite set {ARG , BV, RSTR, BODY, LEFT i/ , RIGHT i/ }, an anchor a, and exactly one argument v, which is either an ordinary object language variable x /e/u or a hole h. Definition 3 (RMRS structure under construction with a stack of slots).",12,13
4280,17008846,"For c ∈ C Ω , let the degree of c, denoted #(c), be the overall number of slashes and backslashes in c. By C Ω [k] := {c ∈ C Ω | #(c) ≤ k} ∪ { * }, we denote the set of all categories whose degree is at most k, plus the wildcard symbol * .",71,72
4281,227905651,"The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of a word.",6,7
4282,15378133,"There is also a distinguished root node TOP in each forest, denoting the goal item in parsing, which is simply S 0,l where S is the start symbol and l is the sentence length.",29,30
4283,14046602,"After these steps, we compute the vocabulary for each dataset as the set of all non-singleton types in the training data augmented with a special out-of-vocabulary symbol.",33,34
4284,227905611,"On the other hand, when we see the application of relations parallel, background, or elaboration we place a boundary symbol indicating that the text spans corresponding to the two arguments of these RST relations pertain to different intent segments.",22,23
4285,225103260,"However, for subsequent timesteps, our model generates a ""CopyNext"" symbol (CN) instead of copying another token from source.",13,14
4286,225103260,"Likewise, we do the same for the CopyNext symbol using a linear layer W C 2 R D⇥1 with input d t and output score c t .",9,10
4287,18616120,"2006) create a xRS rule headed by a pseudo, nonsyntactic nonterminal symbol that subsumes the phrase and corresponding multi-headed syntactic structure; and one sibling xRS rule that explains how the non-syntactic nonterminal symbol can be combined with other genuine nonterminals so as to obtain genuine parse trees.",13,14
4288,18616120,"2006) create a xRS rule headed by a pseudo, nonsyntactic nonterminal symbol that subsumes the phrase and corresponding multi-headed syntactic structure; and one sibling xRS rule that explains how the non-syntactic nonterminal symbol can be combined with other genuine nonterminals so as to obtain genuine parse trees.",39,40
4289,18616120,"An  alignment Ã is defined as a subset of the Cartesian product of source and target symbol positions: Ã ⊆ {(j, i) : j = 1, . . . ,",17,18
4290,28929215,We additionally perform two last experiments: Language-dependent letter embeddings with separation symbol (l-emb-sep).,14,15
4291,28929215,"This is the same as l-emb, but we introduce a new separation symbol SEP between the tags and the characters, solving the problem that it is not clear where the tag ends and the word starts.",15,16
4292,28929215,Language-dependent tag embedding with separation symbol (t-emb-sep).,7,8
4293,28929215,"This is equivalent to t-emb, but we again insert a new separation symbol SEP between the tags and the input word's characters.",15,16
4294,28929215,The l-emb-sep and t-emb-sep results show that a separation symbol clearly improves the model's performance.,17,18
4295,7536187,"A context, u is any finite string beginning with a special distinguished start symbol and ending with some sequence in T * , that is, u ∈ {start • T * }.",14,15
4296,7536187,"For any string α, define hd(α) to be the function that returns the first symbol in the string, tl(α) to be the function that returns suffix of α minus the first symbol, and |α| to be the length of α, with hd( ) = tl( ) = and | | = 0.",16,17
4297,7536187,"For any string α, define hd(α) to be the function that returns the first symbol in the string, tl(α) to be the function that returns suffix of α minus the first symbol, and |α| to be the length of α, with hd( ) = tl( ) = and | | = 0.",35,36
4298,7536187,"Let H u be a distribution on next symbols-that is, objects in T ∪ {stop}-conditioned on a given context u. For an N-gram model of order N , the probability of a string β in T * is given by K N start (β • stop), where K N u (α) is defined as: K N u (α) = 1 α = H f N (u) (hd(α)) × K N u•hd(α) (tl(α)) otherwise , (2) where f n (•) is a context-management function which determines which parts of the left-context should be used to determine the probability of the current symbol.",131,132
4299,7536187,"Our model joins other NLP work attempting to do sequence generation where each symbol is generated based on a rich featural representation of previous symbols (Bilmes and Kirchhoff, 2003; Duh and Kirchhoff, 2004) , though we focus more on phonology-specific representations.",13,14
4300,14215180,Such rules result in an exploding number of states during parsing as the Earley dot symbol moves from left to right.,15,16
4301,16393647,"Words may be abbreviated by using a single letter to represent the entire word, using a special symbol to precede either the first or last letter of the word while truncating the rest of the word, using a double-letter contraction such as ""bb"" or ""cc"", or removing most or all of the vowels in a word in order to shorten it.",18,19
4302,8703600,<-' the arrow symbol form a semantic unit that should not be split.,5,6
4303,2218985,"Tree transformations Prior to training, we alter the annotation of empty elements so that the terminal label is a consistent symbol (ϵ), the preterminal label is the type of the empty element, and -NONEis deleted (see Figure 2b ).",21,22
4304,2218985,"This simplifies the lattices because there is only one empty symbol, and helps the parsing model to learn dependencies between nonterminal labels and empty-category types because there is no intervening -NONE-.",10,11
4305,9004869,"~ne running text is presented to the user (well, the program) as if consisting of one immensely long string (without the line heads) and in which the originnl line divisions are represented by number signs (or some other unique symbol not appearing otherwise in the text). %",45,46
4306,3176110,"For example, both the words 'bashful' and 'шибам' have the symbol 'ʃ' in their IPA sequences which indicate a possible mapping between the character sequences 'sh' and 'ш'.",15,16
4307,3176110,"First, we convert the phoneme dictionary of each language into feature vectors, i.e. we convert each word into a feature vector of n-gram character sequences and similarly, we also, convert the IPA representations into feature vectors of n-gram IPA symbol sequences.",47,48
4308,3176110,"For brevity, we refer to the spaces of n-gram character and IPA symbol sequences as character and phonemic spaces respectively.",15,16
4309,3146611,"Computational Complexity and Efficient Implementation The parsing algorithm (Algorithm 1) begins with n + 1 disjoint structures (the words of the sentence + ROOT symbol), and terminates with one connected structure.",27,28
4310,5863231,"For instance, by taking the phrase pair: ( the big fish ; les gros poissons ) and removing the phrase pair ( big ; gros ) we create the rule ( the X fish ; les X poissons ) The symbol X is called a non-terminal, since the translation rule is viewed as a synchronous contextfree grammar rule.",42,43
4311,226283507,"Understanding this sentence requires knowing the answer to all these questions and relies on the reader's knowledge about this world: a detective investigates crime, police officers restrict access to the crime scene, and a badge can be a symbol of authority.",42,43
4312,226283507,"2017) indicates that a detective is a T O police officer, and is C O finding evidence; that evidence can be L A a crime scene; and that a badge is a T O authority symbol.",39,40
4313,11183249,We assume that a morphological analyzer assigns all possible analyses to a given terminal symbol.,14,15
4314,226283862,"Then at each time step, the decoder outputs either a symbol from the output vocabulary or a pointer to an input token.",11,12
4315,102350767,"r k followed by an EOS symbol, and compute the markov transition probabilities over this sequence: p trans (r 1 , r 2 , . . . ,",6,7
4316,815755,"Note that, since the nodes of the tree are given by the input sentence, a dependency tree T = (V, A) for a sentence W is uniquely defined by the arc set A. For convenience, we will therefore equate the tree with the arc set and and use the symbol T for the latter, reserving the symbol A for arc sets that are not necessarily trees.",55,56
4317,815755,"Note that, since the nodes of the tree are given by the input sentence, a dependency tree T = (V, A) for a sentence W is uniquely defined by the arc set A. For convenience, we will therefore equate the tree with the arc set and and use the symbol T for the latter, reserving the symbol A for arc sets that are not necessarily trees.",63,64
4318,21730518,"Also, 49 mistakes (about 5% of accuracy lost) concerned the inflection of the procent word, expanded from the % symbol.",24,25
4319,475811,"For both the corpora, we split the text into sentences, tokenize, convert into lower-case, remove punctuations, and collapse each digit to a symbol ""0"" (e.g. ""1996"" gets collapsed to ""0000"").",29,30
4320,8266313,"To account for the optionality of adjunction, there are additional rules allowing any adjunction requirement to be rewritten as the symbol ǫ, a terminal symbol of the RTG.",21,22
4321,8266313,"To account for the optionality of adjunction, there are additional rules allowing any adjunction requirement to be rewritten as the symbol ǫ, a terminal symbol of the RTG.",26,27
4322,8266313,"The terminal symbols of the RTG are thus the tree identifiers and the symbol ǫ, and its nonterminals are X S and X A for each terminal or non-terminal X of SemXTAG.",13,14
4323,12229972,All numbers are mapped to a single <num> symbol.,10,11
4324,8581046,"To account for the optionality of adjunction, there are additional rules allowing any adjunction requirement to be rewritten as the symbol ǫ, a terminal symbol of the RTG.",21,22
4325,8581046,"To account for the optionality of adjunction, there are additional rules allowing any adjunction requirement to be rewritten as the symbol ǫ, a terminal symbol of the RTG.",26,27
4326,8581046,"The terminal symbols of the RTG are thus the tree identifiers and the symbol ǫ, and its nonterminals are X S and X A for each terminal or non-terminal X of SEMXTAG.",13,14
4327,8581046,"The semantics associated with the left-hand-side symbol (here, Sem;S;N;VP;V, with the semicolon representing semantic conjunction) are composed of the semantics associated with this rule and those associated with each of the right-hand-side symbols.",10,11
4328,8581046,"For instance, the above DCG rule becomes : rule(s,init,Top,Bot,n0V,Sem;S;NP;VP;V) --> [runs], {lexicon(runs,n0V,[run])}, ... We implement restrictions on adjunctions by adding an additional argument to the grammar symbols, namely a vector of non-negative integers representing the number of non-null adjunctions of each type that are in the derivation subtree dominated by the symbol.",79,80
4329,8581046,"In DCG terms, a non-null adjunction of a category X is represented as the expansion of an x/aux symbol other than as ǫ.",23,24
4330,8581046,"So, for example, a DCG symbol associated with the vector [1,0,0,0,0], where the five dimensions of the vector correspond to the n, np, v, vp, and s categories, respectively, dominates a subtree containing exactly one n/aux symbol expanded by a non-epsilon rule, and no other aux symbol expanded by a non-epsilon rule.",7,8
4331,8581046,"So, for example, a DCG symbol associated with the vector [1,0,0,0,0], where the five dimensions of the vector correspond to the n, np, v, vp, and s categories, respectively, dominates a subtree containing exactly one n/aux symbol expanded by a non-epsilon rule, and no other aux symbol expanded by a non-epsilon rule.",49,50
4332,8581046,"So, for example, a DCG symbol associated with the vector [1,0,0,0,0], where the five dimensions of the vector correspond to the n, np, v, vp, and s categories, respectively, dominates a subtree containing exactly one n/aux symbol expanded by a non-epsilon rule, and no other aux symbol expanded by a non-epsilon rule.",62,63
4333,9711438,The diamond symbol shows the configuration which works best over all corpora.,2,3
4334,220445922,"The decoder takes a sample from the latent dimension z-space, and uses that as an input to output the question q. It receives a ""start"" symbol and proceeds to output a question word by word until it produces an ""end"" symbol.",30,31
4335,220445922,"The decoder takes a sample from the latent dimension z-space, and uses that as an input to output the question q. It receives a ""start"" symbol and proceeds to output a question word by word until it produces an ""end"" symbol.",47,48
4336,198183803,"the continuation is explicitly marked (e.g., with a '+' symbol at the end of an incomplete tweet), all connectives are annotated (even though the arguments may span across tweets).",13,14
4337,202719230,"While this information can be deduced from the position of the symbol within the structure, there is a benefit in making it more explicit.",11,12
4338,202719230,"We incorporate typing information by concatenating to the embedding vector of each input symbol one of three embedding vectors, S, E or R, where S is concatenated to structural elements (opening and closing brackets), E to entity symbols and R to relation symbols.",13,14
4339,202719230,"Referring Expressions The step-by-step system generates entities by first generating an indexed entity symbols, and then lexicalizing each symbol to the string associated with this entity in the input structure (i.e., all occurrences of the entity 11TH MISSISSIPPI IN-FANTRY MONUMENT will be lexicalized with the full name rather than ""it"" or ""the monument"").",23,24
4340,225062242,Pre-processing before pre-training • all users' mentions have been substituted with a placeholder (@USER); • all URLs have been substituted with a with a placeholder (URL); • emojis have been replaced with text (e.g. → :pleading face:) using Python emoji package; • hashtag symbol has been removed from hasthtags (e.g. #kadiricinadalet → kadiricinadalet); • extra blank spaces have been replac §ed with a single space; • extra blank new lines have been removed.,58,59
4341,225062242,In particular: • all users' mentions have been substituted with a placeholder (@USER); • all URLs have been substituted with a with a placeholder (URL); • emojis have been replaced with text (e.g. → :pleading face:) using Python emoji package; • hashtag symbol has been removed from hasthtags (e.g. #kadiricinadalet → kadiricinadalet); • extra blank spaces have been replaced with a single space.,54,55
4342,203694031,"News data includes the stock symbol, date and time issued, source, news headline, sentiment (0 for neutral news, 1 for positive news and -1 for negative news), polarity of negative sentiment, polarity of positive sentiment and polarity of neutral sentiment.",5,6
4343,203689782,"Like the ending part, the beginning part of sentence also is not just words which beginning with upper letter like 'The', 'Given', 'This', also include symbol character like '(', '-'.",35,36
4344,7919491,"verbargs-unlex, nounargs-unlex (114M, 195M items) Like the above, but only the head word and the top-1000 occurring words in the English-1M subcorpus are lexicalized -other words are replaced with a * W * symbol.",45,46
4345,227231283,In particular: • all users' mentions have been substituted with a placeholder (@USER) -only for retraining; • all URLs have been substituted with a with a placeholder (URL) -only for retraining; • emojis have been replaced with text (e.g. → :pleading face:) using Python emoji package -both for retraining and fine-tuning; • hashtag symbol has been removed from hasthtags (e.g. kadiricinadalet → kadiricinadalet) -both for retraining and fine-tuning; • extra blank spaces have been replaced with single spaces -both for retraining and fine-tuning.,67,68
4346,17206181,It is also equally applicable for alignment between a pair of symbol sequences representing either graphemes or phonemes. (,11,12
4347,17157887,"Features We use 86 standard shallow features extracted from the input (source) sentences and their corresponding translation (target) sentences, and also monolingual and parallel corpora: • source & target sentence lengths and their ratio; • source & target sentence type/token ratio; • average source word length; • source & target sentence unigrams, bigrams and trigram language model probabilities and perplexities obtained using the source/target side of the corpus used to train the SMT system as monolingual corpus; • target sentence trigram language model probability trained on a POS-tagged corpus of the target language (Europarl); • average frequency of unigrams, bigrams and trigrams in the source sentence belonging to each frequency quartile of a corpus of the source language (Europarl); • average frequency of source sentence unigrams in a source language corpus (Europarl); • percentage of unigrams, bigrams and trigrams in the source sentence belonging to each frequency quartile of a corpus of the source language (Europarl); • percentage of distinct unigrams, bigrams and trigrams in the source sentence seen a corpus of the source (Europarl); JEC 2010 ""Bringing MT to the User"" Denver, CO • average number of translations per source word in the sentence, as given by probabilistic dictionaries produced by GIZA++ (Och and Ney, 2003) extracted from the parallel corpus used to train the SMT system, thresholded using different percentages (0.01, 0.05, 0.10, 0.20, 0.50), unweighted or weighted by the direct or inverse frequency of the words in the source language corpus; • percentages of punctuation symbols, numbers, content-/ non-content words in the source & target sentences and their ratio; • number of mismatching opening/closing brackets in the target sentence; • whether target sentence contains mismatched quotation marks; • number of mismatches of each of the following superficial constructions between the source and target sentences: brackets, each punctuation symbol (and all of them together), numbers, either in absolute terms or normalized by sentence length; and • proportion of words in the source and target with initial/all/none capital letters, or only capital letters and symbols, and the ratio between the proportions of words in the source and target sentences with such case patterns 3 .",362,363
4348,16311779,The % symbol was converted to percent.,2,3
4349,2659484,"Instead of a single NP symbol, these models hypothesize that there are many different NP symbols, NP 1 , . . . ,",5,6
4350,2659484,"The splits are evaluated based on an information gain criteria, and splits that are not useful are merged back into their parent symbol, resulting in a smaller grammar (if the symbols B 1 and B 2 are merged back into B, the rules A → B 1 C and A → B 2 C are merged into A → B C).",23,24
4351,2659484,The smoothing procedure joggles the probability mass of the grammar and moves some probability from the split symbol to its parent.,17,18
4352,2659484,"The pre-splitting according to agreement-features properties caused data sparseness, aided over-fitting, and hurt parsing performance: The smoothing procedure of the BerkeleyParser shares some probability-mass between various splits of the same symbol, but was not applied in our case (no information flowed between, for example, NP Masc,Plural and NP Masc,Singular ).",41,42
4353,2659484,"‫ם‬ ‫חכ‬ ‫הוא‬ ('he [is] smart masc '), ‫ה‬ ‫דהימ‬ ‫מ‬ ‫היתה‬ ‫היא‬ ('she was amazing/ fem '), but not with nouns ‫ל‬ ‫סמ‬ ‫היתה‬ ‫היא‬ ('she was a-symbol masc ').",42,43
4354,1585636,"♥ = ♠ 0, else (3) where the symbol ♣ is a placeholder for a possible target translation (up to 4 words), the symbol ♥ indicates a contextual (lexical or semantic) element for the verbal predicate v, and the symbol ♠ represents the value of ♥.",11,12
4355,1585636,"♥ = ♠ 0, else (3) where the symbol ♣ is a placeholder for a possible target translation (up to 4 words), the symbol ♥ indicates a contextual (lexical or semantic) element for the verbal predicate v, and the symbol ♠ represents the value of ♥.",29,30
4356,1585636,"♥ = ♠ 0, else (3) where the symbol ♣ is a placeholder for a possible target translation (up to 4 words), the symbol ♥ indicates a contextual (lexical or semantic) element for the verbal predicate v, and the symbol ♠ represents the value of ♥.",48,49
4357,14036650,"When a discrete symbol w is used as a neural network's input, the corresponding embedding vector is assumed.",3,4
4358,9927054,"For the case of keyboard input, each h i is a discrete symbol representing one of the 26 letters.",13,14
4359,243959875,We also show the results of the state-of-the-art models presented in the literature or public leaderboards (* symbol is used to refer to the unpublished works) for the available datasets (upper block).,24,25
4360,2578674,"Since the language model includes the possibility of a word being the first word of a sentence, we may add a special ""start symbol"" to every disordered sentence and force it to be the first word in the output sentence.",25,26
4361,1642392,"2015) , and replace a word with the unknown-word symbol with probability that is inversely proportional to the frequency of the word.",12,13
4362,1642392,A word w appearing #(w) times in the training corpus is replaced with the unknown symbol with probability p unk (w) = α #(w)+α .,18,19
4363,6666052,"A common approach is to designate a special ""unknown-word"" symbol, whose associated vector will be used as the word representation whenever an OOV word is encountered at test time.",13,14
4364,6666052,"In order to train the unknown-word vector, a possible approach is to replace all the words appearing in the training corpus less than a certain number of times with the unknown-word symbol.",36,37
4365,6666052,"During training, we replace a word with the unknown-word symbol with probability that is inversely proportional to frequency of the word.",12,13
4366,6666052,"Formally, we replace a word w appearing #(w) times in the training corpus with the unknown symbol with a probability: p unk (w) = α #(w) + α Using this approach we learn a vector representation for unknown words with minimal impact on the training of sparse words.",20,21
4367,6855603,"Thus, an alleged phonological rule of the type: ""V -+ (back)/..."" is meaningless unless the symbol ""V"" itself (and not only objects classified as ""V""s) appears in the text.",21,22
4368,13251960,"The database can be searched by gene symbol, chromosomal location, or disorder.",7,8
4369,13251960,"During processing, SemGen normalizes gene symbols using the preferred symbol from LocusLink.",10,11
4370,13251960,The final interpretation with LocusLink gene symbol is shown in (9).,6,7
4371,13251960,"9) EGR1|INTERACT_WITH|AR As we retrieve the LocusLink symbol for a gene, we also get the GO terms associated with that gene.",8,9
4372,3239175,The expansion continues until the partial hypothesis ends with a special end-of-sentence symbol that occurs at the end of all N-best strings.,16,17
4373,6444309,"The function labels are indicated by appending a -XXX suffix to the non-terminal symbol, where the XXX mark the function of the node.",15,16
4374,13156058,"Transition-Based Dependency Parsing We represent an input sentence as a string w = w 0 • • • w n , n ≥ 1, where each w i with i = 0 is a lexical symbol and w 0 is a special symbol called root.",38,39
4375,13156058,"Transition-Based Dependency Parsing We represent an input sentence as a string w = w 0 • • • w n , n ≥ 1, where each w i with i = 0 is a lexical symbol and w 0 is a special symbol called root.",45,46
4376,13156058,"Set V w = {i | 0 ≤ i ≤ n} denotes the symbol occurrences in w. For i, j ∈ V w with i = j, we write i → j to denote a grammatical dependency of some unspecified type between w i and w j , where w i is the head and w j is the dependent.",15,16
4377,13156058,At each step the parser applies some transition that updates the stack and/or consumes one symbol from the input.,15,16
4378,13156058,"Again, we use the vertical bar to denote the append operator, and write β = i|β to indicate that i is the first symbol occurrence of β; consequently, we have β = [i + 1, . . . ,",25,26
4379,13156058,We use the symbol to denote the binary relation formed by the union of all transitions of a parser.,3,4
4380,13156058,"n], ∅), consisting of an empty stack, a buffer with all the nodes representing the symbol occurrences in w, and an empty set of constructed dependencies (arcs).",20,21
4381,13156058,"For technical reasons, we assume that our parser starts with a symbol $ ∈ V w in the stack, denoting the bottom of the stack.",12,13
4382,13156058,"Finally, when processing substrings that are entirely in β R (i ≥ ) we can restrict the transitions that we explore to those that generate arcs that either are in the gold tree t G , or have a parent node which is not present in γ (see conditions in Algorithm 1 Computation of the loss function 1: T [0, 1]([$, $0]) ← 0 shift node 0 on top of empty stack symbol $ 2: for i ← 1 to − 1 do 3: T [i, i + 1]([γ[i − 1], γ[i − 1]γ[i]]) ← 0 shift node γ[i] with γ[i − 1] on top of the stack 4: for i ← to |γ| do 5: for h ← 0 to i − 1 do 6: T [i, i + 1]([γ[h], γ[h]γ[i]]) ← 0 shift node γ[i] with γ[h] on top of the stack 7: for d ← 2 to |γ| do consider substrings of length d 8: for i ← max{0, − d} to |γ| − d do i = beginning of substring 9: j ← i + d j − 1 = end of substring 10: PROCESSCELL(T , i, i + 1, j) We omit the range k = i + 2 to max{i + 2, } − 1 11: for k ← max{i + 2, } to j do factorization of substring at k 12: PROCESSCELL(T , i, k, j) 13: return T [0, |γ|]([$, $0]) + i∈[0, −1] Lc(σ[i], tG) 14: procedure PROCESSCELL(T , i, k, j) 15: for each key [h1, h2h3]) defined in T [i, k] do 16: for each key [h3, h4h5]) defined in T [k, j] do h3 must match between the two entries 17: loss la ← T [i, k]([h1, h2h3]) + T [k, j]([h3, h4h5]) + δG(h5 → h4) 18: if (i ) ∨ δG(h5 → h4) = 0 ∨ (h5 ∈ γ) then 19: T [i, j]([h1, h2h5]) ← min{loss la , T [i, j]([h1, h2h5])} cell update la 20: lossra ← T [i, k]([h1, h2h3]) + T [k, j]([h3, h4h5]) + δG(h4 → h5) 21: if (i < ) ∨ δG(h4 → h5) = 0 ∨ (h4 ∈ γ) then 22: T [i, j]([h1, h2h4]) ← min{lossra, T [i, j]([h1, h2h4])} cell update ra 23: loss la 2 ← T [i, k]([h1, h2h3]) + T [k, j]([h3, h4h5]) + δG(h5 → h2) 24: if (i < ) ∨ δG(h5 → h2) = 0 ∨ (h5 ∈ γ) then 25: T [i, j]([h1, h4h5]) ← min{loss la 2 , T [i, j]([h1, h4h5])} cell update la 2 26: lossra 2 ← T [i, k]([h1, h2h3]) + T [k, j]([h3, h4h5]) + δG(h2 → h5) 27: if (i < ) ∨ δG(h2 → h5) = 0 ∨ (h2 ∈ γ) then 28: T [i, j]([h1, h2h4]) ← min{lossra 2 , T [i, j]([h1, h2h4])} cell update ra 2 lines 18, 21, 24, 27), because we know that incorrectly attaching a buffer node as a dependent of another buffer node, when the correct head is available, can never be an optimal decision in terms of loss.",82,83
4383,250390482,"For each frame in input speech features, the model first predicts either a token label from the vocabulary or a special blank symbol.",23,24
4384,250390482,"When a label is predicted, the model continues to predict the next output; when the model predicts a blank symbol, it proceeds to the next frame indicating no more labels can be predicted with current frames.",21,22
4385,8294974,"We represent an input sentence as a string w = w 0 • • • w n , n ≥ 1, where token w 0 is a special root symbol and, for each i ∈ [1, n], token w i = (i, a i , t i ) encodes a lexical element a i and a part-ofspeech tag t i associated with the i-th word in the sentence.",30,31
4386,8294974,"Arc (h → d) is not in A, and the only way we could create (h → d) from c is by reaching a new configuration with d in the topmost stack symbol, which amounts to say that σ 1 can be reduced by a correct transition.",37,38
4387,14229358,"If i is larger than the length of the input sequence, an embedding of a special E symbol is used, similarly to (Faruqui et al.,",18,19
4388,14229358,"Every time the network outputs the ""step"" symbol, p input is promoted by setting it with the embedding that represents the next input sequence position.",9,10
4389,14229358,"Every time the network outputs a symbol other than the ""step"" symbol, p output is promoted by setting it with the embedding for the next output sequence position.",6,7
4390,14229358,"Every time the network outputs a symbol other than the ""step"" symbol, p output is promoted by setting it with the embedding for the next output sequence position.",13,14
4391,14229358,"o i−1 -The feedback input, containing the embedding of the prediction (either a character, an integer representing an index in the input, or the ""step"" symbol) from the previous decoder RNN step.",31,32
4392,14229358,"After that, if the next output character is not the epsilon symbol as seen in the alignment in Figure 2 .2 we also produce a ""step"" action.",12,13
4393,26316355,"Parser Operation The dependency parser is initialized by pushing the words and their representations (we discuss word representations in Section 4) of the input sentence in reverse order onto B such that the first word is at the top of B and the ROOT symbol is at the bottom, and S and A each contain an empty-stack token.",46,47
4394,26316355,"Processing completes when B is empty (except for the empty-stack symbol), S contains two elements, one representing the full parse tree headed by the ROOT symbol and the other the empty-stack symbol, 7 and A is the history of transition operations taken by the parser.",13,14
4395,26316355,"Processing completes when B is empty (except for the empty-stack symbol), S contains two elements, one representing the full parse tree headed by the ROOT symbol and the other the empty-stack symbol, 7 and A is the history of transition operations taken by the parser.",31,32
4396,26316355,"Processing completes when B is empty (except for the empty-stack symbol), S contains two elements, one representing the full parse tree headed by the ROOT symbol and the other the empty-stack symbol, 7 and A is the history of transition operations taken by the parser.",39,40
4397,224704618,"The input document is fed into the bidirectional encoder, then the contextual embeddings of the i th [CLS] symbol are used as the sentence representations.",21,22
4398,14391663,The first rule in the grammar expands from a start symbol S to a special START record R(start).,10,11
4399,235097290,"In contrast, green serves as a religious/sacred symbol in Islam as Muhammad's favorite color. (",10,11
4400,237578861,"In the first form, it contains Z as a symbol for the time expression, see the examples in the rows for date, time, duration and set in the range of values category in Table 2 .",10,11
4401,231855232,"In our case the prompt is derived from (e 1 , e 2 , r), followed by a special symbol, and we train the LM to produce the corresponding sentence s. To derive the prefix we apply a pre-defined template associated with each relation r 9 .",22,23
4402,8646729,"Given this, lexical selection will select the three trees shown in Figure 3 , together with the relative clause tree betanxBEnx for the relation symbol coupe.",25,26
4403,8646729,"For instance, given the KB symbol equippedWith, while lexical selection will return the set of trees shown in Table 1 , if the hypertagger predicts the SubjRelPU class for this literal, then the tree combination step of the generation algorithm will only consider the trees labeled with that syntactic class.",6,7
4404,8646729,"It includes 10,020 lexical entries for 1,296 concepts and relations and has an average lexical ambiguity rate (number of lexical entries per KB symbol) of 7.73.",24,25
4405,8646729,"From this, we extract for each KB symbol in the input query the TAG tree and the syntactic class used to produce this verbalization.",8,9
4406,5012981,"Furthermore, φ(t) and ψ(s) are binary vectors indicating whether a KB symbol/word is present or absent in t/s. Thus, f (t) and g(s) are the embeddings of t and s and S t/s scores their similarity by taking their dot product.",14,15
4407,1966007,"In this paper, we will use Conditional Random Fields (CRF) (Lafferty, 2001) to capture the cognitive model in terms of finite state automata, with each form state dependent on the previous form state and on the current symbol word being processed.",44,45
4408,1966007,"4 The Proposed Computational Cognitive model In our proposed approach, consider each observed symbol as the tuple: {word, POS tag, NP chunk number}.",14,15
4409,9583401,"Furthermore, it produces all sub components of a parse and thus produces a useable result without the need to parse to a specific symbol.",24,25
4410,36106310,"the prominent stretch of simultaneous speech in this FT matches a stretch of speech that is of a ""weaker"" category than the present in a turn type strength hierarchy, TSFt, implicitly reflected in Table 1 but more formally defined as: TSH: r > t > a > f > b > l > m > u > where the symbol "">"" (here) stands for the two-place predicate ""is stronger than"".",65,66
4411,2512012,"We represent an input sentence as a string w = w 0 • • • w n , n ∈ N 0 , where token w 0 is a special root symbol, and each w i with i ∈ [1, n] is a lex- ical token.",31,32
4412,2512012,We use symbol to denote the union of all transition relations of a parser.,2,3
4413,16223112,We lowercase the remaining words and introduce # as start symbol and $ as end symbol of a word.,10,11
4414,16223112,We lowercase the remaining words and introduce # as start symbol and $ as end symbol of a word.,15,16
4415,15143281,"Similarly, all mentions of user-names (which are denoted by the @ symbol) were replaced with the token ""USERNAME"", since they also can not help in sentiment classification.",15,16
4416,247594288,"5) sHE concat (s, d) = P E(as, d 2 )|P E(bs, d 2 ) (6) where the symbol | denotes vector concatenation.",27,28
4417,201717296,"However, the list also contains interesting political lexicon, such as йолка ('Christmas tree': the distorted ukrainized spelling of the Russian word, which became a symbol of the people's resistance to political violence during the Ukrainian revolution of dignity in 2013-2014) and проффесор (again, a distorted spelling of the word 'professor', which was used for mocking the fugitive pro-Russian president, who held this title, but allegedly misspelt it in an official document).",31,32
4418,16473757,"A token can be a word or a punctuation symbol, and each of these neighboring tokens must be in the same sentence as .",9,10
4419,339805,Differences that are not statistically significant are marked with the symbol • .,10,11
4420,17015871,"This hypernym list contains concepts such as 'a spatially limited location', 'a two-dimensional shape', 'a written or printed symbol', 'a visual attribute of things that results from the light they emit or transmit or reflect'.",27,28
4421,65485,"All words in the training set were downcased, all numbers were converted into a generic symbol, and all singleton unigrams and bigrams were removed.",16,17
4422,122829,"The model consists of an encoder-decoder neural network with a dedicated control mechanism: in each step, the model attends to a single input state and either writes a symbol to the output sequence or advances the attention pointer to the next state from the bi-directionally encoded sequence, as described visually in Figure 1 .",32,33
4423,122829,"This analysis also sheds light on the representations such models learn for the morphological inflection generation task, showing how they encode specific features like a symbol's type and the symbol's location in a sequence.",26,27
4424,122829,"This analysis also sheds light on the representations such models learn for the morphological inflection generation task, showing how they encode specific features like a symbol's type and the symbol's location in a sequence.",31,32
4425,122829,"We implement this behavior using an encoder-decoder neural network, with a control mechanism which determines in each step of the decoder whether to write an output symbol or promote the attention pointer the next element of the encoded input.",29,30
4426,122829,"s i−1 ∈ R E , which is an embedding for the predicted output symbol in the previous decoder step.",14,15
4427,122829,"Previous works on neural sequence transduction include the RNN Transducer (Graves, 2012) which uses two independent RNN's over monotonically aligned sequences to predict a distribution over the possible output symbols in each step, including a null symbol to model the alignment.",41,42
4428,122829,2016) improved this by replacing the null symbol with a dedicated learned transition probability.,8,9
4429,237485277,"Specifically, we treat each variable (e.g., x, y), math operator (e.g., =, ×, +), and numeric value (e.g., integers, fractions, and decimal numbers) as a single math symbol.",44,45
4430,14378350,It is represented by /ə/ symbol (Naim 2009) .,5,6
4431,14378350,"When the half consonant cluster (Virama/Halant ◌् ) appears in Devanagari script, it is mapped in English with the letter symbol 'a`' which appears in between two consonant phonemes.",25,26
4432,584909,"If this module found this symbol to be title or surname, the word next and previous to this token as the case may be for title or surname respectively, will be transliterated not translated.",5,6
4433,224814107,BERT uses a masked language modeling (MLM) strategy to corrupt an input sentence by replacing some tokens with a [MASK] symbol.,24,25
4434,21710637,"That is, for each pair of transitions with the same inner symbol, q 1 a:b/p 1 − −−−− → r 1 q 2 b:c/p 2 − −−−− → r 2 , the composed transducer has a transition q 1 q 2 a:c/p 1 p 2 − −−−−− → r 1 r 2 .",12,13
4435,21710637,"Input Transducers: M 1 = (Q 1 , Σ, Γ, s 1 , F 1 , δ 1 ) M 2 = (Q 2 , Γ, ∆, s 2 , F 2 , δ 2 ) Output Transducer: M 1 • M 2 1: A ← {s 1 s 2 } Queue of states to process 2: Q ← {s 1 s 2 } Set of states created so far 3: δ ← ∅ Transition function 4: while |A| > 0 do 5: q 1 q 2 ← pop(A) 6: for q 1 a:b/p 1 − −−−− → r 1 ∈ δ 1 do 7: for q 2 b:c/p 2 − −−−− → r 2 ∈ δ 2 do 8: δ(q 1 q 2 , a, c, r 1 r 2 ) += p 1 p 2 9: if r 1 r 2 Q then 10: Q ← Q ∪ {r 1 r 2 } 11: push(A, r 1 r 2 ) 12: return (Q, Σ, ∆, s 1 s 2 , F 1 × F 2 , δ) la gata una R 0 1 1 2 T 1 2 O the one all transitions from q 1 with all transitions from q 2 that have the same inner symbol.",250,251
4436,21710637,"m − 1, then state q's outgoing transitions on inner symbol b can be found starting at the offset stored in R[qm + b].",12,13
4437,21710637,O[k] is the outer symbol of the kth transition. •,5,6
4438,21710637,Both transducers are sorted according to their inner symbol on the CPU and copied to the device.,8,9
4439,21710637,"The outer loop launches a CUDA kernel for each inner symbol b ∈ Γ. For example, to compose the start states in Figure 1 , three kernels will be launched (one for la, gata, and una).",10,11
4440,21710637,Our approach obtains better speedups when the input transducers are able to generate a large amount of edges for each symbol b and each state tuple on the result.,20,21
4441,21710637,"The two inner loops represent the threads of the kernel; each composes a pair of transitions sharing an inner symbol b. Because these transitions are stored contiguously (Figure 2 ), the reads can be coalesced, meaning that the memory reads from the parallel threads can be combined into one transaction for greater efficiency.",20,21
4442,21710637,"Figure 2 shows how the edges for a transducer are stored in global memory to achieve coalesced memory operations each time the edges of a symbol b associated with a state tuple q 1 ,q 2 need to be composed.",25,26
4443,21710637,"For symbol gata, no edges can be composed given R[gata + 1] − R[gata] = 0 on both machines, meaning that no edges read or output that symbol.",1,2
4444,21710637,"For symbol gata, no edges can be composed given R[gata + 1] − R[gata] = 0 on both machines, meaning that no edges read or output that symbol.",31,32
4445,21710637,"If n 1 edges can be composed for a symbol b on one machine and n 2 from the other one, the kernel will generate n 1 n 2 edges.",9,10
4446,36116614,"We have assumed that only the syntax properties of dropped words matter for reordering of the rest of the sentence, so we have simply substituted these words with a special symbol that is guaranteed not to occur anywhere else in the data sets.",31,32
4447,36116614,"It is worth noting, though, that step 4 can be done separately as well, because single words are always left unchanged during the translation process (except for special symbol standing for dropped words), and step 9 isn't really configurable, so further we will only focus on extraction and scoring of the rules.",32,33
4448,36116614,Also we have manually added an entry into the rules table in order to delete the words that shouldn't be present in the target sentence (if we are substituting all these words with a special symbol as described in section 2.1).,37,38
4449,228076553,"2015) over the last layer encoder hidden states h t , we learn a context vector c t ∈ R 2H that is used to summarize the source attentional context when we predict target symbol ŷt ; we initialize c 0 = 0.",35,36
4450,228076553,"At each time step, we feed two inputs to the decoder: the context vector c t−1 ∈ R 2H and the embedding of the predicted decoder output symbol e ŷt−1 ∈ R E from the previous time step.",29,30
4451,228076553,"The target gender g is mapped to an embedding e g ∈ R J which is learned during training and concatenated together with the decoder hidden state h (d) t , the context vector c t , and the embedding of the predicted symbol from the previous time step e ŷt−1 to create vector z t = [h (d) t ; c t ; e ŷt−1 ; e g ] ∈ R H+2H+E+J .",45,46
4452,18375349,It is represented by /ə/ symbol (Naim 2009) .,5,6
4453,196200400,Normalizing hashtags by deleting underscores and the # symbol. •,8,9
4454,15049973,The first layer of the network projects these symbols onto an embedding space (each type of symbol has its own embedding matrix).,17,18
4455,338760,"tweet- • audience-features, capturing properties of the addressee of tweets, in those cases that a tweet is directed at someone (via the @ symbol).",28,29
4456,926149,"Initially, S 0 and M 0 are empty, and B 0 contains the input sentence with the first word at the front of B and a special root symbol at the end.",30,31
4457,12114060,Each token is assigned a classification symbol depending on its type.,6,7
4458,12114060,"M is a metric unit and C is a currency symbol lying in the dictionary of metric standards and currencies, respectively.",10,11
4459,8661576,"For the Twitter conversations, this was extended to replacing all user mentions (words beginning with @) throughout the utterance with a placeholder '@user' symbol, as these are often repeated in a conversation.",28,29
4460,2973141,"This is not quite the same as removing the word from the vocabulary -true out-of-vocabulary words are mapped to the embedding for the 'unknown word' symbol, whereas these 'pruned-out' words are mapped to a zero embedding.",31,32
4461,51870827,"Once all valid triplets are generated, the decoder will generate NA triplets, which means ""stopping"" and is similar to the ""eos"" symbol in neural sentence generation.",27,28
4462,5072513,"Specifically, we identify the time steps at which the decoder produces unknown words (denoted by the special symbol UNK).",19,20
4463,16222659,"We can reduce the vocabulary size, by replacing words that occur below a minimum frequency threshold with a special unknown symbol (UNK).",21,22
4464,16222659,We use a special end of utterance symbol (END) to create the input sequence.,7,8
4465,16222659,We can reduce the vocabulary size by replacing out of vocabulary (OOV) with a special unknown symbol (UNK).,18,19
4466,16222659,We can preserve linguistic structure by assigning a unique symbol to each OOV word.,9,10
4467,16222659,"We can thus restrict assigning a new symbol only within a conversation(Q1, Q2, R1) and reuse the symbols across conversations.",7,8
4468,16222659," Syntactic sequence model uses NSU question (Q2), conversation context (Q1, A1) and a symbol map, to generate the resolved question (R1).",20,21
4469,16222659,"This symbol map helps in two important ways: it helps preserve the linguistic structure, and at the time of prediction it helps replace unknown symbol with the original word.",1,2
4470,16222659,"This symbol map helps in two important ways: it helps preserve the linguistic structure, and at the time of prediction it helps replace unknown symbol with the original word.",26,27
4471,16222659,"However, it is not possible to determine which word does this symbol correspond to, as there will be typically many UNK words in an input sequence.",12,13
4472,16222659,Syntactic sequence model addresses this problem by having a symbol map for the current conversation.,9,10
4473,16222659,Syntactic sequence model however focuses solely on the position of OOV word in the sequence to assign a new unknown symbol and completely discards similarity between OOV words.,20,21
4474,16222659,Each OOV word can then be assigned a new symbol based upon its word category index.,9,10
4475,16222659,"Greece UNK1 sport UNK2 football UNK3 India UNK4 Semantic sequence model assigns a new symbol to each OOV word, based on the word category index.",14,15
4476,16222659,"Semantic sequence model takes as input a NSU question Q2, conversation context (Q1, A1) and a cluster symbol map.",21,22
4477,16222659,"As compared to syntactic sequence model, we can have multiple OOV words assigned to the same cluster symbol token.",18,19
4478,16222659,We can replace the cluster symbol token (such as CL3) by replacing it with Q2 OOV word that was assigned to this cluster.,5,6
4479,16222659,"Input sequence is generated by concatenating question (Q1), answer (A1) and NSU question (Q2) with a special end of utterance symbol (END).",27,28
4480,16222659,Maximum length of sequence symbol map for a conversation was found to be 15.,4,5
4481,7730925,"When a symbol is revisited, we replace the symbol with ""-RET-"".",2,3
4482,7730925,"When a symbol is revisited, we replace the symbol with ""-RET-"".",9,10
4483,7730925,"We additionally add the revisited symbol before ""-RET-"" to decide where the reentrancy is introduced to.",5,6
4484,7730925,The sequence also captures the branching information of each relation explicitly by representing it with a start symbol and an end symbol specific to that relation.,17,18
4485,7730925,The sequence also captures the branching information of each relation explicitly by representing it with a start symbol and an end symbol specific to that relation.,21,22
4486,18772898,"2013b) , we directly take their word2vec model and replace anchor surface with the entity symbol in the training corpus.",16,17
4487,2782776,"German and Russian sentences tend to be longer than English, hence require more consecutive waits before being able to emit the next English symbol.",24,25
4488,512323,For each input symbol w t the source states of all possible transitions can be read in a coalesced form and stored in shared memory allowing faster execution times.,3,4
4489,512323,"δ : Σ → R Q×Q is the transition function: if M is in state q and the next input symbol is a, then δ[a][q, q ] is the weight of going to state q .",21,22
4490,512323,"Parallel Algorithm Our parallel implementation is based on Algorithm 1 for Viterbi and Algorithm 2 for forwardbackward, but parallelizes the loop over t, that is, over the transitions on symbol w t .",32,33
4491,512323,"For each input symbol a, the transitions on S and T are sorted first by source state and then by target state; this improves memory locality slightly.",3,4
4492,512323,"For each input symbol w t , one thread is launched per transition, that is, for each nonzero entry of the transition matrix δ[w t ].",3,4
4493,3042025,"In this work, the trigram language model was employed to calculate the symbol emission probability of the HMM.",13,14
4494,3042025,"2006) also used an HMM, but employed output symbol probabilities produced by an SVM classifier, instead of emission probabilities obtained from a trigram language model.",10,11
4495,12258556,"In this step, the system takes the target (frame-evoking) phrase t and corresponding frame type f predicted by the previous stages, and independently fills each role of f with a word or phrase from the sentence, or the symbol OTHER to indicate that the role has no (local) overt argument.",45,46
4496,202540773,"and the following two possible extracted documents: ""The hydrogen atom is an electrically neutral atom, usually denoted using the symbol H."", and ""The hydrogen is a chemical element with a single electron orbiting its nucleus.""",22,23
4497,229365724,"In the sentence fragment, we replace each masked token to a special symbol [M], so the number of words in the sentence is not changed.",13,14
4498,229365724,"In the unsupervised setup, we mask the fragment by replacing consecutive tokens with symbol [M] from random start position u. It first chooses 30% from input tokens, and each i-th token will be replaced as (1) an unchanged i-th token by 80% of the time, (2) a random token by 10% of the time, and (3) a masked token [M] by 10% of the time.",14,15
4499,229365724,"For the test set, we replace symbol £ to ""pound"" in source sentences as pre-processing.",7,8
4500,8941980,"A DFA is a finite state machine, where for each pair of state and input symbol, there is one and only one transition to a next state.",16,17
4501,8941980,"D R starts processing of an input sequence from a start state s R , and for each input symbol, it makes a transition to a state given by a transition function Φ R .",19,20
4502,8941980,"Whenever D R lands in an accept state, the symbol sequence till that point is accepted by D R .",10,11
4503,8941980,to denote terminal symbols in the symbol set Σ. Figure 5 details the algorithm used to build the AND/OR graph.,6,7
4504,8941980,N U LL else if R is a symbol a ∈ Σ then Return createN ode(name = a) else Decompose R such that R → R <regexp> if <regexp> is empty then if R == (X) or X+ or X * or X?,8,9
4505,5518086,"A pattern consists of the same symbols as character types, and contains no sequential redundant occurrences of the same symbol.",20,21
4506,250390634,"In each state, the model may emit a symbol on the first, the second or on both tapes.",9,10
4507,250390634,"M X Y end 1-2δ -τ τ τ τ δ δ ϵ ϵ 1-ϵ-τ 1-ϵ-τ Figure 1: Pair Hidden Markov Model The state M is the match state, where the model simultaneously emits one symbol on each tape.",38,39
4508,250390634,"In state X only a symbol to the first tape is emitted, and likewise for state Y and the second tape.",5,6
4509,250390634,"When the model reaches the end state (where no symbol is emitted), each tape contains a symbol sequence.",10,11
4510,250390634,"When the model reaches the end state (where no symbol is emitted), each tape contains a symbol sequence.",19,20
4511,250390634,"The alignment scores between any pair of symbol tokens are obtained by counting all threefold alignments where these symbols occur in the first and last column, weighted by the Hamming similarity between the entire first and last row.",7,8
4512,250390634,"For this purpose each symbol in the MSA is replaced by the corresponding Dolgopolsky class (Dolgopolsky, 1986) .",4,5
4513,250390634,"If the test data contain symbols not occurring in the training data, their emission probabilities are set to the minimal emission probability of any symbol from the training data, and emission probabilities are re-normalized in the trained pHMM.",25,26
4514,250390634,"As prediction, the symbol with the highest posterior probability was chosen.",4,5
4515,215826646,We use the symbol '|' as the concatenation operator.,3,4
4516,28950708,The compound word's characters is fed as input to the encoder and is translated to a sequence of characters representing the split words ('+' symbol acts as a separator between the generated split words).,28,29
4517,17973122,IF the current token AND both the two tokens before it match any of the patterns shown in Table 2 AND the next token is a word-token (other than a symbol) THEN the current token is a sentence end marker.,33,34
4518,2402112,"The frequent sentence starter heuristic, for example, tests whether a given word w 2 forms a collocation with a preceding sentence boundary w 1 , that is, with the sentence boundary symbol <S> inserted by the type-based first stage of Punkt.",34,35
4519,2402112,"The system never reclassifies a period following a possible initial as a sentence boundary marker because we assume that if a single letter is indeed not used as an abbreviation but, for example, as a mathematical symbol or if it is an ordinary word, there will usually be enough occurrences of this type without a final period so that the type-based stage will classify all periods following instances of the type in question as sentence boundary periods.",38,39
4520,2402112,Punkt is conceived as a language and domain-independent corpus preprocessing tool that can be used out of the box for all languages that use an alphabetic writing system and employ the same symbol to mark abbreviations and the end of sentence.,34,35
4521,2402112,It also induces a list of abbreviations from the training corpus by considering as an abbreviation every token in the training corpus that contains a possible end-of-sentence symbol but does not indicate a sentence boundary.,31,32
4522,15665215,"This consists of a quintuple (G, KB, KB root , λ, M ) where: i) G = (V G , E G ) is a noisy knowledge graph; ii) KB = (V KB , E KB ) is a companion knowledge base providing a ground-truth taxonomy; iii) KB root is the root node of the reference knowledge base KB (if several top-level nodes exist, an artificial root can be created by connecting them all); iv) λ is a conventional symbol to represent the ""undefined concept"", i.e., a place-holder for empty mappings; v) M : V G → V KB ∪ {λ} is the function, which maps nodes of V G into nodes of V KB or into the undefined concept λ.",100,101
4523,500806,Most of these errors involve incorrectly omitting a punctuation symbol.,9,10
4524,85152,We add a symbol for topic match (for the context-dependent tasks).,3,4
4525,13743580,The feature value is also not defined if the token itself is a punctuation symbol or contains any special symbol or digit.,14,15
4526,13743580,The feature value is also not defined if the token itself is a punctuation symbol or contains any special symbol or digit.,19,20
4527,12241221,"In particular, w N represents the question mark symbol '?'.",9,10
4528,7044889,"1990) introduced the idea of L0 as the basic bootstrap language task based on a simple toy world model, while Harnad (1987; 1990; 1991) used the term ""symbol grounding"" and argued strongly that even simulated worlds weren't enough, and earlier still (Hayes, 1979 ) had used the phrase ""naive physics"" to describe what he thought was needed.",34,35
4529,5167474,"As a preprocessing step, low frequency (< 100) predicates and arguments are mapped to a special UNK (unknown) symbol.",23,24
4530,5167474,"Similarly, arguments consisting of only digits are mapped to a NUMB (number) symbol.",15,16
4531,5167474,"During testing, predicate and arguments not observed during training are mapped to the same UNK symbol.",16,17
4532,207906720,"The top and bottom blocks represent 'unofficial' submissions, which are not considered for the primary ranking because they used training data beyond the white-listed resources (indicated by the symbol ""∦""), arrived after the closing deadline ("" §""), or were prepared by the task co-organizers as points of reference ("" †"").",34,35
4533,12836168,The Universal Dependencies Treebank is more consistent in this respect: it separates entities like '1$' into two tokens: a numeral (NUM) and a symbol (SYM).,30,31
4534,12836168,"This left us with 1564 words from all Universal Tag classes (excluding PUNCT, X and SYM, as we hardly want to predict punctuation or symbol tag).",27,28
4535,248779959,"We feed the resulting vector into a seq2seq model and encode the surface token as a sequence of characters using a BiLSTM, followed by a decoder that generates an output sequence of characters, using space as a special symbol signaling morphological boundaries.",40,41
4536,248779959,"The decoder LSTM outputs a sequence of characters, where a special empty symbol indicates a morphological segment boundary.",13,14
4537,248779959,"For tasks involving both segments and labels (Part-of-Speech Tagging, Morphological-Features Tagging, Named-Entity Recognition) we expand this network in a multi-task learning setup; when generating an end-of-segment (space) symbol, the model also predicts task label, and we combine the segment-label losses.",48,49
4538,162169,"The symbol indicates that the feature template also has a variant that is conjoined with r, the name of the role being filled; and indicates that the feature 30 Here is an example in the FrameNet 1.5 training data where this occurs.",1,2
4539,67855999,"There are several possible ways to condition the sequence-to-sequence model on z -for example, append z to the end of the input sequence, or use z as the START symbol for the decoder.",35,36
4540,216651425,"The global maximium in both cases would be achieved when the emission probabilities (or feature weights, in the case of MRF) map each observation symbol to a single state.",27,28
4541,14081838,"w N ) as a graph G S = V S , E S with the sentence words and the dummy root symbol as vertices and a directed edge between every pair of distinct words and from the root symbol to every word.",22,23
4542,14081838,"w N ) as a graph G S = V S , E S with the sentence words and the dummy root symbol as vertices and a directed edge between every pair of distinct words and from the root symbol to every word.",39,40
4543,33164500,"The symbol ✓denotes that the estimated value would be correct, while the symbol * denotes incorrect.",1,2
4544,33164500,"The symbol ✓denotes that the estimated value would be correct, while the symbol * denotes incorrect.",14,15
4545,33164500,"The symbol ✓denotes that the estimated value would be correct, while the symbol * denotes incorrect.",1,2
4546,33164500,"The symbol ✓denotes that the estimated value would be correct, while the symbol * denotes incorrect.",14,15
4547,17516627,"With the goal to handle errors caused by separation by a newline, we extended the search space by adding candidate pairs (either EE, or ET) that were separated by a new line, but for which the end of the line consisted of a comma, or colon symbol.",52,53
4548,17246494,"At each time step t , the decoder's hidden state is updated: z t = RNN(z t −1 , y t −1 , c t ) (1) The update uses the previous hidden state z t −1 , the previous target symbol y t −1 and the time dependent context c t , which is computed by an attention mechanism α t,t over the source sentences' context vectors: c t = T x ∑ t=1 α t ,t h t (2) α t ,t ∝ e f (z t −1 ,h t ) (3) g is a feedforward neural network with a softmax activation function in the output layer which returns the probability of the next target symbol.",45,46
4549,17246494,"At each time step t , the decoder's hidden state is updated: z t = RNN(z t −1 , y t −1 , c t ) (1) The update uses the previous hidden state z t −1 , the previous target symbol y t −1 and the time dependent context c t , which is computed by an attention mechanism α t,t over the source sentences' context vectors: c t = T x ∑ t=1 α t ,t h t (2) α t ,t ∝ e f (z t −1 ,h t ) (3) g is a feedforward neural network with a softmax activation function in the output layer which returns the probability of the next target symbol.",133,134
4550,17246494,a. What Swiss mathematician and teacher was responsible for instituting the use of the symbol for π in mathematical notation?,14,15
4551,17246494,b. What Swiss mathematicians and teachers were responsible for the introduction of the symbol for π in math notation?,13,14
4552,250390899,"3, (•) symbol and τ denotes dot product and scalar temperature parameter respectively.",5,6
4553,248512863,"3 Notation We will use four elementary symbols to facilitate the discussion of this task: 1M, 1F, 2M and 2F. The digit part of the symbol refers to the grammatical persons (1 st or 2 nd ) and the letter part refers to the grammatical genders (masculine or feminine).",30,31
4554,14633379,Multiple non-fact tokens are collapsed to a single symbol.,10,11
4555,19655453,Besides we have a fresh non-terminal which is the start symbol.,12,13
4556,10321376,"Using this example we describe the strategies we used for special cases in the transformation from Figure 1(b) to Figure 1(c): • ignore the unaligned target word, e.g. e 1 • the unaligned source word should follow its preceding word, the unaligned feature is kept with a * symbol, e.g. f * 2 is after f 1 • when one source word is aligned to multiple target words, only keep the alignment that links the source word to the first target word, e.g. f 4 is linked to e 5 and e 6 , only f 4 − e 5 is kept.",52,53
4557,16932170,"Represents an individual meaning, e.g., rain (as in the natural phenomenon), and can be encoded into a term (symbol).",24,25
4558,16932170,"Stands for the ""symbol"" in the triangle of reference. •",4,5
4559,16932170,An alternative line of thinking is to define the meanings first and then associate the corresponding terms (reference → symbol).,20,21
4560,1541076,Word types with less than five occurrences were replaced by a special UNKNOWN symbol.,13,14
4561,6880619,The aggregate PMI score of a pair of aligned strings (where gaps may be inserted at any position) is defined as the sum of the PMI scores of the aligned symbol pairs.,32,33
4562,6880619,"Matching a symbol with a gap incurs a penalty, with different penalties for initial and non-initial positions in a sequence of consecutive gaps.",2,3
4563,201685317,The first input to the decoder is the vector representation of a special start symbol v start ∈ IR dv .,14,15
4564,5545615,"The RNNG uses three different actions: • NT(X), where X ∈ N , introduces an open nonterminal symbol onto the stack, e.g., ""(NP""; • GEN(x), where x ∈ Σ, generates a terminal symbol and places it on the stack and buffer; and • REDUCE indicates a constituent is now complete.",20,21
4565,5545615,"The RNNG uses three different actions: • NT(X), where X ∈ N , introduces an open nonterminal symbol onto the stack, e.g., ""(NP""; • GEN(x), where x ∈ Σ, generates a terminal symbol and places it on the stack and buffer; and • REDUCE indicates a constituent is now complete.",44,45
4566,5545615,"2015) directly predict the sequence of nonterminals, ""shifts"" (which consume a terminal symbol), and parentheses from left to right, conditional on the input terminal sequence x, while Choe and Charniak (2016) used a sequential LSTM language model on the same linearized trees to create a generative variant of the Vinyals et al. (",17,18
4567,5545615,"The head of the composed representation for ""S"" at the top of the tree is attached to a special root symbol and functions as the head of the sentence.",22,23
4568,5880140,The analysis terminates when the buffer is empty and the only symbol in the stack is an axiom of the grammar.,11,12
4569,5880140,"If d 0 is a temporary symbol, there must be at least one non temporary symbol in S 1: .",6,7
4570,5880140,"If d 0 is a temporary symbol, there must be at least one non temporary symbol in S 1: .",16,17
4571,5880140,"If X is a temporary symbol and if B is empty, there must be a non-temporary symbol in either S 1: or D 1: .",5,6
4572,5880140,"If X is a temporary symbol and if B is empty, there must be a non-temporary symbol in either S 1: or D 1: .",19,20
4573,5880140,RR(X) s 0 is not a temporary symbol.,8,9
4574,5880140,RL(X) d 0 is not a temporary symbol.,8,9
4575,487442,"If N o is too small, then beneficial conflations (collapse punctuation marks, replace all digits with one symbol) are not found.",20,21
4576,227230699,"Among those, we noted the presence of a hyphen symbol produced by DeepL on the NEWS data set, different from the one used in human translations.",10,11
4577,17642623,"Examples of the frequently occurring concept specific words in Konkani newspaper corpus are: • taraMgAM-noun, decorated pole with symbol of tutelary divinity on its top. •",22,23
4578,17975165,The symbol U in the Table 4 and 5 stands for undefined cases.,1,2
4579,17975165,"For break cases, a syllable breaking symbol (i.e. B in the flowchart) is inserted at each syllable boundary of the input string.",7,8
4580,3101470,"Ethiopic Script is syllabic, each symbol represents 'consonant + vowel' characteristics.",6,7
4581,189999820,In 80% of the time we randomly replace the character by a symbol from the target vocabulary.,13,14
4582,6738867,"A context-free grammar is a formal system that describes a language by specifying how any legal text can be derived from a distinguished symbol called the axiom, or sentence symbol.",25,26
4583,6738867,"A context-free grammar is a formal system that describes a language by specifying how any legal text can be derived from a distinguished symbol called the axiom, or sentence symbol.",32,33
4584,6738867,"It consists of a set of productions, each of which states that a given symbol can be replaced by a given sequence of symbols.",15,16
4585,15271991,"We decided to output the context in which the preposition occurred as a string value in an additional attribute column named context in the basic information section (not included in Figure 2 ), while marking the pertinent preposition in the string by an unique symbol (**).",46,47
4586,465873,"In this paper, we treat a. (sets of cognates) as given, and focus on problems b. and c. 1  Given a corpus of cognate sets, 2 we first aim to find as much regularity as possible in the data at the sound (or symbol) level.",50,51
4587,465873,"We try to refrain from a priori assumptions or ""universal"" principles-e.g., no preference to align consonants with consonants, to align a symbol with itself, etc.",27,28
4588,465873,"We showed that aligning more than one symbol at a time-e.g., 2-2-gives better performance.",7,8
4589,465873,Berkeley models handle context by conditioning the symbol being generated upon the immediately preceding and following symbols.,7,8
4590,465873,"The simplest form of symbol alignment is a pair (σ : τ ) ∈ Σ × T , a single symbol σ from the source alphabet Σ with a symbol τ from the target alphabet T .",4,5
4591,465873,"The simplest form of symbol alignment is a pair (σ : τ ) ∈ Σ × T , a single symbol σ from the source alphabet Σ with a symbol τ from the target alphabet T .",21,22
4592,465873,"The simplest form of symbol alignment is a pair (σ : τ ) ∈ Σ × T , a single symbol σ from the source alphabet Σ with a symbol τ from the target alphabet T .",30,31
4593,465873,"Each symbol is coded as a vector of its phonetic features, e.g., j a l k a j a l g [ ζ χ φ ψ ] [ α β γ δ ] [ ξ π μ ω ] k = [ξ χ φ ψ].",1,2
4594,465873,"For each symbol, first we code a special Type feature, with values: K (consonant), V (vowel), dot (insertion / deletion), or # (word boundary).",2,3
4595,465873,"Contexts: While coding each feature of the symbol, the model is allowed to query a fixed and finite a set of candidate contexts.",8,9
4596,465873,"A node N in this tree holds a distribution over the values of feature X of only those symbol instances in the complete data that have reached node N , by following the context queries from the root downward.",18,19
4597,465873,"When coding a symbol α based on another symbol found in the context C of α-for example, C = (τ, −K, M): at level τ , position -K, and one of the features M-the next edge down the tree is determined by that feature's value; and so on, down to a leaf.",3,4
4598,465873,"When coding a symbol α based on another symbol found in the context C of α-for example, C = (τ, −K, M): at level τ , position -K, and one of the features M-the next edge down the tree is determined by that feature's value; and so on, down to a leaf.",8,9
4599,465873,"Each candidate refers to some symbol found on σ-level or τ -level, at some relative position P , and to one of that symbol's features F .",5,6
4600,465873,"Each candidate refers to some symbol found on σ-level or τ -level, at some relative position P , and to one of that symbol's features F .",26,27
4601,465873,"In the old approach, we aligned symbols jointly, and hoped to find symbol pairs that align to each other frequently.",14,15
4602,465873,"In the new approach, we code symbols separately one by one on the source and target level, and A. we code the symbols one feature at a time, and B. while coding each feature, we allow the model to use information from any feature of any symbol that has been coded previously.",50,51
4603,10514987,To do a fuzzy search we specify the ∼ symbol at the end of each token of the SMS query.,9,10
4604,216562282,"Additional information Roughly half of the puzzles contain remarks on individual characters and diacritics in the inventory of the foreign language, e.g. ""In the following orthography a colon (:) marks a long vowel, and the P symbol marks a glottal stop (like the sound in the middle of uh-oh)"".",40,41
4605,18312025,"If an input passage contains at least one spelling error, the output format is ""pid [, location, correction]+"", where the symbol ""+"" indicates there is one or more instance of the predicted element ""[, location, correction]"". """,26,27
4606,8818263,The suggested terms are the names starting with the entered query string as well as Next to each co-references in the suggestion box is a small graph symbol.,29,30
4607,1276498,"For example, if two vowels in Gurmukhi come together, then Shahmukhi symbol Hamza is placed in between them.",13,14
4608,2970201,"For each segment, the mean is calculated and a symbol is assigned based on a set of breakpoints that divide the distribution space into α equiprobable regions, where α is the alphabet size.",10,11
4609,2970201,"Most of the words in our dataset were of the phonetic structure CVC (consonant-vowel-consonant) or CCVCC, thus, w = 5 was chosen as the length of symbol string for capturing the motion characteristics.",34,35
4610,227231835,"Here we adopt the word embedding of the special symbol [CLS], which marks the beginning of a text, from the penultimate layer as the text embedding of the whole text t, represented as e = f emb (t|θ emb ), where θ emb represents parameters in text embedding component, and are fine-tuned while training.",9,10
4611,53223158,"This reflects, after seeing a start-ofsentence symbol, the probability of a sentence appearing and being followed by an end-of-sentence token.",9,10
4612,237558723,"Then, merge rules that combine two symbols in the base vocabulary into a new symbol are learned iteratively until a desired final vocabulary size is reached.",15,16
4613,222341867,"2014) and replacing ""unknown"" tokens with a special symbol ""2"" (Chaganty et al.,",11,12
4614,34044275,"In the latter case, a check mark using a traffic light color scheme indicates the overall teacher rating, and an information symbol indicates that the teacher provided annotations of errors or other comments.",23,24
4615,218974395,"Then, a [MASK SEN] symbol is inserted to replace the original Sentence k in the passage if Q belongs to P .",7,8
4616,218974395,"Furthermore, if query Q is randomly selected from other documents, the label of the position [CLS] symbol is ""1"", and all other labels are ""0"".",20,21
4617,218974395,"Therefore, a specific task for [CLS] symbol is designed.",9,10
4618,218974395,"Then all blanks in the passages are replaced with 4 https://github.com/ymcui/cmrc2019 [MASK SEN] symbols, and the [CLS] symbol works as the prediction for the incompatible short sentence.",22,23
4619,218974395,2019) symbol to all passage is 1.,2,3
4620,218974395,"2 (a), the average attention of this symbol to the previous sentence (2.96) and next sentence (2.74) is far more than the value to the rest of passage (0.61).",10,11
4621,235196067,"We thus represent pinyin as a symbol sequence, e.g., {z, h, o, n, g, 1} for ""中"".",6,7
4622,235196067,"2014) , which encodes the pinyin of the i-th character x i as: ha i,j = GRU( ha i,j−1 , E(p i,j )) (2) where E(p i,j ) is the embedding of the pinyin symbol p i,j , and ha i,j is the j-th hidden states of the GRU.",50,51
4623,226227207,"Let v a generic node, we indicate with h vj ∈ R d and c vj ∈ R d respectively the hidden state and the memory cell of its j-th child node; hence, the L-ary Tree-LSTM cell computation is defined by the following equations: t v = σ   W t x v + L j=1 U t j h vj + b t   t ∈ {i, o, u}, c v = i v u v + L j=1 f vj c vj , f vk = σ   W f x v + L j=1 U f kj h vj + b f k   k ∈ [1, L], h v = o v tanh(c v ), (1) where c v ∈ R d , h v ∈ R d and x v ∈ R n are the memory cell state, the hidden state and the label of node v; {i v , o v , u v } ∈ R d are the input gate, the output gate and the update value, respectively, and f vk is the forget gate associated with k-th child of v. The symbol σ denotes the logistic sigmoid function and denotes the elementwise product.",224,225
4624,225094185,"If p has not generated any child in direction dir yet, we use a special symbol NULL to represent s. • If decision dec is CONTINUE, p generates a new child c with probability P CHILD (c|p, s, dir, val) .",16,17
4625,227231635,The symbol @@ acts as a sub-word unit separator.,1,2
4626,14131077,"For the noun or pronoun in the document, which has the frequency equal to or greater than 2, we randomly choose one position where the noun or pronoun is located on, and replace it with a specific symbol blank .",40,41
4627,14131077,"Second, after the answer word A is chosen, the sentence that contains A is defined as a query Q, in which the answer word A is replaced by a specific symbol blank .",33,34
4628,227230701,"BiRNN Given that the input sequences s is in the order of starting from the first symbol s 1 to the last one s T , we use a bidirectional RNN to abstract the representations of the symbols by summarizing information from both directions.",16,17
4629,227230701,The BiRNN contains a forward − −−−− → RN N which reads the symbol s i from s 1 to s T and a backward ← −−−− − RN N which reads from s T to s 1 .,13,14
4630,227231744,"As shown in Figure 2 (b), the computation procedure is otherwise quite standard, except the special treatment of q CLS , which is the symbol representing the whole sentence, shown in darker blue in the figure.",28,29
4631,243865584,"However, when y t is a rare word, which is often replaced by ""[UNK]"" symbol, y t from the embedding matrix contains little information on the word.",20,21
4632,232240457,"We evaluate the efficacy of type grammar-based constraints, a word-to-symbol lexicon, and transition system state features in this task.",16,17
4633,232240457,The symbol without the suffix or pipes is called the stem.,1,2
4634,232240457,"While this leads to a longer action sequence for parsing, the parser's access to complete constituents allows promotion-based symbol generation for unary operators such as type shifters and standard bottom-up type analysis for constrained parsing.",22,23
4635,232240457,"A vertex, v i = (s i , a i ) ∈ V , is a pair of a ULF symbol s i , and its alignment a i -the index of the word from which s i was produced.",22,23
4636,232240457,The PROMOTE phase enables contextsensitive symbol generation.,5,6
4637,232240457,"This phase allows the parser to generate a symbol using w next as a foundation, or generate an arbitrary symbol that is not aligned to any word in β.",8,9
4638,232240457,"This phase allows the parser to generate a symbol using w next as a foundation, or generate an arbitrary symbol that is not aligned to any word in β.",20,21
4639,232240457,It generates a symbol s consisting of a stem and suffix extension e from w next .,3,4
4640,232240457,"SymGen(s) adds an unaligned symbol (s, NONE) to V p and proceeds to the PUSH phase. •",5,6
4641,232240457,"The oracle maintains s next , the symbol in the foremost vertex of V g that has not yet been added to G p .",7,8
4642,232240457,"Otherwise, SymGen(v next ) Step 5-7 allow the oracle to handle the generation of symbols that are not in word order, by skipping any words that come earlier than the symbol order; and generating symbols that cannot be aligned with SymGen for any reason. •",34,35
4643,232240457,"This enforces bottomup parsing, which is necessary for both the promotion-based symbol generation and type composition constraint. •",14,15
4644,232240457,"Figure 4 : The model consists of a sentence-encoding BiLSTM, a symbol-encoding LSTM, and an action-decoding LSTM.",14,15
4645,232240457,New symbols generated in the GEN and PROMOTE phases of the transition system are appended to the symbol sequence.,17,18
4646,232240457,The transition system supplies hard attention pointers that select the relevant word and symbol embeddings.,13,14
4647,232240457,"As ULF symbols are generated during the parsing process, the symbol embedding sequence s 1:m , which is the concatenation of a symbol-level learned embedding and the CharCNN feature vector over the symbol string, is encoded by a stacked unidirectional LSTM of L s layers.",11,12
4648,232240457,"As ULF symbols are generated during the parsing process, the symbol embedding sequence s 1:m , which is the concatenation of a symbol-level learned embedding and the CharCNN feature vector over the symbol string, is encoded by a stacked unidirectional LSTM of L s layers.",25,26
4649,232240457,"As ULF symbols are generated during the parsing process, the symbol embedding sequence s 1:m , which is the concatenation of a symbol-level learned embedding and the CharCNN feature vector over the symbol string, is encoded by a stacked unidirectional LSTM of L s layers.",37,38
4650,232240457,"We take this idea and modify the tracking mechanism to find the most relevant word, w i , and symbol, s j , for each phase. •",20,21
4651,232240457,ARC and PROMOTE*: The symbol s j in the rightmost cache position and aligned word w i . •,6,7
4652,232240457,PUSH: The symbol s j generated in the previous action and aligned word w i . •,3,4
4653,232240457,Otherwise: The last generated symbol s j and the word w i in the leftmost β position.,5,6
4654,232240457,"ARC, PROMOTE features: For the two cache positions, their token features and the word, symbol 6 , and dependency distance between them; furthermore, their first three outgoing and single incoming dependency arc labels and their first two outgoing and single incoming ULF arc labels. •",18,19
4655,232240457,PROMOTEARC features: Same as the PROMOTE features but for the rightmost cache position use the node/symbol generated in the preceding Pro-moteSym action. •,18,19
4656,232240457,"Action Encoder/Decoder The action sequence is encoded by a stacked unidirectional LSTM with L a layers where the action input embeddings, h a 1:q are concatenations of the word and symbol encodings.",35,36
4657,232240457,"ULF Lexicon To improve symbol generation, we introduce a lexicon with possible ULF atoms for each word.",4,5
4658,232240457,When generating a word-aligned symbol the stem is searched in the lexicon.,6,7
4659,232240457,"We use RoBERTa-Base embeddings with frozen parameters, 300 dimensional GloVe embeddings, and 100 dimensional t i , l i , p i , action, and symbol embeddings.",30,31
4660,232240457,The symbol encoder and action decoder are 2 layers.,1,2
4661,232240457,"However, the symbol generation method and features designed for ULFs result in a performance lead over using an AMR parser with minimal representational assumptions.",3,4
4662,232240457,"B Full ULF Alignment Details The ULF-English alignment system takes into account the similarity of the English word to the ULF atom without the type extension, the similarity of the type extension with the POS tag, and the relative distance of the word and symbol in question.",48,49
4663,236459908,"In the data input, we use the [SEP] symbol to separate the sentence pairs together.",11,12
4664,236459908,Then use the [SEP] symbol to concatenate Lemma that appears in each sentence in the sentence pair.,6,7
4665,12233462,"If a token in all caps begins with a period, it is classified as a stock index symbol.",18,19
4666,207976984,"in string some symbol like ""%"", ""#"", ""$"", ""&"" and "":"" English String  the problem of the overlap and multi-participation of the append nodes elegantly.",3,4
4667,232222189,"The input fed to the embedding models are obtained by concatenating two branches of inputs, which are separated using the '[sep]' symbol.",26,27
4668,238856763,"However, each event type or argument role is treated as an atomic symbol, ignoring their rich semantics in these approaches.",13,14
4669,238856763,"2020) , we use a special symbol [EVENT] to emphasis the trigger detection task.",7,8
4670,226281527,An initial stack symbol Z 0 6.,3,4
4671,226281527,"Particularly, Lemma B.1 will be used to combine the information of the state vector, input and the symbol at the top of the stack.",19,20
4672,226281527,"For each input symbol x ∈ Σ, its corresponding input vector x ∈ Q |Σ| will be a one-hot vector.",3,4
4673,226281527,"If the underlying stack takes empty string as input at particular step, the RNN will take a special symbol as input which also have a unique one-hot representation similar to other input symbols.",19,20
4674,226281527,The push and pop operations will always be in the form of one-hot vectors and this will ensure that retrieving the top element provides a one-hot encoding of the stack symbol.,34,35
4675,226281527,"The first layer of the feedforward network σ(W h h t−1 + W x x t + b) will produce the vector h (1) t−1 = [(q t−1 , x t ), τ top t−1 , ω t−1 ] , where τ top t−1 ∈ Q |Γ| denotes a one-hot vector representation of the symbol at the top of our stack representation and the subvector (q t−1 , x t ) ∈ Q |Q|×|Σ| is a unique one-hot vector for each pair of state q and input x. Thus, the vector h (1) t−1 is of dimension |Q|.|Σ| + 2|Γ|.",62,63
4676,226281527,"The vector (q t−1 , x t ) can be obtained by using Lemma B.1 where Φ = Q and Ψ = Σ. The vector corresponding to the symbol at the top of the stack can be easily obtained using the top operation (σ(4ω t−1 − 2)) defined in section A.1.",29,30
4677,226281527,"In the second layer, we will use Lemma B.1 again to obtain a unique one-hot vector for each combination of the state, input and stack symbol.",29,30
4678,226281527,"The output of the second layer of the feedforward network will be of the form h (2) t−1 = [(q t−1 , x t , τ top t−1 ), τ top t−1 , ω t−1 ], where the subvector (q t−1 , x t , τ top t−1 ) is a unique one-hot encoding for each combination of the state q ∈ Q, input x ∈ Σ and a stack symbol τ ∈ Γ. Hence, the vector h (2) t−1 will be of the dimension |Q|.|Σ|.|Γ|+2|Γ|.",80,81
4679,226226658,The generation comes to a halt when the special end-of-sentence symbol is emitted or the maximum message length L max is reached.,14,15
4680,226226658,"3 The generation of the discrete symbol m t at each time step t can be described by the following: Image h s t = GRU Speaker (<bos>, MLP 1 (I i )) t = 0 GRU Speaker (m t−1 , h s t−1 ) t > 0 Speaker to project each GRU hidden state-one for each time step-into vectors with dimensionality equal to the predefined vocabulary size of the emergent language.",6,7
4681,235266101,"Mathematically, the predicted probability of each tag for (t i , t j ) can be estimated via: p I i,j = sigmoid(W I • h I i,j + b I ), (12) where I ∈ {s, e} is the symbol of subtask indicator, denoting segment extraction and edge prediction respectively, and each dimension of p I i,j denotes the probability of a tag between t i and t j .",53,54
4682,227230544,"A special symbol, <NORM>, was added 𝑛-1 times to the beginning of the sentence and to the end of the sentence respectively to represent padding.",2,3
4683,249191476,Entity Mask: Mask all entities in the sentence with specific symbol [mask] .,11,12
4684,224818089,"In order to retain semantic information, we replace tokens with a special symbol < unk > .",13,14
4685,224818089,We constrain input information by replacing words with generic unknown symbol < unk >.,10,11
4686,232221438,The first sub-column with the person symbol shows either existing human annotations when logged out or the own annotations when logged in.,8,9
4687,227231321,"In a vanilla BERT model, the hidden representations of [CLS] token is used as a symbol to represent one sentence or a pair of sentences.",18,19
4688,196174574,"Encoder-Decoder Framework Given a document X = (x 1 , x 2 , ..., x m ), the encoder maps its corresponding symbol representations E = (e 1 , e 2 , ..., e m ) to a sequence of continuous representations Z = (z 1 , z 2 , ..., z m ), where m is the length of document.",27,28
4689,196174574,"The symbol ""+"" stands for the corresponding module is added on the ""Basic model"" which is a vanilla Transformer with 4 identical layers.",1,2
4690,248496425,S ∈ N is the start symbol.,6,7
4691,234790176,"And the decoder RNN translates c into the target sentence according to: s t = f (y t−1 , s t−1 , c) p(y t |y <t , s i ) = g(y t−1 , s t , c) (1) where s t is the state at time t, y t is the predicted symbol at time t, g is a classifier over the vocabulary, and y <t denotes the history {y 1 , . . . ,",62,63
4692,234790176,"We introduce the encoder-decoder framework to model ED task, mainly considering the following advantages: a) the separate encoder module is flexible in fusing sentence-level and documentlevel semantic information and b) the RNN decoder model (1) can capture sequential event tag dependency as the predicted tag vectors before t will be used as input for predicting t-th symbol.",68,69
4693,235254277,"L-PCFGs extend PCFGs by associating a word, i.e., the lexical head, with each grammar symbol.",19,20
4694,235254277,"Specifically, they make a strong independence assumption on the generation of the child word such that it is only dependent on the nonterminal symbol.",24,25
4695,235254277,"A CFG is defined as a 5-tuple G "" pS, N , P, Σ, Rq where S is the start symbol, N is a finite set of nonterminal symbols, P is a finite set of preterminal symbols, 1 Σ is a finite set of terminal symbols, and R is a set of rules in the following form: where w p , w q P Σ are the headwords of the constituents spanned by the associated grammar symbols, and p, q are the word positions in the sentence.",25,26
4696,233423437,"In this work, we present a new parameterization form of PCFGs based on tensor decomposition, which has at most quadratic computational complexity in the symbol number and therefore allows us to use a much larger number of symbols.",26,27
4697,233423437,A major challenge in employing a large number of nonterminal and preterminal symbols is that representing and parsing with a PCFG requires a computational complexity that is cubic in its symbol number.,30,31
4698,233423437,"A CFG is defined as a 5-tuple G = (S, N , P, Σ, R) where S is the start symbol, N is a finite set of nonterminal symbols, P is a finite set of preterminal symbols, 2 Σ is a finite set of terminal symbols, and R is a set of rules in the following form: S → A A ∈ N A → BC, A ∈ N , B, C ∈ N ∪ P T → w, T ∈ P, w ∈ Σ PCFGs extend CFGs by associating each rule r ∈ R with a probability π r .",27,28
4699,233423437,"It is convenient to represent the probabilities of the binary rules in the tensor form: T h A ,h B ,h C = π A→BC , T ∈ R n×m×m , where T is an order-3 tensor, m = n + p, and h A ∈ [0, n) and h B , h C ∈ [0, m) are symbol indices.",69,70
4700,233423437,"The inside algorithm computes the probability of a symbol A spanning a substring w i,j = w i , . . . ,",8,9
4701,233423437,"We use the tensor form of PCFGs to rewrite Equation 1 as: s h A i,j = j−1 k=i h B ,h C T h A ,h B ,h C ⋅ s h B i,k ⋅ s h C k+1,j = j−1 k=i T h A ⋅ s k+1,j ⋅ s i,k , (2) where s i,j , s i,k , and s k+1,j are all m-dimensional vectors; the dimension h A corresponds to the symbol A. Thus s i,j = j−1 k=i T ⋅ s k+1,j ⋅ s i,k . (",98,99
4702,233423437,"3 + mdl 2 ) (Cohen One problem with TD-PCFGs is that, since we use three matrices U, V and W to represent tensor T of binary rule probabilities, how we can ensure that T is non-negative and properly normalized, i.e., for a given left-hand side symbol A, ∑ j,k T h A ,j,k = 1.",58,59
4703,233423437,We use shared symbol embeddings E s ∈ R m×k (k is the symbol embedding dimension) in which each row is the embedding of a nonterminal or preterminal.,3,4
4704,233423437,We use shared symbol embeddings E s ∈ R m×k (k is the symbol embedding dimension) in which each row is the embedding of a nonterminal or preterminal.,14,15
4705,233423437,"We first compute an unnormalized Ũ by applying a neural network f u (⋅) to symbol embeddings E s : Ũ = f u (E s ) = ReLU E s M (1) u M (2) u , where M (1) u ∈ R k×k and M (2) u ∈ R k×d are learn- able parameters of f u (⋅).",17,18
4706,233423437,"2019a) and define them as: Q h T ,h w = π T →w = exp(u T w f t (w T )) ∑ w ′ ∈Σ exp(u T w ′ f t (w T )) , r h A = π S→A = exp(u T A f s (w S )) ∑ A ′ ∈N exp(u T A ′ f s (w S )) , where w and u are symbol embeddings; f s (⋅) and f t (⋅) are neural networks that encode the input into a vector (see details in Kim et al. (",82,83
4707,233423437,Note that the symbol embeddings are not shared between preterminal rules and start rules.,3,4
4708,233423437,The symbol embedding dimension k is set to 256.,1,2
4709,233423437,Influence of symbol number Figure 1 illustrates the change of F1 scores and perplexities as the number of nonterminals and preterminals increase.,2,3
4710,233423437,"We can see that, as the symbol number increases, the perplexities decrease while F1 scores tend to increase.",7,8
4711,233423437,"TD-PCFGs rely on Kruskal decomposition of the binary-rule probability tensor to reduce the computational complexity of PCFG representation and parsing from cubic to at most quadratic in the symbol number, which allows us to scale up TD-PCFGs to a much larger number of (nonterminal and preterminal) symbols.",32,33
4712,227230314,"Under this definition, we adapt A as follows: Âl ij = z l j • A ij + n m=1 z l m • A im , (3) where Âl represents the pruned dependency matrix of the l-th layer, symbol • means multiplication, and a small quantity is added to the denominator to avoid numerical instabilities.",47,48
4713,227230448,"To this end, we estimate the similarity between scene speakers and character names by means of symbol-level n-gram Jaccard similarity.",17,18
4714,174800654,Our hand-made symbol correspondences and featurizations are distributed with this project's code repository.,4,5
4715,53094980,"If a word is at the beginning of the sentence, we add a special symbol SOS as its history context, and if a word is at the end of the sentence, we add another special symbol EOS as its future context.",15,16
4716,53094980,"If a word is at the beginning of the sentence, we add a special symbol SOS as its history context, and if a word is at the end of the sentence, we add another special symbol EOS as its future context.",38,39
4717,53094980,The decoding process stops when the end symbol EOS is generated.,7,8
4718,53094980,The decoding process starts with the embedding for the word boundary symbol EOS and a randomly initialized hidden state h 0 .,11,12
4719,2496242,"For convenience in GPU training, we treat all sequences as fixed length, either clipping them or padding with a dummy symbol.",22,23
4720,2496242,"To avoid this, we subtract 10 from the objective for each one-symbol word.",14,15
4721,227230409,"For example, the Villain of a story may cause harm or injury to some member of the Hero's family: Propp names this plot function Villainy and assigns it the symbol A. He defined 31 such functions.",32,33
4722,220284501,The symbol # is inserted between the srcmsd and tgtmsd.,1,2
4723,220284501,The symbol # is inserted between the first srcmsd and the second srcform as well as between the second srcmsd and tgtmsd.,1,2
4724,233231232,"The output of the dummy lemma generation process is a triple of a dummy lemma, a special symbol COPY and the dummy lemma, which is added to the initial ""wug test"" training set for data augmentation.",18,19
4725,19265222,"To this end, it employs data augmentation for counteracting overfitting and a copy symbol for processing characters unseen in the training data.",14,15
4726,19265222,"We also implement a mechanism, the copy symbol, which allows the system to copy unseen characters from an input lemma to the resulting word form.",8,9
4727,19265222,"Unfortunately, due to time constraints, we were only able to use the copy symbol in Task 2 of the shared task.",15,16
4728,19265222,"In order to solve the problem of missing characters, we use a special copy symbol.",15,16
4729,19265222,"After reinflection, each copy symbol is reverted back to the original unknown symbol as shown in Figure 4 .",5,6
4730,19265222,"After reinflection, each copy symbol is reverted back to the original unknown symbol as shown in Figure 4 .",13,14
4731,19265222,Reversion is performed by substituting the ith copy symbol in the output string with the ith unknown symbol in the lemma.,8,9
4732,19265222,Reversion is performed by substituting the ith copy symbol in the output string with the ith unknown symbol in the lemma.,17,18
4733,19265222,"For Task 2, we use the copy symbol as explained in Section 4.",8,9
4734,19265222,"Unfortunately, we were unable to run experiments using the copy symbol for Task 1 because of time constraints.",11,12
4735,227905486,"We define a training dataset consisting of n samples as D = (X 1 , T 1 ), ..., (X n , T n ) , where X i ∈ X is a chem- ical compound name and T i ∈ T is the SMILES string of X i for 1 ≤ i ≤ n. Our objective is to learn a mapping function f that realizes f (X i ) = T i from D. Figure 2 overviews the Transformer-based prediction of SMILES strings from chemical compound names, where <s> is a special symbol denoting the start and end of a sequence.",103,104
4736,227905486,"As ""C"" and ""O"" are elements and ""="" is a subsidiary symbol representing a double bond, the proposed constraint function treats the number of atoms of each element (""C"" and ""O"") as the error to be minimized, and disregards the ""="" symbol.",17,18
4737,227905486,"As ""C"" and ""O"" are elements and ""="" is a subsidiary symbol representing a double bond, the proposed constraint function treats the number of atoms of each element (""C"" and ""O"") as the error to be minimized, and disregards the ""="" symbol.",57,58
4738,15494629,"However, we do not record a repetition (where the output equals the input) as a unique decision; rather, all repetitions are marked with a special symbol in the label sequence y, i.e. all repetitions are marked alike in the output.",30,31
4739,15494629,"Decision sequences thus reflect the possible choices we have for each input symbol (including the boundary markers < and >)-we may repeat the symbol, delete the symbol, or output some other sequence of symbols.",12,13
4740,15494629,"Decision sequences thus reflect the possible choices we have for each input symbol (including the boundary markers < and >)-we may repeat the symbol, delete the symbol, or output some other sequence of symbols.",26,27
4741,15494629,"Decision sequences thus reflect the possible choices we have for each input symbol (including the boundary markers < and >)-we may repeat the symbol, delete the symbol, or output some other sequence of symbols.",30,31
4742,15494629,"Somewhat surprisingly, this consistently yielded worse results on the development sets compared with training a separate model for each The current input symbol prevsymbol The previous input symbol prevsymbol2 The input symbol two to the left prevsymbol3 The input symbol three to the left previoustwo The previous two input symbols nextsymbol The next input symbol nextsymbol2 The input symbol two to the right nexttwo The   lemma-to-MSD (track 1) or MSD-to-MSD (track 2), and we settled for using separate models.",23,24
4743,15494629,"Somewhat surprisingly, this consistently yielded worse results on the development sets compared with training a separate model for each The current input symbol prevsymbol The previous input symbol prevsymbol2 The input symbol two to the left prevsymbol3 The input symbol three to the left previoustwo The previous two input symbols nextsymbol The next input symbol nextsymbol2 The input symbol two to the right nexttwo The   lemma-to-MSD (track 1) or MSD-to-MSD (track 2), and we settled for using separate models.",28,29
4744,15494629,"Somewhat surprisingly, this consistently yielded worse results on the development sets compared with training a separate model for each The current input symbol prevsymbol The previous input symbol prevsymbol2 The input symbol two to the left prevsymbol3 The input symbol three to the left previoustwo The previous two input symbols nextsymbol The next input symbol nextsymbol2 The input symbol two to the right nexttwo The   lemma-to-MSD (track 1) or MSD-to-MSD (track 2), and we settled for using separate models.",32,33
4745,15494629,"Somewhat surprisingly, this consistently yielded worse results on the development sets compared with training a separate model for each The current input symbol prevsymbol The previous input symbol prevsymbol2 The input symbol two to the left prevsymbol3 The input symbol three to the left previoustwo The previous two input symbols nextsymbol The next input symbol nextsymbol2 The input symbol two to the right nexttwo The   lemma-to-MSD (track 1) or MSD-to-MSD (track 2), and we settled for using separate models.",40,41
4746,15494629,"Somewhat surprisingly, this consistently yielded worse results on the development sets compared with training a separate model for each The current input symbol prevsymbol The previous input symbol prevsymbol2 The input symbol two to the left prevsymbol3 The input symbol three to the left previoustwo The previous two input symbols nextsymbol The next input symbol nextsymbol2 The input symbol two to the right nexttwo The   lemma-to-MSD (track 1) or MSD-to-MSD (track 2), and we settled for using separate models.",55,56
4747,15494629,"Somewhat surprisingly, this consistently yielded worse results on the development sets compared with training a separate model for each The current input symbol prevsymbol The previous input symbol prevsymbol2 The input symbol two to the left prevsymbol3 The input symbol three to the left previoustwo The previous two input symbols nextsymbol The next input symbol nextsymbol2 The input symbol two to the right nexttwo The   lemma-to-MSD (track 1) or MSD-to-MSD (track 2), and we settled for using separate models.",59,60
4748,227231039,"3, we first investigate the hypothesis in the case of random symbol strings, with random embeddings both of the roles and of the symbols that fill them.",12,13
4749,220935822,"Some noteworthy pre-processing techniques included non-word symbol removal, word segmentation, manually removing common text extensions in headlines (e.g. ""-live updates"").",10,11
4750,10942952,"It assigns a probability to a word sequence w = {w 0 , w 1 , • • • , w L } as follows: P(w i |w i−1 , • • • , w 0 ) = P(w i |h i−1 0 ) P(w) = L i=1 P(w i |h i−1 0 ) (1) (2) where w 0 is the start of sentence symbol <s>.",71,72
4751,248780212,"The symbol '+'/'-' indicates that the higher/lower the value is, the better the model performs.",1,2
4752,235097494,"Rather than treating them as unique expressions, they were nor-malized by replacing them by the symbol XfZ for formulas, and XsZ for symbols.",18,19
4753,195776257,"Each label l i ∈ L c is a three tuple (n i , c i , u i ) where: n i is an integer that encodes the number of ancestors in the tree shared between a word w i and its next one w i+1 (computed as relative variation with respect to n i−1 ), c i is the non-terminal symbol shared at the lowest level in common between said pair of words, and u i (optional) is a leaf unary chain that connects c i to w i .",68,69
4754,227231108,"In most cases, the value of a concept will be denoted in a message by a specific symbol, which we call its expression.",18,19
4755,233189587,"Then at each time step, the decoder outputs either a symbol from the output vocabulary or uses a pointer generator network to generate pointers to tokens in the source sequence.",11,12
4756,218977400,We denote intent classification annotations with the -> symbol.,9,10
4757,8338282,"We use five buckets (5, 5), (5, 10), (10, 15), (20, 30), (45, 60) to accommodate QA pairs of different length, e.g., a question of length 4 and an answer of length 8 will be put in bucket (5, 10), and pad questions and answers with a special symbol "" PAD"" when needed.",73,74
4758,236478008,"2) Unlike human languages, programming language is a formal symbol system and has much stricter syntax rules than human languages, so we can make use of the AST information derived from a code snippet.",11,12
4759,218763425,"3  At the core of our analysis is the concept of an unbiased sample from the model, which we obtain by ancestral sampling: iteratively sampling from distributions of the form Cat(f (x, y <j ; θ)), each time extending the generated prefix y <j with an unbiased draw, until the end-of-sequence symbol is generated.",66,67
4760,248780227,"It is formally denoted as a sequence of tokens d = {t 1 , t 2 , • • • , t l }, where t i is either a word token or a mathematical symbol (e.g., a number or an operator).",37,38
4761,239009411,"We posit that it is crucial to represent the distinct textual signals in the embeddings for enhanced discriminative power, thereby achieving the effect of symbol-like characteristics with neural approaches.",25,26
4762,227231581,"Description Length Gain DLG measures the wordnesshood of an n-gram s according to the change of the description length of a dataset D with and without treating s as a segment; formally, DLG(s) is calculated by DLG(s) = DL(D) − DL(D[r → s] ⊕ s) (7) where D[r → s] ⊕ s is the revised dataset of the original D with all the occurrences of s in D replaced by a single symbol r, and with the original n-gram s appended to the end.",84,85
4763,222002239,The symbol @@ represents an in-word morpheme boundary.,1,2
4764,245329425,"2020) , the span scoring module SCORER(X , l, r).SEG for finding probability of word is computed by using a biaffine operation over the left boundary representation of character x l and the right boundary representation of character where W ∈ R (d+1)×d and the symbol ⊕ denote the concatenation operation.",48,49
4765,227231849,"Steps 3, 4: Steps 3 and 4 define some helper transducers: InsertBx optionally inserts a boundary symbol in any context, and RemoveBx will remove a boundary symbol in any context.",19,20
4766,227231849,"Steps 3, 4: Steps 3 and 4 define some helper transducers: InsertBx optionally inserts a boundary symbol in any context, and RemoveBx will remove a boundary symbol in any context.",30,31
4767,227231849,"+, and transduces all possible character continuations up to a morph boundary symbol, [ 0:Ax ]+ 0:Bx.",13,14
4768,227231849,"Step 7: Step 7 composes the defined transducers to yield the FST which takes input flexibly considering the possibility of a morph boundary symbol at any position, InsertBx, uses this input to form prefix candidates, UpToBx, restricts the space of prefix candidates using the grammar, Prefixes, and finally deletes the boundary symbols from the suggested morph completion strings, RemoveBx.",24,25
4769,227231118,"A referent can include a quantifier or an indexical symbol, which refer to the quantity and the current instance of a concept, respectively.",9,10
4770,227231118,"If there is an indexical symbol # inside a referent field, it could be resolved to a specification of some instance with respect to its type label (Sowa, 2000) .",5,6
4771,227231118,"By the definitions in WordNet and conceptual graphs, we assign Arabic numerals for articles/numbers, denote the indexical symbol for determiners, and remain the empty field for existential quantifier.",21,22
4772,7579436,"L{G) is the set of all recursive matrices derived by the grammar G. L(G,I) is the set of all strings derived from the recursive matrices in L(G) by the interpretation l. A regular (context-free) grammar G that generates recursive matrices is a grammar with terminal symbols Vec 1 , nonterminals N, a start symbol S from N and a set P of regular ( context-free) rules.",63,64
4773,23075531,"2008) proposed three source spurious word deletion models 2 to calculate the translation probability for any source word to be translated into the special empty symbol, ε.",26,27
4774,219182334,"In linguistic applications of TAG and related formalisms such as V-TAG , it is useful to associate each elementary structure ( tree set in the case of V-TAG) with at least one lexical item (i.e., terminal symbol) .",43,44
4775,5118415,4 In this paper the symbol 'I' represents invalidity in either upward or downward entailment.,5,6
4776,233189601,"For the aforementioned sentence-aspect pair for ASA, it is normally organized by concatenating X and A to form a special sequence of [[CLS], X , [SEP ], A, [SEP ]], and then feed it into an encoder, i.e., BERT, to obtain the hidden vectors by [h 0 , H X , H A ] = BERT (X , A) (4) where h 0 denotes the hidden vector for the textinitial symbol [CLS], and H X , H A the embedding matrices of words in X and A, respectively.",91,92
4777,51986906,"Edit Distance With the set of ASCII characters as the alphabet under consideration, the edit operations considered were : • Substitution of a single symbol by another symbol from the alphabet First, we ran the k-means algorithm to cluster the entire data provided in the dataset.",25,26
4778,51986906,"Edit Distance With the set of ASCII characters as the alphabet under consideration, the edit operations considered were : • Substitution of a single symbol by another symbol from the alphabet First, we ran the k-means algorithm to cluster the entire data provided in the dataset.",28,29
4779,15549578,This formalization reduces to the counting of number of symbol occurrences and the computation of edit distances.,9,10
4780,29113773,"Introduction Language models (LMs) estimate the likelihood of a symbol sequence {x t } T t=0 , based on the joint probability, p(x 0 , . . . ,",11,12
4781,29113773,"Specifically, at each time step, the decoder appends every symbol in the vocabulary to each sequence in the current candidate set.",11,12
4782,29113773,2017) propose to predict the next symbol based on a fusion of the hidden states in the ASR/MT and language models.,7,8
4783,29113773,The afore-discussed language models are generative in the sense that they merely model the joint distribution of a symbol sequence (Eq. (,20,21
4784,29113773,"N-gram models (Chen and Goodman, 1996) assume that each symbol depends on the previous N − 1 symbols.",14,15
4785,212429708,"p n }, if p j occurs freely in A i then rank(p j ) < rank(p i ) (2) We use the meta-symbol ≡ for identity between expressions, e.g., E 1 ≡ E 2 , and in abbreviations.",28,29
4786,18076825,"In this paper, after Chinese word segmentation and part of speech tagging, we choose the word with the symbol ""/r such as ""什么""(what), ""为什么"" (why ), ""怎么 样"" (how ), ""谁"" (who) as IW.",20,21
4787,16770600,"2：The method of searching the father node to one concept node of the first type: to any concept node (Node(i)), find the nearest colon j which is ahead of Node(i), if the number of the symbol ""{"" equals to the number of ""}"" between the area of Node(i) and colon j, so the concept node which is ahead of colon j is the father node of Node(i).",42,43
4788,16770600,"3：The method of searching the father node from concept nodes of the second type: to any concept node (Node(i)), find the nearest colon j which is ahead of Node(i), if the number of the symbol ""{"" is one more than the number of ""}"" between the area of Node(i) and colon j, so the concept node which is ahead of colon j is the father node of Node(i).",41,42
4789,219416,"That is because people do not simply regard a sense of a word as an abstract symbol, but a concrete entity that has many properties.",16,17
4790,6561501,"Each tag is a pseudo-binary-bit string of {0, 1, #}, where ""#"" is the ""doesn't care"" symbol.",30,31
4791,6561501,"The position a symbol occupies in the string corresponds to a particular semantic feature of the agent, with ""#"" indicating that the corresponding feature is not critical for the formulation of the classifier phrase, even though the noun referent owns such a feature.",3,4
4792,227230883,"2019) , except that they will have an artificial symbol MASK, but ours will not.",10,11
4793,226262320,This issue arises when a problem-solving model substitutes a number stated in an algebraic word problem into an abstract symbol for generalization.,21,22
4794,226262320,"As shown in Figure 1 (b), when using an Op token, the number 8 is changed into an abstract symbol 'N 1 '.",23,24
4795,226262320,"Meanwhile, when using an Expression token, the number 8 is not transformed into a symbol.",16,17
4796,226262320,"The 4 means of symbol usage are; (1) generalizing common patterns, (2) representing unknowns in an equation, (3) indicating an argument of a function, and (4) replacing arbitrary marks.",4,5
4797,235829155,"Abstraction is an essential property of contextfree grammar: each compound expression e will be abstracted as a simple nonterminal symbol N (e), then it can be combined with other expressions to produce more complex expressions, no matter what details e originally has.",20,21
4798,235829155,"Then we modify Equation 4 : r t i t>1 = ⎧ ⎪ ⎨ ⎪ ⎩ r t−1 i i < ît−1 Tree-LSTM(r t−1 i , r t−1 i+1 ) i = ît−1 r t−1 i+1 i > ît−1 r t i = Linear(Emb(N (v t i ))) v t i ∈ V z r t i v t i ∈ V z (6) Equation 6 means that: in nonterminal nodes, the bottom-up message passing will be reduced from r t i to a nonterminal symbol N (v t i ), thus mimicking the abstraction setup in context-free grammar.",96,97
4799,208613916,"2  In an FSA, each edge is labeled with a symbol y from an alphabet Σ, and any traversal from the starting state to the final state(s) represents a unique string y in the language.",12,13
4800,208613916,"After consuming the prefix y 1:i of a string y, A provides a score A(y i+1 | y 1:i ) for every symbol y ∈ Σ. Invoking A.accepts(s) tests if a string s is in L(A).",27,28
4801,38016588,"To deal with irregular verbs, the semi-consonants w and y, together with the'alif symbol are also ignored.",17,18
4802,21667026,"x m , #, t, where # is a special symbol to denote the start and end of a word, and the encoding of the derivational transformation t is concatenated to the input characters.",12,13
4803,226955790,1 for the incremental utterance segmentation tags around a w symbol for the current word.,10,11
4804,218971761,The symbol φ is used to represent the set of all weights and bias terms of the underlying encoder network.,1,2
4805,226254408,"We include the ⊥ symbol on the learned transformation, P f , to indicate its orthogonality.",4,5
4806,43839109,"The steps taken by us are given below: Nukta Normalization for Hindi Hindi Characters such as क (ka), ख (kha), ग (ga), ज (ja), ड (ḍa), ढ (ḍha), फ (pha), झ (jha), take up nukta symbol to form क़ (qa), ख़ (kḫa),ग़ (ġa),ज़ (za),ड़ (da),ढ़ (ṛha),फ़ (fa),झ़ 5 https://developers.google.com/transliterate/ (zha), respectively.",60,61
4807,17141837,This is resolved by translating the word to a special ∅ symbol instead.,11,12
4808,17141837,"By definition, this ∅ symbol will be part of the domain of all variables.",5,6
4809,17141837,"Since for all words, both trigrams suggest a unique translation, the domains of the three words, D 1 , D 2 , and D 3 , each contain two candidate translations, as well as the symbol ∅, which is always included as a possible translation.",39,40
4810,17141837,"As for the former, the overlap value is only added to the domain of the order variable y ij if words i and j can be translated to the same target word, or more formally, if their domains overlap, D i ∩ D j = ∅. As an illustration, D 2 and D 3 both contain the word ""impossible"", and therefore, D 23 and D 32 contain the symbol OVERLAPS.",77,78
4811,110419,"Each dependency type is represented in the vector by the outside word it involves, or by the symbol 'nil', which indicates that this type of dependency does not occur in the phrase under consideration.",18,19
4812,219177267,"Figure 2 illustrates this through the ""+"" symbol, signifying that the paraphrases are appended to the seed templates when mapping to LF and creating the final NL-QL pairs.",9,10
4813,219559167,They identify five categories of cohesion: [S1] Palin actually turned against the bridge project only after it became a national symbol of wasteful spending. [,23,24
4814,4944928,"The segmental structure in the phonological tree (a) is the basic pronunciation of the sentence Mary likes Peter, where each symbol represents a speech sound.",23,24
4815,4944928,"A ranked alphabet is a finite set Σ of pairs (f, k), where f is a symbol and k ∈ N is its rank.",20,21
4816,4944928,"We indicate the arity of predicate symbols in the same way as the rank of symbols in ranked alphabets, i.e., by writing P (n) if P is a predicate symbol of arity n. The set of all well-formed formulas in Λ without free variables (i.e., the set of sentences of Λ) is denoted by F Λ .",33,34
4817,4944928,"If S is a set, we say that a predicate symbol P (n) is S-typed if it comes with an associated type (s 1 , . . . ,",11,12
4818,4944928,Each interface symbol is given a type that indicates which trees it is intended to link with each other.,2,3
4819,4944928,"For example, if we want to make use of ternary links called TIE, each linking a node of t 1 with a node of t 3 and a node of t 4 , we use the interface symbol TIE : 1 × 3 × 4.",39,40
4820,4944928,"This interface symbol would then be interpreted as a predicate ψ TIE ⊆ V (T, 1) × V (T, 3) × V (T, 4).",2,3
4821,4944928,"INT ) is a structure C = |T |; (ψ I ) I∈I , such that • T ⊆ T k Σ , • ψ I ⊆ V (T, i 1 ) × • • • × V (T, i l ) for every in- terface symbol I : i 1 × • • • × i l in I, and • C satisfies the interface conditions in Φ (if each symbol I ∈ I is interpreted as ψ I ).",52,53
4822,4944928,"INT ) is a structure C = |T |; (ψ I ) I∈I , such that • T ⊆ T k Σ , • ψ I ⊆ V (T, i 1 ) × • • • × V (T, i l ) for every in- terface symbol I : i 1 × • • • × i l in I, and • C satisfies the interface conditions in Φ (if each symbol I ∈ I is interpreted as ψ I ).",79,80
4823,4944928,"Let Λ be ordinary first-order logic with equality, and consider the Millstream system MS over Σ = {• (2) , a (0) , b (0) , c (0) , d (0) } which consists of two identical modules M 1 = M 2 that simply generate T Σ (e.g., using the regular tree grammar with the single nonterminal S and the rules 3 S → •[S, S] | a | b | c | d ) and a single interface symbol BIJ : 1 × 2 with the interface conditions ∀x : lab {a,b,c,d} (x) ↔ ∃y : BIJ(x, y) ∨ BIJ(y, x), ∀x, y, z : (BIJ(x, y) ∧ BIJ(x, z)∨ BIJ(y, x) ∧ BIJ(z, x)) → y = z, ∀x, y : BIJ(x, y) → z∈{a,b,c,d} (lab z (x) ∧ lab z (y)).",98,99
4824,4944928,"i l }| ≤ 1 for each interface symbol I : j 1 × • • • × j m ), then the completion problem is solvable for all regular MSO Millstream systems.",9,10
4825,192997,"Edge labels are taken from a doubly ranked alphabet Σ, meaning that Σ is a finite set of symbols in which every symbol a has source and target ranks rank src (a), rank tar (a) ∈ N determining the number of sources and targets, respectively, that an edge label's a is required to have.",23,24
4826,192997,A Millstream alphabet is a doubly ranked alphabet Σ in which the target rank of each symbol is either 1 or 0.,16,17
4827,192997,"A tree or link symbol a may be denoted by a (k) to indicate that rank src (a) = k. In the following, the term tree refers to an acyclic graph in which all edges are tree edges, each node is the target of exactly one edge, and there is exactly one node (the root) that is not a source of any edge.",4,5
4828,192997,"In this paper there is only one link symbol, this link symbol is of source rank 2, and connects nodes across the two trees every configuration consists of.",8,9
4829,192997,"In this paper there is only one link symbol, this link symbol is of source rank 2, and connects nodes across the two trees every configuration consists of.",12,13
4830,192997,"Since our example is extremely simple, it suffices to choose the graph that consists of the edge labelled with the root symbol S (2) of the syntactic tree (together with the three attached nodes).",22,23
4831,13040597,Predicting such trigram subsequences for each letter of a word eventually results in three output symbol predictions for each letter.,15,16
4832,13040597,"Since the overlapping trigrams that are predicted are just atomic symbols to the underlying learning algorithm, a classifier will only predict output symbol trigrams that are actually present in the data it was trained on.",23,24
4833,13040597,"At the foundation of this constraint-satisfactionbased inference procedure, more briefly constraint satisfaction inference, is the assumption that the output symbol sequence should preferably be constructed by concatenating the predicted trigrams of output symbols, rather than by chaining individual symbols.",23,24
4834,13040597,"If a trigram prediction is considered to be of insufficient quality, the procedure backs off to symbol bigrams or even symbol unigrams.",17,18
4835,13040597,"If a trigram prediction is considered to be of insufficient quality, the procedure backs off to symbol bigrams or even symbol unigrams.",21,22
4836,13040597,"Soft constraints are perfect for expressing our preference for symbol trigrams, with the possibility of a back off to lower-degree n-grams if there is reason to doubt the quality of the trigram predictions.",9,10
4837,13040597,"In the following, y i,j denotes the candidate symbol for letter x j predicted by the trigram assigned to letter x i .",11,12
4838,13040597,"Given the goal of retaining predicted trigrams in the output symbol sequence as much as possible, the most important constraints are simply the trigrams themselves.",10,11
4839,13040597,"In Figure 1 , this is the case for the letter ""d"", for which both the symbol ""d"" and ""t"" are predicted.",19,20
4840,13040597,"b u s - b o o k i b u - s -- b o o k i n u -k --- b o o k i n g -k I --i o o k i n g k I N -i - o k i n g I N - i -- k i n g N - -- As weighted constraints are defined over overlapping subsequences of the output sequence, the best symbol assignment for each letter with respect to the weights of satisfied constraints is decided upon on a global sequence level.",76,77
4841,13040597,This may imply taking into account symbol assignments for surrounding letters to select the best output symbol for a certain letter.,6,7
4842,13040597,This may imply taking into account symbol assignments for surrounding letters to select the best output symbol for a certain letter.,16,17
4843,13040597,"In contrast, in non-global approaches, ignorant of any sequential context, only the local classifier prediction with highest confidence is considered for selecting a letter's output symbol.",31,32
4844,13040597,"Constraints derived from this class are weighted according to the following rules: • for a trigram constraint, the weight is simply the base classifier's confidence value for the class c * ; • for a bigram constraint, the weight is the sum of the confidences for all trigram classes in the nearest-neighbor set of x that assign the same symbol bigram to the letters spanned by the constraint; • for a unigram constraint, the weight is the sum of the confidences for all trigram classes in the nearest-neighbor set of x that assign the same symbol to the letter spanned by the constraint.",65,66
4845,13040597,"Constraints derived from this class are weighted according to the following rules: • for a trigram constraint, the weight is simply the base classifier's confidence value for the class c * ; • for a bigram constraint, the weight is the sum of the confidences for all trigram classes in the nearest-neighbor set of x that assign the same symbol bigram to the letters spanned by the constraint; • for a unigram constraint, the weight is the sum of the confidences for all trigram classes in the nearest-neighbor set of x that assign the same symbol to the letter spanned by the constraint.",105,106
4846,13040597,"Nonetheless, this preference for trigrams may be abandoned if composing a certain part of the output sequence from several symbol bigrams or even unigrams results in higher rewards than when trigrams are used.",20,21
4847,173990628,"Concretely, BERT constructs an input sequence by prepending a singleton or pair with a ""[CLS]"" symbol and delimiting the two sentences of a pair with "" [SEP] .""",20,21
4848,173990628,The representation learned for the [CLS] symbol is used as an aggregate sequence representation for the later classification task.,8,9
4849,173990628,h l N ]) (4) The representation at final layer L for the [CLS] symbol is used as the sequence representation h L [CLS] .,19,20
4850,5350909,Sequences of input symbols and output symbols are converted into windows of fixed-width input symbols each associated with one output symbol.,22,23
4851,5350909,"Within a window, fixed-width subsequences of adjacent input symbols, representing a certain contextual scope, are mapped to one output symbol, typically associated with one of the input symbols, for example the middle one.",24,25
4852,5350909,2001) optimize the likelihood of segmentations of output symbol sequences through variations of Viterbi search.,9,10
4853,5350909,"Sequences of input symbols, predicted output symbols, and real output symbols are converted into windows of fixed-width input symbols and predicted output symbols, each associated with one output symbol.",33,34
4854,21705918,with the symbol '@'.,2,3
4855,21705918,The last row is a new symbol we added to replace identifiable information for the sake of privacy.,6,7
4856,5939791,The equal sign (=) is used as a filler symbol.,10,11
4857,17500098,"the symbol A refers to (ignite-3a agent engine-2a) • constraints -e.g.,",1,2
4858,1967322,"the symbol A refers to (ignite-3a agent engine-2a), and (c) constraints -e.g.,",1,2
4859,13075323,2) FORM: Word form or punctuation symbol.,8,9
4860,227231873,"The two sentences are separated by a separator, and an identification symbol is added to the front and the end of each sentence to indicate the beginning and end of the sentence.",12,13
4861,225062422,"The data was then tabularized with features, including company name, ticker symbol, date of call, and call transcript.",13,14
4862,2754453,"This fact is necessary to be distinguished especially by composing the corresponding rharacteristics, or -what is in substance the sam~-it is necesse-r~ ~ to join to ar~ word-characteristic c the symbol express~.ng a va-"" l riable for the basic for.ms of word, so that we shall write X,c 1 v,,here we can put for X the real basic forms of words.",35,36
4863,2920966,"When an expansion rule is applied to a non-terminal symbol, to which there is already given a concrete word, the word is assigned to the main constituent of the expanded part.",11,12
4864,2920966,"A verb or a noun is taken as the main constituent of the non-terminal symbol ""sentence"" (initial symbol).",16,17
4865,2920966,"A verb or a noun is taken as the main constituent of the non-terminal symbol ""sentence"" (initial symbol).",22,23
4866,2920966,"Semi-terminal derivation: Expansion rules are applied on non-terminal symbols, to the stage where there is no symbol to be expanded.",22,23
4867,2920966,"I Terminology A set of the derived main constituents for a symbol z: A set of all the SWC which can be the main constituent of a non-terminal symbol z or the main constituents of a phrase which is generated by successive expansions of the main constituents of the original z. S(z) = (s,, , s~z, ..... ) S(s~ ) ~ (s~) is assumed.",11,12
4868,2920966,"I Terminology A set of the derived main constituents for a symbol z: A set of all the SWC which can be the main constituent of a non-terminal symbol z or the main constituents of a phrase which is generated by successive expansions of the main constituents of the original z. S(z) = (s,, , s~z, ..... ) S(s~ ) ~ (s~) is assumed.",31,32
4869,2920966,At the n-th stage of the generation: It is supposed that a word is already assigned to the symbol.,21,22
4870,2920966,I. The double line indicates the main constituent of a phrase symbol which is written one line above.,11,12
4871,2920966,"i) Given z, w, s, where s4-~w, s e S(z) (ii) A tree structure whose top symbol is z is constructed by the method explained in 3.",24,25
4872,2920966,"z : XI'X~ ....... X~-~y l"" X~,-yI"" X~y .......... y~.X~y~+~. (I) The symbol z which is written on the left side means that this transformational rules should be applied to the phrase z. Xi, Xs, ..... , X~are either the elements of M or words themselves.",20,21
4873,2920966,"Among them we have a special symbol ~, which indicates that for this symbol ~, there might or might not correspond some term in a~investigated phrase z • That is, ~ expresses an arbitrary term.",6,7
4874,2920966,"Among them we have a special symbol ~, which indicates that for this symbol ~, there might or might not correspond some term in a~investigated phrase z • That is, ~ expresses an arbitrary term.",14,15
4875,12743641,"As to weak generative power , it has to generate not only the sets of strings gene-2ated by a context-free phrase structure grammar, but also at least set of all strings of the form xa_~x, where ~ is a symbol o~he the output vocabulary and ~ is any string of such symbols not containing ~; cf.,",43,44
4876,12743641,"We can say, informally, that no symbol re~d in the input is deleted by the rules of a transducer, except in cases where that symbol can be determined uniquely by the resulting output.",8,9
4877,12743641,"We can say, informally, that no symbol re~d in the input is deleted by the rules of a transducer, except in cases where that symbol can be determined uniquely by the resulting output.",29,30
4878,12536212,"Analysis of a sentence is performed by generation of all possible strings from the initial symbol ""Sentence"" by means of a phrase-structure component, a transformational component, and a phonological component.",15,16
4879,12536212,"rule with the same argument pair, and an input string of words each of which is associated with a unique terminal symbol.",22,23
4880,12536212,"The analysis of the sequence of terminal symbols Cl.-.c n is initiated with a pushdown store (PDS) containing some designated initial symbol (""SE"" in the case of a natural language.",23,24
4881,12536212,"Also, a word in an input string may be associated with more than one terminal symbol.",16,17
4882,12536212,"Since NP' can be a recursive symbol, there is no way of assigning all the possible branch numbers that NP' can be associated with in any finite number of rules.",7,8
4883,1890099,"ways of improving such a grammar have been suggested: a PDS has been connected with a PSG (Yngve 1960 (Yngve , 1961 (Yngve , 1962)) ; the use of (the first symbol has already been taken care of).",38,39
4884,1890099,"There should, of course, be a proviso for the symbol ""..."" itself, so that it will not be copied onto the TM taoe.",11,12
4885,1890099,"The structure that underlies a symbol X 1 may be bound up with a special PS derivation, so that rules concern_ ing structures like, say, X 1 + X 2 + X~ will be ambiguous in their a~plication.",5,6
4886,1890099,"2 p so/s In fact, it makes no difference whether one ex_ pands a symbol on the basis of a rule to be af_ fixed to the constituent by means of a sub_ script, or on the basis of a rule contained so_ mewhere else in the grammar.",18,19
4887,1890099,"The essential is that ~eration proceeds from left to right, and one symbol is F)roduced at a time. (",13,14
4888,1890099,"When I put a subscript on a symbol that is part of a string, and I want to mark off a struct_ ure that is based on several symbols occurring in a certain order, I will have to mark a Ii the symbols of my string in the same way, and this way of :~larking must be unique, i.e. de_ fine a unique path through the rules.",7,8
4889,33615342,"If a grammar rule describes the nature of a P-marker, the label given to each node in the P-marker must have an unambiguous definition which relates the meaning of the symbol to the strings supplied as texts.",35,36
4890,33615342,This criterion will be a dichotomous decision whether or not a given symbol string belongs to the language in question.,12,13
4891,33615342,We have thus far discussed the syntactic function of symbol strings in terms of their acceptable contexts.,9,10
4892,33615342,"If a symbol string belongs to more than two sets of strings, their meet replaces the symbol string.",2,3
4893,33615342,"If a symbol string belongs to more than two sets of strings, their meet replaces the symbol string.",17,18
4894,33615342,where the symbol (+) means an alternative choice.,2,3
4895,216914236,"In the training stage, the format of input is: where [ASP] is a special symbol followed by an aspect term, and [BOS] is appended before the source sentence.",18,19
4896,8862147,"We supplemented this built-in n-gram smoothing, with our own smoothing on the token level by replacing low-frequent words with symbols reflecting certain orthographic features of the original word, and numbers with a symbol only encoding the number of digits in the original number.",40,41
4897,8862147,"As a final example, in the field book entry the registration number for the specimen is preceded by the symbol RMNH; in the database this standard symbol is stripped of and only the number is stored.",20,21
4898,8862147,"As a final example, in the field book entry the registration number for the specimen is preceded by the symbol RMNH; in the database this standard symbol is stripped of and only the number is stored.",28,29
4899,8862147,"Genus As a simple illustration of this, when encountering the symbol RMNH in a field book entry, this most likely indicates the start of a new (registration number) segment.",11,12
4900,8862147,"However, in the database, on which all language models are trained, RMNH never occurs as a symbol in the registration number column; it does occur a few times in the column for special remarks but never at the start of the text.",19,20
4901,8862147,"As a result, a segmentation model trained on the contents of the database, on encountering the symbol RMNH will always opt for continuing the existing segment as opposed to starting a new one, which is most likely the better choice.",18,19
4902,8862147,"Among others, we added the RMNH symbol in front of registration numbers, and randomly changed some month numbers from Arabic numerals to Roman numerals.",7,8
4903,16066714,"2006a) ; a classifier is trained to predict a relation label, or a symbol signalling the absence of a relation, for each pair of tokens in a sentence 2 .",15,16
4904,576802,"is known as the ""doesn't care"" symbol, and is used to indicate that the corresponding semantic feature is not critical for the formulation of the classifier phrase, even though the noun referent owns such a feature.",9,10
4905,227231186,"In particular, the conventional practice in neural encoders of replacing rare words with a unique out-of-vocabulary symbol can affect the ability to instantiate the correct schema elements during code generation (Dong and Lapata, 2016) .",21,22
4906,822620,"Participants could also not leave any gap empty (they were instructed to use the ""-"" symbol for empty references).",18,19
4907,218399557,2005) distinguish between 'segment periodicity' and 'symbol periodicity'.,10,11
4908,19275476,There have been attempts at doing a preliminary global analysis of the complete symbol string on this basis to assess in advance the possibilities of each path of the analysis [9] .,13,14
4909,19275476,"One of them is the well-known 'compiler compiler' that would never reinterpret a part of the symbol string if the part has once been accepted, consequently it is unable to recognize certain structures.)",20,21
4910,19275476,"/i/ Those parsezs that proceed with ""msxim~ width"" from level to level working on the full symbol string, first produce all the reductions that may be achieved by applying a single rule~ than those that may be obtained by applying two rules and so on until the part-structu_~es thus obtained are &Tadually linked.",18,19
4911,19275476,"/In the analysis that proceeds from top downwards, these correspond to the derivations produced by applying two, there, ... rules, followed by the comparison of the terminal symbols thus obtained with the symbol string being analyse~./ /ii/ The ps.rse~sthat proceed with a ""minimum width"" and the ""steepest slope', while gradually extending the elements of the symbol string take the first opportunity to apply a rule and will not extend the analysis to a new symbol until there are new rules that could be built on the rules applied so far.",36,37
4912,19275476,"/In the analysis that proceeds from top downwards, these correspond to the derivations produced by applying two, there, ... rules, followed by the comparison of the terminal symbols thus obtained with the symbol string being analyse~./ /ii/ The ps.rse~sthat proceed with a ""minimum width"" and the ""steepest slope', while gradually extending the elements of the symbol string take the first opportunity to apply a rule and will not extend the analysis to a new symbol until there are new rules that could be built on the rules applied so far.",66,67
4913,19275476,"/In the analysis that proceeds from top downwards, these correspond to the derivations produced by applying two, there, ... rules, followed by the comparison of the terminal symbols thus obtained with the symbol string being analyse~./ /ii/ The ps.rse~sthat proceed with a ""minimum width"" and the ""steepest slope', while gradually extending the elements of the symbol string take the first opportunity to apply a rule and will not extend the analysis to a new symbol until there are new rules that could be built on the rules applied so far.",85,86
4914,19275476,"A new strateKy su~j~ested for analyzin~CF lanAmaKes The exponential increase in the time of analysis in various systems of analysis is obviously due to the increase in the number and depth of impasses, to their various branches--in short to their dangerousness increasing with the length of the symbol string.",56,57
4915,19275476,"The linesrity of the increase in the process of analysis may be best achieved if the symbol string to be analyzed can be segmented in accordance with the highest level rules applicable and these parts could be analyzed During segmentation we apply a ""principle of segmentation"" that is analogous ""4;o the principle discussed in connection with the ""maximum hierarchization"": the shortest component that is nearest to the beginning of the seEment or to the end of the previous component, is taken and used until it becomes evident that for some reason the given segmentation is not applicable.",16,17
4916,19275476,"Thus it is a Boolean matrix B/a,n/; its element B/a,n/ is a truth function whose value is t if and only if the grammar allows the terminal symbol ~ to be the •first element of the terminal rewritin~ of the n th rule.",36,37
4917,19275476,"The transitive continuation matrix of the rules C_/a,n/ is a Boolean matrix whose element C/a,D/ is t if end only if the terminal symbol a is whichever but not the first element of the terminal strings of the n th rule.",29,30
4918,17118441,"It seems to embed in a logiclike formalism all the information needed for the symbol grounding process of a robot, such as relation between linguistic objects as well as roles.",14,15
4919,247748973,"Contrasting this with a traditional BERT-based approach, we see that, in the traditional BERT-based approach approach, the input is pre-pended with another special symbol ([CLS] in case of BERT and <s> in case of RoBERTa).",32,33
4920,247748973,"In BERT-with-consistency-loss, we concatenate an extra symbol with the special symbol.",13,14
4921,247748973,"In BERT-with-consistency-loss, we concatenate an extra symbol with the special symbol.",17,18
4922,247748973,We call the extra symbol [CLSPara] .,4,5
4923,3203102,"In IBM model 1, a latent alignment function a : j → i maps patterns in v n 1 (current sentence) to patterns in u m 0 (preceding sentence), where u 0 is a special NULL symbol which models insertion.",42,43
4924,35563289,"In IBM Model 1, a latent alignment function a : j → i, maps patterns in v n 1 (the current sentence) to patterns in u m 0 (the preceding sentence), where u 0 is a special NULL symbol which models insertion of patterns.",45,46
4925,189928186,"x n , where all x i come from some finite alphabet V, and x n is an end-of-sequence symbol.",24,25
4926,189928186,"2017) , we compute a softmax probability vector for this label from the last activation y (L) n , obtained after reading the end-of-sequence symbol.",31,32
4927,189928186,"n} → { * , 0, 1} for n ∈ N. An input restriction ρ is applied to a transformer by fixing, when the input length is n, the input symbol x i to the value ρ n (i) whenever ρ n (i) ∈ {0, 1}.",35,36
4928,189928186,"If such a symbol is found, the model rejects, else it accepts.",3,4
4929,189928186,"n} the symbol 1 or 0 with probability q/2 each (q ∈ (0, 1) chosen later), and * with probability 1 − q. On those input bits where ρ (2) n (i) = * , we restrict this random restriction to agree with ρ (2) n (i).",3,4
4930,189928186,The idea behind the proof is that the impact of any single input symbol on the output of the transformer is small if the input is long: Lemma 5.,13,14
4931,189928186,"If we exchange one input symbol x i (i < n), then the change in the resulting activation y (L) n at the decoder layer is bounded as O( 1 n ) with constants depending on the parameter matrices.",5,6
4932,189928186,"Given a prefix of a string drawn from this distribution, we ask the transformer to predict the next symbol from Σ = {0, 1, ENDOFSEQUENCE}.",19,20
4933,189928186,Note that the next symbol can be ENDOFSEQUENCE if and only if the prefix has an even number of 1s.,4,5
4934,189928186,"As n → ∞, crossentropy on predicting the next symbol converges to unigram chance level (PARITY), or is at least separated from the optimal cross-entropy by some constant ǫ > 0 (2DYCK).",10,11
4935,189928186,"Therefore, a Lipschitz-continuous prediction function cannot robustly assign different next-symbol probabilities after even and odd numbers of 1s, and cross-entropy will converge to unigram chance level.",15,16
4936,189928186,"Let the height H(x) of a word x be the number of opening brackets minus the number of closing brackets in x. When iteratively sampling a symbol sequence using a pushdown automaton for 2DYCK, the height H n of the prefix x up to length n forms a Markov chain taking values in N. The prefix x is unbalanced if and only if H n > 0, this is always the case whenever n is odd.",27,28
4937,189928186,"These tools provide theoretical understanding of differences between self-attention and theoretically more wellstudied recurrent architectures: Recurrent networks such as LSTMs can perfectly emulate finite-state automata, and therefore can model any finite state language with optimal cross-entropy, as long as the state transition and symbol emission distributions are Markovian.",52,53
4938,189928186,"an x, the next symbol is a closing bracket with constant nonzero probability (1 − p).",5,6
4939,189928186,"If x can be followed by, say, ')' but not ']', then there is a string x ′ , differing only in one input position, but for which the next symbol can be ']' but not ')'.",38,39
4940,189928186,"As x was assumed to end with a closing bracket, the exchanged symbol is not the last symbol of x, and thus the transformer's predictions on x and x ′ differ only by O( 1 n ).",13,14
4941,189928186,"As x was assumed to end with a closing bracket, the exchanged symbol is not the last symbol of x, and thus the transformer's predictions on x and x ′ differ only by O( 1 n ).",18,19
4942,218973879,"All the linguistic features are joined with the original word or a subword using the pipe (""|"") symbol.",21,22
4943,44011055,"From the programming point of view, the formalization reduces to the counting of number of symbol occurrences and the computation of edit distances.",16,17
4944,14493554,The second concern in preprocessing is symbol normalization.,6,7
4945,14493554,"Firstly, we replace all the numbers in monolingual and parallel training corpus with a common symbol (a sample phrase is illustrated in Fig.",16,17
4946,2112402,"For a non-connected word, the components will be separated by the symbol: -.",14,15
4947,2112402,"We will schematize the list by: -i or if need be by: 1 Z 3 or again by: i Z 3 I Z i Z i. Designation of lists and their types We will designate a list by a symbol (name, a set of letters and digits).",42,43
4948,2112402,We will enclose the symbol by the sign @ .,4,5
4949,2112402,We will define a morphism by a symbol (set of letters and digits).,7,8
4950,2112402,"We shall p1""ecede this symbol by the sign $.",4,5
4951,6403885,"Motivation The expressive power of 2-SCFG is gained through looking for continuous phrases that contain other continuous phrases and replacing the subphrases with one non-terminal symbol X. However, due to its heavy dependency on phrasal continuities and the computational complexity constraints, HPB model often fails to capture some useful translation patterns, such as long-distance reordering of time adverbials.",29,30
4952,6403885,The procedure of the additional rule generation is as follows: First X The notable difference between rules (3) and ( 5 ) lies in non-terminal symbols k X and the gap symbol .,36,37
4953,6403885,"For the relative-frequency translation probability, the gap symbol can be viewed as a special token, and we give a count of one to each discontinuous phrase pair occurrence.",10,11
4954,6403885,"In the ordinary HPB rules, the non-terminal symbol k X builds the correspondence for continuous subphrases.",10,11
4955,6403885,"However, the discontinuous phrase rules ((6), ( 7 ) and ( 8 )) use the symbol } , { e f to record the gap information, which has no requirement for strict alignment.",21,22
4956,6403885,"According to the procedure of discontinuous phrase extraction (Section 3.1), the gap symbol represents the sequence of unspecified words, which are aligned with the substrings on the left or right side of corresponding phrases.",15,16
4957,6403885,"In order to reserve the flexibility, we introduce the symbol to match any number of words, including NONE.",10,11
4958,6403885,"Due to the placeholder symbol in each transformed rule, we should treat the problem as fuzzy matching and have to integrate more than one corresponding HPB rules in decoding.",4,5
4959,6403885,"Finally for the transformed glue rules (starting with the symbol S in Table 1 ), we record the alignment between and so that they can provide discontinuity and reordering information to help the HPB decoder connect two adjacent source phrases together, which play the similar role as glue rules.",10,11
4960,6403885,"The left column shows various conditions of using the rules, for example the symbol tgt represents the integration of the rules which only have the gap on the target side.",14,15
4961,18516340,"Transduction grammars are generative grammars that, from the start symbol, generate sentence pairs.",10,11
4962,55524012,Manually annotated features which were taken from the previous corpus annotation scheme are indicated by a * symbol.,17,18
4963,9040709,"The asterisk ""*"" indicates the head element, and its usage is to propagate all the related features/linguistic information of the head symbol to the reduced non-terminal symbol in the left hand side.",26,27
4964,9040709,"The asterisk ""*"" indicates the head element, and its usage is to propagate all the related features/linguistic information of the head symbol to the reduced non-terminal symbol in the left hand side.",33,34
4965,18432637,"For example, following is one of the productions used in the MT system for Portuguese to Chinese translation: S → NP 1 VP* NP 2 PP NP 3 {[NP 1 VP 1 NP The production has two components beside the reduced syntactic symbol on left hand side, the first modeling Portuguese and the second Chinese.",47,48
4966,18432637,"The symbol marked by an ""*"" is designated as head element in pattern, this allows the features of designated head symbol propagate to the reduced non-terminal symbol in the left hand side of production rule, hence to achieve the property of features inheritance in CSG formalism.",1,2
4967,18432637,"The symbol marked by an ""*"" is designated as head element in pattern, this allows the features of designated head symbol propagate to the reduced non-terminal symbol in the left hand side of production rule, hence to achieve the property of features inheritance in CSG formalism.",23,24
4968,18432637,"The symbol marked by an ""*"" is designated as head element in pattern, this allows the features of designated head symbol propagate to the reduced non-terminal symbol in the left hand side of production rule, hence to achieve the property of features inheritance in CSG formalism.",31,32
4969,18432637,"Definitions Let L be a context-free language defined over terminal symbol V T and generated by a context-free grammar G using non-terminal symbol V N disjointed with V T , starting symbol S, and productions of the form A → w where A is in V N and w in (V N ∪V T )*.",12,13
4970,18432637,"Definitions Let L be a context-free language defined over terminal symbol V T and generated by a context-free grammar G using non-terminal symbol V N disjointed with V T , starting symbol S, and productions of the form A → w where A is in V N and w in (V N ∪V T )*.",28,29
4971,18432637,"Definitions Let L be a context-free language defined over terminal symbol V T and generated by a context-free grammar G using non-terminal symbol V N disjointed with V T , starting symbol S, and productions of the form A → w where A is in V N and w in (V N ∪V T )*.",37,38
4972,18432637,"Let Z as a set of integers, each non-terminal symbol in V N is assigned with an integer, Γ(V N ) = {W ω | W ∈ V N , ω ∈ Z}.",12,13
4973,18432637,− S ∈ V N is the initial symbol.,8,9
4974,18432637,"3 Similar for the designation of head element in productions, the only symbol from the RHS of production will inherently be the head element.",13,14
4975,18432637,"In the parsing and generating algorithm, the features information are propagated to the reduced symbol from the designated head element in pattern, hence to realize the mechanism of features inheritance.",15,16
4976,18432637,"In our prototyping system, each symbol has its original weight, and according to preference measurement at the time in checking the feature constraints, a penalty is used to reduce from the weight to give the effective weight of associated features in a particular context.",6,7
4977,18432637,"The corresponding target rule for the parsed string is determined and attached to the reduced syntactic symbol, which will be used for rewriting the target translation in phase of generation.",16,17
4978,18432637,"ACTIONs/GOTOs Ste PARSE(grammar,x 1 … x n ) x n+1 ⇐ ⊥ U i ⇐∅ (0 ≤ i ≤ n) U 0 ⇐v 0 for each terminal symbol x i (1 ≤ i ≤ n) P⇐∅ for each node v ∈ U i-1 P⇐P∪v if ACTION[STATE(v),x i ] = ""shift s'"", SHIFT(v,s') for each ""reduce p""∈ACTION[STATE(v),x i ], REDUCE(v,p) if ""acc""∈ACTION[STATE(v),x i ], accept if U i =∅, reject Translation as Parsing Our Portuguese-to-Chinese translation (PCT) system is a transfer-based translation system by using the formalism of Constraint-Based Synchronous Grammar (CSG) as its analytical grammar.",35,36
4979,222327390,"However, since some languages do not typically use whitespace between words (e.g., Thai), we used the heuristic of SentencePiece meta symbol U+2581 to designate the beginning of the word.",25,26
4980,13968851,"By adding information, we mean in particular adding a role not specified so far (e.g. instrument, duration) or updating an already existing data (e.g. cheese + mozarella → 'cheese, e.g. mozarella', the symbol '+' being in this paper the integration operator).",41,42
4981,34358095,We will use the symbol // to separate the two parts of such sentences. (,4,5
4982,34358095,"Insert as the second symbol, and move the initial symbol's diacritic (if any) to this one.",4,5
4983,34358095,"Insert as the second symbol, and move the initial symbol's diacritic (if any) to this one.",10,11
4984,34358095,Add an underdot to the first symbol. *,6,7
4985,1660039,">"" The symbol nphr designates a ""stray NP"".",3,4
4986,222315141,"This is a way of addressing the symbol grounding problem put forward by (Harnad, 1990) in a way that is compatible with formal semantics.",7,8
4987,10192929,"The nltk.sem.drt module introduces a DRS() constructor which takes lists of discourse referents and conditions as initialization parameters: (1) DRS ([j,d] ,[John(j), dog(d), sees(j,d)]) On top of the functionality available for fol expressions, drt expressions have a 'drs-concatenation' operator, represented as the + symbol.",70,71
4988,10192929,The + symbol is overloaded so that drt expressions can be added together easily.,2,3
4989,38817769,"Hamad (1990) calls this the symbol grounding problem, and claims that it will not be possible to pass an unrestricted Taring Test without symbol grounding -that is our symbols, or words, need to have some connection to a sensory-motor experience of the world.",7,8
4990,38817769,"Hamad (1990) calls this the symbol grounding problem, and claims that it will not be possible to pass an unrestricted Taring Test without symbol grounding -that is our symbols, or words, need to have some connection to a sensory-motor experience of the world.",26,27
4991,38817769,"There was no evidence of understanding, and according to the symbol grounding principle there was not possibility of understanding.",11,12
4992,5845296,"In these definitions, the items preceded by the symbol ~Q"" are templates, in the sense of PATR-II.",9,10
4993,218974528,"The symbol ""+"" at the end of a sign gloss indicates repetition.",1,2
4994,34245575,"As such, this attention model takes as input the previous target symbol, the previous decoder hidden state, the context annotation vectors as well as the source vector from the main attention model.",12,13
4995,34245575,"s i = f (s i−1 , y i−1 , c i , g 1 c c i ) (9) p(y i |y 1 , ..., y i−1 , x, x c ) = g(y i−1 , s i , c i , g 2 c c i ) (10) Each gate has its own set of parameters and depends on the previous target symbol, the current source representation and the decoder hidden state, at time i − 1 for g 1 and i for g 2 .",71,72
4996,8679880,"Consider for example the following definitions (where ""nelist"" and 'elist"" stand for nonempty list and empty list respectively, and T subsumes every type): Example 4 a. list =~ nelist V elist b. FIRST : T ] nelist [REST : list J Here, the denotation of the type symbol list is the set of all possible ground lists.",57,58
4997,8679880,"In practice, a constraint solver could recursively enumerate all these solutions; an alternative proposed by Zajac would be to treat the symbol LIST as the best finite approximation of the infinite set of all lists.",23,24
4998,2220926,"Frames were formed from utterances in the corpus by replacing all but the most frequently-occurring words in the corpus with a placeholder symbol, turning corpus utterances into lexically-based schematic template sentences with slots that can be filled by inserting single words (for example, ""Don""t X it"", ""That""s your X"", ""It""s very X"").",24,25
4999,10767908,The start symbol is S1.,2,3
5000,10767908,"The initial grammar S1 and the initial backoff grammar S2 are tied together by a single symbol START, which has two production rules: 99 START → S1 1 START → S2 These two rules are obligatory, but their weights may be changed.",16,17
5001,10767908,"A start symbol other than the default S1 may be specified (e.g., NP, S2, START, etc.),",2,3
5002,10767908,A start symbol may again be specified; this time the default is START.,2,3
5003,10767908,"First, we generate a collection C by sampling 20 sentences from each team's probabilistic grammar, using S1 as the start symbol. (",23,24
5004,10767908,"Each team's full grammar (using START as the start symbol to allow backoff) is used to parse Ĉ. This gives us the log 2 -probability of each sentence in Ĉ; the cross-entropy score is the sum of these log 2probabilities divided by the length of Ĉ. Group discussion While the teaching assistant is running the evaluation scripts and compiling the results, the instructor leads a general discussion.",11,12
5005,577634,"For these cases, we use the special sentence-start symbol S to serve as context.",11,12
5006,577634,"Similarly, we use the end symbol E for the right-side context of the right-frontier.",6,7
5007,6983774,"Their general form is: Aj because Sj (we use here the term 'because' which is vaguer than the implication symbol used in formal argumentation, because natural language is not so radical).",23,24
5008,36568471,"Conversely, for lower values of τ it will skew the distribution-thereby facilitating the outcome of the originally more probable symbol.",22,23
5009,36568471,"For τ values approaching zero, we recover the simple argmax decoding procedure of picking the highest probability symbol at each step, whereas for high enough τ the LM degenerates into a random process in which at any given step all symbols are equally probable regardless of the history.",18,19
5010,36568471,"An RNNLM processes an input sequence one step t at a time, feeding the input symbol x t through three affine transformations with their corresponding nonlinearities.",16,17
5011,13970782,"First, from start symbol S, sample latent annotation from multinomial with probability π S[x] p S[x],0,l for each x ∈ H. Next, given annotated non-terminal A[x] and i, k, sample possible child nodes and split positions from multinomial with probability: p(B, C, j) = 1 p A[x],i,k • y,z∈H θ A[x]→BC β A[x]BC→yz p B[y],i,j p C[z],j,k (14) Here the probability is calculated by marginalizing all possible latent annotations for B, C, and θ A[x]→BC β A[x]BC→yz is the probability of choosing B[y], C[z] to expand A[x] , and p B[y],i,j p C[z],j,k are the probabilities for B[y] and C[z] to be responsible for word span w i,j and w j,k respectively.",4,5
5012,13970782,"If node N i is a pre-terminal node above terminal symbol w, then for x∈H b i T [x] = θ Ni[x]→w (16) 2.",12,13
5013,1726405,"Given a sequence of input symbols, come up with programs that encode each symbol as a sequence of bits and decode bit sequences as symbol sequences in such a way that the encoded sequence is as short as possible. •",14,15
5014,1726405,"Given a sequence of input symbols, come up with programs that encode each symbol as a sequence of bits and decode bit sequences as symbol sequences in such a way that the encoded sequence is as short as possible. •",25,26
5015,1726405,"Hardware examples like Agre's network models of prioritized argumentation for problem solving and decision making (1997) demystify computation, and help to show why the knowledge level or symbol level is just an abstract, functional characterization of a system.",31,32
5016,9707387,"In standard logic, we determine entailment by checking whether T ∧ KB ⇒ H. (Unless we need to make the distinction explicitly, we overload notation and use the symbol T for the logical form computed for the text, and H for the logical form computed for the query.)",31,32
5017,9707387,x n ) for a new function symbol f .,7,8
5018,14102566,"Their general form is: A j because S j (we use here the term 'because' which is more vague than the implication symbol used in formal argumentation, because natural language is not so radical).",26,27
5019,215542563,x n ) for a new function symbol f .,7,8
5020,2512152,"On the second iteration, there are only three word tokens left uncovered: the start symbol, sees, and the end symbol.",16,17
5021,2512152,"On the second iteration, there are only three word tokens left uncovered: the start symbol, sees, and the end symbol.",23,24
5022,2512152,"This includes 7621 occurrences with tag DT, 3 with tag SYM (symbol), and 1 time with LS (list item marker).",13,14
5023,18161496,"Since the French NP is aligned only to the English NP, the set of left-hand sides is {NP::NP}, where we use the symbol ""::"" to separate the source and target sides of joint nonterminal label or a rule.",28,29
5024,686695,Notice that the table will also contain many 5-grams with the spurious concatenation symbol.,15,16
5025,14199087,Figure 2 shows examples of slash units by single utterances (slash units are delimited by the symbol '/').,17,18
5026,14199087,"In slash units spanning multiple utterances, the symbol '--' is marked both at the end of the first utterance and at the start of the last utterance.",8,9
5027,29406790,"The VerbExp symbol is composed of a main verb (be, have, verb particle constructions associated with state verbs or factives such as is based on, relies upon, etc.)",2,3
5028,29406790,"The Evaluative symbol covers a variety of evaluative forms typical of consumer evaluations: Adjective Phrase (AP), adverbs (e.g. necessary), evaluatives with the right-adjunction of an NP or a PP (e.g. expensive for a 2 stars hotel).",2,3
5029,5953091,"A node descriptor consists of either an atomic symbol, e.g. agr, cat, bar, or of two atomic symbols separated by a slash, e.g. cat/C, head/OBJECT.",8,9
5030,5953091,"In the first case the symbol is the value of the described node, in the second the symbol before the slash is the node's value and the symbol after it is its name.",5,6
5031,5953091,"In the first case the symbol is the value of the described node, in the second the symbol before the slash is the node's value and the symbol after it is its name.",18,19
5032,5953091,"In the first case the symbol is the value of the described node, in the second the symbol before the slash is the node's value and the symbol after it is its name.",29,30
5033,5953091,A macro is simply a symbol which has been specified as a shorthand for some other sequence of symbols.,5,6
5034,16911568,"The CF rule (I) s --) NP vP says that whenever you have the symbol S you may rewrite it as NP VP, i.e. as the set NP, VP with NP written before the VP.",17,18
5035,16911568,"Given a string of lexical items, find some sequence of rules from the grammar which will combine items from the string together so that all that remains is a single structure, labelled with the start symbol of the grammar and covering the whole of the original text.",37,38
5036,16911568,"This means that if you were working top down, you would not even know how the start symbol might be rewritten without considering all the metarules that might expand the basic ID rules which rewrite it; working bottom up would be no better, since you would always have to worry about basic ID rules which might be altered so they covered the case you were looking at.",18,19
5037,8670915,"If the order of categories is not important in some cases, we use the symbol ""|"" to represent either the forward slash or the backward slash.",15,16
5038,8670915,All trees are generated from a special start symbol TOP.,8,9
5039,8670915,"the left-headed (L) rules with the first child symbol as the head category, e.g. the forward composition rules; 4.",12,13
5040,8670915,"the right-headed (R) rules with the second child symbol as the head category, e.g. the backward composition rules.",12,13
5041,8670915,Binary trees are generated top-down recursively from the start symbol TOP.,11,12
5042,8670915,"Specifically, an additional boundary model is defined to capture complex language aspects, in which boundary words are generated from a special symbol independently for each span covered by tree nodes.",23,24
5043,5852298,They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history.,15,16
5044,5852298,"Synchronous Adaptor Grammars Model A Pitman-Yor Synchronous Adaptor Grammar (PYSAG) is a tuple G = (G s , N a , a, b, α), where G s = (N , T s , T t , R, S, Θ) is a Synchronous Context-Free Grammar (SCFG) (Chiang, 2007) , N is a set of nonterminal symbols, T s /T t are source/target terminal symbols, R is a set of rewrite rules, S ∈ N is the start symbol, Θ is the distribution of rule probabilities, N a ⊆ N is the set of adapted nonterminals, a ∈ [0, 1], b ≥ 0 are vectors of discount and concentration parameters both indexed by adapted nonterminals, and α are Dirichlet prior parameters.",101,102
5045,5852298,Then synchronous trees are generated in the top-down fashion from the start symbol S (line 3) for each yield pair.,14,15
5046,2551793,Gestures are represented in the ink meaning lattice as symbol complexes of the following form: G FORM MEANING (NUMBER TYPE) SEM.,9,10
5047,2551793,"I and G differ only in cases where the gesture symbol on G is SEM, in which case the corresponding I symbol is the specific interpretation.",10,11
5048,2551793,"I and G differ only in cases where the gesture symbol on G is SEM, in which case the corresponding I symbol is the specific interpretation.",22,23
5049,2551793,The epsilon symbol (eps) indicates that a stream is empty in a given terminal.,2,3
5050,2551793,The gesture lattice (Figure 8 ) is turned into a transducer I:G with the same symbol on each side except for the SEM arcs which are split.,18,19
5051,6338652,"In MST-based dependency parsing the directed graph G x = (V x , E x ) is defined for each sentence x where V x = {x 0 = root, x 1 , ..., x n } E x = {(i, j) : x i = x j , x i ∈ V x , x j ∈ V x − root} That is, G x is a graph where all the words and the root symbol are vertices and there is a directed edge between every pair of words and from the root symbol to every word.",88,89
5052,6338652,"In MST-based dependency parsing the directed graph G x = (V x , E x ) is defined for each sentence x where V x = {x 0 = root, x 1 , ..., x n } E x = {(i, j) : x i = x j , x i ∈ V x , x j ∈ V x − root} That is, G x is a graph where all the words and the root symbol are vertices and there is a directed edge between every pair of words and from the root symbol to every word.",106,107
5053,227905372,"In the Pre-Processing phase they performed the steps such as tokenization, special symbol and number removal, stop word removal and word stemming.",15,16
5054,150371752,R b( s aRa ) R −1 b( s aR −1 a ) b l(a) Border action and activity Box-removal d implements the Aristotelian slogan no time without change under the assumption that (B) all predicates appearing in a box (string symbol) express change.,49,50
5055,32936404,NMT with Tagging Model Translating rare words is hard for a conventional NMT model with a fixed relatively small vocabulary so that a single unk symbol is used to represent the large number of out-of-vocabulary (OOV) words.,25,26
5056,32936404,"We only extract those pairs whose both source and target side words are person name tags (labeled by our NER tagger), and represent the tag as a $TERM symbol in this paper.",32,33
5057,32936404,"Given an input sentence, we first recognize the person named entities with our NER tagger, then generate BPE segmentation for the plain sentence, and mark each subword unit which is part of a person named entity with a single name-aware symbol finally.",45,46
5058,32936404,We mark the source tokens to which each target unk symbol is most aligned with the method of Luong et al. (,10,11
5059,32936404,Then we replace the recovered person names with a single $TERM symbol.,12,13
5060,32936404,"In addition, we also replace all the number named entities greater than 5000 of source sentences with a single number-aware symbol.",23,24
5061,32936404,"English→Chinese Systems For English→Chinese translation task, if a target unk symbol cannot be recovered by named entity tagging and translation model, we directly replace the target unk symbol with its aligned English word according to the attention weights.",11,12
5062,32936404,"English→Chinese Systems For English→Chinese translation task, if a target unk symbol cannot be recovered by named entity tagging and translation model, we directly replace the target unk symbol with its aligned English word according to the attention weights.",30,31
5063,203648270,"i) For any string s of length > 0, let α s be the symbol that occurs first in s. (ii) For any symbol q and language L, let L[q] be the set of strings that, with q attached to the right, belong to L L[q] := {s | sq ∈ L}.",16,17
5064,203648270,"i) For any string s of length > 0, let α s be the symbol that occurs first in s. (ii) For any symbol q and language L, let L[q] be the set of strings that, with q attached to the right, belong to L L[q] := {s | sq ∈ L}.",27,28
5065,203648270,or by MSO A -formulas built with unary predicate symbols P a labeled by a ∈ A and the binary predicate symbol S (for successors).,21,22
5066,203648270,"Note that ∀xχ Σ (x) is an MSO Σ -sentence stating ( †) exactly one symbol from Σ occurs at every string position except for the last position, where no symbol from Σ occurs.",18,19
5067,203648270,"Note that ∀xχ Σ (x) is an MSO Σ -sentence stating ( †) exactly one symbol from Σ occurs at every string position except for the last position, where no symbol from Σ occurs.",34,35
5068,203648270,"For A ⊆ A, let one A (x) be the MSO disjunction one A (x) := a∈A P a (x) saying some symbol from A occurs in position x, and let atm B (x 1 . . .",30,31
5069,203648270,"Now, ψ Σ,B (x) is the conjunction of (1), ( 2 ) and (3) below, where (1) ensures b i + b i is followed by (allowing for the case where x is the last position of the string), and (3) puts atoms before and after x whenever a symbol from Σ occurs at x b i+1 + b i+1 for i from 1 to n − 1 n−1 i=1 (one {b i ,b i } (x) ⊃ ∃y(xSy ∧ one {b i+1 ,b i+1 } (y))) (1) KAT PDL Boolean in B formula ϕ action in Σ program (e.g., test ϕ?)",67,68
5070,53582290,"Note that these strings do not (necessarily) offer any information concerning real duration: a fluent may occur in several string positions, but this does not affect any interpretation of its duration, only its relation to other fluents, i.e. if the symbol a appears in both ↵ i and ↵ i+1 , the event it stands for is not understood as being twice as long as if the symbol had only appeared in ↵ i .",46,47
5071,53582290,"Note that these strings do not (necessarily) offer any information concerning real duration: a fluent may occur in several string positions, but this does not affect any interpretation of its duration, only its relation to other fluents, i.e. if the symbol a appears in both ↵ i and ↵ i+1 , the event it stands for is not understood as being twice as long as if the symbol had only appeared in ↵ i .",73,74
5072,20178458,"Thus, if α i = α i+1 for any 1 ≤ i < n, then either α i or α i+1 may be safely deleted from s without affecting the interpretation of the string, as the remaining symbol is simply taken as representing a longer moment.",40,41
5073,20178458,"With two strings s and s of the same length n built from an alphabet Σ, the powerset of some fixed set A, the superposition s & s of s and s is their componentwise union: α 1 • • • α n & α 1 • • • α n := (α 1 ∪ α 1 ) • • • (α n ∪ α n ) (3) For convenience of notation, we will use boxes rather than curly braces { } to represent sets in Σ, such that each symbol α in a string s corresponds to exactly one box.",100,101
5074,20178458,"These last two points are interesting in particular, as they lead to a specific kind of superposition between strings s, s ∈ Σ + when some symbol α ∈ s is equal to some other symbol α ∈ s .",28,29
5075,20178458,"These last two points are interesting in particular, as they lead to a specific kind of superposition between strings s, s ∈ Σ + when some symbol α ∈ s is equal to some other symbol α ∈ s .",37,38
5076,20178458,"To achieve this, when a symbol α in s is also present in s , and the asynchronous superposition of these strings is desired, padding is carried out as normal, but superposition is only permitted of those results of padding in which the indices of the matching symbols are equal.",6,7
5077,20178458,"Allen (1983) gives a transitivity table showing the inferred possible relations between two events a and c, given the relation between each and an intermediary event, b. Each cell of the table shows simply the symbol which represent the binary relation -we may improve on the readability of this by showing explicitly the well-formed event-string(s) formed by the asynchronous superposition in each case.",39,40
5078,15734769,Results were automatically generated and visualized in GIS using both symbol and density mapping.,10,11
5079,15734769,"Graduated symbol maps Graduated symbol map is a type of map that uses symbols of different sizes to represent geographic entities (Thrall, 1999) .",1,2
5080,15734769,"Graduated symbol maps Graduated symbol map is a type of map that uses symbols of different sizes to represent geographic entities (Thrall, 1999) .",4,5
5081,15734769,The symbol we choose is circle.,1,2
5082,15734769,There is one exception of using the membership for making the graduated symbol map.,12,13
5083,15734769,Graduated symbol maps on three analytical levels are shown in Fig.,1,2
5084,15734769,Kernel density map Results produced by graduated symbol maps are not continuous.,7,8
5085,1210373,"Next, we define a model M to be a pair (O, V) where O is a finite ordered tree (T,)~) and V is a function from Prop to the powerset of r. That is, V assigns to each propositional symbol a set of nodes.",46,47
5086,1210373,"Consider the context free phrase structure grammar G = (S, N, T, P) where S is the start symbol of G; where N, the set of non-terminal symbols, is {S, NP, VP, N, V, DET, CONJ }; where T, the set of terminal symbols, is {the, a, man, woman, donkey, beat, stroke, and, but}; and where P, the set of productions, is: S , NP VP[ S CONJS NP ) DET N VP ~ V NP N , man JwomauJ donkey V ~ beat [ stroke D ET ) the [ a CONJ , and I but Let's consider how to capture the parse trees of this grammar by means of constraints formulated in L T. The first step is to fix our choice of Prop.",23,24
5087,1210373,"First, we insist that each node is labeled by at least one propositional symbol: [3(Vp~teuT p) achieves this.",14,15
5088,1210373,"Second, we insist that each node is labeled by at most one propositional symbol: (p =~ Aq~((NuT)\{p}) ""~q) achieves this.",14,15
5089,1210373,"Third, we insist that the root of the tree be decorated by the start symbol S of the grammar: s =:, S achieves this.",15,16
5090,1210373,"is a binary relation on W that is a partial function; and V is a function that assigns to each propositional symbol (that is, each ~ 6 ,4), a subset of W. 7 [] Our satisfaction definition for L F wffs is as follows.",22,23
5091,16111846,"So every symbol of s contains all the fluents (information) of the corresponding symbol of s , and possibly more.",2,3
5092,16111846,"So every symbol of s contains all the fluents (information) of the corresponding symbol of s , and possibly more.",15,16
5093,1539574,"Given a finite alphabet A, MSO A -sentences are formed from a binary relation symbol S (encoding successor) and a unary relation symbol U a , for each a ∈ A. We then interpret an MSO A -sentence against a string over the alphabet 2 A of subsets of A, deviating ever so slightly from the custom of interpreting against strings in A + .",15,16
5094,1539574,"Given a finite alphabet A, MSO A -sentences are formed from a binary relation symbol S (encoding successor) and a unary relation symbol U a , for each a ∈ A. We then interpret an MSO A -sentence against a string over the alphabet 2 A of subsets of A, deviating ever so slightly from the custom of interpreting against strings in A + .",25,26
5095,1539574,"As for the homomorphisms ρ B , the idea is that B picks out a subset of A, leaving each a ∈ A that is not in B as an ""auxiliary marker symbol"" -a staple of finite-state language processing (Beesley and Karttunen, 2003; Yli-Jyrä and Koskenniemi, 2004; Hulden, 2009) .",34,35
5096,1539574,"65 S n := {(i, i + 1) | i ∈ [n − 1]} and for each a ∈ A, (iii) [[U a ]] is a subset of [n] interpreting the unary relation symbol U a .",49,50
5097,1539574,"B-specified strings and containment Recall that spec(B) is the MSO B -sentence saying every string position has exactly one symbol from B (∀x) b∈B (U b (x) ∧ b ∈B−{b} ¬U b (x)) and observe that there is no trace of σ A x or complementation or intersection (interpreting ¬∃x and ∧) in the language Spec A (B) := ρ A B ( b∈B b ) * of strings encoding MSO A -models satisfying spec(B), for B ⊆ A. That is, although Spec A (B) and L A (spec(B)) from the previous section specify the same set of strings, they differ as expressions, suggesting different automata.",22,23
5098,2960601,"Given a finite alphabet Σ, a system MSO Σ of monadic second-order logic is set up with a binary relation symbol (for successor) and a unary relation symbol for each symbol in Σ so that the formulae of MSO Σ define precisely the regular languages over Σ (minus the null string ).",23,24
5099,2960601,"Given a finite alphabet Σ, a system MSO Σ of monadic second-order logic is set up with a binary relation symbol (for successor) and a unary relation symbol for each symbol in Σ so that the formulae of MSO Σ define precisely the regular languages over Σ (minus the null string ).",32,33
5100,2960601,"Given a finite alphabet Σ, a system MSO Σ of monadic second-order logic is set up with a binary relation symbol (for successor) and a unary relation symbol for each symbol in Σ so that the formulae of MSO Σ define precisely the regular languages over Σ (minus the null string ).",35,36
5101,4888401,"Over the alphabet Pow(A) of subsets of A, let us represent A by the string s(A) def = x 1 x 1 , x 2 x 2 x 2 , x 3 x 3 of length 5, each box representing a symbol (i.e. a subset of A) and arranged in chronological order with time increasing from left to right much like a film/cartoon strip (Fernando, 2004 ).",45,46
5102,4888401,"Recalling that the demarcation s(A) • of a string s(A) contains occurrences of bgn-I and I-end, for each I ∈ domain(A), let us associate with I the set I • def = {bgn-I | I ∈ I} ∪ {I-end | I ∈ I} from which we build the alphabet Σ I,E def = Pow((I × E) ∪ I • ) so that a symbol (i.e., element of Σ I,E ) is a set with elements of the form I, e , bgn-I and I-end.",83,84
5103,7112682,"We say e ∈ E is left-bounded in s if s is a non-empty string such that e ∈ α where α is the first symbol of s -or equivalently, pre(e) ∈ E s ± .",29,30
5104,7112682,"Although the set of strings s in which e is left-bounded is not π {e} -closed, the equivalences e ∈ α ⇐⇒ bc(ρ {e} (s)) ∈ ( e ) + ( | ) (where α is the first symbol of the non-empty string s) and pre(e) ∈ E s ± ⇐⇒ π {pre(e)} (s ± ) = pre(e) give two different functions f for which the set is f -closed -viz.,",50,51
5105,13546359,"These are used in two ways: i) they impose a hard constraint on constituent spans, in that no constituent (other than sentence root) may extend over a punctuation symbol, and ii) they contribute to the model, specifically in terms of the statistics of words seen adjacent to a phrasal boundary.",33,34
5106,6406048,"4 Working out the idea formally Starting over and proceeding a bit more rigorously now, given a first-order signature L, throw in, for every n-ary predicate symbol R E L, a fresh n-ary predicate symbol/~ and extend the map : to these symbols by setting R = R. Then, interpret/~ in an L-structure M as the complement of R /~M _ IMI'-R M. So, without loss of generality, assume that we are working with a signature L equipped with such a map :, and let M be an L-model obeying the complementarity condition above (readily expressible in the first-order language).",33,34
5107,16298796,Intuitively A ~ B says that making the sequence of feature transitions encoded by A leads to the same node as making the transition sequence coded by B. The symbol 0 is a name for the null transition.,29,30
5108,16298796,"A feature structure is a triple (W, {Rt}tez:, V), where W is a non-empty set (the set of nodes); each Rz is a binary relation on W that is also a partial function; and V (the valuation) is a function which assigns each propositional symbol p E S a subset of W. Note that as we have defned them features structures are merely multimodal Kripke models, 4 and we often refer to feature structures as models in what follows.",59,60
5109,16298796,"As the symbol 0 is to act as a name for the null transition, in what follows we shall assume without loss of generality that 0 ¢ £, and we will denote the identity relation on any set of nodes W by R0.",2,3
5110,16298796,"In fact demanding extensionality (that is, working only with models in which each atomic symbol is true at at most one node), does make it easy to find a decidable fragment.",16,17
5111,85459471,"sub takes two constituents, the second of which must contain the symbol *h. When the operator is evaluated the first argument is inserted into the position of *h in the second argument. """,12,13
5112,248118705,"To do this, we define a function v a (a, S), which takes an argument symbol a and a schema S and returns the element-wise mean of the word vectors of all lexical predicates in S applied to argument a: v a (a, S) = i [v w (pred i (S, a))] We also define a function v s (ϕ, S), which takes a step formula ϕ and a schema S and returns vector concatenation of ϕ's verb predicate and the vector representations of each of ϕ's arguments: v s (ϕ, S) = v w (ϕ verb ) + + + + i [v a (ϕ arg i , S)] Figure 6 illustrates this vectorization process using three ""argument type"" formulas as the values of the pred function.",20,21
5113,248118705,"2019) , we convert these pseudo-English symbol sequences into proper English.",9,10
5114,226283817,"Second, there is a necessity of disambiguation of mathematical notation because a letter or symbol in formulae is not used in a constant single meaning in a document (Greiner-Petter et al.,",15,16
5115,226283817,"It is difficult to perform such a grounding in the scopes of existing tasks or MLP tools because many of the tools and approaches are not capable of carrying multiple meanings for a single symbol in a document (Greiner-Petter et al.,",34,35
5116,226283817,"A morpheme can be a single letter or symbol (e.g., 𝑥, 𝜃, , ×, =, and ) or strings consisting of a few letters (e.g., log and argmax).",8,9
5117,222177746,"This typically involves two types of advanced grounding: symbol grounding (Harnad, 1990) , which bridges symbolic natural language and continuous visual perception, and common grounding (Clark, 1996) , which refers to the process of developing mutual understandings through successive dialogues.",9,10
5118,222177746,"2017) ; Udagawa and Aizawa (2019) , the continuous nature of visual context introduces challenging symbol grounding of nuanced and pragmatic expressions.",18,19
5119,222177746,"In addition, by leveraging exophoric references which directly bridge natural language and the visual context, we can conduct essential analyses related to symbol grounding across the two modalities (Subsection 4.2).",24,25
5120,28178914,"Given an input example which consists of a lemma and a set of features, a sequence of symbols for the system is represented as {S Start f + x + S End | f ∈ Σ ϕ , x ∈ Σ L }, where S Start and S End represents a start symbol and an ending symbol respectively, Σ ϕ a set of features, Σ L a set of symbols in a language, and + the repetition of one or more symbols.",55,56
5121,28178914,"Given an input example which consists of a lemma and a set of features, a sequence of symbols for the system is represented as {S Start f + x + S End | f ∈ Σ ϕ , x ∈ Σ L }, where S Start and S End represents a start symbol and an ending symbol respectively, Σ ϕ a set of features, Σ L a set of symbols in a language, and + the repetition of one or more symbols.",59,60
5122,28178914,"Dimension We used 300 for symbol embeddings, 200 for hidden layers, and 200 for context (attention) vectors, while Kann and Schütze (2016a) used 300 for symbol embeddings and 100 for hidden layers (the dimension of context vectors was not described).",5,6
5123,28178914,"Dimension We used 300 for symbol embeddings, 200 for hidden layers, and 200 for context (attention) vectors, while Kann and Schütze (2016a) used 300 for symbol embeddings and 100 for hidden layers (the dimension of context vectors was not described).",32,33
5124,220048083,"For example, both the frame-level classification outputs (CTC paths) ""HHE∅L∅LOO"" and ""∅HHEEL∅LO"" are mapped to text sequence ""HELLO"", where ∅ is the blank symbol.",35,36
5125,7261487,"♥ = ♠ 0, else (2) where the symbol ♣ is a placeholder for a possible target word, the symbol ♥ indicates a contextual element for the chain word s j i (e.g., the preceding word in the jth sentence or the succeeding word in the lexical chain LC s ), and the symbol ♠ represents the value of ♥.",11,12
5126,7261487,"♥ = ♠ 0, else (2) where the symbol ♣ is a placeholder for a possible target word, the symbol ♥ indicates a contextual element for the chain word s j i (e.g., the preceding word in the jth sentence or the succeeding word in the lexical chain LC s ), and the symbol ♠ represents the value of ♥.",23,24
5127,7261487,"♥ = ♠ 0, else (2) where the symbol ♣ is a placeholder for a possible target word, the symbol ♥ indicates a contextual element for the chain word s j i (e.g., the preceding word in the jth sentence or the succeeding word in the lexical chain LC s ), and the symbol ♠ represents the value of ♥.",60,61
5128,16222367,"It consists of: (1) a root symbol, which is a type constructor and denotes a class of entities, (2) attribute labels, which are record field symbols.",9,10
5129,16222367,"The root symbol is person; id, born and father are three sub-wterms which have either constants or types as values.",2,3
5130,16222367,"Thus, for all rules of the general form: Z --> W, X2, T. where Z, W and T are any kind of non-terminal symbol, a control has to be made on the wellformedness of X 2 if X 2 is a barrier.",31,32
5131,235391043,"2021) compress the attributes of a note, including pitch, duration, and velocity, into one symbol and reduces duplicated position events.",19,20
5132,16601909,"A PCFG (N, W, R, S, θ) consists of a start symbol S, N and W disjoints sets of nonterminals and terminal symbols respectively.",17,18
5133,381219,"The lefthand-side label is decided heuristically: a (coarsened) ""noun"" label if the German OOV starts with a capital letter, a ""number"" label if the OOV contains only digits and select punctuation characters, an ""adjective"" label if the OOV otherwise starts with a lowercase letter or a number, or a ""symbol"" label for anything left over.",64,65
5134,14382738,-What's the meaning of an equal symbol with a delta over it?,7,8
5135,1069316,"A standard hierarchical phrase-based system was trained with rule shape indicator features, obtained by replacing terminals in translation rules by a generic symbol.",25,26
5136,247130566,"We choose 152217 English words to build the vocabulary and replace all the out-of-vocabulary (OOV) words in the corpora with the symbol ""<unk>"".",27,28
5137,247130566,"We select the top 30000 frequent words from the cleaned corpora to construct a small vocabulary, and replace the out-of-vocabulary words in the cleaned corpora with the symbol ""<oos>"" according to the customized vocabulary.",32,33
5138,9851,Currency-symbol normalization is avoided.,2,3
5139,12443460,"Blackburn (1989) has explored the effects of adding of a new type of symbol, called nominals, to tense logic.",15,16
5140,11527572,One classifier decides whether a punctuation symbol should be inserted after a given constituent.,6,7
5141,11527572,The other classifier decides whether a symbol should be inserted before a given constituent.,6,7
5142,11527572,The most probable symbol (including the NULL symbol) is inserted between each terminal node.,3,4
5143,11527572,The most probable symbol (including the NULL symbol) is inserted between each terminal node.,8,9
5144,17082869,GF uses the symbol ** for record updates.,3,4
5145,15047929,"The iterator of t = a (z) is the integer z. An arbitrary type is a string of simple types separated by the symbol ⊗. In particular, the empty string is a type, denoted I. The lexicon of a pregroup grammar consists of a set of pairs word : T where the type T ∈ C(B) captures the different grammatical roles a word may play.",25,26
5146,15047929,2) The expression after the equality symbol above is a composite of tensor products the factors of which are either inequalities between basic objects or lexical morphisms.,7,8
5147,233295981,"Collaborative grounding is distinct from perceptual (or symbol) grounding (Harnad, 1990; He et al.,",8,9
5148,5062917,"MSO A is Monadic Second Order logic with a unary relation symbol U a for each a ∈ A, plus a binary relation symbol S for successors.",11,12
5149,5062917,"MSO A is Monadic Second Order logic with a unary relation symbol U a for each a ∈ A, plus a binary relation symbol S for successors.",24,25
5150,9270454,"If we have a context Γ, y : Y ( x), ∆ and quantifier symbol Q, then we can form a quantifier phrase Q y:Y ( x) in that context.",17,18
5151,9270454,Quantifier symbol Q is interpreted as quantifier Q i.e. an association to every 1 set Z a subset Q (Z) ⊆ P(Z).,1,2
5152,27756839,The idea is simply to prevent attempting the construction of a major constituent unless the first entry symbol is well qualified.,17,18
5153,7726461,"In other words, the pattern shall not contain the symbol ""."".",10,11
5154,233189642,"Collaborative grounding not symbol grounding Collaborative grounding is the process of seeking and providing incremental evidence of mutual understanding through dialog; we view the ongoing exchange of speaker and hearer roles as fun-damental to conversation (Benotti, 2010; Benotti and Blackburn, 2014) .",3,4
5155,233189642,"We consider collaborative grounding to be distinct from symbol grounding (Harnad, 1990) though they interact in interesting ways (Larsson, 2018) .",8,9
5156,233189642,"Human meaning attributions do not rely on the accurate perceptions and perfect memory sought by symbol grounding, but on a collaborative negotiation process in which language speakers coordinate their perceptual memories and linguistic usage with other members of their communities.",15,16
5157,233189642,"We won't cover work from robotics and symbol grounding; for that see e.g. (Roy and Reiter, 2005; Bohus and Rudnicky, 2009; Bohus et al.,",8,9
5158,11991955,"When the child selects a symbol on the communication board, the board speaks and the robot responds.",5,6
5159,11991955,"The child communicates by selecting a symbol on a communication board, which is translated into an utterance using a speech synthesiser.",6,7
5160,11991955,"The main values that this project adds to existing systems are that: • the child is offered an exciting, creative and fun activity • the child can play and interact with other peers on equal terms • the child can explore language in stimulating cooperation with the robot and with other children By being able to use a symbol-based communication board the children are given an opportunity to play, interact, explore language, and at the same time learn to use tools for alternative and augmentative communication.",60,61
5161,11991955,"They were asked questions about the behaviour of the robot and answered by putting symbol cards either at the ""fun"" side of the mat or at the ""boring/not nice"" side.",14,15
5162,53644329,symbol denotes a revealed preference on behalf of the speaker that anticipates a response.,0,1
5163,53644329,"symbol, a student may interpret it as a prompt to respond.",0,1
5164,21719360,"For example, forming a circular shape with the thumb and the index finger can be in the same culture the OK symbol, or can refer to a round object, a ring, the number zero etc.",22,23
5165,6561519,5 Selecting an appropriate symbol to represent each relation is a vexed problem.,4,5
5166,6561519,"The ∧ symbol was chosen to evoke the logically similar bitwise XOR operator of the C programming language family; regrettably, it may also evoke the Boolean AND function.",2,3
5167,6561519,"The | symbol was chosen to evoke the Sheffer stroke commonly used to represent the logically similar Boolean NAND function; regrettably, it may also evoke the Boolean OR function.",2,3
5168,15456486,"The preference towards a language-script pair λσ for expressing a type of sentiment is given by the probability pr(λσ| ; T ) = pr( |λσ; T )pr(λσ|T ) pr( |T ) (1) However, pr(λσ), which defines the prior probability of choosing λσ for a tweet is dependent on a large 2 Tweets in mixed script are rare and hence we do not include a symbol for it, though the framework does not preclude such possibilities.",75,76
5169,10649778,"Consider first the lambda-DRS for each of these words separately (we use the ; operator to indicate a merge (signifying dynamic conjunction) between DRS, and we take the @ symbol to mean function application): The superlative introduces a complex condition, stating that whatever entity is selected with property P, it is taller than all other entities with property P. We simply assume that superlatives introduce a two-place ordering relation, a relation which can be characterised as asymmetric and transitive.",35,36
5170,250390682,"After that, we calculate the similarity s i j = s(p i , c i j ) = 1 − d(p i , c i j ) max(|p i |, |c i j |) to the correct pronunciation p i for each candidate c i j , where d(•, •) denotes symbol-based edit distance.",56,57
5171,52008644,"Given a sequence of discrete symbols sampled from a finite alphabet, find the most common pair of adjacent symbols and replace all instances of the pair with instances of a new single symbol, extending the alphabet by one, repeat until no more pairs can be found or some other criterion is fulfilled.",33,34
5172,52008644,"Table 1 illustrates an example where we have a sequence S and an alphabet A. We show the first two iterations of the algorithm, at each step identifying the most common pair in the sequence and replacing it by a new symbol.",42,43
5173,18398233,"Tile symbol ""/"" separates the IUs.",1,2
5174,11088707,A binder can be a λ or a quantifier (existential or universal); an op can be a Boolean connective or the equality symbol.,25,26
5175,634172,"Two of the base forms represent vowels in isolation, but the rest are for consonants (or semi-vowels classed as consonants) and thus correspond to CV pairs, with the first order being the base symbol with no explicit vowel indicator.",39,40
5176,216653102,"We define a PCFG G to be a tuple W , N , R, S, θ : a set of terminals W , a set of nonterminals N , productions R, start symbol S ∈ N and a vector of rule probabilities θ.",35,36
5177,216653102,A sequence of terminals (the yield) is generated by recursively rewriting nonterminals as sequences of child symbols (either a nonterminal or a symbol).,25,26
5178,216653102,"Rewriting terminates when the derivation has reached a terminal symbol such as ""cake"" (which does not rewrite).",9,10
5179,21730294,"and are tokenized based on orthographic-word boundaries, a white space or a colon-like symbol (፡).",18,19
5180,21730294,"In order to equate the preprocessing steps of both languages, the sentences are preprocessed in a manner similar to Amharic corpora as follows: as case folding letters are lowercased, numbers are replaced by a placeholder # symbol, hyphenated words are split, contracted forms (clitics) are conflated (e.g., is n't into isn't), part-of-speech tags are discarded, and they are tokenized based on white space.",39,40
5181,52948134,Note that there are six trigrams in total due to an additional boundary symbol at both ends of the token.,13,14
5182,11287744,"Diacritization in Arabic (sometimes called vocalization or voweling) can be defined as a symbol over and underscored letters, which are used to indicate the proper pronunciations as well as for disambiguation purposes.",15,16
5183,35867892,"For example, the heart symbol ♥ was omitted when tweets were tokenised by the taggers.",5,6
5184,17364080,"For example, the heart symbol ♥ was omitted when tweets were tokenised by the taggers.",5,6
5185,5523142,Since there is no second item on the stack the symbol '-' is used for representing unavailable items.,10,11
5186,220046693,"The standard paradigm of masked language modeling is to substitute a subset of tokens in the input sentence by a special symbol [MASK], and predict the missing tokens by the residual ones.",21,22
5187,220046693,"2018) : we randomly select 10% of the tokens in x, of which 80% are substituted with a special symbol [MASK], 10% are substituted with a random token in the vocabulary, and 10% are kept unchanged.",23,24
5188,248780364,We add a language ID symbol for four languages at the start of each sentence.,5,6
5189,1331321,"The models contain neither the same number of hidden states nor the same emission symbol alphabet; therefore, our comparison will be primarily qualitative.",14,15
5190,221140098,"In each step, all symbol representations are weighted to create a new representation for the given symbol.",5,6
5191,221140098,"In each step, all symbol representations are weighted to create a new representation for the given symbol.",17,18
5192,221140098,The explanation is generated by searching for most probable symbol sequence using beam search during inference.,9,10
5193,221140098,"Related work Classical approach to machine commonsense reasoning is to use knowledge graphs and symbol manipulation to infer an answer to a commonsense question, or to find out if a statement agrees with common sense.",14,15
5194,12329555,"The observation symbols are drawn from the alphabet ∑={σ 1 , σ 2 , …, σ M }, and the initial probability distribution is Π=[π i ] where π i is the probability of a sequence beginning with observation symbol σ i .",41,42
5195,12329555,"For a first-order HMM, the observation symbol alphabet is defined as above, along with a set of hidden states S={s 1 ,s 2 ,…,s N }.",9,10
5196,12329555,The second stochastic layer of the model governs the production of observation symbols: the emission probability distribution is B=[b ik ] where b ik is the probability of state i emitting observation symbol k. The notion that dialogue has an overarching unobservable structure that influences the observations is widely accepted.,33,34
5197,12329555,Production nodes are defined by their observation symbol alphabet and an emission probability distribution over the symbols; HHMMs do not require a global symbol alphabet.,7,8
5198,12329555,Production nodes are defined by their observation symbol alphabet and an emission probability distribution over the symbols; HHMMs do not require a global symbol alphabet.,24,25
5199,1556508,"A firstorder HMM, in which each hidden state depends only on the immediately preceding hidden state, is defined by the following components: • ∑ = {σ 1 , σ 2 , …, σ M }, Dialogue Modeling with HMMs In this work, the observation symbol alphabet ∑ is given.",51,52
5200,248512883,"Connectionist temporal classification (CTC) were also explored for adaptive policy by treating the blank symbol as wait action (Chousa et al.,",16,17
5201,248512883,"Namely, we only output a new token if 1) it is not the blank symbol and 2) it is different from the previous token.",16,17
5202,248512883,"For CTC, this reduces excessive blank symbol predictions (Kim et al.,",7,8
5203,195063973,"The conviction can be interpreted as the frequency that the feature set makes an incorrect prediction of the entity type -the ratio of the expected frequency that F occurs with entities of other types than t. Let symbol → denote the mapping of a feature set to an entity and E be the set of all entities (or, more precisely, all articles that can deal with an entity).",37,38
5204,140210852,"The percept-symbol correspondence is, thereafter, established by a symbolic system, which handles the grounding of measured attributes values to corresponding predicate symbols through the use of predicate grounding relations, e.g., a certain peek in a color histogram, measured as a color attribute, is mapped to a corresponding predicate symbol 'red'.",3,4
5205,140210852,"The percept-symbol correspondence is, thereafter, established by a symbolic system, which handles the grounding of measured attributes values to corresponding predicate symbols through the use of predicate grounding relations, e.g., a certain peek in a color histogram, measured as a color attribute, is mapped to a corresponding predicate symbol 'red'.",57,58
5206,174800228,The corresponding value of the STOP key is a special symbol represented by an all-zero vector.,10,11
5207,12317426,"We start by sampling from the output distribution at step t and, then, we recursively feed back the sampled symbol, together with any other previous output, to condition the generative distribution at step t + 1.",21,22
5208,12317426,Equation 3 shows formally the text generation process for a symbol at step t where w t−1 is the generated symbol at step t−1 and S refers to any given sampling method.,10,11
5209,12317426,Equation 3 shows formally the text generation process for a symbol at step t where w t−1 is the generated symbol at step t−1 and S refers to any given sampling method.,20,21
5210,12317426,"w t = S[P (w t |w t−n , w t−(n−1) , ..., w t−1 )] (3) An obvious approach towards sampling is to select the symbol that maximizes the probability of the entire generated sequence (argmax decoding).",32,33
5211,12317426,"For a large vocabulary (e.g. in the case of a word-level LM), the search quickly becomes impractical and is usually approximated by means of beam search (including the extreme case of using a beamsize equal to 1, which corresponds to picking the most probable symbol at each step).",51,52
5212,12317426,"Finally, we force the LM to generate fully terminated sentences by including endof-sentence symbols (EOS) during training time and discarding any output sentence that reaches a maximum number of characters m without having generated the EOS symbol -thus, we consider the generation of a single sentence finished whenever the EOS symbol is produced and we only generate sentences with a maximum number of characters m. This is motivated by the fact that very long sentences tend to degenerate into poor-quality text.",41,42
5213,12317426,"Finally, we force the LM to generate fully terminated sentences by including endof-sentence symbols (EOS) during training time and discarding any output sentence that reaches a maximum number of characters m without having generated the EOS symbol -thus, we consider the generation of a single sentence finished whenever the EOS symbol is produced and we only generate sentences with a maximum number of characters m. This is motivated by the fact that very long sentences tend to degenerate into poor-quality text.",56,57
5214,12317426,"Conversely, lower values of τ will skew it, thereby facilitating the outcome of the originally more probable symbol.",19,20
5215,12317426,"For τ values approaching 0, we recover the simple argmax decoding procedure of picking the highest probability symbol at each step.",18,19
5216,12317426,"More formally, given a binary column vector x t representing the input symbol at step t, we retrieve its corresponding embedding w t through w t = W m x t , where W m is the embedding matrix with dimensionality R M xV .",13,14
5217,12317426,"Each classifier symbol refers to a classification experiment using the data at the arrow's source (first subscript) for training and the data at the arrow's target (second subscript) for testing (note that training only has to be performed 3 times, one per dataset).",2,3
5218,5079594,"For example, the nominal URL in Figure 2 is tagged as other (X) and + is tagged as symbol (SYM) rather than conjunction (CCONJ).",21,22
5219,5079594,"Tokens that do not have a syntactic function (see Figure 1 , discussed at greater length in the next section) were usually annotated as other (X), except for emoticons, which are tagged as symbol (SYM), following UD_English-EWT.",39,40
5220,5079594,"In our annotation, there are five types of non-syntactic tokens commonly seen in tweets: sentiment emoticons, retweet markers and As discussed above, these are generally tagged with the other (X) part of speech, except emoticons, which are tagged as symbol (SYM).",49,50
5221,234353602,"The input to the encoder contains a source sentence x, a special symbol ""ALIGN"", and the past translation z (k−1) : x = x [""ALIGN""] z (k−1) , (12) Transformer Encoder Transformer Decoder Dot Product where operation denotes the concatenation of two sequences.",13,14
5222,7218315,"2015a) ; in a Pointer Network, the only way to generate output is to copy a symbol from the input.",18,19
5223,7218315,"Any non-terminal symbols in α must be aligned to the same non-terminal symbol in β, and vice versa.",16,17
5224,31356274,"Decoding completes when B is empty (except for the empty-stack symbol), regardless of the state of S. Since each token in B is either moved directly to O or S every step, the total number of actions equals to the length of input sentence.",13,14
5225,16074941,Palin actually turned against the bridge project only after it became a national symbol of wasteful spending.,13,14
5226,16074941,"Union: Ms. Palin supported the bridge project while running for governor, but turned against it when it became a national scandal and a symbol of wasteful spending.",25,26
5227,226283443,"Meanwhile, the top node is the protected symbol of the stack (which will never be popped out).",8,9
5228,226283443,"We treat remote edges in the same way as primary edges, except for those with a special star (*) symbol.",22,23
5229,808493,"Under FOPL a rule can be written as of the form P (a) ∧ Q(a)∧Q(b)∧R(c) ⇒ Action1(a)∧Action2(b)∧Action1(c) where P, Q and R are predicates a, b and c are constant symbols Action is a function symbol The rule-base system of Vaakkriti is developed considering the factors as predicates and the tokens as constant symbols.",41,42
5230,842523,"Examples of this include substitution by digits (""7"" for ""seven""), abbreviations or acronyms (""US"" for ""United States""), symbols (euro symbol for ""Euro""), or reductions of compound words (""elections"" for ""state-elections"").",34,35
5231,207980391,"Meanwhile, top node is the protect symbol of stack (never be popped out).",7,8
5232,207980391,"We treat remote edges the same as primary edge, except the edge label added with a special symbol, i.e. star(*).",18,19
5233,44066484,"To overcome the ambiguities in data, techniques like ensemble are often adopted (Di- Dependency parsing Neural machine translation st (σ, β, A), where σ is a stack, β is a buffer, and A is the partially generated tree ($, y1, y2, ..., yt) , where $ is the start symbol.",64,65
5234,26591736,"Sometimes we may use the symbol d i interchangeably with the embedding vector d i to refer to the i-th document, and use d d to denote the vector representation of document d. Similar conventions apply to word and tag embeddings.",5,6
5235,4260942,"Recently, covnectionist approaches (NETtalk: Sejnowski and Rosenberg, 1987) mid memory-based reasoning approaches (MBRtalk: Slanfill and Waltz, 1986) have been proposed as alternatives tt) the traditional symbol-manipttlation approach.",37,38
5236,4260942,"We will argue that within the symbol manipulation approach, a modular architecture with rite syllable as a central tmit is to be preferred.",6,7
5237,1310204,"For simplicity, we have used the symbol v o so far, but in HABCNN-QP/QAP we compute two different document representations: v oq , for which attention is computed with respect to Q; and v oa for which attention is computed with respect to A. r i also has two versions, one for Q: r iq , one for A: r ia .",7,8
5238,222140632,A whitespace is replaced by a special symbol .,7,8
5239,222140632,A whitespace is replaced by the special symbol .,7,8
5240,12401528,"We remark that Query Encoder can find the representation of a rather general class of symbol sequences, agnostic to the actual representation of the query (e.g., natural language, SQL, etc).",15,16
5241,18449288,"We filter these tweets of punctuation, stop words, hyperlinks, usernames, and the 'RT' retweet symbol, and use the top 20,000 word types.",20,21
5242,51878324,"In this case, we add an empty jongsung symbol e such that a character always has three (jamos).",9,10
5243,51878324,"These two levels of subword features can be successfully integrated into jamo-level n-grams by ensuring a character has three jamos, adding empty jongsung symbol to the sequence.",28,29
5244,51878324,"Since we add the empty jongsung symbol e when decomposing characters, we can find jamo-level trigrams representing a single character in the decomposed jamo sequence of a word.",6,7
5245,51878324,"In addition, if a character lacks jongsung, the symbol e is added.",10,11
5246,51878324,"Note that setting n=3-6 and adding the jongsung symbol makes this model as a specific case of our model, containing jamo-level ngrams (n=3-6) and character-level n-grams (n=1-2) as well.",10,11
5247,51878324,"Next, addition of an empty jongsung symbol e to jamo sequence, which reflects Koreanspecific linguistic regularities, improves the quality of word vectors.",7,8
5248,6068000,"Although the Prop-Bank (Xue and Palmer, 2003) normalizes these semantic roles into certain symbols, such as Arg0-Arg5, the same symbol can have different semantic meanings when paired with different predicates, and thus cannot be learned well.",28,29
5249,14364318,"Additionally, a rule can contain one or more nonterminals X, represented as a nonterminal node on the source side and a nonterminal symbol on the target side.",24,25
5250,14364318,No node outside the subgraph is aligned to a symbol in the target side and no symbol outside the target side is aligned to a node in the subgraph.,9,10
5251,14364318,No node outside the subgraph is aligned to a symbol in the target side and no symbol outside the target side is aligned to a node in the subgraph.,16,17
5252,14364318,The rule contains at least one node aligned to at least one symbol.,12,13
5253,14364318,Rule subtraction replaces the subrule's subgraph and symbol sequence with a nonterminal symbol in the enclosing rule.,8,9
5254,14364318,Rule subtraction replaces the subrule's subgraph and symbol sequence with a nonterminal symbol in the enclosing rule.,13,14
5255,14364318,No node outside the subgraph is aligned to a symbol in the target side and no symbol outside the target side is aligned to a node in the subgraph.,9,10
5256,14364318,No node outside the subgraph is aligned to a symbol in the target side and no symbol outside the target side is aligned to a node in the subgraph.,16,17
5257,14364318,The rule contains at least one terminal node aligned to at least one terminal symbol.,14,15
5258,237605569,"2020) , the input can be denoted as ([CLS], KB, Ĥ, [SEP], s n , [SEP]), where [CLS] and [SEP] are special symbol for classification token and separator token.",40,41
5259,14171478,"Therefore, each symbol in X is represented by a single vertex.",3,4
5260,15419664,Contains digits and percentage: If the token contains digits and percentage symbol then the feature 'ContainsDigitsAndPercentage' is set to 1.,12,13
5261,221097205,"Chat-specific modifications In the case of the concatenation-based approaches, multi-source context encoder, and the Doc-Star-Transformer, we add the speaker symbol as special token to the beginning of each sentence.",32,33
5262,243865161,"1 -4 denotes headers with abbreviation, polysemy, verb-object phrase and special symbol, respectively.",15,16
5263,243865161,"To have a more quantitative analysis of our dataset, we count the ratio of headers containing four lexical features, including abbreviation, symbol characters, verb-object phrase and capitalized character.",24,25
5264,243865161,"Then, the target header H, the rest of the headers S, and the selected cell values V are concatenated by a separator symbol ""[sep]"".",25,26
5265,238743942,"The symbol "">"" in the first column denotes precedence of a quantifier over another (i.e. higher probability).",1,2
5266,236460087,"Specifically, to generate SQL query y i j for q i j , we concatenate all its prior questions q i 1 , • • • , q i j−1 with a special symbol [SEP]: q i j , [SEP], q i j−1 , • • • , [SEP], q i 1 .",34,35
5267,33015047,"Therefore, the target language vocabulary includes only high-frequency words (e.g., 30,000 high-frequency words) in training; other words are treated as out-of-vocabulary (OOV) and substituted with a special symbol such as ""<unk>"" in the output.",42,43
5268,33015047,"The symbol has no meaning, so the output has reduced quality.",1,2
5269,225075639,We replace all of these symbols by a standard symbol (e.g. cite) for evaluation.,9,10
5270,202774502,"We say that T 0 is the start symbol, and for each symbol T i ∈ N , we say that i is its rank, and we say that L has a rank of 0.",8,9
5271,202774502,"We say that T 0 is the start symbol, and for each symbol T i ∈ N , we say that i is its rank, and we say that L has a rank of 0.",13,14
5272,202774502,"T 0 → (b/ :IMP 1 T 1 ($1) :IMP 2 T 1 ($1)) (r 1 ) T 0 → (x/L) (r 2 ) T 0 → (s/L) (r 3 ) T 0 → (x/L :TOPICOF T 0 ) (r 4 ) T 1 ($1) → (b/ :DRS T 1 ($1)) (r 5 ) T 1 ($1) → (e/L :PIVOT $1 :THEME T 0 ) (r 6 ) T 1 (x) → (x/L :PARTOF T 0 ) (r 7 ) Our grammar derives strings by first rewriting the start symbol T 0 , and at each subsequent step rewriting the leftmost function in the partially derived string, with special handling for variable references described below.",142,143
5273,202774502,"The start symbol T 0 at step 1 is rewritten by production r 1 in step 2, and the new b variable introduced at this step is deterministically renamed to the unique name b 1 .",2,3
5274,202774502,Each outgoing edge points to a nonterminal symbol representing a subgraph.,7,8
5275,218974197,The derivation of (2) proceeds as follows: Suffix ∼:(ng)u to root aghnagh-: i) Allomorph symbol (ng) surfaces as Ø because the root aghnagh-does not end in a vowel.,19,20
5276,218974197,"Suffix ∼ f (g/t)u to stem aghnaa-: v) Allomorph symbol (g/t) surfaces as -g because the derived stem aghnaa-ends in two vowels, yielding stem aghnaagu-.",14,15
5277,218974197,"This is shown in the derivation of aghnaaguq: Steps i-iv) aghnagh -∼:(ng)u -∼ f (g/t)u -q Steps v-vi) aghnagh -∼:(ng)u -∼ f (g/t)u -q Step vii) aghnagh -∼:(ng)u -∼ f (g/t)u -q In order to achieve such behavior conforming to Assumption 2, Chen and Schwartz (2018) introduced two devices: • an explicit morpheme boundary symbol  Each rewrite rule in the Chen and Schwartz (2018) Analyzer is then constrained to operate only on the symbols adjacent to the leftmost morpheme boundary; this ensures that rewrite rules only apply to the leftmost unprocessed morpheme, thus complying with Assumptions 1 and 2.",75,76
5278,2072309,"RGL Recognition To recognize RGG, we exploit the property that every nonterminal including the start symbol has rank at least one (Definition 5), and we assume that the corresponding external node is identified in the input graph.",16,17
5279,202770347,"The controller c t is the probability of choosing from knowledge graph entities V, while 1 − c t is the probability of choosing from generic words W. Note that we take the controller as a special symbol KB in generic words, so the term 1 − c t is already multiplied to w t .",38,39
5280,17213767,"The Dependency and Boundary Models Our models follow a standard generative story for head-outward automata (Alshawi, 1996a) , restricted to the split-head case (see below), 1 over lexical word classes {cw}: first, a sentence root cr is chosen, with probability PATTACH(cr | ⋄; L); ⋄ is a special start symbol that, by convention (Klein and Manning, 2004; Eisner, 1996) , produces exactly one child, to its left.",67,68
5281,3666178,"We restrict the weights in our RNNs to the ratio-nal numbers Q. In addition, we reserve the use of a special symbol $ to mark the start and end of an input string.",24,25
5282,3666178,"In the beginning, it heavily biases the next symbol prediction towards a, but counters it starting at t = 1.",9,10
5283,3666178,"The first layer of the RNN R with a single alphabet symbol a uses one neuron n and has the following behavior: h −1 (n ) = 0 h s,t (n ) = σ h s,t−1 (n ) + 1 The second layer uses neuron n and takes h s,t (n ) as input at time t: h s,t (n) = σ h s,t (n ) − 2 E s,t (a) = h s,t (n) E s,t ($) = 0 E s,t (a) = 1 2 if t ≤ 1 e (t−1) 1+e (t−1) otherwise.",11,12
5284,3666178,"This stores the current input symbol in neuron x m , so h s,t (x m ) = I(s t ).",5,6
5285,216641856,"2019) argues that the existing pretrained language models that are based on autoencoding, such as BERT, suffer from the discrepancy of the pre-training and fine-tuning stage because the masking symbol [MASK] will never appear in the fine-tuning stage.",36,37
5286,174799768,"Post-processing We exclude the period symbol from all the captions, convert numbers to words using num2words 3 and correct grammar errors by languagetool 4 .",7,8
5287,174799768,"The input to our model are mel-frequency cepstral coefficient (MFCC) audio features (Davis and Mermelstein, 1980) and the output is a sequence of words {y m } M m=1 , each of which is a symbol from the dictionary.",43,44
5288,53295957,"The input of the model is a source text {x i } = x 1 , ..., x N , and the output is a sequence of summary words {y t } = y 1 , ..., y T , each of which is a symbol from the dictionary V. Text Embedding Online posts include lots of morphologically similar words, which should be closely embedded.",49,50
5289,233297051,"We use the symbol ""&"" to denote ""and"", and ""->"" to denote ""entails"".",3,4
5290,1206836,"Note that Y can be rewritten either via production r or s. terminals, a finite set of productions P , and a start symbol S ∈ N .",24,25
5291,1206836,"From Table 2 and Figure 7 , we can see that FUSE p,0,x 1 ,s,2,x 8 (v 5 , v 1 , {W}) holds since v 5 = h v (c p , v 1 ), v 1 = h v (c s , v 8 ), v 11 = h v (x 1 , v 1 ) = h v (x 8 , v 8 ), ANC p,0,x 1 (v 5 , v 6 , {W}) and ANC s,2,x 8 (v 1 , v 11 , {W}) The Precondition of the Transducer Let X be in N , then P X = {p ∈ P |L(p) = X}, and an X-derivation tree is a derivation tree with respect to X as the start symbol (in this case, the root will have label in P X ).",150,151
5292,1206836,"For start symbol S of G, let τ = τ S .",2,3
5293,1798864,"'s (2010, Table 1 ) evaluation revealed a bug: MSTParser normalized all numbers to a <num> symbol, which decreased its scores in the evaluation tool used with Stanford dependencies.",21,22
5294,227904567,"Afterward, we use BERT to predict the most likely character of each [MASK] symbol, which is considered as correction candidates.",16,17
5295,229371222,"To linearize the proof in a format convenient for a generative model, we conjoin rules and their conclusions using a ""%"" symbol, express conjunctive rule conditions with a ""&"" symbol, and use ""#"" to denote the inverse implication (""←"").",24,25
5296,229371222,"To linearize the proof in a format convenient for a generative model, we conjoin rules and their conclusions using a ""%"" symbol, express conjunctive rule conditions with a ""&"" symbol, and use ""#"" to denote the inverse implication (""←"").",35,36
5297,229371222,"If no proof exists, the symbol ""None"" is used.",6,7
5298,229371222,"We then collect the alternative f actM s for each Q. The abduction task is then, given C and Q, identify the set of all alternative f actM s, i.e.: C, Q → f actM 1 , ..., f actM i If there is no single f actM that can be added to make Q true, then the symbol ""None"" is output.",65,66
5299,243865641,Why can't we ever seem to find a transformer where our winding symbol is both simple and reversible?,13,14
5300,243865641,"2015) to represent frequent symbol sequences in the text, and this tokenisation is performed on all new input prompts to generate text from the model.",5,6
5301,1211005,FORM: word form or punctuation symbol.,6,7
5302,25437880,"The standard set of token-level features used in the model include word identity, POS tag (from CoreNLP), and a list of binary features indicating whether the token is a digit, title (i.e., the first token only is uppercase), uppercase word, hyphenated word, or if the token is a punctuation mark (colon, fullstop or another symbol).",69,70
5303,9397228,"The likelihood of a sequence of word tokens, w = w i m i=1 , with each w i ∈ V, factors as L(w; C) = m i=1 p(C(w i )|C(w i−1 ))p(w i |C(w i )), (3.1) where C(w 0 ) is a special start-of-sequence symbol.",61,62
5304,2720624,"These exemplars take the form of a vector of, typically, nominal features, describing a linguistic problem and its context, and an associated class symbol representing the solution to the problem.",27,28
5305,236486074,"This form involves a preverb saa being affixed directly to a transitive verb bisbis, a reduplicated plural form of the verb which was listed directly in the H&R wordlist (the symbol ˆmarks morpheme boundaries).",32,33
5306,9289495,"The whole sentence is rooted at a designated special symbol ROOT, thus the dependency graph for a sentence is constrained to be a rooted, directed tree.",9,10
5307,6157828,"NE provides an efficient and effective way to represent and manage large-scale networks, alleviating the computation and sparsity issues of conventional symbol-based representations.",24,25
5308,218973742,It should be noted that the separator in the training data is not the standard pipe symbol but Unicode FFE8.,16,17
5309,233862165,The first symbol on each line (e.g. Bible.,2,3
5310,924818,"In case of English, there are indicators like uppercase character, dot mark, Dollar and Pound symbol etc.",18,19
5311,220046429,"Masking Every entity word x i in X, which is defined to be not associated with non-entity label ""O"", in sequence X is firstly randomly masked with OOV symbol ""[UNK]"" as follows: x u i = ""[UNK]"" if y i = ""O"" ∩ > p x i otherwise , (6) where constant p is a threshold and is uniformly sampled between 0 and 1.",34,35
5312,220046429,"To generate the i-th word in the local context sequence [x j+1 , x j+2 , • • • , x k−1 ], we first apply a RNN-decoder with its initial hidden state from the latent variable z f jk and the first observation from the embedding of ""[SOS]"" symbol to recursively obtain hidden state − → r f i as follows: − → r f i = − → f (x i , − → r f i−1 ), (9) This RNN-decoder specifically does parameter sharing with the forward pass RNN-encoder in Eq. (",59,60
5313,202616800,"But, because of the many named-entities, contractions and unusual character repetitions, this strategy is not effective for UGC as it leads the input sentence to contain many unknown BPE tokens (that are all mapped to the special symbol <UNK> before translating).",43,44
5314,237605592,"s n , [SEP]), where [CLS] is the special symbol for representing the whole sequence, and [SEP] is the special symbol to separate non-consecutive token sequences (Devlin et al.,",15,16
5315,237605592,"s n , [SEP]), where [CLS] is the special symbol for representing the whole sequence, and [SEP] is the special symbol to separate non-consecutive token sequences (Devlin et al.,",29,30
5316,237453543,"In Figure 3 for instance, there could be other British divers but we are only interested in selecting the cell marked with a star symbol ( ).",25,26
5317,21690773,"To test if the dependency annotation of two identical (sub-)sequences of words are the same, we consider that heads outside the sequence are replaced by a special symbol and head indices are replaced by their relative position.",29,30
5318,216552897,"Legal professionals often think about how to solve tasks from rulebased and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods.",12,13
5319,216552897,"To summarize, some efforts concentrate on symbol-based methods, which apply interpretable hand-crafted symbols to legal tasks (Ashley, 2017; Surden, 2018) .",7,8
5320,216552897,"More specifically, symbol-based methods concentrate more on utilizing interpretable legal knowledge to reason between symbols in legal documents, like events and relationships.",3,4
5321,216552897,We summarize three primary challenges for both embedding-based and symbol-based methods in LegalAI: (1) Knowledge Modelling.,11,12
5322,216552897,"Moreover, we illustrate several embedding-based and symbol-based methods and explore the future direction of LegalAI. (",9,10
5323,216552897,"Symbol-based Methods In this section, we describe symbol-based methods, also named as structured prediction methods.",10,11
5324,216552897,Deep learning methods can be employed for symbol-based methods for better performance.,7,8
5325,216552897,"Although embedding-based methods can achieve promising performance, we still need to consider combining symbol-based with embedding-based methods in LJP.",16,17
5326,216552897,"Take TopJudge as an example, this model formalizes topological order between the tasks in LJP (symbol-based part) and uses TextCNN for encoding the fact description.",17,18
5327,216552897,"By combining symbol-based and embedding-based methods, TopJudge has achieved promising results on LJP.",2,3
5328,216552897,"Comparing the results between TextCNN and TopJudge, we can find that just integrating the order of judgments into the model can lead to improvements, which proves the necessity of combining embedding-based and symbol-based methods.",36,37
5329,216552897,"Nevertheless, no matter what kind application is, we can apply embedding-based methods for better performance, together with symbol-based methods for more interpretability.",22,23
5330,216552897,"In the future, for these existing tasks, researchers can focus on solving the three most pressing challenges of LegalAI combining embedding-based and symbol-based methods.",26,27
5331,237552879,"Because there are multiple aspects to the Manchester paradigm, we provide three connections between the city and question answering: in the nineteenth century, the city's regiment used the mythical Sphinx as a symbol, it is the home to University Challenge, and it is where Alan Turing outlined the Turing Test.",36,37
5332,237552879,"To Teach: The Sphinx, Manchester's Standard Manchester's Regiment used the Sphinx as its symbol (Farmer, 1901) .",17,18
5333,53081706,"this nns using 'this' instead of 'these' or vice versa, e.g., 'you don't know what these symbol Acknowledgements This research was supported by Grant No.",24,25
5334,49568810,"2014) and replaced ""unknown"" tokens in each system with a special symbol ( ).",14,15
5335,214608366,"Like the Skip-Gram objective (Equation 2 ), we model the likelihood of a context word given the center sense P (c i j | s i k ) using softmax, P (c i j | s i k ) = exp ( c i j ⊤ s i k ) ∑ |V | j=1 exp ( c ⊤ j s i k ) , (14) where the bold symbol s i k is the vector representation of sense s j k from S, and c j is the context embedding of word c j from C. Computing the softmax over the vocabulary is timeconsuming.",77,78
5336,51868252,Named Entity Abstraction (NE): Each NE is mapped to a unique symbol corresponding to its category.,13,14
5337,51868252,"It is constructed by creating a feature for the ordered tuple of tokens comprising the chunk where all tokens are mapped to a single ""blank"" symbol or its extended POS except punctuations and stopwords.",27,28
5338,226262291,"Thus, we introduce a special token symbol [NLB] as the placeholder.",7,8
5339,52124918,We consider phoneme-ngrams ranging from 3-grams to 6grams and append a special start symbol < and end symbol > to the word.,17,18
5340,52124918,We consider phoneme-ngrams ranging from 3-grams to 6grams and append a special start symbol < and end symbol > to the word.,21,22
5341,56451541,"Given the list of matras (or vowelsigns), limiters (or punctuations) and the virama (or halant) symbol 7 for a script, it uses a set of rules to identify the syllables, which are given as follows: • If the given character is a limiter, it is considered as a separate syllable. •",22,23
5342,56451541,"We extended the existing code for syllabification to Telugu by simply adding the required lists of matras, limiters and the virama symbol which were obtained using the Unicode chart for Telugu 8 .",22,23
5343,224511993,"The Hamilton product (denoted by the ⊗ symbol) of two Quaternions P ∈ H and Q ∈ H is defined as: P ⊗ Q =(r P r Q − a P a Q − b P b Q − c P c Q )+ (r P a Q + a P r Q + b P c Q − c P b Q )i+ (r P b Q − a P c Q + b P r Q + c P a Q )j+ (r P c Q + a P b Q − b P a Q + c P r Q )k+ (5) Activation function on Quaternions: Similar to (Tay et al.,",8,9
5344,8070939,"The normalized attentional weights α ij measure the relative importance of the source words for the current decoding step and are computed as a softmax with normalization factor Z summing over i: α ij = 1 Z exp s s j−1 , h i (3) s(•) is a feed-forward neural network with a single layer that estimates the importance of source hidden state h i for producing the next target symbol y j , conditioned on the previous decoder state s j−1 .",76,77
5345,8070939,"The decoding at step j assumes a special start-of-sequence symbol y 0 and is computed as s j = LSTM E trg (y j−1 ), s j−1 , and then st = tanh(W hs [s j ; c j ] + b hs ) The conditional probability that the j-th target word is generated is: p(y j | y <j , x) = softmax(W so st + b so ).",13,14
5346,153312413,S is initialized with the special Root symbol.,7,8
5347,233189616,Repetition repeatedly uses the same symbol or idea to make it unforgettable. •,5,6
5348,195064006,"2015) , which is primarily a recurrent neural network (RNN) which reads from an external memory before outputting a symbol. (",22,23
5349,59037252,"""ippuDu saraina nirNayam mIru tIskOvAli"" (You should take the right decision now) is an imperative statement by Mr. Chandrababu Naidu whereas ""dayachesi fAn gurtu kE mI amUlyamaina vOTu veyyAlani mA prArthana""(We request you to please vote for the 'fan' symbol) is a request made by Ms. Sharmila.",46,47
5350,208051720,"In most cases we use the punctuation symbol as the node label, with one exception: we replace the double-quote character ("") with quot because the parser treats the double quote as a special character.",7,8
5351,18017180,The symbol @@ marks subword units generated by the BPE algorithm.,1,2
5352,90258162,"and symbol replacements (like ""$"" to ""dollar"", ""="" to ""is equal to"" etc).",1,2
5353,210722425,"Start with special symbol: Whether the word starts with any special symbol or character like !,",3,4
5354,210722425,"Start with special symbol: Whether the word starts with any special symbol or character like !,",12,13
5355,2229477,For each end-of-sequence symbol that is selected among the highest scoring candidates the beam is reduced by one and the translation is stored into a final candidate list.,7,8
5356,2229477,"Fan out has an upper bound of the size of the beam, but can be decreased either due to early stopping (we reduce the beam every time we predict a end-of-sentence symbol) or by the proposed pruning schemes.",37,38
5357,21697740,"In case there is a token in the validation or testing dataset which was not present in the training dataset, it is replaced with the unknown token symbol.",28,29
5358,219302365,Kay's (1987) pattern notation uses a G symbol before the C slot that needs to be doubled.,10,11
5359,219302365,"Kay's analysis in fact abstracts out the vocaliza- ]n the sanle spirit, but with a different mechanism, our Form II and Form V patterns contain an X symbol that appears after the consonant slot to be copied.",31,32
5360,219302365,"The X symbol is subsequently realized via finite-state variation rules as a copy of the preceding consonant in a phonological grammar (/darras/) or, in an orthographical system such as ours, as an optionally written shadda diacritic (~_~3).",2,3
5361,219302365,"Third Context: X is preceded by a surface C, another X and any symbol, no matter what follows.",15,16
5362,30939416,"As in (Ballesteros and Nivre, 2013) the buffer starts with the root symbol at the end of the sequence.",15,16
5363,51869051,"The target word vocabulary V y is extended with a special step symbol step Whenever step is predicted as the output symbol, the hard attention is moved to the next encoder state.",12,13
5364,51869051,"The target word vocabulary V y is extended with a special step symbol step Whenever step is predicted as the output symbol, the hard attention is moved to the next encoder state.",21,22
5365,1603570,Variables and maintenance of symbol tables in a frame-based environment. •,4,5
5366,10651440,∼ is the one-to-one alignment of the nonterminals between γ and α; w contains non-negative weights associated with each rule; is a label-symbol specifying the root node of the source span covering γ.,32,33
5367,905565,"y Ty ) of length T y , let h i be the annotation of the source symbol at position i, obtained by concatenating the forward and backward encoder RNN hidden states, h i = [ − → h i ; ← − h i ] , and s j be the decoder hidden state at position j. decoder initialization Bahdanau et al. (",17,18
5368,905565,"h Tx } and the previously decoded symbol y j−1 in order to update its hidden state s j , which is further used to decode symbol y j at position j, s j = cGRU att (s j−1 , y j−1 , C) Our conditional GRU layer with attention mechanism, cGRU att , consists of three components: two GRU state transition blocks and an attention mechanism ATT in between.",7,8
5369,905565,"h Tx } and the previously decoded symbol y j−1 in order to update its hidden state s j , which is further used to decode symbol y j at position j, s j = cGRU att (s j−1 , y j−1 , C) Our conditional GRU layer with attention mechanism, cGRU att , consists of three components: two GRU state transition blocks and an attention mechanism ATT in between.",26,27
5370,905565,"The first transition block, GRU 1 , combines the previous decoded symbol y j−1 and previous hidden state s j−1 in order to generate an intermediate representation s j with the following formulations: s j = GRU1 (yj−1, sj−1) = (1 − z j ) s j + z j sj−1, s j = tanh W E[yj−1] + r j (U sj−1) , r j = σ W r E[yj−1] + U r sj−1 , z j = σ W z E[yj−1] + U z sj−1 , where E is the target word embedding matrix, s j is the proposal intermediate representation, r j and 3 All the biases are omitted for simplicity.",12,13
5371,905565,"The attention mechanism, ATT, inputs the entire context set C along with intermediate hidden state s j in order to compute the context vector c j as follows: c j =ATT C, s j = Tx i α ij h i , α ij = exp(e ij ) T x k=1 exp(e kj ) , e ij =v a tanh U a s j + W a h i , where α ij is the normalized alignment weight between source symbol at position i and target symbol at position j and v a , U a , W a are the trained model parameters.",86,87
5372,905565,"The attention mechanism, ATT, inputs the entire context set C along with intermediate hidden state s j in order to compute the context vector c j as follows: c j =ATT C, s j = Tx i α ij h i , α ij = exp(e ij ) T x k=1 exp(e kj ) , e ij =v a tanh U a s j + W a h i , where α ij is the normalized alignment weight between source symbol at position i and target symbol at position j and v a , U a , W a are the trained model parameters.",92,93
5373,2070787,"The following is a brief outline of regular expressions in the Xerox notation: For each symbol s, the regular expression s denotes a regular language consisting of the single string ""s"".",16,17
5374,2070787,"The preceding plus signs of these ""tag"" symbols are included simply to improve the human readability of the resulting strings; because the plus sign is normally a special Kleene Plus symbol in regular expressions, it is literalized in the examples below with a preceding percent sign. [",33,34
5375,2070787,"A-B \B ""B the zero-length string (often called E) bracketing; denotes the same language as A the concatenation of B after A the union of A and B the intersection of A and B optionality, equivalent to [ A I 0 ] Kleene star iteration, zero or more concatenations of A equivalent to [ A A* ]) i.e. one or more concatenations of A the regular language A, ignoring any instances of B any symbol, i.e. the union of all single-symbol strings language A, minus all strings in language B equivalent to [?",87,88
5376,2070787,"A-B \B ""B the zero-length string (often called E) bracketing; denotes the same language as A the concatenation of B after A the union of A and B the intersection of A and B optionality, equivalent to [ A I 0 ] Kleene star iteration, zero or more concatenations of A equivalent to [ A A* ]) i.e. one or more concatenations of A the regular language A, ignoring any instances of B any symbol, i.e. the union of all single-symbol strings language A, minus all strings in language B equivalent to [?",96,97
5377,2070787,"-B], the union of all single-symbol strings minus the strings in B equivalent to [?*",9,10
5378,2070787,"Consonant spreading, as in Form IX and Form XII, and biliteral roots also use the morphophonemic X symbol (Beesley, 1998c) .",19,20
5379,19973349,"Let h i be the annotation of the source symbol at position i, obtained by concatenating the forward and backward encoder RNN hidden states, h i = [ − → h i ; ← − h i ], the set of encoder states C = {h 1 , . . . ,",9,10
5380,19973349,"Conditional GRU with attention We follow the Nematus implementation of the conditional GRU with attention, cGRU att : s j = cGRU att (s j−1 , E[y j−1 ], C) , (2) where s j is the newly computed hidden state, s j−1 is the previous hidden state, C the source context and E[y j−1 ] is the embedding of the previously decoded symbol y i−1 .",71,72
5381,19973349,"Layer GRU 1 generates an intermediate representation s j from the previous hidden state s j−1 and the embedding of the previous decoded symbol E[y j−1 ]: s j = GRU 1 (s j−1 , E[y j−1 ]) .",23,24
5382,19973349,"The attention mechanism, ATT, inputs the entire context set C along with intermediate hidden state s j in order to compute the context vector c j as follows: c j =ATT C, s j = Tx i α ij h i , α ij = exp(e ij ) Tx k=1 exp(e kj ) , e ij =v a tanh U a s j + W a h i , where α ij is the normalized alignment weight between source symbol at position i and target symbol at position j, and v a , U a , W a are trained model parameters.",85,86
5383,19973349,"The attention mechanism, ATT, inputs the entire context set C along with intermediate hidden state s j in order to compute the context vector c j as follows: c j =ATT C, s j = Tx i α ij h i , α ij = exp(e ij ) Tx k=1 exp(e kj ) , e ij =v a tanh U a s j + W a h i , where α ij is the normalized alignment weight between source symbol at position i and target symbol at position j, and v a , U a , W a are trained model parameters.",91,92
5384,19973349,"The target word vocabulary V y is extended with a special step symbol (V y = V y ∪ { STEP }) and whenever STEP is predicted as the output symbol, the hard attention is moved to the next encoder state.",12,13
5385,19973349,"The target word vocabulary V y is extended with a special step symbol (V y = V y ∪ { STEP }) and whenever STEP is predicted as the output symbol, the hard attention is moved to the next encoder state.",32,33
5386,19973349,"1) (thus removing the soft-attention mechanism): s j = GRU s j−1 , E[y j−1 ]; h a j , (3) where the cell input is now a concatenation of the embedding of the previous target symbol E[y j−1 ] and the currently attended encoder state h a j .",44,45
5387,19973349,"The end-of-sentence symbol can only be generated if the hard attention mechanism has reached the end of the input sequence, enforcing full coverage; 2.",6,7
5388,19973349,The STEP symbol cannot be generated once the end-of-sentence position in the source has been reached.,2,3
5389,19973349,"Next, we map these operations to the final sequence of steps and target tokens according to the following rules: • For each matched pair of tokens x, y we produce symbols: STEP y; • For each inserted target token y we produce the same token y; • For each deleted source token x we produce STEP ; • Since at initialization of the model a 1 = 1, i.e. the first encoder state is already attended to, we discard the first symbol in the new sequence if it is a STEP symbol.",89,90
5390,19973349,"Next, we map these operations to the final sequence of steps and target tokens according to the following rules: • For each matched pair of tokens x, y we produce symbols: STEP y; • For each inserted target token y we produce the same token y; • For each deleted source token x we produce STEP ; • Since at initialization of the model a 1 = 1, i.e. the first encoder state is already attended to, we discard the first symbol in the new sequence if it is a STEP symbol.",99,100
5391,14317748,"Let ""Art be defined as a single symbol (with a multi-character print name) that occurs always and only with !",8,9
5392,14317748,"+ u ^Def ^Nom + a ^Def ^Acc + i ^Def ^Gen + u n ^Indef ^Nom + a n ^Indef ^Acc + i n ^Indef ^Gen Overall, the resulting lexicon will generate legal strings that look like the following k i t a a b + u ^Def ^Nom d a a r i s + a n ^Indef ^Acc 1 ^Art + k i t a a b + a ^Def ^Acc b i ^NeedGen + 1 ^Art + d a a r i s + i'^Def ^Gen and many illegal strings including 1 ^Art + k i t a a b + a n ^Indef ^Acc b i ^NeedGen + 1 ^Art + d a a r i s + a ^Def .^Acc b i ^NeedGen + 1 ^Art + d a a r i s + i n ^Indef ^Gen As in standard Two-Level Morphology notation, let l:s be a symbol pair relating a lexical symbol I and a surface symbol s. Let l: denote all symbol pairs from the alphabet of the rules with I on the lexical side (not specifying what is on the surface side); and let :s denote all symbol pairs from the alphabet with s on the surface side.",160,161
5393,14317748,"+ u ^Def ^Nom + a ^Def ^Acc + i ^Def ^Gen + u n ^Indef ^Nom + a n ^Indef ^Acc + i n ^Indef ^Gen Overall, the resulting lexicon will generate legal strings that look like the following k i t a a b + u ^Def ^Nom d a a r i s + a n ^Indef ^Acc 1 ^Art + k i t a a b + a ^Def ^Acc b i ^NeedGen + 1 ^Art + d a a r i s + i'^Def ^Gen and many illegal strings including 1 ^Art + k i t a a b + a n ^Indef ^Acc b i ^NeedGen + 1 ^Art + d a a r i s + a ^Def .^Acc b i ^NeedGen + 1 ^Art + d a a r i s + i n ^Indef ^Gen As in standard Two-Level Morphology notation, let l:s be a symbol pair relating a lexical symbol I and a surface symbol s. Let l: denote all symbol pairs from the alphabet of the rules with I on the lexical side (not specifying what is on the surface side); and let :s denote all symbol pairs from the alphabet with s on the surface side.",165,166
5394,14317748,"+ u ^Def ^Nom + a ^Def ^Acc + i ^Def ^Gen + u n ^Indef ^Nom + a n ^Indef ^Acc + i n ^Indef ^Gen Overall, the resulting lexicon will generate legal strings that look like the following k i t a a b + u ^Def ^Nom d a a r i s + a n ^Indef ^Acc 1 ^Art + k i t a a b + a ^Def ^Acc b i ^NeedGen + 1 ^Art + d a a r i s + i'^Def ^Gen and many illegal strings including 1 ^Art + k i t a a b + a n ^Indef ^Acc b i ^NeedGen + 1 ^Art + d a a r i s + a ^Def .^Acc b i ^NeedGen + 1 ^Art + d a a r i s + i n ^Indef ^Gen As in standard Two-Level Morphology notation, let l:s be a symbol pair relating a lexical symbol I and a surface symbol s. Let l: denote all symbol pairs from the alphabet of the rules with I on the lexical side (not specifying what is on the surface side); and let :s denote all symbol pairs from the alphabet with s on the surface side.",170,171
5395,14317748,"+ u ^Def ^Nom + a ^Def ^Acc + i ^Def ^Gen + u n ^Indef ^Nom + a n ^Indef ^Acc + i n ^Indef ^Gen Overall, the resulting lexicon will generate legal strings that look like the following k i t a a b + u ^Def ^Nom d a a r i s + a n ^Indef ^Acc 1 ^Art + k i t a a b + a ^Def ^Acc b i ^NeedGen + 1 ^Art + d a a r i s + i'^Def ^Gen and many illegal strings including 1 ^Art + k i t a a b + a n ^Indef ^Acc b i ^NeedGen + 1 ^Art + d a a r i s + a ^Def .^Acc b i ^NeedGen + 1 ^Art + d a a r i s + i n ^Indef ^Gen As in standard Two-Level Morphology notation, let l:s be a symbol pair relating a lexical symbol I and a surface symbol s. Let l: denote all symbol pairs from the alphabet of the rules with I on the lexical side (not specifying what is on the surface side); and let :s denote all symbol pairs from the alphabet with s on the surface side.",177,178
5396,14317748,"+ u ^Def ^Nom + a ^Def ^Acc + i ^Def ^Gen + u n ^Indef ^Nom + a n ^Indef ^Acc + i n ^Indef ^Gen Overall, the resulting lexicon will generate legal strings that look like the following k i t a a b + u ^Def ^Nom d a a r i s + a n ^Indef ^Acc 1 ^Art + k i t a a b + a ^Def ^Acc b i ^NeedGen + 1 ^Art + d a a r i s + i'^Def ^Gen and many illegal strings including 1 ^Art + k i t a a b + a n ^Indef ^Acc b i ^NeedGen + 1 ^Art + d a a r i s + a ^Def .^Acc b i ^NeedGen + 1 ^Art + d a a r i s + i n ^Indef ^Gen As in standard Two-Level Morphology notation, let l:s be a symbol pair relating a lexical symbol I and a surface symbol s. Let l: denote all symbol pairs from the alphabet of the rules with I on the lexical side (not specifying what is on the surface side); and let :s denote all symbol pairs from the alphabet with s on the surface side.",208,209
5397,14317748,The symbol : by itself therefore stands for any symbol pair in the alphabet.,1,2
5398,14317748,The symbol : by itself therefore stands for any symbol pair in the alphabet.,9,10
5399,14317748,"Assuming that each feature symbol like ^Art is realized in surface strings as zero, the empty string, the necessary constraints are imposed by the following two-level rules: Rule 1 specifies that the lexical ""Art symbol never occurs in a string where it is followed by the qndef symbol.",4,5
5400,14317748,"Assuming that each feature symbol like ^Art is realized in surface strings as zero, the empty string, the necessary constraints are imposed by the following two-level rules: Rule 1 specifies that the lexical ""Art symbol never occurs in a string where it is followed by the qndef symbol.",40,41
5401,14317748,"Assuming that each feature symbol like ^Art is realized in surface strings as zero, the empty string, the necessary constraints are imposed by the following two-level rules: Rule 1 specifies that the lexical ""Art symbol never occurs in a string where it is followed by the qndef symbol.",53,54
5402,14317748,"Compiled into a finite-state transducer and consulted constantly during analysis, it ""remembers"" when it sees an ""Art symbol by moving into an ""I saw ^Art"" state.",23,24
5403,14317748,"However, this method requires a separate feature-unification mechanism to run in parallel with the usual morphological symbol processing, and this may degrade performance unacceptably.",19,20
5404,14317748,"Getting back to the Arabic examples, we can build the lexicon so that the lower-side language includes symbol @U.ART.YES@ as part of the definite article and so that the incompatible indefinite case suffixes are all marked @U.ART.NO@. In the course of analyzing a word, if a definite article is present, the lookup routine will find symbol @U.ART.YES@, interpret it as an epsilon for purposes of matching the input string, but setting and remembering that feature ART is set to value YES.",20,21
5405,14317748,"Getting back to the Arabic examples, we can build the lexicon so that the lower-side language includes symbol @U.ART.YES@ as part of the definite article and so that the incompatible indefinite case suffixes are all marked @U.ART.NO@. In the course of analyzing a word, if a definite article is present, the lookup routine will find symbol @U.ART.YES@, interpret it as an epsilon for purposes of matching the input string, but setting and remembering that feature ART is set to value YES.",60,61
5406,14317748,"If the symbol @U.ART.NO@ is found further on in the analysis path, the unification 7 will fail and the analyzer will be forced to backtrack to try to find another solution.",2,3
5407,14317748,"Conclusion Pure finite-state networks have no stack or other ""memory"" to store information about what morphemes or features have been accumulated; each transition from one state to the next depends only on the current input symbol.",40,41
5408,14317748,where V'Def:0 denotes all symbol pairs other than ^Def:0 and .#.,4,5
5409,196621534,"In cases where documents exceed our length limit of 1000 sub-word tokens, we use a break symbol (<BRK>) instead of <END> and start the next sequence with a continuation symbol (<CNT>) instead of <BEG>.",19,20
5410,196621534,"In cases where documents exceed our length limit of 1000 sub-word tokens, we use a break symbol (<BRK>) instead of <END> and start the next sequence with a continuation symbol (<CNT>) instead of <BEG>.",38,39
5411,16266186,"Because the upper-side string is returned as the result of an anMysis, it is often more helpful to define the upper-side string as a baseform (here a root) folh,wed by a set of symbol tags designed to represent relevant morphosyntactic features of the attalysis.",42,43
5412,8630862,Kay's (1987) pattern notation uses a G symbol before the C slot that needs to be doubled.,10,11
5413,8630862,Then the following C does the same but consumes the radical symbol in the usual way.,11,12
5414,8630862,"Kay's analysis in fact abstracts out the vocaliza- In the same spirit, but with a different mechanism, our Form II and Form V patterns contain an X symbol that appears after the consonant slot to be copied.",30,31
5415,8630862,"The X symbol is subsequently realized via finite-state variation rules as a copy of the preceding consonant in a phonological grammar (/darras/) or, in an orthographical system such as ours, as an optionally written shadda diacritic (~r,~.~).",2,3
5416,8630862,"Third Context: X is preceded by a surface C, another X and any symbol, no matter what follows.",15,16
5417,215826482,"The baseline features are the subword units obtained using BPE together with the annotation of the subword structure using IOB format by marking if a symbol in the text forms the beginning (B), inside (I), or end (E) of a word.",25,26
5418,215826482,A separate tag (O) is used if a symbol corresponds to the full word.,10,11
5419,215826482,"In the figure we use the symbol ""*"" to indicate that syntactic information is used on the target (eg.",6,7
5420,5963618,"Let h i be the annotation of the source symbol at position i, obtained by concatenating the forward and backward encoder RNN hidden states, h i = [ − → h i ; ← − h i ], the set of encoder states C = {h 1 , . . . ,",9,10
5421,5963618,"Decoder initialization The decoder is initialized with start state s 0 , computed as the average over all encoder states: s 0 = tanh W init Tx i=1 h i T x Conditional GRU with attention We follow the Nematus implementation of the conditional GRU with attention, cGRU att : s j = cGRU att (s j−1 , E[y j−1 ], C) (2) where s j is the newly computed hidden state, s j−1 is the previous hidden state, C the source context and E[y j−1 ] is the embedding of the previously decoded symbol y i−1 .",103,104
5422,5963618,"Layer GRU 1 generates an intermediate representation s j from the previous hidden state s j−1 and the embedding of the previous decoded symbol E[y j−1 ]: s j = GRU 1 (s j−1 , E[y j−1 ]) .",23,24
5423,5963618,"The attention mechanism, ATT, inputs the entire context set C along with intermediate hidden state s j in order to compute the context vector c j as follows: c j =ATT C, s j = Tx i α ij h i , α ij = exp(e ij ) Tx k=1 exp(e kj ) , e ij =v a tanh U a s j + W a h i , where α ij is the normalized alignment weight between source symbol at position i and target symbol at position j and v a , U a , W a are trained model parameters.",85,86
5424,5963618,"The attention mechanism, ATT, inputs the entire context set C along with intermediate hidden state s j in order to compute the context vector c j as follows: c j =ATT C, s j = Tx i α ij h i , α ij = exp(e ij ) Tx k=1 exp(e kj ) , e ij =v a tanh U a s j + W a h i , where α ij is the normalized alignment weight between source symbol at position i and target symbol at position j and v a , U a , W a are trained model parameters.",91,92
5425,5963618,"The target word vocabulary V y is extended with a special step symbol (V y = V y ∪ { STEP }) and whenever STEP is predicted as the output symbol, the hard attention is moved to the next encoder state.",12,13
5426,5963618,"The target word vocabulary V y is extended with a special step symbol (V y = V y ∪ { STEP }) and whenever STEP is predicted as the output symbol, the hard attention is moved to the next encoder state.",32,33
5427,5963618,"1) (thus removing the soft-attention mechanism): s j = GRU s j−1 , E[y j−1 ]; h a j (3) where the cell input is now a concatenation of the embedding of the previous target symbol E[y j−1 ] and the currently attended encoder state h a j .",43,44
5428,5963618,"The end-of-sentence symbol can only be generated if the hard attention mechanism has reached the end of the input sequence, enforcing full coverage; 2.",6,7
5429,5963618,The STEP symbol cannot be generated once the end-of-sentence position in the source has been reached.,2,3
5430,5963618,"Next, we map these operations to the final sequence of steps and target tokens according to the following rules: • For each matched pair of tokens x, y we produce symbols: STEP y; • For each inserted target token y, we produce the same token y; • For each deleted source token x we produce STEP ; • Since at initialization of the model a 1 = 1, i.e. the first encoder state is already attended to, we discard the first symbol in the new sequence if it is a STEP symbol.",90,91
5431,5963618,"Next, we map these operations to the final sequence of steps and target tokens according to the following rules: • For each matched pair of tokens x, y we produce symbols: STEP y; • For each inserted target token y, we produce the same token y; • For each deleted source token x we produce STEP ; • Since at initialization of the model a 1 = 1, i.e. the first encoder state is already attended to, we discard the first symbol in the new sequence if it is a STEP symbol.",100,101
5432,14510744,"When deciding whether to generate another child in the direction dir or the STOP symbol, we use the P dmv stop (STOP |c h , dir , adj , c f ) model.",14,15
5433,594,"For (i), operators are provided that construct enriched automata from a simple string automaton, in particular giving it a kind of doubly-linked structure so that the symbol repetition inherent in reduplication translates into following backwards-pointing technical transitions.",32,33
5434,594,"The three aspects of reduplication or symbol repetition, truncation or symbol skipping and infixation or transitive, non-immediate precedence of symbols are reflected in three regular expression operators, .",6,7
5435,594,"The three aspects of reduplication or symbol repetition, truncation or symbol skipping and infixation or transitive, non-immediate precedence of symbols are reflected in three regular expression operators, .",11,12
5436,594,"1 may be defined in a modular fashion through the intersection of strings of symbol sets that mention only certain dimensions (here: phonemes and synchronisation bits), being underspecified for the unmentioned dimensions.",14,15
5437,594,"In that scenario a symbol represents an information resource that needs to be produced at least once, then can be consumed arbitrarily often.",4,5
5438,594,"Two elsewhere cases deal with closed syllables and the possible presence of a technical symbol: Aspectual affixes It is time to concentrate on the most interesting part, and that is how to define the affixes.",14,15
5439,7891416,"Formally, an AoG is defined as a 5-tuple G = (S, Σ, N, R, Θ), where • S is a root node (or a start symbol) representing a complete task. •",36,37
5440,7891416,"4 For each state mapping hypothesis, we add a completed edge between indices m and m + 1 in the chart, with s k as its symbol and a probability p based on the similarity between v m and s k .",28,29
5441,3854494,"n"", ""JM**shopB"" contain a special symbol '*'.",9,10
5442,53093335,"In this study, the intent tag is handled as a symbol, and the slot-tags and slot-values are handled as a sequence of symbols.",11,12
5443,53093335,"In addition, reconstruction consistency from a semantic vector to symbol semantic frame was jointly enforced to prevent the method from learning trivial degenerate mappings (e.g. mapping all to zeros).",10,11
5444,2749334,Pinyin is originally designed as the phonetic symbol of a Chinese character.,7,8
5445,2749334,"In Chinese Pinyin system, tone is represented as an accent symbol over Latin letters, which is not convenient to input and thus usually ignored in most Chinese keyboard input methods.",11,12
5446,234337054,"After obtaining the primitive set P and symbol set S, we need to ground each symbol with its associated primitives. (",7,8
5447,234337054,"After obtaining the primitive set P and symbol set S, we need to ground each symbol with its associated primitives. (",16,17
5448,234337054,2015) adapts a greedy approach where each symbol is assigned to the closest primitive without considering its validity.,8,9
5449,234337054,"s i , p j ) ∈ Feasibility set F, (1) where the dist function measures the Euclidean distance between the symbol s i and primitive p j .",24,25
5450,234337054,F defines the geometric constraints for symbol grounding.,6,7
5451,234337054,"For example, the parallel symbol could only be assigned to two lines with the same slopes and the perpendicular symbol is only valid to two orthogonal lines.",5,6
5452,234337054,"For example, the parallel symbol could only be assigned to two lines with the same slopes and the perpendicular symbol is only valid to two orthogonal lines.",20,21
5453,17798040,"A set of ""symbol grounding functions"" are used to bridge between the heterogeneous attributes (described later).",4,5
5454,17798040,"and p x(k) α | x (k) i is what we call a ""symbol grounding function"", i.e., the probability of observing x(k) α given the word x (k) i .",17,18
5455,17798040,These symbol grounding functions can be either manually defined or automatically learned.,1,2
5456,17798040,"namely the P (a i a j | θ i = ω α , θ j = ω β ) for com- puting Q (n) (θ i = ω α ) , is also based on the attributes of the two edges and their corresponding symbol grounding functions.",49,50
5457,15866329,This lattice is converted by the lexer in a lexeme lattice (a lexeme being here a CFG terminal symbol associated with underspecified f-structures).,19,20
5458,15866329,"Formally, in an Earley parser whose input is a DAG, an error is detected when, whatever the active table T [j], items of the form I = [A → α.tβ, i] in this table are such that in the DAG there is no out-transition on t from node j. We say that a recovery is possible in k on β if in the suffix β = β 1 Xβ 2 there exists a derived phrase from the symbol X which starts with a terminal symbol r and if there exists a node k in the DAG, k ≥ j, with an out-transition on r. If it is the case and if this possible recovery is selected, we put the item [A → αtβ 1 .Xβ 2 , i] in table T [k] .",88,89
5459,15866329,"Formally, in an Earley parser whose input is a DAG, an error is detected when, whatever the active table T [j], items of the form I = [A → α.tβ, i] in this table are such that in the DAG there is no out-transition on t from node j. We say that a recovery is possible in k on β if in the suffix β = β 1 Xβ 2 there exists a derived phrase from the symbol X which starts with a terminal symbol r and if there exists a node k in the DAG, k ≥ j, with an out-transition on r. If it is the case and if this possible recovery is selected, we put the item [A → αtβ 1 .Xβ 2 , i] in table T [k] .",95,96
5460,1247193,"Our solution is to introduce a set of symbol grounding functions, which bridges the heterogeneous attributes of the two graphs and makes general graph matching algorithms applicable to referential grounding.",8,9
5461,1247193,"Symbol Grounding Functions As mentioned in Section 4.1, in referential grounding the discourse graph and the vision graph possess different types of attribute values, therefore we introduce a set of ""symbol grounding functions"", based on which node/edge substitution and insertion costs can be formally defined.",33,34
5462,1247193,We start with node substitution cost to give a formal definition of symbol grounding functions.,12,13
5463,1247193,"Thus, we define the node substitution cost as C N (a, a ) = K k=1 − ln f k (v k , v k ) in which f k (v k , v k ) = p (p ∈ [0, 1] ) is what we call the symbol grounding function for the k-th attribute.",57,58
5464,1247193,"More specifically, a symbol grounding function for the k-th attribute takes two input arguments, namely v k and v k , which are the values of the k-th attribute from node a and a respectively.",4,5
5465,1247193,"The output of the function is a real number p in the range of [0, 1], which can be interpreted as a measurement of the compatibility between a symbol (or word) v k and a visual feature value v k .",32,33
5466,1247193,"For example, suppose that we are defining a symbol grounding function for the attribute of ""spatial location"", i.e. where is an object located in the environment.",9,10
5467,1247193,"A grounding function for the symbol Top can be defined as 4 f Top (v ) = f Top (x, y) = 1 − y 800 if y < 400; 0 otherwise.",5,6
5468,1247193,"Note that we have added a special symbol UNK to represent the ""unknown"" (or ""unspecified"") value of v k .",7,8
5469,1247193,"The node insertion cost C N (a, Λ) is now defined as 5 C N (a, Λ) = K k=1 − ln λ k Currently we set all the symbol grounding functions' outputs for the unknown value (i.e. the λs) to ε, which is an arbitrarily small real number (ε > 0).",35,36
5470,1247193,"Second, our current symbol grounding functions are very simple and intuitive.",4,5
5471,1247193,"In addition, we will explore context-based symbol grounding functions where context will be explicitly modeled.",9,10
5472,18179185,"Since the two corpora have different annotation guidelines, we mapped semantically equivalent Mayo annotations to i2b2 annotations in the following way (Mayo → i2b2): mo → mo; sn, su, do → do; fn, fu →f; fm → N/A. In order to simplify the expression of semantic patterns we assigned a short symbol to dominant patterns.",62,63
5473,18179185,The semantic pattern in the figure is described as a short symbol with the actual semantic pattern in parentheses.,11,12
5474,199552332,Each item is delimited by a comma; the & symbol between tokens in one item means that this signal consists of a word pair in respective spans.,10,11
5475,227905652,"As shown in E2, among all alphabetical chunks, many candidates are URL links, tags related to topics (surrounded by #), or user names (introduced by the ""@"" symbol).",36,37
5476,21175629,"2012) also can be solve by the flag value (0/1) used after % symbol in 'cref' tag, which indicates end of the lexical textual span of a mention.",16,17
5477,218974164,"This process implies that the character 'a' does not have a direct correspondence with the sound 'ae' or '9'; instead, the appropriate symbol to transcribe each pronunciation might have been 'a'.",30,31
5478,218974164,"For example, in the case of logograms such as Chinese characters, there is little relationship between the composition of the character (bushu) and the pronunciation of the symbol (Figure 1 , top).",31,32
5479,218974164,"Beyond the limitation of symbol representation, what makes this more challenging are 1) the different sound produced by the Korean consonants that come to the first and third sound, and 2) the sound change that takes place when the third sound meets the first sound of the next syllable.",4,5
5480,9752897,"At each time step t , it updates its hidden state by z m t = ϕ m (z m t −1 , ỹm t −1 , c m t ), based on the previous hidden state z m t −1 , previous target symbol ỹm t −1 and the time-dependent context vector c m t .",46,47
5481,9752897,"3) With the new hidden state z m t , the probability distribution over the next symbol is computed by p(y t = w|ỹ <t , X n ) ∝ exp(g m w (z m t , c m t , E m y [ỹ t−1 ]), (4) where g m w is a decoder specific parametric function that returns the unnormalized probability for the next target symbol being w. Learning Training this multi-way, multilingual model does not require multi-way parallel corpora but only a set of bilingual corpora.",17,18
5482,9752897,"3) With the new hidden state z m t , the probability distribution over the next symbol is computed by p(y t = w|ỹ <t , X n ) ∝ exp(g m w (z m t , c m t , E m y [ỹ t−1 ]), (4) where g m w is a decoder specific parametric function that returns the unnormalized probability for the next target symbol being w. Learning Training this multi-way, multilingual model does not require multi-way parallel corpora but only a set of bilingual corpora.",75,76
5483,9752897,Each symbol is represented as a 620dimensional vector.,1,2
5484,218973954,2010) Suppression This mechanism replaces some attribute values by a symbol like '*' to indicate those attributes are repressed.,11,12
5485,17698774,A genetic algorithm is used to automatically construct finite automata from training sets of symbol strings.,14,15
5486,4640543,The @@ symbol is a product of byte-pair encoding (and would not be displayed to users in a CAT tool).,2,3
5487,14974322,"A word in HowNet is defined as a set of concepts, and each concept is represented by its up to four different primitives classified as: basic independent primitive (weighted by  1 in formula ( 4 )), other independent primitive (weighted by  2 ), relation primitive (weighted by  3 ), and symbol primitive (weighted by  4 ), where basic independent primitive and other independent primitive are used to calculate the semantic relationship between two concepts and the another two primitives are used to measure the syntactic relationships between two concepts.",62,63
5488,51880354,"To further enhance the capability of our model to handle unseen entities, we propose to use entity masking, which maps the entities in the model training pairs to their types, e.g., we map an entity (literal) ""1967-01-10"" to a type symbol ""DATE"" in the training pairs.",52,53
5489,3888499,"The performance gain of Bi-LSTM + is statistically significant (p < 0.01) on the 60K dataset, which is emphasized using a † symbol.",28,29
5490,236459943,"symbol or any WH-word • Detecting the number of personal pronouns and what kind of personal pronouns they are: first-person, second-person or third-person.",0,1
5491,243865647,2020) runs on the concatenation of utterances divided by a [SEP] symbol and segment embeddings corresponding to the speaker of each utterance.,14,15
5492,248505919,"The data covers ten brain ROIs in the human brain, i.e., Left hemisphere (L), and Right hemisphere (R) for each of the following: (i) early auditory cortex (EAC: A1, LBelt, MBelt, PBelt, and R1) which plays a key role for sound perception since it represents one of the first cortical processing stations for sounds; (ii) auditory association cortex (AAC: A4, A5, STSdp, STSda, STSvp, STSva, STGa, and TA2) which is concerned with the memory and classification of sounds; (iii) posterior medial cortex (PMC: POS1, POS2, v23ab, d23ab, 31pv, 31pd, 7m); (iv) the temporo parieto occipital junction (TPOJ: TPOJ1, TPOJ2, TPOJ3, STV, PSL) which is a complex brain territory heavily involved in several high-level neurological functions, such as language, visuo-spatial recognition, writing, reading, symbol processing, calculation, self-processing, working memory, musical memory, and face and object recognition; and (v) the dorsal frontal lobe (DFL: L_55b, SFL, L_44, L_45, IFJA, IFSP) which covers the aspects of pragmatic processing such as discourse management, integration of prosody, interpretation of nonliteral meanings, inference making, ambiguity resolution, and error repair.",184,185
5493,10991505,"Polygrams can be denoted by its Schläfli symbol {p/q}, where p gives the number of corners, and q states that each corner should be connected to its neighbours q steps away.",7,8
5494,248376820,"Previous studies on symbol-description extraction rely on pattern matching (Yokoi et al.,",3,4
5495,248376820,These methods might work for observed patterns with an assumption of close proximity between symbol and description.,14,15
5496,248376820,We believe that addressing the problem at this fine-grain level is crucial to drive future research toward a better understanding of the complex symbol-description extraction task.,25,26
5497,248376820,"As such, our obtained papers contain a large number of mathematical symbols and equations, allowing a higher yield of extracted symbol-description relations.",22,23
5498,248376820,"For relation, we are particularly interested in two main types of relations: DIRECT, linking a symbol with its definition, and COUNT, linking a description of a con-cept with a symbol that is the number of instances of the concept.",18,19
5499,248376820,"For relation, we are particularly interested in two main types of relations: DIRECT, linking a symbol with its definition, and COUNT, linking a description of a con-cept with a symbol that is the number of instances of the concept.",36,37
5500,248376820,"The distributions of symbol-description relations have long tails, indicating that symbols and descriptions tend to ap-  pear in close proximity.",3,4
5501,248376820,"Symbol tokenizer and detection In this shared task, the uniqueness of the task is detecting mathematical symbol span.",17,18
5502,248376820,These are the overall rules for entity annotations: • We only tag a description if the corresponding symbol presents in the text. •,18,19
5503,248376820,"Descriptions should be short but it must cover the elements in the corresponding symbol, esp.",13,14
5504,248376820,"Symbol tagging: A mathematical symbol can present an operand, an operator, an expression, or combination of these. •",5,6
5505,248376820,"An atomic symbol in PDF format has to be a character, that means, if we have Y hat, neither Y nor hat is considered an atomic symbol, instead ""Y hat"" is a symbol.",2,3
5506,248376820,"An atomic symbol in PDF format has to be a character, that means, if we have Y hat, neither Y nor hat is considered an atomic symbol, instead ""Y hat"" is a symbol.",29,30
5507,248376820,"An atomic symbol in PDF format has to be a character, that means, if we have Y hat, neither Y nor hat is considered an atomic symbol, instead ""Y hat"" is a symbol.",38,39
5508,248376820,"A complex symbol is a combination of multiple symbols and brackets, for example: ""P(x)"", ""Wx"" • An annotated symbol has to be a complete symbol e.g. ""P(x)"" is good, ""P(x"" is not because of lacking the closing parenthesis. •",2,3
5509,248376820,"A complex symbol is a combination of multiple symbols and brackets, for example: ""P(x)"", ""Wx"" • An annotated symbol has to be a complete symbol e.g. ""P(x)"" is good, ""P(x"" is not because of lacking the closing parenthesis. •",26,27
5510,248376820,"A complex symbol is a combination of multiple symbols and brackets, for example: ""P(x)"", ""Wx"" • An annotated symbol has to be a complete symbol e.g. ""P(x)"" is good, ""P(x"" is not because of lacking the closing parenthesis. •",32,33
5511,248376820,"A complex formula can be segmented into atomic symbols, we will annotate at all levels of the complex symbol as long as there are appropriate descriptions available.",19,20
5512,248376820,Relation annotation: • Every annotated symbol/description has to have at least one relation linking to its description/symbol.,6,7
5513,248376820,Relation annotation: • Every annotated symbol/description has to have at least one relation linking to its description/symbol.,21,22
5514,243838303,"Symbols: This covers any special symbol or sign used to express an idea, mood or feelings, e.g. emoticons ""-:)"", emojis "" ""; or as a replacement for a word, e.g. using ""@"" to mean ""at"".",6,7
5515,388111,The symbol 'x' in a PROSITE pattern is used for a position where any amino acid is accepted. ',1,2
5516,127919,"In almost all the current encoding systems including ISO 10646 and Unicode, each Han character is treated as a separate unique symbol and given a separate code point.",22,23
5517,127919,Each IDC symbol shows a typical ideograph character composition structure.,2,3
5518,17774659,"For example, if the user says: ""I want to go downtown"", o to.desc t = downtown, then the state should be s to.desc t = downtown; and when the user says nothing in the next turn, o to.desc t+1 = (where the symbol is a special slot value representing that the user was silent), the state remains s to.desc t+1 = downtown.",51,52
5519,1648044,"Informally, in a LCFRS every nonterminal symbol A is associated with an integer ϕ(A) ≥ 1, called its fan-out, and it generates tuples in (Σ * T ) ϕ (A) .",7,8
5520,14300428,"Every symbol, that is used in the word is set to one and all the other features are zero.",1,2
5521,15937393,"For every node of the tree visited, a new symbol is added to the right hand side of rule, from left to right, as follows: • The anchor of the elementary tree adds the supertag (i.e., the name of the tree), which is a terminal symbol, to the context-free rule. •",10,11
5522,15937393,"For every node of the tree visited, a new symbol is added to the right hand side of rule, from left to right, as follows: • The anchor of the elementary tree adds the supertag (i.e., the name of the tree), which is a terminal symbol, to the context-free rule. •",53,54
5523,15937393,A substitution node in the elementary tree adds its nonterminal symbol to the context-free rule. •,10,11
5524,15937393,"A interior node in the elementary tree at which adjunction may occur adds to the context-free rule the nonterminal symbol X * r or X * l , where X is the node's nonterminal symbol, and l (resp.",21,22
5525,15937393,"A interior node in the elementary tree at which adjunction may occur adds to the context-free rule the nonterminal symbol X * r or X * l , where X is the node's nonterminal symbol, and l (resp.",37,38
5526,15937393,"A set of non-lexicalized rules (i.e., rules that do not generate a terminal symbol) allow us to generate zero or more trees anchored by X l from the symbol X * l .",17,18
5527,15937393,"A set of non-lexicalized rules (i.e., rules that do not generate a terminal symbol) allow us to generate zero or more trees anchored by X l from the symbol X * l .",33,34
5528,15937393,"A particular effort has been made to handle huge grammars (over 1 million symbol occurrences in the grammar), thanks to advanced dynamic lexicalization techniques (Boullier and Sagot, 2007) .",14,15
5529,1876856,"Parsing or generating starts in the start state, and at any given moment makes a transition with a certain probability to another state and emits a symbol.",27,28
5530,1876856,We have a particular symbol and state which correspond to finishing.,4,5
5531,1876856,"A PDFA A is a tuple (Q, Σ, q 0 , q f , ζ, τ, γ) , where • Q is a finite set of states, • Σ is the alphabet, a finite set of symbols, • q 0 ∈ Q is the single initial state, • q f ∈ Q is the final state, • ζ ∈ Σ is the final symbol, • τ : Q × Σ ∪ {ζ} → Q ∪ {q f } is the transition function and • γ : Q × Σ ∪ {ζ} → [0, 1] is the next symbol probability function.",74,75
5532,1876856,"A PDFA A is a tuple (Q, Σ, q 0 , q f , ζ, τ, γ) , where • Q is a finite set of states, • Σ is the alphabet, a finite set of symbols, • q 0 ∈ Q is the single initial state, • q f ∈ Q is the final state, • ζ ∈ Σ is the final symbol, • τ : Q × Σ ∪ {ζ} → Q ∪ {q f } is the transition function and • γ : Q × Σ ∪ {ζ} → [0, 1] is the next symbol probability function.",117,118
5533,5086649,"Moreover, we note rhs k (p), with 1 ≤ k ≤ |p|, the k th symbol of rhs(p).",20,21
5534,5086649,"t j of w, that can be identified by the corresponding range, noted i..j. A complete derivation represents a parse tree whose yield is w, in which each symbol X of range i..j roots a subtree whose yield is t i+1 . . .",33,34
5535,5086649,An instantiated non terminal symbol is a triple noted A i..j where A ∈ N and 0 ≤ i ≤ j ≤ |w|.,4,5
5536,5086649,"Similarly, an instantiated terminal symbol is a triple noted T i..j where T ∈ T and 0 ≤ i ≤ j = i + 1 ≤ |w|.",5,6
5537,5086649,"An instantiated symbol, terminal or non terminal, is noted X i..j .",2,3
5538,5086649,"For any instantiated symbol X i..j , i (resp.",3,4
5539,5086649,"X r ir..jr whose left-hand side is an instantiated non terminal symbol and whose righthand side is a (possibly empty) sequence of instantiated (terminal or non terminal) symbols, provided the followings conditions hold: 1.",15,16
5540,5086649,"X r β * ⇒ G,w w, any symbol X that spans the range i..j can be replaced by the instantiated symbols X i..j .",11,12
5541,5086649,"In an instantiated parse tree, each node label is unique, and therefore we shall not distinguish between a node in an instantiated parse tree and its label (i.e., an instantiated symbol).",34,35
5542,5086649,"The size of a forest is defined as the size of the grammar that represents it, i.e., as the number of symbol occurrences in this grammar, which is defined as the number of productions plus the sum of the lengths of all right-hand sides.",23,24
5543,5086649,"Det 0..1 → the 0..1 N 1..2 → boy 1..2 NP 0..2 → Det 0..1 N 1..2 V 2..3 → We define the extension of an instantiated symbol X i..j , noted E(X i..j ), as the set of instantiated parse trees that have X i..j as a root.",43,44
5544,5086649,A bottom-up traversal of a forest is a traversal with the following constraint: an A i..j -production is visited if and only if all its instantiated righthand side symbols have already been visited; the instantiated symbol A i..j is visited once all A i..jproductions have been visited.,41,42
5545,5086649,"This extra structure takes the form of n-best tables that are associated with each instantiated non terminal symbol (Huang and Chiang, 2005) , thus leading to ranked instantiated non terminal symbols, or simply instantiated symbols when the context is non ambiguous.",19,20
5546,5086649,"A ranked instantiated non terminal symbol is written A i..j , T (A i..j ) , where T (A i..j ) is the n-best table associated with the instantiated symbol A i..j .",5,6
5547,5086649,"A ranked instantiated non terminal symbol is written A i..j , T (A i..j ) , where T (A i..j ) is the n-best table associated with the instantiated symbol A i..j .",39,40
5548,5086649,"Recall that in Example 1, the symbol VP 2..8 can be rewritten using the two following productions : VP 2..8 → V 2..3 NP 3..8 VP 2..8 → VP 2..5 PP 5..8 T (VP 2..8 ) has the following form: 1 P 1 VP 2..8 → V 2..3 NP 3..8 1, 1 1 2 P 2 VP 2..8 → VP 2..5 PP 5..8 1, 1 1 This table indicates that the most likely tree associated with VP 2..8 (line one) has probability P 1 and is built using the production VP 2..8 → V 2..3 NP 3..8 by combining the most likely tree of E(V 2..3 ) (indicated by the first 1 in 1, 1 ) with the most likely tree of E(NP 3..8 ) (indicated by the second 1 in 1, 1 ).",7,8
5549,5086649,"Given an instantiated symbol A i..j and an instantitated production p ∈ P (A i..j ), we define the n-best table of p to be the table composed of the entries P k , p k , v k , l k of T (A i..j ) such that p k = p. Example 3: Second running example.",3,4
5550,5086649,"S 1..3 → A 1..2 B 2..3 A 1..2 → A1 1..2 A1 1..2 → a 1..2 A 1..2 → A2 1..2 A2 1..2 → a 1..2 B 2..3 → B1 2..3 B1 2..3 → b 2..3 B 2..3 → B2 2..3 B2 2..3 → b 2..3 This grammar represents a parse forest that contains four different trees, since on the one hand one can reach (parse) the instantiated terminal symbol a 1..2 through A1 or A2, and on the other hand one can reach (parse) the instantiated terminal symbol b 1..2 through B1 or B2.",111,112
5551,5086649,"S 1..3 → A 1..2 B 2..3 A 1..2 → A1 1..2 A1 1..2 → a 1..2 A 1..2 → A2 1..2 A2 1..2 → a 1..2 B 2..3 → B1 2..3 B1 2..3 → b 2..3 B 2..3 → B2 2..3 B2 2..3 → b 2..3 This grammar represents a parse forest that contains four different trees, since on the one hand one can reach (parse) the instantiated terminal symbol a 1..2 through A1 or A2, and on the other hand one can reach (parse) the instantiated terminal symbol b 1..2 through B1 or B2.",135,136
5552,5086649,In that case introduce the rankset ρ = ρ 1 ∪ ρ 2 and create a new non-terminal symbol A ρ i..j .,20,21
5553,5086649,"After such a transformation, the newly created symbol may appear in the right-hand side of productions that now only differ by their left-hand sides; the factorization spreads to this symbol in a bottomup way.",8,9
5554,5086649,"After such a transformation, the newly created symbol may appear in the right-hand side of productions that now only differ by their left-hand sides; the factorization spreads to this symbol in a bottomup way.",35,36
5555,5086649,"This traversal also takes as input two parameters x and y. It starts with the symbol S 0..|w| and parameters 1 and n. At the end of the traversal, a new decorated forest is built which contains exactly n most likely the parses.",15,16
5556,5086649,"During the traversal, every instantiated symbol A i..j will give birth to decorated instantiated symbols of the form A x,y i..j where x and y are determined during the traversal.",6,7
5557,5086649,Two different actions are performed depending on whether we are visiting an instantiated symbol or an instantiated production.,13,14
5558,5086649,"Visiting an instantiated symbol When visiting an instantiated symbol A i..j with parameters x and y, a new decorated instantiated symbol A x,y i,j is created and the traversal continues on the instantiated productions of P (A i..j ) with parameters that have to be computed.",3,4
5559,5086649,"Visiting an instantiated symbol When visiting an instantiated symbol A i..j with parameters x and y, a new decorated instantiated symbol A x,y i,j is created and the traversal continues on the instantiated productions of P (A i..j ) with parameters that have to be computed.",8,9
5560,5086649,"Visiting an instantiated symbol When visiting an instantiated symbol A i..j with parameters x and y, a new decorated instantiated symbol A x,y i,j is created and the traversal continues on the instantiated productions of P (A i..j ) with parameters that have to be computed.",23,24
5561,5086649,Suppose we are visiting the instantiated symbol A i..j with parameters 5 and 10.,6,7
5562,5086649,"But this optimization is local, it does not take into account the fact that an instantiated symbol may be shared in the initial forest.",17,18
5563,5086649,"It gives birth to two S 1..3 -productions, namely: S 1,3 1..3 → A 1,2 1..2 B 1,1 2..3 S 1,3 1..3 → A 1,1 1..2 B 2,2 2..3 Since this is the only non-trivial step while applying the rectangles algorithm to this example, we can now give its final result, in which the axiom's (unnecessary) decorations have been removed: S 1..3 → A 1,2 1..2 B {1,1} 2..3 S 1..3 → A 1,1 1..2 B {2,2} 2..3 A 1,2 1..2 → A1 1..2 A1 1..2 → a 1..2 A 1,2 1..2 → A2 1..2 A2 1..2 → a 1..2 B 1,2 2..3 → B1 2..3 B1 2..3 → b 2..3 B 2,2 2..3 → B2 2..3 B2 2..3 → b 2..3 Compared to the forest built by the ranksets algorithm, this forest has one less production and one less non-terminal symbol.",216,217
5564,220403427,The process of creating these connections is called symbol grounding and has been studied for nearly three decades.,8,9
5565,220403427,"Related Work Grounding is used to obtain the meaning of an abstract symbol, e.g. a word, by linking it to perceptual information, i.e. the ""real"" world (Harnad, 1990) .",12,13
5566,5569514,"If the subargument is a terminal symbol (i.e., k + 1 = k ′ and X k ′ ∈ T ), ρ is such that l + 1 = u and a u = X k ′ .",6,7
5567,5569514,Note that several occurrences of the same terminal symbol may be instantiated by different ranges. •,8,9
5568,5569514,"If the subargument is a variable symbol (i.e., k + 1 = k ′ and X k ′ ∈ V ), any occurrence (c[i ′ ][j ′ ][m], c[i ′ ][j ′ ][m ′ ]) of X k ′ is instantiated by ρ.",6,7
5569,5569514,"Thus, each occurrence of the same variable symbol must be instantiated by the same range. •",8,9
5570,5569514,"∨ predicate (S, q 0 ..f |F | ) with f i ∈ F 8 and the terminal symbol a can be instantiated by the range p a ..q a D only if (p a , a, q a ) is a transition in δ.",21,22
5571,5569514,The variable symbol X can be instantiated by the range p X ..q X D only if p X ..q X D is valid.,2,3
5572,5569514,"Of course, we will consider that the language (a finite number of strings) associated with each occurrence of each instantiated symbol is represented by a DAG.",23,24
5573,5569514,"The Bottom-Up Walk For this principle algorithm, we assume that for each instantiated clause in the forest, a DAG will be associated with each occurrence of each instantiated symbol.",32,33
5574,5569514,"More precisely, for a given instantiated A 0 -clause, the DAGs associated with the RHS symbol occurrences are composed (see below) to build up DAGs which will be associated with each argument of its LHS predicate.",17,18
5575,5569514,"For each occurrence of a terminal symbol t in the LHS arguments we associate a (new) DAG whose only transition is (p, t, q) where p and q are brand new states with, of course, p < q. 4.",6,7
5576,5569514,The non-empty set {abc} is the final result assigned to the instantiated start symbol.,17,18
5577,5569514,"13 Assume that ρ0[k] = p..q D , that the decoration DAG associated with the k th argument of A0( ρ0) is D ′ p..q = (Q ′ p..q , Σ p..q , δ ′ p..q , p ′ , F ′ p..q ) (we have L(D ′ p..q ) ⊆ L(D p..q )) and that α0[k] = α 1 k Xα 2 k and that i..j D is the instantiation of the symbol X in cρ.",98,99
5578,6446314,"In this paper, we present a method which, in practice, allows to use parsers for languages defined by very large context-free grammars (over a million symbol occurrences).",31,32
5579,6446314,"Indeed, we aim to deal with grammars that have, say, over a million symbol occurrences and several hundred thousands rules.",16,17
5580,6446314,"Preliminaries Context-free grammars A CFG G is a quadruple (N, T, P, S) where N is a non-empty finite set of nonterminal symbols, T is a finite set of terminal symbols, P is a finite set of (context-free rewriting) rules (or productions) and S is a distinguished nonterminal symbol called the axiom.",65,66
5581,6446314,"A nonterminal symbol A is nullable iff it can derive the empty string (i.e., A + ⇒ G ε).",2,3
5582,6446314,A CFG is reduced iff every symbol of every production is a symbol of at least one complete derivation.,6,7
5583,6446314,A CFG is reduced iff every symbol of every production is a symbol of at least one complete derivation.,12,13
5584,6446314,This is represented by the instantiated nonterminal symbol A[i..j].,7,8
5585,6446314,"In fact, each symbol which appears in a complete derivation may be transformed into its instantiated counterpart.",4,5
5586,6446314,"If w ′ w ′′ ∈ T * , we call derivation any sequence of the form (q ′ , w ′ w ′′ ) ⊢ X1 • • • X k • • • Xp in which each symbol X k derives the ter- minal string x k ∈ T * (we have X k * ⇒ G x k and w2 = x1 • • • x k • • • xp), then the instantiated production A[i0..ip] → X1[i0..i1] • • • X k [i k−1 ..i k ] • • • Xp[ip−1..ip] with i0 = |w1| + 1, i1 = i0 + |x1|, . . . ,",40,41
5587,6446314,A symbol is useful (otherwise useless) if it is both productive and reachable.,1,2
5588,6446314,"If for each couple (a, b) ∈ T 2 in which a is a terminal symbol that can terminate (the terminal strings generated by) X and b is a terminal symbol that can lead (the terminal strings generated by) Y , there is no transition on b that can follow a transition on a in the DAG w, it is clear that the production A → • • • XY • • • can be safely erased.",18,19
5589,6446314,"If for each couple (a, b) ∈ T 2 in which a is a terminal symbol that can terminate (the terminal strings generated by) X and b is a terminal symbol that can lead (the terminal strings generated by) Y , there is no transition on b that can follow a transition on a in the DAG w, it is clear that the production A → • • • XY • • • can be safely erased.",35,36
5590,6446314,"If for each couple (a, b ′ ) in which a has the previous definition and b ′ is a terminal symbol that can lead (the terminal strings generated by) Y p , there is no transition on b ′ that can follow a transition on a in the DAG w, the production Y p−1 → α p Y p β p can be erased if it is not valid in another context.",23,24
5591,6446314,"If for each couple (a ′ , b) in which b has the previous definition and a ′ is a terminal symbol that can terminate (the terminal strings generated by) X p , there is no transition on b that can follow a transition on a ′ in the DAG w, the production X p−1 → α p X p β p can be erased if it is not valid in another context.",23,24
5592,6446314,"11 For each production A → αX 0 X 1 • • • X p−1 X p γ in P c and for each symbol pairs (X 0 , X p ) of non- nullable symbols s.t.",24,25
5593,6446314,"Any 11 Consider the source string bcab for which we have a + < c, but not a ≪ c. pair (a, b) of A 1 is such that the terminal symbol a may terminate a phrase of X 0 while the terminal symbol b may lead a phrase of X 1 • • • X p .",35,36
5594,6446314,"Any 11 Consider the source string bcab for which we have a + < c, but not a ≪ c. pair (a, b) of A 1 is such that the terminal symbol a may terminate a phrase of X 0 while the terminal symbol b may lead a phrase of X 1 • • • X p .",47,48
5595,6446314,"Analogously, any pair (a, b) of A 2 is such that the terminal symbol a may terminate a phrase of X 0 X 1 • • • X p−1 while the terminal symbol b may lead a phrase of X p .",17,18
5596,6446314,"Analogously, any pair (a, b) of A 2 is such that the terminal symbol a may terminate a phrase of X 0 X 1 • • • X p−1 while the terminal symbol b may lead a phrase of X p .",36,37
5597,6446314,"In order to check that the input sentence w starts and ends by valid terminal symbols, we augment the adjacent relation with two elements ($, ε, S) and (S, ε, $) where $ is a new terminal symbol which is supposed to start and to end every sentence.",46,47
5598,6446314,"If X is a non-nullable symbol, we compute the set L = {(a, b) | a ←֓ t X σ ↔ Y * Z Z→αU β U ֒→ t b}.",7,8
5599,6446314,"If Y is a non-nullable symbol, we compute the set R = {(a, b) | a ←֓ t U Z→αU β Z * X σ ↔ Y ֒→ t b}.",7,8
5600,1308373,"GRE has traditionally addressed the following question: [G]iven a symbol corresponding to an intended referent, how do we work out the semantic content of a referring expression that uniquely identifies the entity in question? (",11,12
5601,21723178,The input and output vocabularies contain a special out-of-alphabet symbol.,13,14
5602,21723178,The vocabulary of each experiment consists of top 200 most occurring characters in a training set and a special symbol (<UNK>) for unknown characters.,19,20
5603,49657137,The '-' symbol in an input sequence symbolizes that an answer is requested.,4,5
5604,788562,We used S as the starting symbol of the grammar and n is the length of the initial string.,6,7
5605,226262192,"To investigate the effect of SimMatrix representation, we compared it with the representation of the [CLS] symbol (denoted as BERT+[CLS]).",19,20
5606,226262192,"BERT defines its input to begin with the special symbol [CLS], whose representation has been commonly used as a representation of sentence pair (Devlin et al.,",9,10
5607,237558793,Italian and English phones were merged when they shared the same IPA symbol; 7 this dictionary included 48 phones.,12,13
5608,7701494,"However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system.",16,17
5609,13911272,"The isSymbol feature checks whether a token is a symbol (>, <) or not.",9,10
5610,234345291,For example the symbol ◌് also known as virama is used to replace the inherent schwa sound of consonants with ŭ.,3,4
5611,241583431,"For example, in the sentence ""Doctor X says..."", we find that the masked symbol ""X"" represents a person.",18,19
5612,219530995,and S is a non-terminal that is the special start symbol.,12,13
5613,219530995,"Let us introduce a special symbol ⊥ and extend V w G and V w I(G) to any weight function w so that if w is not-well defined for G, then V w G (σ) = ⊥ and likewise for V w I(G) .",5,6
5614,219530995,"V (T k−1 )] ⊗ * Z(T ) where I T k ×d S is the identity tensor for the space S d 1 ×...×d i ×d S , T k ∈ S d 1 ×...×d i , and d s is the dimension assigned for the terminal symbol S. The permutation π is defined as follows: [1, 2, . . . ,",54,55
5615,219530995,"× d n × d S where d S is the dimension assigned to the start symbol S, and d 1 , . . . ,",16,17
5616,243864624,"Multi-modal transformers are required to not only learn to perform symbol grounding, e.g. mapping natural language symbols into visual representations as defined by Harnad (1990 ) and a language model, but also learn to fuse information from two modalities, the nature of which has been an open question in the field (Lu et al.,",12,13
5617,1508909,"For standard recurrent decoders, at each time step t, the hidden state h d t ∈ R k h is calculated using the dependent input symbol y t−1 ∈ R kw and the previous hidden state h d t−1 : h d t = f (y t−1 , h d t−1 ) (1) where f (•) is a recurrent neural network such as vanilla RNN, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) , and Gated Recurrent Unit (GRU) (Cho et al.,",27,28
5618,1025989,"The preference relation has the following semantics: Let (]) and \|/ be two attribute-value structures, P a set of preference rules, and '<pref the preference ordering symbol, (j ) is preferred before y, written (| ) <pref y. if and only if * there exists a preference rule r such that -Minor(r) subsumes ([) and -Major(r) subsumes y and -for all r': if Major(r') subsumes (]) and Minor(r') subsumes (| ) then r <prefrule r', or * there exists a path p in both ([) and y such that The two attribute-value structures represent the two possible readings for the Swedish noun 'vaxelladshus': one singular and one plural reading.",35,36
5619,16450218,The models employed typically contain two sorts of probabilities: transition probabilities and symbol probabilities.,13,14
5620,16450218,"Given an input sequence (e.g. a sentence) and these two sets of probabilities, one may compute the probability of each possible tag assignment by multiplying all the applicable symbol and transition probabilities.",31,32
5621,16450218,A weakness with this approach is that symbol probabilities are difficult to estimate for words.,7,8
5622,16450218,"For these words, there are not enough observations make accurate estimates of symbol probabilities.",13,14
5623,16450218,"Compounding this problem, Samuelsson has shown that symbol probabilities are more significant in improving accuracy than transition probabilities (Samuelsson 1993) .",8,9
5624,16450218,"Together these suggest that, if one is not satisfied with the accuracy of a stochastic part-of-speech tagger, one should attempt to improve symbol probability estimation.",28,29
5625,16450218,XPOST avoids reliance on hand-tagged corpora by using a hidden Markov model (HMM).l The Baum-Welch {ot forward-backward) algorithm enables one to estimate symbol and transition probabilities of an HMM without hand-tagged training data (Baum 1972 ).,31,32
5626,207845897,"A word x i is assigned the label y i = argmax t2T 0 (p t ⇤ ↵ i,t ) if p t ⇤ ↵ i, y i is greater than a small threshold ✏ and it is neither a punctuation symbol nor a stop-word (Figure 2 ).",46,47
5627,207845897,"This results in the model picking out spurious words alongside the actual entity, which necessitates the use of stop-word and symbol removal.",23,24
5628,207845897,"This issue can potentially be mitigated with either a more sophisticated tagger module or a better stopword/symbol removal mechanism, which we leave to future work.",18,19
5629,235624050,"3  It should be noted that these automatically generated informative priors also contain noise: keyboard layouts have spurious mappings because each symbol must be assigned to exactly one key in the QWERTY layout, and Unicode-constrained visual mappings might prevent the model from learning correspondences between punctuation symbols (e.g. Arabic question mark → ?).",23,24
5630,235624050,All remaining filtered characters are replaced with a special UNK symbol in all splits except for the target-side test.,10,11
5631,235624050,symbol stands for UNK.,0,1
5632,16911199,"Considering all this the general form for an edge will be as follows: <C S->E needs cli si->ei, cl2 S2->C2, ..., cln Sn->Cn> Where C is a category, the cli are lists of categories (which will be shown inside square brackets), S and the si are positions in the chart and E and the ei are positions in the chart or the special symbol An edge of this type in the chart means that the parser is trying to find a constituent of category C, spanning from S to E. In order to do so it must then satisfy all the needs listed (cli si->ei).",73,74
5633,218516712,"One original script character can be aligned to two consecutive Latin characters or vice versa: for example, when a phoneme is represented with a single symbol on one side but with a digraph on the other (Figure 1 ), or when a character is omitted on one side but explicitly written on the other (e.g. short vowels not written in unvocalized Arabic but written in transliteration, or the Russian soft sign representing palatalization being often omitted in the romanized version).",27,28
5634,218516712,"Rather than attempting to operationalize the notions of phonetic or visual similarity, we choose to read the likely mappings between symbols off humancompiled resources that use the same underlying principle: phonetic keyboard layouts and visually confusable symbol lists.",38,39
5635,218516712,This led the Unicode Consortium to publish a list of symbols and symbol combinations similar enough to be po- tentially confusing to the human eye (referred to as confusables).,12,13
5636,218516712,6 This list contains not only exact homoglyphs but also strongly homoglyphic pairs such as Cyrillic and Latin lO. We construct a visual prior for the Russian model from all Cyrillic-Latin symbol pairs in the Unicode confusables list.,33,34
5637,218516712,"We attribute the better performance of the model with the phonetic prior to the sparsity and restrictiveness of the visually confusable symbol mappings, or it could be due to the phonetic substitutions being more popular with users.",21,22
5638,218516712,The informative priors used in our experiments are constructed using sets of character mappings compiled for other purposes but using the same underlying principle (phonetic keyboard layouts and the Unicode confusable symbol list).,32,33
5639,174800747,"We define the IPA mark ω IPA = {C | V}, where the symbol C indicates that this arc is part of a transduction rule listed in the consonant section of the Wikipedia English IPA Help page.",16,17
5640,174800747,"The embedding sizes of tokens, including the input symbol, output symbol and states, and marks are all 500.",9,10
5641,174800747,"The embedding sizes of tokens, including the input symbol, output symbol and states, and marks are all 500.",12,13
5642,16288463,"In this paper, we examine several methods for including dynamic, contextually-sensitive binary codes within indirect selection typing methods using a grid with fixed symbol positions.",27,28
5643,16288463,"Finally, we investigate a novel method for displaying the binary codes for each symbol to the user, rather than using cell highlighting, as the means for identifying the required input sequence for the target symbol.",14,15
5644,16288463,"Finally, we investigate a novel method for displaying the binary codes for each symbol to the user, rather than using cell highlighting, as the means for identifying the required input sequence for the target symbol.",37,38
5645,16288463,This sort of indirect selection method amounts to assigning a binary code to every symbol in the grid.,14,15
5646,16288463,The best probabilistic models will take into account what has already been typed to assign probability to each symbol.,18,19
5647,16288463,"To provide another example, the probability of the letter 'u' is not particularly high overall in English (less than 0.02), but if the previously typed symbol is 'q', its probability is very high.",31,32
5648,16288463,"In other words, with row/column scanning methods, the symbol needing the shortest code must move into the upper left-hand corner of the grid.",12,13
5649,16288463,One downside to the variable scanning that results from Huffman scanning is that users cannot anticipate their target symbol's binary code in any given context.,19,20
5650,16288463,"In row/column scanning, the binary code of each symbol is immediately obvious from its location in the grid, hence users can anticipate when they will need to trigger the switch.",11,12
5651,16288463,The last of our alternative methods is a novel approach that displays the code for each symbol at once as a series of dots and dashes underneath the symbol -as used in Morse code -rather than using cell highlighting to prompt the user as in the other conditions.,16,17
5652,16288463,The last of our alternative methods is a novel approach that displays the code for each symbol at once as a series of dots and dashes underneath the symbol -as used in Morse code -rather than using cell highlighting to prompt the user as in the other conditions.,28,29
5653,16288463,"The method requires monitoring of the target cell in the grid and reaction when it is highlighted, since the pattern of highlighting is not predictable from symbol position in the grid, unlike row/column scanning.",27,28
5654,16288463,"The most common language modeling approach is the n-gram model, which estimates probabilities of strings as the product of the conditional probability of each symbol given previous symbols in the string, under a Markov assumption.",27,28
5655,16288463,Note that the probability of the first symbol s 1 is typically conditioned on the fact that it is first in the string.,7,8
5656,16288463,"For example, if the user has typed 'the perso' and is preparing to type the next letter, we estimate P( n | t h e p e r s o ) as well as P( m | t h e p e r s o ) and every other possible next symbol, from a large corpus.",57,58
5657,16288463,Note that smoothing methods mentioned above ensure that every symbol receives non-zero probability mass.,9,10
5658,16288463,"Also note that the space character (represented above as ' ') is a symbol in the model, hence the models take into account context across word boundaries.",15,16
5659,16288463,"For all trials we use a 6×6 grid, as shown in Figures 1 and 2, which includes the 26 characters in the English alphabet, 8 punctuation characters (comma, period, double quote, single quote, dash, dollar sign, colon and semi-colon), a white space delimiter (denoted with underscore) and a delete symbol (denoted with ←).",65,66
5660,16288463,"Probability of the delete symbol ← was taken to be 0.05 in all trials (the same as the probability of an error, see Section 3.2), and all other probabilities derived from the trained n-gram language model.",4,5
5661,16288463,"One key aspect of their method is dealing with errors of omission and commission, i.e., what happens when a subject misses their target symbol.",25,26
5662,16288463,The upshot of this is that users can make an error and still manage to select their intended symbol after the scanning system returns to it.,18,19
5663,16288463,"In Huffman scanning, a subset is highlighted and the user indicates yes or no -yes, the target symbol is in the set; or no, the target symbol is not in the set.",19,20
5664,16288463,"In Huffman scanning, a subset is highlighted and the user indicates yes or no -yes, the target symbol is in the set; or no, the target symbol is not in the set.",30,31
5665,16288463,"If the answer is 'yes' and the set includes exactly one symbol, it is typed.",13,14
5666,16288463,"In such a way, even if the target symbol is in the highlighted set when it is not selected (or vice versa), it is not eliminated from consideration; rather its probability is diminished (by multiplying by 1−p, which in this paper is set to 0.05) and scanning continues.",9,10
5667,16288463,"Eventually the symbol will be highlighted again, much as is the case in row/column scanning.",2,3
5668,16288463,"However, after each bit is entered for the current symbol, rather than multiplying by p and 1−p as detailed in Section 3.2, symbols that have not been selected are eliminated from consideration and will not be highlighted again, i.e., will not be returned to for subsequent selection.",10,11
5669,16288463,"If the user makes a mistake in the input, eliminating the actual target symbol, the only way to fix it is to type another symbol, delete it, and retype the intended symbol.",14,15
5670,16288463,"If the user makes a mistake in the input, eliminating the actual target symbol, the only way to fix it is to type another symbol, delete it, and retype the intended symbol.",26,27
5671,16288463,"If the user makes a mistake in the input, eliminating the actual target symbol, the only way to fix it is to type another symbol, delete it, and retype the intended symbol.",35,36
5672,16288463,"Second, it is possible to type a symbol without ever actively selecting it, if all other symbols in the grid have been eliminated.",8,9
5673,16288463,"For example, if there are two symbols left and the system highlights one symbol, which is rejected, then the other symbol is typed.",14,15
5674,16288463,"For example, if there are two symbols left and the system highlights one symbol, which is rejected, then the other symbol is typed.",23,24
5675,16288463,"Each code includes the dots and dashes required to input that symbol, plus a cursor '|' that indicates how much of the code has already Since these codes are displayed, there is no memorization required to input the target symbol.",11,12
5676,16288463,"Each code includes the dots and dashes required to input that symbol, plus a cursor '|' that indicates how much of the code has already Since these codes are displayed, there is no memorization required to input the target symbol.",43,44
5677,16288463,"Like row/column scanning, once the target symbol has been found in the grid, the input sequence is known in entirety by the user, which can facilitate planning of sequences of actions rather than simply reacting to updates in the interface.",9,10
5678,16288463,"Symbols that are typed in error -as shown in Figure 7 -must be repaired by selecting the delete symbol (←) to delete the incorrect symbol, followed by the correct symbol.",18,19
5679,16288463,"Symbols that are typed in error -as shown in Figure 7 -must be repaired by selecting the delete symbol (←) to delete the incorrect symbol, followed by the correct symbol.",26,27
5680,16288463,"Symbols that are typed in error -as shown in Figure 7 -must be repaired by selecting the delete symbol (←) to delete the incorrect symbol, followed by the correct symbol.",32,33
5681,16288463,"The error rate is higher than in the synchronous condition 1, but there is less scanning past the target symbol.",20,21
5682,16288463,"Among the alternatives being con-sidered are: requiring all codes to have a short press (confirmation) bit as the last bit of the code; having a ""reset"" symbol or gesture; and recalculating codes after some number of bits, greater than one.",34,35
5683,10508462,"These sorts of vertical face representations via ASCII punctuation sequences are widely used in European languages, but in Asian informal text genres another class of emoticons is popular, involving a broader symbol set and with a horizontal facial orientation.",33,34
5684,10508462,Accurate detection of these tokens -and other common sequences of extra-linguistic symbol sequences -is important for normalization of social media text for downstream applications.,13,14
5685,10508462,"Note that, in this case, the ""face"" is followed by a small amount of hiragana, and that the message concludes with a dingbat in the form of a ""heart"" symbol.",36,37
5686,10508462,"We also calculate the same score without flipping the image vertically, which is also used to score possible symbol matches, as detailed in Section 4.3.",19,20
5687,10508462,"Our candidate emoticons, then, are this extensive list of mainly non-linguistic symbol sequences.",15,16
5688,10508462,"Since there are two emitted symbol classes (linguistic L and non-linguistic N ), each HMM state must have two emission probabilities, one for its dominant symbol class (L in A and N in @) and one for the other symbol class.",5,6
5689,10508462,"Since there are two emitted symbol classes (linguistic L and non-linguistic N ), each HMM state must have two emission probabilities, one for its dominant symbol class (L in A and N in @) and one for the other symbol class.",30,31
5690,10508462,"Since there are two emitted symbol classes (linguistic L and non-linguistic N ), each HMM state must have two emission probabilities, one for its dominant symbol class (L in A and N in @) and one for the other symbol class.",46,47
5691,10508462,"S 1 is the graphical similarity of the first symbol with the vertical mirror image of the second symbol, calculated as presented in Section 4.1.",9,10
5692,10508462,"S 1 is the graphical similarity of the first symbol with the vertical mirror image of the second symbol, calculated as presented in Section 4.1.",18,19
5693,10508462,"S 2 is the graphical similarity of the first symbol with the second symbol (not vertically flipped), which gives high scores to the same or similar symbols.",9,10
5694,10508462,"S 2 is the graphical similarity of the first symbol with the second symbol (not vertically flipped), which gives high scores to the same or similar symbols.",13,14
5695,10508462,"All possible rules for a given sequence are instantiated using these templates, by placing each symbol in the a slot with all subsequent symbols in the b slot and scoring, as well as creating all rules with just a alone for that symbol.",16,17
5696,10508462,"All possible rules for a given sequence are instantiated using these templates, by placing each symbol in the a slot with all subsequent symbols in the b slot and scoring, as well as creating all rules with just a alone for that symbol.",44,45
5697,10508462,"For the second symbol 'o', the algorithm would evaluate the similarity between 'o' and each of the four symbols to its right , o, ; and ).",3,4
5698,10508462,"Similarly, the single symbol on the right-hand side is for the NM (nose/mouth) region.",4,5
5699,17133978,"For each basic period t i , there is a unique corresponding symbol a i in the alphabet Σ of calendar XREs, as illustrated in Figure 1 .",12,13
5700,17133978,a n corresponds to the finite subset of the timeline; we denote it as T. timeline: A single calendar XRE defines a regular language (a set of symbol strings) corresponding to a set of possibly disconnected periods of time.,30,31
5701,17133978,"Such a string contains a symbol a i followed by a j , where i ≥ j. It is not meaningful as a representation of time, as a period of time may not be followed by the same or a preceding period.",5,6
5702,6946103,The feature fires when all (n+2) parts fire in the instance (reflected by the ∧ symbol in φ).,18,19
5703,39130572,2016) proposed to employ a sequence-to-sequence model by introducing a fictitious symbol at the end of an utterance of which tag represents the corresponding domain and intent.,16,17
5704,8060954,"3n1n2(lex(n1)i\lex(n2)J\n1-< n2i\lab{n1,john)J\ lab(n2, waiks) A Vn(lex(n) -t (n = n1 V n = n2))) 2 (3) \ln 1 (lab(n1,john) -t 3n3 (/ab(n3,11p) i\n3 <ln1/\ a+(n3) =n1 Au(n3) = John/\ Vn{a+(n) = n1 -t (n = n3 V n = n1))/\ Vn(a-(n) = n 1 -t n = n1))) ( = n2 -t (n = n4 V n = n1 V n = n2))J\ a-(ns) = a-(n6) =n2A Vn(a-{n) = n2 -t (n = ns V n = 116 V n = n2))/\ u(n4) = q(n 6 )(q(ns)) A q(n1) = >.v.wa/k v)) The function symbol a+ used in these descriptions positively anchors nodes to lexical nodes, a-negatively anchors nodes and q gives a node its semantic value. /'---....",135,136
5705,1850309,A symbol µ as a notation for model parameters related to vertex features.,1,2
5706,1850309,5) A symbol λ as a notation for model parameters related to edge features.,3,4
5707,51868684,"In representation of the rules, we follow the general linguistic convention which is to put round brackets on optional elements and '+' symbol for multiple possible occurrences of a word or phrase.",25,26
5708,208245190,"If a space is added, then to preserve word alignment we replace the corresponding space in the output by a special symbol (#).",22,23
5709,208245190,"In inference mode (see Section 5.4), this symbol will be replaced by a space.",10,11
5710,208245190,A special extra symbol ($) is added to mark that a spurious space was added and should be eventually deleted again when the model is used in inference mode.,3,4
5711,208245190,"The models generate characters starting from the start symbol (<) and stop at the end symbol (>) or at a predefined sequence length given as a hyper-parameter, whichever comes first.",8,9
5712,208245190,"The models generate characters starting from the start symbol (<) and stop at the end symbol (>) or at a predefined sequence length given as a hyper-parameter, whichever comes first.",17,18
5713,245838290,"Another subtype of nummod, nummod:entity, appears to be used only in the Russian treebanks, especially in relation to the symbol '№'.",24,25
5714,5569742,<eos> is a special symbol used to separate question and answer.,6,7
5715,232404543,"2020) provide language labels for each token in the dataset -HIN, ENG, 0, where 0 usually corresponds to a symbol or other special characters in a Tweet.",23,24
5716,2853888,"|s) = 0.2 denotes the probability of replacing a punctuation symbol s (replacing s by none denotes deletion); and for a real word (not a punctuation symbol) w, P (none|w) = P (, |w) = P (.|w) = P (?",11,12
5717,2853888,"|s) = 0.2 denotes the probability of replacing a punctuation symbol s (replacing s by none denotes deletion); and for a real word (not a punctuation symbol) w, P (none|w) = P (, |w) = P (.|w) = P (?",31,32
5718,2853888,|w) = 0.2 denotes the probability of inserting a punctuation symbol after w (inserting none after w denotes no insertion).,11,12
5719,24316001,"This is how the first rule used in the translation procedure looks like: All rules have this common form: The left-hand side of the translation symbol, =>, is a structural description.",29,30
5720,235352900,"2018) , reference resolution (Kennington and Schlangen, 2015) , symbol grounding (Kameko et al.,",13,14
5721,237291795,"The tokenizer then converts each such span into a discrete symbol (a token) with no internal structure, effectively discarding the token's orthographic information.",10,11
5722,235097202,The selected Gold hashtags are the keywords reported in Table 1 that contains the * symbol.,15,16
5723,235097202,"The symbol is substituted with the corresponding stance (""sì"" or ""no"").",1,2
5724,12861120,8 A convention on Twitter is to refer to other users using the @ symbol followed by their unique username.,14,15
5725,907564,This semantic abstraction captures the underlying semantic proximity by categorizing multitudinous surface-form proper names into one representing symbol.,19,20
5726,248779890,"In an HRG production 𝐴 → 𝑅, each external node of 𝑅 is mapped to a node of the same colour in 𝐴. H5 rewrites the start symbol to a hyperedge that connects to one node.",28,29
5727,248779890,"Line 22 records the successful parses, i.e. the items that cover all DMRS nodes and whose c-hyperedge label equals the start symbol 𝑆 of 𝐺. Finally, the successful parse with the highest log probability is returned.",24,25
5728,45694917,"The symbols of the code are readily divided into letters and non-letter symbol, the latter being normal symbols of written language (space, case shifts, comma, point etc.)",14,15
5729,45694917,"In the first computational phase the text is sorted into words and separators between words; a word is defined as an unbroken sequence of letter symbols, with the exception that a lower case symbol is allowed if the word begins with a capital.",35,36
5730,14061182,We marked all incoherent topics with a special symbol ∅. This process took one of the authors about 4 hours to complete.,8,9
5731,238259938,"Then these sentences are concatenated with a special symbol [SSEP] to form the final target sequence y, containing all the sentiment quads for the given sentence.",8,9
5732,238259938,"Specifically, instead of mapping the label to the natural language form with the projection functions introduced in Sec 3.2, we map each label to a special symbol, similar as the number index in the classification-type models, for representing each label class.",28,29
5733,238259938,"For example, we map the positive class as SP1; (2) w/o aspect category semantics: P c (c j ) = ACj where we project the aspect category c j to a symbol with its index j 3 .",37,38
5734,238259938,"Comparing the ablations on the sentiment polarity and aspect category, the model suffers more when the aspect category is projected to an indexed symbol.",24,25
5735,208268343,The input to the model is an image and the start symbol < s > of a description and the output is produced by the language model decoder.,11,12
5736,243865296,"As depicted in the upper-left portion of Figure 1 , we first mark each aspect term in the sentence with a special symbol (e.g., different brackets like ""{}"" and ""[]""), before feeding it into the translation system.",24,25
5737,243865296,"If there are multiple aspect terms in one sentence, we mark them orderly with the predefined special symbol list.",18,19
5738,243865296,"Formally, suppose in the source sentence, the i-th to j-th tokens x S i:j are marked with the special symbol t. After our label projection, we will label the span in the translated sentence x T marked by the same special symbol t with the sentiment polarity of x S i:j .",27,28
5739,243865296,"Formally, suppose in the source sentence, the i-th to j-th tokens x S i:j are marked with the special symbol t. After our label projection, we will label the span in the translated sentence x T marked by the same special symbol t with the sentiment polarity of x S i:j .",50,51
5740,53083098,"In the standard language model setting, we compute h 1 using a start-of-sentence symbol for w 0 (""<s>""), and a special starting state h 0 (usually set to be a vector of zeros 0).",18,19
5741,226283534,"Note that L includes blank, which is a special symbol for CTC labelled for the data in which no labels are aligned.",10,11
5742,23204325,"The results with symbol"" "" are retrieved from the original papers, and those starred ( * ) one are from Dong et al. (",3,4
5743,21720529,"For example, given a sentence t = (t 1 ,t 2 ,...,t n ), in which the symbol • denotes a concatenation operation, the BiLSTM function can be represented as: BiLSTM(t 1:n , i) = LSTM Forward (t 1:i ) • LSTM Backward (t i:n ).",24,25
5744,2193818,"Our Emoji Lexicon uses abbreviated versions of the Unicode ""names,"" or informative glosses, that describe the symbol in words.",20,21
5745,235097243,Standard autoregressive language models perform only polynomial-time computation to compute the probability of the next symbol.,17,18
5746,235097243,"While this is attractive, it means they cannot model distributions whose next-symbol probability is hard to compute.",15,16
5747,235097243,"If the prefix x has positive prefix probability, then it admits a local conditional probability ( | x) ( x )/ ( x) for each symbol ∈ , where the denominator is interpreted as a local normalizing constant.",29,30
5748,235097243,"This is the conditional probability that if a random string starts with the prefix x, the next symbol is .",18,19
5749,235097243,"There is also a probability ($ | x) 1 − ∈ ( | x) = ˜ ( x)/ ( x) ≥ 0 that the string ends immediately after x; the special symbol $ ∉ represents ""end of string.""",36,37
5750,235097243,"While the computable weighted languages allow any computable function as ˜ , most architectures for defining weighted languages (e.g., RNNs or Transformers) do only a bounded or linear amount of work per input symbol.",36,37
5751,235097243,Most such architectures again do only a bounded or linear amount of work per input symbol.,15,16
5752,235097243,"2020) , where a neural autoregressive language model generates an analysis of the prefix x via latent intermediate symbols before predicting the next output symbol.",25,26
5753,235097243,"Here the marginal distribution of the next observed symbol can require superpolynomial time to compute (if #P ≠ FP, which follows from NP P/poly).",8,9
5754,235097243,"Theorem 1 could likewise be evaded by other autoregressive approaches that invest superpolynomial computation in predicting the next symbol (Graves, 2016) .",18,19
5755,235097243,"The alphabet of this language contains at least the symbols {0, 1, #}, where # is a separator symbol, and any other symbols needed to encode as .",23,24
5756,235097243,"We can think of this distribution as describing an efficient procedure for randomly generating a string from left to right so that the procedure generates the th symbol in time (poly( )), terminates with probability 1, has positive probability of producing any string in , and has zero probability of producing any string not in .",27,28
5757,235097243,"is ELN because there exists a Turing machine that computes from input x -in time (poly(| x|))the probability that the next symbol generated after the prefix x would be , under the above procedure.",22,23
5758,235097243,"Our presentation here makes use of an infinite alphabet that includes symbols such as and ¬ for all ∈ N >0 , as well as symbols such as 0, 1, ∧, ∨. We implicitly invoke some prefix-free encoding scheme to translate each symbol into a fixed string over the finite alphabet .",48,49
5759,235097243,"The computation (ẑ ) proceeds by simulating the sequence of choices in the above procedure that would be required to generate ẑ, and then returning the probability that the procedure would generate symbol next.",34,35
5760,246904992,"The Czech language rules require the use of lower quotes symbol (""), the reference uses straight upper quotes (""), but SENTBASE uses often (25 occurrences in 15 segments in tst-COMMON) two comma symbols (, ,).",10,11
5761,21726941,"Deleting each freestanding punctuation symbol unless the speech recognizer for that language supports inputting the symbol by saying its name (""hello comma Doctor Nduom comma how are you question mark"").",4,5
5762,21726941,"Deleting each freestanding punctuation symbol unless the speech recognizer for that language supports inputting the symbol by saying its name (""hello comma Doctor Nduom comma how are you question mark"").",15,16
5763,21726941,"If a new currency symbol is added, like the Indian rupee sign was in 2010 (Everson, 2010) , we want to notice and expand the currency character lists used by our Thrax grammars.",4,5
5764,44096233,"In AI research, reasoning has been strongly associated with logic and symbol manipulation, as epitomized by work in automated theorem proving (Fitting, 2012) .",12,13
5765,6157443,"We introduce polyglot language models, recurrent neural network models trained to predict symbol sequences in many different languages using shared representations of symbols and conditioning on typological information about the language to be predicted.",13,14
5766,6157443,We apply these to the problem of modeling phone sequences-a domain in which universal symbol inventories and cross-linguistically shared feature representations are a natural fit.,16,17
5767,6157443,"In addition, we maintain V with a special symbol for each language (e.g., φ english , φ arabic ).",9,10
5768,6157443,"Language symbol vectors are parameters in the new lookup table X ∈ R d×|#langs| (e.g., x english , x arabic ).",1,2
5769,3021306,"For example JAVA is the stock symbol for Sun Microsystems, and ""Ginger Spice"" is a stage name of Geri Halliwell.",6,7
5770,11525632,We used a special padding symbol when the actual context was shorter than the window.,5,6
5771,102351463,"2017) including lowercasing, tokenizing and replacing digits with digit symbol.",11,12
5772,16603726,Each of these states emits only a single symbol.,8,9
5773,227904589,"Following each information source symbol, the correctness of the retrieved response is given.",4,5
5774,44979057,has led to a bifurcation of com putational research which both proceed from Newell's (1980; New ell & Simon 1976 ) account of a physical symbol system.,28,29
5775,44979057,A physical symbol system is a system which subscribes to the laws of physics and which at any given time contains a set of struc-125 Proceedings of NODALIDA 1987 tured objects called 'expressions' or 'symbol structures'.,2,3
5776,44979057,A physical symbol system is a system which subscribes to the laws of physics and which at any given time contains a set of struc-125 Proceedings of NODALIDA 1987 tured objects called 'expressions' or 'symbol structures'.,37,38
5777,44979057,"In addition, a physical symbol system comprises a set of process es that may create, change, destroy, and reproduce expressions.",5,6
5778,44979057,"A physical symbol system, then, is a machine which produces a con tinuous, but continually changing, stream of symbol structures.",2,3
5779,44979057,"A physical symbol system, then, is a machine which produces a con tinuous, but continually changing, stream of symbol structures.",22,23
5780,44979057,"The precondition for the proper functioning of a physical symbol system is the notion of i n t e r p r e t a t i o n , as defined in computer science (Newell 1980:158) .",9,10
5781,44979057,He formulates the following hy pothesis : ( 6) The P h y s i c a l Sym bol S y s te m H y p o t h e s i s The necessary and sufficient conditions for a physical system to exhibit general intelligent action is that it be a physic al symbol system.,57,58
5782,44979057,"Newell, 1980:170 'Necessary' and 'sufficient' in this connection mean, respective ly, that a system displaying what we would be prepared to call intelligent behaviour, will always upon closer inspection turn out to be a physical symbol system; and a physical symbol system of sufficient size will always be amenable to organization in such a way that it will display behaviour that we would be pre pared to call intelligent.",43,44
5783,44979057,"Newell, 1980:170 'Necessary' and 'sufficient' in this connection mean, respective ly, that a system displaying what we would be prepared to call intelligent behaviour, will always upon closer inspection turn out to be a physical symbol system; and a physical symbol system of sufficient size will always be amenable to organization in such a way that it will display behaviour that we would be pre pared to call intelligent.",49,50
5784,44979057,"-127 -Clearly, interest in the properties of the symbol differs accord ing to which of the two directions one follows.",9,10
5785,44979057,"In these terms the semiotic process can be seen as a complex series 127 Proceedings of NODALIDA 1987 of steps whereby a particular physical object undergoes various internal processes (Newell calls them 'symbolic')/ whereby they are turned into meaningful structures, le structures that det ermine the symbol system's subsequent behaviour.",49,50
5786,44979057,-128 - D i g r e s s i o n : N o t a t i o n a l s y s t e m s The fundamental property of a symbol is that it is an object man ifest to the senses.,35,36
5787,44979057,But -as Newell made clear -a symbol may be of a complex internal structure.,6,7
5788,44979057,"This structure will in some cas es be amenable to description by means of a notational system, v i z those cases where the atomic parts of every symbol in the scheme constitute a set that satisfies the five requirements on notational systems formulated by Nelson Goodman ( 1976 ): ( 7)(a) S y n t a c t i c d i s c r e t e n e s s : the decision whether some arbitra ry inscription belongs to a particular character and not another is deterministic;  The interesting thing about these claims in our connection is that the alphabet satisfies them, whereas larger linguistic enti ties as a rule do not. (",29,30
5789,44979057,"Johnson-Laird (1983:2-3) The symbol has become a mental code with psychological reality, a view which harks back to P e i r c e 's notion of 'interpretant'.",9,10
5790,44979057,"Rather than solve it, cognitive science believes to have d i s s o l v e d it -with reference to the physical embodiment of symbol systems and the representation hierarchy (Newell 1980 (Newell :172-75, 1982;; Haugeland 1982; Johnson-Laird 1983:399ff; Pylyshyn 1984; but cf.",27,28
5791,1296471,"The spelling rules are specified as a pair (lexical symbol : surface symbol), and the context in which that pair is acceptable.",10,11
5792,1296471,"The spelling rules are specified as a pair (lexical symbol : surface symbol), and the context in which that pair is acceptable.",13,14
5793,1296471,"A lexical symbol can be one of three types: a lexical character from the declared lexical alphabet; a lexical set, declared over a range of lexical characters; or the symbol 0 (zero) which represents the null symbol.",2,3
5794,1296471,"A lexical symbol can be one of three types: a lexical character from the declared lexical alphabet; a lexical set, declared over a range of lexical characters; or the symbol 0 (zero) which represents the null symbol.",33,34
5795,1296471,"A lexical symbol can be one of three types: a lexical character from the declared lexical alphabet; a lexical set, declared over a range of lexical characters; or the symbol 0 (zero) which represents the null symbol.",42,43
5796,1296471,Similarly there are three possibilities for the surface symbol.,8,9
5797,1296471,"The ""---"" can be thought of as marking the position of the symbol pair + :e. Within our formalism there are no built-in conventions concerning morpheme boundaries.",14,15
5798,1296471,"Another example in our English description is the ""E-deletion"" rule: E-Deletion: e:0 <=> =:C2 ---< +:0 V:=> or <C:C V:V> ---<+:0 e:e> or {g:g c:c} ---< +:0 {e:e i:i}> or 1:0---+:0 or c:c ---< +:0 a:0 t:t b:b > where V, C and C2 represent particular subsets of the alphabets, and the = sign matches any symbol (roughly speaking).",107,108
5799,1296471,"If separate rules were given for each alternative left and right context there would be the undesirable effect of each one blocking the other, since rules are treated as conjoined; that is, all rules must match for a sequence of symbol pairs to be acceptable.",43,44
5800,1296471,A pair consisting of a lexical symbol and a surface symbol is a feasible pair if either it is a concrete pair (see below) or consists of two identical symbols from the intersection of the lexical and surface alphabets.,6,7
5801,1296471,A pair consisting of a lexical symbol and a surface symbol is a feasible pair if either it is a concrete pair (see below) or consists of two identical symbols from the intersection of the lexical and surface alphabets.,10,11
5802,1296471,"Concrete pairs are those pairs appearing in the rules (assuming any ""where"" clauses are expanded into explicit enumeration) which are made up of characters in the alphabets or null symbol only (i.e. containing no sets).",33,34
5803,1296471,"That is, in the same way that a traditional context-free grammar has a single distinguished symbol which is used to define complete derivations, our morphological model has a distinguished category.",18,19
5804,1296471,"A entry skeleton is of the same general form as a lexical entry, but various parts of it may contain the ampersand symbol (&), to mean ""the same as in the original entry"", or variables which have appeared in the pre-condition (and hence would have been bound in the matching process).",23,24
5805,1296471,"The null symbol (0) is a part of the formalism, and hence is not regarded as part of either alphabet (but may occur in rules anywhere that a normal alphabet symbol might occur).",2,3
5806,1296471,"The null symbol (0) is a part of the formalism, and hence is not regarded as part of either alphabet (but may occur in rules anywhere that a normal alphabet symbol might occur).",34,35
5807,1296471,+:0 a morpheme boundary symbol may be deleted on the surface.,5,6
5808,2305711,The or construct states that • can correspond to 0 (the null symbol) when (and only when) in eir3urr of the two given contexts.,13,14
5809,10702716,SAOB ['abe^.bara] • förkortad beteckning: symbol som ersåier flera skrivtecken.,9,10
5810,236459874,Label smoothing [symbol] is set to the default value of 0.1.,3,4
5811,12974290,We assume the grammar has only one start symbol and rebuild the parse tree from the state that is labelled with the start symbol.,8,9
5812,12974290,We assume the grammar has only one start symbol and rebuild the parse tree from the state that is labelled with the start symbol.,23,24
5813,198942923,"In figure 2 , the symbol for house combined with the symbol for medical form the word hospital, clinic.",5,6
5814,198942923,"In figure 2 , the symbol for house combined with the symbol for medical form the word hospital, clinic.",11,12
5815,198942923,The fundamental rules concede that the same word should be able to be built with the symbol female replacing person (non-gendered).,16,17
5816,198942923,"Second, we used the encoding scheme found in The Official Blissymbolics Dictionary, where each Bliss symbol, word or character, is given a unique 4-5 digit numeric ID.",17,18
5817,198942923,"The current encoding scheme assigns each symbol with a unique ID, however modified symbols do not have a unique ID.",6,7
5818,248779883,"As one navigates in that direction, the probability that the target symbol is found on that end of the list grows, and the regions for those letters grow accordingly.",12,13
5819,248779883,"Let Σ be a vocabulary of characters, including a special end-of-string symbol.",16,17
5820,248779883,"In addition, the dynamic model also provides a concrete implementation of dynamic updates of symbol counts (Update).",15,16
5821,201624024,"The output may be ambiguous, for example the input string şç produces both şç (analysis, preceding transliteration) and a special symbol щ (also analysis) standing for Cyrillic щ (surface form).",24,25
5822,201624024,"First the rules for vowel harmony and other phonological phenomena were removed from the twol transducer that implements the normative orthography, leaving only unconstrained symbol mappings.",25,26
5823,248299749,"The source coding theorem for symbol codes states that the entropy of a character distribution is the lower bound on the expected number of bits required to encode a character (MacKay, 2002) .",5,6
5824,15302622,"Unconstrained: an analyzer where there are no constraints on variables, except that they must be at least one symbol long, i.e. match Σ + .",20,21
5825,204848126,"As Table 2 shows, besides a beginning-of-word symbol (ˆ), we also add an end-of-word symbol (EOW) ($) to the input.",12,13
5826,204848126,"As Table 2 shows, besides a beginning-of-word symbol (ˆ), we also add an end-of-word symbol (EOW) ($) to the input.",26,27
5827,204848126,"We expect that this addition will improve the performance of the model, as it helps to supervise the training signal more clearly by restricting the distribution of s and + as compounding elements to occur only after the EOW symbol.",40,41
5828,39556465,"Verbs that have a direct, non-finite dependent (a dependent with the NDT dependency relation NDT explicitly marks headings using the | symbol, tagged as a clause boundary (clb) along with information in the morphological features (<overskrift>).",25,26
5829,39556465,These are converted to the UD symbol tag SYM.,6,7
5830,15843926,"A symbol a ∈ Σ leaf by itself is a tree, and if A ∈ Σ intern and t 1 , . . . ,",1,2
5831,15843926,We will use the symbol t for trees and symbol τ for sequences of trees.,4,5
5832,15843926,We will use the symbol t for trees and symbol τ for sequences of trees.,9,10
5833,15843926,"As usual, a context-free grammar (CFG) is represented by a 4-tuple (Σ, N, S, R), where Σ and N are two disjoint finite sets of terminals and nonterminals, respectively, S ∈ N is the start symbol, and R is a finite set of rules, each of the form A → α, where A ∈ N and α ∈ (Σ ∪ N ) * .",50,51
5834,15843926,By grammar symbol we mean a terminal or nonterminal.,2,3
5835,15843926,"A latent-variable CFG (L-CFG) differs from a CFG in that each nonterminal, except the start symbol S † , is of the composite form A ( ) , where A is a surface symbol and is a latent symbol.",22,23
5836,15843926,"A latent-variable CFG (L-CFG) differs from a CFG in that each nonterminal, except the start symbol S † , is of the composite form A ( ) , where A is a surface symbol and is a latent symbol.",40,41
5837,15843926,"A latent-variable CFG (L-CFG) differs from a CFG in that each nonterminal, except the start symbol S † , is of the composite form A ( ) , where A is a surface symbol and is a latent symbol.",45,46
5838,15843926,"The symbol $ is the end-of-sentence marker, which we will need for technical reasons.",1,2
5839,15843926,We will use a bar-symbol for this mark.,6,7
5840,15843926,"The terminals of G M are those of G. The nonterminals of G M are S and composite nonterminals of the form A ( ) , where A is a nonterminal from G and the latent symbol is a triple There are three types of rules in G M .",36,37
5841,15843926,"This is easily justified, as initially the stack is empty, and the first symbol after an occurrence of S must be $.",15,16
5842,15843926,"We can also add the function first intern, which returns the internal symbol just above the leftmost leaf.",13,14
5843,46894722,"Decoder Decoder takes input context vector C from the encoder, and computes the hidden state at time t as, s t = f dec (E y (y t-1 ), s t-1 , c t ) Subsequently, a parametric function out k returns the conditional probability using the next target symbol being k. Here, the concept of teacher forcing is utilized, the strategy of feeding output of the model from a prior time-step as input.",55,56
5844,18753353,"Standard n-gram language models can be presented in the following well-known backoff formulation: P(w | h) = P(w | h) if c(hw) > 0 α(h) P(w | h ) otherwise (1) where w is the word (or symbol) being predicted based on the previous history h, and h is the longest proper suffix of h (or if h is a single word/symbol).",49,50
5845,18753353,"Standard n-gram language models can be presented in the following well-known backoff formulation: P(w | h) = P(w | h) if c(hw) > 0 α(h) P(w | h ) otherwise (1) where w is the word (or symbol) being predicted based on the previous history h, and h is the longest proper suffix of h (or if h is a single word/symbol).",78,79
5846,18753353,The model's symbol table maps from sym-bols in the model to unique indices that label the arcs in the WFST.,3,4
5847,18753353,We use indices from this symbol table to define a total order < V on our vocabulary augmented with start-of-string token which is assigned index 0.,5,6
5848,18753353,"The colexicographic interval [x, y) then denotes the set of sequences z such that x ≤ z < y. For example, assuming symbol indices the=1 and end=2, the colexicographic ordering of the states in Figure 1 is: Colex.",26,27
5849,18753353,"For the BWB trials, no symbol or n-gram frequency cutoffs were used, but for the search queries, as part of the preprocessing and counting, we selected the 4 million most common words from the collection to include in the vocabulary (all others mapped to an out-of-vocabulary token) and limited 4-grams and 5-grams to those occurring at least twice or 4 times, respectively.",6,7
5850,239890006,"2018) : If the resulting hypothesis from diverse beam-search starts with a different symbol, they are presented as new results.",16,17
5851,10812676,"The user may specify how the arcs of the resulting FSA are to be labeled; by default, each arc in the FSA corresponds to a byte in the input string, but the string may also be interpreted as a UTF-8-encoded string-in which case each arc label corresponds to a codepoint in the Unicode Basic Multilingual Plane-or according to a user-provided symbol table.",71,72
5852,10812676,"As in Thrax, a string enclosed in square brackets are interpreted as a single generated symbol rather than as sequences of bytes or codepoints.",16,17
5853,5948172,"A bimorphism B = (g, A 1 , Λ, A 2 ) consists of a regular tree grammar g generating trees over some ranked alphabet Σ, two Σ-algebras A 1 and A 2 which interpret each symbol in Σ as an HR operation (thus evaluating every tree to two hypergraphs), and a Σ-indexed family Λ of alignments between the two interpretations of each σ ∈ Σ. The semantics of B is a set of bigraphs.",42,43
5854,5948172,"Terms, regular tree grammars, and algebras A ranked alphabet is a pair (Σ, rk) where Σ is an alphabet and rk : Σ → N is a mapping associating a rank with each symbol of Σ. Often we just write Σ instead of (Σ, rk).",38,39
5855,5948172,For every We assume that each variable e i is labeled by a distinguished symbol ⊥ ∈ Σ and depict e i by e i instead of ⊥ .,14,15
5856,5948172,"For each occurrence of a symbol γ ∈ Γ k in s, there is a γ-labeled edge with 2k + 2 tentacles in s .",5,6
5857,5948172,"In the first phase, for each symbol σ in Σ k , we define a graph H σ IO of type syn σ A , inh σ A as shown in Fig.",7,8
5858,5948172,"A (Σ, Γ)-HR algebra A is a (Σ, Γ)-attribute grammar algebra ((Σ, Γ)-AG algebra), if each symbol σ in Σ is interpreted as described above.",25,26
5859,235742735,"2017) is riddled with collections of hard questions that require external knowledge to answer (e.g., ""What is the symbol on the hood often associated with?"")",22,23
5860,235742735,"another cluster requires external knowledge to answer (""What is the symbol on the hood often associated with?"").",12,13
5861,239890018,They are indistinguishable from a standard symbol character by the human eye.,6,7
5862,239890018,"We choose to handle images as traditional symbol characters, so that they can be exploited by the structuration process, in particular by the list identification module.",7,8
5863,239890018,We infer the symbol by identifying multiple left-aligned lines introduced by the same single-character token.,3,4
5864,239890018,"Indeed, the end of list items are computed while computing paragraph structures: a list item ends when the next list item starts (i.e., same bullet symbol, same indentation) or when less indented text objects starts.",29,30
5865,16582463,"These constraints can have the following three forms: •<attribute> = w •<attribute> = [] •<attribute> = ε The symbol w denotes some string, [] denotes an empty dag, and ε denotes that no constraint is placed on the value (i.e., one only states that the attribute in question should be present).",23,24
5866,220056969,"URL's, emojis like ':)' and the symbol '#' are removed using regex.",10,11
5867,220056969,"Phase 2 involves checking that all the tweets have been classified and correct symbol that is, either '1' or '0' have been used for it.",13,14
5868,8419991,The pair alphabet of T is the set of input and output symbol pairs related by T .,12,13
5869,8419991,An identity pair relates a symbol to itself.,5,6
5870,226283986,Removal of the hash symbol in hashtags.,4,5
5871,226283986,"Removal of url symbol -""HTTPURL"". •",3,4
5872,16438316,"Proceedings of NODALIDA 1999 Tree-DOP recom bines fragments starting from the leftmost non-terminal frontier node, and replaces this with a fragment having the same root symbol.",30,31
5873,8110552,"Boxer gives two options: to interpret the copula as were it an ordinary transitive verb (--copula false), or by introducing an equality symbol between two entities (--copula true).",26,27
5874,8110552,"With --mwe no a compound name such as Barack Obama is represented by two naming conditions (with the non-logical symbols barack and obama), and with --mwe yes as a single naming condition (with symbol barack~obama).",39,40
5875,202723595,Phonological Feature Trees -Acquisition Phonological feature trees are data structures that define a set of user specified symbol-tophonological feature attribute mappings.,17,18
5876,202723595,They are repositories of phonological information explicitly linked to particular symbol sets.,10,11
5877,202723595,Their creation was driven by the need to have a structured symbol-feature inventory that could be used as a knowledge source for phonological document generation and mapping.,11,12
5878,202723595,Feature profiles are multilingual resources as they are intended to define a full inventory of phonological feature information for a complete symbol set across a number of languages.,21,22
5879,202723595,"Thus, from an abstract feature set, language-specific symbol-to-feature mappings are constructed.",11,12
5880,202723595,"The data structure described here not only encodes associations between symbols and one particular feature set, it is intended to annotate mappings between a symbol set and multiple feature sets.",25,26
5881,202723595,"A finite-state automaton modelling the legal combinations of sounds for a supplied domain -a phonotactic automaton -is partially learned (phase 1); this structure is then used to automatically generate the interface for the feature definition module (phase 2); once feature-to-symbol associations have been created in phase 2, the optimisation phase generalises over the feature set, supplying additional information regarding logical relations among individual as well as sets of features (phase 3); finally the information contained within the phonological feature trees is used by the lexical generation module (phase 4).",51,52
5882,202723595,The feature definition module takes this phonotactic automaton as its input and extracts every unique occurrence of a phonological symbol from the automaton and dynamically creates a graphical user environment that allows the user to define feature associations for those symbols.,19,20
5883,202723595,"Users then simply create associations by pointing and clicking, first at the symbol and then at the features that are to be associated with it.",13,14
5884,202723595,"If the symbol-to-feature mappings are manipulated, updates to our knowledge base are carried out using XSL, the stylesheet language for transforming XML documents.",2,3
5885,202723595,"Given a set of alternative features, instead of creating a separate feature tree for that particular phonological data set, it seems a more efficient use of the information to store them within a superstructure that models symbol-feature associations over a number of feature sets.",38,39
5886,15684367,"For training the hidden event language model, we took the same vocabulary as was used for training the main language model, added the compound connector symbol to it, and estimated a trigram model over the union of the subcorpora, using a cutoff value of 2 and Kneser-Ney smoothing.",27,28
5887,8305945,"For example, all segments are at least characterised with respect to four attributes (phonation, manner, place and phonetic symbol).",22,23
5888,18036414,"Rule Conflicts Rule conflicts occur when two-level rules require, that a lexical symbol is realized in two different ways in the same context.",15,16
5889,18036414,Two left-arrow rules concerning the same lexical symbol are in conflict if they require the symbol to be realized in two different ways in the same context.,9,10
5890,18036414,Two left-arrow rules concerning the same lexical symbol are in conflict if they require the symbol to be realized in two different ways in the same context.,17,18
5891,18036414,"A Probabilistic Interpretation of Rule Conflicts Consider a grammar, which has two left-arrow rules R 1 and R 2 concerning the lexical symbol x. The rules are defined x:y ⇐ C 1 and x:z ⇐ C 2 respectively.",25,26
5892,18036414,"We say that context-inclusion holds between two rules R 1 and R 2 ,whose centers have the same lexical symbol, x:y ⇐ Cl 1 Cr 1 ; and x:z ⇐ Cl 2 Cr 2 ; iff (Cl 1 Σ * Cr 1 ) ⊂ (Cl 2 Σ * Cr 2 ).",22,23
5893,18036414,"Context-inclusion is a partial ordering, so we use the symbol < for it and write R 1 < R 2 .",12,13
5894,18036414,"If these are the only rules for the lexical symbol x, then the first rule gets penalty weight 1 and the second gets penalty weight 2.",9,10
5895,18036414,"Now, consider the rule a:b ⇐ x:y ; If it is the only rule concerning the lexical symbol a, it will get the penalty weight 1.",22,23
5896,18036414,"E.g. the rule Gradation of K to 0 will be broken down into two sub-rules K:0 <= [h | Liquid | Vowel:] _ ClosedCoda; K:0 => [h | Liquid | Vowel:] _ ClosedCoda; We call the left-arrow rules, which are formed, L 0 , L j and L v and the right-arrow rules R 0 , R j and R v according to the surface symbol in their center.",81,82
5897,6150721,"The transition δ(q i , a, A) = (q j , λ, {A ′ } M ) is, for example, an instruction to read a, move from q i to q j and pop a stack symbol A from either the stack or the bag and push a symbol A ′ into the bag.",44,45
5898,6150721,"The transition δ(q i , a, A) = (q j , λ, {A ′ } M ) is, for example, an instruction to read a, move from q i to q j and pop a stack symbol A from either the stack or the bag and push a symbol A ′ into the bag.",56,57
5899,6150721,"Instead of production rules of the form S → NP VP tree fragments of the following form are introduced: S d d NP VP In derivation, trees with root labels A are plugged into trees with leaf nodes labeled by A. If a tree is obtained with root label S (the start symbol) and all leaf nodes are labeled by terminal symbols, the tree is a parse of its yield.",55,56
5900,6150721,"the subject of the finite verb V n , are read, and when NP i for 1 ≤ i < n is read the stack symbol for the corresponding embedded verb V i is pushed into the bag.",26,27
5901,7535576,"The symbol for inaudible (and therethre untranscribed) speech (...) -was simply added to the lexicon and assigned the ""t)art-of-speech"" major delimiter (mad), which is the category assigned to full stops, etc.",1,2
5902,204848226,2017) relate affordances to the symbol grounding problem.,6,7
5903,204848226,"In symbolic form, affordances are expressed through symbols, and every symbol enjoys certain relations with other symbols.",12,13
5904,8840570,When we use the + symbol we put together all the sentences extracted from the respective corpora.,5,6
5905,203688233,"Stem Patterns For stems, a * symbol marks a position where an affix can (but does not have to be) inserted.",7,8
5906,203688233,"In effect, the + symbol determines where the stem must be placed in relation to the affix, while the * symbol allows other affixes to be attached in its place.",5,6
5907,203688233,"In effect, the + symbol determines where the stem must be placed in relation to the affix, while the * symbol allows other affixes to be attached in its place.",22,23
5908,7581351,"This is similar to dependency parsing but instead of mapping the whole sentence, we map only the identified entity set x (e.g., house) to a dependency structure y. Given the entity set x with n terms, a dependency is a tuple (p, c) where p ∈ {0, ..., n} is the index of the parent term in entity set x, p = 0 is the root-symbol (only appears as parent) and c ∈ {1, ..., n} is the index of the child term in the entity set.",80,81
5909,209009596,"And the symbol ""Ours + Char"" means an additional character-level pre-trained embedding JMT (Hashimoto et al.,",2,3
5910,17352296,"We call σ the root symbol of t. The root rank of t is defined by rk(t) = k. The set of positions of t is recursively defined by pos(t) = ε ∪ i∈{1,...,k} {iρ | ρ ∈ pos(t i )}.",5,6
5911,17352296,"The symbol of t at ρ, denoted by t(ρ), is defined as the root symbol of t| ρ .",1,2
5912,17352296,"The symbol of t at ρ, denoted by t(ρ), is defined as the root symbol of t| ρ .",17,18
5913,17352296,"Also note that, for sorted trees, the symbol at a position determines the rank at this position; therefore we will use rk also for symbols from Σ. A commutative semiring is an algebraic structure = (R, +, •, 0, 1) such that (R, +, 0) and (R, •, 1) are commutative monoids, • is distributive over +, and 0 is annihilating w.r.t. •.",9,10
5914,17352296,Equivalences via Binarizations In this section we will present three different surjective mappings h : T Γ → U Σ where Σ is an alphabet and Γ is a sorted alphabet with the maximum rank of a symbol being 2.,37,38
5915,139415,"For on the fly estimation of SNR of the input, entropy is computed from the sequential label input X:  where n is the number of labels in the codebook and p is the probability distribution function of X that describes the frequency proportion of each symbol in the input.",48,49
5916,226283685,"Before feeding them into classifier, tweets were pre-processed using the following steps: • removal of the hashtag symbol (#) and all the user mentions (@USER) • removal of stopwords using NLTK 4 library • removal of Non-ASCII characters • removal of all the emoticons, symbols, numbers, special characters.",21,22
5917,218974527,"The first one is the token, whereas the second one is reserved for lemma (represented by an underscore symbol if not specified).",20,21
5918,21656928,Each filename is composed of the serial number and the list of the categories abbreviations separated by the underscore symbol and the .txt suffix.,19,20
5919,395839,"The symbol • represents any DA so that n (•|d j,i−1 ) \j,i = 1≤k≤K n (k|d j,i−1 ) \j,i .",1,2
5920,235097215,"The symbol denotes that we keep all the instances from this category (i.e., ""# Negative Examples"" or ""# Max Instances"").",1,2
5921,5224177,"The data are given in quite a rough format, with a numerical input state, a numerical output state and a transition symbol.",23,24
5922,5224177,"Complex symbols In the first example, the symbol was a simple string (""pri""), but ALeksas allows complex symbols made of a string, a set of grammatical values and a set of operations on the grammatical context (see Value recording).",8,9
5923,5224177,"The three mentioned parts of symbol are separated by semi-colons (in the previous example, the last part is empty).",5,6
5924,5224177,The number of features bound to a symbol is free.,7,8
5925,5224177,"For example, instead of listing all the prefixes as transitions, it is possible to declare a transition by a generic symbol (written without quotes) : Ex. (",22,23
5926,5224177,"Generic symbols may be recursive, that is, a generic symbol (ex.",11,12
5927,5224177,"For example, all the symbols derived from the generic symbol Pfx may inherit the value prefix (PFX) for the feature morphological type (MT) : Ex. (",10,11
5928,5224177,"The last component of the complex symbol, operations on the context, is described in the next paragraph.",6,7
5929,5224177,"ALeksas defines four operations which may be carried on the register, two mutators and two accessors : >X : adds the symbol X <X : suppresses the symbol X +X : asserts the presence of the symbol X -X : asserts the absence of the symbol X All these operations, which can be combined by &, appear in the third part of the complex transition symbol.",23,24
5930,5224177,"ALeksas defines four operations which may be carried on the register, two mutators and two accessors : >X : adds the symbol X <X : suppresses the symbol X +X : asserts the presence of the symbol X -X : asserts the absence of the symbol X All these operations, which can be combined by &, appear in the third part of the complex transition symbol.",30,31
5931,5224177,"ALeksas defines four operations which may be carried on the register, two mutators and two accessors : >X : adds the symbol X <X : suppresses the symbol X +X : asserts the presence of the symbol X -X : asserts the absence of the symbol X All these operations, which can be combined by &, appear in the third part of the complex transition symbol.",40,41
5932,5224177,"ALeksas defines four operations which may be carried on the register, two mutators and two accessors : >X : adds the symbol X <X : suppresses the symbol X +X : asserts the presence of the symbol X -X : asserts the absence of the symbol X All these operations, which can be combined by &, appear in the third part of the complex transition symbol.",49,50
5933,5224177,"ALeksas defines four operations which may be carried on the register, two mutators and two accessors : >X : adds the symbol X <X : suppresses the symbol X +X : asserts the presence of the symbol X -X : asserts the absence of the symbol X All these operations, which can be combined by &, appear in the third part of the complex transition symbol.",71,72
5934,221516638,"Removing different types of patterns such as URLs were replaced with URL token in the dataset, @USERNAME was converted to USER token and hashtags, # symbol was removed from the dataset.",27,28
5935,220935795,"Apart from preprocessing steps like lower casing, punctuation and symbol removal, we use oversampling to make the target class ratio 1 for handling class imbalance as the model tends to get biased towards one class on the given dataset.",10,11
5936,8098660,Our motivation is to quantify the degree to which language models can make the simplest scanning interfaces -such as showing one symbol at a time rather than a scanning a grid -competitive in terms of typing speed.,21,22
5937,8098660,"We also investigate the scanning methods of highlighting just one cell in a grid at any given time or showing one symbol at a time without a grid, and show that they yield commensurate performance when using higher order n-gram models, mainly due to lower error rate and a lower rate of missed targets.",21,22
5938,8098660,"One common approach is row/column scanning on a matrix of characters, symbols or images (a 'spelling grid'), which allows the user of a binary yes/no switch to select the row and column of a target symbol, by simply indicating 'yes' (pressing a button or blinking an eye) when the row or column of the target symbol is highlighted.",45,46
5939,8098660,"One common approach is row/column scanning on a matrix of characters, symbols or images (a 'spelling grid'), which allows the user of a binary yes/no switch to select the row and column of a target symbol, by simply indicating 'yes' (pressing a button or blinking an eye) when the row or column of the target symbol is highlighted.",70,71
5940,8098660,"For any given scanning method, the use of a binary switch to select from among a set of options (letter, symbols, or images) amounts to the assignment of binary codes to each symbol.",37,38
5941,8098660,"For example, the standard row/column scanning algorithm works by scanning each row until a selection is made, then scanning each column until a selection is made, and returning the symbol at the selected row and column.",34,35
5942,8098660,"The length of the binary code for a symbol is re- M G A F E C _ 9 7 6 5 3 4 Y 1 Z X W U T S R Q O N H B L K I V 8 P J 2 D Figure 1 : Spelling grid such as that used for the P300 speller (Farwell and Donchin, 1988) . ' '",8,9
5943,8098660,"Preliminaries and background Alternative text entry Of the ways in which AAC typing interfaces differ, perhaps most relevant to the current paper is whether the symbol positions are fixed or can move dynamically, because such dynamic layouts facilitate integration of richer language models.",26,27
5944,8098660,"In this paper we will make use of a static grid, or a single letter linear scanning interface, yet scan in a way that allows for the use of contextual language model probabilities when constructing the binary code for each symbol.",42,43
5945,8098660,"1 A ← V £ initialize A as symbol set V 2 k ← 1 £ initialize bit position k to 1 3 while |A| > 1 do 4 P ← {a ∈ A : a[k] = 1} 5 Q ← {a ∈ A : a[k] = 0} 6 Highlight symbols in P 7 if selected then A ← P 8 else A ← Q 9 k ← k + 1 10 return a ∈ A £ Only 1 element in A Binary codes for typing interfaces Row/column scanning, as outlined in the previous section, is not the only means by which the spelling grid in Figure 1 can be used as a binary response typing interface.",8,9
5946,8098660,Assign a unique binary code to each symbol in the symbol set V (letters in this case).,7,8
5947,8098660,Assign a unique binary code to each symbol in the symbol set V (letters in this case).,10,11
5948,8098660,"For each symbol a ∈ V , there are |a| bits in the code representing the letter.",2,3
5949,8098660,Let a[k] be the k th bit of the code for symbol a. We will assume that no symbol's binary code is a prefix of another symbol's binary code.,12,13
5950,8098660,Let a[k] be the k th bit of the code for symbol a. We will assume that no symbol's binary code is a prefix of another symbol's binary code.,19,20
5951,8098660,Let a[k] be the k th bit of the code for symbol a. We will assume that no symbol's binary code is a prefix of another symbol's binary code.,28,29
5952,8098660,"Given such an assignment of binary codes to the symbol set V , the algorithm in Figure 2 can be used to select the target symbol in a spelling grid.",9,10
5953,8098660,"Given such an assignment of binary codes to the symbol set V , the algorithm in Figure 2 can be used to select the target symbol in a spelling grid.",25,26
5954,8098660,"Additionally, single symbol auditory presentation would be possible, for visually impaired individuals, something that is not straightforwardly feasible with the sets of symbols that must be presented when using Huffman codes.",3,4
5955,8098660,"In principle, the symbols that are being predicted (hence typed) can be from a vocabulary that includes multiple symbol strings such as words.",21,22
5956,8098660,"The current problem is to use symbol prediction for that core typing interface, and this paper will focus on predicting single ASCII and control characters, rather than multiple character strings.",6,7
5957,8098660,"This is because, at each symbol in the string, the already typed prefix string is given -there is no ambiguity in the prefix string, modulo subsequent repairs.",6,7
5958,8098660,"For typing, once the symbol has been produced and not repaired, the model predicting the next symbol is given the true context.",5,6
5959,8098660,"For typing, once the symbol has been produced and not repaired, the model predicting the next symbol is given the true context.",18,19
5960,8098660,"Those characters are: the 26 letters of the English alphabet, the space character, a delete symbol, comma, period, double and single quote, dash, dollar sign, colon and semi-colon.",18,19
5961,8098660,"Binary codes Given what has been typed so far, we can use a character n-gram language model to assign probabilities to all next symbols in the symbol set V .",29,30
5962,8098660,Hence the binary code assigned to each symbol in the symbol set differs depending on what has been typed before.,7,8
5963,8098660,Hence the binary code assigned to each symbol in the symbol set differs depending on what has been typed before.,10,11
5964,8098660,"These include: (i) row/column scanning, both auto scan (button press selects) and step scan (lack of button press selects); (ii) Scanning with a Huffman code, either derived from a unigram language model, or from an 8-gram language model; and (iii) Scanning with a linear code, either on the 6×6 grid, or using RSVP, which shows one symbol at a time.",79,80
5965,8098660,All errors in typing were required to be corrected by deleting (via ←) the incorrect symbol and re-typing the correct symbol.,17,18
5966,8098660,All errors in typing were required to be corrected by deleting (via ←) the incorrect symbol and re-typing the correct symbol.,24,25
5967,8098660,"Spaces are treated like any other symbol in our language model -they must be typed, thus they are pre- dicted along with the other symbols.",6,7
5968,8098660,"The errors are highlighted in red, followed by the backarrow symbol to remind users to delete.",11,12
5969,8098660,"Hence, even if a wrong row is selected, the correct symbol can still be typed.",12,13
5970,8098660,"As with row/column scanning, when the wrong character is typed, the backarrow symbol must be chosen to delete it.",16,17
5971,8098660,"if the incorrectly selected set has more than one member -then we need some mechanism for allowing the target symbol to still be selected, much as we have a mecha- nism in row/column scanning for recovering if the wrong row is selected.",19,20
5972,8098660,"Recall that if a selection leads to a single symbol, then that symbol is typed.",9,10
5973,8098660,"Recall that if a selection leads to a single symbol, then that symbol is typed.",13,14
5974,8098660,"Otherwise, if a selection leads to a set with more than one symbol, then all symbol probabilities (even those not in the selected set) are updated based on the error probability and scanning continues.",13,14
5975,8098660,"Otherwise, if a selection leads to a set with more than one symbol, then all symbol probabilities (even those not in the selected set) are updated based on the error probability and scanning continues.",17,18
5976,8098660,"If a non-target (incorrect) symbol is selected, the delete (backarrow) symbol must be chosen to correct the error, after which the typing interface returns to the previous position.",8,9
5977,8098660,"If a non-target (incorrect) symbol is selected, the delete (backarrow) symbol must be chosen to correct the error, after which the typing interface returns to the previous position.",17,18
5978,8098660,"Three key questions must be answered in such an approach: (1) how are symbol probabilities updated after a keystroke, to reflect the probability of error? (",16,17
5979,8098660,"If the original probability of a symbol is q, then the updated probability of the symbol is pq if it starts with a '1' and (1−p)q if it starts with a '0'.",6,7
5980,8098660,"If the original probability of a symbol is q, then the updated probability of the symbol is pq if it starts with a '1' and (1−p)q if it starts with a '0'.",16,17
5981,8098660,A symbol is finally selected when the user selects a branch leading to a single symbol.,1,2
5982,8098660,A symbol is finally selected when the user selects a branch leading to a single symbol.,15,16
5983,8098660,"To understand why this is the case, consider that a non-target (incorrect) symbol can be chosen according to the approach in the previous paragraph only with a final keystroke error.",17,18
5984,8098660,"Any keystroke error that does not select a single symbol does not eliminate the target symbol, it merely re-adjusts the target symbol's probability along with all other symbols.",9,10
5985,8098660,"Any keystroke error that does not select a single symbol does not eliminate the target symbol, it merely re-adjusts the target symbol's probability along with all other symbols.",15,16
5986,8098660,"Any keystroke error that does not select a single symbol does not eliminate the target symbol, it merely re-adjusts the target symbol's probability along with all other symbols.",24,25
5987,8098660,"Hence, no matter how many keystrokes have been made, the probability that a selected symbol was not the target symbol is simply the probability that the last keystroke was in error, i.e., 1−p.",16,17
5988,8098660,"Hence, no matter how many keystrokes have been made, the probability that a selected symbol was not the target symbol is simply the probability that the last keystroke was in error, i.e., 1−p.",21,22
5989,8098660,"For the Huffman and linear scanning approaches that we are investigating, that is not the case: any cell can be highlighted (or symbol displayed) at any time, even multiple times in a row.",25,26
5990,8098660,Bits per character represents the number of keypress and non-keypress (timeout) events that were used to type the symbol.,22,23
5991,8098660,"For any given symbol the bits may involve making an error, followed by deleting the erroneous symbol and retyping the correct symbol.",3,4
5992,8098660,"For any given symbol the bits may involve making an error, followed by deleting the erroneous symbol and retyping the correct symbol.",17,18
5993,8098660,"For any given symbol the bits may involve making an error, followed by deleting the erroneous symbol and retyping the correct symbol.",22,23
5994,8098660,"Alternately, the subject may scan pass the target symbol, but still return to type it correctly, resulting in extra keystrokes, i.e., a longer binary code than optimal.",9,10
5995,8098660,"The long code rate is the percentage of correctly typed symbols for which a longer than optimal code was used to type the symbol, by making an erroneous selection that does not result in typing the wrong symbol.",23,24
5996,8098660,"The long code rate is the percentage of correctly typed symbols for which a longer than optimal code was used to type the symbol, by making an erroneous selection that does not result in typing the wrong symbol.",38,39
5997,8098660,"Monitoring a single cell, recognizing symbol identity and pressing the switch is apparently somewhat harder than finding the symbol on a grid and waiting for the cell to light up.",6,7
5998,8098660,"Monitoring a single cell, recognizing symbol identity and pressing the switch is apparently somewhat harder than finding the symbol on a grid and waiting for the cell to light up.",19,20
5999,15416624,"The number tagger handler, the symbol tagger handler, the English tagger handler, the number G2P handler, the symbol G2P handler, the English G2P handler and the English synthesizer engine handler were simply installed into the existing TTS system.",6,7
6000,15416624,"The number tagger handler, the symbol tagger handler, the English tagger handler, the number G2P handler, the symbol G2P handler, the English G2P handler and the English synthesizer engine handler were simply installed into the existing TTS system.",21,22
6001,218974474,"In such cases, the graphs and diacritical marks used by the learner are preserved in the transcript, as long as a suitable symbol can be found. (",24,25
6002,19044566,"The tokens and lemmas were therefore preprocessed by removing # characters from tokens and lemmas that contain them, and mapping URLs, numbers, and any token or lemma containing the @ symbol to the special tokens URL, NUMBER, and USER, respectively.",33,34
6003,14919948,"A begin marker is followed by a symbol indicating a specific period, such as y2007 for the year 2007 and Jan for a January.",7,8
6004,1662949,"The auxiliary labels encode: A separator to indicate multiple original edges encoded in this label; Ancestor-number indicating that in the original graph, an edge with this label is drawn from the dependent to the n-th ancestor instead of the direct parent of this tree edge; A reverse-edge symbol to indicate edges that have reversed direction compared to the original graph.",57,58
6005,211052345,"Furthermore, to prevent the mismatch problem between pre-training and fine-tuning, BERT do not simply mask the token to the symbol [M ASK], but replace the chosen token with (1) the [M ASK] 80% of the time (2) a random token 10% of the time (3) the unchanged token 10% of the time. •",25,26
6006,211052345,We use symbol [B M ISC] to mark the beginning of the title phrase and [I M ISC] to mark rest parts of the title.,2,3
6007,75134934,"A different MTL approach is to share all parts of a model, but prepend a task-specific symbol to the input string to enable it to learn task-specific features (cf.",19,20
6008,9140751,"We only preserve word types that appear more than 100 times and replace all others with a special symbol, resulting in a vocabulary of size around 188k.",18,19
6009,13577488,"Words with a different analysis were marked with a symbol ""LOOK"", which was added also to some random words to minimize the risk of only guessing the other annotator's analysis.",9,10
6010,355666,"rule is drawn graphically with its left-hand side as the parent node and the symbols of the right-hand side as the daughters, forming a triangle, the leftmost symbol is the left corner of the triangle.)",33,34
6011,355666,"Beginning with an empty element list, the procedure is applied by performing the operations repeatedly until the list contains only the element (,S,n), where S is the grammar's start symbol, and all of the words in the given sentence have been processed.",37,38
6012,355666,"Since the symbol X is the first symbol in the first part of this element, the Z phrase can contain an X phrase at this point in the sentence, so the X phrase is put in the Z phrase, and the result of this action is indicated by the new elements (,X,p) (Y1 ... Yn,Z,F').",2,3
6013,355666,"Since the symbol X is the first symbol in the first part of this element, the Z phrase can contain an X phrase at this point in the sentence, so the X phrase is put in the Z phrase, and the result of this action is indicated by the new elements (,X,p) (Y1 ... Yn,Z,F').",7,8
6014,13165579,"Angle brackets (<>) note references to other concepts, and hash marks (#) indicate a symbol that has not been instantiated as a concept.",20,21
6015,17482680,"Multilinguality is also useful for first language learning, e.g., by displaying the parallel text in a symbol language such as Blisssymbolics.",18,19
6016,221819174,"Slot ""duration"": Not Specified label: ... plains why the label for slot ""who"" in Figure 1 has the symbol * at the beginning.",24,25
6017,15789309,"These options allow the individual to indirectly select a symbol based on some process for scanning through alternatives (Lesher et al.,",9,10
6018,15789309,"Methods of word, symbol, phrase and message prediction via statistical language models are widespread in both direct selection and scanning devices (Darragh et al.,",4,5
6019,15789309,"In such a way, Sandy can indicate the first desired symbol.",11,12
6020,15789309,"Because most text generation AAC devices typically already rely upon symbol, word and phrase prediction from statistical language models to speed text input, the predictions of the conversation partner could be used to influence (or adapt) the language model.",10,11
6021,15789309,"The target string is displayed at the top of the terminal window, one character at a time, with the carat symbol showing white space word boundaries.",22,23
6022,219304069,"For an ASR system, this will be interpreted as another phone symbol and result in different phone sets.",12,13
6023,219304069,It is interesting to see what happens if the symbol is treated as a segment.,9,10
6024,219304069,"Adding a SA symbol and therefore an SA triphone model, seems to make the systems more robust.",3,4
6025,5412138,The symbol _ indicates the absence of a word boundary (with evidence of cliticisation).,1,2
6026,3578009,On each vertice in this graph is shown an input and output symbol.,12,13
6027,3578009,The symbol #a is a disambiguation symbol which is required in Kaldi to make the FST determinizable.,1,2
6028,3578009,The symbol #a is a disambiguation symbol which is required in Kaldi to make the FST determinizable.,7,8
6029,16108530,"Given a raw collection of etymological data (the corpus)-we first aim to find the ""best"" alignment at the sound or symbol level.",23,24
6030,16108530,"Because our main goal is to develop methods that are as objective as possible, the models make no a priori assumptions or ""universal"" principles-e.g., no preference to align vowel with vowels, or a symbol with itself.",40,41
6031,16108530,"The models are not aware of the identity of a symbol across languages, and do not try to preserve identity, of symbols, or even of features-rather they try to find maximally regular correspondences.",10,11
6032,16108530,"As we develop our alignment models at the sound or symbol level, in the process of evaluation of these models, we also arrive at modeling the relationships among groups of languages within the family.",10,11
6033,16108530,"The simplest form of such alignment at the symbol level is a pair (σ : τ ) ∈ Σ × T , a single symbol σ from the source alphabet Σ with a symbol τ from the target alphabet T .",8,9
6034,16108530,"The simplest form of such alignment at the symbol level is a pair (σ : τ ) ∈ Σ × T , a single symbol σ from the source alphabet Σ with a symbol τ from the target alphabet T .",25,26
6035,16108530,"The simplest form of such alignment at the symbol level is a pair (σ : τ ) ∈ Σ × T , a single symbol σ from the source alphabet Σ with a symbol τ from the target alphabet T .",34,35
6036,16108530,"etc... The alignment on the right then consists of the symbol pairs: (v:.), (",11,12
6037,16108530,"If we know that certain symbol pairs align frequently, the joint distribution will have spikes, and lower entropy.",5,6
6038,16108530,"Rather than aligning symbol pairs, we align the corresponding features of the symbols.",3,4
6039,16108530,"Features We will code each symbol, to be aligned in the complete data, as a feature vector.",5,6
6040,16108530,"A node N in this tree holds a distribution over the values of X of only those symbol instances in the complete data that have reached in N by following the context queries, starting from the root.",17,18
6041,16108530,"For example, when coding a symbol α based on another symbol found in the context of α-at some level (say, target), some position (say, -K), and one of its features (say, M)-the next edge down the tree is determined by that feature's value; and so on, down to a leaf.",6,7
6042,16108530,"For example, when coding a symbol α based on another symbol found in the context of α-at some level (say, target), some position (say, -K), and one of its features (say, M)-the next edge down the tree is determined by that feature's value; and so on, down to a leaf.",11,12
6043,16108530,"Each cell V (i, j)-marked X in the Figure-stores the cost of the most probable path so far: the most probable way to have scanned σ through symbol σ i and τ through τ j : V (i, j) = min      V (i, j − 1) +L(. :",32,33
6044,16108530,"Each candidate refers to some symbol found on the source (σ) or the target (τ ) level, at some relative position P , and to one of that symbol's features F .",5,6
6045,16108530,"Each candidate refers to some symbol found on the source (σ) or the target (τ ) level, at some relative position P , and to one of that symbol's features F .",32,33
6046,16108530,"We augment the set of possible values at every node with two additional special branches: =, meaning the symbol at the queried position is of the wrong type and does not have the queried feature, and #, meaning the query ran past the beginning of the word.",20,21
6047,16108530,"However, certain alignments, such as t∼t/d, p∼p/b, and k∼k/g between Finnish and Estonian, cannot be explained by the multiple-symbol model.",32,33
6048,9051396,"We introduce several models for alignment of etymological data, that is, for finding the best alignment, given a set of etymological data, at the sound or symbol level.",30,31
6049,9051396,"The simplest form of such alignment at the symbol level is a pair (σ : τ ) ∈ Σ × T , a single symbol σ from the source alphabet Σ with a symbol τ from the target alphabet T .",8,9
6050,9051396,"The simplest form of such alignment at the symbol level is a pair (σ : τ ) ∈ Σ × T , a single symbol σ from the source alphabet Σ with a symbol τ from the target alphabet T .",25,26
6051,9051396,"The simplest form of such alignment at the symbol level is a pair (σ : τ ) ∈ Σ × T , a single symbol σ from the source alphabet Σ with a symbol τ from the target alphabet T .",34,35
6052,9051396,"2 To model also insertions and deletions, we augment both alphabets with the empty symbol, denoted by a dot, and use Σ .",15,16
6053,9051396,"The (historically correct) alignment on the right consists, e.g., of symbol pairs: (i:i), (.:g), (e:e), (n:.).",14,15
6054,9051396,"σ N , τ N ) of N word pairs, we first choose an alignment of each word pair (σ i , τ i ), which we then use to ""transmit"" the data, by simply listing the sequence of the atomic pairwise symbol alignments.",48,49
6055,9051396,This can be done by transmitting a special symbol # that we use only at the end of a word.,8,9
6056,9051396,"Each cell stores the cost of the most probable path so far: the most probable way to have scanned σ up to symbol σ i and τ up to τ j , marked X in the Figure: V (σ i , τ j ) = min      V (σ i , τ j−1 ) +L(. :",23,24
6057,9051396,"The cost of each symbol in the suffix can be coded, for example, according to: a uniform language model: each source symbol costs − log 1/(|Σ| + 1); a unigram model: for each source symbol σ (including #), compute its frequency p(σ) from the raw source data, and let cost(σ) = − log p(σ); a bigram model; etc.",4,5
6058,9051396,"The cost of each symbol in the suffix can be coded, for example, according to: a uniform language model: each source symbol costs − log 1/(|Σ| + 1); a unigram model: for each source symbol σ (including #), compute its frequency p(σ) from the raw source data, and let cost(σ) = − log p(σ); a bigram model; etc.",25,26
6059,9051396,"The cost of each symbol in the suffix can be coded, for example, according to: a uniform language model: each source symbol costs − log 1/(|Σ| + 1); a unigram model: for each source symbol σ (including #), compute its frequency p(σ) from the raw source data, and let cost(σ) = − log p(σ); a bigram model; etc.",41,42
6060,15146910,"Targeting NLP applications on resource constrained devices, Ganchev and Dredze (2008) suggest eliminating the symbol-table (also known as the alphabet, dictionary, etc.),",17,18
6061,15146910,"It is important to note that, rather than being primarily aimed at dimensionality reduction, the approach of Ganchev and Dredze (2008) aims to save resources by discarding the symbol-table.",32,33
6062,15146910,Usually such unseen features will be filtered out and discarded as they will not correspond to an entry in the model's symbol-table.,22,23
6063,15146910,"However, when by-passing the symbol-table and applying the feature hashing directly on the string level, we risk introducing some noise by mapping such previously unknown features into our feature vectors.",7,8
6064,6489231,"For example, punctuation symbols are assigned to the top of the syntactic trees by Alpino; so we use a punctuation rule to combine a punctuation symbol that immediately precedes an EDU with the EDU, and another rule to combine a punctuation symbol that immediately follows an EDU with the EDU.",27,28
6065,6489231,"For example, punctuation symbols are assigned to the top of the syntactic trees by Alpino; so we use a punctuation rule to combine a punctuation symbol that immediately precedes an EDU with the EDU, and another rule to combine a punctuation symbol that immediately follows an EDU with the EDU.",44,45
6066,227230366,Users create and use hashtags by placing a hash symbol in front of a word or unspaced phrase in a tweet.,9,10
6067,6306988,"Part B: a sequence of bits, 1 for each arc that is the last one in the source state, otherwise 0, converted into hexadecimal symbols, 4 bits in each symbol. •",34,35
6068,6306988,"Part C: a sequence of bits, 1 for each state that is final, otherwise 0, converted into hexadecimal, 4 bits in each symbol.",27,28
6069,222310758,Stacked models are mentioned using + symbol (e.g. model 1 + model 2 ) and phrase with NE is appended to model name if improved known data rate is used.,6,7
6070,2758905,"This paper introduces several models for aligning etymological data, or for finding the best alignment at the sound or symbol level, given a set of etymological data.",20,21
6071,2758905,"The simplest form of such alignment at the symbol level is a pair (s, t) ∈ Σ × T , a single symbol s from the source alphabet Σ with a symbol t from the target alphabet T .",8,9
6072,2758905,"The simplest form of such alignment at the symbol level is a pair (s, t) ∈ Σ × T , a single symbol s from the source alphabet Σ with a symbol t from the target alphabet T .",25,26
6073,2758905,"The simplest form of such alignment at the symbol level is a pair (s, t) ∈ Σ × T , a single symbol s from the source alphabet Σ with a symbol t from the target alphabet T .",34,35
6074,2758905,"We augment both alphabets with the empty symbol, denoted by a dot, and write Σ .",7,8
6075,2758905,"s N , t N ) of N word pairs, we first choose an alignment of each word pair (s i , t i ), which we then use to ""transmit"" the data, by simply listing the sequence of the atomic pairwise symbol alignments.",48,49
6076,2758905,"This can be done by transmitting a special symbol # that we do not use in any other context, only at the end of a word.",8,9
6077,2758905,"Each cell stores the probability of the most probable path to that point: the most probable way to have scanned the source word σ up to symbol σ i and the target word up to τ j , marked X in Figure 1 .",27,28
6078,2758905,"This does not necessarily mean many selfalignments of a symbol with itself, since a change may apply to many occurrences, e.g., all occurrences of the sound h at the end of a word have disappeared in Finnish.",9,10
6079,2758905,"Note that in 3-D a non-empty symbol in one language may align to the deletion symbol "".""",10,11
6080,2758905,"Note that in 3-D a non-empty symbol in one language may align to the deletion symbol "".""",19,20
6081,2758905,"The cell in the re-alignment matrix V (σ i , τ j , ξ k )-the cumulative cost of the cheapest path leading to the cell (i, j, k)-is calculated via dynamic programming, from the symbol-alignment costs L(σ : τ : ξ): V (σ i , τ j , ξ k ) = min                          V (σ i−1 , τ j , ξ k ) +L(σ i : . : .)",43,44
6082,2758905,"Introducing 2x2 alignment helps to reduce the cost further, but produces many spurious symbol pairs, because certain combinations of sounds appear frequently within a single language.",14,15
6083,67855643,"As soon as the /S symbol is appended, the sequence is removed from the beam and added to the set of complete sequences.",5,6
6084,6222921,"IPA symbol for letters for which they are not the letter itself: ü = y, ö = ø, õ = 7, ä = ae, a = A The work described here permits different kinds of definitions for measuring the distances, including those used in (Covington, 1998; Kondrak, 2000; Nerbonne and Heeringa, 1997; Somers, 1999) .",1,2
6085,9936008,This representation corresponds to the 1-to-n alignment of Bisani and Ney (2008) because each input symbol is associated with a possibly empty sequence of outputs.,21,22
6086,9936008,"Like many string-to-string translation tasks, spelling correction can be formulated as sequence labeling: the correction system receives a string of input symbols and associates each input symbol with a (possibly empty) sequence of output symbols as shown in Figure 1 .",32,33
6087,9936008,"In order to incorporate symbol contexts into their models, they formulate stringto-string translation as a sequence labeling task.",4,5
6088,9936008,"1) The Sequitur system (Bisani and Ney, 2008) implements a joint generative model on input and output strings using graphones, which are units consisting of one input symbol and a possibly empty sequence of output symbols. (",32,33
6089,9936008,"In order to accelerate training, we use aligned training data (consisting of symbol pairs) instead of treating the alignment of input and output strings as a latent variable.",14,15
6090,9936008,"Unstructured Classifier This classifier represents the conditional probability p(y|x) of a normalization y = (y 1 , ..., y T ) given an input x = (x 1 , ..., x T ) in an unstructured manner, that is p(y|x) = T t=1 p(y t |x, t) This corresponds to making the assumption that output symbols y t and y u (t = u) are independent given the input x. To determine the probabilities p(y t , |x, t), we first map each input position (x, t) to a context L x t R, where L and R are regular languages, and the input position (x, t) matches L x t R, that is x 1 ... x t−1 x t x t+1 ... x T ∈ L x t R. The symbol is a special symbol which does not occur in any input string or output string.",155,156
6091,9936008,"Unstructured Classifier This classifier represents the conditional probability p(y|x) of a normalization y = (y 1 , ..., y T ) given an input x = (x 1 , ..., x T ) in an unstructured manner, that is p(y|x) = T t=1 p(y t |x, t) This corresponds to making the assumption that output symbols y t and y u (t = u) are independent given the input x. To determine the probabilities p(y t , |x, t), we first map each input position (x, t) to a context L x t R, where L and R are regular languages, and the input position (x, t) matches L x t R, that is x 1 ... x t−1 x t x t+1 ... x T ∈ L x t R. The symbol is a special symbol which does not occur in any input string or output string.",159,160
6092,9936008,"If none of them occur more than n T H times, the single symbol x t is used to define the context.",14,15
6093,9936008,"− z ] Note that deletions and insertions are treated here as ordinary substitutions, and the empty string ε is thus treated like any other symbol.",26,27
6094,9936008,"In practice, we represent inputs as sequences of pairs separated 1 by a special symbol • which is neither an input nor a potential output symbol.",15,16
6095,9936008,"In practice, we represent inputs as sequences of pairs separated 1 by a special symbol • which is neither an input nor a potential output symbol.",26,27
6096,9936008,Let us look at the following regular expression in Xerox syntax: • # # • t [t|th|O] • e [e|c|O] • # # • The • symbol unambiguously outlines the sequence of input and output symbol pairs.,31,32
6097,9936008,Let us look at the following regular expression in Xerox syntax: • # # • t [t|th|O] • e [e|c|O] • # # • The • symbol unambiguously outlines the sequence of input and output symbol pairs.,40,41
6098,9936008,"Before feature extraction, we pad the aligned strings with this auxiliary symbol in order to formulate correspondences occurring in string-inital and and string-final positions.",12,13
6099,9936008,"The second pair of the sequence contains an input symbol t and a set of potential output symbols, of which the O symbol denotes a deletion.",9,10
6100,9936008,"The second pair of the sequence contains an input symbol t and a set of potential output symbols, of which the O symbol denotes a deletion.",23,24
6101,233189610,"For the input feeding, the input to decoder comprises the predicted symbol embedding and gated attention vector c t : c t = p 0 u t + p 1 v t (6) The two-headed gate mechanism provides extra interpretability in the form of a three-way answer about what is relevant at a time step: the lemma, the inflections, or both.",12,13
6102,233189610,"Inverse mapping A and group prediction steps by generation type: By inverting salient alignments, we construct a mapping from input positions to prediction steps grouped by a symbol corresponding to generation type.",29,30
6103,233189610,"The latter is identified for each alignment a j by the type of input: we denote generation from the lemma's characters (a j = X i ) by symbol g, whereas that from a tag (a j = F k ) is denoted by indexed symbol f k. The special case of copying a character from the lemma, i.e. a j = X i and x i = y j , is denoted by symbol c. Thus, a position in F can be mapped to only one group of prediction steps (the type of generation is unique and defined by the tag's position), whereas that in X can be mapped to up to two groups, g and c. Some input positions might be absent in the constructed mapping, if not present in salient alignments, e.g. X 9 in Table 2 .",31,32
6104,233189610,"The latter is identified for each alignment a j by the type of input: we denote generation from the lemma's characters (a j = X i ) by symbol g, whereas that from a tag (a j = F k ) is denoted by indexed symbol f k. The special case of copying a character from the lemma, i.e. a j = X i and x i = y j , is denoted by symbol c. Thus, a position in F can be mapped to only one group of prediction steps (the type of generation is unique and defined by the tag's position), whereas that in X can be mapped to up to two groups, g and c. Some input positions might be absent in the constructed mapping, if not present in salient alignments, e.g. X 9 in Table 2 .",50,51
6105,233189610,"The latter is identified for each alignment a j by the type of input: we denote generation from the lemma's characters (a j = X i ) by symbol g, whereas that from a tag (a j = F k ) is denoted by indexed symbol f k. The special case of copying a character from the lemma, i.e. a j = X i and x i = y j , is denoted by symbol c. Thus, a position in F can be mapped to only one group of prediction steps (the type of generation is unique and defined by the tag's position), whereas that in X can be mapped to up to two groups, g and c. Some input positions might be absent in the constructed mapping, if not present in salient alignments, e.g. X 9 in Table 2 .",80,81
6106,233189610,"Then, we construct patterns of lemma and inflected form, by replacing characters at aligned positions with an indexed value of the generation type symbol, e.g. (c, X j , Y k ) → index; x j → c index ; y k → c index .",25,26
6107,233189610,"In X, this can result in an aggregated symbol, e.g. replacing X i with c 1;2 ; g 1 means that position X i is aligned to three target positions, two of which are generated by copying x i .",9,10
6108,233189610,"At the same time, we update the symbolic mapping: if two adjacent symbols are collapsed, we replace their string mappings with a single mapping from the strings concatenation to the generation symbol.",34,35
6109,233189610,"Afterward, we replace all subwords in the input lemma at nonsalient positions, S j ∈ a, with a dedicated symbol, e.g. asterisk *.",22,23
6110,233189610,"At this stage, to make the patterns more readable, we perform an unmasking operation within each group: if a particular symbol is used to substitute one substring that is the same for all examples within a group, we replace the symbol back with this substring.",23,24
6111,233189610,"At this stage, to make the patterns more readable, we perform an unmasking operation within each group: if a particular symbol is used to substitute one substring that is the same for all examples within a group, we replace the symbol back with this substring.",44,45
6112,233189610,"For instance, if the pattern from our example c 1 re → c 1 f4 1 f3 1 f2 1 f4 2 represents one such group, and symbol f4 2 is used to substitute only one string no, which is the same across all data points in the group, we can unmask the string, to obtain a pattern c 1 re → c 1 f4 1 f3 1 f2 1 no.",29,30
6113,233189610,"For each presented pattern, we show an example mapped to this pattern and symbol mapping information.",14,15
6114,233189610,"The latter lists, for each symbol in the pattern, all substrings mapped to this symbol along with their frequencies (within a group), if the number of distinct substrings is less than five elements.",6,7
6115,233189610,"The latter lists, for each symbol in the pattern, all substrings mapped to this symbol along with their frequencies (within a group), if the number of distinct substrings is less than five elements.",16,17
6116,233189610,"Otherwise, we show average length (≈) of substrings mapped to this symbol, or exact length (=), if it is the same for all of them.",14,15
6117,233189610,"These symbol mappings also include bijection cases (↔) that were unmasked after grouping examples (as described in §4.3) We observe that in all three cases, the patterns recover inflection morphemes listed in grammars for studied grammatical categories as well as their form variation.",1,2
6118,8718435,"Similarly, the most frequent mistake the NATLIB model made on the same test was deleting a hyphen symbol ""-"", which happened 663 times.",18,19
6119,18969591,He uses the crosshatch symbol to indicate the point at which there is an interruption.,4,5
6120,18969591,"He states that ""we need rules for deciding how to fit that symbol, and the words before and after it, into a coherent structure[. . . ]""",13,14
6121,18969591,"and subsequently asks the question ""Where in the tree do we attach the interruption symbol?""",15,16
6122,233189533,"Background 2.1 Byte-pair Encoding (BPE) Originally, BPE is a data compression technique based on replacing the most common pair of consecutive bytes with a new symbol (Gage, 1994) .",30,31
6123,233189533,"Each symbol is generated with a certain probability, and hence carries a certain information content (Juola, 1998; Ehret and Szmrecsanyi, 2016a; Ehret, 2016b; Koplenig et al.,",1,2
6124,233189533,"The higher the probability of a symbol, the lower its information content.",6,7
6125,233189533,"In the first operation, the algorithm merges the most frequent pair of consecutive characters within the corpus, e.g., ('e','d') → ('ed'), thus creating a new symbol that is added to the vocabulary.",37,38
6126,52840188,"2014) , that allows a model to search for parts of a source sequence that are relevant to predicting a target symbol.",22,23
6127,52840188,"Original is the version of the corpus as provided by its authors, where emojis are replaced with a sequence of characters describing the symbol.",24,25
6128,52840188,K end with a boundary symbol.,5,6
6129,52840188,"5  We consider two boundary symbol types: space, which marks the end of a word in a partial predicted sequence, and a special eow symbol, marks the end of a completed predicted sequence.",6,7
6130,52840188,"5  We consider two boundary symbol types: space, which marks the end of a word in a partial predicted sequence, and a special eow symbol, marks the end of a completed predicted sequence.",28,29
6131,52840188,The search process ends at the synchronization point where the last symbol of the best scored hypotheses (using the combined ED and LM score) is the end of complete prediction symbol eow.,11,12
6132,52840188,The search process ends at the synchronization point where the last symbol of the best scored hypotheses (using the combined ED and LM score) is the end of complete prediction symbol eow.,32,33
6133,52840188,The decoding process scores the hypotheses at two levels: normally working at the character level with ED scores and adding the LM scores only when it hits a boundary symbol.,30,31
6134,12247911,The start symbol is S : CAT s .,2,3
6135,9400830,"We define Ω = Σ can ∪ {|}, where the symbol '|' marks segmentation boundaries.",13,14
6136,9400830,K end with a boundary symbol.,5,6
6137,9400830,The boundary symbol can be either end of word symbol '< /w >' or a segmentation boundary symbol '|'.,2,3
6138,9400830,The boundary symbol can be either end of word symbol '< /w >' or a segmentation boundary symbol '|'.,9,10
6139,9400830,The boundary symbol can be either end of word symbol '< /w >' or a segmentation boundary symbol '|'.,19,20
6140,9400830,y s1−1 (without the last boundary symbol) is considered one morpheme by the LM.,7,8
6141,9400830,K end with a boundary symbol.,5,6
6142,9400830,The decoding process ends at a synchronization point where the last symbol of the best scored hypothesis (using the combined cED and LM score) is an end-of-word symbol.,11,12
6143,9400830,The decoding process ends at a synchronization point where the last symbol of the best scored hypothesis (using the combined cED and LM score) is an end-of-word symbol.,33,34
6144,9400830,The described decoding process therefore scores the segmentation hypotheses at two levels: normally working at the character level with cED scores and adding the LM scores only when it hits a boundary symbol.,33,34
6145,9400830,To synchronize the LC score with LM scoring process described before we assign it only at the synchronization time steps and attach it to the boundary symbol.,26,27
6146,9400830,y s1 where y s1 is a boundary symbol.,8,9
6147,9400830,y s1−1 and attached to the boundary symbol y s1 is calculated as the negative value of the absolute difference between the morpheme length and input word length divided by the the input length: LC(y 1 . . .,7,8
6148,16439519,pp All strings starting with # are variables and the > symbol is the dominance operator.,11,12
6149,8275098,The core symbol of the dialogue behavior diagram is an activity state where an action is carried out in the dialogue and labeled arrows are conditional transitions (guards) between states.,2,3
6150,15454959,The lexicon is expanded in terms of phones with vocabulary size of 113 words and 39 phones in ARPAbet (advanced research project agency) symbol set is used for experimentation.,25,26
6151,197678735,"Existing methods for optimizing BCI systems with language information either focus on improving the accuracy of symbol classifiers by adding priors from language models (Oken et al.,",16,17
6152,219307674,"Since the clusters Brodda mentioned were mostly consonant clusters, I replaced all vowels with one and the same symbol, hoping to get more matching of close neighbors.",19,20
6153,219307674,With this in mind I replaced all voiceless plosives with a common symbol.,12,13
6154,653058,The gloss of the held sign (following a <> symbol) is removed. •,11,12
6155,18069506,"The second variable is the event class of the verb, for which we use the symbol Caus.",16,17
6156,5657928,1 The symbol sg is a quantifier that define the count of the entities exactly one.,2,3
6157,14941111,"In this output, parse element specifies a (constituent) parse tree with a collection of spans, each of which consists of a root symbol (e.g., S) and child nodes (ids).",26,27
6158,248157415,"For instance, the adjective ""cold"" should not be labeled as optional in the triple (""Berlin Wall""; ""is [infamous] symbol of""; ""[the] cold war"").",28,29
6159,1576258,"a j is an n-gram inside a proposed word, and a 0 and a q are both the word boundary symbol, # 5 .",23,24
6160,1576258,"Specifically, some phonemes that are normally considered distinct are combined into one symbol, which we call a bi-phone symbol.",13,14
6161,1576258,"Specifically, some phonemes that are normally considered distinct are combined into one symbol, which we call a bi-phone symbol.",22,23
6162,248377759,"For example, we surround the predicate with the symbol <P>, subject with <S> and object with <O>.",9,10
6163,248377759,"For example, for predicting the object given the predicate extracted from previous iteration, the extracted predicate is marked in the sentence using the <P> symbol and the sentence is consequently passed through the transformer for predicting the object using the object head.",28,29
6164,2827736,We provided a supertag as an atomic symbol similar to a POS tag and didn't split it into a list of argument and result categories.,7,8
6165,250390726,"We treat the boundary marker ""@@"" as a single symbol 1 .",11,12
6166,250390726,"The results that changed, or were added after the competition deadline, are marked with the symbol ⋆ in the tables.",17,18
6167,53235209,"We have used standard text processing techniques with some modification to better suit the sentiment and affect domain: • All the letters are converted to lower case form • Significant amount of words are elongated with repeated number of characters such as ""ANGRYYYYYYYYYYY"", we have limited these consequent characters to maximum of 2 • All the hyperlinks are removed as they do not serve the sentiment that is conveyed by the text itself and might relate to the sentiment pointed out by that links • For words represented in hastags we remove ""#"" symbol, and if the word is not found in the vocabulary we try to segment it using Viterbi algorithm (Segaran and Hammerbacher, 2009) • Usernames are replaced with ""mention"" token • Compacted versions of word phrases such as ""wasn't"", ""when's"", etc.,",99,100
6168,44102747,"To handle hashtags, # symbol is removed from all the words.",5,6
6169,237513626,"For instance, the adjective cold should not be labeled as optional in the triple (""Berlin Wall""; ""is infamous symbol of""; ""[the] cold war"").",24,25
6170,8370249,"POS Tagging produces a part-of-speech tag as an annotation on each word or symbol, which is useful in cases such as identifying whether the word ""may"" is being used as a verb or as a noun (the month).",17,18
6171,195065096,"Other common issue is protein or gene names may coincide with common English words, e.g. for (symbol for foraging).",18,19
6172,234487195,The symbol indicates the possibility to concatenate different kinds of embeddings.,1,2
6173,3735413,The symbol # was appended to the beginning and end of the word to enable the n-gram features to capture prefixes and suffixes.,1,2
6174,3016128,"In other words, each symbol ζ i ∈ z Q annotates a single query term.",5,6
6175,14759985,The transitions can accept any non-<break> symbol (represented here as <w>) and at each length a <break> is modelled by a backwards arc that restarts the 'counter' for the next SU.,7,8
6176,17091504,"For each group, the symbol used is '(('.",5,6
6177,247362687,"This means that although a system may correctly identify a text span as being the description of a certain symbol, this classification will only be deemed correct in the evaluation if linked to the correct mention of said symbol.",19,20
6178,247362687,"This means that although a system may correctly identify a text span as being the description of a certain symbol, this classification will only be deemed correct in the evaluation if linked to the correct mention of said symbol.",39,40
6179,247362687,"This feature can be used to inform the design of a system in two ways: Either, the task of relation extraction can be simplified by reducing the choices given to a classifier based on the entity types of two spans (i.e., a symbol cannot be the description to another symbol, therefore any such prediction can be disregarded), or the entity type classification can be informed by the relation extraction (i.e., if we identify a span A as the description of another span B, span A must be a description, while span B must be a symbol).",46,47
6180,247362687,"This feature can be used to inform the design of a system in two ways: Either, the task of relation extraction can be simplified by reducing the choices given to a classifier based on the entity types of two spans (i.e., a symbol cannot be the description to another symbol, therefore any such prediction can be disregarded), or the entity type classification can be informed by the relation extraction (i.e., if we identify a span A as the description of another span B, span A must be a description, while span B must be a symbol).",54,55
6181,247362687,"This feature can be used to inform the design of a system in two ways: Either, the task of relation extraction can be simplified by reducing the choices given to a classifier based on the entity types of two spans (i.e., a symbol cannot be the description to another symbol, therefore any such prediction can be disregarded), or the entity type classification can be informed by the relation extraction (i.e., if we identify a span A as the description of another span B, span A must be a description, while span B must be a symbol).",107,108
6182,19783999,"The ""#"" symbol is removed and the word itself is retained. •",4,5
6183,218974297,"The final ranking score is calculated as follows: score(u, c, c i−1 ) = p(c|c i−1 ) × rank score(u, c) (7) with rank score(u, c) being the score of the respective ranking function scoring the relevance of the utterance u for class c. If the utterance is the first in the interview, the previous class is set to a special class symbol START.",72,73
6184,14273499,"We directly take these action symbols a as the representation of the utterance meaning so-far, or in other words, as its logical form; hence, the learning task is to predict an action symbol as soon as it is appropriate to do so.",38,39
6185,14273499,"We make the distinction between actions and action symbol (or action description), because we make use of the fact that the same action may be described in different ways.",8,9
6186,220325764,"Maynard and Greenwood (2014) suggests that hashtag sentiment is an essential symbol of sarcasm, and authors often used hashtags to emphasize sarcasm.",13,14
6187,67864431,From here-on we will we use the bolded symbol x i to indicate the vector space value of the word assigned to the variable x i .,10,11
6188,13307682,"When any lengthening has been observed, it would be marked, using the convention of the colon symbol "":"".",18,19
6189,6911595,The letter input is fed through the MLP and a phonetic symbol (or possibly epsilon) is output by the network.,11,12
6190,6911595,The MLP is trained using letter-context and symbol output pairs from a large phonetic dictionary.,9,10
6191,235258259,"All classes are handled alike in the two domains except for the symbol class (where a dash is generally a til (to) but silent in the sport domain), and the SPORT class is unique to sports news.",12,13
6192,235258259,"The symbol class mostly suffers from the strict Finally, the slight inaccuracy of the plain class, which should remain unchanged, resulted mainly from words being misclassified to the LETTERS class (NATO − → N A T O) and mistakes in the manual data.",1,2
6193,218977389,"To pass, the sentence must: • be at least 10 letters • be between 5 and 15 words • only contain characters from the Icelandic alphabet or any of the Icelandic punctuation symbols • start with a capital letter • end with a punctuation symbol • appear in the database of modern Icelandic inflection (Bjarnadóttir, 2012) Since Icelandic is a highly inflected language, simply checking if all words in a sentence appear in a dictionary would greatly limit the number of sentences that would pass this preprocessing step.",46,47
6194,218977389,A special symbol was additionally prepended and appended to each phonetization to denote the start and end of sentences.,2,3
6195,204401827,We design the symbol USD-EUR(20-10) to represent the prediction for the USD-EUR exchange rate with 20 minutes input time and 10 minutes prediction delay.,3,4
6196,52099556,"The symbol ""+"" indicates that the higher the better, while the symbol ""-"" indicates that the lower the better.",1,2
6197,52099556,"The symbol ""+"" indicates that the higher the better, while the symbol ""-"" indicates that the lower the better.",14,15
6198,23247513," In Tables 1 and 2 , the symbol * represents a space 'inside' a word spelling, the last character of a word spelling being usually a space.",8,9
6199,7593224,"Therefore current BCI-spellers suffer from low symbol rates and researchers have turned to various hierarchical symbol trees to achieve system speedups (Serby et al.,",8,9
6200,7593224,"Therefore current BCI-spellers suffer from low symbol rates and researchers have turned to various hierarchical symbol trees to achieve system speedups (Serby et al.,",17,18
6201,7593224,"The sequence of stimuli are presented at relatively high speeds, each subsequent stimulus replacing the previous one, while the subject tries to perform mental target matching between the intended symbol and the presented stimuli.",31,32
6202,7593224,"In contrast, our approach is to distribute the stimuli temporally and present one symbol at a time using RSVP and seek a binary response to find the desired letter, as shown in Figure 2 .",14,15
6203,7593224,"When the user sees the target symbol, the brain generates an evoked response potential (ERP) in the EEG; the most prominent component of this ERP is the P300 wave, which is a positive deflection in the scalp voltage primarily in frontal areas and that generally occurs with a latency of approximately 300 ms.",6,7
6204,7593224,"Likewise, if either the maximum number of sequences has already been shown or if the maximum cumulative score equals or exceeds 0.9, then the associated symbol (for all symbols except the backspace) is added to the end of the list of previously-detected symbols, the user is able to take a break of indefinite length, and then the system continues with the next epoch.",27,28
6205,7593224,"If the symbol having the maximum cumulative score is the backspace symbol, then the last item in the list of previouslydetected symbols is removed and, like before, the user can take a break and then the system continues with the next epoch.",2,3
6206,7593224,"If the symbol having the maximum cumulative score is the backspace symbol, then the last item in the list of previouslydetected symbols is removed and, like before, the user can take a break and then the system continues with the next epoch.",11,12
6207,7593224,"The symbol duration was set to 400 ms, the duty cycle was set to 50%, and the maximum number of sequences per trial was set to 6.",1,2
6208,7593224,"The only requirement given to them concerning the chosen text was that they must not, at any point in the experiment, change what they are planning to type and they must correct all mistakes using the backspace symbol.",39,40
6209,7593224,"For example, for each mistyped nonbackspace symbol, a backspace is required to delete the incorrect symbol.",7,8
6210,7593224,"For example, for each mistyped nonbackspace symbol, a backspace is required to delete the incorrect symbol.",17,18
6211,7593224,"Likewise, if a backspace symbol is detected although it was not the symbol that the subject wished to type, then the correct symbol must be retyped.",5,6
6212,7593224,"Likewise, if a backspace symbol is detected although it was not the symbol that the subject wished to type, then the correct symbol must be retyped.",13,14
6213,7593224,"Likewise, if a backspace symbol is detected although it was not the symbol that the subject wished to type, then the correct symbol must be retyped.",24,25
6214,7593224,"As shown in the figure, the mean number of sequences for each correctly-typed symbol is 14.4 and the mean number of sequences per symbol is 5.1 (the latter of which has a maximum value of 6 in this case).",16,17
6215,7593224,"As shown in the figure, the mean number of sequences for each correctly-typed symbol is 14.4 and the mean number of sequences per symbol is 5.1 (the latter of which has a maximum value of 6 in this case).",26,27
6216,7593224,The mean number of sequences for each correctlytyped symbol for this subject is 1.4 and the mean number of sequences per symbol is also 1.4.,8,9
6217,7593224,The mean number of sequences for each correctlytyped symbol for this subject is 1.4 and the mean number of sequences per symbol is also 1.4.,21,22
6218,7593224,"Notice that in 15 out of 20 epochs the classifier was able to detect the intended symbol on the first epoch, which corresponds to a single-trial presentation of the symbols, and no mistakes were made for any of the 20 symbols.",16,17
6219,14925597,"In addition, nodes are marked with binary features called bits, prefixed with a + symbol in the notation, that capture unstructured pieces of information such as tense and number.",16,17
6220,25492437,The query system uses feature-value pairs and specification-feature pairs of the form <string> =<string> joined with the & symbol.,27,28
6221,9781535,"Any symbol waiting for translation (say, the mapping of a concept into a word) needs to be stored, learning Chinese, while the very same persons may have little problems with Italian, Spanish, or Japanese.",1,2
6222,2202801,Stanford Reader Encoding Each word or entity symbol is mapped to a d-dimensional vector via embedding matrix E ∈ R d×|V | .,7,8
6223,11301067,"We represent the pattern as a string made up of numbers to indicate radical position, of the symbol V to indicate the position of the vocalism, and of pattern consonants (if needed) following (Habash et al.,",18,19
6224,11301067,"The additional tiers can be left underspecified or empty (ε), which is both represented with the symbol 0.",19,20
6225,11301067,"For example, the information in (3) is conceptually represented in the multi-tier system as follows (only the top three tiers are filled in since no processing has taken place yet): In the implementation, each column becomes one MTT (i.e., a single symbol in the underlying FST), and the information in ( 7 ) is actually represented as follows: (8) [w0000] [a0000] [+0000] [l0000] [i0000] [+0000] [y0000] [+0000] [V0a00] [1d000] [2ς000] [V0u00][3w000] [+0000] [ ū0000] [+0000] [h0000] [u0000] After applying phonological rules, the fourth (phonological) tier has been filled in in each MTT: (9) [w00w0] [a00a0] [+0000] [l00l0] [i00i0] [+0000] [y00y0] [+0000] [V0aa0] [1d0d0] [2ς0ς0] [V0uu0][3w000] [+0000] [ ū00 ū0] [+0000] [h00h0] [u00u0] Note that the last radical in the stem [3w000] did not map to the phonological layer due to the morphophonemic rules discussed in the previous section.",51,52
6226,53074147,"A simple and efficient way to apply the decoder model during testing is to generate the most likely word at each time step, until an end symbol has been generated or the maximal number of time steps has been reached.",27,28
6227,53074147,Greedy decoding terminates when the end symbol is generated.,6,7
6228,53074147,"In the same-len version a complete hypothesis either wins (when it is on top) or is pruned, which means that all candidates on the beam are of the same length (when counting the end symbol).",40,41
6229,235212560,"This symbol usually signifies plurality and, in a lot of cases upon translating in the ET-VRO direction, the models chose not to add the ""q"", although it would have been correct.",1,2
6230,235422606,"Each token is a symbol accompanied by a feature set, a set of key-value pairs that maps feature names to boolean values.",4,5
6231,235422606,This addi- tional information is meant to aid the solver understand the meaning of a symbol they may not have seen before.,15,16
6232,202602597,A modified version of the sequence-to-sequence model was used to validate each generated output symbol with the help of some hand-written rules and modified the softmax function on the decoder side.,18,19
6233,785805,"1 Related Work The idea of connecting words to what they denote in the real world via perceptual features goes back at least to Harnad (1990) , who coined ""The Symbol Grounding Problem"": ""[H]ow can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads?""",48,49
6234,1868042,"Some of the specific features that they use for this purpose are the occurrence of the ""smiley"" symbol, abbreviations, slang words, and different function words.",19,20
6235,227231221,A top-level sentence node is added and artificial pre-terminal nodes are inserted where necessary so that each preterminal corresponds to exactly one terminal symbol as it would be the case for words and POS tags in a standard syntax tree.,27,28
6236,218973837,"All error counts were computed by SequiturG2P, with ""string errors"" referring to the number of words in which any error occurred, and ""symbol errors"" referring to the total number of errors encountered in the train/test set.",27,28
6237,226283888,"For example, word can be split into character n-grams (e.g. compatibility → 'com', '##pa', '##ti', '##bility'), where ## is a special symbol for representing the presence of a sub-word that was recognized.",45,46
6238,52965066,y m one symbol y t at a time by computing p(y t |y 1 . . .,3,4
6239,52965066,"Instead of forcing the decoder to decide on a single output symbol in each decoding step, we apply beam search (Cho et al.,",11,12
6240,52965066,"In the word-based model, each input symbol x t and output symbol y t denotes a token.",9,10
6241,52965066,"In the word-based model, each input symbol x t and output symbol y t denotes a token.",14,15
6242,52965066,"In contrast, in the character-based model, each input and output symbol denotes a single character.",14,15
6243,17015539,"Although our test set is small, the performances indicated with the dagger symbol ( †) lead to a statistically significant difference from the performance when including all features.",13,14
6244,51912665,"Note that the random string r is in most cases not a valid word in L. Additionally, we prepend a special symbol S r to the input which indicates the current task to the encoder, e.g., ""OUT=AE"" or ""OUT=POS"".",22,23
6245,51912665,"As before, we additionally feed a special symbol S w into the model which signals the current task to the encoder.",8,9
6246,5107134,"Previous investigations of such selfcorrections have typically relied on timeconsuming manual analyses of the keystroke logs 4 using either the replay function (Malkiel, 2009a) or visualization of the data (Dragsted, 2012) illustrated in Figure 6 , where the symbol '•' represents the space key and '◄' stands for backspace.",44,45
6247,8660037,We use the symbol // to segment the text in IUs (but see also the symbols //+ in section 3).,3,4
6248,8660037,We use the symbol < to mark the Pre-N and the > to mark the post-N. These tags can be considered as explicit counterparts of commas in writing. (,3,4
6249,8660037,"15) il y a plein de trucs < tu les vois après > en fait > les défauts (Rhapsodie) there are plenty of things < you see them later > actually > the faults It is possible that, due to a particular communicative structure, the illocutionary force is carried only by a part of a DU and that the nucleus forms a DU with another IC: (16) to my mother <+ I don't speak anymore (constructed example) (17) two euros >+ it costs (translation, Blanche-Benveniste 1990) The addition of the symbol + indicates that the IC on one and the other side are parts of the same UR.",110,111
6250,8660037,"As in the case of a dependency relation crossing the IC border, we add a + symbol to indicate that the illocutionary frontier is not a limit to the DU.",17,18
6251,8660037,"We propose two equivalent ways to note this, either by placing the inserted utterance between parentheses as in (23) or by using the symbol # to indicate that the utterance is continued later at the following occurrence of #: (24) a. I woke up #// you're going to laugh //# in the morning at five o'clock // (constructed example) These two notations are strict equivalents __""("" = ""#//"" and "")"" = ""//#""__, but the symbol # also allows the encoding of more complex cases such as the following example, where SPK1 is interrupted three times by SPK2.",26,27
6252,8660037,"We propose two equivalent ways to note this, either by placing the inserted utterance between parentheses as in (23) or by using the symbol # to indicate that the utterance is continued later at the following occurrence of #: (24) a. I woke up #// you're going to laugh //# in the morning at five o'clock // (constructed example) These two notations are strict equivalents __""("" = ""#//"" and "")"" = ""//#""__, but the symbol # also allows the encoding of more complex cases such as the following example, where SPK1 is interrupted three times by SPK2.",100,101
6253,6140331,"Note that '-' before a symbol indicates the coda position, as in a checked (Rusheng) syllable.",7,8
6254,6140331,"Following the IPA convention, a dot under a symbol is used to denote a syllabic consonant.",9,10
6255,6140331,4) Special utterance terminators: these terminators all begin with the + symbol and end with one of the three basic utterance terminators.,13,14
6256,248780334,"Concretely, given a text that has been segmented into EDUs e 1 • • • e n , a special symbol [CLS] is placed at the beginning of each EDU.",21,22
6257,16121317,"Different linguistic symbols can give rise to similar speech sounds, while the same linguistic symbol may also be realized in different pronunciations.",15,16
6258,131268,"Hanyu Pinyin without any tone symbol represents ""tone 5"".",5,6
6259,693801,"Free vowels and their corresponding diacritical marks are considered to be the same symbol Where V g (V p ): Finite set of graphemes (phonemes), which are vowels C g (C p ): Finite set of graphemes (phonemes), which are consonants.",13,14
6260,693801,α ∈ V g is a special symbol that represents schwa.,7,8
6261,693801,We define a syllabification to be valid if all the syllables are valid (i.e. strings of the form C p *V p C p *) and every symbol in the word is a part of one and only one syllable in the syllabification.,30,31
6262,18592893,Each syllable of laughter was labeled with one token of the symbol @ (see Example 4a).,11,12
6263,18592893,Longer laughter was indicated by a single symbol @ with the duration in the parentheses (see Example 4b).,7,8
6264,8344352,Each syllable of laughter was labeled with one token of the symbol @ (see example 4a).,11,12
6265,8344352,Longer laughter was indicated by a single symbol @ with the duration in the parentheses (see example 4b).,7,8
6266,38421597,"Consequent characters written in the same foreign language are treated as one word, as most languages use the space symbol as the word-segmentation mark.",20,21
6267,617740,"In the presented form, F(a, b) = c, F is the symbol of degree of freedom (df); a is the df value in the whole tested data set; b is the df value of the deviation between the data set; and c is the output value of df.",15,16
6268,236460076,"Method 1: sentence (TF-S) Given x tm , y tm for x, the first method utilizes word embedding and position embedding of y tm to represent m as follows: m = E tm = [E w (y 1 tm ) + E p (y 1 tm ), • • • , E w (y J tm ) + E p (y J tm )] (5) where E w and E p are word embedding and position embedding respectively, J is the length of y tm and the symbol + denotes a simple addition operator.",104,105
6269,236460076,"So, the second method takes the similarity score into account and it defines m as follows: m = s tm × E tm (6) where s tm = sim(x, x tm ) is the similarity score and the symbol × denotes the scalar-multiplication.",43,44
6270,236460076,"Therefore, the third method defines m as follows: m = A tm • s tm × E tm (7) where the symbol • denotes an operator between a vector and a matrix such that m j = s tm × E j tm if A j tm = 0 E j tm if A j tm = 1 (8) TM Augmented NMT Suppose the encoded TM x tm , y tm is denoted by m, a sequence of vectors.",25,26
6271,647006,"The decoder predicts the output y one time step t at a time, based on e. The probability for each output symbol y t hence depends on e and all previously generated output symbols: p( y|e) = T t=1 p(y t |e, y 1 • • • y t−1 ) where T is the length of the output sequence.",22,23
6272,18840574,"The segmentation symbol # marks a free morpheme boundary, while ∼ flags a following suffix.",2,3
6273,618848,"Tag symbol Explanation @>x language selection, x may be 0, 1, or 2.",1,2
6274,618848,"Therefore, the authors decide to combine the t-th syllable's lexical tone and pitch-contour VQ (vector quantization) code with its left and right adjacent syllables' lexical tones to define the t-th observation symbol, O t , as 1 1 392 56 8 , 0 6, 0 7.",42,43
6275,618848,"Therefore, in the synthesis phase (or testing phase), besides the time (syllable index within a sentence) and state (prosodic-state index) axes, a third axis to index the 8 possible observation-symbol candidates, must be added.",42,43
6276,618848,"However, in SPC-HMM, the definition of observation symbol in equation ( 1 ) does not include syllable initials and finals.",11,12
6277,6360793,"Alignment Since, by design, each word token corresponds to a symbol token (the same word type) in the NLF expression, the only substantive issue in determining the alignment is the occurrence of multiple tokens of the same word type in the sentence.",12,13
6278,6360793,Set the current expression to a newly created symbol from the dependent word.,8,9
6279,11640905,"Lengthening was transcribed using the symbol "":"" following the lengthened syllable(s), e.g. u:m:.",5,6
6280,1964529,"2003) , determining the meaning of the word ""我們"" (we) may be more difficult than solving the non-text symbol problem.",25,26
6281,1964529,"When disambiguating the meaning of some non-text symbols, such as ""/"", "":"", and ""-"" the keywords to decide the pronunciation of the special symbols may be within a fixed distance from the given symbol.",44,45
6282,11782752,"Presently, Gurmukhi has 38 consonants, 10 vowels letters (independent vowels), 9 vowel symbols (dependent vowels), 2 symbols for nasal sounds, and 1 symbol that duplicates the sound of consonants (Bhatia, 1993; Malik, 2006) .",31,32
6283,11782752,"When certain consonants occur together, a special conjunct symbol halant character ◌੍ before the consonant is used to combine the essential parts of each letter.",9,10
6284,11782752,"Vowel Mapping Independent vowels Dependent vowels Gurmukhi Devanagari Gurmukhi Devanagari ਅ अ ◌ਾ ◌ा, ਆ आ ਿ◌ ि◌, ਇ इ ◌ੀ ◌ी, ਈ ई ◌ੁ ◌ु , ਉ उ ◌ੂ ◌ू , ਊ ऊ ◌ੇ ◌े , ਏ ए ◌ੈ ◌ै , ਐ ऐ ◌ੋ ◌ो, ਓ ओ ◌ੌ ◌ौ ਔ औ Besides these vowels, Punjabi has two more symbols, ੳ and ੲ. No symbol is present in Hindi for these two symbols.",88,89
6285,15030849,"Hence, ""梧桐"" has become a symbol of separation in literary works, similar to that ""olive twigs"" symbolizes peace in the Western world.",8,9
6286,10738005,"However, the parser failed to identify the boundaries of some sentences, particularly if there was an abbreviation with a period or a colon symbol occurring in them.",25,26
6287,18543864,"Each symbol in the Cangjie codes corresponds to a key on the keyboard, e.g. "" "" and "" "" collocate with ""W"" and ""L"", respectively.",1,2
6288,15481453,"Figure 1 shows that the decomposed subband signals A and D are the approximation and detail parts of the input speech signal obtained by using the high-pass filter and low-pass filter, implemented with the Daubechies family wavelet, where the symbol ↓2 denotes an operator of downsampling by 2.",45,46
6289,15481453,"2 ) The Teager energy of the noisy speech signal [ ( )] d y n ψ is given by [ ( )] [ ( )] [ ( )] 2 [ ( ), ( )] d d d y n s n u n s n u n ψ ψ ψ ψ = + + , (3) where where the symbol ⋅ denotes the inner product.",68,69
6290,21954016,"1977) observe, a person's accent is a powerful symbol of ethnicity and psycholinguistic distinctiveness.",11,12
6291,8727754,"After applying the subtree extraction, the subtree can contain more word tokens in addition to those contained in the n-span, which are replaced by a common symbol.",30,31
6292,216144548,"One may notice the following particularities of the text: • Some words such as especially, whose and spent contain the ""s"" or long ""s"", an archaic form of the letter ""s"" which can easily be confused with the conventional symbol for the letter ""f"". •",48,49
6293,216144548,"The line-format in the text will conform to the shape of the image, meaning that word forms at the end of a line ending on an ""-"" symbol need to be joined to their complementary part on the following line to ensure completeness.",32,33
6294,8066686,"If a ∈ cp(S ρ ), then op P a ( xS σ y z) = xS σ (op Sσ ρ ( y)) zS ρ (op Sρ a ( )) , where z ∈ ( |(S ρ : ρ ∈ P )Ω * P ), and y is the rightmost substring matching this conditions which does not contain any symbol S ρ and is not in any F ρ .",68,69
6295,18079144,Most of earlier works were treating word as atomic symbol and were assigning one hot vector to each word.,9,10
6296,737306,"The standard construction assumes a deterministic FSA (see next section) and operates both automata in parallel: Whenever the PDA makes a move on a certain input symbol a, this move is combined with a corresponding move of the FSA on a (see Hopcroft and Ullman (1979) for details of the construction).",29,30
6297,737306,"B k where a is an alphabet symbol, A is a nonterminal (phrase) symbol and B 1 . . .",7,8
6298,737306,"B k where a is an alphabet symbol, A is a nonterminal (phrase) symbol and B 1 . . .",16,17
6299,737306,"A weighted context-free grammar (WCFG) G over a semiring K is a 4-tuple N, Σ, S, P : N is a finite set, the non-terminals, Σ is an alphabet, S ∈ N the start symbol, and P a finite set of pairs A → β, c ∈ (N × (Σ ∪ N ) * ) × K, the set of weighted rules.",48,49
6300,737306,"Given a WCFG G = N, Σ, S, P over K, a GLR(0) automaton (for Generalised LR) M = Q, ∆, q 0 , F, δ, τ over K is a finite-state automaton with Q, F and q 0 defined as for FSAs; ∆ is N ∪ Σ. Since M is required to be deterministic, δ : Q × ∆ → Q is a partial transition function which maps -when defined -a state q and a symbol a ∈ ∆ to a follow state.",92,93
6301,737306,"Given some input string w, it creates another GLR(0) automaton M as a result by repeatedly applying its two main operations: 2 • Shift: When the recognizer reads an input symbol a ∈ Σ in state q, it adds a transition q a − → δ(q, a) to M . •",34,35
6302,737306,"To compute M , we start with a set containing only the LR(0) item S → •S where S is a new super start symbol.",25,26
6303,737306,But even then quite big item sets may result since treebank grammars often have several thousand rules for expanding a single nonterminal symbol.,22,23
6304,737306,"5 Since the right hand sides of grammar rules expanding a given nonterminal symbol often share common prefixes, left-factoring the grammar (cf.",13,14
6305,737306,"In line 20, it is checked whether a transition leaving ancestor state q, q with symbol B q A already exists.",17,18
6306,737306,"Additionally, a new start symbol S is introduced and trivially weighted rules S → X i are added to the rule set such that X i is labeling a transition from q 0 M to a final state.",5,6
6307,1106545,"In fact, if we would take this into account, we would end up in a potentially infinite number of different sets of rules, some of them requiring additional (lightweight) tests and actions, going beyond simple symbol matching; see (Krieger, 2012) for such a set of rules that model valid time, turning binary relations into quaternary ones.",41,42
6308,1106545,"Further rule regimes are possible as long as they are expressible in HFC 's rule language which permits standard symbol matching, additional LHS tests, and RHS actions.",19,20
6309,5287713,It shows a simple finite state automaton of a single with through three nodes along the way from the initial symbol on the left to the end symbol on the right.,20,21
6310,5287713,It shows a simple finite state automaton of a single with through three nodes along the way from the initial symbol on the left to the end symbol on the right.,27,28
6311,5287713,"As Figure 6 shows the entry for grater in the Oxford Advanced dictionary (Wehmeier 2005) uses only hints through specific examples as to what sort of graters there may be in English The node <MOT> matches an arbitrary word in INTEX, the symbol <E> covers an empty element, used here in disjunction the syntactic category <DET> to turn the latter optional.",47,48
6312,15389347,"The expansion of a feature-node graph is the labeled directed graph (V , L V , E , L E ) built by expanding the set of nodes to V = V {(v, l) ∈ V × L|l ∈ F (v)} with labels L V (v) = L V (v) for all v ∈ V and L V ((v, l)) = l for all v ∈ V, l ∈ F (v) and correspondingly adding edges to get the complete set E = E {(v, (v, l))|l ∈ F (v)}, with a special symbol for the labels of newly introduced edges, i.e. L E (v, (v, l)) = .",123,124
6313,12085916,äh das einzige blaue symbol oben rechts 7.,4,5
6314,226284016,"For each line, a dictionary is constructed to hold the synset data with the following structure and key attributes: • offset: byte offset for lookup of relationships • type: part of speech type • words: a list of word dictionaries containing lemma sense: the numerical representation of which use of the lemma the synset describes • nPointers: number of outbound pointers the synset has • pointers: a list of pointer dictionaries containing symbol: the WordNet specified symbol representing relationship type (enumerated in Table 1 ) offset: the target byte offset of the pointer pos: part of speech of the pointer source/target: a special 4-digit hexadecimal designation from WordNet that determines the specificity of the relationship, e.g. from a certain word-sense belonging to this synset to another word-sense, or from the synset to another synset.",80,81
6315,226284016,"For each line, a dictionary is constructed to hold the synset data with the following structure and key attributes: • offset: byte offset for lookup of relationships • type: part of speech type • words: a list of word dictionaries containing lemma sense: the numerical representation of which use of the lemma the synset describes • nPointers: number of outbound pointers the synset has • pointers: a list of pointer dictionaries containing symbol: the WordNet specified symbol representing relationship type (enumerated in Table 1 ) offset: the target byte offset of the pointer pos: part of speech of the pointer source/target: a special 4-digit hexadecimal designation from WordNet that determines the specificity of the relationship, e.g. from a certain word-sense belonging to this synset to another word-sense, or from the synset to another synset.",85,86
6316,226284016,"It is worth noting that two of the relationships share the \ relationship symbol, which represents either a noun pertainym or derivation from an adjective based on context and results in a total of 26 unique syntactic edge labels within the WAFFLE graph.",13,14
6317,20677260,"t n ) is APPO Gtv[love] Gn[rose] (a) AM term fO || Gtv[love] renrt →O Gn[rose] (b) HR term ���� �� � ���� � ���� (c) G tv [love] ���� �� ���� � (d) ren rt →O in G n [rose] Convention: To increase readability and since in this paper we focus on the functions and rarely refer to a symbol itself, we will denote a function corresponding to a symbol with the symbol itself.",94,95
6318,20677260,"t n ) is APPO Gtv[love] Gn[rose] (a) AM term fO || Gtv[love] renrt →O Gn[rose] (b) HR term ���� �� � ���� � ���� (c) G tv [love] ���� �� ���� � (d) ren rt →O in G n [rose] Convention: To increase readability and since in this paper we focus on the functions and rarely refer to a symbol itself, we will denote a function corresponding to a symbol with the symbol itself.",105,106
6319,20677260,"t n ) is APPO Gtv[love] Gn[rose] (a) AM term fO || Gtv[love] renrt →O Gn[rose] (b) HR term ���� �� � ���� � ���� (c) G tv [love] ���� �� ���� � (d) ren rt →O in G n [rose] Convention: To increase readability and since in this paper we focus on the functions and rarely refer to a symbol itself, we will denote a function corresponding to a symbol with the symbol itself.",108,109
6320,20677260,"I.e. we will use the same notation for both symbol and associated function, not making the distinction between F and f as in the definition above.",9,10
6321,20677260,"But as a general principle in this paper, this common notation always refers to the function in text and definitions, and to the symbol in terms such as in Figures 2a and 2b .",25,26
6322,8101711,"We use 0 for the cost of mapping a symbol to itself, 1 to map it to a different symbol, including the empty symbol (covering the costs of indels), and ∞ for non-allowed mappings 2 We maintain therefore that (unnormalized) costs higher than the minimum will never correspond to longer alignment lengths.",9,10
6323,8101711,"We use 0 for the cost of mapping a symbol to itself, 1 to map it to a different symbol, including the empty symbol (covering the costs of indels), and ∞ for non-allowed mappings 2 We maintain therefore that (unnormalized) costs higher than the minimum will never correspond to longer alignment lengths.",20,21
6324,8101711,"We use 0 for the cost of mapping a symbol to itself, 1 to map it to a different symbol, including the empty symbol (covering the costs of indels), and ∞ for non-allowed mappings 2 We maintain therefore that (unnormalized) costs higher than the minimum will never correspond to longer alignment lengths.",25,26
6325,218973996,"The symbol ' । ',",1,2
6326,18506179,17An initial symbol is a phrase licensed by the grammar as a sentence that can stand alone.,2,3
6327,3947306,"The following pair of tweets hinges on the symbol Reunion: the first sets the scene, the second makes wholesale reuse of the ""dictionary"" interpretation: Last night I dreamt I was a soldier, attending a reunion.",8,9
6328,3947306,soldiers attend reunions -whose agent or whose object is a match for the dream symbol.,14,15
6329,3947306,"In this next example, the dream symbol matches the agent of the triple: Last night I dreamt I was a reporter, conducting inquiries.",7,8
6330,14244160,"Variants of these using feature weighting by entropy reduction were systematically compared, as was the representation of diphthongs (as one symbol or two).",22,23
6331,14244160,This problem can be solved by replacing each phonetic symbol by a vector of features.,9,10
6332,10015691,"Tsarfaty and Sima'an (2008, 2010) introduce an additional layer into the parsing process that directly models the subcategorization of a non-terminal symbol without taking word order into consideration.",26,27
6333,3187359,"Further, false positives are removed using heuristics such as length of consecutive author name tokens, symbol or digit in token and POS tag.",17,18
6334,195477606,"Each sentence is separated into characters where a special symbol ""|||"" is used to mark a word boundary.",9,10
6335,195477606,"Clauses are represented as sequences without changing the order, where a special symbol ""|||"" is used to start a new clause.",13,14
6336,14530112,"Each language uses an alpha-syllabic writing system, where each symbol encodes a syllable, rather than a single phoneme.",12,13
6337,1771612,These operations are deletion and insertion of a symbol and substitution of a symbol by another symbol.,8,9
6338,1771612,These operations are deletion and insertion of a symbol and substitution of a symbol by another symbol.,13,14
6339,1771612,These operations are deletion and insertion of a symbol and substitution of a symbol by another symbol.,16,17
6340,1771612,so that the scanpaths can be represented as symbol sequences.,8,9
6341,10184884,"The production rules in ITGs are of the following form (Wu, 1997) , with a notation similar to what is typically used for SDTSs and SCFGs in the right column: A → [BC] A → B 1 C 2 , B 1 C 2 A → BC A → B 1 C 2 , C 2 B 1 A → e | f A → e, f A → e | ǫ A → e, ǫ A → ǫ | f A → ǫ, f It is important to note that RHSs of production rules have at most one source-side and one targetside terminal symbol.",115,116
6342,11639828,"We report three evaluation metrics: full form accuracy, morpheme F 1 (Van den Bosch and Daelemans, 1999) and average edit distance to the gold segmentation with boundaries marked by a distinguished symbol.",36,37
6343,88188,"The writing systems of these languages are alpha-syllabic, i.e. each symbol represents a syllable.",13,14
6344,88188,"An adaptor grammar consists of terminals V and non-terminals N , (including a start symbol, S), and initial rule set R with probability p, like a Probabilistic Context Free Grammar (PCFG).",17,18
6345,3166162,"As a symbol is processed in the ¤ automaton (say, § © ), the RTN control jumps to the respective subautomaton's initial state (so, from "" % in (8) to a state "" 0 ¢ in the sub-automaton for § 1© ), keeping the return address on a stack representation.",2,3
6346,17541330,"7 Step 2 adds a set of instantiated f-annotation schemes to each symbol, based on the instantiation of metavariables from step 1.",14,15
6347,17541330,2 (corresponding to the original root symbol) contains a complete possible collection of instantiated f-constraints.,7,8
6348,17541330,"To exclude analyses whose f-structure is not ¥ (for which we are generating strings) a new start symbol is introduced ""above"" the original root symbol.",21,22
6349,17541330,"To exclude analyses whose f-structure is not ¥ (for which we are generating strings) a new start symbol is introduced ""above"" the original root symbol.",30,31
6350,17541330,10 Providing all possible combinations of augmented category symbols on the right-hand rule sides in y $ % ensures that the newly constructed rules can be reached from the root symbol in a derivation.,32,33
6351,17541330,We can overload the projection function Cat again such that Cati 43 :¥ :5 :6 q 87 3 for all augmented category symbol of the new format; likewise Cati q for a cfg.,25,26
6352,216144740,"Words that do not align well according to the predefined threshold, such as Hebrew insertions, are removed and replaced by a fixed symbol (H) both in the source and target texts.",24,25
6353,216144740,"Note that the tanwīn symbols when combined with alif were standardised to the modern style, so the diacritic symbol appears on top of the alif and not on the letter preceding it.",19,20
6354,216144740,"Instead, the model produces a distribution over all alignments of all possible labels while facilitating an extra character (the blank symbol) added to its softmax layer in order to produce the alignment.",22,23
6355,216144740,We achieved this by randomly setting a percentile of the nonspace symbols in the input sequence (before doubling the letters) to the blank symbol.,25,26
6356,6078424,"This delivers the solution u n s u i t a b l e, to be read on the last line from right to left after the symbol •. 2 that were obtained from the three runs on the BTEC tasks.",28,29
6357,236459821,"We set the number of non-reserved symbols (N in the above grammar) to 5,000, and the number of ""synonyms"" of each logical symbol (K,L,M) to be 5.",29,30
6358,236459821,"We run k-fold cross validation, in each iteration choosing one symbol per class (i.e., one ∧, one ∨, one ¬) as the class exemplars, and then classifying the remaining points using cosine similarity.",13,14
6359,236459821,"We set k to 125, so that we observe every symbol combination as exemplars.",11,12
6360,17428569,"2 can derive the derivation tree t shown at the top right from the start symbol S. x 1 • x 2 2: x 1 • x 2 VP → r 2 (V, NP) 1: x 1 • x 2 2: x 2 • x 1 NP → r In a second step, we can then interpret t into a tuple (a 1 , . . . ,",15,16
6361,17428569,"This is an s-graph grammar that consists of an RTG with start symbol S whose rules are shown in the left column, along with a string and a graph interpretation.",14,15
6362,283356,"In order to ask which aspects of meaning are compositional, we provide the following working definition: 1 (1) A meaning system (or subsystem) is compositional if: • it consists of a finite (but possibly very large) number of arbitrary atomic symbol-meaning pairings; • it is possible to create larger symbol-meaning pairings by combining the atomic pairings through a finite set of rules; • the meaning of any non-atomic symbol-meaning pairing is a function of its parts and the way they are combined; • this function is possibly complex, containing special cases for special types of syntactic combination, but only draws on the immediate constituents and any semantic contribution of the rule combining them; and • further processing will not need to destructively change a meaning representation created in this way to create another of the same type.",49,50
6363,283356,"In order to ask which aspects of meaning are compositional, we provide the following working definition: 1 (1) A meaning system (or subsystem) is compositional if: • it consists of a finite (but possibly very large) number of arbitrary atomic symbol-meaning pairings; • it is possible to create larger symbol-meaning pairings by combining the atomic pairings through a finite set of rules; • the meaning of any non-atomic symbol-meaning pairing is a function of its parts and the way they are combined; • this function is possibly complex, containing special cases for special types of syntactic combination, but only draws on the immediate constituents and any semantic contribution of the rule combining them; and • further processing will not need to destructively change a meaning representation created in this way to create another of the same type.",61,62
6364,283356,"In order to ask which aspects of meaning are compositional, we provide the following working definition: 1 (1) A meaning system (or subsystem) is compositional if: • it consists of a finite (but possibly very large) number of arbitrary atomic symbol-meaning pairings; • it is possible to create larger symbol-meaning pairings by combining the atomic pairings through a finite set of rules; • the meaning of any non-atomic symbol-meaning pairing is a function of its parts and the way they are combined; • this function is possibly complex, containing special cases for special types of syntactic combination, but only draws on the immediate constituents and any semantic contribution of the rule combining them; and • further processing will not need to destructively change a meaning representation created in this way to create another of the same type.",85,86
6365,10230907,"Note that we can specify an assertion with infinite repetition range such as in (11) where the $ symbol indicates that the signal ""AWVALID"" will eventually occur. (",20,21
6366,10230907,"The overlapped implication is denoted by the symbol | −> while the nonoverlapped implication is denoted by the symbol |=> as shows, respectively.",7,8
6367,10230907,"The overlapped implication is denoted by the symbol | −> while the nonoverlapped implication is denoted by the symbol |=> as shows, respectively.",19,20
6368,10230907,"Note, each predicate symbol and functional symbol must be assigned to non-zero natural number which represents its arity.",4,5
6369,10230907,"Note, each predicate symbol and functional symbol must be assigned to non-zero natural number which represents its arity.",7,8
6370,10230907,"Also, if f is a function symbol of arity n and t 1 , . .,",7,8
6371,10230907,"Here , ⊥, ¬, ∧, and ∨ have the same meaning as in first-order logic, P is a predicate symbol of arity n and t 1 , . .,",25,26
6372,10230907,"t n are terms; TP is a binary predicate symbol that only corresponding to temporal prepositions such as when, after, before, and until.",10,11
6373,243864631,"Plans are to develop our system to addresses the symbol grounding problem (Harnad, 1990; Steels, 2003; Cangelosi, 2010; Misra et al.,",9,10
6374,37433060,"We note that symbol ""-"" is used to denote the classical negation symbol ¬. The last two statements describe when the action cannot be executed.",3,4
6375,37433060,"We note that symbol ""-"" is used to denote the classical negation symbol ¬. The last two statements describe when the action cannot be executed.",14,15
6376,2149328,"In this paper, we use the symbol X as a placeholder for a noun.",7,8
6377,2149328,We also use the symbol Y or Z as an anonymous subject or object in English translations to make the distinction between transitive and intransitive verbs clear or make them sound more natural.,4,5
6378,210045139,"Note that we observed a high positive sentiment score for all emojis with kiss symbol or kissing face in our data, compared to Novak's scores.",14,15
6379,13770114,"2 , the input speech signal is obtained by using the high-pass filter and low-pass filter [14] , implemented with the Daubechies family wavelet, where the symbol ↓ 2 denotes an operator of downsampling by 2.",33,34
6380,209094994,"All sensitive information was replaced with corresponding semantic placeholder codes of the encountered semantic type (e.g., each specific email address was replaced by the type symbol EMAIL), not by an alternative semantic token, i.e., a pseudonym.",27,28
6381,13831954,Using Levenshtein Distance on entire interviews and treating each token as a symbol we calculated edit distances that ranged between 21 and 148 with an average of about 54.,12,13
6382,201692852,"If the prefix is already a balanced Dyck word, then the target is a special symbol '$', which also makes it a recognition task for balanced Dyck words.",16,17
6383,201692852,"It generates a sequence of closing brackets to eagerly complete the Dyck prefix into a balanced Dyck word, and the generation stops when the '$' symbol is predicted.",28,29
6384,201692852,"The main target of an instance is the matching closing bracket given the prefix, and if the prefix is balanced, a special symbol '$' is the target.",24,25
6385,218974344,"All sensitive information was replaced with corresponding semantic placeholder codes of the encountered semantic type (e.g., each email address was replaced by the type symbol EMAIL), not by an alternative semantic token, i.e., a pseudonym.",26,27
6386,2862121,"The ""N"" symbol represents the ""not the query term"".",4,5
6387,2862121,"The rest of the terms in a document are treating as another kind of observation, which is the ""N"" symbol in the Figure 3 .",22,23
6388,1100057,"A first step towards capturing the meaning, however, is learning to translate this description to symbols in the target representation, in this case to the max symbol.",29,30
6389,1100057,"We also extract hierarchical phrases (Chiang, 2007) using a variant of the SAMT method of Zollmann and Venugopal (2006) Other features relate to general information about abstract symbol categories, as specified in see-also assertions, or hyper-link pointers.",32,33
6390,1100057,"This model uses the occurrence of word-component symbol pairs as binary features, and aims to see if word co-occurrence alone is sufficient to for ranking representations.",9,10
6391,219304866,A complete patch asserts that a complete grammatical entity ( correspond ing to some terminal or non-terminal symbol of the grammar) has been found.,19,20
6392,219304866,"If an application requires all possi ble parses, then the algorithm can be modified to 1 R s is the number of rules for S (the start symbol); P a is the number of permutations of inputs of graph.",29,30
6393,52290717,"The piles in the dataset were serialized into a sequence by encoding them into 6 lists delimited by a special symbol, each of them containing a sequence of color tokens or a special empty symbol.",20,21
6394,52290717,"The piles in the dataset were serialized into a sequence by encoding them into 6 lists delimited by a special symbol, each of them containing a sequence of color tokens or a special empty symbol.",35,36
6395,52290717,"In particular, we first tested whether the model was able to recover the original meaning of a word that had been replaced with a new arbitrary symbol -e.g. """,27,28
6396,33178414,"number, rate telic show information: ""成績"" (scores), ""知名度"" (prestige/popularity) mark telic show information: ""鉤"" (checks), ""問號"" (question marks) symbol telic show information: ""字"" (words) 4.3.2 ""da 3 ""+Complex Types: Dot Exploitation Complex type arguments can have more than one meaning facets.",41,42
6397,208332724,"At each decoding step, we concatenate the forward LSTM output of the current pointed token and the backward LSTM output of the next token, and calculate a softmax distribution of all possible function words, as well as a special symbol ⇒ , which moves the pointer to the next token.",42,43
6398,208332724,"In step (1) the symbol ⇒ is predicted, therefore we move the pointer from the h to d (+1) ; in step (2) a new token f 1 is created and attached to d (+1) ; in step (3) another token f 2 is created and attached to f 1 ; in step (4) the pointer is moved to d (+2) ; in step (5) the pointer is moved again to $, which terminates the process and outputs the sequence [h, d (+1) , f 1 , f 2 , d (+2) ].",6,7
6399,208332724,"At each step, the decoder attends to the input vectors and predicts an output, which could be a symbol to copy the current input character, a symbol to ignore the current input character, or a character from the alphabet to generate a new one.",20,21
6400,208332724,"At each step, the decoder attends to the input vectors and predicts an output, which could be a symbol to copy the current input character, a symbol to ignore the current input character, or a character from the alphabet to generate a new one.",29,30
6401,6365996,This proof gives rise to several new facts: the pass symbol is found to forward entail or imply (shown using the set inclusion symbol v) the kick symbol.,11,12
6402,6365996,This proof gives rise to several new facts: the pass symbol is found to forward entail or imply (shown using the set inclusion symbol v) the kick symbol.,25,26
6403,6365996,This proof gives rise to several new facts: the pass symbol is found to forward entail or imply (shown using the set inclusion symbol v) the kick symbol.,30,31
6404,6365996,"The adverbial modifier, which is previously unanalyzed, is treated as an entailing modifier v c , which results in a reverse entailment or implication (shown using the symbol w) when inserted (or substituted for the empty symbol ) on the hypothesis side.",30,31
6405,6365996,"The adverbial modifier, which is previously unanalyzed, is treated as an entailing modifier v c , which results in a reverse entailment or implication (shown using the symbol w) when inserted (or substituted for the empty symbol ) on the hypothesis side.",41,42
6406,6365996,The overall effect of inserting the adverbial modifier (shown in red) is then propagated up the proof tree leading to an Uncertain inference (shown using the # symbol).,30,31
6407,6365996,"Such rules can be used to skip words that don't contribute direct meaning to a constituent or are unanalyzed, which is represented using the empty word symbol w .",28,29
6408,6365996,"For example, the constituent symbol block in Figure 4 .3 is marked as an intransitiveplay relation and pink1 as a play-argument, both of which combine to create a well-formed MR.",5,6
6409,6365996,"Similarly, the game symbol free kick l is broken down to two concepts: free kick and purple team (or l).",4,5
6410,6365996,"For example, knowing how the meaning, or denotation, of the symbol purple7 in general relates to the meaning of purple team, or how score relates to kick.",13,14
6411,6365996,"With the empty symbol , other symbols can also be arbitrarily added/dropped via the insert and delete rules.",3,4
6412,6365996,A string is produced by pairing the yield of each matching subtree using a delimiter /. Subtrees that do not have a matching role in the other tree or are modifier expressions are isolated and aligned to the empty symbol .,39,40
6413,126181046,2016) extended unk tags symbol to specific symbols that can present name entities.,5,6
6414,218973901,"In cases where there are two dates, one after the other, with the second in parenthesis following an arrow symbol, e.g.,C1400 (→a1376), the first date refers to the dating of a manuscript, and the second, the date of composition 6 .",21,22
6415,9722641,"We treated a Cangjie basic symbol as if it was a word, and com-puted the number of occurrences of n-grams based on the OCCs of the words in Clist.",5,6
6416,1690435,"h(ẽ, C(c)) = { 1, if ẽ = 2 and C(c).µ = ν 0, else (2) where 2 is a placeholder for a possible target translation (up to 3 words or NULL), µ is the name of a contextual (lexicon or sense) feature for the source word c, and the symbol ν represents the value of the feature µ. We extract both the lexicon and sense features from a ±k-word window centered on the word c. The lexicon features are defined as the preceding k words, the succeeding k words and the word c itself: {c −k , ..., c −1 , c, c 1 , ..., c k }.",63,64
6417,6109685,"We treated a basic Cangjie symbol as if it was a word, and computed the number of occurrences of n-grams based on the OCCs of the characters in Clist.",5,6
6418,51869366,"For example, the tokenization need to transform all different digital numbers into a single numeric symbol for semantic clustering in general cases, but should leave the numbers intact in courses such as mathematics, where exact numbers from students are expected for accurate processing.",16,17
6419,850161,"For GML, a constant symbol may represent a gene mention (e.g. ""CD59"") or its unique database entry (e.g. the Entrez Gene ID ""966"").",5,6
6420,850161,"Note that we use the symbol, !",5,6
6421,850161,"Transition feature: ( , ) ( , ) ( , ) Pr e cede x y Label x L Label y L     Note that the symbol ""+"" in the above formula directs the MLN learning algorithm to associate the formula with a different weight depending on variables containing the ""+"" notation.",30,31
6422,962605,"If the input contains at least one spelling errors, the output format is: NID, location [, location]*, where the symbol ""*"" indicates there is zero or more of the predicting element ""[, location]"".",26,27
6423,14198180,Compound modifiers are reduced to their lemmas and marked with a symbol that distinguishes them from other words.,11,12
6424,241852450,"We search across learning rate, hidden dimension, symbol embedding dimension, dropout, weight decay, and, respective to each model, number of stacked LSTMs and number of transformer heads/layers.",9,10
6425,5677636,"Hence, x, y are set as below:   x (i) + δ (i) − Segmenter x δ (i) x (i) Decoder y (i) (1) = taani, y (1) = táa− x (2) = ka, y (2) = ká− x (3) = se, y (3) = sé x (4) = y (4) = gin Subsequently, 7 codewords are obtained from differential tone encoding, then departed in 4 code segments corresponding to 4 syllables: δ (1) =[(+1, 2,´), (+1, 3, −), (−1, 4, n), (−1, 5, i)] δ (2) =[(+1, 7,´), (+1, 7, −)] δ (3) =[(+1, 9,´)] δ (4) =∅ As a remark, the first syllable in the above is the most concerned by noise : one dash symbol insertion and 2 character removals are observed.",203,204
6426,5677636,Prediction error only in character position is as weak (1.17%) as error rate (0.023%) for both character position and symbol error.,25,26
6427,12087109,"For example, the virgule symbol ""/"" was often used in place of both comma and full-stop, which proves problematic for sentence boundary detection.",5,6
6428,2775235,"The set P is constructed from D and the alignment variables A k,j as follows: P = {(l, k, h, m) : l = f ∧ (e, k, A k,h , A k,m ) ∈ D} We say the k'th sentence receives a full parse under the dependencies P if the dependencies (f, k, h, m) for k form a projective tree over the entire sentence: that is, each word has exactly one head, the root symbol is the head of the entire structure, and the resulting structure is a projective tree.",101,102
6429,2775235,"The dependency structures are assumed to be full trees: that is, they correspond to fully projected trees with the root symbol as their root. •",22,23
6430,462553,"+NULL"" denotes that we replace the generated target phrases with a special symbol /NULL0 in test sets.",14,15
6431,462553,"To directly measure the improvement obtained by the phrase generation, we replace the generated target phrases with a special symbol ""NULL"" in test sets.",20,21
6432,40082808,"Introduction In recent years, events have been encoded as strings of a regular language, where a symbol in the language represents a set of predicate logic formulae that hold at a particular temporal instant, and the order of the symbols is associated with temporal order.",18,19
6433,40082808,"These constructions suggest a relationship between the frequency of events, similar to the dependency of symbol frequencies on other symbol frequencies found in many context-free languages (L = a n b n ).",16,17
6434,40082808,"These constructions suggest a relationship between the frequency of events, similar to the dependency of symbol frequencies on other symbol frequencies found in many context-free languages (L = a n b n ).",20,21
6435,40082808,"Representing event A as the symbol a, event B as the symbol b, and an instant of time in which events may be occurring, but which are not relevant to our analysis by P, we get strings of the form aP * b, aP * bP * bP * a, bP * bP * aP * aP * a etc.",5,6
6436,40082808,"Representing event A as the symbol a, event B as the symbol b, and an instant of time in which events may be occurring, but which are not relevant to our analysis by P, we get strings of the form aP * b, aP * bP * bP * a, bP * bP * aP * aP * a etc.",12,13
6437,40082808,"∆ P is the set of transitions of the form (q i , a, A) → (q j , γ) interpreted as: when in state q i , with input symbol a, and symbol A at the top of the stack, go to state q j and replace A by the string γ, and δ A is the set of transitions of the form (q i , a) → (q j ) interpreted as: when in state q i with input symbol a, go to state q j .",36,37
6438,40082808,"∆ P is the set of transitions of the form (q i , a, A) → (q j , γ) interpreted as: when in state q i , with input symbol a, and symbol A at the top of the stack, go to state q j and replace A by the string γ, and δ A is the set of transitions of the form (q i , a) → (q j ) interpreted as: when in state q i with input symbol a, go to state q j .",40,41
6439,40082808,"∆ P is the set of transitions of the form (q i , a, A) → (q j , γ) interpreted as: when in state q i , with input symbol a, and symbol A at the top of the stack, go to state q j and replace A by the string γ, and δ A is the set of transitions of the form (q i , a) → (q j ) interpreted as: when in state q i with input symbol a, go to state q j .",94,95
6440,40082808,"No input symbol is being read while these stack operations are performed, therefore R should remain in state (q x , q y ).",2,3
6441,40082808,"Rules of type 3 ensure that if R is in a state (q 1 , q y ) with an input symbol a ∪ b, and encounters the terminal symbol a on its stack, along with there being a transition in A from q y to q z on input b, then R will transition to state (q 1 , q z ), and delete a from its stack.",22,23
6442,40082808,"Rules of type 3 ensure that if R is in a state (q 1 , q y ) with an input symbol a ∪ b, and encounters the terminal symbol a on its stack, along with there being a transition in A from q y to q z on input b, then R will transition to state (q 1 , q z ), and delete a from its stack.",31,32
6443,18728597,"If a hashtag is comprised of a single word that exists in our dictionary, we only remove the hashtag symbol.",20,21
6444,13856980,The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules.,3,4
6445,13856980,"These approaches can be categorized into two groups: 1) augmenting the nonterminal symbol X with informative labels, and 2) attaching distributional linguistic knowledge to each nonterminal in hierarchical rules.",14,15
6446,5831740,"We used 61 phonetic models, but in counting errors we folded homophones together and effectively used the reduced CMU/MIT 39 symbol set.",23,24
6447,7964967,"Section 3 describes an initial 3050 word, 52 symbol, writer dependent experiment.",9,10
6448,7964967,"Section 4 discusses a more ambitious 25,595 word, 86 symbol, writer dependent system involving multiple writers.",10,11
6449,7964967,A 7-state HMM model was chosen to represent each symbol (see Figure 4 ).,11,12
6450,7964967,"On a 25,595 word, 86 symbol, writer dependent task over six writers, an average of 4.1 % word error rate and an average of 1.4% character error rate was achieved.",6,7
6451,3200905,"some have nothing (a1), and some have symbol marks (a2, a3) between the pair; 2) Separate annotation.",10,11
6452,9964819,"Direct annotation is the most widely used form in the Web, where English meaning often follows after Chinese terminology, and some have symbol marks such as bracket parentheses and bracket, and some have nothing, e.g. ""白朗峰Mont Blanc"".",24,25
6453,237532404,"2019) where the taskspecific information is used in the decoder side, our symbol-based prompt as additional input to the bi-directional encoder can potentially achieve taskaware contextualizations.",14,15
6454,218974311,"In addition, in our analysis we have cosidered two sounds as similar if they are represented with the same IPA symbol.",21,22
6455,6647043,"We consider the 50,000 most frequent surnames in the 1990 census 1 , and extract those entries that also appear in the CMU Pronouncing Dictionary 2 , giving us a set of 45,841 surnames with their phoneme representations transcribed in the Arpabet symbol set.",42,43
6456,167261,The conversion maps each unicode character to its corresponding phonemic character and inserts a single symbol (representing the schwa/central vowel) after all consonants that are not followed by the halanta.,15,16
6457,167261,"The toolkit performs unidirectional one-to-many alignments, meaning that a single symbol in its source string can be aligned to at most one symbol in its target.",15,16
6458,167261,"The toolkit performs unidirectional one-to-many alignments, meaning that a single symbol in its source string can be aligned to at most one symbol in its target.",27,28
6459,1023167,"MADA, which uses the Buckwalter Arabic morphological Analyzer databases (Buckwalter, 2004 ), provides the necessary information to determine Hamzat-Wasl through morphologically tagging the definite article; in most other cases it outputs the special symbol ""{"" for Hamzat-Wasl.",40,41
6460,233289749,"Thus, the probability of the segment is p(y i 0 |h i ∅ , start i ) k j=1 p(y i j |h i j−1 , y i j−1 ) where y i k is the end-of-segment symbol.",42,43
6461,233289749,"A key difference between our model and these approaches is that the pre-training data for large transformer models is usually large enough that only about 15% of training tokens are masked, while we need to estimate the generation probability for every possible segment of x. Since the usual method for masking is to replace the masked token(s) with a special symbol, only one span can be predicted with each forward pass.",65,66
6462,4975449,"We phonemicize our vectors by associating each word's vector to the word's ARPAbet symbol sequence, as provided in the CMU Pronouncing Dictionary (Carnegie Mellon University, 2014).",15,16
6463,4975449,"If multiple types have the same ARPAbet symbol sequence (and are thus homophones), we discard them all.",7,8
6464,5548872,A mark is a special symbol or substring that by itself can form a token even when it appears within a graphic word.,5,6
6465,5548872,A delimiter is a special symbol or code that by itself cannot form a token but can work to delimit tokens.,5,6
6466,19096382,Adapting the Softmax Bias The last layer of the model predicts the probability of the next symbol in the sequence using the output from the recurrent layer using the softmax function to create a normalized probability distribution.,16,17
6467,16692787,"Feature Design In the current work, we assume q to be a first-order Markov process which can be described by symbol transition probabilities a ij = P ( q t+1 = S j | q t = S i ) . (",23,24
6468,266903,This paper presents a novel framework for building symbol-level control modules of animated agents and robots having a spoken dialogue interface.,8,9
6469,266903,"It corresponds to the symbol-level control module of a system that can engage in tasks in a single small domain, and it employs fixed control strategies.",4,5
6470,6666295,"An original word form and a word form from the dictionary are compared symbol by symbol, and if alternations are found in the inflectional part of the word, this counts as a morphological error; if the word form contains mistake in its stem part, it is considered to be a made-up word (Rosen et al.,",13,14
6471,6666295,"An original word form and a word form from the dictionary are compared symbol by symbol, and if alternations are found in the inflectional part of the word, this counts as a morphological error; if the word form contains mistake in its stem part, it is considered to be a made-up word (Rosen et al.,",15,16
6472,20266890,Starting and ending symbol are attached to each source sentence.,3,4
6473,41968976,"Θ is the start symbol used to represent the whole sentence, i.e. γ 0 : Θ → X , X The definition for BS is omitted because it is identical to the P S case.",4,5
6474,41968976,"The combinaions of rules can be regarded as a series of synchronous derivation steps from the start symbol Θ. For instance, the P S 5 1 and the target sentence in Figure 2 is generated as follows: To combine the rules, we adopt the CYK chart parsing algorithm, which regards the span [m, l] of the chart as P S l m .",17,18
6475,35568654,"For example, in the case 1 of Figure 3 , the source sentence has special symbol (UR´), which leads to the translation error ""UR\x{00b4}"".",16,17
6476,9073505,The class symbol of the learning instance (Fragment or Non-Fragment) indicates whether the focus item is an incompletely uttered word or not.,2,3
6477,9073505,"Subsequently, the extracted feature values and the class symbol were arranged into a flat, fixedlength format of 23 elements, illustrated in Table 2 .",9,10
6478,1020959,"Since the phoneme symbol set used by Hempelmann (based on (Sobkowiak, 1991) ) differs from that used in the CMU dictionary, the Hempelmann costs are mapped to match the CMU inventory.",3,4
6479,1020959,symbol is used for epsilon transitions and indicates segment insertions and deletions.,0,1
6480,498,"In words, it is the probability that, given the sentence wl...w,, a symbol X generates ws...wt.",18,19
6481,15772469,"Table 3 presents the performance of CRF when omitting different level features for imperfect hits and the symbol ""-"" denoted the test without a level feature.",17,18
6482,10687323,"The somewhat surprising result actually reinforces the Saussurian view that underlying the systemic linguistic structure, assignment of linguistic content for each linguistic symbol is arbitrary.",23,24
6483,27189755,"Figure 1 Dictionary Entries Viewed by Panorama Pro As can be seen, many components of a dictionary entry can be highlighted by color and font variations and by some symbol prefixed.",30,31
6484,10672196,"First, each string in the database is broken into tokens, where a token is either a contiguous sequence of alpha-numeric characters or a punctuation symbol.",28,29
6485,16536097,"We used 61 phonetic models, but in counting errors we folded homophones together and effectively used the reduced CMU/MIT 39 symbol set.",23,24
6486,1925790,"1989) and others in using linear logical implication, denoted by the symbol .",13,14
6487,198959696,"The stem is marked by the symbol # on both sides, 4) Gloss: the English translation of the lemma appears in this field.",6,7
6488,9004962,Consider the following GMTG: 11)   Production (8) asserts that symbol vanishes in translation.,13,14
6489,9004962,"A generalized multitext grammar with dimensions ( -GMTG for short) is a tuple u vB ¥ 7 98 ¡ u7 X ¡ ¦w ¡ © where 7 98 , 7 X are finite, disjoint sets of nonterminal and terminal symbols, respectively, xR 7 8 is the start symbol and w is a finite set of productions.",52,53
6490,9004962,We omit symbol from -GMTG whenever it is not relevant.,2,3
6491,9004962,"A start link of a -GMTG is a -dimensional link where at least one component is ¥ G § Q © , the start symbol, and the rest of the components are ¥ © .",24,25
6492,9004962,"The algorithm to convert a GMTG to GCNF has the following steps: (1) add a new start-symbol (2) isolate terminals, (3) binarize productions, (4) remove "" 's, (5) eliminate useless links and terminals, and (6) eliminate unit productions.",21,22
6493,12347110,"A traditional context-free grammar (CFG) is a four-tuple G = (N, ~, P, S), where N is a finite set of nonterminal symbols, ~ is a finite set of terminal symbols such that N N ~ = O, p is a finite set of productions and S E N is a special designated start symbol.",68,69
6494,12347110,"Productions in P are denoted by symbol Pr, 1 < r < IPI, and have the form Dr ~ Zr,I Z,.",6,7
6495,12347110,"If the return value is 1, add an initial edge e for that rule to chart; for all the chart entries (subtrees) d beginning at end(e)÷l, if g is the active symbol in the RHS (right-hand-side) of e and match(g, c') returns 1, then call extend(e, cl).",36,37
6496,849336,"We notice that most simple disfluencies such as repetitions have been omitted by OYEZ annotators, while more complex ones are often present and annotators have used the '...' symbol at locations of filled pauses or repetitions.",31,32
6497,849336,"We observe that roughly 30% of the interruption points in CAREFUL SCOTUS are associated with the '...' symbol in the OYEZ transcripts; therefore, we add '...' symbols after 1/3 of the interruption points in the SWBD.",20,21
6498,15114510,"work vnth structured data of arbitrary length laden w~th variables They also have powerful symbol-matching faczhtms (as ts appropriate for lower-level text analysas) In contrast, the ANNS are able to deal wtth fuzzy and inexact proceasmg (as ts revolved m nnportance determination and raterlevel feature mappings) (McClelland and Rumelhartl 1986, Rumelhart and McClelland, 1986) 4 Empirical Evidence As the first and most cructal step m unplementmg COSY-MATS, a prototype of its content selectmn ANN was developed Tins ts a standard feed-forward back-propagation network (Rumelhatt et al, 1986) Tins ANN receives m&wdual text sentences from the text to be snmmansed, hand-coded 2 by means of the identified pragmatic features, and assagns to them degrees of maportance It has been a major assumption behind tins work that it m feature combmatzons rather than individual features that charactense sentence importance (Sectmns 1 & 2) An ANN learns such interactions naturally, wlnch m why the connectlomst paradigm was.adopted for the content selection task The training corpus conststed of 1,8801 sentences m total, taken from the real-world text collection described m Sectmn 2 1,100 of them are sentences largely out of thetr context, wtule the remmnmg 780 sentences make up 29 full texts In contrast to the dwersaty of the former subcorpus, each of the latter texts ts approxLmately 23 sentences long and was fully encoded The encoding was camed out by 5 mchvlduals on the basas of the above-mentmned manual wlnch exemphfies the correlations between the surface and the more abstract features m the proposed scheme The manual was used m order to standarchse the encoding process as much as possable, as well as to vahdate the proposed ways m wlnch the evaluatmn of the abstract pragmatic features can be objectified and fully automated later on m the completed system Experiments to date (cf (Aretoulah, 1996) ) have demonstrated the superiority of the pragmattc features over input to the ANN from aLcross the three levels of abstraction (58 1% vs 56 1% success on average, where 'success' coincides with agreement vnth the judgement made by the human encoder regardmg the level of nnpo/'tance of the corresponding sentence) The snnultaneous use of control experiments wtth nomy data S has ensured the vah&ty of these results (50 1% success) In addttlon, the testmg on whole texts has prowded comparable results to those acqmred with molated sentences, namely 56 8% success on average, thts suggests that the pragmatic features are sufficiently abstract to capture tuerar-ch~cal and structural aspects of the corresponchng dmcourses The dlversaty of the corpus m terms of subject matter, text type and length provides sutBcaent ewdence for the appropriateness of the pragmatic fea-2given that the remaining components of COSY-MATS have not been tmplemented as yet, 8These used characterLstlcs of the text that should be n~elevant to the content selection task, such as 'The second word m the sentence ends m a vowel' tures for the Ingh-level representatlon of texts from any domain or text type Moreover, the portabflity Of these pragmatlc content selectlon features has also been partly proved wlth experiments on whole texts (AretoulaJa, 1996) These re&cared that only a small amount of retraining ~s reqmred for the ANN to deal wlth new text types, winch mvolves a hmited number of representatlve texts Thus, what is pre&cted to dL~er between text types is the relatlve influence of each of the identflled features m the final wmghtmg of the corresponding sentence Generating Draft Sllmmarles The 'draft' s11,nmanes that result after concatenating the sentences of the input text that were selected by the ANN as Important are, on the whole, adequate for current awareness purposes (See (Aretoula]a, to appear) for a detailed evuluatmn of tins and other draft output) The ANN recelves a single --coherent and largely cohemve---text each tIme, rather than a collection of unrelated texts Sentence selection was based on the 24 pragmatic features used for their encoding and the statmt~cal correlatlous among them, as mchcated m the tratmng corpus Most Impor-tantly1 by faltering out the sentences for winch the AnN &d not have a clear dectslon, I e by adapting the corresponchng threshold on-/me, content selectmn can be more fiue-grarned and the output summanes more brief An example draft summary for a newspaper article after the apphcatlon of tiLtS type of fdtermg ~s shown below In tins case, there was 8~ 6~ agreement between the ANN decision and the corresponding human judgement regarding the importance of m&v~dual sentences m thin article 4 (I) Moscow e&tors fee] the old-fashmned grip of the state (Headline) (~) Intense party pressure for the &enuseal of a prominent hberal e&tor and a new campmgn to d~sere&t the ra&cal pohtw~sn Bona Yeltsm both apparently with the badang of President Gorbachev -have rinsed fears among reformers of a conservative swing by the Soviet leaderslup (5) On Monday evemug, he was summoned to the Central Comm,ttee to be told m so many words by Va&m Medvedev, the Pohtburo member m charge of ideology, that he should leave has post (6) The move follows a harsh talk dehvered last week by Mr Gorbachev to a group of semor Soviet e&tors, m which he gave several a dressing down (12) Some joumalmts are talking of a protest strike. (",16,17
6499,6352278,"k P(cl... ca) = P(c, I#) 1Y[ P(c, I~,-,)P(#1~)(7) Here, special symbol ""#"" indicates the word boundary marker.",20,21
6500,6462360,"Documents, represented with symbol B in Figure 1 , go through a similar procedure in the Data Processing Module as did a query in the Query Processing Module.",4,5
6501,6462360,"The output of the data processing module is processed documents with stemmed words and their associated concepts, represented with symbol D in Figure 1 .",20,21
6502,6462360,"Currently, we select and send top 200 segments per query (symbol E in Figure 1 ) to the Final Answer Formulation module.",12,13
6503,6462360,Segment Score ¢ Normalized Original Score where symbol max is a normalization factor and symbol diff is the proximity difference between a query and a candidate segment for a given pair of keywords.,7,8
6504,6462360,Segment Score ¢ Normalized Original Score where symbol max is a normalization factor and symbol diff is the proximity difference between a query and a candidate segment for a given pair of keywords.,14,15
6505,6462360,"This answer is the output fed into the translation module, if necessary, shown as symbol F in Figure 1 .",16,17
6506,453138,"We hypothesize that (i) the initial symbol in the grammar (i.e. Sentence) always starts with the single category generic_np, the grammatical function (subject, object) of which is undetermined.",8,9
6507,453138,"The generic_np dominated by the initial symbol sentence in (i) of Figure 2 is parsed as an element moved from the position occupied by np_trace in (iii), and therefore corresponds to the category np_trace dominated by subject in Figure 3 (placed on the next page for space reasons).",6,7
6508,5947331,"For example, assuming the saturated category symbols 'S' and 'NP', here is a simple CCG lexicon (modalities omitted): John NP (1) Mary NP loves (S\NP)/NP The combinatory projection of a CCG lexicon is its closure under a finite set of resource-sensitive combinatory operations such as forward application (2), backward application (3), forward type raising (4), and forward composition (5): X/Y Y ⇒ X (2) Y X\Y ⇒ X (3) X ⇒ Y /(Y \X) (4) X/Y Y /Z ⇒ X/Z (5) CCG A, S, L over alphabet Σ generates string s ∈ Σ * just in case s, S is in the combinatory projection of lexicon L. The derivation in Figure 1 shows that CCG (1) generates the sentence John loves Mary, assuming that 'S' is the distinguished symbol, and where > T, > B and > denote instances of forward raising, forward composition and forward application respectively: Lexical redundancy in CCG CCG has many advantages both as a theory of human linguistic competence and as a tool for practical natural language processing applications (Steedman, 2000) .",173,174
6509,5947331,"The atomicity ratio is calculated by dividing the number of saturated category symbol-tokens by the number of lexical entries, i.e. 36 22 = 1.6.",12,13
6510,5947331,"I-CCGs An inheritance-driven CCG (I-CCG) over alphabet Σ is an ordered 7-tuple A, A , B, B , b, S, L , where A, A is a type hierarchy of saturated category symbols, B, B , b is an inheritance hierarchy of lexical types over the set of category descriptions over A∪B, S is a distinguished symbol in A, and lexicon L is a function from Σ to A ∪ B. Given an appropriate A,B -compatibility relation on the categories over A∪B, the combinatory projection of I-CCG A, A , B, B , b, S, L can again be defined as the closure of L under the CCG combinatory operations.",74,75
6511,5947331,"Given some type hierarchy A, A of saturated category symbols and some lexical inheritance hierarchy B, B , b , we define a class of 'category models', i.e. binary trees where every leaf node carries a maximal saturated category symbol in A, every non-leaf node carries a directional slash, and every branch is labelled as either a 'result' or an 'argument'.",44,45
6512,219308995,Inside probability is defined as the probability of the words or tags in the constituent given that the constituent is dominated by a particular nonterminal symbol; see Figure 2 .,25,26
6513,219308995,"The inside probability of the constituent N~, k is defined as: --= p(tj,k IN') where N i represents the ith nonterminal symbol.",27,28
6514,219308995,"Initially, fl is set to 1 for each terminal symbol, since our input is given as a stream of tags, which are our terminals.",10,11
6515,18680883,"By clicking the magnifying glass symbol next to an entity pair, more details are shown, including the entity pair's FREEBASE ids and a list of sentences that match the selected patterns.",5,6
6516,9368557,"either because the MILts not recogmsed (absent from the list,, elhpsts, spelling mistake, etc ), or because the maximum size of the abstract has been reached We shouldstress both the fact that the jurors can only detect ruptures in the textual segments, and not thetr completeness, and that argumentation connectors (such as zndeea~ furthermore, etc ) are not considered to be MIL's This decision was taken after a preliminary validation by ten or so readers, and was confirmed by the jurors SERAPHIN uses a special symbol [ ] to show that two sentences are not adjacent m the source text, such as m the following example It ts easy to imagine the huge number of texts and wrztten documents that ts produced by a company hke EDF [Y Of course, all ""language productwn"" may appear to come fi'om an abstract source, a umque source . (",97,98
6517,9368557,"Text N°6) This symbol avoids the problem of the reader mistakenly reconstructing argumentation chains Criterion 3 Presence of ""tautologtcal"" sentences :.",6,7
6518,250390690,"In addition, we deploy a rule-based symbol tokenizer to improve the detection of the exact boundary of symbol entities.",9,10
6519,250390690,"In addition, we deploy a rule-based symbol tokenizer to improve the detection of the exact boundary of symbol entities.",20,21
6520,250390690,"MRC-based NER using a symbol tokenizer: Unlike the PURE system of (Zhong and Chen, 2021 ) that exploits the standard span-based NER of (Lee et al.,",6,7
6521,250390690,"MRC-based NER with a symbol tokenizer Figure 1 shows our MRC-based NER model, that extracts mathematical symbols and descriptions from scientific documents.",6,7
6522,250390690,"Symbol tokenizer as a pre-tokenizer We discovered in our preliminary experiment that SciBERT's tokenizer is not optimal for extracting the boundaries of mathematical symbols, because non-alphanumeric characters are important in the mentions of symbol-type entities.",39,40
6523,250390690,"We perform a Symbol Tokenizer Input text: Importantly , $\\mathcal{M}$ is still a Bayesian model … SciBERT Tokenizer Processed tokens: 'i', 'mpo', '##rt', '##ant', '##ly', ',', '$', '\\', … 'i', 'mportantly', ',', '$', '\\', 'mathcal', '{', 'M', '}', '$' … Figure 2: Two-step tokenization process for NER model with a symbol tokenizer rule-based symbol tokenizer as a pre-tokenizer before applying SciBERT's tokenizer to precisely detect the boundary of symbol-type entities.",115,116
6524,250390690,"We perform a Symbol Tokenizer Input text: Importantly , $\\mathcal{M}$ is still a Bayesian model … SciBERT Tokenizer Processed tokens: 'i', 'mpo', '##rt', '##ant', '##ly', ',', '$', '\\', … 'i', 'mportantly', ',', '$', '\\', 'mathcal', '{', 'M', '}', '$' … Figure 2: Two-step tokenization process for NER model with a symbol tokenizer rule-based symbol tokenizer as a pre-tokenizer before applying SciBERT's tokenizer to precisely detect the boundary of symbol-type entities.",120,121
6525,250390690,"We perform a Symbol Tokenizer Input text: Importantly , $\\mathcal{M}$ is still a Bayesian model … SciBERT Tokenizer Processed tokens: 'i', 'mpo', '##rt', '##ant', '##ly', ',', '$', '\\', … 'i', 'mportantly', ',', '$', '\\', 'mathcal', '{', 'M', '}', '$' … Figure 2: Two-step tokenization process for NER model with a symbol tokenizer rule-based symbol tokenizer as a pre-tokenizer before applying SciBERT's tokenizer to precisely detect the boundary of symbol-type entities.",138,139
6526,250390690,"The symbol tokenizer seperates mathematical symbols based on capital letters, numbers, and special characters (e.g., %, $, }, {).",1,2
6527,250390690,Our symbol tokenizer's rules are derived heuristically from a training dataset 3 .,1,2
6528,250390690,"The question-augmented input X ′ is formulated as follows: X ′ =[CLS], q 1 , • • • , q m , [SEP], x 1 , • • • , x n Type Text SYMBOL symbol PRIMARY description ORDERED ordered Table 1 : Natural language forms mapped for entity types Then, as a pre-trained language model, we apply SciBERT's encoder trained from scientific domain documents to obtain contextualized representations T ∈ R n×d over n tokens in a given document X, where d is the dimensionality of SciBERT's hidden representation.",44,45
6529,250390690,"SYMBOL is a mathematical symbol, PRIMARY is a primary description, and ORDERED is a description of multiple terms.",4,5
6530,250390690,"Such examples include COUNT(E 1 , E 2 ) and DIRECT(E 1 , E 2 ) where E 1 is a symbol-type entity and E 2 is a description-type entity.",21,22
6531,250390690," To examine the effect of the symbol tokenizer, Table 3 compares the frequencies and recalls of SYMBOL-type entities whose exact gold spans are correctly obtained when and without the symbol tokenizer.",7,8
6532,250390690," To examine the effect of the symbol tokenizer, Table 3 compares the frequencies and recalls of SYMBOL-type entities whose exact gold spans are correctly obtained when and without the symbol tokenizer.",33,34
6533,250390690,"In this case, recall is defined as the ratio of the number of symbol-type entities whose exact span boundaries are extractable using a given tokenizer to the total number of symbol-type entities.",14,15
6534,250390690,"In this case, recall is defined as the ratio of the number of symbol-type entities whose exact span boundaries are extractable using a given tokenizer to the total number of symbol-type entities.",33,34
6535,250390690,Effect of symbol tokenizer on NER task Effect of removing non-relational entities Analysis of RE model Conclusion Our system shows first for all subtasks of SemEval-2022 Task 12: 'linking mathematical symbols to their descriptions'.,2,3
6536,250390690,"To improve the performance, the symbol tokenizer for NER model, regularization, and ensemble methods, for RE model are used.",6,7
6537,250390690,"To improve the performance further, future work should look into data augmentation and mathematical symbol and description-aware pretraining.",15,16
6538,6243853,Inside probability is defined as the probability of the words or tags in the constituent given that the constituent is dominated by a particular nonterminal symbol.,25,26
6539,10012351,The emission matrix contains the probabilities for each emission symbol to occur at each state.,9,10
6540,28972578,"Because the end of sentence is represented by the special symbol E, we can start sampling a word with the probability proportional to p(E|c N N −k ).",10,11
6541,28972578,"Because the end of the sentence and its parts of speech are represented using the special symbol E, we can start sampling a word with the probability proportional to p(E|c N N −k , E)p(E|m) like NPYLM.",16,17
6542,28972578,"Because the length of each segment can be represented using the non-zero indices g, the bi-gram transition probability is rewritten as p(z g i g i−1 |z g i−1 g i−2 ) = p(g i − g i−1 |g i−1 − g i−2 ) (13) where m g i is omitted in the case of PYHSMM, and each integer, such as g i − g i−1 , is considered as a symbol or label.",80,81
6543,6169836,"The OOVs in the test set were treated as a symbol, ""<unk>"".",10,11
6544,6169836,"This means that we need to handle hesitation problem in raw-level symbol sequence, such as phoneme sequence.",13,14
6545,6169836,6 ) and ( 9 ) of a hesitation symbol will be estimated properly.,9,10
6546,2440415,"Each slot is labelled with a symbol, which denotes the type of object supposed to go there, but since Cloddy Hans is not very bright, he needs help understanding these labels.",6,7
6547,466754,"One of the first studies to investigate such constancy is Genzel and Charniak (2002) , in which the authors proposed the Constant Entropy Rate (CER) hypothesis: in written text, the entropy per signal symbol is constant across sentence positions in discourses.",39,40
6548,466754,"In order to estimate out-of-context entropy per word (i.e. per signal symbol) for each sentence position, articles were divided into a training set (95% of all stories) for training language models and a test set (the remaining 5%) for analysis (see Table 1 for details).",16,17
6549,219306256,"The content of this expectation is expressed in (U5): U5: daf~ irgcndwo die Bushaltestcllc noch cingezcichnct wird, da ira... (that the bus stop was drawn into it somewhere, there in the... ) Obviously the speaker expects that the drawing will contain a symbol representing a bus-stop near to the building.",50,51
6550,219306256,"One non-terminal symbol is used for the D(iscourse) segments, whereas the terminals are the S(entences).",4,5
6551,9503313,"The content of this expectation is expressed in (U5): U5: daB irgendwo die Bushaltestelle noch eingezeichnet wird, da im... (that the bus stop was drawn into it somewhere, there in the... ) Obviously the speaker expects that the drawing will contain a symbol representing a bus-stop near to the building.",51,52
6552,9503313,"One non-terminal symbol is used for the D(iscourse) segments, whereas the terminals are the S(entences).",4,5
6553,6407031,"Design of input WFST and grammar FST In input WFSTs and grammar FSTs, each arc representing state transitions has a label in the form of ""a:b/c"" denoting its input symbol, output symbol, and weight, in this order.",36,37
6554,6407031,"Design of input WFST and grammar FST In input WFSTs and grammar FSTs, each arc representing state transitions has a label in the form of ""a:b/c"" denoting its input symbol, output symbol, and weight, in this order.",39,40
6555,6407031,"Input symbol ε means a state transition without any input symbol, that is, an epsilon transition.",1,2
6556,6407031,"Input symbol ε means a state transition without any input symbol, that is, an epsilon transition.",10,11
6557,6407031,Output symbol ε means no output in the state transition.,1,2
6558,6407031,"For example, a state transition ""please:ε/1.0"" is executed when an input symbol is ""please,"" no output symbol is gen-erated, and 1.0 is added to the accumulated weight.",16,17
6559,6407031,"For example, a state transition ""please:ε/1.0"" is executed when an input symbol is ""please,"" no output symbol is gen-erated, and 1.0 is added to the accumulated weight.",24,25
6560,6407031,"Here, every word can be replaced by the symbol F that represents an insertion or substitution error.",9,10
6561,6407031,"Moreover, the error symbol DEL can be inserted into its output symbol sequence at any position, which corresponds to deletion errors in ASR results.",4,5
6562,6407031,"Moreover, the error symbol DEL can be inserted into its output symbol sequence at any position, which corresponds to deletion errors in ASR results.",12,13
6563,6407031,"Here, date-repeat=Mon denotes an LU result, and $ is a symbol for marking words corresponding to a concept.",16,17
6564,6407031,"An accumulated weight for a single utterance is defined as the sum of these weights as shown be- Here, E accepted denotes a set of accepted words corresponding to elements of each grammar rule, and E error denotes a set of words that are not accepted and that have either error symbol.",53,54
6565,6860686,"Introducing the Formalism Let ,C be a first-order DL language with equality made up of unary predicate symbols Q,P,G, DP, binary predicate symbols infl(uences) and ans(wers), a ternary predicate symbol Utt, a function symbol whether, constants a, b, ask, ass, clr and ack, and an infinite set Var of variables x. Var includes a set V1 = { LMa , LMb, UTTI of special individual variables and a set V2 = {FACTS, QUD a , QUDb, PENDING,, PEND ING} of stack variables.",41,42
6566,6860686,"Introducing the Formalism Let ,C be a first-order DL language with equality made up of unary predicate symbols Q,P,G, DP, binary predicate symbols infl(uences) and ans(wers), a ternary predicate symbol Utt, a function symbol whether, constants a, b, ask, ass, clr and ack, and an infinite set Var of variables x. Var includes a set V1 = { LMa , LMb, UTTI of special individual variables and a set V2 = {FACTS, QUD a , QUDb, PENDING,, PEND ING} of stack variables.",46,47
6567,6860686,We also introduce a function symbol head to be applied to stack variables.,5,6
6568,5475550,"The terminal tasks in the hierarchy (UserID, Fund, and Shares) derive canonical values of domain attributes (such as fund symbol) from parsed portions of user input.",24,25
6569,3129121,The information concerning interlocutor and multifunctionality is encoded in a single symbol and denoted by means of a n-tuple.,11,12
6570,3129121,For both cases multifunctionality is taken into account by including all occurring communicative functions in each symbol.,16,17
6571,11975567,"Correction Subdlalogues In Example 1, the system responds to the user's first utterance by producing a rising tone, illustrated by the ® symbol, to indicate successful interpretation and execution of the command, in this case creation of a CEV, a type of vehicle. (",25,26
6572,11975567,"Unsuccessful interpretation is indicated by a falling tone, illustrated by the ® symbol.)",13,14
6573,5048980,"For a successful command, it produces a rising tone, illustrated by the ® symbol in utterances 2 and 4.",15,16
6574,5048980,"For an unsuccessful command it produces a falling tone, illustrated by the ® symbol in utterances 12 and 14.",14,15
6575,219309129,"For example, t h e v e h i c l e of thought does not require words (but only concepts) nor does it have such i n t r i n s i c p r o p e r t i e s as s l z e o r shape, Rather it consists, as do a l l computations, of t h e transformation of formal symbollc expressions whose terms a r e given an i n t e n t i o n a l i n t e r p r e t a t i o n by t h e t h e o r e t l c a n , In o t h e r words, thought r s a symbol mahipulation process.",137,138
6576,219309129,"The much misused term ftsemantlcs"" r e f e r s t o t h e l n t e r p r e t a t l o n of a symbol system ( I n t h l s case language ) i n t o some o t h e r domain.",33,34
6577,219309129,A13 o t h e r symbol s t r u ct u r e s whlch a r e r e f e r r e d t o a s semantlc a r e r e a l l y supports f o r t h e deductive apparatus.,6,7
6578,219309129,"In a system whlch does contaln a percept,ual component t h e r e has t o be some f a c l l l t y f o r t r a n sl a t f n g between t h e perceptual a n a l y s r s and t h e l l ng u l s t i r ; analysis, I n order t o deal witht h e Itsemant i c content"" of sentences and percepts we must provide t h e p o t e n t i a l f o r crosssmodallty and e x t r a -l i n g u l s t l c correspondence, I have sdggested t h a t ~t h e most parslmonlous n e w of how t h l s occurs 1s that) t h e end products of both perceptual and l l n g u l s t l c analyses a r e c o n c e p t u a l , s t r u c t u r e s , o r expressions i n a s i n g l e symbol systcin whlch we c a l l mentalese.",208,209
6579,219309129,There have sometimes been objections t o t h e view t h a t percepts a r e conceptually anhlysed i n t o articulated symbol systems.,26,27
6580,235417379,"The tools all had fictitious names to ensure that participants were unfamiliar with them, and the tools varied along a number of feature dimensions including color, shape, size, texture, symbol, and pattern.",34,35
6581,10105041,"Specifically, when the non-terminal symbol X is derived from its covered span f (X), the boundary tags should be updated.",7,8
6582,14072263,"Additionally, work in symbol grounding has supported robotic actions based on natural language interactions (Jacobsson et al.,",4,5
6583,14072263,These translation functions support symbol grounding and enable the robot to use both semanticallydescribed information along with sensed data.,4,5
6584,219310228,"w h i i e a n y symbol c a n b e u s e d t o r e p r e s e n t mr o b j e c t o r p a r t t h e r e o f i n a d e g c r i p t i o n , t h e p a r t i c u l a r r e p r e r a n t a t i o n o f such i n a n image i s c o n e t r a l ned by o t h e r r e p r e 8 e n t a t i o n r --q i v e n t h a t t h e i n t e r p o r k i o n e p a t i a l r e l a t i o n s m u s t be r e t a i n e d i n t h e imaqe r e p t e e e n t a t i o n . %",8,9
6585,56976,"Most importantly, categories are expanded in a demand-driven fashion, with information being percolated both bottom-up from the lexicon and top-down from the grammar's start symbol.",33,34
6586,3890388,"But what else did he, or anyone else in FS, offer but symbol-to-symbol transformations?",14,15
6587,3890388,"But what else did he, or anyone else in FS, offer but symbol-to-symbol transformations?",18,19
6588,735221,"For each case, we implemented generators that allowed for either the initial symbol or the entire token to be deleted (e.g., @Hertz to Hertz, @Hertz to ).",13,14
6589,208175606,"This allows the conjugate Dirichlet distribution to integrate to one, leaving the marginalized joint likelihood expression with the normalizing constants: p(X|z) = p(X|z, Φ)p(Φ|β)dΦ (2) = Γ(W β) Γ(β) W K K k=1 W w=1 Γ(n k U,w + β) Γ(n k U + β) , where W is the vocabulary set; n k U,w is number of times word w is assigned topic k in all U utterances of the document collection; n k U is number of times topic k appears in U ; and the symbol Γ refers to the Gamma function.",104,105
6590,18653017,Dialogue Manager The Dialogue Manager (DM) is activated by a message that includes an activation context symbol.,18,19
6591,150369071,"Hence, the symbol w 1:n refers to concatenated string of n vectors w 1 , w 2 , ..., w n .",3,4
6592,5816981,"In the table, the symbol † denotes the lowest accuracy which has a significant improvement over the baseline at p=0.05 for the two models.",5,6
6593,5816981,The symbol ‡ denotes the adding of a single feature yields a significant improvement for the model at p=0.005.,1,2
6594,5702549,"1991) ), and it is not something that simply gets replaced by a predicate symbol of the same arity.",16,17
6595,18251836,"A very simple case is legal-notice, which invariably contains the copyright symbol or the word itself.",14,15
6596,69289238,"We present a new method for unsupervised learning of multilingual symbol (e.g. character) embeddings, without any parallel data or prior knowledge about correspondences between languages.",10,11
6597,69289238,The learned representations open the possibility of fully unsupervised comparative studies of text or speech corpora in low-resourced languages with no prior knowledge regarding their symbol sets.,27,28
6598,69289238,"In an extreme case, two corpora may use completely distinct symbol sets, e.g. different scripts.",11,12
6599,69289238,"To be applicable to extreme cases of very little overlap between symbol vocabularies (e.g. different scripts, or types of phonological transcription), it does not assume a correspondence even between common symbols.",11,12
6600,69289238,We present a method that is able to discover similarities between inter-lingual symbol pairs by exploiting similarities between their respective intra-lingual distributions over contexts of occurrence.,14,15
6601,69289238,This demonstrates its potential to recover correspondences between symbol pairs on the basis of distributional statistics without any other connection between the observed corpora.,8,9
6602,69289238,"Whilst their model shares information between languages, we focus on modeling commonalities at the level of symbol embeddings.",17,18
6603,69289238,A particular area where symbol alignment is required is cognate discovery -finding words with a common linguistic origin.,4,5
6604,69289238,The issue of cross-lingual symbol alignment also arises in other tasks and similar approaches are used.,6,7
6605,69289238,"'s learning problem is similar to ours, applied to word meaning rather than symbol correspondence.",14,15
6606,69289238,"Whilst a similar technique could perhaps be applied to the present task, our method focuses specifically on similarities in local contexts of symbol use, rather than similarities in the structure of embedding spaces, which are less informative in the case of small vocabularies of characters or phonemes.",23,24
6607,69289238,"Each side (L-ngram and R-ngram) may be a single symbol, represented by the symbol's embedding (which becomes L-vec/R-vec), or a bi-or tri-gram, whose embeddings are concatenated and projected by a linear transformation to get a vector for the ngram, L-vec or R-vec.",15,16
6608,69289238,"Each side (L-ngram and R-ngram) may be a single symbol, represented by the symbol's embedding (which becomes L-vec/R-vec), or a bi-or tri-gram, whose embeddings are concatenated and projected by a linear transformation to get a vector for the ngram, L-vec or R-vec.",20,21
6609,69289238,"One example is cognate discovery, where the learned similarities may bring advantages over the assumed or initial correspondences used in related work, for example where distinct symbol sets are used.",28,29
6610,69289238,"We plan to apply XSYM to other symbol sequences, in particular, to sequences of phonetic symbols from speech (like List, 2014) .",7,8
6611,69289238,"It is able to learn comparable representations of symbols from multiple languages that use distinct symbol sets, learning to exploit similarities in the context distributions of the symbols across languages, even though the symbols in the contexts are also drawn from distinct vocabularies.",15,16
6612,69289238,Discovered strong symbol correspondences (especially if the method is applied to phonetic sequences) could also be of typological interest in themselves.,2,3
6613,69289238,The method presented is a generic representation learning technique for symbol sequences.,10,11
6614,3904621,"A rule with more than one symbol on the left-hand side makes no sense in the PATR II conception of grammar, but it makes perfectly good sense when the function of a rule is to change one string of categories into another string, as in the COMIT conception.",6,7
6615,9879031,"Thus the implication symbol → is a ""shortcut"" in that it represents elaborations whose arguments are in the same EDU.",3,4
6616,219300721,"for sj <-so, si, s2,..., s""..4 do Let currentsink be source; For each symbol a of si do currentsink f-update(currentsink, a); Return source.}",22,23
6617,219300721,Check for Existing Outgoing Edge and Update Frequencies Assume the symbol currently being scanned is a. We need to check the current sink whether there is already an outgoing edge labelled a before we create a state named new-sink.,10,11
6618,219300721,This implies that the current symbol has contributed one more ffscount to the strings represented by s'.,5,6
6619,3889052,In a CFG based grammar one associates with each category symbol a complex of features that are exploited by the grammar in a variety of ways.,10,11
6620,3889052,The notion of nonterminal symbol is flexible now.,4,5
6621,13532125,"Unification is a very powerful symbol manipulation tool, apart from anything else.",5,6
6622,2139969,Using the distinguished symbol when with the syntax (when trigger consider-trying b.ody) or completely equivalently using the distinguished symbol to with the syntax (to trigger consider-trylng body) creates a plan [high level goal-oriented procedure] that can be invoked by pattern directed invocation by a trigger Which matches trigger.,3,4
6623,2139969,Using the distinguished symbol when with the syntax (when trigger consider-trying b.ody) or completely equivalently using the distinguished symbol to with the syntax (to trigger consider-trylng body) creates a plan [high level goal-oriented procedure] that can be invoked by pattern directed invocation by a trigger Which matches trigger.,22,23
6624,3929627,"Connectionist and Heuristic Search Models For most of its history, the heuristic search, logic, and ""physical symbol system"" [19] paradigms have dominated AI.",20,21
6625,3929627,"AI has stuck almost exclusively with heuristic search and symbol systems, using them in a wide variety of natural language processing models and programming languages, ranging from ATN's, most other natural language parsing systems, and planning based models (e.g. for pragmatics) to Prolog and Planner [9] .",9,10
6626,3905296,"But I think this is a general failing of almost all NLP programs currently in existence: the meaning of a word is best represented not as a symbol, but as an aggregate of connected mlcrofeatures.",28,29
6627,219307489,"But where i n a symbol s y s & m , such a s l a n g u a g e , s h a l l ( w e l o o h f o r e v i d e n c e of t h e i r r e l a t i o n t o s o c i a l s t r u c t u r e s ?",5,6
6628,219307489,"I n h i s c l a s s i c a l t r e a tment of symbol systems, Itelson Goodman (1968) focused e$clusively on the o b j e c t i v e o r informative o r semantic a s p e c t s of symbols, t h a t i s , t h e r e l a t i o n t h a t e x i s t s between t h e symbol and t h e event k t rep r e s e n t s , denotes, expresses oa exemplifies.",20,21
6629,219307489,"I n h i s c l a s s i c a l t r e a tment of symbol systems, Itelson Goodman (1968) focused e$clusively on the o b j e c t i v e o r informative o r semantic a s p e c t s of symbols, t h a t i s , t h e r e l a t i o n t h a t e x i s t s between t h e symbol and t h e event k t rep r e s e n t s , denotes, expresses oa exemplifies.",89,90
6630,219307489,"And the second dimension, represented by t h e v e r t ic a l a x i s , i s t h a t s p e c i f y i n g a r e l a t i o n between symbol and meaning o r between s i g n j f i e r and s i g n if i e d .",48,49
6631,219307489,"The f i r s t we may c a l l the s o c i a l o r i n t e rpersonal meaning of a eymbol and t h e second t h e l o g i c a l meaning of t h a t symbol, l i n g u i s t i c o r otherwise.",51,52
6632,219307489,"t h e r e q u e s t assumes a d i ff e r e n t i a l i n f e r i o r i t y of t h e speaker and t h e d e c l a ra t i v e assumes, perhaps, t h e e q u a l i t y of t h e p a r t i c ip a n t s , The p o i n t I w i s h t o emphasize i s t h a t a n u tt e r a n c e is s i m u l t a n e o u s l y doing two t h i n g s --i t i s s p e c i f y i n g t h e l o g i c a l r e l a t i o n between symbol and r e f e r e n t , t h e v e r t i c a l dimension of F i g u r e 1, and i t i s s p e c i f y i n g a s o c i a l r e l a t i o n between t h e i n t e rlocuto'rs.",170,171
6633,219307489,"Together, t h e y c o n t r i b u t e t o t h e meaning of t h e symbol, u t t e r a n c e o r e x p r e s s i o n .",26,27
6634,59849058,"I n s t e a d , t h e e s s e n c e o f t h e c o n c e p t o r t o k e n i s c a p t u r e d i n a s e t o f f e a t u r e s a s s o c i a t e d w i t h t h e symbol.",80,81
6635,59849058,"The p r o c e s s o f r e f e r e n t i d e n t i f i c a t i o n w i l l a t t e m p t t o i s o l a t e one t o k e n , o r second b e s t , a s e t of c a n d i d a t e t o k e n s f o r e a c h c o n c e p t symbol i n t h e incoming g r a p h by m e a n s o f a f e a f u r e -i n t e r s e c t i n g a l g a r i t h m d e s c r i b e d i n (Rl).",101,102
6636,3912478,"But where in a symbol system, such as language, shall we look for evidence of their relation to social structures?",4,5
6637,3912478,"In his classical treatment of symbol systems, Nelson Goodman (1968) focused exclusively on the objective or informative or semantic aspects of symbols, that is, the relation that exists between the symbol and the event it represents, denotes, expresses or exemplifies.",5,6
6638,3912478,"In his classical treatment of symbol systems, Nelson Goodman (1968) focused exclusively on the objective or informative or semantic aspects of symbols, that is, the relation that exists between the symbol and the event it represents, denotes, expresses or exemplifies.",35,36
6639,3912478,Again note that the reality principle has to do with the relation between symbol and referent while the latter has to do with the relation between speakers.,13,14
6640,3912478,Let us consider what some of these social constraints on the meaning of a symbol may be.,14,15
6641,3912478,"The point I wish to emphasize is that an utterance is simultaneously doing two things--it is specifying the logical relation between symbol and referent, the vertical dimension of Figure i, and it is specifying a social relation between the interlocutors.",23,24
6642,3912478,"Together, they contribute to the meaning of the symbol, utterance or expression.",9,10
6643,3912478,All sentences appear to do both simultaneously--as we have suggested the symbol simultaneously stands in a specifiable relation to a meaning--it represents a proposition--and in a specifiable relation to the speaker and his listener.,13,14
6644,1662628,"We apply the ""Evidence"" method in the ""Dice Factory"" setting of MacKay and Peto (1994) , to obtain a pseudo-count `6 0a for every symbol ¥ cb d by treating each ( ¤b & as a sample of (not from) a random process e fC ¥ hg ( iG , in a Multinomial/Dirichlet setting.",33,34
6645,1662628,We perform an approximate maximization of i nu S S using a simulated annealing procedure in which each trial move takes a symbol ¥ or ( out of the cluster to which it is tentatively assigned and places it into another.,22,23
6646,15037905,The cornerstone of mainstream AI is the idea of symbol manipulation.,9,10
6647,405662,"x n , with the first and last being a special boundary symbol x 1 = # ∈ Σ which is never deleted, mutated, or created.",12,13
6648,405662,"In this case, the conditioning environment is t = x i and the current rightmost symbol p in y i .",16,17
6649,6375093,"A labelled-mark, an interval bar, and a bar with an attached label are some of the more familiar symbol classes available in SAGE.",22,23
6650,6375093,Each symbol class consists of a definition of the spatial relationship among a set of graphemes and the correspondence between the parameters of this set and attributes types in a data set.,1,2
6651,6428271,"A dependency structure D over a sentence is a set of dependencies (arrows) which form a planar, acyclic graph rooted at the special symbol ROOT, and in which each word in sentence appears as an argument exactly once (Klein and Manning, 2004) .",26,27
6652,49908618,ros2vhmsg annotates its messages with a special metadata symbol and uses that information to avoid processing messages that it already published.,8,9
6653,17492482,"Here, the =⇒ symbol expresses a causal directly increases relationship between two BEL functions.",5,6
6654,14453288,"Background Probabilistic context-free grammars Let G = (T, N, S, R) be a Context-Free Grammar in Chomsky normal form with no useless productions, where T is a finite set of terminal symbols, N is a finite set of nonterminal symbols (disjoint from T ), S ∈ N is a distinguished nonterminal called the start symbol, and R is a finite set of productions of the form A → B C or A → w, where A, B, C ∈ N and w ∈ T .",67,68
6655,14453288,"Further, let G A = (T, N, A, R), i.e., a CFG just like G except that the start symbol has been replaced with A, so, P G A (t|θ) is the probability of a tree t whose root node is labeled A and P G A (w|θ) is the sum of the probabilities of all trees whose root nodes are labeled A with yield w. The Inside algorithm takes as input a PCFG (G, θ) and a string w = w 0,n and constructs a table with entries p A,i,k for each A ∈ N and 0 ≤ i < k ≤ n, where p A,i,k = P G A (w i,k |θ), i.e., the probability of A rewriting to w i,k .",27,28
6656,2119628,"It generates ""predictions"" for each input symbol in turn.",8,9
6657,2119628,"This distribution, along with the actual value of the preceding few characters, is used to predict each upcoming symbol.",20,21
6658,2119628,"To encode the next symbol, it starts with the maximum-order model (order 5).",4,5
6659,2119628,"The model in Table 1 is used as follows: Suppose the character following tobeornottobe is o. Since the order 2 context is be, and the upcoming symbol has already been seen once in this context, the order 2 model is used for encoding in this case, and the encoding probability is 1/2.",28,29
6660,2119628,Thus the symbol o would be encoded in 1 bit.,2,3
6661,2119628,"Figure 5 (a) shows two states per symbol for order 1, there are three states per symbol for order 2, five for order 3, eight for order 4, thirteen for order 5, and so on.",9,10
6662,2119628,"Figure 5 (a) shows two states per symbol for order 1, there are three states per symbol for order 2, five for order 3, eight for order 4, thirteen for order 5, and so on.",19,20
6663,210054698,"Jäger proposes a similarity metric based on weights for symbol pairs given by pointwise mutual information, the values for which were learned from a training set of cognate pairs.",9,10
6664,210054698,"He uses two methods to encode a phonetic symbol into vector, a onehot encoding and one based on phonetic features, achieving better performance with one-hot encodings for two out of three language families.",8,9
6665,14212250,The ontological dimension The formalisms we want to compare are all based on the use of symbol structures to represent meaning.,16,17
6666,14212250,"Each symbol or structure of symbols plays a role in reasoning processes which underlie language activities, and there are a number different approaches to dealing with them.",1,2
6667,14212250,"In stating that ""primitives are to be found in all natural language understanding systems"" (1977, p. 19 ) he seems to be using the term 'primitive' to cover any formal symbol used in a semantic system.",36,37
6668,3920851,"In other words, thought is a symbol manipulation process.",7,8
6669,3920851,"The much misused term ""semantics"" refers to the interpretation of a symbol system (in this case language ) into some other domain.",13,14
6670,3920851,2 All other symbol structures which are referred to as semantic are really supports for the deductive apparatus.,3,4
6671,3920851,"I have suggested that the most parsimonious view of how this occurs is that the end products of both perceptual and linguistic analyses are conceptual structures, or expressions in a single symbol system which we call mentalese.",32,33
6672,3920851,There have sometimes been objections to the view that percepts are conceptually analysed into articulated symbol systems.,15,16
6673,3920851,"The problem here is that it is still not very clear what the force of the claim is when we say that concepts, qua interpreted symbol ~, are innate.",26,27
6674,218974538,"Despite Luxembourg having three official languages, i.e. French, German and Luxembourgish, only the latter was recognized as the unique national language of the country in 1984 and has become an important symbol for national identity (Gilles, in press) since.",34,35
6675,218973912,"In 1984, the Luxembourgish language, which arised out of a Central Franconian dialect, was allocated the status of the only national language of the country and is an essential symbol for national identity today.",32,33
6676,14497125,"We can replace all instances of [SA in the path-based inference rules of Section 2 by the composition SUB-/SUP and still have valid rules except that we now have paths on the left of the ""÷"" symbol.",41,42
6677,14497125,"Let us, therefore, extend our syntax of path-based inference rules to allow a path of arc compositions on the left of the ""÷"" symbol.",29,30
6678,17493348,"The first type is a semantics-based context free grammar where each non-terminal symbol represents a semantic tag (indicating semantic information such as the semantic type of an object, etc).",16,17
6679,17493348,"Each word (i.e., the terminal symbol) in the lexicon relates to one or more semantic tags.",7,8
6680,3917961,"The general form of each grammar rule is: {Rule (name) priority: <priority) in (packet) <pattern) --> <action)} Each pattern is of tl~e form : [(description of 1st buffer constituent)] [<2nd)] [<3rd>] The symbol ""="", used only in pattern descriptions, is to be read as ""has the feature(s)"".",61,62
6681,3917961,"The symbol ""V"" signifies the fact that the subject position of (2c) is filled by an NP that dominates no lexical structure. (",1,2
6682,3917961,A packet of grammar rules would tllen be explicitly associated with each symbol oil the right hand side of each phrase structure rule.,12,13
6683,8567444,"3) above by grouping or merging symbols (in this case, regions) and then treating the group as a new symbol to be aligned.",23,24
6684,16930212,"In one version of the corpus, we replaced sequences of user spellings with the tag ""SPELL"" and disfluencies with the symbol ""DISF"".",23,24
6685,250390469,We use the ¬ symbol to denote negation.,4,5
6686,1139492,2014) whereby each value has been replaced by a symbol representing its corresponding slot.,10,11
6687,237420912,s] is a special delimiter symbol.,6,7
6688,218974092,"Posts on Twitter or Facebook may, for instance, 'end' a sentence with an emoticon, an asterisk or a pipe symbol rather than a punctuation symbol.",24,25
6689,218974092,"Posts on Twitter or Facebook may, for instance, 'end' a sentence with an emoticon, an asterisk or a pipe symbol rather than a punctuation symbol.",29,30
6690,219682425,"Here, the N T symbol stands for a non-terminal.",5,6
6691,219682425,"When a non-terminal is predicted, the subsequent symbol or token is predicted by applying the decoder to the hidden vector representation of the non-terminal.",10,11
6692,219682425,The $ symbol indicates a terminal.,2,3
6693,42957292,"We mark this dependence with the ""∼"" symbol.",9,10
6694,193721,"We mark this dependence between the two phrases with the symbol ∼. For instance, for the German equivalent of make in (5), machen, we would mark Effect:AN∼BM not specifying + or -, where AN represents the accusative object and BM (adverbial of manner) covers the adjectival secondary predicate.",10,11
6695,193721,"For this unit, we construct a combination between a given phrase label and the term ""Context"", connected by the ∼ symbol.",24,25
6696,102480676,"Cuz I need a lean', used to talk about hiring players due to new hires typically leaning on a Liverpool symbol when posing for a photo right after signing for the club.",22,23
6697,203690693,"We consider a special symbol End as another destination of state transition, which triggers the termination of the state sequence at that level.",4,5
6698,195600,S: An S is a complete tree headed by a predicate (i.e. S is the start symbol).,18,19
6699,18892448,This extension is done by creating a copy of the given SOG and adding to its end an intra-group relation symbol defined in Section 2.2 corresponding to the given label and group.,22,23
6700,1984316,"Specifically, this action measures whether the system is able to generate the special [kbquery] symbol to initiate a KB query, as well as how accurate the corresponding KB query arguments are.",17,18
6701,3249207,"That is, the JAPE rule would not be able to extract events that cross the boundary of sentence symbol.",19,20
6702,5373460,"Indeed, all acceptors A k become transducers T k where the first transition emits the symbol <Ck> and the last transition the symbol </Ck>.",16,17
6703,5373460,"Indeed, all acceptors A k become transducers T k where the first transition emits the symbol <Ck> and the last transition the symbol </Ck>.",25,26
6704,5373460,"Except these start and end tags, no other symbols are emitted: all words in the concept or background transducers emit an empty symbol.",24,25
6705,18541604,"The set of context independent (CO units we used in this study is a fixed set of 47 phone-like units (PLU's), in which each PLU is associated with a linguistically defined phoneme symbol.",39,40
6706,320447,"Raw posts from Twitter included mentions (the symbol @ followed by a user name), quotes (written with letters ""RT""), hashtags (a word preceded by the symbol #), and URLs.",8,9
6707,320447,"Raw posts from Twitter included mentions (the symbol @ followed by a user name), quotes (written with letters ""RT""), hashtags (a word preceded by the symbol #), and URLs.",34,35
6708,10975522,We first collect all sets of PLU's such that all the units which have the same middle phone symbol p are grouped into the same set S e regardless of their context and environment.,19,20
6709,51918733,<arb> is a symbol that accepts any word.,5,6
6710,1886389,The ∼ symbol refers backwards to InsectWorm in this case.,2,3
6711,16996042,The feature value is also not defined (ND) if the token itself is a punctuation symbol or contains any special symbol or digit.,17,18
6712,16996042,The feature value is also not defined (ND) if the token itself is a punctuation symbol or contains any special symbol or digit.,22,23
6713,16996042,"Contains symbol: This binary valued feature has been incorporated to check whether the current token contains any special symbol (e.g., %, $ etc.).",1,2
6714,16996042,"Contains symbol: This binary valued feature has been incorporated to check whether the current token contains any special symbol (e.g., %, $ etc.).",19,20
6715,16996042,"The ME based system has demonstrated the best accuracy of 81.75% for the development set with the context window of size three, i.e., previous one, current and the next one words, prefixes and suffixes of length up to three characters of the current word only, dynamic POS information of the previous word, NE tag of the current word, symbol feature, length of the word and features extracted from the lexicon and the inflection lists.",66,67
6716,19016820,"The symbol u is target word embedding, and the symbol v is context word embedding, where n and m represent size of u and v embeddings respectively.",1,2
6717,19016820,"The symbol u is target word embedding, and the symbol v is context word embedding, where n and m represent size of u and v embeddings respectively.",10,11
6718,19016820,The symbol λ is an weighting parameter between CDMScore(y) and DepScore(y).,1,2
6719,19016820,"The symbol m is the modifier word of the dependency word pair, h is the head word, freq(m,h) is frequency of the (m,h) dependency, and freq(h) is frequency of the h. If freq(m,h) is 0, we will replace it by 1/total word dependency.",1,2
6720,248780294,"The two examples are separated by two newlines and a separator token used during generation as the stop symbol (i.e., ###).",18,19
6721,248780294,"Finetuning GPT-2 (FT GPT-2) Each part of the input (supporting document, question, definition) is prepended with a new special symbol (i.e., <con-text>, <question>, <definition>) and the model is trained in the standard causal language model loss.",25,26
6722,920235,Both source and target language words were prefixed with a begin symbol B and suffixed with an end symbol E which correspond to start and end states.,11,12
6723,920235,Both source and target language words were prefixed with a begin symbol B and suffixed with an end symbol E which correspond to start and end states.,18,19
6724,247793069,"The symbol means the predicting sentiment is correct, and the other symbol means the predicting sentiment is wrong.",1,2
6725,247793069,"The symbol means the predicting sentiment is correct, and the other symbol means the predicting sentiment is wrong.",12,13
6726,28214,"Because the token whose head is not found will not be tagged with ""r"", a unique symbol ""O"" is enough to express this case.",19,20
6727,1026566,"Notes: There is one non-terminal symbol. """,8,9
6728,1026566,"UW"" denotes ""unknown word"" and is also the start symbol.",12,13
6729,10911994,Both Hindi and English words were prefixed with a begin symbol B and suffixed with an end symbol E which correspond to start and end states.,10,11
6730,10911994,Both Hindi and English words were prefixed with a begin symbol B and suffixed with an end symbol E which correspond to start and end states.,17,18
6731,10911994,The 50 most frequent of these character sequences were added to English symbol inventory.,12,13
6732,10911994,"The English training words were re segmented based on the new symbol inventory, i.e., if a character was a part of an n-gram, it was grouped with the other characters in the ngram.",11,12
6733,948365,"Tables 1 and 2 show some keywords in Turner's taxonomy, and the symbol ""//"" is to separate different emotion types.",14,15
6734,6178751,"That noun was replaced by the symbol [?],",6,7
6735,824837,"We denote ρ(t) to be the root symbol of tree t. When writing these rules, we avoid notational overhead by introducing a short-hand form from Galley et al. (",8,9
6736,824837,"Following TSG terminology (see Figure 2 ), we call these ""variable nodes"" such as x 2 :NP-C substitution nodes, since when applying a rule to a tree, these nodes will be matched with a sub-tree with the same root symbol.",50,51
6737,824837,"If r = (t, s, φ) is a rule, and d i is a (sub-) derivation with the root symbol of its source projection matches the corresponding substitution node in r, i.e., ρ(E(d i )) = φ(x i ), then d = r(d 1 , . . . ,",26,27
6738,824837,"Similarly, we now marginalize over all derivations D(τ * ) = {d | E(d) = τ * } that translates English tree τ into some Chinese string and apply the Viterbi approximation again to search for the best derivation d * : c * = C(d * ) = C(argmax d∈D(τ * ) Pr(d)) (6) Assuming different rules in a derivation are applied independently, we approximate Pr(d) as Pr(d) = r∈d Pr(r) (7) where the probability Pr(r) of the rule r is estimated by conditioning on the root symbol ρ(t(r)): Pr(r) = Pr(t(r), s(r) | ρ(t(r))) = c(r) r :ρ(t(r ))=ρ(t(r)) c(r ) (8) where c(r) is the count (or frequency) of rule r in the training data.",102,103
6739,19012550,"auf Muenzen das Zei= <lb n=""0001.24"" /> chen der ersten Stadt The hyphenation itself should not be visual later in the rendered representation of the XML document, so we have removed the delimiter symbol and defined this syllable division as an attribute rend of the surrounding <w> element.",37,38
6740,19012550,"This attribute has to represent the correct spelling of the hyphenated word without any delimiter symbol For transforming these annotations according to our TEI schema, we used our transforming technology FNTRANSFORM which is implemented in PROLOG.",15,16
6741,1596215,She becomes an Iranian symbol.,4,5
6742,512101,We can encode each symbol of an alphabet V using log |V | bits so encoding a string s of length |s| from alphabet V takes C V (s) = log |V |(|s| + 1) bits (we add an extra string termination symbol for separability).,4,5
6743,512101,We can encode each symbol of an alphabet V using log |V | bits so encoding a string s of length |s| from alphabet V takes C V (s) = log |V |(|s| + 1) bits (we add an extra string termination symbol for separability).,46,47
6744,423521,"The impression conveyed by Sproat ( 2010 ) is that we are claiming such similarity by itself is sufficient to prove that the Indus script, or indeed any symbol system, is linguistic.",29,30
6745,423521,Figure 1 (b) plots the block entropies of various types of symbol sequences as the block size is increased from N = 1 to N = 6 symbols.,13,14
6746,423521,"without a true decipherment, we formulate the question as an epistemological problem, namely, one of estimating the posterior probability of the hypothesis H L that an unknown symbol sequence represents language, given various properties P 1 , P 2 , P 3 , . . .",30,31
6747,423521,"3) Use of Diacritical Marks: Indus symbols are often modified by the addition of specific sets of marks over, around, or inside a symbol.",27,28
6748,423521,"This is similar to linguistic scripts, including later Indian scripts which use such ligatures and diacritical marks above, below, or around a symbol to modify the sound of a root consonant or vowel symbol. (",25,26
6749,423521,"This is similar to linguistic scripts, including later Indian scripts which use such ligatures and diacritical marks above, below, or around a symbol to modify the sound of a root consonant or vowel symbol. (",36,37
6750,423521,"5) Syntactic Structure: The script exhibits distinct language-like syntactic structure including equivalence classes of symbols with respect to positional preference, classes of symbols that function as beginners and enders, symbol clusters that prefer particular positions within texts, etc. (",35,36
6751,423521,"In other words, we now know that various types of symbol sequences, from natural sequences such as DNA and proteins to man-made systems such as music and Fortran, occupy quite different entropic ranges compared to linguistic systems (Figure 1(b) ; Figure 8 in Schmitt and Herzel [1997] ).",11,12
6752,16939574,"1: A typical example of Indus sign sequence (having 8 distinct signs) occurring at the top of a seal, with the picture of a ""unicorn"" in the foreground (i.e., the field symbol), one of the common animal motifs observed in such artifacts.",39,40
6753,16939574,"1: A typical example of Indus sign sequence (having 8 distinct signs) occurring at the top of a seal, with the picture of a ""unicorn"" in the foreground (i.e., the field symbol), one of the common animal motifs observed in such artifacts.",39,40
6754,16939574,"None of them show any significant correlation with the modules, implying that the signs are not simple attributes of either artifact or symbol portrayed in a seal, nor were the use of certain sign subsets confined exclusively to certain regions.",23,24
6755,12048308,"A possible way to reconstruct such a proto-language is by multiple aligning (see Prokić, 2009) all pronunciations of a single word and use the most frequent phonetic symbol at each position in the reconstructed word.",32,33
6756,53083290,Each symbol in the input text is represented by a state which emits this symbol on a single transition that moves to the next state.,1,2
6757,53083290,Each symbol in the input text is represented by a state which emits this symbol on a single transition that moves to the next state.,14,15
6758,53083290,The transition emitting the final symbol in the string leads to the sole accepting state.,5,6
6759,53083290,"We follow each possible arc from the current FST state, each producing a new child beam element, and feed the output symbol into the RNN (unless it is ).",23,24
6760,219307791,"The non-terminal symbols are syllables (Syll), metrical-feet (Ft), prosodic words (PrWd), and the start symbol (S).",27,28
6761,1906241,"Each noun appears only once in a given row of the grid; if a noun appears multiple times, its grid symbol describes the most important of its syntactic roles: subject if possible, then object, or finally other.",22,23
6762,52008842,"The symbol ""in_core"" denotes the in-domain center and ""out_core"" indicates the out-of-domain center.",1,2
6763,52008842,"In this figure, each symbol indicates one sentence which is represented by the normalized sentence embedding output σ s i from a softmax operation.",5,6
6764,6863482,Table 2 shows a comparison between the AREncDec based models (the models with symbol) in which the proposed models significantly reduce the slot error rate across all datasets by a large margin about 2% to 4% that are also improved performances on the BLEU score when comparing the proposed models against the previous approaches.,14,15
6765,13451336,We can eliminate the existential quantifier from the formulae in (6) via replacing the variables bound by the existential quantifier with a new function symbol f in (6a) or with a Skolem constant in (6b).,26,27
6766,239768421,"However, out of multiple neighbors for a web element, only a few are informative, e.g., a web element having a currency symbol near a set of digits seems relevant.",25,26
6767,2887257,2015) observed better performance of the LTG-CNN when substituting the subject mention with a special symbol.,18,19
6768,16276879,"The notation adopted here observes the convention that the argument symbol is always to the right of the slash and the result symbol is to the left, no matter which order function and argument combine in.",10,11
6769,16276879,"The notation adopted here observes the convention that the argument symbol is always to the right of the slash and the result symbol is to the left, no matter which order function and argument combine in.",22,23
6770,16276879,"The symbol S stands for clauses, to which I add the features such as fin (finite) or stat (stative) indicated by the subscripts since these features influence the nominative marking of arguments.",1,2
6771,5226638,We build a finite-state transducer × x i that maps symbol to x i in the counting semiring.,12,13
6772,5226638,"To enumerate the alternative spellings for a misspelled word, we generate all the words in onecharacter edit distance with the input word, where we consider one symbol insertion, deletion or substitution, or transposition of adjacent symbols.",28,29
6773,6105200,"As a result, transformed trees with unfinished structure have the representation of Figure 2 , which gives away the positive benefits of the right-corner transform in representing repair by propagating a special repair symbol (EDITED) through the grammar.",36,37
6774,51918632,The run-time of recently developed probabilistic inference models for situated symbol grounding of natural language instructions depends on the complexity of the representation of the environment in which they reason.,12,13
6775,51918632,In this paper we propose a model of language and perception for the problem of adapting the configuration of the robot perception pipeline for tasks where constructing exhaustively detailed models of the environment is inefficient and inconsequential for symbol grounding.,38,39
6776,51918632,The results demonstrate that by adapting perception we get significant gains in terms of run-time for perception and situated symbol grounding of the language instructions without a loss in the accuracy of the latter.,21,22
6777,51918632,A perception system that does not use the context of the instruction when interpreting the observations would inefficiently construct detailed world model that is only partially utilized by the symbol grounding algorithm.,29,30
6778,51918632,"2013) frame the problem of language understanding as a symbol grounding problem (Harnad, 1990) .",10,11
6779,51918632,This model leverages the hierarchical structure of the syntactically parsed instruction and conditional independence assumptions across constituents of a discrete symbol space to improve the run-time of probabilistic inference.,20,21
6780,51918632,A limitation of current applications of probabilistic graphical models for natural language symbol grounding is that they do not consider how to efficiently convert observations or measurements into sufficiently detailed representation suitable for inference.,12,13
6781,51918632,"We differentiate our approach by expanding the diversity and complexity of perceptual classifiers, enabling verbs to modify object representations, and presenting an end-to-end approach to representation adaptation and symbol grounding using computationally efficient probabilistic graphical models.",34,35
6782,51918632,"2014) frame this problem as a symbol grounding problem, i.e. inferring the most likely set of groundings (Γ s * ) given a syntactically parsed instruction Λ = {λ 1 , ..., λ m } and the world model Υ. Γ s * = arg max γ 1 ...γn∈Γ s p(γ 1 ...γ n |Λ, Υ) (2) Here, the world model Υ is a function of the constructs of the robot's perception pipeline (P ), and the raw observations z t .",7,8
6783,51918632,"The set of all groundings Γ s = {γ 1 , γ 2 , ..., γ n } is called as the symbol space.",24,25
6784,51918632,Thus the symbol space forms a finite space of interpretations in which the instruction will be grounded.,2,3
6785,51918632,"To add this degree of freedom in the construction of the perception pipeline, we define additional set of symbols which we refer to as conditionally dependent perceptual symbols: Γ CD P = {γ e i ,e j |e i , e j ∈ E ; i = j} (10) The expression of the symbol γ e i ,e j refers to running the element e i from the perception pipeline on the subset of objects which were classified positive by the element e j .",60,61
6786,51918632,Finally the complete perceptual symbol space is: Γ P = {Γ ID P ∪ Γ CD P } (11) EXPERIMENTAL DESIGN Herein with our experiments we demonstrate the utility of our language perception model for the task of grounded language understanding of the manipulation instructions.,4,5
6787,51918632,"Γ P defines the set of perceptual symbols which are used by the language perception model, and Γ S defines the set of symbols which are used by the symbol grounding model.",30,31
6788,51918632,The elements e i ∈ E in our perception pipeline are selected such that they can model the robot's environment with a spectrum of semantic and metric properties which will be necessary towards performing symbol grounding and planning for all of the instructions in our corpus.,35,36
6789,51918632,"The space of symbols for the symbol grounding model is similar to the representation defined in (Paul et al.,",6,7
6790,51918632,"Thus the complete symbol space for the symbol grounding model is: Γ S = { Γ O ∪Γ L ∪, Γ C ∪Γ G ∪Γ R ∪Γ SR ∪Γ PC } (19) Corpus For training and testing the performance of the system we generate an instruction corpus using the linguistic patterns similar to that described in (Paul et al.,",3,4
6791,51918632,"Thus the complete symbol space for the symbol grounding model is: Γ S = { Γ O ∪Γ L ∪, Γ C ∪Γ G ∪Γ R ∪Γ SR ∪Γ PC } (19) Corpus For training and testing the performance of the system we generate an instruction corpus using the linguistic patterns similar to that described in (Paul et al.,",7,8
6792,51918632,One copy of the corpora is annotated for training LPM using (Γ P ) while another for training the symbol grounding model using (Γ S ).,20,21
6793,51918632,The second claim is that reasoning in the context of these optimal representations also reduces the inference run-time of the symbol grounding model.,22,23
6794,51918632,"In the third experiment, we compare the inference time of the symbol grounding model reasoning in the context of the adaptively generated optimal world models ( T 3 , eq.",12,13
6795,51918632,"Specifically, the learning characteristics of LPM, the impact of LPM on the perception run-time, and the impact the adaptive representations on the symbol grounding run-time.",27,28
6796,51918632,It shows that the symbol grounding run-time when reasoning in the context of detailed world models( Υ ) grows as a function of the world complexity.,4,5
6797,51918632,"While recent probabilistic frameworks have advanced our ability to interpret the meaning of complex instructions in cluttered environments, the problem of how language can channel the interpretation of the raw observations to construct world models which are necessary and sufficient for the symbol grounding task is not extensively studied.",43,44
6798,51918632,"This provides run-time gains in terms of both perception and symbol grounding, thereby improving the speed with which collaborative robots can understand and act upon human instructions.",12,13
6799,18979477,For example the probability of deleting a function word from a string is not the same as deleting a symbol in RNA structure.,19,20
6800,173990966,"Introduction A hashtag is a keyphrase represented as a sequence of alphanumeric characters plus underscore, preceded by the # symbol.",20,21
6801,173990966,"2016) sentiment analysis model under three configurations: (a) tweets with hashtags removed, (b) tweets with hashtags as single tokens excluding the # symbol, and (c) tweets with hashtags as segmented by our system, Hash-tagMaster.",29,30
6802,14320895,Then we have the following list of combinatory rules: ( 5) a. (>) X/Y Y ⇒ X (<) Y X\Y ⇒ X b. (>B) X/◇Y Y/◇Z ⇒ X/◇Z (<B) Y\◇Z X\◇Y ⇒ X\◇Z c. (>BB ×) X/×Y Y\×Z ⇒ X\×Z (<BB ×) Y\×Z X\×Y ⇒ X/×Z d. (>T) X ⇒ Y/ i (Y\ i X) (<T) X ⇒ Y\ i (Y/ i X) e. LWRAP: Y X↓◇Y ⇒ X We will simply omit the symbol '*' in the derivations hereafter.,119,120
6803,14320895,15) Backward Composition (<B): 1 Y\Z$ X\Y ⇒ X\Z$ where the symbol Z$ stands for Z and all lefttward-looking functor categories into Z. In derivation ( 14 ) we encounter another problem.,15,16
6804,13035084,"But they are pronounced as their respective alveolar counterparts ""න""-/n/ and ""ල""-/l/. Similarly, the symbol ""ෂ"" representing the retroflex sibilant /Í/, is pronounced as the palatal sibilant ""ශ""-/ß/. The corresponding aspirated symbols of letters ක, ග, ච, ජ, ට, ඩ, ත, ද, ප, බ namely ඛ, ඝ, ඡ, ඣ, ඪ, ථ, ධ, ඵ, භ respectively are pronounced like the corresponding unaspirates (Karunatillake, 2004) .",17,18
6805,13035084,"ර්""-/r/ immediately following a consonant can be marked by the symbol ""◌ "" added to the bottom of the consonant preceding it.",10,11
6806,13035084,"Similarly, ""ය්""-/j/, immediately following consonant can be marked by the symbol ""◌ "" added to the right-hand side of the consonant preceding it (Karunatillake, 2004) . """,13,14
6807,25434922,"Note that although the teacher speaks a word each time, the learner still has to learn to generate a fullsentence ended with an end-of-sentence symbol.",29,30
6808,2285483,"If its segmentation can be either C1C2/C3 or C1/C2C3 depending on context meaning, the C1C2C3 is called an overlap ambiguity string (OAS), such as ""將軍(a general)/用(use)"" and ""將(to get)/軍用(for military use)"" (the symbol ""/"" indicates a word boundary). (",47,48
6809,3107595,"The rule of type-raising for (first) object NPs would be written as in (28) , where the symbol 'VP' is used as an abbreviation for S/LNP: (28) NPobi => VP/(VPINP) The raised category for objects states that the objects are the expressions that combines with open verb phrases to yield verb phrases.",23,24
6810,52870582,"Note that the symbol ALL stands for a mixed collection of all BN, ED and BD word-pairs generated from the UDN 2001 corpus.",3,4
6811,19007354,"Then: • X -> Y Z denotes a horizontal production rule saying that the nonterminal X horizontally generates two symbols Y and Z. • X |-> Y Z denotes a vertical production rule saying that the nonterminal X vertically generates two symbols Y and Z. • X -> Y or X |-> Y equivalently denote a unary production rule saying that the nonterminal X generates a symbol Y. We assume that all rules are binary without loss of generality, since any grammar can be mechan-ically binarized without materially changing the parse tree structure, just as in the case of ordinary 1D grammars.",71,72
6812,192516,The symbol † indicates statistical significance over an appropriate baseline at p < 0.01 level.,1,2
6813,16950766,Let us assume further that the genitive case marker no serves as apredication operator designated by the symbol which recover propositional functions associated with nominalized predicate phrases.,17,18
6814,10335219,"If its segmentation can be either AB/C or A/BC depending on different context, the ABC is called an overlap ambiguity string (OAS), such as "" (a general)/ (use)"" and "" (to get)/ (for military use)"" (the symbol ""/"" indicates a word boundary); (2) Combination ambiguity (CA), take a character string AB as an example.",53,54
6815,51918763,"Model Architecture We Sentence Encoder Sentence encoder E transforms a symbol sequence (i.e., a sentence) into a continuous vector.",10,11
6816,51918763,"We inserted a special symbol "" ARTICLE BOUNDARY "" to each  We set the dimensionalities of the word embeddings, hidden states of the BiLSTM, and hidden layers of the MLPs to 100, 200, and 100, respectively.",4,5
6817,8586038,Out-of-vocabulary words are replaced with the unknown symbol unk .,11,12
6818,11960966,"The details of supplementary principles, symbol illustrations, semantic roles, phrasal structures and applications of the CCTB can be found in (CCTB portal; Chen et al.,",6,7
6819,8899978,"Note that the symbol (NV+NN+VV+AN+DV) stands for a mixed collection of all auto-generated meaningful NV, NN, VV, AN and DV word-pairs.",3,4
6820,946369,"Let 1 ( ,..., ) S sp c c = be a semantic pattern with S constituent concepts, i.e., length S. The concept combination is defined as 1 2 3 ((...( ) ) ... ), s S c c c c c ⊕ ≡ ⊕ ⊕ ⊕ ⊕ (4) where ⊕ denotes the symbol representing the combination operator over the HAL space, s c ⊕ denotes a new concept generated by the concept combination.",63,64
6821,9323396,"Equally importantly, we have the unary lexical rules that generate foreign words: X(e) → e/f To make the generative story complete, we also have a top rule that goes from the unlexicalized start symbol to the highest lexicalized nonterminal in the tree: S → X(e) Figure 1 (right), shows our example sentence's tree under the new model.",39,40
6822,23147164,"Throughout the paper, examples that are adapted from naturally occurring texts (novels), including (11), are marked with the dagger symbol ( †) at the end, and their sources are provided in Appendix A. Also, for ease of presentation, some long examples are presented in the form of: (i) the preceding context, (ii) the key segment, and (iii) the following context, where original Japanese texts and/or glosses are omitted from (i) and (iii). (",26,27
6823,7434269,"An HMM for discrete symbol observations is characterized by the following: -the state set Q ={q i }, where 1 ≤ i ≤ N, N is the number of states -the number of distinct observation symbol per state M -the state-transition probability distribution A={a ij }, where a ij =P[q t+1 =j|q t =i], 1 ≤ i,j ≤ N -the observation symbol probability distribution B={b j (k)}, where ] | [ ) ( j q v o P k b t k t j = = = , 1 ≤ i,j ≤ N -the initial state distribution π={π i }, where π i =P[o t =v k |q t =j] , 1 ≤ i,j ≤ M .",4,5
6824,7434269,"An HMM for discrete symbol observations is characterized by the following: -the state set Q ={q i }, where 1 ≤ i ≤ N, N is the number of states -the number of distinct observation symbol per state M -the state-transition probability distribution A={a ij }, where a ij =P[q t+1 =j|q t =i], 1 ≤ i,j ≤ N -the observation symbol probability distribution B={b j (k)}, where ] | [ ) ( j q v o P k b t k t j = = = , 1 ≤ i,j ≤ N -the initial state distribution π={π i }, where π i =P[o t =v k |q t =j] , 1 ≤ i,j ≤ M .",40,41
6825,7434269,"An HMM for discrete symbol observations is characterized by the following: -the state set Q ={q i }, where 1 ≤ i ≤ N, N is the number of states -the number of distinct observation symbol per state M -the state-transition probability distribution A={a ij }, where a ij =P[q t+1 =j|q t =i], 1 ≤ i,j ≤ N -the observation symbol probability distribution B={b j (k)}, where ] | [ ) ( j q v o P k b t k t j = = = , 1 ≤ i,j ≤ N -the initial state distribution π={π i }, where π i =P[o t =v k |q t =j] , 1 ≤ i,j ≤ M .",76,77
6826,7434269,The word itself is defined as a discrete symbol observation.,8,9
6827,3152657,"Let Y (v) be understood as a symbol observable on the node v, taking a value from an alphabet K = {k 1 , . . . ,",9,10
6828,3152657,"As there can be only one input labeling function, we treat the following ordered pair as the observed symbol: Y (v) = (L src (v), F src (v)) where L src (v) is the source-language lemma of the node v and F src (v) is its source-language formeme.",19,20
6829,17673190,"The following process is one of the simplification processes for making ""N"" -Change each noun (or each noun compound) to a symbol ""N"".",25,26
6830,17673190,"-Change ""pa (adjective)"" and ""pv (verb)"" to a symbol ""P"".",16,17
6831,9385494,"Note that, for the WikiAnswers dataset, we clip the vocabulary size 2 to 50K and use the special UNK symbol for the words outside the vocabulary.",22,23
6832,2156477,"An additional challenge of tweets is that many are ""near duplicates"" (e.g., shared tweets with an added symbol or comment).",21,22
6833,18425749,We created a hashtag from each term in the lexicon by appending a # symbol on the front and removing whitespace.,14,15
6834,5400507,"Hashtags are created by adding the '#' symbol as a prefix to a word or a multi-word phrase that consists of concatenated words without whitespace (e.g., #welovehashtags).",9,10
6835,5400507,"2007a; Mohammad, 2012) , and adding a '#' symbol as a prefix to these lexicon entries could potentially give us lists of emotion hashtags, it would be unlikely to find multi-word phrases or stylistic variations frequently used in tweets.",13,14
6836,5400507,"While it is possible that some of these tweets are actually positive instances for e, our hope is that the vast majority of them will not belong to emotion e. We experimented with feature options such as bigrams, unigrams with the '#' symbol stripped off from hashtags, etc.,",46,47
6837,2300310,The results of this test are indicated with the ⇓ symbol in Tables 3 and 4 .,10,11
6838,10205309,"Chinese Wordhood Roughly put, a Chinese character is regarded as an ideographic symbol representing syllable and meaning of a ""morpheme"" in spoken Chinese.",13,14
6839,14185426,"The body of a rule is a comma-separated list of constituents, of which there are four basic types: Rule constituents, embedded Prolog code, symbol lists and expansion macros.",29,30
6840,14185426,A symbol list may not be empty.,1,2
6841,237099271,We extend this approach to utilize transformer model's versatility to generate more complex constructs such as symbol representations.,17,18
6842,8788311,"Variations of SCFGs go back to Aho and Ullman (1972) 's Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997) , which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005) , which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003) , which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al. (",55,56
6843,2047959,"In the case of Google, only basic boolean operators are supported (AND, OR, NOT), and the function of the wildcard symbol (*) is limited, difficult to decipher and may have changed over time.",26,27
6844,9973873,Every byte of this 32-bit integer is considered as a separate symbol.,13,14
6845,9973873,We keep all such eight-symbol strings in a perfect hash.,6,7
6846,10195661,"These unclassified instances are tagged with the symbol ""?"".",7,8
6847,10195661,"The symbol "" "" in Table 3 denotes that the combination of the corresponding row and column is erroneous.",1,2
6848,10195661,"The symbol ""-"" denotes that no error can be detected by the table.",1,2
6849,10195661,"The words in brackets denote the corpora used to learn decision lists; the symbol ""+FB"" means that the feedback corpus was simply added to the general corpus.",14,15
6850,9330386," When reading, say, symbol a in State 2, we follow the failure link to 1, producing output D. Then we use the a-transition from 1 to arrive at State 3.",6,7
6851,9330386,"When reaching a state p and a text symbol σ such that δ(p, σ) is not defined we apply a series of failure transitions until we arrive at a state p n such that δ(p n , σ) = p is defined.",8,9
6852,9330386,We first complete the initial state q 0 adding loop transitions with any symbol σ ∈ Σ such that there is no outgoing σ-transition from q 0 in the trie.,13,14
6853,9330386,The transition output for a loop transitions with symbol σ is σ.,8,9
6854,9330386,Recall that in this case for each state q and each symbol σ ∈ Σ such that q does not have a σ transition in the transducer trie T D a new σtransition with suitable output needs to be added.,11,12
6855,9330386,"To create a wikilink, concept names such as ""United Kingdom"" or ""Kingdom University"" mentioned in the text are replaced by anchor elements of the form <a href=""/wiki/United_Kingdom""> United Kingdom</a> <a href=""/wiki/Kingdom_University""> Kingdom University</a> In order to automatize this form of hyperlinking, rewriting dictionaries may be used that list relevant concept names (e.g. ""United Kingdom"") together with the corresponding anchor elements Algorithm 1 Construction of an f-transducer for a given alphabet Σ and rewrite dictionary D. Trie(Σ, D) @1 q0 ← new state() @2 Q ← {q0} @3 δ ← ∅ @4 λ ← ∅ @5 ϕ ← ∅ @6 for α, β ∈ D do @7 q ← q0 @8 for each symbol σ of α do @9 p ← δ(q, σ) @10 if p = nil then @11 p ← new state() @12 Q ← Q ∪ {p} @13 δ(q, σ) ← p @14 λ(q, σ) ← ε @15 q ← p @16 F ← F ∪ {q} @17 ϕ(q) ← β @18 return Σ, Q, q0, δ, λ, ϕ, ∅ f-transducer(Σ, D) @1 Σ, Q, q0, δ, λ, ϕ, f ← T rie(Σ, D) @2 for σ ∈ Σ do @3 q ← δ(q0, σ) @4 if q = nil then @5 δ(q0, σ) ← q0 @6 λ(q0, σ) ← σ @7 else @8 if !",143,144
6856,9941829,The symbol ˆt means that t is not the preceding letter.,1,2
6857,17575562,"These unclassified instances are tagged with the symbol ""?"".",7,8
6858,16120013,"The symbol '~', as in HowNet, refers to the head concept of the definition which is 'school department|學系' as in (1).",1,2
6859,16120013,The symbol '~' denotes the head concept which is 'eat' in this example.,1,2
6860,7936880,These unclassified instances are tagged with the symbol '?'.,7,8
6861,7936880,"The symbol "" "" in Table 4 denotes that the combination of the corresponding row and column is erroneous.",1,2
6862,7936880,"The symbol ""-"" denotes that no error can be detected by the table.",1,2
6863,5843144,Dictionaries with single deletions are constructed by deleting the symbol at a fixed position n in all words of a given dictionary.,9,10
6864,5843144,"Here the primitive operations are the substitution of one symbol for another symbol, the deletion of a symbol, and the insertion of a symbol.",9,10
6865,5843144,"Here the primitive operations are the substitution of one symbol for another symbol, the deletion of a symbol, and the insertion of a symbol.",12,13
6866,5843144,"Here the primitive operations are the substitution of one symbol for another symbol, the deletion of a symbol, and the insertion of a symbol.",18,19
6867,5843144,"Here the primitive operations are the substitution of one symbol for another symbol, the deletion of a symbol, and the insertion of a symbol.",25,26
6868,5843144,"Entry (i, j) of T AST (P, T) has value h if h is the minimal Levenshtein distance between p 1 • • • p i and a substring of T with last symbol (position) t j (j).",39,40
6869,5843144,"Horizontal transitions encode ""normal"" transitions in which the text symbol matches the expected next symbol of the pattern.",11,12
6870,5843144,"Horizontal transitions encode ""normal"" transitions in which the text symbol matches the expected next symbol of the pattern.",16,17
6871,5843144,"For defining states, input vectors and transitions of A ∀ (k), the following two definitions are essential: Definition 2 The characteristic vector χ(w, V) of a symbol w ∈ Σ in a word V = v 1 • • • v n ∈ Σ * is the bitvector of length n where the ith bit is set to 1 iff w = v i .",33,34
6872,5843144,"w n , the set of active states of A(P, k) reached after reading the ith symbol w i of W is a subset of triangular area i (0 ≤ i ≤ min{n, m + k}).",18,19
6873,5843144,"Furthermore, the set of active states that is reached after reading symbol w i depends only 1.",12,13
6874,5843144,Here $ is a new symbol that does not occur in W. Second imagine that we get to triangular area i after reading the ith letter w i (cf.,5,6
6875,5843144,Here the symbol | denotes bitwise OR.,2,3
6876,5843144,"Let S P i denote the set of active positions of A(P, k) that are reached after reading the ith symbol w i (1 ≤ i ≤ n).",22,23
6877,5843144,The symbol stands for either 1 or 0.,1,2
6878,5843144,"Let A ∀ (k) = Γ, Q ∀ , q ∀ 0 , F ∀ , δ ∀ denote the universal deterministic Levenshtein automaton for bound k. We assume that we can access, for each symbol σ ∈ Σ and each index 1 ≤ i ≤ m + k, the characteristic vector χ(σ, p i−k • • • p i • • • p r ) , where r = min{m, i + k + 1}, in constant time (cf.",39,40
6879,5843144,"At each step, a symbol σ read in A D representing the ith symbol of the current dictionary path is translated into the bitvector χ(σ, p i−k • • • p i • • • p r ), r = min{m, i + k + 1}, which is used as input for A ∀ (k).",5,6
6880,5843144,"At each step, a symbol σ read in A D representing the ith symbol of the current dictionary path is translated into the bitvector χ(σ, p i−k • • • p i • • • p r ), r = min{m, i + k + 1}, which is used as input for A ∀ (k).",14,15
6881,5843144,"push (<0,ε, q D 0 , q ∀ 0 >); while not empty(stack) do begin pop (<i, W, q D , q ∀ >); for σ in Σ do begin χ := χ(σ, p i−k • • • p i • • • p r ); D 1 := δ D (q D , σ); q ∀ 1 := δ ∀ (q ∀ , χ); if (q D 1 <> NIL) and (q ∀ 1 <> NIL) then begin W 1 := concat(W, σ); push(<i + 1, W 1 , q D 1 , q ∀ 1 >); if (q D 1 ∈ F D ) and (q ∀ 1 ∈ F ∀ ) then output(W 1 ); end; end; end; Starting with the pair of initial states q D 0 , q ∀ 0 , position i = 0, and the empty word ε, each step of the traversal adds a new symbol σ ∈ Σ to the actual word W and leads from a pair of states q D , q ∀ ∈ Q D × Q ∀ to δ D (q D , σ), δ ∀ (q ∀ , χ) .",197,198
6882,5843144,"In order to overcome this problem, we can draw on the following observation: If d L (P, W) ≤ 3, then W can be represented in the form W = W 1 W 2 , where there are seven alternatives, inlcuding the following four: In the remaining three alternatives, W 1 = W 1 a m+1 ends with the symbol a m+1 , and W 2 = a m W 2 starts with a m .",68,69
6883,5843144,"With V [i] we denote the word that is obtained from a word V by deleting the ith symbol of V. For |V| < i, we define V [i] = V. By a dictionary with output sets, we mean a list of strings W, each of which is associated with a set of output strings O(W).",20,21
6884,5843144,Here & is a special symbol that marks the border between codes and original words.,5,6
6885,5843144,"In Phase 1, which ends when the special symbol & is read, we compute an initial path of A D in which the corresponding sequence of transition labels represents a code α such that d L (κ(P), α) ≤ k. All paths of this form are visited.",9,10
6886,5843144,"The ratio of the number of visits to the total number of symbols in the list of words D gives the average number of visits per symbol, denoted v 0 .",26,27
6887,5843144,"For the three dictionaries, the following values were obtained: BL GL TL Average number v 0 of visits per symbol 0.1433 0.3618 0.7335 Average time t 0 for one visit (in µs) 0.0918 0.1078 0.0865 Given an input V, we may consider the total number n V of symbols in the list of correction candidates.",21,22
6888,6717552,observation probability b ij (o t ) : the probability that symbol o t is emitted when state s i transits to state s j .,12,13
6889,3265924,"The rest of the dictionary remains unchanged, because a new word either begins with a symbol different from the first symbols of all words already in the automaton; the beginning symbol of the new word is lexicographically placed after those symbols; or it shares some (or even all) initial symbols of the word previously added to the dictionary; the algorithm then creates a forward branch, as the symbol on the label of the transition must be later in the alphabet than symbols on all other transitions leaving that state.",16,17
6890,3265924,"The rest of the dictionary remains unchanged, because a new word either begins with a symbol different from the first symbols of all words already in the automaton; the beginning symbol of the new word is lexicographically placed after those symbols; or it shares some (or even all) initial symbols of the word previously added to the dictionary; the algorithm then creates a forward branch, as the symbol on the label of the transition must be later in the alphabet than symbols on all other transitions leaving that state.",32,33
6891,3265924,"The rest of the dictionary remains unchanged, because a new word either begins with a symbol different from the first symbols of all words already in the automaton; the beginning symbol of the new word is lexicographically placed after those symbols; or it shares some (or even all) initial symbols of the word previously added to the dictionary; the algorithm then creates a forward branch, as the symbol on the label of the transition must be later in the alphabet than symbols on all other transitions leaving that state.",74,75
6892,3265924,"The following should be noted about this solution: memory requirements are higher, as we keep more than one isomorphic state at a time, the function replace_or_register must remain recursive (as in the sorted version), and the argument to replace_or_register must be a string, not a symbol, in order to pass subsequent symbols to children.",52,53
6893,3201741,"Its main component is a second LSTM-based RNN, which works over its own internal state s t and the previous output token y t−1 : s t = lstm((y t−1 • c t )W S , s t−1 ) (2) It is initialized by the last hidden encoder state (s 0 = h n ) and a special starting symbol.",66,67
6894,16731433,"That is, we predict each letter e i based on the pair h=(e[1,i- 1], f) ∈ H. Formally, we map H×E into a d-dimensional feature space ϕ: H×E → R d , where each ϕ k (h,e)(k∈{1,..,d}) corresponds to a condition defined in terms of the history h and the currently predicted letter e. In order to model string termination, we augment E with a sentinel symbol $, and we append $ to each e from D. Given a transliteration dictionary D, we transform the dictionary in a set of |E| binary learning problems.",82,83
6895,2445242,"In Figure 2 , document dj is represented by an symbol with a mainly lightcolored background, but with a small darkcolored section.",10,11
6896,16332736,"Probabilities capture structural context, since nodes in the parse forest partially encode a configuration of the graph-structured stack and lookahead symbol, so that, unlike a standard PCFG, the model discriminates between derivations which only differ in the order of application of the same rules and also conditions rule application on the PoS tag of the lookahead token.",23,24
6897,237513923,"The '|' symbol is used for separating slot-and-value pairs from each other, while the '=' is used within each pair to separate the value from the slot name.",4,5
6898,12871696,"Note that C i (a) denotes the number of times candidate a violates the constraint with index i. Definition 3 Given a constraint set CON with k constraints indexed {1 ... k} and two candidates that share the same input, the function erc CON (a, b) returns an ERC α = α 1 , ..., α k that describes the rankings under which a b. 1 erc(a, b) = α 1 , ..., α k where      α i = W if C i (a) < C i (b) α i = L if C i (a) > C i (b) α i = e if C i (a) = C i (b) The symbol W in α i of erc(a, b) = α is a mnemonic for the fact that C i favors a (the winner), whereas an L in coordinate i is a mnemonic for the fact that C i favors b (the loser).",142,143
6899,2787289,"Terminal productions produce a token in each stream, or a token in one stream with the null symbol ∅ in the other.",18,19
6900,16027193,This line of reasoning suggests that typologically frequent properties should tend to exhibit (a) (b) q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 0.05 0.10 0.15 0.20 0.25 0.5 0.6 0.7 0.8 0.9 Frequency vs Trigram Entropy Typological Frequency of Pattern Trigram Entropy (bits/symbol) q high low 0.5 0.6 0.7 0.8 0.9 greater simplicity (according to some metric) than those that are rarer.,69,70
6901,16027193,"Interestingly, the entropies of the above-median patterns are tightly constrained to a narrow band of values (variance 0.012 square bits/symbol), whereas the below-median patterns show much greater variation in their complexity (variance 0.028 square bits/symbol).",25,26
6902,16027193,"Interestingly, the entropies of the above-median patterns are tightly constrained to a narrow band of values (variance 0.012 square bits/symbol), whereas the below-median patterns show much greater variation in their complexity (variance 0.028 square bits/symbol).",47,48
6903,15276369,"An adaptor grammar consists of terminals V , nonterminals N (including a start symbol S), initial rules R and rule probabilities p, just as in a PCFG.",14,15
6904,1723810,"Given that systems, at any particular time, tend 7 In practice, a code length exactly equal to the negative log of the probability of a particular symbol may be unattainable, and the relationship in Equation ( 8 ) becomes an approximation which may be better in some cases than others.",29,30
6905,17271239,"In addition, it is known that, unlike in the training data, the runtime data will have a company name immediately before every ticker symbol.",26,27
6906,17271239,The question facing the builder of the system is how to combine the CRF with rules based on the must-find company list and the company-name-before-every-ticker-symbol fact.,36,37
6907,15943580,"Skounakis (2003) described the HHMM as multiple ""levels"" of HMM states, where lower levels represents each individual output symbol, and upper levels represents the combinations of lower level sequences.",23,24
6908,15943580,"Consider a sentence from a CoNLL-2004 corpus: where the part-of-speech tag associated with each word is attached with an underscore, the clause information is identified by the S symbol and the chunk information is identified by the rest of the symbols NP (noun phrase), VP (verb phrase), PP (prepositional phrase) and O (null complementizer).",34,35
6909,18064933,81 The symbol X stands for 'NP or S'.,2,3
6910,10686027,"This is problematic since the former are separate symbols, occupying a node in the lattice, whereas the latter are appended to another symbol (e.g., ""<w> morph eme </w>"", 4 nodes, versus ""morph eme#"", 2 nodes).",24,25
6911,2137750,"Probably, we could add this necessity to the DA corresponding to A1 manually, but it is infeasible for all the sentences to distinguish the fine-grained semantic information by adding abstract symbol to DA.",34,35
6912,4392292,"According to the definition of POC-NLW template, the state set of HMM corresponds to the Wl-Pn tag set, and the symbol set is composed of all characters.",26,27
6913,9271185,"The symbol SEM 1 and SEM 2 refer to the semantic feature sets in Section 3.2.1 and 3.2.1 respectively, and SYN refers to the syntactic feature set in Section 3.1.",1,2
6914,18984135," In these rules, is the Start symbol; is the category for concatenating combination whereas for inverted combination.",8,9
6915,6621916,"At the triangle, the connection of a symbol with an object is mediated by a concept, and each element constructs its own system.",8,9
6916,3838943,"The symbol ""X"" stands for any of the three intensities as identified for the emotional expressions.",1,2
6917,16788586,"Furthermore, we represent phase heads with the * symbol; thus, v* and C* are phase heads.",9,10
6918,221761320,"  Decoding stops when a slot-specific end-ofsentence symbol is generated, which is possible to be the first output if the slot does not appear in the dialog.",11,12
6919,5610997,"the symbol ""!""",1,2
6920,5610997,"For example, the word "" "" (school) in the phrase ' "" (to school) is a fixed component and the symbol 'A' in """" """" """" (to go to A) is an variable component.",25,26
6921,16042670,"symbol; and the Type-3 sentences are those that have ""you"" as the subject of a reporting verb (e.g., ""think"", ""say"", ""believe"").",0,1
6922,16522900,"The paired symbols (w i , t i ) are reinterpreted as consisting of an input symbol w i and output symbol t i .",17,18
6923,16522900,"The paired symbols (w i , t i ) are reinterpreted as consisting of an input symbol w i and output symbol t i .",22,23
6924,16522900,The output symbol sequence (π 2 ) from the lowest weight path is T * .,2,3
6925,218974091,"2016b) by jointly learning and applying BPE on both source and target sides with 32K merge operations and apply it with vocabulary threshold = 1 (meaning that when re-applying BPE each subword must have been seen at least once at training time, and is otherwise replaced by the UKN symbol).",55,56
6926,218974091,"The method returns a factored representation of the text in Moses style (where each word is represented as a chain of features separated by the pipe symbol |), and words making up the identified MWEs (as well at its lemma) are joint with an underscore.",27,28
6927,7337757,"If the pair exists in our bilingual dictionary, we remove it from the sentence and replace it with a special symbol, '*'.",21,22
6928,7337757,"If our resources were perfect, i.e., if word alignment made no errors, the dictionary had perfect coverage and our corpora induced perfect statis- tics, then all remaining text (other than the special symbol) in the parallel text would be part of MWEs.",37,38
6929,3189641,"P ( z, w, c|β, η, λ) (1) = P ( z|β, η, λ)P ( w, c| z, β, η, λ) = P ( z)P ( w| z)P ( c| z) = A a=1 ∆(n A +η) ∆(η) T z=1 ∆(n Zw +β) ∆(β) T z=1 ∆(n Zc +λ) ∆(λ) The updating equation from which the Gibbs sampler draws the hidden variable for the current state j, i.e., the conditional probability of drawing the k th author K k j , the i th topic Z i j , and the c th cited author C c j tuple, given all the hyperparameters and all the observed documents and authors, cited authors except the current assignment (the exception is denoted by the symbol ∀¬j), is defined in Equation 2 .",151,152
6930,29149097,"Thus ( 1,$) Q J + is the probability of the optimal translation, where the $ symbol is the sentence boundary marker.",18,19
6931,17089673,A segmentation process may find that a symbol stream should not be delimited even though subcomponents of the stream have been seen elsewhere.,7,8
6932,1353171,"Thus ( 1,$) Q J + is the probability of the optimal translation, where the $ symbol is the sentence boundary marker.",18,19
6933,12784143,"The first two methods accommodate short suffixes inside the leaf nodes, and prune leaf nodes corresponding to the end marker symbol.",21,22
6934,15468293,The symbol '::' is sometimes replaced with an equals sign (=) to denote an equation.,1,2
6935,15468293,"In the third heuristic, we adopt a ""trick"" described by Langlais and Yvon (2008) , called S-TRICK based on a simple requirement of sharing the same first or last symbol.",36,37
6936,15468293,"Formally, it can be stated as: H4: Whenever a symbol occurs more frequently in A′ than it does in B′ and C′, the analogical equation is bound to fail and need not be solved. [ : : : : ]",12,13
6937,17711157,"Linguistic words are made up of one or more syllables and are also separated by the same symbol, ""tsheg"", thus there is a lack of word boundaries in the language.",17,18
6938,247475908,"Concretely, the prompt for the i th sample (prompt i , i ∈ [1, n]) is ""(h * i ) t i ⇒ k i "" 1 , and the prompt for the current dialogue context (prompt curr ) is ""(h * ) t ⇒"", where we use the symbol ""⇒"" to guide the LM for knowledge generation.",62,63
6939,17540487,"Appositives The annotation guidelines for proteins apparently specified that when a gene name was present in an appositive with its symbol, the symbol was selected as the gold-standard argument.",20,21
6940,17540487,"Appositives The annotation guidelines for proteins apparently specified that when a gene name was present in an appositive with its symbol, the symbol was selected as the gold-standard argument.",23,24
6941,17332616,Specify that symbol '↔' is used to represent alignment.,2,3
6942,17332616,mi mi mi p nj nj nj q C C C T T T          On the left of alignment symbol is a Chinese BaseNP.,28,29
6943,986803,Our baseline system lacks the capacity to address such cases because all the features it employs are independent of the phrases being moved; these are modeled only as an unlexicalized generic nonterminal symbol.,33,34
6944,16661392,"For instance, the Indirect speech relation might be identified by finding its direct counterpart, usually marked by a dash symbol in the text, and the Translation relation might be easily detected by using a multi-lingual dictionary.",21,22
6945,27090528,"The alignment is computed with the help of a confusion matrix in which costs for phonetic symbol deletion, insertion and substitution are defined taking into account various phonological processes that occur in fluent speech, such as anticipatory assimilation, phone elision and epenthesis.",16,17
6946,27090528,"For a given phrase alignment, each pair of aligned phoneme symbols is converted into a single token by joining the symbols with symbol ':' as a delimiter.",23,24
6947,26671118,"All nonspeech events were labelled with the symbol ""9"".",7,8
6948,26671118,"All hesitations were marked with the symbol ""э-"", regardless of their actual pronunciation.",6,7
6949,15863489,"The text pre-processing steps including tokenization, lowercasing and replacing all digits with symbol digit are applied.",15,16
6950,7788730,We use training corpus to compute metrics for every possible intermediate symbol.,11,12
6951,7788730,"For every possible intermediate symbol, i.e. every ngram of the original symbols, denoted by w, we compute the following two metrics: 1.",4,5
6952,7788730,The two metrics indicate the goodness of a possible intermediate symbol w: num(w) indicates how many ICs labeled by w are likely to be generated in parsing; while ctr(w) represents how much w can contribute to the generation of CCs.,10,11
6953,7788730,"We define the fashion of binarizations in Section 2, where we encode an intermediate symbol using the ngrams of original symbols (content) it derives.",15,16
6954,7788730,O-Trie encodes an intermediate symbol using the its parent and the symbols surrounding it in the original rule (context).,6,7
6955,7788730,"Both I-Trie and O-Trie are equivalent encodings, resulting in equivalent grammars, because they both encode using the complete content or context information of an intermediate symbol.",31,32
6956,52012949,"All indicated pauses, which used the symbol '#' were changed to commas.",7,8
6957,207847960,2018) trained 50 instances of a sequenceto-sequence model on a symbol replacement task.,13,14
6958,8798141,"The reason why FL* outperforms HFL may be due to the very definition of HFL's broken typewriter function: as the segments, e.g. {x, y}, are collapsed into a single symbol, the indicator captures not only minimal alternations like wx ∼ wy, but also word pairs such as xy ∼ yx.",37,38
6959,238260069,One is to enumerate the symbol pairs in the order that they were added to the vocabulary in the building phase.,5,6
6960,238260069,"For each symbol pair, we scan the current sequence and replace all their occurrences with the merged symbol.",2,3
6961,238260069,"For each symbol pair, we scan the current sequence and replace all their occurrences with the merged symbol.",18,19
6962,238260069,"The latter start with a special symbol ## (in BERT), which is called the suffix indicator and is denoted as ♯ in this paper.",6,7
6963,2442439,"Rules have the form X → ē, f , where ē and f are phrases containing terminal symbols (words) and possibly co-indexed instances of the nonterminal symbol X. 2 Associated with each rule is a set of translation model features, φ i ( f , ē); for example, one intuitively natural feature of a rule is the phrase translation (log-)probability φ( f , ē) = log p(ē| f ) , directly analogous to the corresponding feature in non-hierarchical phrase-based models like Pharaoh (Koehn et al.,",31,32
6964,11405242,"Topics are defined as single word preceded by a # symbol, and replies are single words preceded by a @ symbol.",10,11
6965,11405242,"Topics are defined as single word preceded by a # symbol, and replies are single words preceded by a @ symbol.",21,22
6966,2146994,All the tweets in our corpus contain only ASCII characters and we additionally stripped the tweets of words beginning with the @ or # symbol.,24,25
6967,235848394,Let 𝑝(𝑥) be the distribution of a pronounced symbol (this can be an intended phoneme or an intended feature of a stimulus) and 𝑞(𝑥) that of one of its contrasting alternatives.,9,10
6968,235848394,"We think of symbols (in the context of Section 2, symbols are consonants) as distributions, because every intended symbol is cashed in as a distribution of potential outcomes on the side of the perceiver.",22,23
6969,236486231,We use the symbol / 0 to denote that a UD parse tree has no compchain.,3,4
6970,1403377,"For example the rule ""t→d.ε^^^"""" says that translate ""t"" to ""d"" whenever ""t"" occurs between ε (null symbol) and # (word boundary); which ultimately means that if ""t"" occurs in the end of a noun it will be replaced by ""d"".",27,28
6971,199661488,"Rewriting rules have the following form: (1.1) The Schema of a Phrase-Structure Rewriting Rule By replacing (rewriting) the symbol A with B and C, this rule generates a tree structure with A dominaung B and C. Conceptually, the derivation order of rewriting rules is top-down.",25,26
6972,199661488,"Parsers for contextfree PS-grammars, for example, cannot possibly apply the rules of the grammar directly because the rules rewrite an initial start symbol, while the parser takes sentences as input The standard solution to this dilemma consists in computional routines which reconstruct the grammatical analysis in an indirect way by building large intermediate structures (e.g., ""state sets"", ""charts"", ""tables"") which are not part of the grammar.",27,28
6973,218974108, The arrow symbol translates as 'precedes'.,3,4
6974,16956934,The only nonterminal used is a new symbol usually refered to as 'X'.,7,8
6975,15736291,"Because glides are also consonants, the symbol C in this paper refers to all the other consonants except glides.",7,8
6976,15991124,The symbol |S| denotes the number of elements in the set S which is derived by the logical formula after vertical bar.,1,2
6977,15991124,The symbol m denotes total number of segments in the test set.,1,2
6978,2604984,end for for each sentence s do for each rule r used in s do for each terminal symbol (word) t in the RHS of r do for each category c referred to by r in t do increase count(c) end for end for end for end for {Use count(c)'s as weights.},18,19
6979,2604984,"For parsingrelated WPA, we assume that, even if a given rule does not mention the PoS of a terminal symbol, 2 2 For example, in unification grammars and constraintbased grammars a terminal may be identified only by the that PoS may be read off the parse tree, so the conditional weights may still be calculated.",21,22
6980,15462703,"Line (3) follows from the fact that the missing term pos ( N ) neg ( N ) = 1 since the occurrence of N , the special symbol <BOUNDARY>, in a document with a positive sentiment is equally likely to its occurrence in a document with a negative sentiment.",30,31
6981,218974233,"After the diamond symbol '◊' there are examples of verbal and attributive collocations for the headword supplied with the citations, e.g. zarabotnaya plata 'wage', plata sdel'naya 'accord loan', plata podennaya 'day rate payment' etc.",3,4
6982,218974233,"The diamond symbol '◊' introduces the most typical collocates for the headwords, while the tilde symbol '~' corresponds to the phraseological units (Figures 3 and 4 ).",2,3
6983,218974233,"The diamond symbol '◊' introduces the most typical collocates for the headwords, while the tilde symbol '~' corresponds to the phraseological units (Figures 3 and 4 ).",18,19
6984,218974233,"Dictionary of the Russian Language Altogether we extracted 11,210 phraseological units that were marked with a special diamond '◊' symbol; the total number of headwords was 5,955 (which is more than 7% of the total word list of the dictionary).",21,22
6985,218974233,We also analyzed and extracted collocations from quotations that were not highlighted in the entries and marked them with the asterisk '*' symbol.,24,25
6986,232021561,The symbol γ 100 marks the absolute value of the negative slope coefficient of the regression model for λ = 100.,1,2
6987,12857568,"To learn an embedding for unknown characters, we cast characters occurring only once in the training set to a special symbol.",21,22
6988,12311990,"Orthographic features -(word) starts with an upper case letter, -starts with a lower case letter, -starts with a symbol, -starts with a digit, -contains upper case letter, -contains a lower case letter, -contains a symbol -contains digit.",21,22
6989,12311990,"Orthographic features -(word) starts with an upper case letter, -starts with a lower case letter, -starts with a symbol, -starts with a digit, -contains upper case letter, -contains a lower case letter, -contains a symbol -contains digit.",41,42
6990,708363,"Thus, assuming that the target symbol A has already been aligned to source node α from an earlier derivation step, the likelihood of expanding (α, A) with the above production rule depends on three factors: 1.",6,7
6991,12165062,"In LWFGs, each nonterminal symbol is a left-hand side in at least one ordered nonrecursive rule and the empty string cannot be derived from any nonterminal symbol.",5,6
6992,12165062,"In LWFGs, each nonterminal symbol is a left-hand side in at least one ordered nonrecursive rule and the empty string cannot be derived from any nonterminal symbol.",30,31
6993,12165062,"S ∈ N G is the start nonterminal symbol, and ∀A ∈ N G , S A (we use the same notation for the reflexive, transitive closure of ).",8,9
6994,12165062,"1995) , the LWFG parser has the following characteristics: each item is augmented with a syntagma; the Constraint rule is a new inference rule, and the goal items are associated to every nonterminal in the grammar, not only to the start symbol (i.e., LWFG parser is a robust parser).",46,47
6995,8062486,"Use symbol 0 for all unspanned positions, copy terminal symbols as they are, and use symbols -1, -2, -3, and -4 to transcribe X 1 and X 2 from the first rule, and X 1 and X 2 from the second rule.",1,2
6996,8062486,Repeat the non-terminal symbol on all spanned positions.,5,6
6997,2315102,"Generic symbols in translation rules (i.e., the non-terminal symbol ""X"") were replaced with structured information at multiple levels of abstraction, using a tree-grafting approach that we describe subsequently.",12,13
6998,2315102,At the start of the 8 weeks the decoder used translation rules with a single generic non-terminal symbol.,19,20
6999,2315102,"Hiero uses grammars with a single non-terminal symbol ""X"" rather than using linguistically informed non-terminal symbols.",9,10
7000,15252695,The symbol (↓) is used to indicate the substitution site.,1,2
7001,15252695,The foot node is marked by a symbol (*) and must have the same label as the root node of the tree.,7,8
7002,17889718,"i i i i i RS i i i … = ′ → × → = = (2) Suppose that a lexicalized CFG rule has the following form: ) ( )... ( ) ( ) ( )... ( ) ( 1 1 1 1 k k m m r R r R h H l L l L h F → where F(h), H(h), , and are all lexicalized non-terminal symbols; F(h) is the lefthand-side symbol or parent symbol, h is the pair of head word and its POS label; H is a head child symbol; and and are right and left modifiers of H. Either k or m may be 0, and k and m are 0 in unary rules.",90,91
7003,17889718,"i i i i i RS i i i … = ′ → × → = = (2) Suppose that a lexicalized CFG rule has the following form: ) ( )... ( ) ( ) ( )... ( ) ( 1 1 1 1 k k m m r R r R h H l L l L h F → where F(h), H(h), , and are all lexicalized non-terminal symbols; F(h) is the lefthand-side symbol or parent symbol, h is the pair of head word and its POS label; H is a head child symbol; and and are right and left modifiers of H. Either k or m may be 0, and k and m are 0 in unary rules.",93,94
7004,17889718,"i i i i i RS i i i … = ′ → × → = = (2) Suppose that a lexicalized CFG rule has the following form: ) ( )... ( ) ( ) ( )... ( ) ( 1 1 1 1 k k m m r R r R h H l L l L h F → where F(h), H(h), , and are all lexicalized non-terminal symbols; F(h) is the lefthand-side symbol or parent symbol, h is the pair of head word and its POS label; H is a head child symbol; and and are right and left modifiers of H. Either k or m may be 0, and k and m are 0 in unary rules.",112,113
7005,17889718,"Given the left hand side, the generation process of the right hand side can be decomposed into three steps: ) ( i i r R ) ( i i l L ) ( i i r R ) ( i i l L • Generate ∏ + = = 1 1 ) , , | ) ( ( k i i i R H h F r R P P where is a STOP symbol which is added to the set of nonterminal symbols.",76,77
7006,17889718,"1 1 + + k k r R • Generate the left modifiers with probability: ∏ + = = 1 1 ) , , | ) ( ( m i i i L H h F l L P P where is the STOP symbol. ) (",45,46
7007,765039,"are the word boundary symbol #, by multiplying the probabilities of all bigrams in the word.",4,5
7008,765039,"This is because positing a boundary corresponds to the generation of an additional symbol, #, which otherwise does not have to be generated.",13,14
7009,5678772,B → γ where γ is a terminal symbol (or a word in this case).,8,9
7010,5678772,Therefore the right hand side of the second rule form must be a sequence of terminal symbols (or a phrase) but not a single symbol (a word).,26,27
7011,5678772,There are a number of notions concerning this algorithm: • A CFG rule has a head symbol on the right hand side.,17,18
7012,5678772,These symbol will be added to a nonterminal on the side having a change.,1,2
7013,5678772,"In the previous example, since a head noun in the word tree has been expanded on the right, the corresponding symbol in phrase tree is NN-H+.",22,23
7014,5678772,They proposed using a pseudo nonterminal symbol that subsumes the phrase and corresponding multi-headed syntactic structure.,6,7
7015,5678772,One new xRs rule is required to explain how the new nonterminal symbol can be combined with others.,12,13
7016,198974655,"In the following descriptions, a simple symbol (ex.",7,8
7017,198974655,"p) denotes a non-vector, a bold-faced lower-case symbol (ex.",15,16
7018,198974655,"p) denotes a vector, and a bold-faced upper-case symbol (ex.",14,15
7019,26138698,"The symbol * denotes that we, instead of using their released codes, carefully reimplement these models for the sake of making the comparisons as fairly as possible.",1,2
7020,170078653,"Following BERT, we use the first symbol as the sentence-wise representation u, and compute its matching score against all the tree node to predict the representation of dialog acts Â. Word Embedding Position Embedding In the decoder, we adopt take as input any length features x 1 , • • • , x n , each with dimension of 64, in the first layer, since we have 10 heads, the dimension for each head is 6, thus the key, query feature dimensions are fixed to 6, the second layer with dimension of 9, the third with dimension of 2.",7,8
7021,221865854,"In FOL, predicates are used to denote a property of objects or a relation between objects and every predicate symbol comes with an arity larger or equal to 1.",20,21
7022,199022721,"Since these words never occur in the classical poetry corpus, the generation module will take them as a UNK symbol and generate totally irrelevant poems.",20,21
7023,52193570,"The most salient character is ""ping"" (nuphar, a kind of plant, a symbol of loneliness) and the second one is ""qi"" (seven), according to their saliency scores r(ping) = 0.53 and r(qi) = 0.17.",17,18
7024,52193570,"As shown in Figure 4 , by tf-idf weighting, two informative characters ""hong yan"" (wild goose, a symbol of autumn) are selected correctly, which leads to the generation of word ""qiu xing"" (sadness in autumn) in the fourth line in Figure 1 .",24,25
7025,53083178,"Based on these rewarders, the total reward is: R(P) = 4 ∑ j=1 α j * Rj (P), (14) where α j is the weight and the symbol ˜means the four rewards are re-scaled to the same magnitude.",35,36
7026,2493904,A rule typically consists of a parent state (left) and its child states and output symbol (right).,17,18
7027,247244896,"Given a triple (h,r,t), the first encoder BERT hr is used to compute the relation-aware embedding for the head entity h. We first concatenate the textual descriptions of entity h and relation r with a special symbol [SEP] in between.",45,46
7028,10041554,"The attachment function α : E → V * maps each hyperedge e ∈ E to a sequence of pairwise distinct vertices from V, where we call the length of α(e) the arity of e. The labeling function ℓ : E → Σ maps each hyperedge to a symbol in some ranked alphabet Σ, where the rank of ℓ(e) is e's arity.",50,51
7029,10041554,"Σ is a ranked alphabet of edge labels, N ⊂ Σ a set of nonterminal symbols, S ∈ N a special start symbol, and R is a finite set of weighted rules.",24,25
7030,10041554,"As with PCFGs, a weighted HRG is probabilistic if the weights of all rules with the same ranked symbol A on the lefthand side sum to one.",19,20
7031,10041554,"Much like with CFG, where each step of a derivation replaces a symbol by a substring, each step of an HRG derivation replaces an edge with a certain nonterminal symbol label by the right-hand side graph of some rule with the same symbol on its left-hand side.",13,14
7032,10041554,"Much like with CFG, where each step of a derivation replaces a symbol by a substring, each step of an HRG derivation replaces an edge with a certain nonterminal symbol label by the right-hand side graph of some rule with the same symbol on its left-hand side.",31,32
7033,10041554,"Much like with CFG, where each step of a derivation replaces a symbol by a substring, each step of an HRG derivation replaces an edge with a certain nonterminal symbol label by the right-hand side graph of some rule with the same symbol on its left-hand side.",46,47
7034,10041554,"Start by assigning the left-hand side nonterminal symbol according to label(parent(η), r), which returns a symbol determined by η's parent with rank r, the number of vertices in common between η and its parent.",9,10
7035,10041554,"Start by assigning the left-hand side nonterminal symbol according to label(parent(η), r), which returns a symbol determined by η's parent with rank r, the number of vertices in common between η and its parent.",21,22
7036,10041554,"The function label just returns a nonterminal symbol of a given rank, chosen to match the number of external vertices of the righthand side.",7,8
7037,10041554,There are many possible choices of label; it can even be a function that always returns the same symbol for a given rank.,19,20
7038,10041554,"We set aside every 10 th graph for the test set, and estimate the models from the remaining 5,008, replacing terminals occurring ≤ 1 times in the training set with special symbol UNK.",33,34
7039,10041554,"This amounts to counting the number of times each rule r with left-hand side symbol A is extracted and then computing its weight θ r according to θ r = exp Ψ(n r + β) − Ψ( r ′ :r ′ =A→h n r ′ + β) , where n r is the frequency of r and Ψ is the standard digamma function.",16,17
7040,3545055,The derivation starts with a single edge labeled with the nonterminal symbol S .,11,12
7041,3545055,"A hyperedge replacement grammar (HRG) is a tuple G = N, T, P, S where • N and T are finite disjoint sets of nonterminal and terminal symbols • S ∈ N is the start symbol • P is a finite set of productions of the form A → R, where A ∈ N and R is a graph fragment over N ∪ T .",40,41
7042,198185049,"We begin with an implication ((0, n) • c) ⇒ ((0, n) ROOT) in the store, where c is the starting category of the grammar and 'ROOT' is a distinguished grammarexternal symbol.",44,45
7043,201306636,"<bos>"" is a special begin-of-sequence padding symbol.",13,14
7044,5239696,The parse is just the MR with each symbol labeled with its grammar rule.,8,9
7045,5239696,The probability of each symbol in the MR is conditioned on the MR grammar rules that derive its parent symbol.,4,5
7046,5239696,The probability of each symbol in the MR is conditioned on the MR grammar rules that derive its parent symbol.,19,20
7047,5239696,"Defining symbol probabilities in terms of their parents' grammar rules (as opposed to parent symbols as in a standard PCFG) distinguishes between functions and predicates with the same name but different semantics (Wong and Mooney, 2006) .",1,2
7048,5239696,Let args i be the set of indices of the children of the node at path i; and R i be the grammar rule that derives the symbol at i according to the MR parse.,28,29
7049,5239696,"P (MR) =P (R ǫ ) i∈paths j∈args i P (R i•j |j, R i ) (1) In other words, each node in the tree is generated according to the probability of the MR rule that derives it conditioned on (1) the MR rule R i that derives its parent symbol and (2) its position j beneath that parent.",61,62
7050,5239696,"Source tree language model: P (R i•j |j, R i ) Rule type 1 in Table 1 begins the process by transitioning from start state q start to q R order , where the grammar rule R ranges over those rules with the start symbol S on the left hand side.",47,48
7051,5239696,"Choosing exactly which q R order to transition to corresponds to the decision of choosing the root symbol of the MR tree (the symbol generated by R), and these transducer rules define the P (R ǫ ) term in equation 1, i.e., the probability of the grammar rule corresponding to the root symbol of the MR tree.",17,18
7052,5239696,"Choosing exactly which q R order to transition to corresponds to the decision of choosing the root symbol of the MR tree (the symbol generated by R), and these transducer rules define the P (R ǫ ) term in equation 1, i.e., the probability of the grammar rule corresponding to the root symbol of the MR tree.",24,25
7053,5239696,"Choosing exactly which q R order to transition to corresponds to the decision of choosing the root symbol of the MR tree (the symbol generated by R), and these transducer rules define the P (R ǫ ) term in equation 1, i.e., the probability of the grammar rule corresponding to the root symbol of the MR tree.",58,59
7054,5239696,"For each pair of MR grammar rules R p and R c , we add a transducer rule of the form of rule type 2 that transitions from the states associated with R p to those for R c if R c generates a valid child of the symbol generated by R p .",48,49
7055,5239696,"Thus, the choice of state transition here corresponds to choosing the child of the last generated symbol of the input tree.",17,18
7056,5239696,"With rules described in the next section, state q R c order then writes the symbol to the input tree specified by MR grammar rule R c .",16,17
7057,5239696,"Argument order is indicated by j, a permutation of the numbers 0, 1, ...n − 1, and j k is the k th number in the permutation, indicating which argument appears at position k. State q R f words,1 generates the words for f , state q R f words,0 replaces the symbol W with the empty string, and the states q R f arg,k select the grammar rule with which to generate the k th child.",58,59
7058,17786494,"They are listed in Table 2 , where the symbol ""+"" indicates that for every value of the variable preceding ""+"" there is a separate weight for the corresponding formula.",9,10
7059,7939732,"One straightforward lexical/preterminal rule is added for each lexical item in the MG, and the MCFG's start symbol is c 0 .",21,22
7060,7939732,"This formulation defines a distribution over MCFG derivations in terms of a random branching process that begins with probability 1 at the start symbol and recursively expands frontier nodes N , drawing branching decisions from the the conditional distribution p(• | N ); the process terminates when lexical items have been produced on all frontiers.",23,24
7061,800974,"A poster begins by publishing a post along with an emotion, then a replier responds to the post and labels it with an emotion symbol.",25,26
7062,14571902,"The semantic class of the root of the tree is set to a special start symbol, represented by the integer 0.",15,16
7063,17661253,"w 2 ~w2 w 1 A B ~w1 C D Table 1: Contingency table 1 2 1 2 1 2 1 2 ( , ) ( , ) ( , ) log ( ) ( ) p w w RMI w w freq w w p w p w = × i log ( ) ( N A A ) A B A C × = × + × + (1) Identifying Low-Frequent Features and Opinion Words In Chinese reviews, one linguistic rule ""noun+ adverb* adjective+"" occurs frequently and most of the instances of the rule are used to express positive or negative opinions on some features, i.e., ""机 身/noun 比较/adverb 小巧/adjective"" (The body is rather delicate) , where each Chinese word and its part-of-speech is separated by the symbol ""/"".",154,155
7064,47016928,Number of predicted bigrams 1 : 6 Number of hit predicted bigrams: 4 Bigram count for bonus: 3 (hit gold answer 1) Vanilla P2: 4 / 6 = 0.67 Adapted P2 (α = 1.0): (4 + 3) / (6 + 3) = 0.78 1 Include period symbol and omit lemmatization.,57,58
7065,209521369,"Then, a two-layer bidirectional LSTM (Hochreiter and Schmidhuber, 1997) will read the embedded word one-by-one and produce a sequence of hidden states {q i } m 1 : q f i = LST M (emb(w i ), q f i−1 ) (4) q b i = LST M (emb(w i ), q b i+1 ) (5) q i = [q f i , q b i ] (6) Each decoding step, the embedding vector of previously symbol and previous hidden state are sent to LSTM.",99,100
7066,15618972,"It is represented using four tuples {V, , R, S} V is the set of non-terminals is the set of terminals R has the set of production rules from V to (V ∪ ) * S is the start symbol sequence occurring just after the genitive should be in agreement.",46,47
7067,220058301,"Data Pre-processing Our pre-processing pipeline begins by removing non-printable ASCII characters, lowercasing text, normalizing additional white-space, and control character and replacing any escaped characters with the corresponding symbol by our in-house script.",38,39
7068,196181536,"In S-Aligned (Chiang and Chen, 2019), the encoder is designed to understand the semantics of problems, and the decoder focuses on deciding which symbol to generate next over semantic meanings of the generated symbols.",30,31
7069,15392824,"To generate the character blocks, we resort to the word's phonetic symbol and some common pronunciation rules.",13,14
7070,15392824,"Li and Liu, 2012) extended this character-level MT by leveraging phonetic symbol, translating nonstandard words to phones first, and then phones to standard words.",15,16
7071,15392824,"Therefore, we propose to segment an English word according to its phonetic symbol or pronunciation, and then use these segments as units in the MT system.",13,14
7072,15392824,"For the character-block sequence labeling system, we also performed some manual correction to fix some bad alignment between the character blocks and their phonetic symbol.",27,28
7073,2920957,"We obtain the following distribution on pairs (w, c) of words and semantic classes: p(w, c) = K k=1 p(c k | c π(k) )p(w k | c k ), with c 0 being equal to a special symbol denoting the root of the tree.",47,48
7074,19789961,WE denotes word embedding feature and other symbol are as listed in section 4.,7,8
7075,972946,"The ACTION/GOTO I s13 I NP(d,et(a)N(dog)) I Pet(a) v(saw) v(saw) • o (ii) (iii) Figure 1 : Stacks After two morestrings are shifted, say ""a dog"", and the parser encounters the end-of-asentence symbol ""$"" (Fig.l(ii)), the next action, ACTION(s4,$), should be ""reduce by NP-~Det N"".",53,54
7076,18878239,"The articles posted in July 2006 are used here, and they are divided into 341,932 smaller units by the full stop symbol.",22,23
7077,18878239,"Although the articles are separated by a full stop into shorter units, these units are not necessarily identical to sentences due to the conventional usage of the Chinese period symbol.",30,31
7078,53616566,"In order to capture nati, the projection function has to consider two factors when choosing whether or not to project a symbol: I) the local context in the string, and II) which symbols are already on the tier.",22,23
7079,53616566,"The context specifies that σ should be projected whenever both of the following hold: it occurs between the substrings b (look-back) and a (look-ahead), and the tier constructed so far ends in t. The value of i determines the size of the input window, which includes the look-ahead and lookback spans, as well as the symbol itself.",69,70
7080,53616566,"The value of j indicates how far back along the tier we can look, including the current symbol.",18,19
7081,53616566,"Let Σ := {a, b, c} and consider the tier projection that always projects the first and last symbol of the string, always projects a, never projects c, and projects b only if the previous symbol on the tier is a. This projection is IOSL-(2,2).",23,24
7082,53616566,"Let Σ := {a, b, c} and consider the tier projection that always projects the first and last symbol of the string, always projects a, never projects c, and projects b only if the previous symbol on the tier is a. This projection is IOSL-(2,2).",43,44
7083,53616566,"The final context ensures that b is projected regardless of what precedes or follows in the input, but only if the previous symbol on the tier is a. Given the previous constraints, this is equivalent to saying that b is only projected if it is the first b encountered after seeing an a earlier in the string.",23,24
7084,53616566,"More precisely, the following symbol must be a vowel, a glide, /m/, or /n/ itself (Whitney, 1889).",5,6
7085,53616566,The projection rules for each symbol are sufficiently simple that this does not introduce any inaccuracies.,5,6
7086,53616566,"IOSL-(2,1) • Project √ if the previous tier symbol is R. IOSL-(1,2) • Project P if the previous tier symbol is √ and the next two input symbols are [n] and S. IOSL-(3,2) • Project C if the previous tier symbol is R, √ , or S, unless C is [n] and the next input symbol is S. IOSL-(2,2) • Project every retroflex (not just those matching R) if the previous tier symbol is S. IOSL-(1,2) • Don't project anything else.",9,10
7087,53616566,"IOSL-(2,1) • Project √ if the previous tier symbol is R. IOSL-(1,2) • Project P if the previous tier symbol is √ and the next two input symbols are [n] and S. IOSL-(3,2) • Project C if the previous tier symbol is R, √ , or S, unless C is [n] and the next input symbol is S. IOSL-(2,2) • Project every retroflex (not just those matching R) if the previous tier symbol is S. IOSL-(1,2) • Don't project anything else.",21,22
7088,53616566,"IOSL-(2,1) • Project √ if the previous tier symbol is R. IOSL-(1,2) • Project P if the previous tier symbol is √ and the next two input symbols are [n] and S. IOSL-(3,2) • Project C if the previous tier symbol is R, √ , or S, unless C is [n] and the next input symbol is S. IOSL-(2,2) • Project every retroflex (not just those matching R) if the previous tier symbol is S. IOSL-(1,2) • Don't project anything else.",45,46
7089,53616566,"IOSL-(2,1) • Project √ if the previous tier symbol is R. IOSL-(1,2) • Project P if the previous tier symbol is √ and the next two input symbols are [n] and S. IOSL-(3,2) • Project C if the previous tier symbol is R, √ , or S, unless C is [n] and the next input symbol is S. IOSL-(2,2) • Project every retroflex (not just those matching R) if the previous tier symbol is S. IOSL-(1,2) • Don't project anything else.",64,65
7090,53616566,"IOSL-(2,1) • Project √ if the previous tier symbol is R. IOSL-(1,2) • Project P if the previous tier symbol is √ and the next two input symbols are [n] and S. IOSL-(3,2) • Project C if the previous tier symbol is R, √ , or S, unless C is [n] and the next input symbol is S. IOSL-(2,2) • Project every retroflex (not just those matching R) if the previous tier symbol is S. IOSL-(1,2) • Don't project anything else.",84,85
7091,53616566,We also impose the requirement that the previous tier symbol is √ as P needs a left root boundary between R and [n] to become a blocker.,9,10
7092,53616566,"In contrast to √ , it does not depend on other material in the string, so it should be projected not only immediately after R but also if the previous tier symbol is √ (from which we can infer that the tier symbol before that is R).",32,33
7093,53616566,"In contrast to √ , it does not depend on other material in the string, so it should be projected not only immediately after R but also if the previous tier symbol is √ (from which we can infer that the tier symbol before that is R).",44,45
7094,53616566,"A C between [n] and a retroflex inhibits the latter's ability to block nati, so we also need to project C if the previous tier symbol is S (our tier stand-in for [n]).",29,30
7095,53616566,"As arbitrary retroflex segments are projected only if the previous tier symbol is S, projecting C after S effectively blocks projection of retroflexes.",11,12
7096,53616566,The projection of R is less restricted than that of arbitrary retroflexes as the latter are only projected if the previous tier symbol is S. This discrepancy matters only in cases where a C-segment occurs after [n] .,22,23
7097,89638,"As soon as the EoS symbol is generated, we generate the next sentence.",5,6
7098,125969731,"Second, instead of using direct embeddings of rules a t−1 and par(n t ) in LSTM f , we use another Bi-LSTM across the left and right sides of the rule (using separator symbol SEP) and use the final hidden state as inputs to LSTM f instead.",37,38
7099,52125417,Remaining values are replaced with the UNK symbol.,7,8
7100,16096,His method creates a Huffman tree for each set of productions that expand the same non-terminal symbol.,18,19
7101,16096,"Then, we begin with a designated start-symbol S, and expand a non-terminal symbol by choosing the production whose Huffman code representation is identical to the portion of the Figure 9 An example of the Wayner (1995) mimicry method.",9,10
7102,16096,"Then, we begin with a designated start-symbol S, and expand a non-terminal symbol by choosing the production whose Huffman code representation is identical to the portion of the Figure 9 An example of the Wayner (1995) mimicry method.",18,19
7103,16096,"In the embedding example given in Figure 9 (b), the secret bitstring is 1101110 and a symbol ""•"" is used to indicate the current bit in reading the string.",19,20
7104,16096,"At the beginning, the prefix string of the secret message •1101110 is ""1"" which is associated with the second production, so the start-symbol S is expanded to AC.",28,29
7105,14011527,Swanson and Charniak (2012) used binary feature representations of CFG and Tree Substitution Grammar (TSG) rules replacing terminals (except for function words) by a special symbol.,31,32
7106,237048455,It was also used for making pin plates and pin dishes and many artists produced on-oniex sculptures with various animals and figures attached Question: Is there such a stone as Brazilian onyx True → False Passage: Atomic number -The atomic number or proton number (symbol Z) of a chemical element is the number of protons found in the nucleus of an atom.,49,50
7107,17115494,"Definition 1 A FSTG over languages L 0 and L 1 is a tuple G = 〈N , Σ, ∆, S, R〉, where N is a finite nonempty set of nonterminal symbols, Σ is a finite nonempty set of L 0 tokens, ∆ is a finite nonempty set of L 1 tokens, S ∈ N is the designated start symbol and R is a finite nonempty set of finite-state transduction rules on the forms: A → e/ f B, A → ε/ε where A, B ∈ N and e/ f ∈ (Σ * × ∆ * ) − {ε/ε}.",68,69
7108,17115494,"To see why, simply imagine the degenerate case where every biterminal has exactly one unique preterminal symbol associated with it.",17,18
7109,17115494,"Definition 2 A PFSTG over languages L 0 and L 1 is a tuple G = 〈N , P, Σ, ∆, S, R〉, where N is a finite nonempty set of nonterminal symbols, P is a finite nonempty set of preterminal symbols, disjoint from N , Σ is a finite nonempty set of L 0 tokens, ∆ is a finite nonempty set of L 1 tokens, S ∈ N is the designated start symbol and R is a finite nonempty set of preterminalized finite-state transduction rules on the forms: A → QB, A → ε/ε, Q → e/ f where A, B ∈ N , Q ∈ P and e/ f ∈ (Σ * × ∆ * ) − {ε/ε}.",84,85
7110,17115494,The bracketing PFSTG has only one nonterminal symbol and one preterminal symbol.,7,8
7111,17115494,The bracketing PFSTG has only one nonterminal symbol and one preterminal symbol.,11,12
7112,17115494,Splitting helps We can introduce new nonterminal or preterminal symbols by splitting off some of the probability mass of an existing symbol to one or more new symbols.,21,22
7113,17115494,Splitting into new symbols is a problem that can be formulated as splitting the probability mass of one symbol to several existing symbols.,18,19
7114,17115494,The assumption that the probability mass of the symbol being split could end up in any of the existing symbols makes it necessary to have some mechanism to control the destination of the probability mass.,8,9
7115,17115494,"Remember that we are dealing with preterminalized grammars, and being able to have different splitting policies depending on where in the rule the symbol occurs is important.",24,25
7116,17115494,"To keep x as a symbol, it needs to retain some of the probability mass, and the simplest way to do this is to have non-zero pseudo counts for it; this is the approach we took.",5,6
7117,17115494,"General nonterminal (and preterminal) splitting algorithm To refresh memory, a rule consists of a single nonterminal (or preterminal) symbol (i) on the left-hand side, and a sequence of nonterminal (or preterminal) and biterminal symbols on the right-hand side (φ).",23,24
7118,17115494,"Consider the preterminal P. We will split it into P e/ f (non-singletons), P f (input singletons) and P e (output singletons), and divide the set of rules so that the singletons must be produced by the singleton symbols, and the non-singletons must be produced by the non-singleton symbol.",63,64
7119,17115494,In a PLITG in normal form there may be at most one preterminal symbol on the right-hand side.,13,14
7120,17115494,"Model Definition 3 A syntax-directed transduction grammar (SDTG) in normal form is a tuple 〈N , Σ, ∆, S, R〉 where N is a finite nonempty set of nonterminal symbols, Σ is a finite nonempty set of input language symbols, ∆ is a finite nonempty set of output language symbols, S ∈ N is the designated start symbol, and R is a finite nonempty set of syntax-directed transduction rules on the forms: S → A, A → ϕ; π, A → e/ f where A ∈ N , ϕ ∈ N N N * , π is a permutation vector over ϕ, and e/ f ∈ (Σ * × ∆ * ) − (ε/ε).",69,70
7121,17115494,"Definition 4 An inversion transduction grammar (ITG) in normal form is an SDTG in normal form, where the number of nonterminals allowed on the right-hand side is exactly two for all nonterminals except the start symbol (which may only have one).",40,41
7122,17115494,"The theory of promoting preterminals to nonterminals is to insert unary rules, where the preterminal rewrites into exactly one nonterminal symbol.",21,22
7123,195185527,"Syntax-aware Semantic Role Labeler Our semantic role labeler (upper block in Figure 2 ) estimates the probability of role r given the hidden states of candidate argument word i and predicate word p: p(r|t i , t p , l) ∝ exp(W l,r (t i • t p )), (4) where t i and t p are representations for word i and predicate p, respectively, and l is the lemma of predicate p; symbol • denotes concatenation and ∝ signifies proportionality.",88,89
7124,53223447,"2017) , including tokenizing and lowercasing the text, replacing all digit characters with the ""#"" symbol and label all name entities with CoreNLP toolkit 1 .",19,20
7125,49653963,"Vocabulary We assume a unique vocabulary for both encoder and decoder that comprises the words occurring during training, the out-of-vocabulary token, and the special symbol used to mark the position of the predicate, thus V = {v 1 , ..., v N } ∪ {U N K, < P RED >}.",30,31
7126,49653963,The symbol <PRED> in each source marks the predicate for which the model is expected to generate a correct predicate-argument structure.,1,2
7127,14004511,Digit and symbol features.,2,3
7128,30615279,The boundary feature embeddings are calculated as follows: E i = Emb c (c i ) ⊕ Emb c (c i+1 ) ⊕ Emb b (b i ) g(b i ) = σ(W g E i + b g ) Emb bf (b i ) = g(b i )h i (7) where ⊕ is the symbol for concatenation.,63,64
7129,250390889,Note that we treat the special symbol [CLS] (or <s>) as the impossible answers for implicit opinions that without corresponding holders.,6,7
7130,250390889,"Since BERT-like models are more sensitive to sentence-pair input, we concatenate h, t, e with a special symbol [PAD] and treat them together as sentence B. Concretely, we build the sequence pack in the form of: x = [[CLS]; s; [SEP]; h[PAD]t[PAD]e; [SEP]] (5) Under the circumstances of implicit opinion, the empty h, t, e terms are replaced with a special token [EMP] .",24,25
7131,2303379,"Unless otherwise specified, the character dictionary is extracted from the training set and unknown characters are mapped to a special symbol that is not used elsewhere.",21,22
7132,18051891,"Pictures of this kind can be used without any further explanation on the basis of common socially-mediated understanding, e.g., a heart as a symbol of love, an apple and the snake as an symbol of original sin, or the peace symbol.",27,28
7133,18051891,"Pictures of this kind can be used without any further explanation on the basis of common socially-mediated understanding, e.g., a heart as a symbol of love, an apple and the snake as an symbol of original sin, or the peace symbol.",38,39
7134,18051891,"Pictures of this kind can be used without any further explanation on the basis of common socially-mediated understanding, e.g., a heart as a symbol of love, an apple and the snake as an symbol of original sin, or the peace symbol.",46,47
7135,14045960,"And the out-of-character-set words will be replaced with a symbol ""<UNK>"" which represents Unknown.",15,16
7136,15459163,"In collaborative bootstrapping, we consider the use of two partial functions 1 h and 2 h , which either output a class label or a special symbol ⊥ denoting 'no decision'.",27,28
7137,2053926,"Then, when taking step (a, b) with color c ∈ colors (which we denote by the symbol (a, b) c in Algorithm 1), we assess the 'goodness' of this decision by the 'likelihood' L that the current subsequences of x and y selected by the step (a, b) 'belong to'/'are of' color (or state) c. As will be seen below, this allows to very conveniently identify (or postulate) 'latent classes' for character subsequences, while increasing the algorithm's running time only by a constant factor.",21,22
7138,52139984,"To take (27) as an example, a semantic structure is built up as it is incrementally parsed, as shown in each step of ( 28 In ( 28 )-( 29 ), the symbol ?",39,40
7139,49191384,"The symbol ""+"" indicates that the higher the value is, the better the model performs.",1,2
7140,49191384,"The symbol ""-"" is the opposite.",1,2
7141,49191384,"The symbol ""+"" indicates that the higher the value is, the better the model performs.",1,2
7142,49191384,"The symbol ""-"" is the opposite.",1,2
7143,49191384,"The symbol ""+"" indicates that the higher the value is, the better the model performs.",1,2
7144,49191384,"The symbol ""-"" is the opposite.",1,2
7145,247158769,"R * is the target instance to be classified, the symbol ""+"" represents the degree of relevance and ""−"" represents the degree of irrelevance.",11,12
7146,210148249,"Dolatian and Heinz (2018a) , their proof relies on making the training data 'boundary enriched' with the reduplicative boundary symbol ⇠, e.g. the training data for initial-CV reduplication is {(pat, pa⇠pat), (mara, ma⇠mara), etc.}.",23,24
7147,210148249,"For clarity the ⇠ symbol is used throughout this paper to denote the boundary between a base and its reduplicant, however no such boundary is present in the model's training data.",4,5
7148,210148249,"EDs are composed of a recurrent encoder, which sequentially processes an input string to yield a vector representation of the sequence in R n , and a recurrent decoder which takes the encoded representation of the input as a starting state and continues producing outputs until it produces a target stop symbol or reaches an experimenter-defined maximum length.",52,53
7149,210148249,"The symbol ⌃ stands for any segment in the alphabet except for {o, n}.",1,2
7150,210148249,"The boundary symbol ⇠ is a symbol in the output alphabet , and is not necessary.",2,3
7151,210148249,"The boundary symbol ⇠ is a symbol in the output alphabet , and is not necessary.",6,7
7152,210148249,"In the input string, we underline the input symbol which FST will read next.",9,10
7153,210148249,The symbol marks the empty string.,1,2
7154,210148249,"But for other tuples, the form of the arc is: input state input symbol:output string !",15,16
7155,220282290,This paper proposes a method for the joint optimization of constraint weights and symbol activations within the Gradient Symbolic Computation (GSC) framework.,13,14
7156,220282290,This fact is then used to recast the problem of learning constraint weights and symbol activations in GSC as a quadratically-constrained version of learning lexically-scaled faithfulness grammars.,14,15
7157,220282290,Introduction and background This paper proposes a method for the joint optimization of constraint weights and symbol activations within the Gradient Symbolic Computation (GSC) framework.,16,17
7158,220282290,This fact is then used to recast the problem of learning constraint weights and symbol activations in GSC as a quadratically-constrained version of learning lexically-scaled faithfulness grammars.,14,15
7159,10852207,"A pointer is a symbol that stands for a particular morpheme, and the recourse to pointers relies on the assumption that their length is lesser than that of the morphemes they replace.",4,5
7160,10852207,"An alternative to the recourse to a list of pointers consists in using a binary string (in this case 100) where the i-th symbol is set to 1 (or +) if the i-th stem is being pointed to, and to 0 (or -) otherwise.",27,28
7161,26698484,"The documents and summaries are first lowercased and tokenized, and all digit characters are replaced with the ""#"" symbol, similar to (Nallapati et al.,",21,22
7162,18860361,"The symbol ""r"" indicates that it is not a kaaraka relation and the numeral 6 represents sa shthii (sixth) relation.",1,2
7163,550145,"We focus initially on unsupervised learning of morphology for three reasons: first, because we already have a quite successful unsupervised morphological learner; second, the final suffix of a word is typically the strongest single indicator of its syntactic category; and third, analysis of a word into a stem T plus suffix F allows us (given our knowledge that the suffix F is a stronger indicator of category than the stem T) to collapse many distinct stems into a single cover symbol for purposes of analysis, simplifying our task, as we shall see.",88,89
7164,550145,"Next, we replace words belonging to the K most reliable signatures (where K=50 in these experiments) by their associated signature transforms, and we in effect ignore all other words, by replacing them by a distinguished ""dummy"" symbol.",43,44
7165,15518351,"When a sequence of symbols is observed, a language model predicts the probability of occurrence of the next symbol in the sequence.",19,20
7166,15518351,"We model the probability that symbol y succeeds x as P (y = v|x) = e w v φm(x) u∈V e w u φm(x) , (1) where W = {w v } v∈V is the set of parameters, and φ m (x) is the vector of features extracted from x, the sequence preceding y. We will describe the features shortly.",5,6
7167,15518351,Let x 1:i denote the subsequence of x starting at the first position up to the i th position and y i the next symbol in the sequence.,26,27
7168,15518351,"Any symbol in V observed in a given context is a positive example, while any symbols in V that does not appear in this context is a negative example.",1,2
7169,14861139,"On both the obverse and the reverse side there is a tripod (vessel standing on three legs), Apollo's sacred symbol.",23,24
7170,1968269,"Since our goal presently is to identify word-final suffixes, we assume by convention that all words end with an end-of-word symbol (traditionally ""#'), and we then tally the counts of all n-grams of length between two and six letters that appear word finally.",27,28
7171,1968269,"For each stem, we make a list of those suffixes that appear with it, and we call an alphabetized list of such suffixes (separated by an arbitrary symbol, such as period) the stem's signature; we may think of it as a miniparadigm.",30,31
7172,7007059,"In order to make MT applicable for larger example bases while improving or maintaining its speed, we investigate different approaches to efficient approximate sentence matching: a) sequential algorithms for the computation of the similarity, where speed improvements are based on limiting the number of symbol comparisons (e.g. cut-off heuristics); b) data structures, where a traversal of the structure can be employed to compute the similarity; c) sentence indexing and retrieval, where aspects of similarity are traditionally modeled by factors based on frequencies such as tf and idf, but in the case of approximate sentence retrieval should also include word order and position.",48,49
7173,7007059,"We introduce similarity metrics between sequences Q and D with length |Q| and |D|, where i and j denote an index in a sequence and Q i and D j denote the ith and jth symbol in sequence Q and D. Sequential Search Levenshtein Distance Algorithms (LD W F and LD BR ).",36,37
7174,7007059,"The edit distance for two sequences Q and D is defined as the minimum number of edits, i.e. symbol insertions, deletions and substitutions, needed to transform Q into D (Levenshtein, 1966) .",19,20
7175,7007059,"In fact, the existing implementations consider only characters as symbols, while our implementation abstracts from that view and allows any type of symbol as the basic element of a sequence.",24,25
7176,7007059,Each node in a trie stores a single symbol of the corresponding sequence and a node represents the prefix of the key on the path of the root up to that node.,8,9
7177,7007059,"Each node of a ternary search tree stores a single symbol for comparison with a symbol of a search key, and pointers to three children which determine which subtree to search next, based on the result of a comparison, i.e. lower, equal or higher lexicographical order.",10,11
7178,7007059,"Each node of a ternary search tree stores a single symbol for comparison with a symbol of a search key, and pointers to three children which determine which subtree to search next, based on the result of a comparison, i.e. lower, equal or higher lexicographical order.",15,16
7179,5634542,"A HRG over a set of labels C is a rewriting system G = 〈N , T, P, S〉, where N and T ⊂ C are the finite sets of nonterminal and terminal labels (T ∩ N = ), and S ∈ N is the start symbol.",53,54
7180,5634542,A production template p * ∈ P * is realised as a set of rules P by substituting all variables v for any symbol s ∈ N ∪ T : P = {∀ v∈V ∀ s∈N ∪T p * [v/s]}.,23,24
7181,9774141,"Section 3 gives an overview of data construction, including emotional symbol library, emotional dictionary and network slang dictionary.",11,12
7182,9774141,The dictionary of emoticons We construct the dictionary of emoticons by combining emotional symbol library in microblog post with other statistical methods.,13,14
7183,9774141,"Firstly, two laboratory personnel obtain emotional symbol library, and keep the emoticons with the same emotional orientation after their analysis, and then get rid of emotional symbols with ambiguous orientation, the result is described in Table 4 .",7,8
7184,235694578,We use h c to denote the embedding of the special symbol [CLS].,11,12
7185,22002351,2014) proposed a post-processing approach that learns the position of the source word when an UNK symbol is produced during decoding.,19,20
7186,22002351,"By this position information, the UNK symbol (unknown words) can be replaced by the correct translation using a lexical table.",7,8
7187,22002351,"During the training, if the target word is an UNK symbol, or the target word is not in the memory (due to the limited word pairs in the memory), this word is simply skipped from back-propagation.",11,12
7188,231564265,"Background 2.1 Strictly local languages ε denotes the empty string and S * the Kleene closure of the set S. S k denotes the proper subset of S * that only contains strings of length k, and s k represents a string consisting of k occurrences of the symbol s. Let Σ be some fixed alphabet and s ∈ Σ * .",49,50
7189,231564265,"Let P proj : Σ → [0, 1] represent the probability that a symbol in Σ is projected to the tier: for example, P proj (a) := 0.5 indicates that there is a 50% chance that each a symbol will be projected.",16,17
7190,231564265,"Let P proj : Σ → [0, 1] represent the probability that a symbol in Σ is projected to the tier: for example, P proj (a) := 0.5 indicates that there is a 50% chance that each a symbol will be projected.",47,48
7191,231564265,"A sequence y that is a subsequence of x can be written (y n ) n∈J where J ⊆ I. Using this notation, we can define the probability of projecting a particular subsequence y = (y n ) n∈J from the input x = (x n ) n∈I as follows: π P (x)(y) := k∈J P proj (x k )• k∈I\J [1 − P proj (x k )] That is, the probability of projecting the output y given the input x is the product of the probabilities associated with projecting each symbol that is projected and with not projecting each symbol that is not projected.",105,106
7192,231564265,"A sequence y that is a subsequence of x can be written (y n ) n∈J where J ⊆ I. Using this notation, we can define the probability of projecting a particular subsequence y = (y n ) n∈J from the input x = (x n ) n∈I as follows: π P (x)(y) := k∈J P proj (x k )• k∈I\J [1 − P proj (x k )] That is, the probability of projecting the output y given the input x is the product of the probabilities associated with projecting each symbol that is projected and with not projecting each symbol that is not projected.",114,115
7193,231564265,"Note that although TSL (and hence pTSL, for the same reasons discussed in de Santo and Graf, 2017) is not in general closed under relabeling, it is closed if no segment corresponds to more than one abstract symbol.",42,43
7194,231564265,"For example, instead of P proj (x i ), we may use P proj (x i |x i−1 ), P proj (x i |y j−1 ) (where y j−1 is the previously projected symbol), P proj (x i |x i−1 , y j−1 ), etc.",40,41
7195,231564265,"The pTSL-2 grammar used as a counterexample in the previous section does not meet the criterion for this algorithm to be applied: the prohibited 2-factor ba contains the symbol a, which does not have a projection probability of 1.",31,32
7196,1350082,"In this and all of the other grammars in this paper, the start symbol is the non-terminal symbol of the first rule in the grammar.",14,15
7197,1350082,"In this and all of the other grammars in this paper, the start symbol is the non-terminal symbol of the first rule in the grammar.",20,21
7198,11653128,symbol in the predicate definition indicates that every entity mention must take one and only one argument role. (,0,1
7199,9369033,The symbol e and c always indicate an English page and a Chinese page respectively in this paper.,1,2
7200,13047490,"1) where n s (w i ) counts the character string composed of w i and the symbol "".""",19,20
7201,15023737,"Define C x , C y as the meaning code of x, y, and C xend , C yend is respectively the end symbol of x, y. Take f = 0.1, e = 0.5 as a matter of experience.",25,26
7202,11332377,"The first replaces digits with the symbol #, so ""1999"" becomes ####.",6,7
7203,518630,"Since the hidden Markov model comprises an unobservable Markov chain and a set of random processes that can be directly measured, it seems most natural to represent speech as a hidden Markov chain in which the hidden states correspond to the putative unobservable phonetic symbols and the state-dependent random processes account for the variability of the observable acoustic manifestation of the corresponding phonetic symbol.",66,67
7204,18647421,"Its form is designed as, @Subject_Var_N+Operator+Object where ""@"" is a hint symbol to indicate which variable will be transformed.",14,15
7205,17861603,"mark is a special symbol in the MLN language that specifies a hard constraint that for every grounding of s and t, there is exactly one act label.",4,5
7206,17861603,"The ""+"" sign is a special symbol in MLNs that allows us to define multiple weights for a single formula.",8,9
7207,17861603,"Specifically, we can set a different weight for each partially ground formula obtained by grounding all variables in the formula corresponding to the + symbol.",25,26
7208,8247565,"The basic token features (see Table 1 (c)) computed from (1) the candidate trigger word and (2) the surrounding tokens in a window of two; character bigrams and trigrams of the candidate trigger word; word n-grams (n=1,2,3) of the candidate trigger word and its context words in a window of three; whether the candidate trigger word contains a digit; whether the candidate trigger word contains an upper case letter; whether the candidate trigger word contains a symbol.",93,94
7209,8247565,symbol models commonsense knowledge that only one of the types in the domain ∆ ttype of ttype is true for every unique combination of sid and tid.,0,1
7210,8247565,"The ""+"" symbol indicates that each grounding of Formula ( 9 ) may have a different weight.",4,5
7211,9768072,"A Context-Free Grammar (CFG) G = (N, W, R, S) consists of disjoint finite sets of nonterminal symbols N and terminal symbols W , a finite set of rules R of the form A → α where A ∈ N and α ∈ (N ∪ W ) ⋆ , and a start symbol S ∈ N . (",62,63
7212,9768072,"A Probabilistic Context-Free Grammar (PCFG) is a quintuple (N, W, R, S, θ) where N , W , R and S are the nonterminals, terminals, rules and start symbol of a CFG respectively, and θ is a vector of non-negative reals indexed by R that satisfy ∑ α∈R A θ A → α = 1 for each A ∈ N , where R A = {A → α : A → α ∈ R} is the set of rules expanding A. Informally, θ A → α is the probability of a node labelled A expanding to a sequence of nodes labelled α, and the probability of a tree is the product of the probabilities of the rules used to construct each non-leaf node in it.",40,41
7213,9768072,The PCFG generates the distribution G S over the set of trees T S generated by the start symbol S; the distribution over the strings it generates is obtained by marginalising over the trees.,18,19
7214,10551180,"Each utterance is padded by an observed utterance boundary symbol $ to the left and to the right, hence U i,0 = U i,n i +1 = $.",9,10
7215,10551180,"x 1:n is a sequence of elements of Σ of length n. γ is a probability vector of length |Σ| + 1 drawn from a sparse Dirichlet prior, giving the probability for each phoneme and the special word-boundary symbol #.",43,44
7216,1486279,We include the phone symbol [-J to indicate that a phoneme may delete.,4,5
7217,1486279,"Since we have augmented the phone set to include a deletion symbol, the only stumbling block to such an alignment would be if phones insert.",11,12
7218,1486279,Our output set is simply a direct encoding of the 47 element phone set used in Ljolje [1] plus the symbol [-] if the phoneme deletes.,22,23
7219,12514667,"The only constraints provided directly in the lexical entry are the STEM value (kond, corresponding to \lex in Figure 1 ) and the PRED value _search;look.for_v_rel, i.e., the predicate symbol for the semantic relation associated from this entry.",34,35
7220,12514667,The import facility creates the name symbol for that semantic predicate on the basis of the gloss or alternatively of the orthography of the stem (as specified by the user).,6,7
7221,6094747,"The first group of base generators is only based on the type of symbol the equation has, the second group is the pair (#1, #2) to represent equations with one or two symbols.",13,14
7222,16880881,"Here £ cat¤ refers to the information types in Table 1 , and £ sym¤ 3 is a symbol that can be used as a separator for this specific information type.",18,19
7223,16880881,"CONTENT is associated with one of three values: Information type if the token is a keyword; SYM if the token is a symbol; NUM if the token consists only of numeric characters; otherwise, the value 1.",24,25
7224,16880881,"STARTING SYMBOL indicates whether the token is a special punctuation symbol: ENDING SYMBOL and SECOND ENDING SYM-BOL indicate whether the last and second-to-last characters of the token are punctuation symbols, respectively.",10,11
7225,52943630,"2015b) and convolutional neural network (CNN) (Wang and Xu, 2017) , have Context Encoder Segment Decoder Figure 1 : A Segmental Language Model (SLM) works on y = y 1 y 2 y 3 y 4 with the candidate segmentation y 1 , y 2:3 and y 4 , where y 0 is an additional start symbol which is kept same for all sentences.",64,65
7226,52943630,"Therefore, the probability for SLMs to generate a given sentence is the joint probability of all possible segmentation: p(y 1:T ) = T 1 ,T 2 ,... i p(y (i) 1:T i ) = T 1 ,T 2 ,... i T i +1 t=1 p(y (i) t |y (i) 0:t−1 ) (4) where y (i) T i +1 = eos is the end of segment symbol at the end of each segment, and y (i) 0 is the context representation of y (1:i−1) .",87,88
7227,52943630,"Moreover, for sentence generation, SLMs are able to generate arbitrary sentences by generating segments one by one and stopping when generating end of sentence symbol EOS .",26,27
7228,236486315,Code Items Using substrings This method involves finding peripheral letters that map unambiguously to some symbol and then finding plausible splitting points within words to create partial words that can be added to the training data.,15,16
7229,236486315,"First, we identify all word-final letters that always correspond to the same symbol in the transcription.",15,16
7230,230433881,An example is the question What is the stock symbol for Mars candy?,9,10
7231,230433881,"This question is not answerable with any description of a stock symbol (that is, an answer to the what question), because Mars is not a publicly traded company and thus does not have a stock symbol.",11,12
7232,230433881,"This question is not answerable with any description of a stock symbol (that is, an answer to the what question), because Mars is not a publicly traded company and thus does not have a stock symbol.",39,40
7233,230433881,"A better response would be to point out the presupposition failure, as in There is no stock symbol for Mars candy.",18,19
7234,1805495,The generation process is repeated until LSTM outputs the symbol that indicates the end of sentence.,9,10
7235,182952800,"Following Valiant and Paterson (1975) , we define a deterministic one-counter automaton (DCA 1 ) to be a DPA with a stack alphabet consisting of only one symbol.",32,33
7236,182952800,"Similarly, we call a DPA that contains k separate stacks, with each stack using only one stack symbol, a deterministic k-counter automaton (DCA k ).",19,20
7237,182952800,"2018) trained recurrent networks to predict the last appropriate closing parenthesis, given a Dyck-2 sequence without its last symbol.",20,21
7238,182952800,"In all of the languages we investigate in this paper, the open parentheses are always allowable as next symbol; we assign the set of open parentheses the number 0.",19,20
7239,905294,"Let h y (m) denote the parent of x m in y (using a special null symbol when m is the root of the tree), and h y (m) denotes the parent of x m in the predicted tree y .",19,20
7240,5113688,"The s-th sentence is a sequence of words w s = {w s,i } Ns i=0 , where w s,i ∈ L is a word at position i in this sentence and w s,0 = < s > is an artificial symbol that is added at the beginning of each sentence.",48,49
7241,5113688,"The symbol • represents any word in the vocabulary so that n (•|r s,i ) \s,i = l∈L n (l|r s,i ) \s,i .",1,2
7242,5113688,The symbol • represents any possible role to make the probability distribution summing up to 1.,1,2
7243,5113688,The less frequent words were replaced by the symbol <unk>.,8,9
7244,5113688,We combine baseline 4-gram MKN model with other models via linear combination (in the tables denoted by the symbol +) that is simple but very efficient technique to combine LMs.,21,22
7245,243865350,"Instead of using inefficient heuristic rules which perhaps mask correct words in some case (e.g., y 1 y 4 ), REWRITENAT utilizes an additional locator module to learn to explicitly distinguish erroneous translation pieces (e.g., ŷ2 ŷ3 ), annotated as special symbol (i.e., [MASK] ).",47,48
7246,243865350,"Subsequently, the generated representations H r with respect to special symbol ""[MASK]"" are fed to a classifier π r to generate the target words as follows: π r (r t |y r t , X) = softmax(W r h r t + b r ) (4) where W r and b r are trainable parameters, and represent weight matrix and bias vector, respectively.",11,12
7247,243865350,"In details, the words annotated as ""revise"" are substituted by special symbol, denoted as ""[MASK]"", while the remaining hold.",14,15
7248,2995103,All pronunciations use the Arpabet symbol set. •,5,6
7249,14383940,"We use eight types: punctuation, symbol, numeric, uppercase, capitalized, lowercase, non-Latin, and other.",7,8
7250,52013218,y N ] where _GO is a special symbol indicating the begin of a response.,9,10
7251,8857074,"To facilitate the navigation in the deluge of information, microblogging services allow users to insert hashtags starting with the ""#"" symbol (e.g., #followfriday) into their posts to indicate the context or the core idea.",23,24
7252,10590259,"In essence, frequent character n-gram sequences will be merged to form one symbol.",15,16
7253,10590259,"For character-level encoding (CHAR), we preserved word boundaries by replacing space with a special symbol and then separated every character with a space.",19,20
7254,215825879,"To reflect the change, the previous SHIFT operation is modified into a GEN operation defined as follows: • GEN generates a terminal symbol and add it to the stack and the output buffer.",24,25
7255,29764642,"Decoding in generative neural models All of the parsers we investigate in this work (the discriminative parser RD, and the two generative parsers RG and LM, see Section 1) produce parse trees in a depth-first, left-to-right traversal, using the same basic actions: NT(X ), which opens a new constituent with the non-terminal symbol X; SHIFT / GEN(w), which adds a word; and RE-DUCE, which closes the current constituent.",68,69
7256,14519034,"Additionally, the action may be one of many possible non-terminal symbols, in which case the predicted non-terminal symbol is pushed to the stack.",23,24
7257,14519034,"We mapped all the low-frequency words to the unique symbol ""UNK"" and inserted a special symbol ""EOS"" at the end of both source and target sentences.",11,12
7258,14519034,"We mapped all the low-frequency words to the unique symbol ""UNK"" and inserted a special symbol ""EOS"" at the end of both source and target sentences.",19,20
7259,14519034,"The special symbol (""EOS"") is treated as the root node (""ROOT"") of the parsed tree.",2,3
7260,15344879,"w n , where token w 0 is a special root symbol, a span graph is a directed, labeled graph G = (V, A), where V = {s i,j |i, j ∈ (0, n) and j > i} is a set of nodes, and A ⊆ V × V is a set of arcs.",11,12
7261,15344879,"When parsing a sentence of length n (excluding the special root symbol w 0 ), its corresponding dependency tree will have n nodes and n − 1 arcs.",12,13
7262,220058111,"In FSTs, the transition from one state to another depends on the current state and the next input symbol.",19,20
7263,57356,"We only remove the hashtag symbol, keeping the words.",5,6
7264,232021767,"denotation goal for r r value n (""none"") none zero i (""increment"") r is ablated incrementing f (""feedback"") none previous r u (""unused"") increase zero s (""all set"") increase previous r Note: if r is a vector, its size is added before a mode symbol: '3f', '3u', '3s'.",66,67
7265,198898875,"Since seq2seq models apply some operation over the whole vocabulary -usually the softmax operation -when deciding what symbol to output next, large LF vocabularies can slow them down considerably.",17,18
7266,220282373,+ (repeat one or more symbols 3 ) and non-matching symbols are replaced by the symbol-pair found in the alignment: i:o.,18,19
7267,11468837,or http?://* were replaced with URL symbol.,7,8
7268,11468837,Then we replaced numbers with $NUM symbol.,7,8
7269,20414377,"Knowing little about the task, we lemmatize and lowercase the sentence 3 and restructure the CWI inputs as a sequence where the target word is separated by a placeholder symbol < s > followed by the context sentence, e.g. arithmetic < s > the short word math or math are often use for arithmetic , geometry or basic algebra by young student and their school .",30,31
7270,215826749,We also use a special symbol to replace tokens that contain at least one numeric character.,5,6
7271,46939554,"Whenever a partial hypothesis in the beam ends with the sentence end symbol (<EOF>), the counter will be increased by 1 (line 26-28).",12,13
7272,9326499,We have used a simple mechanism where a negative context is defined as the group of words after a negative word until a punctuation symbol.,24,25
7273,18410255,"We emphasize dataset with symbol * for that this dataset appears both in training and test sets, which is very useful to our third submission S2 (see Section 3.4 for more details).",4,5
7274,18410255,"For the data set with symbol * (i.e., headlines), we use all headlines pairs.",5,6
7275,226262361,"In particular, as illustrated in Section 2.2, given the sentence pair P i j , the embedding of the [CLS] symbol from the top layer of BERT is denoted as C i j .",24,25
7276,21692618,"If the deletion of that symbol results in a small decrease in the support (less than a threshold provided by the user), the newly created regular expression is accepted.",5,6
7277,11461990,"For all the examples, a word-by-word gloss is provided under the Chinese expression, which is on the left of the <> symbol used to connect the Chinese expression and its English translation.",28,29
7278,74369,"Negations were discovered based on ""not"" and ""n't"" tokens, and a negated context was defined as a set of words falling between a negation and a ""terminal"" punctuation symbol {., ; , , , !, ?}.",35,36
7279,3087412,"For each non-terminal grammar symbol, the model posits a Hierarchical Dirichlet Process over its refinements (subsymbols) to automatically learn the granularity of syntactic categories.",6,7
7280,3087412,For each observed coarse symbol s: 1.,4,5
7281,3087412,For each subsymbol z of symbol s: (a) Draw word emission multinomial φ sz ∼ Dir(φ 0 ). (,5,6
7282,3087412,b) For each context value c: i. Draw child symbol generation multinomial θ szc ∼ Dir(θ 0 ).,11,12
7283,3087412,"For each child symbol s : A. Draw second-level infinite multinomial over subsymbols π s szc ∼ DP(α, β s ).",3,4
7284,3087412,For each tree node i generated in context c by parent symbol s and parent subsymbol z : 1.,11,12
7285,3087412,Draw coarse symbol s i ∼ Mult(θ s z ).,2,3
7286,3087412,"Each node of the dependency tree is comprised of three random variables: an observed coarse symbol s, a hidden refined subsymbol z, and an observed word x. In the following let the parent of the current node have symbol s and subsymbol z ; the root node is generated from separate root-specific distributions.",16,17
7287,3087412,"Each node of the dependency tree is comprised of three random variables: an observed coarse symbol s, a hidden refined subsymbol z, and an observed word x. In the following let the parent of the current node have symbol s and subsymbol z ; the root node is generated from separate root-specific distributions.",41,42
7288,3087412,First we draw symbol s from a finite multinomial distribution with parameters θ s z c .,3,4
7289,3087412,"As the indices indicate, we have one such set of multinomial parameters for every combination of parent symbol s and subsymbol z along with a context c. Here the context of the current node can take one of six values corresponding to every combination of direction (left or right) and valence (first, second, or third or higher child) with respect to its parent.",18,19
7290,3087412,"Here the selection of π is indexed by the current node's coarse symbol s, the symbol s and subsymbol z of the parent node, and the context c of the current node.",13,14
7291,3087412,"Here the selection of π is indexed by the current node's coarse symbol s, the symbol s and subsymbol z of the parent node, and the context c of the current node.",17,18
7292,3087412,"For each unique coarse symbol s we tie together the distributions π ss z c for all possible parent and context combinations (i.e., s , z , and c) using a Hierarchical Dirichlet Process (HDP).",4,5
7293,3087412,"By formulating the generation of z as an HDP, we can share parameters for a single coarse symbol's subsymbol distribution while allowing for individual variability based on node parent and context.",18,19
7294,3087412,"Finally, we generate the word x from a finite multinomial with parameters φ sz , where s and z are the symbol and subsymbol of the current node.",22,23
7295,3087412,After a node is drawn we generate children on each side until we produce a designated STOP symbol.,17,18
7296,3087412,"Specifically, after drawing a node we first decide whether to proceed to generate a child or to stop conditioned on the parent symbol and subsymbol and the current context (direction and valence).",23,24
7297,3087412,We can combine the stopping decision with the generation of the child symbol by including a distinguished STOP symbol as a possible outcome in distribution θ.,12,13
7298,3087412,We can combine the stopping decision with the generation of the child symbol by including a distinguished STOP symbol as a possible outcome in distribution θ.,18,19
7299,3087412,"No-Split Model Variant In the absence of subsymbol refinement (i.e., when subsymbol z is set to be identical to coarse symbol s), our model simplifies in some respects.",24,25
7300,3087412,"In particular, the HDP gener-ation of z is obviated and word x is drawn from a word distribution φ s indexed solely by coarse symbol s. The resulting simplified model closely resembles DMV (Klein and Manning, 2004) , except that it 1) explicitly generate words x rather than only partof-speech tags s, 2) encodes richer context and valence information, and 3) imposes a Dirichlet prior on the symbol distribution θ.",27,28
7301,3087412,"In particular, the HDP gener-ation of z is obviated and word x is drawn from a word distribution φ s indexed solely by coarse symbol s. The resulting simplified model closely resembles DMV (Klein and Manning, 2004) , except that it 1) explicitly generate words x rather than only partof-speech tags s, 2) encodes richer context and valence information, and 3) imposes a Dirichlet prior on the symbol distribution θ.",80,81
7302,3087412,"First we assume the following mean-field factorization of our variational distribution: q(β, θ, π, φ, z) = q(z) • s q(β s ) • T z =1 q(φ s z )• c q(θ s z c ) • s q(π ss z c ), (7) where s varies over the set of unique symbols in the observed tags, z denotes subsymbols for each symbol, c varies over context values comprising a pair of direction (left or right) and valence (first, second, or third or higher) values, and s corresponds to child symbols.",78,79
7303,3087412,q(z) of child symbol s and subsymbol z in context c when generated by parent symbol s and subsymbol z .,4,5
7304,3087412,q(z) of child symbol s and subsymbol z in context c when generated by parent symbol s and subsymbol z .,16,17
7305,3087412,"Similarly, the updates for q(θ) and q(φ) are given by: q(θ s z c ) = Dir θ s z c ; θ 0 + E q(z) [C s z c (s)] , q(φ s z ) = Dir φ s z ; φ 0 + E q(z) [C s z (x)] , where C s z c (s) is the count of child symbol s being generated by the parent symbol s and subsymbol z in context c and C s z x is the count of word x being generated by symbol s and subsymbol z .",80,81
7306,3087412,"Similarly, the updates for q(θ) and q(φ) are given by: q(θ s z c ) = Dir θ s z c ; θ 0 + E q(z) [C s z c (s)] , q(φ s z ) = Dir φ s z ; φ 0 + E q(z) [C s z (x)] , where C s z c (s) is the count of child symbol s being generated by the parent symbol s and subsymbol z in context c and C s z x is the count of word x being generated by symbol s and subsymbol z .",87,88
7307,3087412,"Similarly, the updates for q(θ) and q(φ) are given by: q(θ s z c ) = Dir θ s z c ; θ 0 + E q(z) [C s z c (s)] , q(φ s z ) = Dir φ s z ; φ 0 + E q(z) [C s z (x)] , where C s z c (s) is the count of child symbol s being generated by the parent symbol s and subsymbol z in context c and C s z x is the count of word x being generated by symbol s and subsymbol z .",109,110
7308,3087412,We also conduct a set of No-Split experiments to evaluate the importance of syntactic refinement; in these experiments each coarse symbol corresponds to only one refined symbol.,23,24
7309,3087412,We also conduct a set of No-Split experiments to evaluate the importance of syntactic refinement; in these experiments each coarse symbol corresponds to only one refined symbol.,29,30
7310,17335672,"We define Ω = Σ∪{+}, where + is a distinguished separation symbol.",13,14
7311,15407650,We represent stack spans by trapezoids ( i Some text and the symbol or scaled j ) in the figures to emphasize that they may or not have tree stucture.,12,13
7312,15407650,"In order to account for unknown words during training, we also adopt the strategy described by Kiperwasser and Goldberg (2016b) , where words in the training set are replaced with the unknownword symbol UNK with probability p unk = z z+f (w) where f (w) is the number of times the word appears in the training corpus.",35,36
7313,229923085,"This is only an approximation, but it helps restore the assumption that there is a coherent connection between a symbol and its type embedding.",20,21
7314,17472619,"We formulate LC parsing as a set of transitions between configurations, each of which is a pair of the stack and the input position (next input symbol).",28,29
7315,17472619,"In Figure 1 a transition σ 1 a − → σ 2 means that the stack is changed from σ 1 to σ 2 by reading the next input symbol a. We use a vertical bar to signify the append operation, e.g., σ = σ |σ 1 denotes σ 1 is the topmost symbol of σ.",29,30
7316,17472619,"In Figure 1 a transition σ 1 a − → σ 2 means that the stack is changed from σ 1 to σ 2 by reading the next input symbol a. We use a vertical bar to signify the append operation, e.g., σ = σ |σ 1 denotes σ 1 is the topmost symbol of σ.",55,56
7317,17472619,"Each stack symbol is either a nonterminal, or a pair of nonterminals, e.g., A/B, which represents a subtree rooted at A and is awaiting symbol B. We also decorate each symbol with depth; for example, σ d−1 |A d means the current stack depth is d, and the depth of the topmost symbol in σ is d − 1.",2,3
7318,17472619,"Each stack symbol is either a nonterminal, or a pair of nonterminals, e.g., A/B, which represents a subtree rooted at A and is awaiting symbol B. We also decorate each symbol with depth; for example, σ d−1 |A d means the current stack depth is d, and the depth of the topmost symbol in σ is d − 1.",30,31
7319,17472619,"Each stack symbol is either a nonterminal, or a pair of nonterminals, e.g., A/B, which represents a subtree rooted at A and is awaiting symbol B. We also decorate each symbol with depth; for example, σ d−1 |A d means the current stack depth is d, and the depth of the topmost symbol in σ is d − 1.",36,37
7320,17472619,"Each stack symbol is either a nonterminal, or a pair of nonterminals, e.g., A/B, which represents a subtree rooted at A and is awaiting symbol B. We also decorate each symbol with depth; for example, σ d−1 |A d means the current stack depth is d, and the depth of the topmost symbol in σ is d − 1.",61,62
7321,17472619,The bottom symbol on the stack is always the empty symbol ε 0 with depth 0.,2,3
7322,17472619,The bottom symbol on the stack is always the empty symbol ε 0 with depth 0.,10,11
7323,17472619,"Given the start symbol of CFG S, it finishes when S 1 is found on the stack.",3,4
7324,17472619,This can be done by first predicting that A's parent symbol is B and its sibling is C; then it unifies two different Bs to combine them.,11,12
7325,17472619,"PRED is simpler, and it just predicts the parent and sibling symbols of A. The input symbols are read by SHIFT and SCAN: SHIFT addes a new element on the stack while SCAN fills in the predicted sibling symbol.",40,41
7326,17472619,"Step Transition Stack Next input symbol 0 ε e 1 SHIFT E 1 f 2 PRED D/B 1 f 3 SHIFT D/B 1 F 2 g 4 PRED D/B 1 A/G 2 g 5 SCAN D/B 1 A 2 c 6 COMP D/C 1 c 7 SCAN D 1 D 1 c D/C 1 A 2 g A/G 2 F 2 f D/B 1 E 1 e (b) Grammar transform The algorithm above can be reformulated as a grammar transform, which becomes the starting point for our application to grammar induction.",5,6
7327,17472619,"The difference is in the start and end conditions: while our parser begins with an empty symbol, Johnson's parser begins with the predicted start symbol, and finishes with an empty symbol.",17,18
7328,17472619,"The difference is in the start and end conditions: while our parser begins with an empty symbol, Johnson's parser begins with the predicted start symbol, and finishes with an empty symbol.",27,28
7329,17472619,"The difference is in the start and end conditions: while our parser begins with an empty symbol, Johnson's parser begins with the predicted start symbol, and finishes with an empty symbol.",34,35
7330,17472619,"This is a hard constraint, and can easily be achieved by removing all chart items (of LC transformed grammar) on which the depth of the symbol exceeds δ.",28,29
7331,247593809,"For example, in Figure 1 , ""rose is a type of flower"" and ""rose is a symbol of love"" are two NL statements expressing the implicit commonsense knowledge.",20,21
7332,247593809,"For example, rose is a type of flower; rose is a symbol of love.",13,14
7333,247593809,Rose is a symbol of what?,3,4
7334,247593809,Rose is a symbol of love.,3,4
7335,247593809,"2019) , we use a special symbol to serve as the separator.",7,8
7336,247593809,"We also use DialoGPT to serve as the backbone model and consider three variables in our TBS model configuration introduced from Sections 3.1 to 3.3: hardmatching or soft-matching, special symbol as separator or NL prompt, and triple-converted-NL to represent knowledge or information seeking QA pairs.",33,34
7337,247593809,Our default model configuration is hard-symbol-NL.,7,8
7338,247593809,"Performance of Response Generation Model variant analysis To find the bestperforming configuration of our TBS method, we consider alternatives as discussed in Sections 3.1 to 3.3, and conduct 4 pairwise comparisons: soft vs. hard, prompt vs. symbol, and QA vs. relationconverted NL format.",40,41
7339,247593809,"We also compare results that combine these alternatives, e.g., soft-symbol-QA (due to space constraints, results are shown in Appendix C.1), however, we do not observe significant improvements after combining these alternatives and our best configuration in terms of average improvement is still hard-symbol-QA.",13,14
7340,247593809,"We also compare results that combine these alternatives, e.g., soft-symbol-QA (due to space constraints, results are shown in Appendix C.1), however, we do not observe significant improvements after combining these alternatives and our best configuration in terms of average improvement is still hard-symbol-QA.",55,56
7341,247593809,We thus use hard-symbol-QA as our final configuration and refer to it as TBS throughout this section.,5,6
7342,247593809,qualitative examples from TBS-hard-symbol-QA.,7,8
7343,247593809,We find that the best overall configuration is hard-symbol-QA.,10,11
7344,218487030,C: … Zinc is a chemical element with symbol Zn and atomic number 30. …,9,10
7345,8406927,"To simulate this, we apply word-level dropout noise to each document, by randomly replacing words by a unique sentinel symbol.",23,24
7346,10061023,"In order to be able to use Tweebank in other parsers, we link the unselected tokens to the wall symbol (i.e. root as the heads).",20,21
7347,17856283,"c ∈ A(c )}, if s is non-frontier and x = SHIFT • {c | c ∈ R(cs 2 , cs 1 ) ∩ A(c)}, if x = REDUCE • {c | c ∈ R(cs 1 , cs 0 ) ∩ A(c)}, if s is non-frontier and x = UNARY • R( , c 0 s 0 ) = ∅ where c 0 s 0 is the conjunctive node corresponding to the gold-standard lexical category of the first word in the sentence ( is a dummy symbol indicating the bottom of stack).",103,104
7348,44150781,"For each query in Table 1 , one response from the testing set is taken as the ground truth, together with responses with more morphological and semantic variations, marked with the symbol ""-"".",33,34
7349,6143941,"Additionally, we assume a special stop symbol, x ∅ , which indicates the end of the subgoal sequence.",7,8
7350,11913192,"We add some special symbols apart from the normal alphabets, digits, and punctuations: <SOW> as the start of the word, <EOW> as the end of the word, <MUL> as multiple characters in the middle of the word squeezed into one symbol, <PAD> as padding equally on both sides, and <UNK> as characters unseen in the training data.",51,52
7351,231692950,"In our implementation, there are 17 labels for POS in total, i.e., ADJ (adjective), ADV (adverb), ADP (ad-position), AUX (auxiliary), CCONJ (coordinating conjunction), DET (determiner), INTJ (interjection), NOUN (noun), NUM (numeral), PART (particle), PRON (pronoun), PROPN (proper noun), PUNCT (punctuation), SCONJ (subordinating conjunction), SYM (symbol), VERB (verb), and X (other).",92,93
7352,15646625,The × symbol denotes interactions; + indicates addition.,2,3
7353,2783746,"We replace singleton words in the training set with an UNK token, normalize digits to a single symbol, and initialize word embeddings for both source and target words from the publicly available word2vec (Mikolov et al.,",18,19
7354,14289184,The hypothesis space within which the CRF operates was determined by the symbol-level IPA map developed in the first step of our work-flow.,12,13
7355,14289184,It is important to note that we allowed many-to-many mapping between orthographic input character sequences and IPA output character sequences in the sense that a single input character can be mapped to multiple IPA symbols and an input multigraph (consisting of multiple orthographic characters) can be mapped to a single IPA symbol.,57,58
7356,14289184,"2016 )), we compare the accuracy and character error rate (CER) of the predictor, when using PanPhon features compared to, and along with: • Basic features pertaining to the usage of orthography-to-IPA symbol translation rules, and whether the IPA symbols were identified as consonants and vowels 9 . •",43,44
7357,2281971,"We use a key which represents each letter in the n-gram with a number or symbol, ordered by first occurrence of the letter, while maintaining whitespace (eg. """,17,18
7358,2281971,"2013) there are two distinct symbol spaces, that of the ciphertext and that of the plaintext and so there is no concept of symmetry.",6,7
7359,17540759,"A dependency context is a discrete symbol denoting a word and its syntactic role in a dependency parse graph (e.g. nsubj she, of : nmod cof f ee, of : nmod −1 cup).",6,7
7360,7872859,"f (w n ) where the function f (w) : V o → V K t ∪ ∅, rewrites a word from the vocabulary of the original problem V o to either a word, a trivial noun compound of length K (e.g., multi-word named entity) from the vocabulary of the the thematic vocabulary V t , or reduces to the empty symbol, i.e., omits the input word entirely; hence the length of s can differ from that of the original problem.",71,72
7361,397698,"We use a special UNKNOWN symbol for entities with no observed types in the training set (i.e., entities that do not appear as subject of a triple).",5,6
7362,397698,"A dependency feature is a symbol for a word having a specific dependency relation, such as compound knowledge, compound −1 base for the knowledge base noun compound.",5,6
7363,397698,"Similar to the Entity Type representations, embeddings of words and dependency features are aggregated by summation followed by L2normalization, and a special UNKNOWN symbol is assigned to tuples whose pair of entities does (Toutanova et al.,",25,26
7364,15616433,"count of ellipses, has stock symbol, count of special symbols ($ !),",6,7
7365,6025595,Let y 0 be the start symbol and y N +1 be the end symbol.,6,7
7366,6025595,Let y 0 be the start symbol and y N +1 be the end symbol.,14,15
7367,10060918,"Accordingly, transduction grammars are able to: where (e , f )is a sentence pair in L 0 and L 1 and S is the start symbol.",29,30
7368,10060918,"Representing the ITG as a tuple ⟨N, V 0 , V 1 , R, S⟩ where N is a set of nonterminals, V 0 and V 1 are the tokens of L 0 and L 1 respectively, R is a set of transduction rules and S ∈ N is the start symbol, each transduction rule can be restricted to one of the following forms: S → A A → [BC] A → ⟨BC⟩ A → e/ϵ A → ϵ/f A → e/f where S, A, B, C are the non-terminals, e, f are tokens in the two languages and ϵ is the empty token.",55,56
7369,39487,"y n to denote the tokens in a sequence y. Each y i is a discrete symbol from a finite dictionary V of size m. Typically, m is large.",16,17
7370,1437405,"In this model, a sentence is regarded as a word sequence Û ½ ( Û ½ Û ¾ ¡ ¡ ¡ Û ) and words are predicted from beginning to end: Å Û Ò ´Ûµ •½ ½ È ´Û Û ½ Ò•½ µ where Û ´ ¼µ and Û •½ is a special symbol called a Ì (boundary token).",57,58
7371,1437405,"Since it is impossible to define the complete vocabulary, we prepare a special token ÍÏ for unknown words and an unknown word spelling Ü ¼ ½ is predicted by the following character-based Ò-gram model after ÍÏ is predicted by Å Û Ò : Å Ü Ò ´Ü ¼ ½ µ ¼ •½ ½ È ´Ü Ü ½ Ò•½ µ (1) where Ü ´ ¼µ and Ü ¼ •½ is a special symbol Ì. Thus, when Û is outside of the vocabulary Ï, Nagata (1994) proposed a stochastic word segmenter based on a word Ò-gram model to solve the word segmentation problem.",80,81
7372,174798153,Each idiom except those in double quotation marks 4 is replaced with a blank symbol.,14,15
7373,5548936,"We denote the ground truth target symbol at step i by y * i , the embedding representation of word y by e(y), and the hidden state of a seq2seq decoder at step i as h i .",6,7
7374,5548936,"Hence, in addition to the intermediate hidden states and final softmax scores, the previous model prediction, ŷi−1 , itself depends on the model parameters, θ, and ideally, should be backpropagated through, unlike the gold target symbol y * i−1 which is independent of model parameters.",42,43
7375,2986409,The most important differences are: • AMR has no explicit structures for universal quantification and negation; • AMR expects different labels for thematic roles (Boxer uses the VerbNet inventory); • AMR assigns no scope for propositional meanings; • AMR is strongly event-oriented (verbalization); • AMR has flat lists of coordinated structures; • AMR has symbol grounding by wikification (for named entities).,66,67
7376,2986409,"We approached the problem with a three-fold strategy: lexical adaptation (changing lexical entries of the Boxer system to match AMR), a recursive translation function from DRS to AMR, and a post-processing step (needed because of the differences in verbalization and symbol labelling in AMR).",50,51
7377,15074743,Also the replaced embeddings have more semantic information of the OOV words than a general UNK symbol.,16,17
7378,218665402,"We remove the ""#"" symbol and replace ""_"" by a space.",6,7
7379,198921712,Introduction Sequence prediction is a problem that involves using historical sequence data (i.e. context) to predict the next symbol or symbols in the sequence.,20,21
7380,198921712,"Training datasets consist of whole sequences and the aim is to learn a model that allows the ranking of potential next symbols for a given test sequence (prefix or context), that is, the most likely options for a single next symbol.",44,45
7381,198921712,"A WFA with k states is a tuple A = a 0 , a ∞ , A σ where a 0 , a ∞ ∈ R k are the initial and final weight vectors respectively, and A σ ∈ k × k is the transi- tion matrix for each symbol σ ∈ Σ. A WFA computes a function f A : Σ * → R defined for each word x = x 1 x 2 . . .",50,51
7382,198921712,"The two-layer LSTM stack was placed on top of the embedding layer that is used to embed each symbol x t of the sequence at position t. The output layer consists of a softmax layer implementing the softmax activation, which outputs the network's prediction of the next symbol of sequence y t = x t+1 .",20,21
7383,198921712,"The two-layer LSTM stack was placed on top of the embedding layer that is used to embed each symbol x t of the sequence at position t. The output layer consists of a softmax layer implementing the softmax activation, which outputs the network's prediction of the next symbol of sequence y t = x t+1 .",51,52
7384,198921712,Following Shibata and Heinz (2017) a 'start' symbol and an 'end' symbol were added to both sides of each training sentence.,11,12
7385,198921712,Following Shibata and Heinz (2017) a 'start' symbol and an 'end' symbol were added to both sides of each training sentence.,17,18
7386,198921712,Symbols are fed into the model from the 'start' symbol.,11,12
7387,174798343,Real-time: The network performs one iteration of computation per input symbol.,13,14
7388,174798343,A strictly k-local grammar over an alphabet Σ is a set of allowable k-grams S. Each s ∈ S takes the form s ∈ where # is a padding symbol for the start and end of sentences.,33,34
7389,3837310,"Again, we identified the simple sentences based on the symbol ""(S"" or ""(ROOT"" in Figure 1 .",10,11
7390,252796,"Upon examination, a disproportionate amount of disagreement in the correcting case was found to be caused by one text that contained many instances of a cover symbol for chemical and other formulas.",27,28
7391,252796,"In the absence of an explicit guideline for tagging this case, the annotators had made different decisions on what part of speech this cover symbol represented.",25,26
7392,13996807,"Numbers preceded by a currency symbol ($, , £) are replaced with the word price (which then indicates the PRICES attribute).",5,6
7393,2870061,"The Wikipedia dataset is split into adjacent n-grams and spaces between words replaced with the ""-"" symbol.",20,21
7394,196621813,A language model accepts a sequence of symbols and predicts the next symbol in the sequence.,12,13
7395,196621813,"Perplexity is the measurement of the confusion or uncertainty of a language model as it predicts the next symbol in a sequence, and the lower the perplexity of a model the better the performance of the model.",18,19
7396,196621813,"H(X) = log N − 1/N k i=1 N i ψ(N i ) (4) where N i is the frequency of unique symbol i, N = N i , K is the number of unique symbols, and ψ(N i ) is the logarithmic derivative of the gamma function of N i .",27,28
7397,227228320,"Unlike soft attention models, this model attends to a single input state at each step and either writes a symbol to the output sequence to favour a higher number of ambifixing verbs in terms of the number of inflected forms.",20,21
7398,202538897,"For an aspect target with more than one word, we first replace the whole target word sequence with a special symbol "" target "", then pass the modified sentence into the dependency parser.",21,22
7399,9573708,"S ∈ N is the start symbol, and P is a finite set of productions.",6,7
7400,9573708,"Given the start symbol S, which is first replaced with X 1 , rule (c) is applied to generate ""X 2 to go"" and its AMR counterpart.",3,4
7401,33284116,"Emoji's special characters, when identified, were also replaced by the symbol "" emoji "".",13,14
7402,33284116,"We also identified expressions that determine negation in a sentence, and replaced these expressions by the symbol "" NOT "", maintaining the adjacent related words unchanged.",17,18
7403,5548421,"word shape: the shape of each character in the word (capital letter, small letter, digit, punctuation, other symbol) • word type: the type of the word (uppercase, digit, symbol, combination ) • Prefixes (all prefixes having length between one to four). •",23,24
7404,5548421,"word shape: the shape of each character in the word (capital letter, small letter, digit, punctuation, other symbol) • word type: the type of the word (uppercase, digit, symbol, combination ) • Prefixes (all prefixes having length between one to four). •",39,40
7405,8686297,Each entry in the dictionary consists of a gene identifier and a corresponding official HGNC symbol.,15,16
7406,8686297,"Contains at most 2 characters and 1 letter but not an official HGNC symbol (e.g., P1).",13,14
7407,8686297,"If a symbol used by Swiss-Prot or SOURCE was not found as a symbol in HGNC or Entrez Gene, but was a non-ambiguous synonym in HGNC or Entrez Gene, then we replaced it by the corresponding symbol of the nonambiguous synonym.",2,3
7408,8686297,"If a symbol used by Swiss-Prot or SOURCE was not found as a symbol in HGNC or Entrez Gene, but was a non-ambiguous synonym in HGNC or Entrez Gene, then we replaced it by the corresponding symbol of the nonambiguous synonym.",15,16
7409,8686297,"If a symbol used by Swiss-Prot or SOURCE was not found as a symbol in HGNC or Entrez Gene, but was a non-ambiguous synonym in HGNC or Entrez Gene, then we replaced it by the corresponding symbol of the nonambiguous synonym.",42,43
7410,8686297,"Briefly, the definition included only human genes, excluded multi-protein complexes and antibodies, excluded chained mentions of genes (e.g., ""HDAC1-and -2 genes""), and excluded gene classes that were not normalizable to a specific symbol (e.g., tyrosine kinase).",45,46
7411,16086013,"currency normalization from a number (any format mentioned in previous step) together with a currency symbol (e.g. ""$ 100"") to a single token digital number with the lexical currency name (e.g. ""100 dollars""); 3.",17,18
7412,5104515,"For (ii), a Pepe the frog image (a symbol used by the American alt-right movement) in the user profile reveals that the user is probably a Trump supporter.",12,13
7413,13007268,"Starting from our initial hypergraph with one edge labeled with the start symbol ""X0"", we select one edge with a nonterminal label in our current hypergraph, and rewrite it using a rule in our HRG.",12,13
7414,13007268,The first rule rewrites the start symbol with a subgraph shown on the r.h.s.,6,7
7415,226262276,"As the original representation of every node has been obtained by the basic encoding, we conduct a dimensional-iteration encoding with GDIN as follows: (1) Encoding along the dimension of innerevent graph: At t-step, for i-th inner-event graph, we formulate the representation of its nodes as H (t) i = [E (t) i ; P x(t) i ; P y(t) i ; K xi(t) i ; K xe(t) i ; K ye(t) i ; K xa(t) i ] ∈ R 7d , where the symbol"";"" denotes concatenation.",109,110
7416,220831090,"For this, Husain (2020) defines seven intensive preprocessing steps; 1) emoji and emoticon conversion to textual label that describe the content of them; 2) letter normalization that converts multiple letters forms to one form such as Alif ‫)إ،آ،أ(‬ to ‫,)ا(‬ Alif Maqsura ‫)ئ،ي(‬ to ‫,)ى(‬ and Ta Marbouta ‫)ة(‬ to ‫,)ه(‬ and reduces repeated letters more than two times within a word to two times only; 3) dialect normalization to convert variation in nouns among dialects to their Modern Standard Arabic (MSA) forms; 4) hyponym conversion to hypernym such as mapping multiple animal names to the word 'animal'; 5) hashtags segmentation to remove the '#' symbol and replace '_' by a space; 6) miscellaneous cleaning process such as removing numbers, HTML tags, more than one space, special symbols, stopwords, and diacritics; 7) upsampling minority class, however, this step is not adopted to the final pipeline because it demonstrates negative effect on the performance of the classifier.",124,125
7417,199379414,"When compressing data files or training a classification model, observations from previously seen data are used to estimate the likelihood of observing a symbol following a given context of up to N characters.",24,25
7418,199379414,"However, PPM automatically backs off to use shorter contexts when a symbol has never been observed in a longer context.",12,13
7419,199379414,"If a 'z' has never been observed after ""i o"" then the process continues, with an additional penalty and further recursive backoff for 'z' using the context of the single symbol ('i').",37,38
7420,1713770,"When used to compress data files, observations from previously seen data are used to estimate the likelihood of observing a symbol 1 in a given context.",21,22
7421,1713770,"However, the method backs off to use shorter contexts when a symbol has not been observed in a longer context.",12,13
7422,1713770,"If an 'h' has never been observed after ""u i"" then the process continues, with an additional penalty, and further recursive backoff for 'h' after a context of a single symbol ('i').",38,39
7423,52138366,"2015) is often described as ""soft,"" as it does not clearly associate a single input symbol with each output symbol, but rather offers a fuzzy notion of what input symbols may be responsible for which symbols in the output.",19,20
7424,52138366,"2015) is often described as ""soft,"" as it does not clearly associate a single input symbol with each output symbol, but rather offers a fuzzy notion of what input symbols may be responsible for which symbols in the output.",23,24
7425,52138366,"In contrast, an alignment directly associates a given input symbol with a given output symbol.",10,11
7426,52138366,"In contrast, an alignment directly associates a given input symbol with a given output symbol.",15,16
7427,52138366,"|x|} |y| , which has an interpretation as the set of all (potentially non-monotonic) alignments from x to y with the restriction that each output symbol y i aligns to exactly one symbol in x ∈ Σ * x .",30,31
7428,52138366,"|x|} |y| , which has an interpretation as the set of all (potentially non-monotonic) alignments from x to y with the restriction that each output symbol y i aligns to exactly one symbol in x ∈ Σ * x .",37,38
7429,247291740,"For each mention of an entity e i , we use the embedding of the start symbol ""*"" as its mention embedding m i j .",16,17
7430,53223890,"The LSTM encoder takes a sequence of Dialog Acts as input, where each slot is a symbol in the vocabulary.",17,18
7431,235765693,"4 Dependents of the collapsed empty nodes are assigned new labels, corresponding to the empty node label and the dependent label joined with the special symbol >.",26,27
7432,237492063,The set of possible values is thus extended with the NA (not applicable) symbol.,15,16
7433,209376338,The output layer has as many dimensions as characters in the alphabet of the target language (including any desired punctuation as well as the blank symbol used for CTC).,26,27
7434,233444328,"x n , where y is a set of n arcs over the tokens and the dummy root symbol x 0 , and each arc (h, m) ∈ y specifies the head, h, and modifier word, m. 2 In this work, we adopt the conceptually-simple edge-factored deep biaffine dependency parser (Dozat and Manning, 2017) , which is competitive with the state of the art in terms of accuracy, The parser assigns a locally-normalized attachment probability P att (head(m) = h | x) to each attachment candidate pair (h, m) based on a biaffine scoring function.",18,19
7435,233444328,"x n , where x 0 is the dummy root symbol, we extract contextualized features at each word position.",10,11
7436,239020555,Doing so reduces the total vocabulary size and increases the hit-rate of each symbol in the dataset.,15,16
7437,239020555,BPE consists in replacing the most common symbol pairs with a new symbol consecutively until the symbol table has a certain size.,7,8
7438,239020555,BPE consists in replacing the most common symbol pairs with a new symbol consecutively until the symbol table has a certain size.,12,13
7439,239020555,BPE consists in replacing the most common symbol pairs with a new symbol consecutively until the symbol table has a certain size.,16,17
7440,239020555,In order to alleviate this problem we tried to fit the infrequent symbols into the vocabulary by trimming one consonant at a time from the beginning or the end of the symbol until they matched a symbol in the vocabulary.,31,32
7441,239020555,In order to alleviate this problem we tried to fit the infrequent symbols into the vocabulary by trimming one consonant at a time from the beginning or the end of the symbol until they matched a symbol in the vocabulary.,36,37
7442,239020555,"Note that sometimes excessive trimming can happen to the point that the original word cannot be restored, but this is still preferable to an unknown symbol.",27,28
7443,239020555,"For languages with a lot of sound changes, like consonant gradation in Finnish, morphological changes can produce a different symbol, which increases the need of data.",21,22
7444,239020555,The embeddings for the each symbol were 620 dimensions long.,5,6
7445,53248930,"Given an instance of a (MR, text) pair, we decompose the MR into eight components (mr j in Figure 2 ), each corresponding to a value for a unique MR key, and add an end-of-sentence symbol (EOS) to denote the end of the encoded sequence.",46,47
7446,8548302,"For instance, for English the symbol % is tagged as NUM by our system while in the Penn treebank it is tagged as Noun.",6,7
7447,224817876,"w n ) , where w 0 denotes the root symbol for the dependency parse tree, and the inputs to the Transformer network are pretrained GloVe embeddings (Pennington et al.,",10,11
7448,21686919,"v n , v n )} ⊆ V × R d , where for each k ∈ [1, n ], v k ∈ V is an IPA symbol assigned by a linguist and v k ∈ R d is a vector of d measurable phonetic quantities.",31,32
7449,21686919,"In short, the IPA symbol v k was assigned as a label for a phoneme with pronunciation v k .",5,6
7450,21686919,"This framework recognizes that the same IPA symbol v (such as /u/) may represent a slightly different sound v in one language than in another, although they are transcribed identically.",7,8
7451,21686919,"In general, there is no reason to suspect that speakers of two languages, whose phonological systems contain the same IPA symbol, should produce that vowel with identical formants.",22,23
7452,21686919,Each entry in the corpus corresponds to a linguist's phonetic description of a language's vowel system: an inventory consisting of IPA symbols where each symbol is associated with two or more formant values.,27,28
7453,21686919,"First, IPA symbols are primarily useful for interlinked phonological distinctions, i.e., one applies the symbol /I/ to distinguish it from /i/ in the given language, rather than to associate it with the sound bearing the same symbol in a second language.",17,18
7454,21686919,"First, IPA symbols are primarily useful for interlinked phonological distinctions, i.e., one applies the symbol /I/ to distinguish it from /i/ in the given language, rather than to associate it with the sound bearing the same symbol in a second language.",40,41
7455,21686919,"Second, field linguists often resort to the closest common IPA symbol, rather than an exact match: if a language makes no distinction between /i/ and /I/, it is more common to denote the sound with a /i/. Thus, IPA may not be as universal as hoped.",11,12
7456,21686919,"However, the performance of the phonemesupervised system is much worse, indicating that, perhaps, while the linguists have the right idea about the number of universal symbols, they did not specify the correct IPA symbol in all cases.",38,39
7457,21686919,"For example, even if /I/ is the proper IPA symbol for the sound, if there is no other sound in the vicinity the annotator may prefer to use more common /i/. Related Work Most closely related to our work is the classic study of Liljencrants and Lindblom (1972) , who provide a simulation-based account of vowel systems.",10,11
7458,21686919,"That paper pretended that each IPA symbol had a single cross-linguistic (F 1 , F 2 ) pair, an idealization that we remove in this paper by discarding the IPA symbols and modeling formant values directly.",6,7
7459,245855876,"For this, they used a simple approach that consists of using a special symbol inserted in the source sentence, indicating the target language.",14,15
7460,39044316,The symbol <Abil> denotes the Ability abstract morpheme.,1,2
7461,1812706,"For prr feature, to distinguish current phrase node and its sisters, we make a symbol to locate the current node.",16,17
7462,52072951,"SPADES contains 93,319 question-answer pairs which were created by randomly replacing entities in declarative sentences with a blank symbol.",20,21
7463,53046408,"f j−1 , h) (3) Decoding starts by passing the start-of-sequence symbol as input.",18,19
7464,53046408,The decoding terminates once the end-ofsequence symbol is produced.,8,9
7465,208262860,In both pairs we calculated vectors of length 100; one model was trained on all tokens which occurred at least 5 times and the second one was trained on text in which all numbers were replaced by one symbol.,39,40
7466,52310841,"To reduce the vocabulary size of the joint models, we replace all words occurring fewer than four times with a spe-cial ""UNK"" symbol.",27,28
7467,52051958,Even whitespace is handled as a normal symbol.,7,8
7468,52051958,"For the sake of clarity, SentencePiece first escapes the whitespace with a meta symbol _ (U+2581), and tokenizes the input into an arbitrary subword sequence, for example: • Raw text: Hello_world. •",14,15
7469,52051958,"SentencePiece reserves vocabulary ids for special meta symbols, e.g., unknown symbol (<unk>), BOS (<s>), EOS (</s>) and padding (<pad>).",12,13
7470,233033383,"Here, we summarize the key elements for cant: (1) Both a cant and its reference (i.e., hidden word) should be in the form of common natural text (not another symbol system, e.g., Morse code). (",39,40
7471,18519167,"The symbol ↓ means performance drop compared with a previous model) As we can see, for test set A, the type features almost contribute nothing; the F-score has a very slight change.",1,2
7472,13589134,"As probability models become more sophisticated, this approach will fail, because (1) the knowledge is represented using internal symbols referring to relations and entities the system discovers for itself, that may not correspond to standard concepts in English, and (2) the meaning of a given internal symbol varies across possi-ble worlds in the inference process.",54,55
7473,3504277,All out-of-vocabulary words are mapped to a special symbol UNK.,12,13
7474,5016370,"2016) , multiencoder translation models have recently been used to incorporate extra-sentential linguistic con-5 Although the non-translation of the concatenation symbol is possible, in practice this was rare (<0.02%).",25,26
7475,18263067,3 show that the baseline model performs 1-2% better across four datasets; this is due to the fact that the LOOKUP model can directly learn character embeddings that capture the semantics of each character symbol for frequent characters.,38,39
7476,18263067,"The LOOKUP embedding do not show such feature, as it learns the embedding individually for each symbol and relies heavily on the training set and the task.",17,18
7477,232021989,We normalize URLS and usernames (tokens that starts with the @ symbol).,12,13
7478,232021989,Hashtags are converted to their constituent word(s) after removing the # symbol.,12,13
7479,4410027,"Specifically, we can compute the context vector c t for tth word by first replacing tth word with a special symbol (e.g. <$>).",21,22
7480,4410027,"2017) , as our first method for integration of context-aware word embeddings, we use a gating function as follows: f e (x t ) = f e (x t ) σ(c t ) (16) = M e 1(x t ) σ(c t ) (17) The symbol represents element-wise multiplication, and σ is element-wise sigmoid function.",56,57
7481,44155184,It recursively applies a tran-sition function to its internal hidden state for each symbol of input sequence.,15,16
7482,44155184,"The hidden state at time step t is computed as a function f of the current input symbol x t and the previous hidden state h t−1 in a recurrent form: h t = f (x t , h t−1 ) . (",17,18
7483,13747323,"4 It takes s0 = BOS, a beginning-of-sequence symbol, so p θ (y1 | BOS) specifies the initial state distribution.",13,14
7484,13747323,"Our contribution is a neural proposal distribution, which goes beyond particle filtering in that it uses a right-to-left recurrent neural network to ""look ahead"" to future symbols of x when proposing each symbol y t .",39,40
7485,248506067,"And investors often use the Twitter cashtag function (a $ symbol followed by a ticker) to organize their particular thoughts around one single stock, e.g., $AAPL, so that users can click and see the ongoing discussions.",11,12
7486,2111076,"The CTC collapsing function achieves this by introducing a special blank symbol, which we denote using "" "", and collapsing any repeating characters in the original length T output.",11,12
7487,2111076,This output symbol contains the notion of junk or other so as to not produce a character in the final output hypothesis.,2,3
7488,2111076,"Again, the character probabilities are a categorical distribution over the symbol set ζ.",11,12
7489,2111076,The output symbol set ζ consists of 33 characters including the special blank character.,2,3
7490,2111076,The model tends to output mostly blank characters and only spike long enough for a character to be the most likely symbol for a few frames at a time.,21,22
7491,44094869,"Information about the meaning of mathematical variables in text is useful in NLP/IR tasks such as symbol disambiguation, topic modeling and mathematical information retrieval (MIR).",18,19
7492,44094869,"Our variable parser is designed to operate on symbol layout trees (SLTs) (Schellenberg et al.,",8,9
7493,44094869,"2012) with the features that are type and variable-centric, such as the 'base symbol of a candidate variable' and 'first letter in the candidate type'.",18,19
7494,44094869,Type and Variable The first letter in the type and base symbol of the candidate variable.,11,12
7495,44094869,Variable The variables and symbols in the candidate variable layout graph (one string per symbol).,15,16
7496,44094869,Variable The base symbol of the candidate variable layout graph.,3,4
7497,44094869,"Tangent Formula Indexing and Scoring Given a mathematical formula, the Tangent indexing algorithm starts from the root node of an SLT and generates symbol pair tuples in a depth-first manner.",24,25
7498,44094869,"For each query formula, the symbol pair tuples are gener- Typed Tangent Indexing and Scoring We have applied the BiLSTM variable typing model to obtain variable typings for all symbols in the documents in the MREC.",6,7
7499,44094869,"Variable groups with no type candidates (e.g., no variable typings have been extracted for a variable) are assigned a missing type symbol (""*"").",24,25
7500,44094869,"Subsequently, variables in the SLT of each formula in d are replaced with their type or the missing type symbol.",20,21
7501,4892899,"The second part of our network, the decoder, is a single GRU, defining a probability distribution over strings in (Σ ∪ S) * , for an alphabet Σ and a separation symbol S: p ED (c | w) = Tc t=1 p(c t | c 1 , . . . ,",36,37
7502,4892899,"For training, we use data from both sets at the same time and mark each example with an additional, task-specific input symbol.",25,26
7503,4892899,we prepend one language-specific input symbol to each instance.,7,8
7504,234487213,"Fuzzy Logic provides both mechanisms to solve the problem of symbol grounding and an extensive prior knowledge in the form of a wide variety of well known fuzzy measures, which can be used as a basis for measuring such quality related aspects.",10,11
7505,234487213,"In general, the symbol grounding problem is shared by every knowledge representation formalism (Harnad, 1990) .",4,5
7506,44063762,"BPE is a data compression algorithm that iteratively replaces the most frequent pair of symbols (here, characters) in a sequence with a single new symbol.",27,28
7507,51873570,Words that occur less than 5 times are replaced with the <unk> symbol.,14,15
7508,4956705,2015) and randomly replace a word w with a special UNK symbol with probability Conclusion We presented a novel multitask approach to learning semantic parsers from disjoint corpora with structurally divergent formalisms.,12,13
7509,51882786,2) p s: the tweet with the hash symbol # removed only.,10,11
7510,51882786,Our idea is that a tweet will be more grammatical complete with only the hash symbol removed if the hashtag is also a content word.,15,16
7511,21726677,The symbol − →(← −) indicates the forward (backward) pass.,1,2
7512,44220219,"Targeted Caption Method Note that a targeted caption is denoted by S = (S 1 , S 2 , ..., S t , ..., S N ), where S t indicates the index of the t-th word in the vocabulary list V, S 1 is a start symbol and S N indicates the end symbol.",54,55
7513,44220219,"Targeted Caption Method Note that a targeted caption is denoted by S = (S 1 , S 2 , ..., S t , ..., S N ), where S t indicates the index of the t-th word in the vocabulary list V, S 1 is a start symbol and S N indicates the end symbol.",61,62
7514,203586495,"2017) or using an unique symbol to replace all these words with a shared embedding (Hermann et al.,",6,7
7515,8188158,"Ignore root, punctuation and some function words) w 1 or w 2 is ROOT, a punctuation symbol or is lemmatized as one of be, have or the and is adjoined to its parent; or In Section 6.2, we report our main results based on the transformations and this notion of subderivation above, and compare those results to other systems.",19,20
7516,232021571,The definitions of the merged synsets were initially combined with a pipe symbol in between them.,12,13
7517,203610243,"Starting from an initial special symbol ŵ0 , we iterate t = 1 . . .",5,6
7518,53103796,As such it collapses some distinctions between similar sounds such as using a single 'r' symbol for all the rhotic sounds.,17,18
7519,14651385,"Let G =< V, Σ, S, R > denote a CFG, where V is a finite set of non-terminals, Σ is a finite set of terminals (disjointed from V), S ∈ V is the start symbol, and R is a set of rewrite rules (or production rules) of the form A → c where A ∈ V and c ∈ (V ∪ Σ) * .",46,47
7520,14651385,"i, Y, j] represents the text spanning from i to j is derived to symbol Y. N and P are non-terminals in the sentiment grammar, and N and P represent polarities of sentiment.",17,18
7521,14651385,"The non-terminal set is denoted as V s = {N, P, S, E}, where S is the start symbol, the non-terminal N represents the negative polarity, and the non-terminal P represents the positive polarity.",26,27
7522,14651385,"Finally, each sentence is derived to the symbol S using the start rules that are the beginnings of derivations.",8,9
7523,14651385,"And the goal is [0, S, n], in which S is the start symbol and n is the length of the input sentence.",18,19
7524,126147495,The # symbol was removed from the tweets along with mapping few popular offensive words to a standard form.,2,3
7525,243948403,"2013) , and use the same four categories: • Γ: lexical production rules (productions where the right-hand symbol (RHS) is a terminal symbol (word)). •",23,24
7526,243948403,"2013) , and use the same four categories: • Γ: lexical production rules (productions where the right-hand symbol (RHS) is a terminal symbol (word)). •",30,31
7527,243948403,γ: nonlexical production rules (productions where the RHS is a non-terminal symbol). •,15,16
7528,11320015,"In order to handle unknown words, during training, all words that are low frequency and capitalized are replaced with the special symbol UNK in both utterance and logical form.",23,24
7529,18119558,"2011) increased the performance of a character-level language model with a multiplicative RNN (m-RNN), the factored approximation of a recurrent neural tensor network (RNTN), which maps each symbol to separate hidden layer weights (referred to as recurrence matrices from hereon).",38,39
7530,18119558,"Besides increasing model capacity while keeping computation constant, this approach has another motivation: viewing the RNN's hidden state as being transformed by each new symbol in the sequence, it is intuitive that different symbols will transform the network's hidden state in different ways (Sutskever et al.,",27,28
7531,18119558,"Unfortunately, having separate recurrence matrices for each symbol requires memory that is linear in the symbol vocabulary size (|V |).",8,9
7532,18119558,"Unfortunately, having separate recurrence matrices for each symbol requires memory that is linear in the symbol vocabulary size (|V |).",16,17
7533,236477942,"Note that the parentheses and the pipe symbol ""|"" are ignored by the model (when relative positioning is used) and is used mostly for notational and programming convenience.",7,8
7534,236477942,"The pipe symbol separates the root portion of the ""slotted"" subtree string.",2,3
7535,221818878,use the same symbol (.).,3,4
7536,1271271,"When a symbol is found that does not match a valid transition, the PTA is augmented with a new path to accept the rest of the string.",2,3
7537,235352806,"2019b) abstract the translated sentences from other languages to English with part-of-speech tags, function words, dependency relation tags, and constituent tags, and train the embedding vectors by concatenating a language representation with a symbol representation.",42,43
7538,199577473,"A single * symbol represents a translation of an original sentence, while * * represents a double translation, i.e. a translation of a translationese sentence.",3,4
7539,29159988,"Unlike Huang and Sagae (2010) and Cross and Huang (2016) , ξ and σ are not shift/reduce scores; instead, they are the (best) label scores of the resulting span: ξ = max X s(j, j + 1, X) and σ = max X s(k, j, X) where X is a nonterminal symbol (could be ∅).",68,69
7540,44113253,"Separation of contextual information The SIMPLE model can use multiple turns of context to infer the response by concatenating them during decoding, using a separator symbol such as EOS for end-of-sentence.",26,27
7541,236478113,"Besides, we employ XBug as the auxiliary metric to evaluate the grammatical correctness of output, which examines basic grammar structure of output, like symbol usage and context matching.",26,27
7542,204578366,"On the left of the ""-"" symbol there is the dataset used to train the system and the decoding process used to generate back-translated data (if any).",8,9
7543,204578366,"On the right of the ""-"" symbol there is the decoding process used to generate hypotheses from the forward model.",8,9
7544,26309941,Hashtag Segmentation: Here the '#' symbol is stripped off from the hashtags.,8,9
7545,221879025,The generation process terminates when the special ending symbol [eos] is generated.,8,9
7546,18468577,"It allows the user to exclude symbol tokens, projective trees, or non-projective trees. •",6,7
7547,18468577,"Our tool uses a slightly different symbol set than eval07.pl: !""#",6,7
7548,29150573,"This strategy can accommodate an open vocabulary, i.e. it eliminates the need for an UNK numeral symbol, as the probability is normalised one digit at a time over the much smaller vocabulary of digits (digits 0-9, decimal separator, and end-of-sequence).",17,18
7549,212649790,"In Natural Language Processing (NLP), which operates on discrete symbol sequences, adversarial attacks can take a variety of forms (Ettinger et al.,",12,13
7550,48356558,The symbol ⊕ represents concatenation.,1,2
7551,196198179,"This is a problem studied primarily in steganography and steganography researchers have a keen interest in linguistic steganography as it presents fundamental challenges (Chang and Clark, 2014) ; the linguistic channel carries few bits per symbol on average (Shannon, 1951; Brown et al.,",38,39
7552,11066688,A distinguishing property of NTPs is that they are differentiable with respect to symbol representations in a knowledge base.,13,14
7553,11066688,"As NTPs operate on distributed representations of symbols, a single handcrafted rule can be leveraged for many proofs of queries with similar symbol representations.",23,24
7554,11066688,"Furthermore, we call the conjunction of atoms before the implication symbol the left-hand side (or body) of the rule and the atom after the implication the right-hand side (or head) of the rule.",11,12
7555,11066688,This structure encodes that such goals encompass a vector repre-sentation of a predicate symbol # 1 and the first argument of the predicate # 2 .,15,16
7556,11066688,"In contrast to discrete unification that checks for symbol equality, we calculate a soft unification from the previous unification success of the outer network τ D+1 and the similarity of predicate and constant representations as follows: τ predicate = sigmoid(v T GRANDFATHEROF v GRANDPA ) (1) τ arg 1 = sigmoid(v T ABRAHAM v ABE ) (2) τ D = min(τ D+1 , τ predicate , τ arg 1 ) (3) AND The new substitution structure calculated by unification instantiates an AND network (Figure 1c ) at depth D that attempts to sequentially prove the left-hand side atoms of the rule given the current substitutions.",8,9
7557,11066688,"Trainable Rules NTPs are not only differentiable with respect to symbol representations in the KB, but also latent symbol representations in first-order rules of predefined structure.",10,11
7558,11066688,"Trainable Rules NTPs are not only differentiable with respect to symbol representations in the KB, but also latent symbol representations in first-order rules of predefined structure.",19,20
7559,11066688,"We can define a rule template # 1 (X, Y )∧# 1 (Y, Z) ⇒ # 2 (X, Z) whose latent predicates v # 1 , v # 2 are trainable parameters and op- timized in the same way as symbol representations.",50,51
7560,11066688,"First, we run the NTP with given ground-truth rules without training symbol or rule representations, and test whether it can act as a discrete theorem prover.",14,15
7561,11066688,The third subplot shows predictions when we let the NTP try to reconstruct training facts only with the help of other facts by learning symbol representations (similar to other representation learning approaches for KB inference).,24,25
7562,11066688,"Conclusion and Future Work We proposed neural theorem provers for knowledge base inference via differentiable backward chaining, which enables learning of symbol representations and parameters of rules of predefined structure.",22,23
7563,21668994,"Copying Mechanism Since most encoder-decoder methods maintain a fixed vocabulary of frequent words and convert a large number of long-tail words into a special symbol "" unk "", the copying mechanism (Gu et al.,",28,29
7564,52896498,"At every decoder timestep, the encoder context vectors are scored by the attention mechanism, and a weighted sum is supplied to the decoder, along with its propagated internal state and last output symbol.",35,36
7565,52967399,"Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine-tuning, as the [MASK] symbol never appears during the fine-tuning stage.",27,28
7566,52967399,"In the table, MASK means that we replace the target token with the [MASK] symbol for MLM; SAME means that we keep the target token as is; RND means that we replace the target token with another random token.",17,18
7567,248512960,"Imagery We leverage the <symbol, imagery> pairs (e.g., <love, rose>) in the ConceptNet knowledge base (Liu and Singh, 2004) imagery generation model from a pretrained model called COMmonsEnse Transformer (Bosselut et al.,",5,6
7568,239998725,"Specifically, given an input sentence S padded by the start symbol [CLS] and the end symbol [SEP], we first obtain N contextualized embeddings {o i } N i=1 for all tokens {t i } N i=1 using BERT.",11,12
7569,239998725,"Specifically, given an input sentence S padded by the start symbol [CLS] and the end symbol [SEP], we first obtain N contextualized embeddings {o i } N i=1 for all tokens {t i } N i=1 using BERT.",18,19
7570,16565880,"We add an ""Embedded Target"" (symbol ET) tag to represent attributes of the targets, embedded in the opinion phrases tagged by the simple tagging scheme.",8,9
7571,16565880,"We also add a ""Negation"" (symbol NO) tag to capture the negation of an opinion which often is located far from the opinion phrases (example 3 and 6 in Figure 2 ).",8,9
7572,102353387,"We introduce a new, symbol-rich tweet dataset for developing computational models of hate speech analysis, leveraging the Urban Dictionary. •",5,6
7573,102353387,"Dataset In this section, we describe the dataset we collected for hate symbol decipherment.",13,14
7574,102353387,"For example, for each term in the set R i ={brown shirt, brown shirts, Brownshirts, brownshirt}, the corresponding definition is ""Soldiers in Hitler's storm trooper army, SA during the Nazi regime..."" After expanding, we obtain 2,105 distinct hate symbol terms and their corresponding definitions.",52,53
7575,102353387,"On average, each symbol consists of 9.9 characters, 1.5 words.",4,5
7576,102353387,"Tweet Collection For each of the hate symbols, we collect all tweets from 2011-01-01 to 2017-12-31 that contain exactly the same surface form of hate symbol in the text.",34,35
7577,102353387,"After filtering out all the tweets classified as non-hate, our final dataset consists of 18,667 (tweet, hate symbol, definition) tuples.",22,23
7578,102353387,"Our Approach We formulate hate symbol deciphering as the following equation: Obj = (u,s,d * )∈X log p(d * |(u, s)) (1) X is the dataset, (u, s, d * ) is the (tweet, symbol, definition) tuple in the dataset.",5,6
7579,102353387,"Our Approach We formulate hate symbol deciphering as the following equation: Obj = (u,s,d * )∈X log p(d * |(u, s)) (1) X is the dataset, (u, s, d * ) is the (tweet, symbol, definition) tuple in the dataset.",52,53
7580,102353387,The inputs are the tweet and the hate symbol in this tweet.,8,9
7581,102353387,The output is the definition of the symbol.,7,8
7582,102353387,"Our objective is to maximize the probability of the definition given the (tweet, symbol) pair.",15,16
7583,102353387,"We first use four recurrent neural networks to encode the (tweet, symbol, definition) pair in the dataset.",13,14
7584,102353387,"Similar to what we do in the Seq2Seq model, there are two encoders for the hate symbol.",17,18
7585,102353387,"The condition vector c is the concatenation of the encoded symbol words, symbol characters, and the tweet text: c = c u ⊕ c sw ⊕ c sc (14) We use multi-layer perceptron (MLP) to model the posterior and the prior in the objective function.",10,11
7586,102353387,"The condition vector c is the concatenation of the encoded symbol words, symbol characters, and the tweet text: c = c u ⊕ c sw ⊕ c sc (14) We use multi-layer perceptron (MLP) to model the posterior and the prior in the objective function.",13,14
7587,102353387,"The decoder in our Variational Decipher model is to model the likelihood p ϕ (d * |z, u, s), which is conditioned on the latent variable, tweet context, and the symbol.",37,38
7588,102353387,"Note that there are no overlapping hate symbols between the training dataset U and the testing dataset D. We end for 43: end function symbol wigwog and Wig Wog have the same definition but one is in the training dataset, the other is in the first testing dataset.",25,26
7589,102353387,"Therefore, the first testing dataset D s is to evaluate how well the model captures the semantic similarities among the tweet contexts in different examples or the similarities among different surface forms of a hate symbol.",36,37
7590,102353387,"In each choice question, the workers are given the hate symbol, the referential definition, the original tweet and two machine-generated plain texts from the Seq2Seq model and Variational Decipher.",11,12
7591,102353387,"Instead, the model should learn the relationships between the con- text information and the definition of the symbol.",18,19
7592,102353387,"Therefore, the different performances of two models on the two testing datasets D s and D d indicate that the Seq2Seq model is better at capturing the similarities among different surface forms of a hate symbol, while the Variational Decipher is better at capturing the semantic relationship between the tweet context and the hate symbol.",36,37
7593,102353387,"Therefore, the different performances of two models on the two testing datasets D s and D d indicate that the Seq2Seq model is better at capturing the similarities among different surface forms of a hate symbol, while the Variational Decipher is better at capturing the semantic relationship between the tweet context and the hate symbol.",56,57
7594,102353387,"Although there exists the symbol Confederate Flag with the same definition in the training set, both models fail on this example.",4,5
7595,102353387,"One possible reason is that the complexity of generating the referential definition grows substantially with the increasing length, so when the tweet context and the symbol itself cannot provide enough information, the generation model cannot learn the relationship between the symbol and its definition.",26,27
7596,102353387,"One possible reason is that the complexity of generating the referential definition grows substantially with the increasing length, so when the tweet context and the symbol itself cannot provide enough information, the generation model cannot learn the relationship between the symbol and its definition.",44,45
7597,102353387,"For the symbol niggering, the Variational Decipher generates the word nigger and Seq2Seq model generates black.",2,3
7598,102353387,Conclusion We propose a new task of learning to decipher hate symbols and create a symbol-rich tweet dataset.,15,16
7599,102353387,The different performance of these two models indicates that the models can be applied to different scenarios of hate symbol deciphering.,19,20
7600,195700180,13 We then replace each token with the special symbol UNK if its type appeared fewer than 5 times in the training portion.,9,10
7601,84843987,"It seems that this particular attention head focuses more on phrases like ""words of sympathy"", ""support', ""symbol of hope"" which are related to the query ""thoughts"".",23,24
7602,84843987,s j−1 : they are a symbol of hope for all those who defend freedom .,6,7
7603,53658002,"The root of the tree is always position 0, where x 0 is a distinguished ""root"" symbol that is prepended to the input sentence.",19,20
7604,235417184,"The input format for this model is simple, just requiring the question and the narrative to be separated by a newline symbol.",22,23
7605,208232005,"As such, a sequence of tokens [w 1 , ..., w |x| ] is generated and we use the first generated [SEP] symbol as the end of the generated question sentence.",27,28
7606,196201122,In Table 5 we also report the statistical significance between OneSeC and its competitors on the ALL dataset by juxtaposing a † symbol next to the score.,22,23
7607,231583223,2019) follows a two-stage approach: (1) content selection at the system input to generate a symbol intermediate representation and (2) generating utterance.,21,22
7608,204812084,"A m ) of R as symbol with rank m, we can define the set AST(G) of abstract syntax trees of G to be the set of all d ∈ T R such that for each position w of d the following holds: if d has label A → σ(A 1 , . . . ,",6,7
7609,2478928,u ) 33: end function with some symbol σ ∈ Σ and ends with σ; we use this property on line 8.,8,9
7610,2478928,u ) u i that starts with some symbol σ ∈ p; l is the set of elements of p we have not yet seen in some previous u i ; and r is the set of indices i such that some previous u i starts with an element of p. Rel is constructed in lines 7-19.,8,9
7611,2478928,"If it is satisfied, we remove the first and last symbol from all u j 1 , . . . ,",11,12
7612,2478928,pop(pd ) returns the left-most symbol of pd and removes it from pd . •,7,8
7613,208030290,"For nominative PAS-QA questions, the special symbol ""author"" can often be an answer, but it is not explicitly expressed in the document.",9,10
7614,224803201,The symbol ε denotes an empty string or sequence.,1,2
7615,224803201,"After the induction, we replace every terminal of the LCFRS by the wildcard symbol "" "", and we refer to the resulting rules as supertags.",14,15
7616,224803201,The symbol | is appended to constituents that result from binarization (this reflects Markovization with a vertical context of 1 and a horizontal context of 0). (,1,2
7617,224803201,"If B 1 is annotated with − , then x 0 is inserted as the first symbol in the first component in c .",16,17
7618,224803201,"If the terminal was swapped during step (5), the terminal σ 2 is removed from c and σ 1 is added as the first symbol in the first component of c. We remark that if B is annotated with − , then it must be the case that B 1 is annotated with − as well or the terminal was swapped during step (5).",27,28
7619,247596715,The decoding process finishes when a special symbol ([sep]) is predicted that indicates the end of the summary. [,7,8
7620,220059804,This construction obtains terminating rules with at least two lexical symbols and each constructed monic rule contains at least one lexical symbol.,21,22
7621,220059804,"Intuitively, one terminal symbol is cut from each terminating rule, which now has at least two occurrences of terminal symbols.",4,5
7622,220059804,This symbol is then pasted into a non-lexical branching rule that reaches the rule it was cut from via its second right-hand side nonterminal.,1,2
7623,220059804,"If there are rules between the terminating rule the symbol was cut from and the rule it shall be pasted into, then the information that a terminal can be pasted through these rules is propagated via the nonterminals.",9,10
7624,220059804,"2018) , we do not need to split the rules' compositions, because we always cut the first symbol.",20,21
7625,220059804,"To decide whether a terminal symbol may be propagated through a derivation, we define the look-ahead language L σ for each σ ∈ Σ as the smallest set L such that L = 1 , . . . ,",5,6
7626,174799778,"In the Chomsky-Schützenberger representation for MCFGs, the brackets fulfil three functions: (i) terminal brackets σ σ stand for a terminal symbol σ, (ii) component brackets ρ and ρ denote beginning and end of substrings produced by the -th component of a rule ρ, and (iii) variable brackets j ρ,i and j ρ,i denote beginning and end of substrings produced by variable x j i in a rule ρ.",26,27
7627,174799778,"14, the function TOMCFGDERIV' checks a set of cow derivations for equivalence of the root symbol and the function COLLECTCHILDREN groups the subtrees via the first component of the successor labels.",17,18
7628,174799778,"us, or of the form [t] for some terminal symbol t. 5 available on https://github.com/tud-fop/ rustomata.",12,13
7629,174799778,"B j( ,n ) i( ,n ) be the nonterminals on the rhs of ρ ( ) , then t is the cow derivation Note that ρ is the root symbol of each t 1 , . . . ,",34,35
7630,18027513,"In this work, we consider the three most frequent 2 and semantically important types of empty category annotations in most Treebank genres: Null complementizers are denoted by the symbol 0.",30,31
7631,16038823,"We assume in this paper that U simply concatenates the sequence of morphs, separating them by the morph boundary symbol #: u = U (m 1 , m 2 , . . .)",20,21
7632,11602050,"Let x i denote the i th character of x. If i < 1 or i > |x|, then x i is the distinguished symbol BOS or EOS (""beginning/end of string"").",25,26
7633,11602050,"In the special case where x i+1 = EOS, the choices are instead INSERT(t) and HALT (where the latter may be viewed as copying the EOS symbol).",29,30
7634,11602050,"This is guaranteed by the following sufficient conditions (we omit the proof for space), which do not seem to appear in previous literature: • For each state q and each symbol b ∈ Σ x , the arcs from q with input label b or must have total probability of 1. (",34,35
7635,21715999,The Japanese TimeBank Corpus' notation is extended by allowing to include the symbol @ in each part of YYYY-MM-DD in the sense that it can represent any number.,13,14
7636,247593935,"From The Great Gatsby, given ""A symbol of Gatsby's lifestyle:"" our model's top-ranked single sentence candidates are: 1.",8,9
7637,9918545,In further discussion we simplify the representation of the model to a standard PCFG with C V as its symbol set and its PCFG rules indexed by outcomes.,19,20
7638,9918545,The correctness of this algorithm comes from the fact that the probability of a rule R expanding a symbol X is precisely the probability of all trees rooted at X whose first rule is R. This implies that the correct sampling distribution is simply the distribution over rules itself.,18,19
7639,9918545,This suggests the use of techinques that have proved effective in grammar estimation that reason over large numbers of possible derivations such as Bayesian tree substitution grammars or unsupervised symbol refinement.,29,30
7640,215754083,"Synchronous rules For the case where neither c s nor c t are the special symbol , the base distribution first generates e s and e t independently, and then samples an alignment between the frontier nodes.",15,16
7641,9168043,"The Core Model The core model uses the following features of a word: • the word's POS tag; • the conjunction of the word's POS tag and its arc label; • the word's last length-one and length-two suffixes (to model written case morphemes); • the conjunction of the word's arc label, its POS tag, and its parent's POS tag; • if the word is the object of a preposition, the preposition it is the object of; • whether the word is a PRD child of a verb (with the identity of that verb conjoined if so); • if the word has a sister which is a subordinating conjunction, and if so, that conjunction conjoined with its arc label; • whether the word is in an embedded clause conjoined with its arc label under the verb of the embedded clause; • if the word is a PRD child of a verb, the verb; • the word's left sister's POS tag conjoined with this word's arc label and its sister's arc label; • whether the word's sister depends on the word or something else; • and the left sister's terminal symbol.",225,226
7642,3266340,"A transition on a delimiter symbol, which always returns to state 0, signifies the end of a derivation process whereby the final form in the daughter language has been arrived at.",5,6
7643,3266340,"In the context of PFSA, the MML is a sum of: • the length of encoding a description of the proposed machine • the length of encoding the dataset assuming it was emitted by the proposed machine The following formula is used for the purpose of computing the MML: where N is the number of states in the PFSA, tj is the number of times the jth state is visited, V is the cardinality of the alphabet including the delimiter symbol, nij the frequency of the ith arc from the jth state, mj is the number of different arcs from the jth state and m} is the number of different arcs on non-delimiter symbols from the jth state.",85,86
7644,3266340,"The arc frequencies are indicated in superscript font above the symbol, except when there is more than one symbol on an arc, in which case the frequencies are denoted by the superscript marker ..... Exclamation marks (""!"")",10,11
7645,3266340,"The arc frequencies are indicated in superscript font above the symbol, except when there is more than one symbol on an arc, in which case the frequencies are denoted by the superscript marker ..... Exclamation marks (""!"")",19,20
7646,232185275,"There are many real world historical buildings that have little to no effect other than being a symbol of an economic entity or symbol of an era or location, and nothing more.",17,18
7647,232185275,"There are many real world historical buildings that have little to no effect other than being a symbol of an economic entity or symbol of an era or location, and nothing more.",23,24
7648,232185275,"Instead, it's just for show, a symbol of rich people.",9,10
7649,102353862,"In all cases, the question is generated by replacing the answer token with a blank symbol.",16,17
7650,218974240,"We convert numbers to text, expand abbreviations, normalize percentage symbol and decimal point, consider the date, contact numbers, etc.",11,12
7651,174800955,"During romanization, each grapheme symbol in Bangla is replaced with a single English letter except that if a consonant grapheme is not followed by a vowel grapheme, ""O"" was added after the romanized symbol of that consonant as the roman symbol for ``অ'', which is usually inherently pronounced in such cases.",5,6
7652,174800955,"During romanization, each grapheme symbol in Bangla is replaced with a single English letter except that if a consonant grapheme is not followed by a vowel grapheme, ""O"" was added after the romanized symbol of that consonant as the roman symbol for ``অ'', which is usually inherently pronounced in such cases.",37,38
7653,174800955,"During romanization, each grapheme symbol in Bangla is replaced with a single English letter except that if a consonant grapheme is not followed by a vowel grapheme, ""O"" was added after the romanized symbol of that consonant as the roman symbol for ``অ'', which is usually inherently pronounced in such cases.",44,45
7654,13661068,"This increase is most likely due to the difference in the BPE vocabularies: while the English and Wubi BPE rules that were learned cover 100% of the dataset, for Chinese this is 98.7% -the remaining 1.3% had to be replaced by the unk symbol under our vocabulary constraints.",48,49
7655,294054,In further discussion we simplify the representation of the model to a standard PCFG with C V as its symbol set and its PCFG rules indexed by outcomes.,19,20
7656,294054,Our goal is to retain the form of the stan-dard incremental recursive sampling algorithm for The correctness of this algorithm comes from the fact that the probability of a rule R expanding a symbol X is precisely the probability of all trees rooted at X whose first rule is R. This implies that the correct sampling distribution is simply the distribution over rules itself.,35,36
7657,294054,This suggests the use of techinques that have proved effective in grammar estimation that reason over large numbers of possible derivations such as Bayesian tree substitution grammars or unsupervised symbol refinement.,29,30
7658,17837595,"Any L2 words that do not appear on this list are mapped to the unknown ""UNK"" symbol, as are all foreign words that are not good translations of any L2 stopword.",18,19
7659,1481562,"As in previous NLI work, we then replace all word tokens that do not occur in a list of 614 common words with an unknown word symbol, UNK.",27,28
7660,1481562,The traditional grammatical model of nonterminal expansion is augmented such that to rewrite a symbol we first choose a grammar from the document's θ η and then choose a rule from that grammar.,14,15
7661,1481562,"We can express the generative model formally by defining the probability of a rule r expanding a symbol s in a sentence labeled η as θ η ∼ Dir(ν η ) z iη ∼ M ult(θ η ) H s ∼ DP (γ, P 0 (•|s)) G ks ∼ DP (α s , H s ) r iηs ∼ G z iη s This is closely related to the application of the Hierarchical Pitman Yor Process used in (Blunsom and Cohn, 2010) and (Shindo et al, 2012) , which interpolates between multiple coarse and fine mappings of the data items being clustered to deal with sparse data.",17,18
7662,195699400,"1) Each loss to be minimized is defined as the negative log likelihood of the ground truth character sequence y * , is computed from: L CTC − ln π∈Φ(y) p(π|x) (2) L att − u ln p(y * u |x, y * 1:u−1 ) (3) where π is the label sequence allowing the presence of the blank symbol, Φ is the set of all possible π given u-length y, and y * 1:u−1 is all the previous labels.",69,70
7663,5260223,"We use Good-Turing smoothing to account for unseen morphemes, all of which are replaced with a single ""unknown"" symbol.",23,24
7664,219308407,"In constituent-based parsing work, the prevailing technique to combat this divide between efficient models and real world data has been to selectively strengthen the dependencies in a CFG by increasing the grammar size through methods such as symbol refinement (Petrov et al.,",40,41
7665,219308407,"We start with contextfree grammars, the components of which are N, T, R, S , where N and T are the sets of nonterminal and terminal symbols respectively, and S is a distinguished nonterminal, the start symbol.",42,43
7666,219308407,The derived trees rooted at the start symbol S are taken to be the trees generated by the grammar.,7,8
7667,219308407,"An auxiliary tree α is an elementary tree containing a single distinguished nonterminal leaf, the foot node, with the same symbol as the root of α.",22,23
7668,219308407,"To avoid double-counting derivations, which can adversely effect probabilistic modeling, type (3) and type (4) rules in which the side with the unapplied symbol is a nonterminal leaf can be omitted.",31,32
7669,219308407,"Furthermore, these rules employ a template in which the stored symbol appears in the left-hand side and in exactly one symbol on the right-hand side where the spine of the auxiliary tree proceeds.",11,12
7670,219308407,"Furthermore, these rules employ a template in which the stored symbol appears in the left-hand side and in exactly one symbol on the right-hand side where the spine of the auxiliary tree proceeds.",23,24
7671,219308407,"All that is required is a check that a given symbol is adjoinable, which is true for all symbols except nonterminal leaves and applied symbols.",10,11
7672,219308407,"The first uses a conservative number of parameters, with a Bernoulli variable for each symbol (OSTAG 1 ).",15,16
7673,219308407,"The second employs more parameters, conditioning on both the node's symbol and the symbol of its leftmost child (OSTAG 2 ).The third is highly parameterized but most prone to data sparsity, with a separate Bernoulli distribution for each Goodman index η (OSTAG 3 ).",12,13
7674,219308407,"The second employs more parameters, conditioning on both the node's symbol and the symbol of its leftmost child (OSTAG 2 ).The third is highly parameterized but most prone to data sparsity, with a separate Bernoulli distribution for each Goodman index η (OSTAG 3 ).",15,16
7675,750754,We built further systems which used the following bundles of features: AttSym Adds the part-of-speech tag or nonterminal symbol of the modifier.,23,24
7676,53298878,"In the proposed model, the encoder is designed to understand the semantics of problems, and the decoder focuses on tracking semantic meanings of the generated symbols and then deciding which symbol to generate next.",32,33
7677,218487733,"w 2 L 2 , where w i j is token i of the j th segment, [CLS] is a special symbol for classification output, and [SEP] is a special symbol to separate any text segments if they exist.",24,25
7678,218487733,"w 2 L 2 , where w i j is token i of the j th segment, [CLS] is a special symbol for classification output, and [SEP] is a special symbol to separate any text segments if they exist.",36,37
7679,146120740,"We define this conditional distribution as p(m | ) = 1 Z( ) n i=1 ψ (m i , m i−1 , ) (2) where ψ(•, •, •) ≥ 0 is an arbitrary potential, Z( ) normalizes the distribution, and m 0 is a distinguished start-of-sequence symbol.",60,61
7680,146120740,"z j ) + b) where z j is the concatenation of the current attended input x j alongside morphological features, m i , and an embedding of the previously generated symbol c j−1 ; and finally φ is an LSTM over the sequence of z j vectors.",33,34
7681,67856687,"To verify which explanation is correct, we generate ungrammatical expressions by removing either one operation token or one closing bracket symbol for each sequence in the test set.",21,22
7682,202780059,Each rule is represented by a unique symbol such asor ∼ sf as introduced in Jacobson ( 2001 ).,7,8
7683,102354918,The refresh symbol marks areas that the user re-generated to get a different sentence (presumably after being unhappy with the first result).,2,3
7684,174800105,"For lack of a better symbol, we use here to signify iterative string concatenation, which otherwise is signified by just writing symbols beside each other, or by the symbol •. Rather than working over words of a sentence, given the formal nature of the proof, the projective MST algorithm must work over symbols of the input word w. Hence the input is a weighted digraph over the symbols of w and the output is a projective MST, T * , over these symbols.",5,6
7685,174800105,"For lack of a better symbol, we use here to signify iterative string concatenation, which otherwise is signified by just writing symbols beside each other, or by the symbol •. Rather than working over words of a sentence, given the formal nature of the proof, the projective MST algorithm must work over symbols of the input word w. Hence the input is a weighted digraph over the symbols of w and the output is a projective MST, T * , over these symbols.",31,32
7686,174800105,"A-gadgets: A(t) := i∈[k] (a i,j (t,1) ,t a i,j (t,2) ,t • • • a i,j (t,k) ,t )• i∈[k] (a i,j (t,1) ,t • a i,j (t,2) ,t • • • a i,j (t,k) ,t ) C-gadgets: C(t) :=( i∈[k] c i,j (t,1) ,t c i,j (t,2) ,t • • • c i,j (t,k) ,t ) •c k,j (t,k) ,t • c k−1,j (t,k−1) ,t • • • c 1,j (t,1) ,t and B-gadgets: B(t) :=L t b k,j (t,k) ,t • • • b 2,j (t,2) ,t b 1,j (t,1) ,t H t b k,j (t,k) ,t •b k−1,j (t,k−1) ,t • • • b 2,j (t,2) ,t b 1,j (t,1) ,t R t , We call the symbol H t the head of the gadget B(t), and L t and R t the gadget's left and right boundary symbols respectively.",240,241
7687,174800105,"a) All arcs (a 1,j ,t , a i,j,t−1 ) and (a 1,j ,t , a i,j,t−1 ), i.e., the first symbol of the tth A-gadget attaches to all symbols of the previous (t − 1th) A-gadget. (",36,37
7688,174800105,"b) All arcs (c 1,j ,t , c i,j,t+1 ) and (c 1,j ,t , c i,j,t+1 ), i.e., the last symbol of the tth C-gadget attaches to all symbols of the next (t + 1th) C-gadget gadget. (",36,37
7689,174800105,"The A-region, together with the symbol L 1 from the B-region can form a tree rooted in L 1 using region connectivity arcs (1a) with boundary connectivity arcs (2a)-all weight 1 arcs.",8,9
7690,174800105,Similarly for the C-region with the symbol R |K| from the B-region (arcs (1b) and (2b)).,8,9
7691,174800105,"There are only two possible heads: (1) the symbol following the gadget (region connectivity arcs (1a) or boundary connectivity arcs (2a)), which by projectivity is excluded because these arcs would cross (b 1,j ,t , a 1,j,t ), or (2) symbols from the C-region, which by projectivity is also excluded because they would cross the arc (b 1,i ,p , a 1,i,p ).",11,12
7692,174800105,"Each unmarked b symbol in B(2) corresponds to a node in V , and is the head of an unmarked symbol from A(1) corresponding to every node in k-clique k 1 .",3,4
7693,174800105,"Each unmarked b symbol in B(2) corresponds to a node in V , and is the head of an unmarked symbol from A(1) corresponding to every node in k-clique k 1 .",21,22
7694,5754528,"A ctree is a rooted tree whose leaves are the words {w i } L i=1 , and whose internal nodes (constituents) are represented as a tuple Z, h, I , where Z is a non-terminal symbol, h ∈ {1, . . . ,",43,44
7695,5754528,"While projective d-parsers can use dynamic programming (Eisner and Satta, 1999 ; Koo and consider an extra root symbol, as often done in the literature.",22,23
7696,5754528,"For the training portion of the English PTB, which has 27 non-terminal symbols, the direct encoding strategy yields 75 labels, while delta encoding yields 69 labels (2.6 indices per symbol).",35,36
7697,5754528,"To reduce complexity, for each node symbol we only consider classes that have been observed with that symbol in the training data.",7,8
7698,5754528,"To reduce complexity, for each node symbol we only consider classes that have been observed with that symbol in the training data.",18,19
7699,221818956,We removed newline symbol <P> from the generated stories and detokenized for better display.,3,4
7700,220045428,"First, a sequence of graph edge labels along the path from v i to v j are obtained, where a direction symbol is added to each label to distinguish the edge direction.",23,24
7701,235359008,"At each time step, the decoder either generates a symbol from the output vocabulary or outputs a pointer to one of the input tokens based on the scores from the final probability distribution.",10,11
7702,19263554,The internal stack σ is always initialized with the root symbol $.,10,11
7703,235368142,"There are two types of actions: output a symbol from the output vocabulary, or output a pointer to one of the input tokens x i .",9,10
7704,236486311,"As our aim is to get a relatively straightforward baseline NLG system, rather than exploring the full range of text representation possibilities, we considered just two ways to represent text: character-based, where raw characters are separate entities and spaces are indicated by a special symbol (three vertical bars); or (tokenised) word-based, where tokenised words form the basic entities.",50,51
7705,5955929,"Here, E y maps any output symbol to a fixeddimensional vector.",7,8
7706,5955929,"Finally, we compute the probability of the output symbol y j given the history y <j using Equation 3 .",9,10
7707,5955929,"m, which copies one symbol from the m input tokens.",5,6
7708,67855815,"Since the dataset contains 24k unique characters, a 32k symbol vocabulary will consist of mostly characters, thereby increasing the average sequence length.",10,11
7709,102483587,replacing actual space characters in either the historical token or the reference normalization with a special symbol that does not otherwise occur in the dataset; and 6.,16,17
7710,227231129,"For utterance-level information, we add a symbol of the information of the t-th utterance UTT t T Y P E in front of the utterance.",9,10
7711,227231129,"Thus, we add a symbol of the speaker of the t-th utterance UTT t spk in front of it for the speaker information.",5,6
7712,227231129,"In the same way, we add a symbol of the speech act of the t-th utterance UTT t act in front of the utterance for the speech act information.",8,9
7713,227231129,"For conversation-level information, we add a symbol of the information CON T Y P E in front of seg B. For example, to incorporate a topic of the conversation, we add a symbol of the topic CON topic in front of the T -th utterance.",9,10
7714,227231129,"For conversation-level information, we add a symbol of the information CON T Y P E in front of seg B. For example, to incorporate a topic of the conversation, we add a symbol of the topic CON topic in front of the T -th utterance.",37,38
7715,5995546,"The symbol ""@"" is a special word indicating the possibility of deletion.",1,2
7716,215416276,"The conditional probability p(y|x) is defined by: ψ(y , y, r i ) = exp(W T y r i + b y ,y ) (1) p(y|x) = n i=1 ψ(y i−1 , y i , r i ) y ∈Y(x) n i=1 ψ(y i−1 , y i , r i ) (2) where Y(x) denotes the set of all possible label sequences for x, ψ is the potential function, W y and b y ,y are parameters and y 0 is defined to be a special start symbol.",101,102
7717,6728280,"For IWSLT'14, we replace words that occur fewer than 3 times with a <unk> symbol, which results in a vocabulary of 24158 English and 35882 German word types.",17,18
7718,227217192,"noun + (case) + verb: object-verb structure realising such as example ( 18 ) and ( 19 When a sentence is ended with the character ག, the sentence ends without vertical symbol ། such as example (10).",37,38
7719,250391026,"B.4 Error Analysis Figure 13 illustrates the confusion matrix for the offensive language dimension (binary classification), while Figure 14 illustrates the confusion matrix for the target classification (offensive messages only) Appendix A: Replicability Preprocessing All experiments have been conducted with common pre-processing steps, namely: • lowercasing of all words • all users' mentions have been substituted with a placeholder (MENTION); • all URLs have been substituted with a with a placeholder (URL); • all ordinal numbers have been replaced with a placeholder (NUMBER); • emojis have been replaced with text (e.g. → :pleading_face:) using Python emoji package; • hashtag symbol has been removed from hasthtags (e.g. #kadiricinadalet → kadiricinadalet); Appendix B: Supplementary Analyses B.1.",123,124
7720,222341640,"Length in Sequence Models Historically, for discrete symbol probabilistic sequence models, no EOS token was included in the standard presentations of HMMs (Rabiner, 1989) or in most following work in speech.",8,9
7721,220047261,"In what follows, we refer to the older versions of the multilingual tasks of SemEval-2013 and SemEval-2015 by juxtaposing the ""*"" symbol (SemEval-2013* and SemEval-2015*).",24,25
7722,247158115,"Thus, we define a uniform symbol E(h, r, t) to represent the score function of any KEG model for evaluating the plausibility of a triple (h, r, t).",6,7
7723,218974300,"Besides, we also want to predict markers beginning single sentences, so we mask the first sentence of Discovery example pairs in 10% of cases by replacing it with a placeholder symbol [S 1 ].",33,34
7724,222341502,Our SAU-Solver generates a universal expression tree explicitly by deciding which symbol to generate according to the generated symbols' semantic meanings like human solving MWPs.,13,14
7725,222341502,"In our UET representation, multiple expression trees underlying a MWP will be integrated as an universal expression tree (UET) via symbol extension.",23,24
7726,222341502,"2018; Chiang and Chen, 2019 ) can handle multiple types of MWPs, they neither generate next symbol by taking full advantage of the generated symbols like a human nor consider the semantic transformation between equations in a problem, resulting in poor performance on the multiple-unknown MWPs, such as the MWPs involving equation set.",19,20
7727,222341502,"1 , UET integrates all expression trees underlying in an MWP into an ensemble expression tree via math operator symbol extension so that the grounded equations of various MWPs can be handled in a unified manner as handling one-unknown linear MWPs.",19,20
7728,222341502,"In our SAU-Solver, the encoder is designed to understand the semantics of MWPs and extract number semantic representation while the tree-structured decoder is designed to generate the next symbol based on the problem-specific target vocabulary in a semantically-aligned manner by taking full advantage of the semantic meanings of the gener-ated expression tree like a human uses problem's contextual information and all tokens written to reason next token for solving MWPs.",33,34
7729,222341502,"Specially, we extend the math operator symbol table by introducing a new operator ; as the lowest priority operator to integrate one or more expression trees into a universal expression tree, as shown in Fig.",7,8
7730,222341502,"With the help of top-down tree-structured decoding and bottom-up subtree semantic transformation, SAU-Solver can generate the next symbol by taking full advantage of generated symbols in a semanticallyaligned manner like human solving MWPs.",26,27
7731,222341502,"Equation Decoder For decoding, inspired by previous works (Xie and Sun, 2019; Chiang and Chen, 2019) , we build a semantically-aligned tree decoder to decide which symbol to generate by taking full advantage of the semantic meanings of the generated symbols with two intertwined processes: top-down treestructured decoding and bottom-up subtree semantic transformation.",34,35
7732,222341502,The stack G maintains the hidden states generated from the parent node while the stack T helps the model decide which symbol to generate by maintaining subtree semantic information of generated symbols.,21,22
7733,222341502,"Although our decoder decodes a universal expression tree in the prefix, to help our model to generate the next symbol in a semantically-aligned manner by taking full advantage of the semantic meanings of the generated expression tree, we design a recursive neural network to transform the semantic representations of the current node and its two child subtrees t l and t r into a high-level embedding t in a bottom-up manner.",20,21
7734,174800180,BPE splits words into symbols (a symbol is a sequence of characters) and then iteratively replaces the most frequent sequences of symbols with a new merged symbol.,7,8
7735,174800180,BPE splits words into symbols (a symbol is a sequence of characters) and then iteratively replaces the most frequent sequences of symbols with a new merged symbol.,28,29
7736,174800180,"In essence, frequent character n-grams merge to form one symbol.",12,13
7737,222209112,"The symbol indicates that three or more than three correlation results over the five random seeds are not statistically significant, namely, p-value > 0.05.",1,2
7738,85517978,"We only train on languages for which the symbol size is relatively modest, a criterion which we fulfil by only using translations with Latin, Cyrillic, and Greek alphabets.",8,9
7739,19181149,"In this paper, we look at a model's ability to generalize on a simple symbol rewriting task with a clearly defined structure.",16,17
7740,19181149,We train models on a symbol replacement task with a well defined generalizable structure.,5,6
7741,19181149,"The task is set up to mimic (albeit, in an oversimplified manner) the input-output symbol alignments and local syntactic properties that models must learn in many natural language tasks, such as translation, tagging and summarization.",19,20
7742,19181149,Each symbol x ∈ X is uniquely associated with its own output alphabet Y x .,1,2
7743,19181149,Output is created by taking each individual symbol x i in the sequence and rewriting it as any sequence of k symbols from Y x i .,7,8
7744,19181149,"To test the extent to which (2) is met, we train 3 seq2seq-attention models with 100,000 randomly generated samples with inputs uniformly generated with lengths 5-10 and no input symbol appearing more than once in a single sample.",36,37
7745,19181149,"If the input symbol A maps to any permutations of a 1 , a 2 , or a 3 , and B maps to permutations of b 1 , b 2 , or b 3 .",3,4
7746,19181149,"Thus, mapping an input symbol to 48 (8 * 3!)",5,6
7747,233204311,"Compound PCFGs A probabilistic context-free grammar (PCFG) in Chomsky normal form can be defined as a 6-tuple (S, N , P, Σ, R, Π), where S is the start symbol, N , P and Σ are the set of nonterminals, preterminals and terminals, respectively.",42,43
7748,162168565,"These are the lowercased word form, word length, number of uppercase letters, number of digits and occurrence of a hashtag, URL, @-mention or symbol.",28,29
7749,67855842,"We below show insights for the elements of the output tuple (n t , c t , u t ), where n t is the number of levels in common between w t and w t+1 , c t is the nonterminal symbol shared at that level, and u t is a leaf unary chain located at w t .",44,45
7750,67855842,"On the one hand, the encoding of the n t s by Gómez-Rodríguez and Vilares (2018) only needs to know about w t and w t+1 paths to generate the label for the time step t. On the other hand, to compute the syntactic distance of a given non-terminal symbol, we need to compute the syntactic distances of its subtree, providing a more global, but also sparser context.",57,58
7751,202542569,"We pre-process both document texts and groundtruth keyphrases, including word segmentation, lowercasing and replacing all digits with symbol <digit>.",21,22
7752,235731777,"In our NS-Solver, we apply four auxiliary tasks to enhance its problem understanding and symbol reasoning ability for generating better programs.",17,18
7753,235731777,"Let ŷi and y i represent the predicted symbol and ground-truth symbol, p i represents the probability of ŷi , the semantic loss is obtained by computing a distance between the predicted distribution and ground-truth distribution as: L P CC = −log i ŷi =y i p i ŷi =y i (1 − p i ) . (",8,9
7754,235731777,"Let ŷi and y i represent the predicted symbol and ground-truth symbol, p i represents the probability of ŷi , the semantic loss is obtained by computing a distance between the predicted distribution and ground-truth distribution as: L P CC = −log i ŷi =y i p i ŷi =y i (1 − p i ) . (",13,14
7755,235731777,This shows that our auxiliary tasks can enhance our NS-Solver to enforce better problem understanding and symbol reasoning.,18,19
7756,102351751,"Similarly to the encoding stage, we map each symbol in the path p v = (s j ) M j=1 to a dense embedding e s j , where M is the path length.",9,10
7757,102351751,"To calculate the probability of the path symbol s j at time step j we first represent the path sequence as h * j = LSTM(e j s , h * j−1 ).",7,8
7758,3031815,"For instance, assume a database schema consisting of a single binary predicate symbol work, storing for each employee the department in which she is employed.",13,14
7759,232185202,"Philosopher Susanne Langer in her essay ""Expressiveness and Symbolism"" stated ""A metaphor is not language, it is an idea expressed by language, an idea that in its turn functions as a symbol to express something"".",36,37
7760,232185202,"For the later, we exploit the SymbolOf relation to make sure the generated sentence that contains the literal sense of the verb has the same symbol as the metaphorical sentence.",26,27
7761,236460196,"For English character input we use an explicit ""shift"" symbol (ˆ) to indicate uppercased characters, to keep the vocabulary size low.",11,12
7762,236460196,"Moreover, the | symbol represents an explicit word boundary.",4,5
7763,236460265,"Depending on the learning strategy, each slice will be either annotated (indicated with a S symbol) or not annotated (indicated with a U symbol).",17,18
7764,236460265,"Depending on the learning strategy, each slice will be either annotated (indicated with a S symbol) or not annotated (indicated with a U symbol).",27,28
7765,557620,"ii) Predicate in a fact is a paraphrase of the question's pattern where we define the pattern to be the topic asked by the question about the entity, and represent it as the question in which the entity mention has been replaced by a special symbol.",48,49
7766,6763105,"Also, all numbers are collapsed to one symbol, and non-alphanumeric sequences are converted to whitespace.",8,9
7767,235265630,"Specifically, given an input sequence x, it first randomly selects a few masked positions and replaces tokens at these positions with a special mask symbol [MASK] .",26,27
7768,233476375,"e N is the output of last encoder layer and d m−1 is the output of the previous decoder layer, with d 0 ≤t initialized to be the embeddings of the action history y <t concatenated with a special start symbol.",42,43
7769,237250087,"For example in the sentence-'Wear mask to protect yourself from #COVID-19 #corona', only '#' symbol was removed during the preprocessing(e.g. '",20,21
7770,247159046,"Let X denotes this modified sequence with inserted markers: X = ...[S], x a , ..., x b , [/S], ..., x c 1 ∪ [O1], ..., x d 1 ∪ [/O1], ..., x c 2 ∪ [O2], ..., x d 2 ∪ [/O2]..., where the tokens jointed by the symbol ∪ share the same position embedding.",76,77
7771,10007241,"As mentioned earlier, Chinese words in a sentence are not separated by any boundary symbol (e.g., a space), so a Chinese word segmentation tool is always required to extract words from a sentence.",15,16
7772,236965823,"The main obstacle is learning to handle a symbol (""jump"") having seen it very few times (or even just once) during training (this also happens in some types of generalization in COGS).",8,9
7773,17424451,"The symbol A ⇔ B denotes a foreign name A is translated and/or transliterated into a Chinese name B. (s1) Victoria Fall ⇔ 維多利亞瀑布 (wei duo li ya pu bu) (s2) Little Rocky Mountains ⇔ 小落磯山脈 (xiao luo ji shan mo) (s3) Great Salt Lake ⇔ 大鹽湖 (da yan hu) (s4) Kenmare ⇔ 康美爾 (kang mei er) (s5) East Chicago ⇔ 東芝加哥 (dong zhi jia ge) Example (s1) shows a name part (i.e., Victoria) and a keyword part (i.e., Fall) of a named location are transliterated and translated into ""維多利亞"" (wei duo li ya) and ""瀑布"" (pu bu), respectively.",1,2
7774,17424451,We introduce a symbol ∆ to cope with the distance issue.,3,4
7775,220280987,"2020a,b) , a web-based system that allows for the user's query as a natural language statement or an inquiry about a relationship at the meta-symbol level (e.g., CHEMICAL, PROTEIN) and then automatically retrieves textual evidence from a background corpora of COVID-19.",32,33
7776,235294133,"A weighted finite-state machine M is a tuple α, {W (a) } a∈A , ω where A is an al- phabet of size A, A def = A ∪ {ε}, each a ∈ A has a symbol-specific transition matrix W (a) ∈ R ≥0 N ×N where N is the number of states, and α, ω ∈ R ≥0 N are column vectors of start and end weights, respectively.",46,47
7777,238582852,"For simplicity, we set e 1 , e c , and e 2 to the same symbol e (i.e., e 1 , e c , e 2 ∈ {e}) whereas r 1 ∈ R 1 and r 2 ∈ R 2 denote different relation symbols, and R 1 and R 2 are the corresponding sets of relation candidates.",17,18
7778,238582852,"We construct a training set by including examples [e 1 , r 1 , e c , r 2 , e 2 ] where r 1 is the same relation symbol throughout while r 2 can be any relation symbol in R 2 (r 1 ∈ {r train }, r 2 ∈ R 2 ).",31,32
7779,238582852,"We construct a training set by including examples [e 1 , r 1 , e c , r 2 , e 2 ] where r 1 is the same relation symbol throughout while r 2 can be any relation symbol in R 2 (r 1 ∈ {r train }, r 2 ∈ R 2 ).",40,41
7780,238582852,"Modeling For each input symbol, we sample a vector from a Gaussian distribution N (0, 0.2 2 I) and freeze it during training.",4,5
7781,238856694,"The functions l B 1 , l B 2 : R L → R K are two linear layers for respectively predicting the posterior mean and variance vectors: µ B i , diag(Σ B i ) ∈ R K that generate the document-topical embedding z B i for document i. The symbol I ∈ R K×K denotes an identity matrix used to create the diagonal Gaussian.",54,55
7782,237012341,Such degradation is modeled as replacement: each symbol in a message is probabilistically replaced with another one.,8,9
7783,237012341,Soft Max Length Each message m is generated by sampling a symbol x t at each time step t and concatenating them until either eos is sampled (self-termination) or the time step reaches max len − 1 (forced termination).,11,12
7784,237012341,"Since message lengths can vary in our signaling game, it is doubtful that every single symbol in a message conveys essential information.",16,17
7785,237012341,"To evaluate effectiveness, we introduce position-wise symbol effectiveness and then head/intermediate/tail effectiveness to cover a weak point in the former.",9,10
7786,237012341,"Position-wise Symbol Effectiveness First, to evaluate how informative symbols are distributed across positions, we introduce positionwise symbol effectiveness, which is a quite similar notion to positional encoding in Rita et al. (",20,21
7787,237012341,Suppose a symbol x k in a message m = x 1 . . .,2,3
7788,237012341,"Then, a listener L is expected to fail to recover an input i correctly if x k is replaced with another symbol y, i.e., i = L(x 1 . . .",22,23
7789,237012341,"Based on this intuition, the symbol effectiveness e(m, k) at position k ∈ {1, . . . ,",6,7
7790,237012341,"Low e(m, k) means that symbol x k is redundant, since the listener L can recover i from most of m[x k := a] (a ∈ A ).",7,8
7791,237012341,That would make it difficult to perform straightforward evaluations for position-wise symbol effectiveness.,13,14
7792,237012341,"To check the symbol effectiveness, we show e k (Eq.",3,4
7793,237012341,"To check the symbol effectiveness, we show e k (Eq.",3,4
7794,237012341,"To check the symbol effectiveness, we show e k (Eq.",3,4
7795,237012341,14) means that the symbol at position k in m is redundant.,5,6
7796,237012341,"Judging from symbol effectiveness, the latter halves of messages tend to be more informative than the former when noise interferes with the listener.",2,3
7797,240288693,"2021) or CONFIRM (Ballesteros and Al-Onaizan, 2017b) , to predict nodes; 2 instead we directly use the node name <string> as the action symbol generating that node.",32,33
7798,235313546,"In the place of EMG feature inputs, this baseline model is given the sequence of phonemes predicted by the full model, but with information about the specific feature being tested removed by collapsing phonemes in each of its confusion sets to a single symbol.",45,46
7799,235313546,"The phonemes input to this model are the maximumprobability predictions output by our primary model at each frame, but with all phonemes from a confusion set replaced with the same symbol.",31,32
7800,53097386,"In the experimental results of this paper, the symbol at the top mark means that the marked result is significantly different (with p = 0.05) with the corresponding result of the agent with the specific mark.",9,10
7801,243840535,"The following alternatives were implemented and experimented with: just repeating the word features for each subword; using the BPE symbol in word features, in the same manner this tag is used in BPE for splitting subwords; and subword tags.",21,22
7802,226283451,This allows the model to successfully encode and learn the symbol and gives us the ability to backwards replace it to the original symbol during the decoding phase.,10,11
7803,226283451,This allows the model to successfully encode and learn the symbol and gives us the ability to backwards replace it to the original symbol during the decoding phase.,23,24
7804,15531603,"Consecutive digits occurring within a word are replaced with the symbol ""#"" .",10,11
7805,15531603,"The vocabulary is limited to the most frequent 100,000 words in North American news corpus (Graff, 2008) , plus one single ""UNK"" symbol for replacing all out of vocabulary words.",27,28
7806,248780090,"Indication (指 事): An abstract method to create a new Chinese characters by directly adding a indicative symbol on a specific position of the glyph of the mother character, the new character meaning is related to the position indicated by symbol.",19,20
7807,248780090,"Indication (指 事): An abstract method to create a new Chinese characters by directly adding a indicative symbol on a specific position of the glyph of the mother character, the new character meaning is related to the position indicated by symbol.",43,44
7808,248780090,"If a new character B (e.g.,""刃"" (knife edge)) is created by adding a symbol on the specific position(e.g.,",19,20
7809,203584077,We define a lexicon as a finite binary relation L ⊂ Σ + × Γ that pairs nonempty character strings from the finite alphabet Σ to a word symbol in the finite alphabet Γ. This terminology matches our keyboard application described above.,28,29
7810,247158606,"In summary, our main contributions are: Tasks In this section, we firstly clarify the symbol definition, and then define the proposed Multimodal Chat Translation task and the existing Multimodal Dialogue Sentiment Analysis task.",17,18
7811,247158606,"The symbol ' * ' denotes that we use pre-trained language models ERNIE, RoBERTa and XLM-R for Chinese, English, and German, respectively.",1,2
7812,8395279,"Normally the re-tweet symbol begins the message and is immediately followed by the user mention of the original author or sometimes a chain of re-tweeters ending with the original author, as in • politicsiswar: RT @KatyinIndy @SamiShamieh: Ghost towns on rise under Obama http://j.mp/cwJSUg #tcot #gop (Deindustrialization of U.S.-Generation Zero) Finally, ""smileys"" are common in Twitter statuses to signal the users' sentiment, as in the following. •",5,6
7813,174798289,"While these models learn a great deal of linguistic structure from these symbol sequences alone, acquiring the essence of basic syntax, it is highly unlikely that this approach can create models that acquire much in terms of semantics or pragmatics, which are integral to the human experience of language.",12,13
7814,174798289,"How might one build neural language models that ""understand"" the semantic content held within the symbol sequences, of any language, presented to it?",17,18
7815,174798289,All symbol sequences were zero-padded and appropriately masked to ensure efficient mini-batching.,1,2
7816,232185260,"A language id symbol (e.g., <java>, <python>) is appended and prepended to the encoder and decoder inputs, respectively.",4,5
7817,174799480,"Figures 4a to 4d show random examples without any unknown word, while the examples shown in Figures 4e and 4f are randomly selected from sentences with unknown words, which are marked with the UNK symbol.",36,37
7818,248780503,"In practice, we will maintain more detailed symbol lists and representation lists.",8,9
7819,2056546,First we removed any translations that contained at least one <unk> symbol.,13,14
7820,247446894,"For sentences without any ending symbol inside documents, periods are manually added.",5,6
7821,235166395,The high level intuition behind our proof is that the attention head can only catch o(n) input positions when we properly fix a small number of symbol in the input sequence.,27,28
7822,235166395,"Throughout the proof, we guarantee that for any i ∈ [n], ∈ [L], the output of the -th layer x i, depends only the input symbol at position i. This is clearly satisfied for = 0, given the it is composed by position embedding and word embedding only.",33,34
7823,235166395,"For any ∈ {1, • • • , L}, given a well-aligned partially assigned input sequence, suppose the input of -th layer x i, −1 depends on the symbol at position i only.",36,37
7824,235166395,"Then by fixing c H 2 (k + 1) O( H) 2 O( Hp) additional positions of the input sequence, we guarantee that the output of -th layer x i, also depends solely on the symbol at position i. Proof of Theorem C.1.",43,44
7825,8374741,"Formal Parsing Model: Scoring Partial Translation Hypotheses This model is essentially an extension of an HHMM, which obtains a most likely sequence of hidden store states, ŝ1..D 1..T , of some length T and some maximum depth D, given a sequence of observed tokens (e.g. generated target language words), e 1..T , using HHMM state transition model θ A and observation symbol model θ B (Rabiner, 1990) : ŝ1..D 1..T def = argmax s 1..D 1..T T t=1 P θ A (s 1..D t | s 1..D t−1 )•P θ B (e t | s 1..D t ) (8) The HHMM parser is equivalent to a probabilistic pushdown automaton with a bounded pushdown store.",74,75
7826,7940733,There is a one-to-one correspondency between the nonterminals in f and e: each nonterminal symbol in f has to also appear in e. The function ∼ captures this bijective mapping between the nonterminals.,19,20
7827,236986922,2019) generates a sentence from left to right sampling the next token from all possible candidates until the end-of-sequence symbol is generated.,24,25
7828,6269148,"In order to perform multiple sequence alignments of X-SAMPA word transcriptions we modified ALPHAMALIG slightly so it could work with the tokens that consist of more than one symbol, such as [""e], [""e:] and [t_S] .",30,31
7829,6269148,The distance between any token in the data set to a gap symbol has the same cost as replacing a vowel with a vowel or a consonant with a consonant.,12,13
7830,6269148,"If two strings are not of equal length, the remaining unaligned tokens are aligned with the gap symbol which rep-resents an insertion or a deletion.",18,19
7831,226262280,"If the token at position i represents a numerical quantity, we replace it with a special symbol [#MASK], and represent its numerical value using e NUM i .",17,18
7832,7586668,"Formally, an SCFG may be considered as a tuple (N, S, T σ , T τ , G) where N is a set of nonterminal symbols of the grammar, S ∈ N is the goal symbol, T σ and T τ are the source-and target-side terminal symbol vocabularies, respectively, and G is a set of production rules of the grammar.",41,42
7833,7586668,"Formally, an SCFG may be considered as a tuple (N, S, T σ , T τ , G) where N is a set of nonterminal symbols of the grammar, S ∈ N is the goal symbol, T σ and T τ are the source-and target-side terminal symbol vocabularies, respectively, and G is a set of production rules of the grammar.",57,58
7834,7586668,"Each rule in G is of the form X → α, γ, ∼ where X ∈ N is a nonterminal symbol, α is a sequence of symbols from N ∪ T σ , γ is a sequence of symbols from N ∪ T τ , and ∼ is a one-to-one correspondence between the nonterminal symbols of α and γ.",22,23
7835,7586668,"A Hiero grammar (Chiang, 2007 ) is an SCFG with only one type of nonterminal symbol, traditionally labeled X. A Hiero grammar can be extracted from a parallel corpus of word-aligned sentence pairs as follows: If (f j i , e l k ) is a sub-phrase of the sentence pair, we say it is consistent with the pair's alignment if none of the words in f j i are aligned to words outside of e l k , and vice-versa.",17,18
7836,7586668,"An SAMT grammar (Zollmann and Venugopal, 2006) is similar to a Hiero grammar, except that the nonterminal symbol set is much larger, and its labels are derived from a parse tree over either the source or target side in the following manner.",21,22
7837,7586668,"For each rule, if the target side is spanned by one constituent of the parse tree, we assign that constituent's label as the nonterminal symbol for the rule.",27,28
7838,2755801,Since each phrase normally has a set of different CCG labels (instead of a single non-termal symbol) we need a way of choosing which label to use when applying the constraint.,19,20
7839,10616734,"Formally, a probabilistic SCFG G is defined by specifying G = N , T S , T T , R, S , where N is a set of nonterminal symbols, T S and T T are the source and target language vocabularies, R is a set of rules and S ∈ N is the root symbol.",59,60
7840,10616734,The nonterminal symbol that is the left-hand side of the SCFG rule is then determined by the syntactic constituent that dominates e (in this case NP).,2,3
7841,10616734,"To introduce nonterminals into the right-hand side of the rule, we can apply rules extracted over sub-phrases of f , synchronously substituting the corresponding nonterminal symbol for the sub-phrases on both sides.",30,31
7842,3580297,In contrast to Wagner and Lowrance (1975) and in line with Damerau (1964) we restrict the swap operation to be only allowed for string X and Y when x i = y i+1 and y i = x i+1 (with x i being the token at position i in string X): x i x i+1 y i y i+1 >< 1 Note that a swap-operation in the alignment is indicated by the symbol '><'.,81,82
7843,3580297,The first number following this symbol indicates the cost of the swapoperation.,5,6
7844,3580297,"A Hidden Markov Model (HMM) is a probabilistic finite-state transducer that generates an observation sequence by starting in an initial state, going from state to state based on transition probabilities and emitting an output symbol in each state based on the emission probabilities in that state for that output symbol (Rabiner, 1989) .",39,40
7845,3580297,"A Hidden Markov Model (HMM) is a probabilistic finite-state transducer that generates an observation sequence by starting in an initial state, going from state to state based on transition probabilities and emitting an output symbol in each state based on the emission probabilities in that state for that output symbol (Rabiner, 1989) .",54,55
7846,3580297,"The PHMM displayed in Figure 1 has three emitting states: the substitution ('match') state (M) which emits two aligned symbols, the insertion state (Y) which emits a symbol and a gap, and the deletion state (X) which emits a gap and a symbol.",37,38
7847,3580297,"The PHMM displayed in Figure 1 has three emitting states: the substitution ('match') state (M) which emits two aligned symbols, the insertion state (Y) which emits a symbol and a gap, and the deletion state (X) which emits a gap and a symbol.",55,56
7848,3580297,"Additionally, probabilities of substituting a symbol with itself were much higher than the probabilities of substituting an arbitrary vowel with another non-identical vowel (mutatis mutandis for consonants), which were in turn much higher than the probabilities of substituting a vowel for a consonant.",6,7
7849,3580297,"To illustrate this procedure, consider the following gold standard alignment of [vl""7k] and [v""7lk] , two Bulgarian dialectal variants of the word 'wolf': v l ""7 k v ""7 l k Every aligned segment pair is converted to a single token by adding the symbol '/' between the segments and using the symbol '-' to indicate a gap.",54,55
7850,3580297,"To illustrate this procedure, consider the following gold standard alignment of [vl""7k] and [v""7lk] , two Bulgarian dialectal variants of the word 'wolf': v l ""7 k v ""7 l k Every aligned segment pair is converted to a single token by adding the symbol '/' between the segments and using the symbol '-' to indicate a gap.",64,65
7851,3580297,"2007) , we found sensible PHMM substitution probabilities (convergence was reached after 1675 iterations, taking about 7 CPU hours): the probability of matching a symbol with itself was significantly higher than the probability of substituting one vowel for another (similarly for consonants), which in turn was higher than the probability of substituting a vowel with a consonant (all t's > 9, p < .001).",28,29
7852,443309,The symbol ⊥ denotes the empty string.,1,2
7853,209765174,"Figure 1 shows the process of predicting the second sentence from the example in Table 1 where the encoder encodes the first sentence (bottom characters of Figure 1 ) while the decoder predicts the second sentence per character and each character (topright of Figure 1 ) is generated based on its previous one (top-left of Figure 1 ) beginning from <s>, the sentence starting symbol.",72,73
7854,209765174,"In preliminary experiments we found that the model works better if we train quatrains from right to left, i.e. instead of generating starting with <s> to predict the first character and so on, we generate from </s>, the ending symbol, to predict the last character and generate backwards terminating with <s>.",46,47
7855,8328845,"In most cases, the derived form is longer than its stem, and accordingly, when we reach the end of the base form, we continue to input an end-of-word symbol.",36,37
7856,8328845,"Secondly, although the model learns whether to copy or produce a new symbol well, some forms are spelled incorrectly.",13,14
7857,67865542,The attention mechanism controls which parts of the source sequence where the decoder should attend to in generating each symbol of target sequence.,19,20
7858,247475929,"In order to measure the effect of contamination when contaminated data is shuffled across the pretraining corpus, we divided clean Wikipedia text into lines (instances which were originally separated by new line symbol).",34,35
7859,11740526,The decoding algorithm starts with the symbol <s> and uses beam search to generate the next word.,6,7
7860,11740526,The generation process stops when we reach the symbol </s>.,8,9
7861,27199275,"Let |s| be the length of a string s, s j the j−th symbol in s and s :−1 the prefix containing the full string s except for the last symbol.",14,15
7862,27199275,"Let |s| be the length of a string s, s j the j−th symbol in s and s :−1 the prefix containing the full string s except for the last symbol.",32,33
7863,27199275,"n − 1 : k i (sa, t) = λ g k i (s, t) + k i (sa, t), k i (sa, tb) = λ g k i (sa, t) + λ 2 m sim(a, b)k i−1 (s, t), k n (sa, t) = k n (s, t)+ λ 2 m |t| j sim(a, t j )k n−1 (s, t :−1 ), k(s, t) = n i=1 µ i k i (s, t), where λ g and λ m are decay hyperparameters for symbol gaps and matches, respectively, and µ i is the weight for the kernel of n-gram order i. The decay hyperparameters smooth the kernel values when sequences are very similar to each other while the n-gram weights help to tune the signal coming from different subsequence lengths.",122,123
7864,27199275,"2 Given two strings s and t, the equations for our vectorised version are defined as S = E s E T t , K 0 = 1, K i = D |s| K i D |t| , K i = λ 2 m (S K i−1 ), k i = λ 2 m j,k (S K i ) j,k , k(s, t) = µ T k, where E s and E t are matrices of symbol embeddings for each string, is the Hadamard product and D ∈ R × R is the matrix D =         0 λ 0 g λ 1 g . . .",88,89
7865,20969045,Tokens outside the vocabulary list are replaced by the UNK symbol.,10,11
7866,7213981,"AFGs are too large to fully extract explicitly; researchers therefore either work with a tractable subset of the fragments (Sima'an, 2000; Bod, 2001; Post and Gildea, 2009; Cohn and Blunsom, 2010) or use a PCFG reduction like that of Goodman (1996a) , in which each treebank node token X i is given its own unique grammar symbol.",68,69
7867,7213981,"If a particular base symbol X is pruned by the PCFG coarse pass for a particular span (i, j) (i.e., the posterior marginal P (X, i, j|s) is less than a certain threshold), then in the full SDP pass we do not allow building any indexed symbol X l of type X for span (i, j).",4,5
7868,7213981,"If a particular base symbol X is pruned by the PCFG coarse pass for a particular span (i, j) (i.e., the posterior marginal P (X, i, j|s) is less than a certain threshold), then in the full SDP pass we do not allow building any indexed symbol X l of type X for span (i, j).",57,58
7869,248780199,"For ""w/ concat"", after obtaining the relation representation R f inal ∈ R 2d (the symbol appeared in Section 2.2) with two views of relations, we perform ""w/ concat"" by concatenating R f inal and P first, i.e., R f inal ⊕ P ∈ R 4d .",19,20
7870,7259581,We present a simple but accurate parser which exploits both large tree fragments and symbol refinement.,14,15
7871,7259581,"We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement.",7,8
7872,7259581,"We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement.",17,18
7873,7259581,"At the same time, many high-performance parsers have focused on symbol refinement approaches, wherein PCFG independence assumptions are weakened not by increasing rule sizes but by subdividing coarse treebank symbols into many subcategories either using structural annotation (Johnson, 1998; Klein and Manning, 2003) or lexicalization (Collins, 1999; Charniak, 2000) .",13,14
7874,7259581,"In this paper, we present a simplified parser which combines the two basic ideas, using both large fragments and symbol refinement, to provide non-local and local context respectively.",21,22
7875,7259581,The two approaches turn out to be highly complementary; even the simplest (deterministic) symbol refinement and a basic use of an all-fragments grammar combine to give accuracies substantially above recent work on treesubstitution grammar based parsers and approaching top refinement-based parsers.,16,17
7876,7259581,"4 G I has base symbols, which are the symbol types from the original treebank, as well as indexed symbols, which are obtained by assigning a unique index to each node token in the training treebank.",10,11
7877,7259581,"The BEGIN rules transition from a base symbol to an indexed symbol and represent the beginning of a fragment from G. The CONTINUE rules use only indexed symbols and correspond to specific depth-1 binary fragment tokens from training trees, representing the internal continuation of a fragment in G. Finally, END rules transition from an indexed symbol to a base symbol, representing the frontier of a fragment.",7,8
7878,7259581,"The BEGIN rules transition from a base symbol to an indexed symbol and represent the beginning of a fragment from G. The CONTINUE rules use only indexed symbols and correspond to specific depth-1 binary fragment tokens from training trees, representing the internal continuation of a fragment in G. Finally, END rules transition from an indexed symbol to a base symbol, representing the frontier of a fragment.",11,12
7879,7259581,"The BEGIN rules transition from a base symbol to an indexed symbol and represent the beginning of a fragment from G. The CONTINUE rules use only indexed symbols and correspond to specific depth-1 binary fragment tokens from training trees, representing the internal continuation of a fragment in G. Finally, END rules transition from an indexed symbol to a base symbol, representing the frontier of a fragment.",57,58
7880,7259581,"The BEGIN rules transition from a base symbol to an indexed symbol and represent the beginning of a fragment from G. The CONTINUE rules use only indexed symbols and correspond to specific depth-1 binary fragment tokens from training trees, representing the internal continuation of a fragment in G. Finally, END rules transition from an indexed symbol to a base symbol, representing the frontier of a fragment.",61,62
7881,7259581,The END rule weight is 0 or 1 depending on whether A is an intermediate symbol or not.,15,16
7882,7259581,"The original DOP1 model weights a fragment f in G as ω G (f ) = n(f )/s(X), i.e., the frequency of fragment f divided by the number of fragments rooted at base symbol X. This is simulated by our weight choices (Figure 2 ) where each fragment f I in G I has weight ω I (f I ) = 1/s(X) and therefore, ω G (f ) = f I ∈π −1 (f ) ω I (f I ) = n(f )/s(X).",38,39
7883,7259581,"Given the weights used for DOP1, the recursive formula for the number of fragments s(X i ) rooted at indexed symbol X i (and for the CONTINUE rule X i → Y j Z k ) is s(X i ) = (1 + s(Y j ))(1 + s(Z k )), (1) where s(Y j ) and s(Z k ) are the number of fragments rooted at indexed symbols Y j and Z k (nonintermediate) respectively.",21,22
7884,7259581,The number of fragments s(X) rooted at base symbol X is then s (X) = X i s(X i ).,9,10
7885,7259581,"The DOP1 model uses binary values (0 if symbol is intermediate, 1 otherwise) as the END rule weight, which is equivalent to prohibiting fragment switching at intermediate symbols.",9,10
7886,7259581,"With the above weights, the recursive formula for s(X i ), the total weighted number of fragments rooted at indexed symbol X i , is different from DOP1 (Equation 1).",22,23
7887,7259581,"If a particular base symbol X is pruned by the coarse pass for a particular span (i, j) (i.e., the posterior marginal P (X, i, j|s) is less than a certain threshold), then in the full grammar G I , we do not allow building any indexed symbol X l of type X for that span.",4,5
7888,7259581,"If a particular base symbol X is pruned by the coarse pass for a particular span (i, j) (i.e., the posterior marginal P (X, i, j|s) is less than a certain threshold), then in the full grammar G I , we do not allow building any indexed symbol X l of type X for that span.",58,59
7889,7259581,"Hence, the projection map for the coarse-to-fine model is π C : X l (indexed symbol) → X (base symbol).",21,22
7890,7259581,"Hence, the projection map for the coarse-to-fine model is π C : X l (indexed symbol) → X (base symbol).",27,28
7891,7259581,"However, the number of indexed symbols in our implicit grammar G I is still large, because every node in each training tree (i.e., every symbol token) has a unique indexed symbol.",28,29
7892,7259581,"However, the number of indexed symbols in our implicit grammar G I is still large, because every node in each training tree (i.e., every symbol token) has a unique indexed symbol.",35,36
7893,7259581,We have around 1.9 million indexed symbol tokens in the word-level parsing model (this number increases further to almost 12.3 million when we parse character strings in Section 5.1).,6,7
7894,7259581,This large symbol space makes parsing slow and memory-intensive.,2,3
7895,7259581,We store the duplicate-subtree counts for each indexed symbol of the collapsed graph (using a hashmap).,10,11
7896,7259581,"When calculating the number of frag-  ments s(X i ) parented by an indexed symbol X i (see Section 3.2), and when calculating the inside and outside scores during inference, we account for the collapsed subtree tokens by expanding the counts and scores using the corresponding multiplicities.",15,16
7897,7259581,"12  Character-level parsing expands the training trees (see Figure 7 ) and the already large indexed symbol space size explodes (1.9 million increases to 12.3 million, see Table 2 ).",20,21
7898,7259581,"The packing shrinks the symbol space size from 12.3 million to 1.1 million, a reduction by a factor of 11.",4,5
7899,7259581,It is reasonable to hope that the gains from using large fragments and the gains from symbol refinement will be complementary.,16,17
7900,7259581,"The basic incorporation of large fragments alone does not yield particularly strong performance, nor does basic symbol refinement.",17,18
7901,7259581,"Additional Deterministic Refinement Basic symbol refinement (parent annotation), in combination with all-fragments, gives test-set accuracies of 88.5% (≤ 40 words) and 87.6% (all), shown as the Basic Refinement model in Table 5 .",4,5
7902,7259581,"Klein and Manning (2003) describe a broad set of simple, deterministic symbol refinements beyond parent annotation.",14,15
7903,7259581,"This additional annotation (see Ad- ditional Refinement, Table 5 ) improves the testset accuracies to 88.7% (≤ 40 words) and 88.1% (all), which is equal to a strong lexicalized parser (Collins, 1999) , even though our model does not use lexicalization or latent symbol-split induction.",55,56
7904,7259581,"14 Conclusion Our approach of using all fragments, in combination with basic symbol refinement, and even without an explicit lexicon, achieves results in the range of state-of-the-art parsers on full scale treebanks, across multiple languages.",13,14
7905,52009128,"It is to use a class symbol to replace both the slot label and its slot value in the source sentence, for example, ""Play $song in $album"".",6,7
7906,52009128,"The class symbol represents the slot label, but without any specific value, as shown in the last row in Table 1 .",2,3
7907,52009128,Representing each slot segment with a single symbol has a great advantage of avoiding a multiword segment to be translated into several non-consecutive segments and not enclosed by the correct slot-label pairs.,7,8
7908,52009128,"In this task, specific words are those class symbols, and specific rules are the correspondence of each class symbol in the two languages.",20,21
7909,17039365,"Basic role classifier The basic role classifier takes the hidden state of the top-layer bidirectional LSTM corresponding to the considered word at position i and uses it to estimate the probability of the role r. Though we experimented with multilayer perceptrons, we obtained the best results with a simple log-linear model: p(r|v i , p) ∝ exp(W r v i ), (1) where v i is the hidden state calculated by BiLST M (x 1:n , i), p refers to the predicate and the symbol ∝ signifies proportionality.",99,100
7910,26781985,"For instance, consider the following derivation: The + symbol is interpreted as the addition of real numbers (add R ) in the first clause but that of vectors (add V ) in the second one.",10,11
7911,2521722,"1 In the definitions, f and P respectively denote a function and a predicate symbol and v denotes a variable.",15,16
7912,2220970,"Each production rule in an SCFG rewrites a non-terminal symbol as a pair of phrases, which may have contain a mix of words and non-terminals symbols.",11,12
7913,2220970,"The Hiero model (Chiang, 2007 ) used a single non-terminal symbol X. Other approaches have read symbols from constituent parses of the training data (Galley et al.,",14,15
7914,2220970,"These rules are formulated as SCFG rules, with a syntactic left-hand nonterminal symbol and two English right-hand sides representing the paraphrase.",15,16
7915,2220970,"For strict overlap, we say that two rules are equivalent if they are identical, that is, if they have the same left-hand side non-terminal symbol, their source sides are identical strings, and their target sides are identical strings. (",31,32
7916,2220970,"That is, two rules are considered equivalent if they are identical after all the non-terminal symbols have been replaced by one equivalent symbol.",25,26
7917,14459969,We use the special symbol to denote a variable-length gap.,4,5
7918,9895822,"For instance, for the ADJP/VP phrase pair capable of and able to, there are the following minus-log probabilities: p(lhs|e1) = 0.1, p(lhs|e2) = 0.3, p(e1|lhs) = 5.0 p(e1|e2) = 1.3, p(e2|lhs) = 6.7, p(e2|e1) = 2.8 p(e1|e2, lhs) = 0.6, p(e2|e1, lhs) = 2.3 where e1/e2 are the phrase pair, and lhs is the left hand side syntactic non-terminal symbol.",86,87
7919,227231412,"Therefore we have used a more detailed method according to characteristic of the code-mixing data (URLs, emoji, hash symbol etc.)",23,24
7920,227231412,"Moreover, we also remove the hash Figure 1 : Illustration of our model symbol from hash-tags as it can be problematic for tokenizers to work with.",14,15
7921,227231412,"As for non-text symbol like emoji and emoticon, we use the (emoji, 2019) library from python and emoticon dictionary from wiki (List of emoticons, 2020) respectively to transform the symbols to text.",5,6
7922,4894130,Operand Access (H): The second operand of an operator symbol e i is always e i−1 .,11,12
7923,4894130,"s e denotes the corresponding constant or operator symbol e (e.g., '+', '=', '0', etc.)",8,9
7924,203836888,"Output y contains a sequence of m action symbols, where each output symbol is from an output vocabulary of size V .",13,14
7925,203836888,"Both vocabularies contain an end-of-sentence symbol which appears at the end of x and y, respectively.",9,10
7926,203836888,"2) During decoding for autoregressive models, we do not use the previous output symbol as input for the next symbol prediction, because output symbols contain information for primitives.",15,16
7927,203836888,"2) During decoding for autoregressive models, we do not use the previous output symbol as input for the next symbol prediction, because output symbols contain information for primitives.",21,22
7928,203836888,3) We end decoding when the last output is end-of-sentence symbol.,15,16
7929,203836888,The decoding ends when argmax ŷj is end-of-sentence symbol.,12,13
7930,203836888,"Some pseudowords are primitive corresponding to a single output symbol, while others are function words that process items.",9,10
7931,203836888,"For the output end-of-sentence symbol, we expect that the attention is on the input end-of-sentence symbol.",8,9
7932,203836888,"For the output end-of-sentence symbol, we expect that the attention is on the input end-of-sentence symbol.",24,25
7933,14971078,"a n−1 b n−1 a n N 2 , where a i , a i are arbitrary words, and either b i is nonempty word, or the special symbol * stands instead of b i , admits arbitrary f i .",30,31
7934,14971078,First of all the last symbol of word w is deleted.,5,6
7935,237365379,"For example, we usually use monkeys to express cleverness, but in Uyghur language it is a symbol of cunning.",18,19
7936,6737912,"Each symbol is automatically associated with a set of descriptive features generally accepted in the fields of phonetics and phonology (e.g. bilabial, alveolar, voiced, voiceless, aspirated) (Ladefoged and Maddieson, 1996) .",1,2
7937,6737912,These features are extremely useful in the sense that they provide series of descriptive labels to each transcribed symbol.,18,19
7938,6737912,"At the core of the Phon alignment algorithm is a function sim(x, y) that assesses the degree of similarity of a symbol x from the first given sequence and a symbol y from the second given sequence.",23,24
7939,6737912,"At the core of the Phon alignment algorithm is a function sim(x, y) that assesses the degree of similarity of a symbol x from the first given sequence and a symbol y from the second given sequence.",32,33
7940,216562482,We ignore the model symbol M in our later notations for simplicity.,4,5
7941,231704317,"2019) where the programmer's policy begins with k numbers of READ, and is followed by switching WRITE and READ, until the source sentence is exhausted or end of sentence (EOS) symbol is written.",36,37
7942,227125888,"and also merged any occurrences of the word ""non-binary"" with ""nonbinary,"" before eliminating characters which were not alphanumeric, space, the ascii apostrophe, or a currency symbol.",35,36
7943,237453579,"Here, [CLS] is a special symbol that denotes downstream classification and [SEP ] is for separating non-consecutive token sequences.",8,9
7944,235212571,"Once the user has finished annotating all samples, the annotation session is finished by clicking the lock symbol in the navigation bar.",18,19
7945,14602909,Note that the symbol '1 ab ' in our matrix multiplications refers to a matrix of size a × b consisting of only 1's.,3,4
7946,6067240,"There is a one-toone correspondence, ∼, between the nonterminals in f and e: each nonterminal symbol in f has to also appear in e. Following Zhao et al. (",20,21
7947,247158648,"The symbol ""-"" denotes results not reported in previous papers.",1,2
7948,247158648,"The symbol ""⋆"" indicates systems that we evaluated by ourselves using their public code and model checkpoints. †",1,2
7949,226222033,"2019) have focused on learning two types of representations: the bottomlevel -a contextual representation centered at a single word, trained by recovering randomly masked tokens or predicting previous and next words, and the top-level text, implicitly represented as a single [CLS] symbol and trained by predicting a relation between input segments, which usually consist of multiple sentences.",50,51
7950,3028245,"In order to deal with the situation where a word in the confusion network is not in the vocabulary of the language model, we need to build another simple transducer, namely, the ""unknown word"" FST (UNK-FST), to map this word to the symbol <unk> that encodes the out-of-vocabulary (OOV) words.",52,53
7951,11637332,The symbol denotes the Hadamard product or element-wise multiplication.,1,2
7952,233240759,"B Dataset Analysis B.1 Data Splits We could not reconstruct some of the Social Media datasets in full (marked with a * symbol in Table 2 ), as with only tweet IDs, we could not obtain the actual tweet text in some cases.",23,24
7953,7384097,"The degree of generalization at each level would then depend on how blocks are represented (e.g., by their lexical content, by a tag denoting the block's syntactic category, or by a generic symbol).",37,38
7954,7384097,"To limit search complexity, a constraint is imposed on the maximum number of source words that may be covered by a non-terminal symbol during decoding (span constraint).",25,26
7955,7384097,"For instance, as shown in Figure 6 , an Arabic-English HSMT grammar is extended with an additional non-terminal symbol X0 that can only generate fully lexicalized phrases, thereby disallowing recursive nesting of hierarchical rules (shallow-1 grammar).",23,24
7956,7384097,"Instead of a single non-terminal X, three different reordering-based labels are used, according to the reordering pattern in which they participate: X for monotonic rules; XSL and XSR for the first and second symbol, respectively, of swapping rules.",41,42
7957,7384097,"The ith symbol of a permutation π will be denoted as π(i) and the identity, or monotone, permutation id is the permutation for which id(i) = i for all i. Figure 1 shows an example of two sentence pairs and their permutations.",2,3
7958,5170126,"Baseline: Hiero Hiero (Chiang, 2007) uses the simplest labeling possible: there is only one non-terminal symbol, X, for all rules.",22,23
7959,5170126,"If a subtree exactly spans the phrase pair, we can use the root label of that subtree to label the nonterminal symbol.",22,23
7960,447617,"An SCFG is a collection of rules {r i } that take the form: r i = C i → α i , γ i , ∼ i , ϕ i , (1) where left-hand side C i is a nonterminal symbol, the source side α i and the target side γ i are sequences of both nonterminal and terminal symbols.",47,48
7961,447617,All terminal and nonterminal symbols in the grammar are mapped to integer symbol id's using a globally accessible vocabulary map.,12,13
7962,447617,"It consists of an integer n, the number of children, and n blocks of two integers each, containing the symbol id a j leading to the child and the child node's address s j (as an index into the source-side array).",22,23
7963,447617,"The children in the link block are sorted by symbol id, allowing for a lookup via binary or interpolation search.",9,10
7964,447617,"It stores the number of rules, m, and then a tuple of three integers for each of the m rules: we store the symbol id of the left-hand side, an index into the target-side trie and a data block id. The rules in the data block are initially in an arbitrary order, but are sorted by application cost upon loading.",26,27
7965,447617,"However, the target trie is a reversed, or upward-linking trie: a trie node retains a link to its parent, as well as the symbol id labeling said link.",29,30
7966,2172129,All other tokens outside the vocabulary list are replaced by the UNK symbol.,12,13
7967,215785932,The BiLSTM model is used to monitor the probability of an end-of-token symbol appearing after any character within a given token sequence.,16,17
7968,4993665,"The object-aware context vector c i for a particular word w i is calculated based on the bilinear attention between the word representation h i and the representations of the objects {g k }: α i,k = softmax k (h i B 1 g k ) (4) c i = k α i,k • g k (5) ĥi = relu (W LANG [h i ; c i ; h i −c i ; h i •c i ]) (6) where the symbol • denotes element-wise multiplication.",99,100
7969,9834527,For example all the forms of possessive suffix -(I)m of Table 2 are replaced by the symbol P1sg.,16,17
7970,218560609,"Meanwhile, Cobb County -Atlanta's geode of changeis home to some of the largest industrial parks in the South, a regional cultural center, a 100year-old manufacturing town and a potent symbol of the former city's cherished Georgian past.",35,36
7971,235482369,One symbol is encoded into an n-dimensional binary vector with one dimension for each possible character.,1,2
7972,235482369,"12] Knowing the probability of the next symbol enables the decoder to sample probable sequences of symbols. """,8,9
7973,15811826,2010) treat the ancestor as the first symbol in a context window that is shared between the ancestor and siblings.,8,9
7974,15811826,"The head of a non-terminal node can thus be extracted by identifying the pre-terminal child, and taking its terminal symbol as head.",24,25
7975,15811826,"An exception is the virtual node sent, which is added to the root of the tree to combine subtrees that are not connected in the original grammar, e.g. the main tree and the punctuation symbol.",36,37
7976,15811826,"For unknown words, we back-off to a special unk token for the sequence models and P l , and to the pre-terminal symbol for the other dependency models.",27,28
7977,14715684,"event, food, instrument, lang, letter, other, plant, product, religion, sport, substance, symbol, technique, term, vehi A QA system typically uses both a taxonomy of expected answers and the taxonomy of named entities produced by its NER to identify which named entities are relevant to a question.",22,23
7978,357270,"For baseline models that see two streams, the abbreviations are joined by a ""×"" symbol (as they treat input pairs as atoms drawn in the cross-product of the two streams' vocabulary).",17,18
7979,357270,"For the backoff models, the abbreviations are joined by a ""+"" symbol (as they combine the information sources with a weighted sum), with the ""extra"" stream name first.",14,15
7980,2043113,"When a word is less than five characters, we pad the remaining characters with the same special symbol.",18,19
7981,2043113,"The output layer has 1286 neurons, which equals to the number of tags in the training set with a RARE symbol.",21,22
7982,18342680,"Consider, again, the example line from Paradise Lost, whose natural analysis is where we use the symbol / to denote stressed (ictic) syllables, 2 and x to denote unstressed (non-ictic) ones, as is done in Steele (1999) and the Princeton Encyclopedia of Poetry and Poetics (Preminger et al.,",19,20
7983,14231235,Each fragment has a root symbol (analogous to the left-hand-side category in a CFG) and a frontier which can consist of terminals (words) and non-terminal symbols to be filled in later in the derivation.,5,6
7984,12025181,"25 Linking to a fragment of a document In the definition of URI, 26 the only provision to refer to parts of a webpage occurs through the use of fragment identifiers using the symbol ""#"", as in the example: http:// server.info/folder/page.html#section2; however, this presumes the existence of identified anchors in the HTML document.",34,35
7985,16343150,This cutoff indicates the least probable category assignment we will permit for the surface symbol s ij .,14,15
7986,12807356,"The specific symbol ""@"" was added into the transcription convention to mention a laughter item.",2,3
7987,16615791,"3 PCFGs and the Dependency Model with Valence Probabilistic Context Free Grammars A Probabilistic Context Free Grammar is a tuple (W , N , S, R, θ), where W and N are sets of terminal and non-terminal symbols, S ∈ N is a distinguished start symbol, and R is a set of production rules.",53,54
7988,16615791,"Minimally, we need rightward-looking R w , leftward-looking L w , and undirected Y w non-terminal labels for each word w. The grammar has a rule for each dependent word d of a head word h from the left (L h → Y d L h ) and from the right (R h → R h Y d ), a rule for each a word w to be the sentence root (S → Y w ), and a rule for each undirected symbol to split into directed symbols (Y w → L w R w ).",94,95
7989,21698016,"In recent years, there has been a surge of interest in the natural language processing related to the real world, such as symbol grounding, language generation, and nonlinguistic data search by natural language queries.",24,25
7990,21698016,"Introduction The field of natural language processing (NLP) has experienced a resurgence of interest in the symbol grounding problem (Harnad, 1990 ).",18,19
7991,21698016,Symbol Grounding One of the most interesting research directions is symbol grounding.,10,11
7992,21698016,"While symbol grounding of a content word to the world is a straight concept, grounding of a modality expression such as ""must"" and ""may"" to images, videos, many other forms of media is an open question.",1,2
7993,21698016,"We believe that there are many other novel applications including bilingual lexicon acquisition for function words and modality expressions based on symbol grounding (Kiela et al.,",21,22
7994,21698016,"This will enable NLP and AI researchers to tackle various new problems such as commentary generation, intelligent game state search, and symbol grounding.",23,24
7995,218974047,"In recent years, there has been a surge of interest in natural language processing related to the real world, such as symbol grounding, language generation, and non-linguistic data search by natural language queries.",23,24
7996,218974047,"Introduction These days, the interest in the symbol grounding problems becomes larger and larger.",8,9
7997,218974047,"For an accurate evaluation of symbol grounding, we should construct a corpus which shows the relationship between natural language texts and non-text world states.",5,6
7998,218974047,"Our high-quality corpus, which is annotated by annotators with high skill of target domain, may help to evaluate the module evaluation of symbol grounding.",26,27
7999,218974047,"Symbol Grounding to Search Tree For describing the machine thought, symbol grounding to the searching tree is an important factor.",11,12
8000,218974047,"The temporal relation and appearance probability, including the path to the future states, are the positive examples for symbol grounding.",20,21
8001,12260053,"In order to reduce the number of sentence candidates, which has a direct impact on alignment computation time, we do not split sentences in the following cases: • Dot in strings of alphanumeric or symbol characters. •",37,38
8002,6447567,1994) that reads an input string symbol-by-symbol and probabilistically produces an output string; thus it can be used to specify a conditional probability on output strings given an input.,7,8
8003,6447567,1994) that reads an input string symbol-by-symbol and probabilistically produces an output string; thus it can be used to specify a conditional probability on output strings given an input.,11,12
8004,6447567,"To capture trends specific to particular sounds, each template is instantiated again using the actual symbol for curr and articulatory values for everything else (e.g., [h]→unvoiced).",16,17
8005,6447567,"An additional template, →out, captures the marginal frequency of the output symbol.",13,14
8006,6718044,Hierarchical Phrase-Based Translation The non-terminal set of a standard hierarchical grammar comprises two symbols which are shared by source and target: the initial symbol S and one generic non-terminal symbol X .,28,29
8007,6718044,Hierarchical Phrase-Based Translation The non-terminal set of a standard hierarchical grammar comprises two symbols which are shared by source and target: the initial symbol S and one generic non-terminal symbol X .,36,37
8008,6718044,The initial symbol S is the start symbol of the grammar.,2,3
8009,6718044,The initial symbol S is the start symbol of the grammar.,7,8
8010,6718044,"To determine the orientation frequency for a hierarchical phrase with non-terminal symbols, the orientation counts of all those phrases are accumulated from which a sub-phrase is cut out and replaced by a non-terminal symbol to obtain this hierarchical phrase.",40,41
8011,6718044,Boundary Non-Terminals Cases where a non-terminal orientation cannot be established at the moment when the hierarchical rule is considered arise when a non-terminal symbol is in a boundary position on target side.,30,31
8012,6718044,"During decoding, a maximum length constraint of ten is applied to all non-terminals except the initial symbol S .",19,20
8013,5757992,We propose a novel framework for improving a word segmenter using information acquired from symbol grounding.,14,15
8014,5757992,"We generate a term dictionary in three steps: generating a pseudo-stochastically segmented corpus, building a symbol grounding model to enumerate word candidates, and filtering them according to the grounding scores.",19,20
8015,5757992,"Finally, we compile the symbol grounding results at all states and incorporate them to an automatic WS.",5,6
8016,5757992,"To the best of our knowledge, this is the first result reporting a performance improvement in an NLP task by symbol grounding.",21,22
8017,5757992,"Stochastically Segmented Corpus Before symbol grounding, we need to segment the text into words that include probable candidate words.",4,5
8018,5757992,"Pseudo-Stochastically Segmented Corpora The computational cost (in terms of both time and space) for calculating the expected frequencies in an SSC is very high 3 , so it is not a practical approach for symbol grounding.",38,39
8019,5757992,"We execute the above procedure m times and divide the counts by m. The law of large numbers guarantees that the approximation errors decrease to 0 when m → ∞. Symbol Grounding As the target of symbol grounding, we use states (piece positions) of a Shogi game and commen-taries associated with them.",36,37
8020,5757992,Grounding Words We build a symbol grounding model using a Shogi commentary dataset.,5,6
8021,5757992,"Unlike normal symbol grounding, the vocabulary contains many word candidates appearing in the pSSC generated from the commentaries.",2,3
8022,5757992,Word Segmentation Using Symbol Grounding Result This section describes a baseline automatic word segmenter and a method for incorporating the symbol grounding result to it.,20,21
8023,5757992,"Training a Word Segmenter with Grounded Words As a first trial for incorporating symbol grounding results to an NLP task, we propose to generate a dictionary based on the symbol grounding result.",13,14
8024,5757992,"Training a Word Segmenter with Grounded Words As a first trial for incorporating symbol grounding results to an NLP task, we propose to generate a dictionary based on the symbol grounding result.",30,31
8025,5757992,We can expect that the word candidates that are given high scores by the perceptron in the symbol grounding result have strong relationship to the positions.,17,18
8026,5757992,"First, we acquire a V -dimensional real-valued vector for each Shogi state S i as the result of symbol grounding.",21,22
8027,5757992,The 1 and sent to the symbol grounding module.,6,7
8028,5757992,The model is trained from the language resources for the Baseline and the symbol grounding result.,13,14
8029,5757992,Thus we can say that WS improvement by symbol grounding is as valuable as the annotation additions.,8,9
8030,5757992,This result shows that the symbol grounding successfully acquired new words with a few erroneous words.,5,6
8031,5757992,Conclusion We have described an unsupervised method for improving word segmentation based on symbol grounding results.,13,14
8032,5757992,The experimental results showed that we can improve word segmentation by using symbol grounding results.,12,13
8033,5757992,"It is interesting to apply the symbol grounding results to an embedding model-based word segmentation approach (Ma and Hinrichs, 2015) .",6,7
8034,27392476,To obtain base forms a token or a group of tokens matched with a symbol marked with the $ character are replaced by their nominal forms.,14,15
8035,27392476,"In the grammar given above, the only symbol marked with $ is NAP .",8,9
8036,27392476,The left-hand side of a rule consists of only one nonterminal symbol.,13,14
8037,27392476,"and '+', which indicate zero or one, zero or more and one or more occurrences of the preceding symbol, respectively.",22,23
8038,27392476,"No loops are allowed, which means that the rewriting process cannot yield to a symbol that appeared on the left hand-side of an applied rule.",16,17
8039,27392476,"For each symbol, it is possible to specify a test or a series of tests performed during the matching process.",2,3
8040,27392476,"Tests, separated with semicolons, are placed in square brackets just after a symbol to which they relate.",14,15
8041,27392476,"In the simple grammar given above, N [pos = subst, ger ] means that a token matched with the symbol N must be a substantive or a gerund, whereas NAP [agreement] means that a sequence of tokens matched with NAP must agree in number, case and gender.",22,23
8042,7769737,Any symbol in alphabet 0 The empty string (epsilon) [ and ] Grouping brackets A -> B Change A to B [..] -> B Epenthesize B || C D Context specifier .#.,1,2
8043,7769737,"Therefore, we coded each input /@/ with the written orthographic symbol it originated from.",11,12
8044,37821487,"In recent years there has been a surge of interest in the natural language prosessing related to the real world, such as symbol grounding, language generation, and nonlinguistic data search by natural language queries.",23,24
8045,37821487,"These attempts at connecting language expressions to real world objects such as images are often called symbol grounding (Harnad, 1990) , an exciting new area in natural language processing.",16,17
8046,37821487,"However, images, videos, and many other forms of media have ambiguities that make symbol grounding difficult.",16,17
8047,37821487,"These may be the first target for the symbol grounding research, an application of our corpus.",8,9
8048,37821487,Symbol Grounding One of the most interesting research directions is symbol grounding.,10,11
8049,37821487,Another interesting aspect of symbol grounding to game states is that we can connect natural language expressions to computer analysis and predictions.,4,5
8050,37821487,Others The NE recognition and/or symbol grounding results allow for various applications.,5,6
8051,37821487,The combination of NE recognition and symbol grounding will enable this.,6,7
8052,37821487,"Because there has never been a corpus of game commentaries related to game states, we believe that there are many other novel applications such as bilingual lexicon aquisition based on symbol grounding (Kiela et al.,",31,32
8053,37821487,"We believe this will enable NLP and AI researchers to begin to tackle various new problems such as symbol grounding, commentary generation, and intelligent game state search.",18,19
8054,215824896,"For the word-initial case, we introduce a special ""start of word"" symbol ( ).",16,17
8055,215824896,"This symbol is prepended to each word during both training and testing, and is assigned the epsilon label during training when there is no word-initial insertion.",1,2
8056,2453822,"The terminal symbols (E) are edge labels, while the nonterminal symbols (N) encode (sub)paths between concepts; S G is the start symbol of G, and P G the set of its productions.",28,29
8057,251701,"Because we do not assign class name to our constituents, i.e. a left hand side symbol for the grammar rules, as the linguists do in treebanks, the comparison ignores the class labels, considering only groups of tags.",16,17
8058,15747640,The entailment symbol signifies that the student understood that MMP.,2,3
8059,7981427,"The terminal symbols (E) are edge labels, while the non-terminal symbols (N) encode (sub)paths between concepts; S G is the start symbol of G and P G the set of its productions.",30,31
8060,9612053,"In addition, we removed morphological features that are not explicitly marked by an overt morpheme -thus each feature symbol beyond the root part-of-speech corresponds to a morpheme.",19,20
8061,16618391,"Instead of a pronoun in the English text, the annotator is presented with an instance of ""that"" or a symbol representing the null-relativizer.",22,23
8062,9096376,"In raw Vietnamese texts, space symbol can be treated as an overload symbol, which is a connector within a word or is a separator between words.",6,7
8063,9096376,"In raw Vietnamese texts, space symbol can be treated as an overload symbol, which is a connector within a word or is a separator between words.",13,14
8064,9096376,"Therefore, the Vietnamese word segmentation (VWS) problem can be defined as a binary categorization task for each space symbol.",21,22
8065,9096376,"If a space symbol is a connector in a word, we will output a symbol ('_') to replace it.",3,4
8066,9096376,"If a space symbol is a connector in a word, we will output a symbol ('_') to replace it.",15,16
8067,9096376,"And if a space symbol is a separator between words, we will maintain it as a space symbol (' ') in the segmented result.",4,5
8068,9096376,"And if a space symbol is a separator between words, we will maintain it as a space symbol (' ') in the segmented result.",18,19
8069,5406324,"Based on the additional assumption that the most frequent symbol in the text is a vowel, the algorithm iteratively selects the symbol which occurs most frequently adjacent to a vowel and determines it to be a consonant.",9,10
8070,5406324,"Based on the additional assumption that the most frequent symbol in the text is a vowel, the algorithm iteratively selects the symbol which occurs most frequently adjacent to a vowel and determines it to be a consonant.",22,23
8071,5406324,"PhonMatrix makes use of Sukhotin's algorithm as a preprocessing step to give a first guess of the class for each symbol, which the user can then modify if it turns out to be wrong.",21,22
8072,5406324,"Additionally, the sign of the φ value, which shows whether the co-occurrence of a certain symbol pair occurs more (+) or less (−) frequently than expected, is displayed in the matrix cell.",19,20
8073,7253541,Then we add the symbol TARGET to F and replace in T the terms being defined with TARGET .,4,5
8074,11631121,Word pair similarity was quantified as the cosine similarity between the activation patterns of the hidden layers at the end-of-sentence symbol.,24,25
8075,11631121,The nearest neighbour is the sentence for which the activation vector at the end of sentence symbol has the smallest cosine distance to the activation vector of the original sentence.,16,17
8076,13865400,"This process is simply obtained by the product of the adaptor and the background, normalized over the different possible vocabulary symbols x t+1 to obtain a probability distribution (as indicated by the proportionality symbol).",35,36
8077,248780243,"For both of these, the input is composed of the concatenation of the query x q and the context x c , subword-tokenized and separated by a [SEP] special symbol.",34,35
8078,248780243,"Additionally, to better separate entity candidate representations and ease their full span identification, we add a trailing special symbol </ec> to each of them; henceforth, we denote this resulting modified context by xc .",20,21
8079,16030505,"Note that delimiter between the tiers is the underscore symbol ("" "").",9,10
8080,2554659,"Therefore, we have separate tags for clitics that occur on their own or as the first part of a pair (PCl, e. g. Foreign words Foreign words are to be marked with the symbol FW.",36,37
8081,15205411,"All punctuation is removed, words are lowercased, and numbers are replaced by a symbol N. All words outside the vocabulary limit (10,000 words) are mapped to a special UNK symbol.",15,16
8082,15205411,"All punctuation is removed, words are lowercased, and numbers are replaced by a symbol N. All words outside the vocabulary limit (10,000 words) are mapped to a special UNK symbol.",33,34
8083,6651164," Data have been anonymized: the names of customers and agents have been replaced by a single symbol (_CUST_ and _AGENT_ respectively) as well as the phone numbers, contract references, addresses and email addresses.",18,19
8084,2387926,"Roughly speaking, a local multi bottom-up tree transducer ( MBOT) has rules that replace one nonterminal symbol N on the source side by a tree, and a sequence of nonterminal symbols on the target side linked to N by one tree each.",20,21
8085,2387926,We estimate the probability p(g|e) through a loglinear combination of component models with parameters λ m scored on the derivations d such that the source tree of d is in the parse forest of e and the yield of the target tree reads (11) Input parse tree probability assigned to s(t) by the parser of e The rule weights required for (1) are relative frequencies normalized over all extracted rules with the same root symbol on the left-hand side.,80,81
8086,16026358,"Thus, the HeavyRhyme symbol could simultaneously capture the most important aspects of both stress and coda constraints.",4,5
8087,235097236,We separate the context c and the definitional context D ŵ with the special symbol </s> and surround the whole text with the tags <s> and </s>.,14,15
8088,15445820,"They facilitate many symbol manipulation tasks, including operations on parse trees and logical forms, and even inference and aspects of dialogue and translation.",3,4
8089,15445820,"Of course this can be accomplished in a standard symbol manipulation language like Lisp, but the requisite multiple lines of code obscure the simple nature of the task.",9,10
8090,15445820,"A (B C)) will match either the symbol A or the list (B C), i.e., the two arguments provide alternatives.",10,11
8091,15445820,Here are a few examples of simple template to template transductions: • (/ X Y) Replaces the symbol X with the symbol Y. • (/ (!,20,21
8092,15445820,Here are a few examples of simple template to template transductions: • (/ X Y) Replaces the symbol X with the symbol Y. • (/ (!,24,25
8093,15445820,"X) could replace any node of a tree, including the root, with a single symbol).",17,18
8094,15445820,replaces all occurrences of a free variable symbol in an expression with a new one. (,7,8
8095,7863488,1) The words in Q containing task information is replaced with symbol T. The revised string is denoted by Q (1) . (,12,13
8096,7863488,2 ) The verbs and their direct objects in Q (1) are replaced with symbol VO.,16,17
8097,7863488,"3) The compound nouns, or adjective + nouns in Q (2) are replaced with symbol MN.",18,19
8098,26545229,"Projecting to the top (in)correct examples, a † symbol is placed behind the more frequent word in a pair.",10,11
8099,21693683,"Annotators can also assign a special symbol to a pair to temporarily skip it, which can be useful when faced with difficult examples.",6,7
8100,4612903,"For instance, AdaGram does not contain separate senses for ""apple fruit"" 11 and ""apple fruit as a symbol"" 12 found in BabelNet.",21,22
8101,1529891,"The system has several layers, with plWordNet in the middle: • top-and medium-level ontology SUMO with plWordNet semi-automatically mapped onto it (Kędzia and Piasecki, 2014) , 10 The symbol WordNet R is a registered trademark.",39,40
8102,7399876,"Whenever we use the + symbol in our results tables, the additional features were combined with existing features log-linearly.",5,6
8103,1987786,"Let us consider a single rule: s -> (WANT :agent i :patient s) The right-hand side is a symbol (WANT :agent :patient) whose tail edges are labeled with states (i and s), and after applying the rule, its head edges are labeled with new states (s).",26,27
8104,1987786,This derivation forest acceptor has the set of rules as its symbol and the set of configurations (state-labelings of the input dag) as its state set.,11,12
8105,1987786,"For this purpose, we use the q (query) command and the v feature: (v (g (f boywants.dag)) boywants.pdf) (v (a (f example.bda)) example.pdf) Dag acceptors are represented as hypergraphs, where the nodes are the states and each hyperedge represents a rule labeled with a symbol.",61,62
8106,18599816,Algebras IRTGs represent the objects and operations symbolically using terms; the object in question is obtained by interpreting each symbol in the term as a function.,20,21
8107,18599816,"In the string case, we describe complex strings as concatenation (con 2 ) of elementary symbols (e.g., a, b); in the tree case, we alternate the construction of a sequence of trees (con 2 ) with the construction of a single tree by placing a symbol (e.g., α, β, σ) on top of a (possibly empty) sequence of trees.",54,55
8108,18599816,A signature is an alphabet Σ where each symbol is equipped with an arity.,8,9
8109,18599816,"A ∆-algebra A consists of a nonempty set A called the domain and, for each symbol f ∈ ∆ with rank k, a total function f A : A k → A, the operation associated with f .",16,17
8110,18599816,q k ∈ Q. We call α the terminal symbol and k the rank of the rule.,9,10
8111,18599816,"If q = q 0 , we drop the superscript and write L(G) for the tree language of G. In the literature, there is a definition of RTG which also permits more than one terminal symbol per rule, strings over Γ trees over Γ example term and denoted object con 2 a b → ab σ con 2 α con 0 β con 0 → σ α β domain Γ * T * Γ (set of sequences of trees) signature ∆ {a| 0 | a ∈ Γ} ∪ {γ| 1 | γ ∈ Γ} ∪ {con k | k | 0 ≤ k ≤ K, k = 1} {con k | k | 0 ≤ k ≤ K, k = 1} operations a : () → a γ : x 1 → γ(x 1 ) con k : (x 1 , . . . ,",37,38
8112,18599816,"We obtain ξ from τ by replacing every label of the form {x j } with x j , and every other label with a fresh symbol.",27,28
8113,18599816,"In order to construct h i (σ) for each symbol σ in ξ, we transform t i into a tree t i with labels from C ∆ i (X) and the same structure as ξ.",11,12
8114,18599816,"Given an algebra A over ∆, a b-rule b over ∆ is called a b-rule over A if, for every t ∈ T ∆ (X k ) and t ∈ b(t), t and t are equivalent in A. Such a b-rule encodes equivalence in A, and it does so in an explicit and compact way: because b(f ) is a regular tree language, a b-rule can be specified by a finite collection of RTGs, one for each symbol f ∈ ∆. We will look at examples (for the string and tree algebras shown earlier) in Section 5.",94,95
8115,18599816,Each symbol a ∈ ∆ i | 0 is mapped to the language {a}.,1,2
8116,18599816,"Each symbol con k , k ≥ 2, is mapped to the language induced by the following RTG with states of the form [j, j ] (where 0 ≤ j < j ≤ k) and final state [0, k]: [j − 1, j] → x j (1 ≤ j ≤ k) [j, j ] → con 2 ([j, j ], [j , j ]) (0 ≤ j < j < j ≤ k) This language expresses all possible ways in which con k can be written in terms of con 2 .",1,2
8117,18599816,It maps con 0 to {con 0 } and each unary symbol γ to {γ(x 1 )}.,12,13
8118,18599816,"Each symbol con k , k ≥ 2, is treated as in the string case.",1,2
8119,18599816,We used the class of all IRTGs with two string algebras and in which h i (α) contains at most one occurrence of a symbol con k for every α ∈ Σ. On such a grammar the b-rules are complete.,26,27
8120,12015882,"By noticing that F( − → u ⊗ − → v ) = F( − → u ) × F( − → v ), with symbol × to denote in this case the cartesian product of the two feature sets, we define the feature set of a transitive sentence as follows: F(svo Rel ) = i F( − − → Sbj i ) × F( − − → Obj i ) ∩ F( − → s ) × F( − → o ) (9) Equation 9 shows that the features of this model are pairs (f sbj , f obj ), with f sbj a subject-related feature and f obj an object-related feature, providing a fine-grained representation of the sentence.",29,30
8121,12015882,"In order to derive their feature inclusion properties, we first examine the form of the sentence vector produced when the verb is composed with a new subject/object pair: − → svo CpSbj = − → s (verb × − → o ) = − → s i − − → Sbj i − − → Obj i | − → o − → svo CpObj = ( − → s T × verb) − → o = − → o i − − → Obj i − → s | − − → Sbj i We can now define the feature sets of the two models using notation similar to that of Equation 6 : F( − → svo CpSbj ) = F( − → s ) ∩ i F( − − → Sbj i ) | F − − → Obj i | − → o F( − → svo CpObj ) = F( − → o ) ∩ i F( − − → Obj i ) | F − → s | −→ Sbj i (11) The symbol | defines a restriction on feature inclusion based on how well the arguments of the sentence fit to the arguments of the verb.",193,194
8122,12015882,"Thus we create verb matrices for intransitive sentences and verb phrases by summing up projectors of the argument vectors, in the following way: v itv := i − − → Sbj i ⊗ − − → Sbj i v vp := i − − → Obj i ⊗ − − → Obj i (13) When these verbs are composed with some subject/object to form a phrase/sentence, each vector in the spanning space is weighted by its similarity (assuming normalized vectors) with the vector of that subject/object, that is: − → sv Prj = − → s T × v itv = i − − → Sbj i | − → s − − → Sbj i − → vo Prj = v vp × − → o = i − − → Obj i | − → o − − → Obj i (14) Translating the above equations to feature inclusion representations will give: F( − → sv Prj ) = i F( − − → Sbj i ) | F − − → Sbj i | − → s F( − → vo Prj ) = i F( − − → Obj i ) | F −− → Obj i | − → o (15) with symbol | to define again a restriction on feature inclusion based on the similarity of the arguments with the subject or object of the sentence/phrase.",233,234
8123,8702399,"If these words are discarded or replaced with unk symbol, any models may not be able to select the correct answers for response during testing.",9,10
8124,4955581,A ranked alphabet is a finite set Σ (of symbols) where every symbol carries a rank (a nonnegative integer).,14,15
8125,4955581,"As another example, the mapping θ could assign to each symbol an operation over real-valued vectors, where each component represents one feature of a log-linear model such as frequencies, probabilities, reals, etc.",11,12
8126,6730628,V Voiced 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 Labial 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 Dental 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Alveolar 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Palatal/Post-alveolar 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 Velar 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 Uvular 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 Glottal 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 Stop 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 Fricative 1 1 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 Affricate 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Nasal 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 Click 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 Approximant 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 Lateral 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 Rhotic 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 Table 2: The ASJP alphabet is given in columns 2 − 35 and the phonetic value of each symbol in the ASJP alphabet.,600,601
8127,14370191,"The symbol (U) denotes systems that have used additional data besides the training set provided by SemEval (i.e., unconstrained in SemEval terminology).",1,2
8128,1658574,"The value may be an atomic symbol, or it may itself be another feature structure.",6,7
8129,1658574,"A dag-to-tree transducer explicit rule has the form r : α → β where α ∈ Q m (Σ) and β ∈ (T ∆ (Q(X n ))) m for m, n ∈ N. Intuitively, this means that the left-hand side still consists of a symbol and m ""incoming states"", while the right-hand side now are m trees over ∆ with states and n variables used to process the n child subdags.",58,59
8130,1658574,"In the tree world, every tree can be broken up into a root symbol and independent subtrees.",14,15
8131,1658574,"Therefore, if an edge reaching a given symbol σ is not followed at all (deleting rule), the transducer is going to choke if not every edge entering σ is ignored.",8,9
8132,1658574,"If no weights are available, other measures can be used (e.g. the number of derivation steps or symbol frequencies).",19,20
8133,15981961,Sometimes we assign a rank (or: arity) k ∈ N to a symbol σ ∈ Σ and then require that every σ-labeled position of a tree has exactly k successors.,15,16
8134,15981961,The foot node is labeled by an additional nullary symbol * .,9,10
8135,204810826,All tokens appearing less than three times were replaced with the unknown word symbol.,13,14
8136,9037546,"These datasets allow us to attempt the task of connecting language expressions to the real world, which is called symbol grounding (Harnad, 1990) .",20,21
8137,5199654,It furthermore contains an initial non-terminal symbol Q. Source sides of the rules are not decorated with syntactic annotation.,8,9
8138,5199654,"The source non-terminal vocabulary contains a single generic non-terminal symbol X. In addition to the extracted grammar, the translation system makes use of a special glue grammar with an initial rule, glue rules, a final rule, and top rules.",13,14
8139,5199654,"Conclusions String-to-tree and tree-to-string translation systems can easily be augmented with non-syntactic phrases by means of phrase table fill-up, a special non-terminal symbol for left-hand sides of nonsyntactic rules in the grammar, and an additional glue rule.",37,38
8140,8195427,"Formally, an ITG is a tuple ⟨N, Σ, ∆, R, S⟩, where N is a finite nonempty set of nonterminal symbols, Σ is a finite set of terminal symbols in L 0 , ∆ is a finite set of terminal symbols in L 1 , R is a finite nonempty set of inversion transduction rules and S ∈ N is a designated start symbol.",70,71
8141,8195427,"An inversion transduction rule is restricted to take one of the following forms: S → [A] , A → [ Ψ + ] , A → ⟨Ψ + ⟩ where S ∈ N is the start symbol, A ∈ N is a nonterminal symbol, and Ψ + is a nonempty sequence of nonterminals and biterminals.",39,40
8142,8195427,"An inversion transduction rule is restricted to take one of the following forms: S → [A] , A → [ Ψ + ] , A → ⟨Ψ + ⟩ where S ∈ N is the start symbol, A ∈ N is a nonterminal symbol, and Ψ + is a nonempty sequence of nonterminals and biterminals.",47,48
8143,8195427,"A biterminal is a pair of symbol strings: Σ * × ∆ * , where at least one of the strings have to be nonempty.",6,7
8144,8195427,"The brackets are frequently left out when there is only one element on the righthand side, which means that S → [A] is shortened to S → A. Like CFGs, ITGs also have a 2-normal form, analogous to the Chomsky normal form for CFGs, where the rules are further restricted to only the following four forms: S → A, A → [BC] , A → ⟨BC⟩, A → e/f where S ∈ N is the start symbol, A, B, C ∈ N are nonterminal symbols and e/f is a biterminal string.",91,92
8145,8195427,"We need one symbol for every nonterminal, L 0 -terminal and L 1 -terminal.",3,4
8146,8195427,"To serialize the rules, we need some kind of delimiter to know where one rule ends and the next starts; we will exploit the fact that we also need to specify whether the rule is straight or inverted (unary rules are assumed to be straight), and merge these two functions into one symbol.",57,58
8147,8195427,"Assuming a uniform distribution over the symbols, each symbol will require −lg 1 N bits to encode (where N is the number of different symbols-the type count).",9,10
8148,8195427,"The above example has 8 symbols, meaning that each symbol requires 3 bits.",10,11
8149,8195427,The transduction grammar that fits the training data the best is the one where the start symbol rewrites to the full sentence pairs that it has to generate.,16,17
8150,8195427,It is also possible to add any number of nonterminal symbols in the layer between the start symbol and the bisentences without altering the probability of the training data.,17,18
8151,8195427,We take advantage of this by allowing for one intermediate symbol so that the ITG conforms to the normal form and always rewrites the start symbol to precisely one nonterminal symbol.,10,11
8152,8195427,We take advantage of this by allowing for one intermediate symbol so that the ITG conforms to the normal form and always rewrites the start symbol to precisely one nonterminal symbol.,25,26
8153,8195427,We take advantage of this by allowing for one intermediate symbol so that the ITG conforms to the normal form and always rewrites the start symbol to precisely one nonterminal symbol.,30,31
8154,8195427,"Our initial ITG thus contains long rules that look like this: S → A A → e 0..T 0 /f 0..V 0 A → e 0..T 1 /f 0..V 1 • • • A → e 0..T N /f 0..V N where S is the start symbol, A is the nonterminal, N is the number of sentence pairs in the training corpus, T i is the length of the i th output sentence, V i is the length of the i th input sentence, e 0..T i is the sequence e 0 e 1 . . .",59,60
8155,17640293,"The above rule can thus be rephrased as: A → B (1) [ a x ] B (2) , [ a x ] B (1) B (2) [ a x ] → a, x where [ a x ] is a preterminal symbol unique for the translation of the terminal a to the terminal x. In this way, rules producing nonterminals and rules producing terminals can be separated.",52,53
8156,17640293,"An LTG over languages L 1 and L 2 is a tuple G = N, Σ, ∆, S, R where N is a finite, nonempty set of nonterminal symbols, Σ is a finite, nonempty set of L 1 symbols, ∆ is a finite, nonempty set of L 2 symbols, S ∈ N is the designated start symbol, and R is a finite, nonempty set of linear transduction rules on the forms: A → a/x B b/y, A → a/x where A, B ∈ N , a, b ∈ Σ * and x, y ∈ ∆ * .",66,67
8157,17640293,A preterminal is to be understood as a nonterminal symbol that can only rewrite into terminal symbols.,9,10
8158,17640293,"Let G = N, Σ, ∆, S, R be an LITG and G = N, P, Σ, ∆, S, R be the corresponding PLITG where: R = {A → [B [ a x ]], [ a x ] → a/x|A → [Ba/x] ∈ R} ∪ {A → [[ a x ] B], [ a x ] → a/x|A → [a/xB] ∈ R} ∪ {A → B [ a x ] , [ a x ] → a/x|A → Ba/x ∈ R} ∪ {A → [ a x ] B , [ a x ] → a/x|A → a/xB ∈ R} where [ a x ] is a preterminal symbol unique for the biterminal a/x, and P is the set of all unique preterminals introduced when building R .",152,153
8159,17640293,"For finite-state transduction grammars, preterminals are meaningless, as they merely change the label of a terminal symbol.",20,21
8160,17640293,"For efficiency reasons, we keep to a bracketing grammar, meaning that there is only one nonterminal and one preterminal symbol.",21,22
8161,10462362,"A bracketing grammar has only one nonterminal symbol, denoted X. A stochastic grammar is one where each rule is associated with a probability, such that ∀X   φ p(X → φ) = 1   While training a Stochastic Bracketing ITG (SBITG) or LITG (SBLITG) with EM, expectations of probabilities over the biparse-forest are calculated.",7,8
8162,2537787,"An LG in normal form is a tuple G L = N, Σ, R, S Where N is a finite set of nonterminal symbols, Σ is a finite set of terminal symbols, R is a finite set of rules and S ∈ N is the designated start symbol.",52,53
8163,2537787,"An LTG in normal form is a tuple T G L = N, Σ, ∆, R, S Where N is a finite set of nonterminal symbols, Σ is a finite set of terminal symbols in language E, ∆ is a finite set of terminal symbols in language F , R is a finite set of linear transduction rules and S ∈ N is the designated start symbol.",72,73
8164,2537787,"2010 ), and represent ITGs that are allowed to have at most one nonterminal symbol in each production.",15,16
8165,2537787,"An LITG in normal form is a tuple T G LI = N, Σ, ∆, R, S Where N is a finite set of nonterminal symbols, Σ is a finite set of terminal symbols from language E, ∆ is a finite set of terminal symbols from language F , R is a set of rules and S ∈ N is the designated start symbol.",69,70
8166,2537787,"Replacing the biterminal with a temporary symbol X, and introducing a rule that rewrites this temporary symbol to the replaced biterminal produces two rules: X → [ X X ] X → e/f This is no longer a bracketing grammar since there are two nonterminals, but equating X to X restores this property.",6,7
8167,2537787,"Replacing the biterminal with a temporary symbol X, and introducing a rule that rewrites this temporary symbol to the replaced biterminal produces two rules: X → [ X X ] X → e/f This is no longer a bracketing grammar since there are two nonterminals, but equating X to X restores this property.",17,18
8168,209053436,The classifier produces a probability distribution over an output vocabulary which includes all the possible synsets plus a special <untagged> symbol for words with no associated tag.,22,23
8169,6570842,"Finally, the nonterminals in the right-hand side that are aligned to w should be replaced by the translation that the alignment requests, provided that the nonterminal matches with the root symbol of the requested translation.",34,35
8170,2496355, It is also interesting to see that the generation of the segment break symbol uses information from segment-initial tokens and punctuations such as question marks.,14,15
8171,3034327,We use the symbol S ( Ē) to denote synthetic Spanish (English) data.,3,4
8172,15308586,"2006) uses a zeroth-order right-markovized version of the treebank which is subsequently augmented with latent symbol refinements in order to improve the fit, using smoothing and a split-merge procedure to avoid overfitting in the EM-based refinement process.",20,21
8173,27257744,"The OTHER tag is the second biggest set in overall, but they have a few occurances in named entities, which correspond to numbers and the & symbol in some brand names.",28,29
8174,6692902,"The grammar constant G can be eliminated by limiting the grammar to a bracketing ITG (BITG) , which only has one nonterminal symbol.",24,25
8175,6692902,Stochastic Bracketing Linear Inversion Transduction Grammar A Bracketing Linear Inversion Transduction Grammar (BLITG) is a BITG where rules may have at most one nonterminal symbol in their production.,26,27
8176,218974458,"Experimental Evaluation To investigate the usefulness of our dataset, we propose three tasks based on its most interesting characteristics: symbol grounding, bounding box linking prediction, and event attribute classification.",21,22
8177,218974458,"Proposed Model To build a model for the symbol grounding task, we have to consider that the step W contains each r-NEs R. Thus we adopted an embedding-based approach, which enables us to calculate the similarity between all combinations of the bounding box and candidate r-NEs.",8,9
8178,218974458,"We replaced words appearing less than three times with an unknown word symbol to have a 17,982-word vocabulary for the LSTM.",12,13
8179,1588739,"A transduction grammar is a grammar that generates sentences in two languages (L 0 and L 1 ) simultaneously; i.e., one start symbol expands into two strings, as for example in Figure 1(b) .",25,26
8180,1588739,"Formally, an ITG in this 2-normal form, which segregates syntactic versus lexical rules, consists of a tuple where N is a set of nonterminal symbols, V 0 and V 1 are the vocabularies of L 0 and L 1 respectively, R is a set of transduction rules, and is the start symbol.",59,60
8181,1588739,"As with a stochastic CFG (SCFG), the probabilities are conditioned on the left-hand-side symbol, so that the probability of rule X → χ is p(χ|X).",20,21
8182,1588739,"A bracketing ITG or BITG or BTG (Wu, 1995a) contains only one nonterminal symbol, with syntactic transduction rules X → [X X] and X → <X X>, which means that it produces a bracketing rather than a labeled tree.",16,17
8183,8981495,"Formally, an ITG is a tuple N, V, ∆, S , where N is a set of nonterminal symbols, ∆ is a set of rewrite rules, S ∈ N is the start symbol and V ⊆ V E × V F is a set of biterminal symbols, where V E is the vocabulary of E and V F is the vocabulary of F .",38,39
8184,8981495,Bracketing ITGs An ITG where there is only one nonterminal (other than the start symbol) is called a bracketing ITG (BITG).,15,16
8185,8981495,"An item is defined as a nonterminal symbol (we use X to denote the anonymous nonterminal symbol of the bracketing ITG) and one span in each language, written as X stuv where 0 ≤ s ≤ t ≤ T corresponds to the span e s..t and 0 ≤ u ≤ v ≤ V corresponds to the span f u..v .",7,8
8186,8981495,"An item is defined as a nonterminal symbol (we use X to denote the anonymous nonterminal symbol of the bracketing ITG) and one span in each language, written as X stuv where 0 ≤ s ≤ t ≤ T corresponds to the span e s..t and 0 ≤ u ≤ v ≤ V corresponds to the span f u..v .",17,18
8187,6858145,"The symbol ""*"" stands for ignored positions.",1,2
8188,9489624,The symbol ↑ denotes average percentage improvement than AP in 4 domains.,1,2
8189,8255284,"Within the web interface of the Corpus LSFB, the annotations may be shown, when they are available, by clicking on the appropriate symbol above the video viewers. •",25,26
8190,36952284,"The sentence is, however, aborted when the word length exceeds 20 or the terminal symbol appears.",16,17
8191,3080112,Word senses marked with the same symbol across languages correspond to the same BabelNet synset.,6,7
8192,29043524,Individual words are preceded by a ∧ symbol and followed with a $ ).,7,8
8193,1031444,"The Symbol-aware Subset Tree Kernel (henceforth, SASSTK), which we introduce here, allows a more fine-grained control over the weights by employing one λ and one α hyperparameter for each non-terminal symbol in the training data.",41,42
8194,1031444,"The calculation uses a similar recursive formula to the SSTK, namely: ∆(n 1 , n 2 ) =            0 pr(n 1 ) = pr(n 2 ) λ x pr(n 1 ) = pr(n 2 ) ∧ preterm(n 1 ) λ x g x (n 1 , n 2 ) otherwise, where x is the symbol at node n 1 and g x (n 1 , n 2 ) = |n 1 | i=1 (α x + ∆(c i n 1 , c i n 2 )) . (",69,70
8195,1031444,"By employing different hyperparameter values for each specific symbol, we can effectively modify the weights of all fragments where the symbol appears.",8,9
8196,1031444,"By employing different hyperparameter values for each specific symbol, we can effectively modify the weights of all fragments where the symbol appears.",21,22
8197,1031444,"We start with the λ gradient: ∂∆ ∂λ =              0 pr(n 1 ) = pr(n 2 ) u pr(n 1 ) = pr(n 2 ) ∧ preterm(n 1 ) ∂(λ x g x ) ∂λ otherwise, where x is the symbol at n 1 , g x is defined in Equation 4 and u is the k-dimensional unit vector with the element corresponding to symbol x equal to 1 and all others equal to 0.",55,56
8198,1031444,"We start with the λ gradient: ∂∆ ∂λ =              0 pr(n 1 ) = pr(n 2 ) u pr(n 1 ) = pr(n 2 ) ∧ preterm(n 1 ) ∂(λ x g x ) ∂λ otherwise, where x is the symbol at n 1 , g x is defined in Equation 4 and u is the k-dimensional unit vector with the element corresponding to symbol x equal to 1 and all others equal to 0.",81,82
8199,1031444,"Instead of optimizing all hyperparameters freely we use a simpler version where we tie λ and α for each symbol to the same value, except for the symbol 'S'.",19,20
8200,1031444,"Instead of optimizing all hyperparameters freely we use a simpler version where we tie λ and α for each symbol to the same value, except for the symbol 'S'.",28,29
8201,31259796,symbol asserts that the labels assigned to a span are mutually exclusive.),0,1
8202,7637628,"This disconnection is also referred to as the symbol grounding problem (Harnad, 1990) .",8,9
8203,7637628,"Here, this model is extended by exploiting both textual and visual features aiming to alleviate the symbol grounding problem of DSMs.",17,18
8204,16484719,"Formally, an ITG is a tuple ⟨N , W 0 , W 1 , R, S⟩, where N is a finite nonempty set of nonterminals, W 0 is a finite set of terminals in the output language L 0 , W 1 is a finite set of terminals in the input language L 1 , R is a finite nonempty set of inversion transduction rules and S ∈ N is a designated start symbol.",77,78
8205,16484719,"An inversion transduction rule is restricted to take one of the following forms: S → [A] , A → [φ + ] , A → ⟨φ + ⟩ where S ∈ N is the start symbol, A ∈ N is a nonterminal, and φ + is a nonempty sequence of nonterminals and biterminals.",39,40
8206,16484719,"The ITG 2-normal form is analogous to the Chomsky normal form for CFGs, where the rules are fur-ther restricted to only the following forms: S → A, A → [BC] , A → ⟨BC⟩, A → e/f, A → e/ϵ, A → ϵ/f where S ∈ N is the start symbol, A, B, C ∈ N are nonterminals, e ∈ W 0 is an L 0 token, f ∈ W 1 is an L 1 token, and ϵ is the empty token.",64,65
8207,16484719,"A bracketing ITG, or BITG, has only one nonterminal symbol (other than the dedicated start symbol), which means that the nonterminals carry no information at all other than the fact that their yields are discrete unit.",11,12
8208,16484719,"A bracketing ITG, or BITG, has only one nonterminal symbol (other than the dedicated start symbol), which means that the nonterminals carry no information at all other than the fact that their yields are discrete unit.",18,19
8209,16484719,"Clubs are wild, and NP ′ is the auxiliary symbol for NP.",10,11
8210,16484719,"Instead, we opt to have each left-hand side symbol be associated with one auxiliary symbol that handles the binarization of that category, and represents a fragment of it.",11,12
8211,16484719,"Instead, we opt to have each left-hand side symbol be associated with one auxiliary symbol that handles the binarization of that category, and represents a fragment of it.",17,18
8212,16484719,"We essentially generate the entire parse forest rooted in the left-hand side, with the right-hand side symbols as the leaves, and every internal node labeled with the auxiliary symbol (see Figure 1 ).",34,35
8213,13450512,"Definition 1 A linear transduction grammar (LTG) over languages L 1 and L 2 is a tuple: G = N, Σ, ∆, S, R where N is a finite nonempty set of nonterminal symbols, Σ is a finite nonempty set of L 1 symbols, ∆ is a finite nonempty set of L 2 symbols, S ∈ N is the designated start symbol and R is a finite nonempty set of production rules on the forms: A → a / x B b / y A → a / x where the nonterminals A, B ∈ N and the biter- minals a / x , b / y ∈ Σ * × ∆ * .",71,72
8214,13450512,The reflexive transitive closure of this relation can be used to define the transduction generated by an LTG as the set of bistrings that can be generated from the grammar's start symbol.,32,33
8215,13450512,"That is: only rules where at least one terminal symbol is produced together with a nonterminal symbol, and rules where the empty bistring is produced, are allowed.",10,11
8216,13450512,"That is: only rules where at least one terminal symbol is produced together with a nonterminal symbol, and rules where the empty bistring is produced, are allowed.",17,18
8217,49310285,"When the <eos> (i.e. end of sentence) symbol is seen, the final time step initializes the decoder recurrent network.",11,12
8218,49310285,"Then, the prediction is fed back to the decoder (i.e. input feeding), to predict the next word, until the <eos> symbol is generated (Sutskever et al.,",27,28
8219,49310285,"The model is organized as a stack of encoder-decoder networks that works in an auto-regressive way, using the previously generated symbol as input for the next prediction.",25,26
8220,1808504,"Same as expressed personal pronouns, they are represented by a node with the #PersPron symbol, e.g. (4) Bushova vláda oznámila, že se svůj plán #PersPron pokusí vzkřísit.",16,17
8221,5528527,"We will also assume that there exists a grammar G = N, Σ, S, R or a transduction grammar (over languages L 0 and L 1 ) G = N, Σ, ∆, S, R (depending on the context), where N is the set of nonterminal symbols, Σ is a set of (L 0 ) terminal symbols, ∆ is a set of (L 1 ) terminal symbols, S ∈ N is the dedicated start symbol and R is a set of rules appropriate to the grammar.",89,90
8222,5528527,"Taking CKY parsing (Cocke, 1969; Kasami and Torii, 1969; Younger, 1967) as an example, the items would have the form A i,j , which is to be interpreted as the span w i..j of the sentence being parsed, labeled with the nonterminal symbol A. The goal item would be S 0,|w| : the whole sentence labeled with the start symbol of the grammar.",55,56
8223,5528527,"Taking CKY parsing (Cocke, 1969; Kasami and Torii, 1969; Younger, 1967) as an example, the items would have the form A i,j , which is to be interpreted as the span w i..j of the sentence being parsed, labeled with the nonterminal symbol A. The goal item would be S 0,|w| : the whole sentence labeled with the start symbol of the grammar.",72,73
8224,5528527,"The symbols A, B and S are nonterminal symbols, while X represents a preterminal symbol.",16,17
8225,5528527,"For LITGs, the items take the form of bispans labeled with a symbol.",13,14
8226,5528527,"Intuitively, this is a good match to context-free grammars, since each rule connects one symbol on the left hand side (the head of the hyperedge) with any number of symbols on the right hand side (the tails of the hyperedge).",18,19
8227,15416634,"Roughly speaking, a local multi bottom-up tree transducer ( MBOT) has rules that replace one nonterminal symbol N on the source side by a tree, and a sequence of nonterminal symbols on the target side linked to N by one tree each.",20,21
8228,15416634,We estimate the probability p(g|e) through a loglinear combination of component models with parameters λ m scored on the derivations d such that the source tree s(d) of d is in the parse forest of e and the yield of the target tree t(d) reads The rule weights required for (1) are relative frequencies normalized over all extracted rules with the same root symbol on the left-hand side.,68,69
8229,21693139,"In order to obtain a reference for cognitive load, a dual-task approach was applied, including a visual-motor primary task that required subjects to learn abstract symbol combinations and an auditory-verbal secondary task to measure the load imposed by the primary task.",31,32
8230,21693139,"For this purpose, a dual-task paradigm was applied: a visual-motor primary task involving the assignment of symbol combinations to a single symbol, while simultaneously memorizing a sequence of five digits from an auditory-verbal secondary task.",22,23
8231,21693139,"For this purpose, a dual-task paradigm was applied: a visual-motor primary task involving the assignment of symbol combinations to a single symbol, while simultaneously memorizing a sequence of five digits from an auditory-verbal secondary task.",27,28
8232,21693139,"Each trial consisted of the following five steps: (1) Digit sound: a random sequence of five digits in the range of 1 to 9 (in random order) was generated by a textto-speech system (in German); (2) Symbol screen: one out of four randomly chosen combinations of abstract geometrical symbols was displayed where the order of the symbols must be considered; (3) Symbol response: one out of four possible symbols in a randomly arranged 2×2 grid had to be selected via mouse click; (4) Feedback: feedback was obtained, accompanied by the correct symbol in case of false responses to foster correct schema acquisition; (5) Digit response: the verbal recall of the five digit sequence of step 1 in correct order was requested.",114,115
8233,21693139,"For primary task efficiency, performance was obtained by symbol response correctness while reaction time needed to select a symbol constitutes the effort component.",9,10
8234,21693139,"For primary task efficiency, performance was obtained by symbol response correctness while reaction time needed to select a symbol constitutes the effort component.",19,20
8235,21693139,"Note, reaction time was related to the visual stimulus regarding the appearance of the 2×2 grid in the symbol response stage.",19,20
8236,21693139,"Primary task condition assignment (easy and difficult) • Primary task performance measures (symbol response correctness, reaction time, efficiency) • Secondary task performance measures (word accuracy, verbal response duration, efficiency) • Cognitive load labels (secondary task efficiency as numeric and nominal values) The progression in primary task and secondary task efficiency over trials, averaged across subjects, is illustrated in Figure 3 .",15,16
8237,21693139,It comprises a visual-motor primary task that required subjects to learn abstract symbol combinations and an auditory-verbal secondary task to measure the load imposed by the primary task.,14,15
8238,3059017,"Preliminaries Ranked Trees A ranked alphabet is a tuple (Σ, rk ) where Σ is an alphabet, i.e., a finite set, and rk : Σ → N assigns an arity to every symbol σ ∈ Σ. Throughout this paper we will identify (Σ, rk ) with Σ. For every k ∈ N the set Σ (k) = {σ ∈ Σ | rk (σ) = k} contains all symbols of arity k. Let H be a set and Σ a ranked alphabet.",37,38
8239,3059017,A Σ-context is a tree in T Σ ({z}) that contains exactly one occurrence of the special symbol z. The set of all Σ-contexts is denoted by C Σ .,23,24
8240,1978269,"To analyze the core, we use a grammar with three main symbols (NE x ), one for each named entity class x. Each class has an associated set of lexical symbols, which occur in a strict order (NE i  x is the ith symbol for class x).",49,50
8241,1978269,"Each NE i is adapted, and can expand to any string of words; the ability to generate multiple words from a single symbol is useful both because it can learn to group collocations like ""New York"" and because it allows the system to handle entities longer than four words.",24,25
8242,1978269,"However, we set the prior on multi-word expansions very low, to avoid degenerate solutions where most phrases are analyzed with a single symbol.",26,27
8243,1978269,We follow Charniak (2001) in assuming that two names are consistent (can be references to the same entity) if they do not have different expansions for any lexical symbol.,32,33
8244,1978269,These are added to the grammar at the named-entity class level (separated from the core by a special punctuation symbol).,22,23
8245,5998868,"As shown in Figure 1 , we first replace the held-out word with a special symbol $, and then, after consuming the remaining words in the sentence, project the h dimensional hidden layer to a p dimensional context layer, and finally predict the held out word with softmax.",17,18
8246,16662635,"ENDCHAR: The last token in the segment, i.e., the symbol that separates the current segment from the next one.",12,13
8247,16662635,"COMPONENT: The feature set contains the information about connective components: (1) whether the segment has a connective component, (2) the string of the component if it exists, (3) whether there exists a component at the beginning of the segment, (4) whether there exists a component at the end of the segment, (5) whether the segment contains only a component and the separating symbol, (6) whether the segment is before all connective components, (7) the distance to the first segment that contains a component as a binary feature, (8) whether the segment is after all connective components, and (9) the distance to the last segment that contains a connective component as a binary feature.",78,79
8248,16273410,The pipe symbol is used to separate contiguous nonoverlapping n-gram matches.,2,3
8249,17947292,"The """" symbol denotes a correct translation, """" an incorrect translation and ""?""",3,4
8250,27432725,An important additional set of features used were the distinctive articulatory (phonological) features corresponding to each IPA symbol.,19,20
8251,14236984,"Compositions of subclasses in which the XTOP N has at most one input symbol in its left-hand sides have already been studied in (Engelfriet, 1975; Baker, 1979; Maletti and Vogler, 2010) .",13,14
8252,14236984,"Suppose that we simply want to compose the rules of Figure 2 , The bottom rule does not satisfy the requirement that there is at most one input symbol in the left-hand side.",28,29
8253,14236984,"LHS(M −1 ) LHS(N ) C y 1 y 2 C z 1 VP z 2 z 3 z 4 Compatibility In the existing composition results for subclasses of XTOPs (Engelfriet, 1975; Baker, 1979; Maletti and Vogler, 2010) the XTOP N has at most one input symbol in its left-hand sides.",53,54
8254,14236984,"Namely, for each output symbol in a right-hand side of M , we can select a rule of N that can consume that output symbol.",5,6
8255,14236984,"Namely, for each output symbol in a right-hand side of M , we can select a rule of N that can consume that output symbol.",27,28
8256,14236984,"Now we have to distinguish two cases: (i) Either var(l 2 | v ) = ∅ and there is no leaf in r 2 labeled by a symbol from Γ. In this case, we have to introduce deletion and look-ahead into N .",30,31
8257,248779875,This method allows for a principled rulebased method for mapping between symbol spaces in cross-lingual speech synthesis without the use of a learned phonetic transformation network like the one described by Tu et al. (,11,12
8258,10791446,"String Kernels Consider a function φ s (x) that counts the number of times a substring s appears in x. A string kernel is defined as: k(x, x ) = s∈Σ * w s φ s (x)φ s (x ), (10) where w s is a non-negative weight for substring s and Σ * is the set of all possible strings over the symbol alphabet Σ. Usually in NLP, each word is considered a symbol, although some previous work also considered characters as symbols (Lodhi et al.,",74,75
8259,10791446,"String Kernels Consider a function φ s (x) that counts the number of times a substring s appears in x. A string kernel is defined as: k(x, x ) = s∈Σ * w s φ s (x)φ s (x ), (10) where w s is a non-negative weight for substring s and Σ * is the set of all possible strings over the symbol alphabet Σ. Usually in NLP, each word is considered a symbol, although some previous work also considered characters as symbols (Lodhi et al.,",86,87
8260,10791446,"This is done by defining a similarity matrix S, which encode symbol similarities.",12,13
8261,1061260,"The new learning objective is similar to the Bayesian maximum a posteriori objective, and makes it possible to learn topdown, which is impossible using maximum likelihood, as the initial grammar that rewrites the start symbol to all sentence pairs in the training data already maximizes the likelihood of the training data.",37,38
8262,1061260,"We need one symbol for every nonterminal, L 0 -terminal and L 1 -terminal.",3,4
8263,1061260,"To serialize the rules, we need some kind of delimiter to know where one rule starts and the next ends; we will exploit the fact that we also need to specify whether the rule is straight or inverted (unary rules are assumed to be straight), and merge these two functions into one symbol.",57,58
8264,1061260,"Assuming a uniform distribution over the symbols, each symbol will require −log 2 ( 1 N ) bits to encode (where N is the number of different symbols-the type count).",9,10
8265,1061260,"The above example has 8 symbols, meaning that each symbol requires 3 bits.",10,11
8266,1061260,The transduction grammar that fits the training data the best is the one where the start symbol rewrites to the full sentence pairs that it has to generate.,16,17
8267,1061260,It is also possible to add any number of nonterminal symbols in the layer between the start symbol and the bisentences without altering the probability of the training data.,17,18
8268,1061260,We take advantage of this by allowing for one intermediate symbol so that the start symbol conforms to the normal form and always rewrites to precisely one nonterminal symbol.,10,11
8269,1061260,We take advantage of this by allowing for one intermediate symbol so that the start symbol conforms to the normal form and always rewrites to precisely one nonterminal symbol.,15,16
8270,1061260,We take advantage of this by allowing for one intermediate symbol so that the start symbol conforms to the normal form and always rewrites to precisely one nonterminal symbol.,28,29
8271,1061260,"Our initial grammar thus looks like this: S → A, A → e 0..T 0 /f 0..V 0 , A → e 0..T 1 /f 0..V 1 , ..., A → e 0..T N /f 0..V N Where S is the start symbol, A is the nonterminal, N is the number of sentence pairs in the training corpus, T i is the length of the i th output sentence (which makes e 0..T i the i th output sentence), and V i is the length of the i th input sentence (which makes f 0..V i the i th input sentence).",57,58
8272,1061260,We will initialize a stochastic bracketing inversion transduction grammar (BITG) to rewrite it's one nonterminal symbol directly into all the sentence pairs of the training data (iteration 0).,18,19
8273,14426470,"Default top-down transducers must have only one symbol on the left-hand sides and thus cannot model some syntactic transformations (like local reordering, for example) without relying on copy and delete operations (Maletti et al.,",9,10
8274,12409708,"We need one symbol for every nonterminal, L 0 -and L 1 -terminal.",3,4
8275,12409708,"We serialize the ITG by concatenating the serialized form of all the rules, assuming that each symbol can be serialized into −lgc bits where c is the symbol's relative frequency in the serialized form of the ITG.",17,18
8276,12409708,"We serialize the ITG by concatenating the serialized form of all the rules, assuming that each symbol can be serialized into −lgc bits where c is the symbol's relative frequency in the serialized form of the ITG.",28,29
8277,12409708,"1 The long ITG simply has all the sentence pairs as biterminals: S → A A → e 0..T 0 /f 0..V 0 A → e 0..T 1 /f 0..V 1 ... A → e 0..T N /f 0..V N where S is the start symbol, A is the nonterminal, N is the number of sentence pairs, T i is the length of the i th output sentence (making e 0..T i the i th output sentence), and V i is the length of the i th input sentence (making f 0..V i the i th input sentence).",58,59
8278,12409708,"The short ITG is a token-based bracketing ITG: S → A, A → [AA] , A → ⟨AA⟩, A → e/f, A → e/ϵ, A → ϵ/f where, S is the start symbol, A is the nonterminal symbol, e is an L 0 -token, f is an L 1 -token, and ϵ is the empty sequence of tokens.",44,45
8279,12409708,"The short ITG is a token-based bracketing ITG: S → A, A → [AA] , A → ⟨AA⟩, A → e/f, A → e/ϵ, A → ϵ/f where, S is the start symbol, A is the nonterminal symbol, e is an L 0 -token, f is an L 1 -token, and ϵ is the empty sequence of tokens.",50,51
8280,7437060,"One plausible explanation is that, when convolution is performed, the beginning and end of a sentence are typically padded with a special symbol or simply zero.",24,25
8281,16807318,"We adopt the → symbol to denote the association between a cue word w and an emotion seed word t. The first step is to derive the association scores between a cue word and every seed word, e.g., Assoc. (",4,5
8282,33914228,"This is declared in the gold-standard annotation, in the configuration file: (Y 2 0 1) The first element of the list is the symbol (label) that identifies the dilemma; meaning, Y is a dilemma with two possible theories/choices, of which (both!)",29,30
8283,33914228,"As earlier, ""positive"" means segmenting at a potential segmentation point (symbol boundaries).",14,15
8284,13391714,"We use the same lexicon as for training, but to deal with unseen content words, an abstract version of each lexical entry is created where the synset ID in its λ-DRS is replaced by the UNKNOWN symbol.",40,41
8285,218974004,"Word Segmentation: Corpus and Models To build our inhouse segmentation corpus 2 we crawled popular Burmese websites and extracted sentences by split ting text using the Burmese fullstop symbol ( ။ ,",29,30
8286,218974004,"The main difference between our representation and the literature is our choice of affricates: voiceless alveolo palatals (/tC/ and /tC h /) instead of the more standard voiceless postalveolars (/Ù/ and /Ù h /), and voiced alveolopalatal /dý/ instead of voiced postalveolar /Ã/. Although not part of the consonant inventory, a special symbol /ð/, a placeless nasal, is often used to denote either a nasal consonant or simply a nasal quality on the vowel syllabic nucleus (Green, 2005; Gruber, 2011) .",59,60
8287,218974004,The grapheme and phoneme alphabets are specified in the grapheme.syms and phoneme.syms symbol table files collocated with the grammar file.,12,13
8288,21707939,"We added a ""start of word"" symbol to avoid mistakes on the first letter of the word.",8,9
8289,21707939,"This was done outside the translation scripts, by adding the ':' symbol to each word before the first letter, and removing it after translation.",14,15
8290,38854523,"Results are restricted by two special symbols which do not appear in the output, compound-L and compound-R. The compound-L symbol is used for forms that can only appear on the left side (e.g. surface form) of a compound, where compound-R is used for forms that can either appear in a compound, or end it.",26,27
8291,21694125,"As French borrowings were transcribed using French orthography, we semiautomatically added a symbol to mark these words.",13,14
8292,226239068,"Then, a non-terminal symbol serves as a place-holder for the relative clause.",6,7
8293,199555509,"We expect that dialects with lower similarity in symbol use can be easier distinguished, which was our motivation to use language modeling as part of the classification system.",8,9
8294,199555509,TF-IDF We use TF-IDF features on the symbol level for n-gram lengths from 1 to 3.,11,12
8295,9730206,The non-terminal set of a standard hierarchical grammar comprises two symbols which are shared by source and target: the initial symbol S and one generic non-terminal symbol X. The initial symbol S is the start symbol of the grammar.,23,24
8296,9730206,The non-terminal set of a standard hierarchical grammar comprises two symbols which are shared by source and target: the initial symbol S and one generic non-terminal symbol X. The initial symbol S is the start symbol of the grammar.,31,32
8297,9730206,The non-terminal set of a standard hierarchical grammar comprises two symbols which are shared by source and target: the initial symbol S and one generic non-terminal symbol X. The initial symbol S is the start symbol of the grammar.,35,36
8298,9730206,The non-terminal set of a standard hierarchical grammar comprises two symbols which are shared by source and target: the initial symbol S and one generic non-terminal symbol X. The initial symbol S is the start symbol of the grammar.,40,41
8299,9730206,"In the Moses implementation, the decoder internally adds a sentence start terminal symbol <s> and a sentence end terminal symbol </s> to the input before and after each sentence, respectively.",13,14
8300,9730206,"In the Moses implementation, the decoder internally adds a sentence start terminal symbol <s> and a sentence end terminal symbol </s> to the input before and after each sentence, respectively.",22,23
8301,199442470,"The decoder takes as input, the context vector C and the cell state h from the encoder, and computes the hidden state at time t as, s t = f dec (E y (y t-1 ), s t-1 , c t ) Subsequently, a parametric function out k returns the conditional probability using the next target symbol k. (y t = k | y < t, X) = 1 Z exp(out k (E y (y t −1), s t , c t )) Z is the normalizing constant, j exp(out j (E y (y t − 1), s t , c t )) The entire model can be trained end-to-end by minimizing the log likelihood which is defined as L = − 1 N N n=1 T y n t=1 logp(y t = y t n , y ¡t n , X n ) where N is the number of sentence pairs, and X n and y t n are the input sentence and the t-th target symbol in the n-th pair respectively.",63,64
8302,199442470,"The decoder takes as input, the context vector C and the cell state h from the encoder, and computes the hidden state at time t as, s t = f dec (E y (y t-1 ), s t-1 , c t ) Subsequently, a parametric function out k returns the conditional probability using the next target symbol k. (y t = k | y < t, X) = 1 Z exp(out k (E y (y t −1), s t , c t )) Z is the normalizing constant, j exp(out j (E y (y t − 1), s t , c t )) The entire model can be trained end-to-end by minimizing the log likelihood which is defined as L = − 1 N N n=1 T y n t=1 logp(y t = y t n , y ¡t n , X n ) where N is the number of sentence pairs, and X n and y t n are the input sentence and the t-th target symbol in the n-th pair respectively.",196,197
8303,41488455,"This functionality represents n-gram features by replacing each plain symbol at a given time with a more complex combination of ""previous"", ""current"", and ""next seen"" symbols based on configured time slots.",11,12
8304,19610674,chemical-elements: Represents 119 pairs of chemical elements with their shortcut symbol (e.g. O -Oxygen).,13,14
8305,21704449,"DR: Decomposing Relational Phrases and Introducing Additional NL Text EB may suffer from data sparseness because each relational phrase is treated as a distinct symbol, and information from the words that compose a relational phrase is completely ignored.",25,26
8306,195750811,We denote this class using '+ +' symbol combination.,9,10
8307,195750811,We denote this class using '+ -' symbol combination. •,9,10
8308,195750811,We denote this class using '-+' symbol combination. •,8,9
8309,195750811,We denote this class using '--' symbol combination.,8,9
8310,195750811,We denote this class using '+ 0' symbol combination. •,9,10
8311,195750811,We denote this class using '-0' symbol combination.,8,9
8312,229365747,"These out-of-vocabulary tokens are replaced by a special symbol UNK in the monolingual target data (see  Experiment Since the intermediate model spot UNK symbol in the inputs during inferencing, inferenced data also contains UNK symbol.",12,13
8313,229365747,"These out-of-vocabulary tokens are replaced by a special symbol UNK in the monolingual target data (see  Experiment Since the intermediate model spot UNK symbol in the inputs during inferencing, inferenced data also contains UNK symbol.",29,30
8314,229365747,"These out-of-vocabulary tokens are replaced by a special symbol UNK in the monolingual target data (see  Experiment Since the intermediate model spot UNK symbol in the inputs during inferencing, inferenced data also contains UNK symbol.",41,42
8315,229365747,Our systems are experimented by excluding sentences with UNK symbol.,9,10
8316,229365747,1-to-1 proportion of real and synthetic data with out-of-vocabulary tokens masked by UNK symbol scored best (18.76) out of all outcomes.,19,20
8317,229365747,Stacking Benefits of both the masking systems (masking OOV tokens with UNK symbol and masking similar tokens) are attained through stacking.,13,14
8318,51880509,2016 ) using a variant of BPE for word segmentation capable of encoding open vocabularies with a compact symbol vocabulary of variablelength subword units.,18,19
8319,2712119,"As usual, a context-free grammar (CFG) is represented by a 4-tuple (Σ, N, S, P ), where Σ and N are two disjoint finite sets of terminals and nonterminals, respectively, S ∈ N is the start symbol, and P is a finite set of rules, each of the form A → α, where A ∈ N and α ∈ (Σ ∪ N ) * .",50,51
8320,2712119,By grammar symbol we mean a terminal or nonterminal.,2,3
8321,2712119,"For technical reasons, a CFG is often augmented by an additional rule S † → S$, where S † / ∈ N and $ / ∈ Σ. The symbol $ acts as an end-of-sentence marker.",30,31
8322,2712119,The symbol a ∈ Σ ∪ {$} is called the follower.,1,2
8323,2712119,"It is not difficult to show that if [A → α •, a] ∈ I k , then α is of the form X j+1 • • • X k , some j, and A → α at position j + 1 is the handle of at least one derivation S ⇒ * rm X 1 • • • X k ax, some x. If furthermore a = 1 : w, where 1 : w is called the lookahead of the current configuration (X 1 • • • X k , w), then this justifies a reduce with A → α, as a step that potentially leads to a complete derivation; this is only 'potentially' because the actual remaining input w may be unlike ax, apart from the matching one-symbol lookahead.",145,146
8324,2712119,"Similarly, if [A → α • aβ, b] ∈ I k , then α = X j+1 • • • X k , some j, and if furthermore a = 1 : w, then a shift of symbol a is a justifiable step.",43,44
8325,2712119,"F k−2 in the earlier definition of E: H(ε, E, B) = G(ε, E, B) H(αA, E, B) = E ,π=F →AE H(α, E , B) • U(E , F ) • p(π) + G(αA, E, B) E 0 F 1 A 1 E 1 E m−1 F m A m E m F E F m+1 A m+1 E m+1 E k−2 F k−1 A k−1 A k B B a Figure 1: Right-most derivation leading to F k−1 → A k−1 A k in viable prefix A 1 • • • A k with lookahead a. Finally, we can express E in terms of these recursive functions, considering the more general case of any rule π = F → β: E(αβ, a, F → β) = E,B H(α, E, B) • U(E, F ) • p(π) • L(B, a) E(α, a, F → β) = 0 if ¬∃ γ α = γβ where: L(B, a) = π=B →a V(B, B ) • p(π) The expected number of times the handle is to be found to the right of α, with the stack being α and the lookahead symbol being a, is: E(α, a, shift) = B F(α, B) • L(B, a) The expected number of times we see a stack α with lookahead a is: E(α, a) = E(α, a, shift) + π E(α, a, π) The probability that a reduce with rule π is the correct action when the stack is α and the lookahead is a is naturally E(α, a, π)/E(α, a) and the probability that a shift is the correct action is E(α, a, shift)/E(α, a).",237,238
8326,2712119,"The definitions of G and H each involve four nonterminals excluding the stack symbol A, so that the Therefore we have implemented an alternative that has a time complexity that is only quadratic in the size of the grammar, at the expense of a quadratic complexity in the length of the input string, as detailed in Appendix A. This is still better in practice if the number of nonterminals is much greater than the length of the input string, as in the case of the grammars we investigated.",13,14
8327,2712119,"Then follows a straightforward process of 'backtracing', which builds the derivation that led to the computed probability associated with the start symbol.",24,25
8328,2712119,"For example, for each distinct lookahead symbol a, there is a (sparse) matrix containing the value of G (F, E, a) at a row and a column uniquely identified by F and E, respectively.",7,8
8329,235097640,Recurrent models are typically used for this task which computes along the symbol positions of the input and output sequences.,12,13
8330,52179777,But it also started to produce ¡end-of-the-word¿ symbol even if s was in the middle of the word.,15,16
8331,21697256,"Multi-word tokens show morphemes linked by a ""+"" 4 http://sac.al-osaimy.com/guidelines symbol.",14,15
8332,21697256,"The original word in the main screen will be replaced by two morphemes linked by ""+"" symbol.",18,19
8333,18192391,This phone set is based on the ARPAbet 2 symbol set developed for speech recognition uses. -,9,10
8334,222310307,"One reason is that NMT systems have a fixedsize vocabulary, typically 10k-100k words; words outside this vocabulary are represented using a special symbol like UNK.",24,25
8335,21707219,"They are represented as binary indicators analogously to the surrounding words of IMS (Zhong and Ng, 2010) , but, differently to them, they are a less variable representation of the context, since synonyms are represented with a single symbol. • """,44,45
8336,250391084,can be considered a symbol of sarcasm and express some contradictory and criticized attitudes. (,4,5
8337,3244701,We limit language models to a fixed vocabulary and map out-of-vocabulary (OOV) tokens to a unique symbol to overcome sparsity and better control the OOV rates among various corpora.,22,23
8338,227165231,"The ability to omit the purpose event cuts across the PURPOSE subtypes in table 2, where the ○ symbol stands for the implicit purpose event.",19,20
8339,3639525,"Part of the reason for this shift from a more straightforward symbol (such as the double helix, which, while still complex, indicates a more orderly, and potentially predictable machinery) to the hairball, Lander writes, is the rise of the computational tools that make it possible to generate vast amounts of ""big data,"" as well as the realization that thorny problems generally involve the intersections between data drawn from multiple, interacting systems.",11,12
8340,21715487,"For the structural case underlying the genitive of negation we use a mnemotechnical symbol np(accgen), since this type of phrase is realised in the accusative or in the genitive, depending on negation.",13,14
8341,14317029,"Working in a custom, browserbased interface, annotators were to tag each relevant token with a supersense category by selecting the token and typing a tag symbol.",27,28
8342,14317029,symbol.,0,1
8343,5586146,"2014) parameterize the per-symbol conditional probability as: P y i |y [0:i−1] ; x = DecoderRNN ((h i ) y i−1 , h i−1 , Attention (h i−1 , x)) (1) for 1 ≤ i ≤ N , where DecoderRNN() is a recurrent neural network that map the sequence of decoder symbols into fixed-length vectors, and Attention() is a function that yields a fixed-size vector summary of the encoder symbols x (the 'focus') most relevant to predicting y i , given the previous recurrent state of the network h i−1 (the 'context').",6,7
8344,5586146,"This yields a total of B ×D beams, each with one additional symbol.",13,14
8345,4981128,"Our parser allows multiple attachments to the ""wall"" symbol, so that multi-rooted analyses can be predicted.",10,11
8346,4981128,"Note that all of the challenges in §2 are handled easily by GFL notation: ""retweet"" information, punctuation, and a URL are not selected by virtue of their exclusion from the GFL expression; in (2) went down is annotated as a MWE using GFL's square bracket notation; in (3) the tokens are grouped into two utterances whose roots are marked by the ** symbol.",76,77
8347,4981128,"c) Attaching the roots of the utterance in tweets to the ""wall"" symbol ( §2.3).",15,16
8348,218974255,Was simplified to give: The symbol is an indispensable ingredient in Greek cuisine .,6,7
8349,896190,"During this phase, the backtick symbol (`) was therefore reserved for MWEs (such as light verb constructions) that contain a noun but should not receive a noun supersense.",6,7
8350,53082357,"Let h i be the hidden state of the source symbol at position i, obtained by concatenating the forward and backward encoder RNN hidden states, h i = [ − → h i ; ← − h i ].",10,11
8351,53082357,"cGRU att takes as input the previous hidden state s j−1 , the source annotations C = h 1 , ..., h T x , and the previously decoded symbol y j−1 in order to update its hidden state s j , which is used to decode symbol y j at position j: s j = cGRU att (s j−1 , y j−1 ,C) (2) cGRU att consists of three components.",30,31
8352,53082357,"cGRU att takes as input the previous hidden state s j−1 , the source annotations C = h 1 , ..., h T x , and the previously decoded symbol y j−1 in order to update its hidden state s j , which is used to decode symbol y j at position j: s j = cGRU att (s j−1 , y j−1 ,C) (2) cGRU att consists of three components.",48,49
8353,53082357,The first combines the previously decoded symbol y j−1 and the previous hidden state s j−1 to generate an intermediate representation s j .,6,7
8354,53082357,"The attention mechanism, AT T , inputs the entire context set C along with intermediate hidden state s j in order to compute the context vector c j : c j = AT T (C, s j ) = T x ∑ i α i j h i (3) α i j = exp(e i j ) ∑ T x k=1 exp(e k j ) (4) e i j = f (s j , h i ) (5) Where α i j is the normalized alignment weight between the source symbol at position i and the target symbol at position j, and f is a feedfoward neural network.",100,101
8355,53082357,"The attention mechanism, AT T , inputs the entire context set C along with intermediate hidden state s j in order to compute the context vector c j : c j = AT T (C, s j ) = T x ∑ i α i j h i (3) α i j = exp(e i j ) ∑ T x k=1 exp(e k j ) (4) e i j = f (s j , h i ) (5) Where α i j is the normalized alignment weight between the source symbol at position i and the target symbol at position j, and f is a feedfoward neural network.",107,108
8356,199535159,"We lowercased all tokens, replaced all numbers by a special symbol and all tokens with less than 5 occurrences with a special token for rare words, resulting in a vocabulary size of 11539.",11,12
8357,44062236,"The encoder maps an input sequence of symbol representations x to a sequence of distributed representations z = (z 1 , z 2 , . . . ,",7,8
8358,8788059,"First, we focus on the projection function Π U N K (•) which switches between preserving the input and output label at the UNK symbol to produce the combined translation t comb (Eq.",27,28
8359,34610629,"It computes a weighted average of all the output from the encoder based on the current decoded symbol, which is why it is also named ""Global Attention.""",17,18
8360,21691538,"In our case, the grammar contains only thirteen rules: • Two rules for applying a complement to the left or to the right of a head: head comp and comp head • Two rules for applying a modifier to the left or to the right of a head: head mod and mod head • Two rules for applying a specifier to the left or to the right of a head: head spec and spec head • One special unary rule for representing the Spanish null subjects, which could be seen as a special case of an empty lexical entry: empty spr • Two rules for binarizing chains of coordinations: coord left and coord right • One rule for applying a clitic to the left of a head: clitic head • One rule for joining a noun with a relative clause that modifies it: head rel • Two rules for applying a punctuation symbol to the left or to the right of a head: head punct and punct head For example, consider the schematic definition of the specifier rule spec head as shown in figure 2 , that applies a specifier to the left of a head.",162,163
8361,13963988,"For prefixes and infixes, we do not add a stop symbol at the end, and use null symbols, which denote unavailable context, for padding to the right.",11,12
8362,15600925,2015) and replace out-of-vocabulary words with a special UNK symbol.,14,15
8363,16126936,"Subword Tags In our experiments, we operate on the level of subwords to achieve open-vocabulary translation with a fixed symbol vocabulary, using a segmentation based on byte-pair encoding (BPE) (Sennrich et al.,",22,23
8364,16126936,"We propose an annotation of subword structure similar to popular IOB format for chunking and named entity recognition, marking if a symbol in the text forms the beginning (B), inside (I), or end (E) of a word.",22,23
8365,16126936,A separate tag (O) is used if a symbol corresponds to the full word.,10,11
8366,16126936,"We treat the concatenation of all morphological features of a word, using a special symbol for underspecified features, as a string, and treat each such string as a separate feature value.",15,16
8367,16126936,"For lemmas, we choose the same vocabulary size as for words, replacing rare lemmas with a special UNK symbol.",20,21
8368,1114678,"Firstly, we initialize the symbol vocabulary with the character vocabulary, and represent each word as a sequence of characters, plus a special end-ofword symbol '•', which allows us to restore the original tokenization after translation.",5,6
8369,1114678,"Firstly, we initialize the symbol vocabulary with the character vocabulary, and represent each word as a sequence of characters, plus a special end-ofword symbol '•', which allows us to restore the original tokenization after translation.",28,29
8370,1114678,"We iteratively count all symbol pairs and replace each occurrence of the most frequent pair ('A', 'B') with a new symbol 'AB'.",4,5
8371,1114678,"We iteratively count all symbol pairs and replace each occurrence of the most frequent pair ('A', 'B') with a new symbol 'AB'.",27,28
8372,1114678,Each merge operation produces a new symbol which represents a character n-gram.,6,7
8373,1114678,"Frequent character n-grams (or whole words) are eventually merged into a single symbol, thus BPE requires no shortlist.",16,17
8374,1114678,"The final symbol vocabulary size is equal to the size of the initial vocabulary, plus the number of merge operations -the latter is the only hyperparameter of the algorithm.",2,3
8375,1114678,"A minimal Python implementation is shown in Al- The main difference to other compression algorithms, such as Huffman encoding, which have been proposed to produce a variable-length encoding of words for NMT (Chitnis and DeNero, 2015) , is that our symbol sequences are still interpretable as subword units, and that the network can generalize to translate and produce new words (unseen at training time) on the basis of these subword units.",47,48
8376,1114678,"This is applicable to any word, and allows for open-vocabulary networks with fixed symbol vocabularies.",16,17
8377,1114678,"In practice, we did not include infrequent subword units in the NMT network vocabulary, since there is noise in the subword symbol sets, e.g. because of characters from foreign alphabets.",23,24
8378,1114678,"We introduce a variant of byte pair encoding for word segmentation, which is capable of encoding open vocabularies with a compact symbol vocabulary of variable-length subword units.",22,23
8379,1254968,The words that appeared less than twice in the training data were replaced with the special symbol UNK.,16,17
8380,53235370,"s i = S(h J 1 , ē i−1 0 ; θ dec ) (3) The target sentence is augmented with a sentence start symbol e 0 , which is an identifier for the output language.",26,27
8381,3288023,"with a subphrase, the leftmost word of this subphrase is considered for p flex_right , or x = j + 1 − n, with n being the length of the subphrase that the rule ends with, or n = 0 if the rule ends with a terminal symbol.",50,51
8382,52143574,"Finally, position information is discarded from the edits, leaving only the substrings, separated by a boundary symbol.",19,20
8383,53223259,"The decoder takes as input, the context vector C and the cell state h from the encoder, and computes the hidden state at time t as, s t = f dec (E y (y t-1 ), s t-1 , c t ) Subsequently, a parametric function out k returns the conditional probability using the next target symbol k. (y t = k | y < t, X) = 1 Z exp(out k (E y (y t −1), s t , c t )) Z is the normalizing constant, j exp(out j (E y (y t − 1), s t , c t )) The entire model can be trained end-to-end by minimizing the log likelihood which is defined as L = − 1 N N n=1 T y n t=1 logp(y t = y t n , y ¡t n , X n ) where N is the number of sentence pairs, and X n and y t n are the input sentence and the t-th target symbol in the n-th pair respectively.",63,64
8384,53223259,"The decoder takes as input, the context vector C and the cell state h from the encoder, and computes the hidden state at time t as, s t = f dec (E y (y t-1 ), s t-1 , c t ) Subsequently, a parametric function out k returns the conditional probability using the next target symbol k. (y t = k | y < t, X) = 1 Z exp(out k (E y (y t −1), s t , c t )) Z is the normalizing constant, j exp(out j (E y (y t − 1), s t , c t )) The entire model can be trained end-to-end by minimizing the log likelihood which is defined as L = − 1 N N n=1 T y n t=1 logp(y t = y t n , y ¡t n , X n ) where N is the number of sentence pairs, and X n and y t n are the input sentence and the t-th target symbol in the n-th pair respectively.",196,197
8385,2024363,"A number of studies have examined how the slur ""nigger"" has been appropriated by African Americans as a way of actively rejecting the connotations it carries, e.g. for comedic purposes, a status symbol, a shorthand term expressing familiarity among friends, or even forgetting what the term ever denoted in the first place (Anderson and Lepore, 2013) .",36,37
8386,9817322,"We use all sentences in the training set, we truncate all sentences longer than 35 words and pad all sentences shorter than 35 words with a special symbol so all sentences are the same size.",28,29
8387,9817322,"We use all sentences in the training set, we truncate all sentences longer than 35 words and pad all sentences shorter than 35 words with a special symbol so all sentences are the same length.",28,29
8388,15241522,"MIXED tokens are also marked with the code-switching boundary, represented with the symbol ' §'.",15,16
8389,15241522,"As introduced earlier, the symbol ' §' indicates the code switching boundary within a word.",5,6
8390,15241522,We mark inflectional group boundaries with the symbol '•' in the examples. (,7,8
8391,15241522,"Special word and symbol sequences, such as mentions, hashtags and URLs, are also tagged using the UD POS tag set.",3,4
8392,13521414,"We segmented the morphologically-analyzed Turkish sentences at every feature boundary, denoted by the ( ) symbol.",18,19
8393,6247789,"Compound modifiers, identified by the WB or wB boundary type, are marked with a reserved symbol '@' at the right edge of the morph.",17,18
8394,12903236,"Scores labeled by the * symbol differ from the official results of the shared task (Guillou et al.,",5,6
8395,21694402,"Furthermore, possible synonyms may be enclosed in a hashtag, thus it would be impossible to compute the similarity between hashtags and the emoji description vector without additional preprocessing, since hashtags may consist in a combination of words plus the hash symbol attached at the front (e.g. ""#autumninjapan"").",43,44
8396,218684951,"the symbol grounding problem (Harnad, 1990) ).",1,2
8397,248780359,"The input x fed into the encoder is formed in a way similar to LASAGNE, which is composed of the previous question, the answer to the previous question, and the current question separated by a symbol ""[SEP]"".",38,39
8398,21702919,"An instance dropping the intervocalic phoneme /d/ in a final syllable can be found in turn 30, marked with a + symbol.",22,23
8399,21702919,"Lengthening examples appear in turns 30 and 40, marked with an = symbol.",13,14
8400,241583493,"Prior work towards the production of parallel meaning banks focused on the alignment of informative translations-translations where more details are included in the target translation than the source text-through an inclusion relation symbol (Bos, 2014) .",36,37
8401,34834757,"A candidate sequence terminates when RNN predicts EOS, the special symbol indicating the end of a sequence.",11,12
8402,201626326,"The encoder maps an input sequence of symbol representations s i = (x 1 ,...,x n ) to a sequence of continuous representations (z 1 ,...,z n ).",7,8
8403,247476207,"Yorùbá has three distinctive tones -high, mid and low tones -but only represents the high tone with the acute symbol and the low tone with the grave symbol in its orthography.",20,21
8404,247476207,"Yorùbá has three distinctive tones -high, mid and low tones -but only represents the high tone with the acute symbol and the low tone with the grave symbol in its orthography.",28,29
8405,48358519,"As the first sentence of a document does not have before-x, we used the stop symbol to form the sentence s = (eos, eos, eos) as the before-x. We used a simple pre-training strategy to train our N M T ISG model: training the regular attention-based NMT model using our implementation of RNNSearch, and then using its parameters to initialize the parameters of the proposed model, except for those related to the operations of the inter-sentence gate.",18,19
8406,24543974,"2011) is then used to tokenize tweets followed by removing the hashtag symbol ""#"" from its attached keywords or topics.",13,14
8407,201641505,The Levenshtein distance in its purest form consists of three basic operations: • Substitution: the act of switching one symbol with another • Deletion: the removal of a symbol • Insertion: the addition of a symbol All of the basic operations are defined as having an uniform cost of one.,21,22
8408,201641505,The Levenshtein distance in its purest form consists of three basic operations: • Substitution: the act of switching one symbol with another • Deletion: the removal of a symbol • Insertion: the addition of a symbol All of the basic operations are defined as having an uniform cost of one.,31,32
8409,201641505,The Levenshtein distance in its purest form consists of three basic operations: • Substitution: the act of switching one symbol with another • Deletion: the removal of a symbol • Insertion: the addition of a symbol All of the basic operations are defined as having an uniform cost of one.,39,40
8410,201641505,"To not penalise matching symbols with substitutions, substitutions can be defined via the Kroneker delta: 1 − δ(c n , r m ) with c n and r m standing for the symbol at position m ∈ {1, 2 . . .",34,35
8411,201641505,"Naturally, in the case where every symbol is wrong and the normalising term is the shorter one of the candidate and the reference, the resulting score may significantly exceed 1.0.",7,8
8412,201641505,"Although EED utilises the same movement technique as CDER, there are a few notable differences: • Edit distance is performed on the character level; • Jumps are performed only upon reaching a blank in the reference; • An additional penalty for multiple matching of the same symbol (coverage cost) is applied Results EED is implemented in C++ and imported in python via a wrapper.",50,51
8413,3331110,"In the sentencepiece model, the 'space' in the original input is also treated as a character and is replaced with the special symbol ' '.",25,26
8414,18497353,"The forward network reads the input sentence in a forward direction: − → z t = − → φ (e x (x t ), − → z t−1 ) , where e x (x t ) is a continuous embedding of the t-th input symbol, and φ is a recurrent activation function.",51,52
8415,18497353,"For each conditional term in the summation, the decoder RNN updates its hidden state by h t = φ(e y (y t −1 ), h t −1 , c t ), (1) where e y is the continuous embedding of a target symbol.",48,49
8416,18497353,This procedure can be understood as computing the alignment probability between the t -th target symbol and t-th source symbol.,15,16
8417,18497353,This procedure can be understood as computing the alignment probability between the t -th target symbol and t-th source symbol.,21,22
8418,18497353,"The hidden state h t , together with the previous target symbol y t −1 and the context vector c t , is fed into a feedforward neural network to result in the conditional distribution: p(y t | y <t , X) ∝ e f y t out (ey(y t −1 ),h t ,c t ) . (",11,12
8419,15309653,"Probabilistic Context-Free Grammars A PCFG is defined by a 5-tuple G C =(N , T , P , S, D) where N is a set of non-terminal symbols, T a set of terminal symbols, and S is the start symbol.",50,51
8420,15309653,"An LCFRS G L =(N , T , P , S, V ) where N is the set of non-terminals, T the terminals and S is the start symbol.",33,34
8421,15309653,Each post p in the corpus is a terminal symbol (i.e. a terminal symbol is a bag of words).,9,10
8422,15309653,Each post p in the corpus is a terminal symbol (i.e. a terminal symbol is a bag of words).,14,15
8423,17829732,"Supporters argued that the Ten Commandments are a recognized symbol of law, with both secular and religious functions.",9,10
8424,18871276,"The ATTRIB score is the proportion of sentences in the article that have a quote symbol, or the words 'said' and 'says'.",15,16
8425,52012024,We replaced the occurrences of numbers and proper nouns with their abstraction symbol NUM and PROP.,12,13
8426,24366304,Edits Changes made during an editing session are also marked by a specific symbol.,13,14
8427,21722663,"In Jacobson (2001) , each morphophonological process is systematically documented and assigned a unique symbol, such as + which designates straightforward affixation of postbase to base.",16,17
8428,21722663,"Modification of base-final te, which is represented by the symbol @, occurs before base-final e is dropped via ∼ f .",12,13
8429,21722663,"As a base cannot end in a consonant and an -e, the only necessary symbol ordering in the postbase @∼ f -raghkiigh is between @ and ∼ f .",16,17
8430,21722663,"e-Hopping will not occur if doing so would result in a three-consonant cluster within the word or a two-consonant cluster at the beginning (Jacobson, 2001) Indicates some degree of modification to base-final te, the degree of which is unpredictable and dependent on the postbase + Adds ending as presented This symbol is excluded from the Yupik analyzer and is implicitly assumed when no other morphophonological symbols are present.",62,63
8431,21722663,"Rules take the form A → B || Γ_∆, where A is rewritten as B in the context of Γ and ∆. In the Yupik analyzer, A typically refers to the substring that is rewritten as B in the morphophonological context determined by Γ and ∆, where ∆ may refer to the morphophonological symbol.",56,57
8432,21722663,"Each character string used in foma was further categorized as either an alphabetic character (Alph) or a morphophonological symbol (MPSymbols), and a morpheme boundary marker ^was introduced at every juncture of an alphabetic character and a morphophonological symbol.",20,21
8433,21722663,"Each character string used in foma was further categorized as either an alphabetic character (Alph) or a morphophonological symbol (MPSymbols), and a morpheme boundary marker ^was introduced at every juncture of an alphabetic character and a morphophonological symbol.",42,43
8434,21722663,"From the underlying string aghnagh^∼:(ng)u^∼ f (g/t)uq, the first postbase applies in its entirety which allows the leftmost morpheme boundary marker to delete, but retains the morphophonological symbol (g/t) until application of the second and final to.open-TRANS.PRS.OPT.2SG.3DU 'Open the window(s) if you (take a) walk' (Jacobson, 2001) (5) Tukuqa neghsameng gaaghaquq tukuqa neghsameng gaaghaquq tukugh read [N][Abs][1SgPoss][SgPosd] neghsagh[N][Abl_Mod][Unpd][Sg] gaagh∼(g1)aqe[V→V][V][Intr][Ind][3Sg] tukugh-∼-ke neghsagh-∼ f -wmeng gaagh-∼(g1)aqe-∼ f (g/t)u-q host-1SG.POSS seal-UNPD.ABL_MOD.SG to.cook-to.be.",32,33
8435,21685731,"a digit ('1980'), or a symbol ('£', '%').",10,11
8436,18635274,"However, when we analyzed only those capitalized SL words that were not part of the vocabulary of the NMT system (and hence they were likely to produce an UNK symbol), the accuracy increased: Table 6 : Results of the evaluation on newstest2016 of our NMT submission (in bold), the simpler strategy for translating unknown words by Jean et al. (",31,32
8437,21711688,"Query Formulation In order to form the query, we remove all the stop word, punctuation symbol from the question.",17,18
8438,17454434,"The construction of the o-form can be expressed ba the structure: END) The stick mark ""|,, is usually used as a symbol of the meta-language.",28,29
8439,17454434,Here it is used as a symbol of the defined language.,6,7
8440,17454434,Integer placed directly after the symbol KONJSYN indicates the number of conjlmction factors.,5,6
8441,6494031,"Our PRNSFM views semantic frames as a sequence: a predicate, followed by the arguments in their textual order, and terminated by a special EOS symbol.",27,28
8442,14919987,"BPE, originally devised as a compression algorithm (Gage, 1994) , is adapted to word segmentation as follows: First, each word in the training vocabulary is represented as a sequence of characters, plus an end-of-word symbol.",45,46
8443,14919987,All characters are added to the symbol vocabulary.,6,7
8444,14919987,"Then, the most frequent symbol pair is identified, and all its occurrences are merged, producing a new symbol that is added to the vocabulary.",5,6
8445,14919987,"Then, the most frequent symbol pair is identified, and all its occurrences are merged, producing a new symbol that is added to the vocabulary.",20,21
8446,14919987,"BPE starts from a character-level segmentation, but as we increase the number of merge operations, it becomes more and more different from a pure character-level model in that frequent character sequences, and even full words, are encoded as a single symbol.",48,49
8447,6199004,This alignment probability measures how relevant i-th context vector of source sentence is in deciding the current symbol in translation.,19,20
8448,6199004,"Thus, in our model, z k 0 is set to a zero vector as follows: Z k 0 = 0 (7) In Figure 2 , the hidden state of ""<start>"" symbol is set to zero vector when y 1 does not belong to parallel part of the sentence pair.",39,40
8449,6199004,"In our baseline model, we limit the vocabulary of both source and target languages to 30K most frequent words, and other words are replaced by a special symbol ""UNK"".",30,31
8450,5679146,"w N }, the WON first transforms every single symbol w c into a low-dimensional continuous vector, i.e., e c = F (w c ) ∈ R D , (1) where F is a learnable function.",10,11
8451,5679146,"We first independently embeds each symbol to dense vectors (e 1 , e 2 , e 3 , e 4 ).",5,6
8452,5679146,"10) In contrast, the WON treats each symbol independently (Eq.",9,10
8453,5679146,"However, in the word ordering task, Equation 10 suffers from the data sparseness problem, as each input w i forms a high-dimensional discrete symbol.",28,29
8454,5679146,"We built a vocabulary of the most frequent 300K words and replaced out-of-vocabulary tokens with a special "" UNK "" symbol.",25,26
8455,12326376,"We segmented the morphologically-analyzed Turkish sentences at every feature boundary, denoted by the ( ) symbol.",18,19
8456,226283768,"For MLM, given the input token sequence, a portion of tokens are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from the corrupted version.",18,19
8457,39731693,"One reason is that a symbol is often assigned to an entity as soon as it is introduced in a problem, such as in: ""Let C be the circle that is ..."" and the entity is thereafter referred to by the symbol but not by a definite noun.",5,6
8458,39731693,"One reason is that a symbol is often assigned to an entity as soon as it is introduced in a problem, such as in: ""Let C be the circle that is ..."" and the entity is thereafter referred to by the symbol but not by a definite noun.",45,46
8459,29931698,"For example, the word cat is hashed as the bag of letter trigram #-ca, c-a-t, a-t-#, where # is a boundary symbol (Liu et al.,",33,34
8460,21712775,Writing system What is assigned to each symbol Case study on a language both morphologically and graphonomically rich We explore the possibility that the richness of languages used in language agnostic character-based approaches is restricted.,7,8
8461,13495961,"The forward network reads the input sentence in a forward direction: − → z t = − → φ (e x (x t ), − → z t−1 ), where e x (x t ) is a continuous embedding of the t-th input symbol, and φ is a recurrent activation function.",51,52
8462,13495961,"For each conditional term in the summation, the decoder RNN updates its hidden state by h t = φ(e y (y t −1 ), h t −1 , c t ), (1) where e y is the continuous embedding of a target symbol.",48,49
8463,13495961,procedure can be understood as computing the alignment probability between the t -th target symbol and t-th source symbol.,14,15
8464,13495961,procedure can be understood as computing the alignment probability between the t -th target symbol and t-th source symbol.,20,21
8465,13495961,"The hidden state h t , together with the previous target symbol y t −1 and the context vector c t , is fed into a feedforward neural network to result in the conditional distribution: p(y t | y <t , X) ∝ e f y t out (ey(y t −1 ),h t ,c t ) . (",11,12
8466,13495961,"For the description below, we use y t −1 and c t for the previous target symbol and the context vector (see Eq. (",17,18
8467,6925721,"For example, Figure 1 shows a dependency tree for the sentence, ""Economic news had  little effect on financial markets"", with the sentences root-symbol as its root.",30,31
8468,6925721,"x n }, where x 0 is the root-symbol.",11,12
8469,6925721,"We define a complete graph G on n + 1 nodes (including the root-symbol x 0 ), where each node corresponds to a word in x and each edge corresponds to a dependency arc between two words.",16,17
8470,15055976,"The symbol C represents a consonant, symbol CC represents a conjunct and the letters in lower case represent matra (vowel). '",1,2
8471,15055976,"The symbol C represents a consonant, symbol CC represents a conjunct and the letters in lower case represent matra (vowel). '",7,8
8472,3131971,"In sequence alignment, similarity function S(i, j) tells us how likely it is that symbol i (in our case, a paragraph label) will be substituted with another symbol j. While we expect that in an alignment between high-scoring essays, an Introduction paragraph is most likely to be aligned with another Introduction paragraph, how much worse should the alignment score be if an Introduction paragraph needs to be mismatched with a Rebuttal paragraph or replaced with an indel?",17,18
8473,3131971,"In sequence alignment, similarity function S(i, j) tells us how likely it is that symbol i (in our case, a paragraph label) will be substituted with another symbol j. While we expect that in an alignment between high-scoring essays, an Introduction paragraph is most likely to be aligned with another Introduction paragraph, how much worse should the alignment score be if an Introduction paragraph needs to be mismatched with a Rebuttal paragraph or replaced with an indel?",33,34
8474,220058142,"Removing extra spaces around the dot symbol of float numbers • Removing unnecessary spaces Because both Chinese language and Japanese language don't use spaces to mark borders of words, we applied segmentation on each side (A branched experiment will be presented later in this report).",6,7
8475,220058142,Our results are styled in bold and the contrastive one is marked with an additional asterisk symbol * .,16,17
8476,21711795,Other words are replaced by a special symbol UNK.,7,8
8477,3302099,"Then, we could obtain a 1.16 million parallel Korean-Arabic corpus after filtering the <unk> symbol from a 2.5 million corpus.",19,20
8478,3302099,"The filtering process consisted of length filtering, deduplication of sentences, and removal of sentences containing the <unk> symbol.",21,22
8479,21696575,"The symbol "" "" denotes a space added after tokenization.",1,2
8480,21697855,"Each training example was split into spaces, with a special source language symbol prepended, as shown in Table 1 .",13,14
8481,21697855,No target language symbol was used because the target language was only English.,3,4
8482,250279831,"p(w) according to the chain rule as follows: p(w) = p(w 1 ) • |w| i=2 p(w i | w <i ) (1) Each ""local"" distribution p(w i | w <i ) is defined over Σ. Traditionally, language models include an EOS symbol; this means they produce a distribution over (an infinite number of) finite strings.",52,53
8483,250279831,"In contrast to most language models, GPT-2 dispenses with the EOS symbol and therefore yields a distribution over infinite strings.",12,13
8484,236688506,If a note corresponds to n > 1 syllables we divide it to n equal duration notes and if a syllable spans n > 1 notes we match it to the first one and assign the next n − 1 notes to a special symbol.,44,45
8485,218974159,Each symbol is associated with a code of exactly four characters.,1,2
8486,218974159,"The correct association between both the symbol and its corresponding code is performed by the system in step ""Convert HamNoSys symbols to their Unicode codes"" (Figure 2 ).",6,7
8487,218674272,"Examples of such noise include special Unicode characters unrelated to the text, e.g., musical note symbol to let hearing impaired viewers know that a song is playing, joined words (either typos or OCR artefacts) such ""Amanonce"" instead of ""A man once,"" and pairs in which the Hebrew part is actually found in English, not translated.",17,18
8488,218974389,"When the raw token is a symbol (e.g., the Greek letter β), this field will contain a text version of the token (here, ""beta"").",6,7
8489,11642690,"An outgoing arc from that state with symbol y has a weight that corresponds to the (negative log of the) conditional probability P Hiero (y t = y|y t−1 1 , x). (",7,8
8490,11642690,"NMT-Hiero Decoding As above, suppose a path to a state in the WFSA accepts a Hiero translation prefix y t−1 1 , and let y t be a symbol on an outgoing arc from that state.",31,32
8491,11642690,"Alternatively, we can think of the algorithm as NMT decoding with revised posterior probabilities: instead of selecting the most likely symbol y t according the NMT model, we adjust the NMT posterior with the Hiero posterior scores and delete NMT entries that are not allowed by the lattice.",22,23
8492,11642690,"This may result in NMT choosing a different symbol, which is then fed back to the neural network for the next decoding step.",8,9
8493,218974173,"Similarly, 23 literal interpretations of false note can be explained by the assumption that the basic meaning of the note is related directly to sound, not to graphical symbol.",30,31
8494,218977391,Molybdenum Molybdenum is a chemical element with the symbol Mo and atomic number 42.,8,9
8495,218974515,A feature space word is a word with a ' ' symbol added to the beginning and end of the word.,11,12
8496,218974068,"The Inuktitut text is in syllabics, a writing system where each consonant-vowel pair is represented by single symbol.",20,21
8497,218974068,We had to provide special handling for: • Characters that can be present in borrowed words (b and H) are mapped to a matching IPA symbol. •,28,29
8498,220047296,"We propose a BERT-based classifier, which predicts the relation of two arguments separated by the symbol [SEP], with additional dense layers as the output.",18,19
8499,182952433,"We decide, whether there should be a stress on each given symbol (or syllable) or not.",12,13
8500,12618880,"s n−1 of length n. The grammar G has a set of symbols denoted by capital letters, including a distinguished goal (root) symbol G. Without loss of generality, we assume Chomsky normal form: each non-terminal rule r in G has the form r = A → B C with weight w r .",25,26
8501,5660940,"We obtain this by giving our model data from both sets at the same time, and marking each example with a task-specific input symbol, cf.",26,27
8502,5660940,"For the input of the encoder, we adapt the format by Kann and Schütze (2016) , but modify it to be able to handle unlabeled data: Given the set of morphological subtags M each target tag is composed of (e.g., the tag 1SgPresInd contains the subtags 1, Sg, Pres and Ind), and the alphabet Σ of the language of application, our input is of the form B[A/M * ]Σ * E, i.e., it consists of either a sequence of subtags or the symbol A signaling that the input is not annotated and should be autoencoded, and (in both cases) the character sequence of the input word.",98,99
8503,17366870,"Formally, let T be a constituency tree consisting of context-free rules of the form r = P → C 1 • • • C d , where P is the parent symbol of rule r and C d 1 = C 1 . . .",34,35
8504,17366870,Parent Annotation We annotate all VPs with their parent symbol.,9,10
8505,17366870,"Unary Deletion We remove all unary productions except the root and preterminal productions, keeping only the bottom-most symbol.",20,21
8506,236460195,<NT> is a special placeholder symbol that represents a substitution point when converting the linearized sequence back to a tree.,7,8
8507,17279080,"We parsed the cTAKES output to extract the following features: a binary feature indicating if the token is the first token in the sentence, the token lemma and normalization forms, its type of token (word/punctuation/symbol/number/contraction) and if it was tagged as any of the following semantic types by cTAKES: medical, procedure, anatomical site, sign/symptom, disease/disorder, and concept.",42,43
8508,218974202,"The steps undertaken are detailed in the following; • Filtering: replacement of URL links (e.g.http://example.com) by the term ""link"", Twitter user names (e.g. @pseudo -with symbol @ indicating a user name) by the term ""person"". •",33,34
8509,218974089,"oznaczone znakiem "" + "" marked with symbol "" + "" interp interp interp PUNCT SYM PUNCT adjunct punct punct Figure 1 : The snippet of a predicted tree of the sentence Tendencje jang zostały oznaczone znakiem ""+"" ('Yang tendencies have been marked with the symbol ""+""').",7,8
8510,218974089,"oznaczone znakiem "" + "" marked with symbol "" + "" interp interp interp PUNCT SYM PUNCT adjunct punct punct Figure 1 : The snippet of a predicted tree of the sentence Tendencje jang zostały oznaczone znakiem ""+"" ('Yang tendencies have been marked with the symbol ""+""').",49,50
8511,972587,"s n of length n. The grammar G has a set of symbols Σ, including a distinguished goal (root) symbol G. Without loss of generality, we assume Chomsky normal form, so each non-terminal rule r in G has the form r = A → B C with weight w r (the negative log-probability of the rule).",22,23
8512,972587,"The items in this space are called inside derivation items, or derivation items where clear, and are of the form D(T A , i, j), specifying an entire tree T A rooted at symbol A and spanning s i+1 . . .",38,39
8513,972587,"The goals in this space, representing root parses, are any derivation items rooted at symbol G that span the entire input.",16,17
8514,972587,"Let G 1 be the input grammar G, and let G 2 , the target grammar of our HA * instance, be the grammar of derivations in G formed by expanding each symbol A in G to all possible inside derivations T A rooted at A. The rules in G 2 have the form T A → T B T C with weight given by the weight of the rule A → B C. By construction, G 1 is a relaxed projection of G 2 ; by assumption G 0 is a relaxed projection of G 1 .",34,35
8515,232249,"For these, the quantifying text (e.g., about, over, nearly), is linked to the number following the currency symbol instead of to the currency symbol as it was in the CoNLL 2008 task.",24,25
8516,232249,"For these, the quantifying text (e.g., about, over, nearly), is linked to the number following the currency symbol instead of to the currency symbol as it was in the CoNLL 2008 task.",30,31
8517,11232678,"We suppose a first-order Markov chain consisting of transitions t ji from letter l j,i-1 ∈ A B to letter l ji ∈ A where A is a regular letter alphabet and A B =A ∪ {B} includes B as an abstract morpheme start symbol which can occur in l j,i-1 .",51,52
8518,11232678,"A transition t ji is now defined as a transition from an abstract symbol in l j,i-1 ∈ {N , B} to a letter in l ji ∈ A. The abstract symbol is N or B depending on whether b ji is 0 or 1.",13,14
8519,11232678,"A transition t ji is now defined as a transition from an abstract symbol in l j,i-1 ∈ {N , B} to a letter in l ji ∈ A. The abstract symbol is N or B depending on whether b ji is 0 or 1.",35,36
8520,4959516,"To construct a reference to a particular entity, the algorithm takes as input a symbol corresponding to the intended referent and a list of symbols corresponding to other entities in focus based the intended referent, known as the contrast set.",15,16
8521,220793898,All models from the table use the ˆspecial symbol to highlight propaganda spans because more than 20% of the sentences contain multiple propaganda spans.,8,9
8522,9468172,"Each grammar G t has a set of symbols denoted with capital letters and a subscript indicating the level in the hierarchy, including a distinguished goal (root) symbol G t .",30,31
8523,9468172,"A grammar G t−1 is a projection of G t if there exists some manyto-one onto function π t which maps each symbol in G t to a symbol in G t−1 ; hereafter, we will use A t to represent π t (A t ).",24,25
8524,9468172,"A grammar G t−1 is a projection of G t if there exists some manyto-one onto function π t which maps each symbol in G t to a symbol in G t−1 ; hereafter, we will use A t to represent π t (A t ).",30,31
8525,7948161,"For instance, given candidate word '應該' corresponding to original string '因該', phonetic symbol of characters '因' and '應' is 'ㄧㄣ' and 'ㄧㄥ' respectively.",18,19
8526,11435118,"For example, the Turkish word yer ('earth') with the marked word boundary #ye r# generates the following Lstem Rsu f f rules: #ye r#, #ye r, ye r#, #ye , ye r, e r#, r#, ye , e r, r, and e , where the symbol '#' signifies the word initial and final positions.",68,69
8527,10168800,"Agenda-Driven Parsing A non-hierarchical, best-first parser takes as input a PCFG G (with root symbol R), a priority function p(•) and a sentence consisting of terminals (words) T 0 . . .",22,23
8528,10168800,"The objects in an agenda-based parser are edges e = I(X, i, j), also called items, which represent parses spanning i to j and rooted at symbol X. We denote edges as triangles, as in Figure 1 .",33,34
8529,10168800,"In particular, each grammar has a distinguished terminal symbol T i t for each word T i in the input and a root symbol R t .",9,10
8530,10168800,"In particular, each grammar has a distinguished terminal symbol T i t for each word T i in the input and a root symbol R t .",24,25
8531,10168800,The IN-BASE rule specifies the base case for a grammar G t : we cannot begin until the outside score for the terminal symbol T is ready in the coarser grammar G t−1 .,26,27
8532,10168800,The OUT-BASE rule states that an outside pass for G t can be started if the inside score of the root symbol for that level R t has been computed.,23,24
8533,10168800,"Starting with an X-Bar grammar, they iteratively refine each symbol in the grammar by adding latent substates via a split-merge procedure.",12,13
8534,1494188,"The first rule type is the TERMINAL production, which rewrites a terminal symbol 2 E as its English word e and a (possibly empty) sequence of foreign words f t .",13,14
8535,1494188,"We can also rewrite a non-terminal symbol A using a UNARY production, which on the English side produces a single symbol B, and on the foreign side produces the symbol B, with sequences of words f l to its left and f r to its right.",8,9
8536,1494188,"We can also rewrite a non-terminal symbol A using a UNARY production, which on the English side produces a single symbol B, and on the foreign side produces the symbol B, with sequences of words f l to its left and f r to its right.",23,24
8537,1494188,"We can also rewrite a non-terminal symbol A using a UNARY production, which on the English side produces a single symbol B, and on the foreign side produces the symbol B, with sequences of words f l to its left and f r to its right.",33,34
8538,1494188,"In Figure 2 , we show a sample sentence pair frag-2 For notational convenience, we imagine that for each particular English word e, there is a special preterminal symbol E which produces it.",30,31
8539,1494188,"When generating foreign word sequences f at a non-terminal (i.e. via the UNARY or BINARY productions), we also allow for the production of foreign words from the non-terminal symbol A. We modify p(f j | e A ) from the previous section to allow production of f j directly from the non-terminal 7 A: p(f j | e A ) = p nt • p(f j | A) + (1 − p nt ) • i∈e A 1 |e A | p t (f j | e i ) where p nt is a global binomial parameter which controls how often such alignments are made.",35,36
8540,53644212,"Therefore, we split the text of the arguments by commas and semicolons, as well as by the symbol '&' or the token and.",19,20
8541,226283651,The symbol means an element-wise multiplication.,1,2
8542,239992804,"As we are interested in potential common wrong predictions between the models, we only visualize wrongly classified samples, and group them by two criteria: the model used (colorencoded) and the gold label (symbol-encoded).",38,39
8543,225507,An atomic formula or atom is a predicate symbol.,8,9
8544,7351365,"In Table 1 , a person name annotated with the symbol * indicates that the entity is bipolarized incorrectly.",10,11
8545,7351365,The symbol ^ indicates that the person name is neutral (or irrelevant) to the teams in the finals.,1,2
8546,7351365,"In addition, the symbol + indicates that the person name represents a relative of a member of the team he/she is bipolarized to; or the name is a misspelling, but it refers to a member of the bipolarized team.",4,5
8547,5479052,"However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.",29,30
8548,218974074,"In total, there are 29,995 such reconstructed forms (44.4% of the resource), indicated with a * symbol in Cifu.",21,22
8549,6525455,The symbol next to each random variable denotes the parameter of its variational distribution.,1,2
8550,10806790,"Annotated corpora on the other hand have the potential to carry varying degrees of granularity of linguistic detail, therefore bypassing the need to translate using SingWriting and then deriving such details from the resulting SignWriting symbol.",36,37
8551,5571810,"Original (in Japanese) 1 '''EU''' 2 * [[AJå]] 3 * [[Europa Universalis]]Á -[[¨À {ÊÀz]]G[[× ¶ ¶¼Ã ¾Ê•]] 4 * [[WÁdÓ]](Ehime University) -[[WÁ z]][[Ã¿]]Ddñ §G[[ dÓ]] 5 '''Eu''' 6 * [[½}Ä«}•]]Gaed 7 * [[½¬¢y•]] -÷""ÕH 8 '''eu''' 9 * [[.eu]] -AJåG[[ 9 ¸{Ê]] 10 * [[ §½]]G[[ISO 639|ISO 639-1 ½ ]] Gloss 1 '''EU''' 2 * [[European Union]] 3 * [[Europa Universalis]] series -a [[histori- cal computer game]] by [[Paradox Interactive]] 4 * [[Ehime University]] -a [[National Univer- sity]] in [[Matsuyama]],[[Ehime Prefecture]] 5 '''Eu''' 6 * [[Europium]]'s chemical element symbol 7 * [[euphonium]] -a brass instrument 8 '''eu''' 9 * [[.eu]] -[[country-code top-level domain]] for the European Union 10 * [[ISO 639|ISO 639-1 language code]] of [[Basque]] [[ ]] shows a link in Wikipedia.",179,180
8552,218974248,"All the available resources are: • A bank of 8000 downloadable pictograms, in colour or black and white, in constant progression, • An image bank, • A video bank LSE (Spanish Sign Language) In addition to their availability, these resources can be manipulated online using three main tools that are highly relevant: • A sentence generator in pictograms, • A symbol generator (image + legend + frame...) • An animated symbol generator (.gif) We also chose Arasaac because in practice it is one of the most widely used pictograms in France for institutions and families caring for people with disabilities.",70,71
8553,218974248,"All the available resources are: • A bank of 8000 downloadable pictograms, in colour or black and white, in constant progression, • An image bank, • A video bank LSE (Spanish Sign Language) In addition to their availability, these resources can be manipulated online using three main tools that are highly relevant: • A sentence generator in pictograms, • A symbol generator (image + legend + frame...) • An animated symbol generator (.gif) We also chose Arasaac because in practice it is one of the most widely used pictograms in France for institutions and families caring for people with disabilities.",83,84
8554,15214701,2 An example of the type of equation we solve here is prediction − predict + symbolize = symbol (from the dev set).,18,19
8555,218974410,"First, there is a special SEQ(α) extension that makes it possible to specify repetitions of the same grammar symbol α.",20,21
8556,222002282,"2016b) , which is capable of encoding open vocabularies with a compact symbol vocabulary of variable-length subword units.",13,14
8557,218973767,"Data Wikidata If we count only labels which differ from one another at least by one symbol, the numbers are much less impressive: 1.87 different labels per entity (median = 1, std = 2.15).",16,17
8558,219310248,"Its inverted cone shape and red colour due to the presence of blood formed a well-known symbol for life and love.""",18,19
8559,219310248,"3 ); • the projection of topological constructions in the signing space onto the 2d plane, a feature already observed by Guitteny (2007) who studied SL discourse supported by educational (explanatory) diagrams; • equal and comparison signs, as in figures 5 and 4; • symbol repetition in enclosed shapes to mean sets of identical objects like in figure 5 More recurrent features can yet be found in the diagrams, the list above only being a sample.",53,54
8560,218974144,The comment symbol is denoted by # and it represents as ignorance to the end of the line.,2,3
8561,184483621,We also remove the hash symbol from hashtags as it can be problematic for tokenizers to work with.,5,6
8562,5584560,Homophonic Ciphers A homophonic cipher uses a substitution key that maps a plaintext letter to more than one cipher symbol.,19,20
8563,5584560,"The number of potential cipher symbol substitutes for a particular plaintext letter is often proportional to the frequency of that letter in the plaintext languagefor example, the English letter ""E"" is assigned more cipher symbols than ""Z"".",5,6
8564,5584560,"The substitution key is, however, deterministic in the decipherment direction-each ciphertext symbol maps to a single plaintext letter.",15,16
8565,5584560,"The Zodiac messages include two interesting ciphers: (1) a 408-symbol homophonic cipher without spaces (which was solved manually by hand), and (2) a similar looking 340-symbol cipher that has yet to be solved.",14,15
8566,5584560,"The Zodiac messages include two interesting ciphers: (1) a 408-symbol homophonic cipher without spaces (which was solved manually by hand), and (2) a similar looking 340-symbol cipher that has yet to be solved.",37,38
8567,5584560,"Data: Figure 1 (bottom) displays the Zodiac-408 cipher (consisting of 408 tokens, 54 symbol types) along with the original plaintext message.",18,19
8568,5584560,"But we already know that the substitution ciphers described here exhibit determinism in the deciphering direction, 1 i.e., although we have no idea about the key mappings themselves, we do know that there exists only a single plaintext letter mapping for every cipher symbol type in the true key.",46,47
8569,5584560,So sampling plaintext choices for every cipher token separately is not an efficient strategyour sampler may spend too much time exploring invalid keys (which map the same cipher symbol to different plaintext letters).,29,30
8570,5584560,"Under this scheme, we sample plaintext letter choices for each cipher symbol type.",12,13
8571,5584560,"For example, if we sample a new choice p new for a cipher symbol which occurs at positions 4, 10, 18, then we update plaintext letters p 4 , p 10 and p 18 with the new choice p new .",14,15
8572,5584560,"In a first sampling pass, we sample from 26 plaintext letter choices (e.g., ""A"", ""B"", ""C"", ...) for every cipher symbol type as before.",33,34
8573,218973844,"To process the datasets, a special end-of-sentence symbol EOS is added to the end of each sentence, and the outof-vocabulary words are mapped to a special token symbol UNK .",12,13
8574,218973844,"To process the datasets, a special end-of-sentence symbol EOS is added to the end of each sentence, and the outof-vocabulary words are mapped to a special token symbol UNK .",35,36
8575,218977358,"The separation of the Mycenaean sequences was based on the following logic: The use of the divider (short upright line on the tablets, conventionally represented by a comma on the transliterated texts) as a word separation symbol and our hypothetical assumption that numeric and metric signs are followed by new sequences.",40,41
8576,218977358,"Although the difference in the first and third case of Figure 6 is not distinct, it is located in the gap between the symbol and the syllable.",24,25
8577,218977358,"Consequently in the vector describing the unknown symbol, the pdf will be uniform in the phonetic values attributed by the 77 syllables, since this is impossible to be an ideogram.",7,8
8578,3532585,"Transitions in M compete with each other if they leave the same state with the same input symbol, which may be empty ( ).",17,18
8579,209202447,"The ∆ symbol indicates the number of errors that we have corrected during the postprocessing, expressed as the difference between the number of errors before and after post-processing.",2,3
8580,2049079,"As a result, EM exploits a lot of rare tags (like FW = foreign word, or SYM = symbol) and assigns them to common word types (in, of, etc.).",21,22
8581,218974550,"The function of each key, although not labeled, can be easily determined by its symbol.",16,17
8582,2717337,"This is because the target table contains significant non-determinism, and because each symbol has multiple possible fertilities, which introduces uncertainty about the length of the target string.",15,16
8583,21691078,"In Table 1 , attributes labelled as antonyms are marked with a ""¬"" symbol.",15,16
8584,14992568,"Second, the most frequent symbol in the corpus is a vowel.",5,6
8585,14992568,"The latter assumption is used to initialize the classification step by claiming that the most frequent symbol is the first member of the vowel class, with the rest of the symbols initially all classified as consonants.",16,17
8586,14992568,With the help of the first assumption the other vowels are then classified by iteratively checking which symbol is less frequently adjacent to the already detected vowels.,17,18
8587,14992568,"Hence, the second assumption that the most frequent symbol in a text is always a vowel cannot be backed up by typological knowledge.",9,10
8588,14992568,"In our sample of texts in different languages, nevertheless the most frequent symbol is always a vowel.",13,14
8589,14992568,s n } we construct an n × n matrix M where the rows represent the first and the columns the second symbol in a bigram sequence and which indicates the number of times the sequences occur in the corpus.,22,23
8590,14992568,"In each cycle of the phase, the symbol with the highest row sum greater than zero is detected and classified as a vowel.",8,9
8591,14992568,The row sum for any symbol s a is calculated by adding up all occurrences of the symbol s a as a first or second member in a sequence ∑ n i=1 m ai .,5,6
8592,14992568,The row sum for any symbol s a is calculated by adding up all occurrences of the symbol s a as a first or second member in a sequence ∑ n i=1 m ai .,17,18
8593,14992568,"After a new vowel has been detected, its row sum is set to zero and all other row sums are updated by subtracting from the sum of the row of each remaining symbol twice the number of times it occurs next to the new-found vowel.",33,34
8594,14992568,The fact that initially the symbol with the highest sum is considered to be a vowel reflects the idea that the most frequent symbol in the corpus has to be a vowel.,5,6
8595,14992568,The fact that initially the symbol with the highest sum is considered to be a vowel reflects the idea that the most frequent symbol in the corpus has to be a vowel.,23,24
8596,14992568,What the row sums after each step actually contain is the difference between the number of times a symbol is found next to a consonant and the number of times it is found next to a vowel.,18,19
8597,14992568,Whenever a new vowel has been detected all occurrences of this vowel have to be subtracted from the other symbols because this symbol is no longer considered to be a consonant.,22,23
8598,14992568,"In the Maltese case, the symbol î is classified as a consonant because it only occurs twice in the corpus in the word eloî where it stands next to a symbol that is clearly a vowel.",6,7
8599,14992568,"In the Maltese case, the symbol î is classified as a consonant because it only occurs twice in the corpus in the word eloî where it stands next to a symbol that is clearly a vowel.",31,32
8600,14992568,"In Swahili, for instance, with the official orthography the symbol c is classified as a vowel because it only occurs in the digraph ch.",11,12
8601,14992568,"After the digraph has been replaced by a single symbol, the classification is correct in all cases.",9,10
8602,14992568,"Sometimes a symbol (e.g., h in Warlpiri) is misclassified because it does not occur in the writing system of the language but is part of a digraph in foreign words (mostly proper names of people or locations in the Bible texts).",2,3
8603,14992568,Another problem of the approach is with orthographies that use the same symbol for both vowels and consonants.,12,13
8604,14992568,For those languages marked as revised the most frequent digraphs have been replaced by a single symbol.,16,17
8605,14992568,The precision is 92.50% and the recall 94.96% (F-Score 0.94) for each transition from one symbol to another.,21,22
8606,14992568,"The results of Sukhotin's algorithm show that the distinction between vowels and consonants, which is vital for any syllabification method, can be induced from raw texts on the basis of the simple assumptions that vowels and consonants tend to alternate and that a vowel is the most frequent symbol in a corpus.",51,52
8607,10753418,"The X symbol stands for the segment to be dropped, L ( ) and R ( ) are conditions over the left and right contexts respectively.",2,3
8608,3204349,"At the sentence ""該(that) 處(place) 的(of) 露 宿者(street sleeper) 只(only) 有(have) 數(some) 人(person) (There are only some street sleepers in that place)"" in the CITYU corpus, ""露/B宿 /M者/E(street sleeper)"" is observed to be an OOV word, while "" 露 /B 宿 /E(sleep on the street)"" is an IV word, where the associated tag of each character is given after the slash symbol.",85,86
8609,218974404,"Accordingly, the corresponding columns remain empty (we do not insert a placeholder symbol).",14,15
8610,218973964,symbol marks questions; other punctuation is not used.,0,1
8611,218977407,Previous work on analysing Japanese recipes has shown that this level of accuracy is sufficient to support tasks including recipe information retrieval and symbol grounding for cross-modal cooking applications.,23,24
8612,218974152,"4 PROPN (proper noun), AUX (auxiliary), SCONJ (subordinate conjunction), SW (symbol) and INTJ (interjection) Recently, Park and Tyers (2019) updated this mapping to the UPOS14.",20,21
8613,218974152,"12 'those ordinary loan words should be assigned a normal POS tag and X tag should be used very restrictively for' (UD annotation guidelines v1, 2014) 13 In the GKT corpus, units are tagged inconsistently with the Sejong SW (symbol) and SL tags even within a sentence.",46,47
8614,218974152,Currency symbol plays an important role in the identification of references to monetary amount in text.,1,2
8615,218974152,"For example, in '$500' ($/SW+500/SN), the numeral value '500' is recognised as a currency value as it is preceded by a currency symbol '$'.",34,35
8616,14850676,"By Elgot and Mezei's theorem, there exists left subsequential L : X * → Z * and right subsequential R : Z * → X * with X ⊆ Z such that T = R • L. Let h : Z * → X * be a function which encodes each word w in Z * by coding each symbol in w as follows.",61,62
8617,14850676,"Assume some enumeration of the symbols in Z and the rewrite the nth symbol of Z as ab n a. For example if Z = {a, b, c} and w = cab then h(w) = abbaaaaba.",13,14
8618,14850676,"The symbol ⊞ (⊟) refers to opaque vowels which are +F (−F ), which block the spread of −F (+F ), and which begins a new harmonic domain spreading +F (−F ).",1,2
8619,14850676,The rightmost symbol interior to a state is the output of the σ function.,2,3
8620,14850676,"Crucially, however, there can also be no transition from state 2 bearing the input symbol ⊟ that can lead (even eventually) to a final state.",16,17
8621,14850676,"To get an intuition why, consider the two subsequential transducers A and B in Table 5 illustrates the role the additional symbol plays in the derivation.",22,23
8622,218974316,"For the two R&R, a special bind symbol ""&+"" is used for concatenation i.e combining morphemes without spaces.",8,9
8623,218974512,"For instance, the position of the disappearing weak radical aleph is marked with a symbol X. This is useful for providing unambiguous contexts for rewrite rules that handle allomorphy, vowel contraction or lengthening and gemination at morpheme boundaries.",15,16
8624,218974512,"For final weak words, we replaced the contracted nominative ending with the symbol X to preserve the position of the weak radical.",13,14
8625,218974512,"2007) , we view finite-state transducers as finite-state acceptors of strings consisting of symbol-pairs x:y, where x and y belong to finite input and output alphabets, respectively.",18,19
8626,218974512,"As a special case, either x or y can be the empty symbol ε.",13,14
8627,218974512,"We 8 denote transitions in T as 5-tuples (s, t, x, y, w) , where s and t denote the source and target state, respectively; x:y is a symbol pair and w t is a real-valued weight.",40,41
8628,218974512,"In order to assign weights to the transitions of transducer T , we need a set P = {p 1 ,...,p m } or strings p = x 1 :y 1 , …, x n :y n , where each string p is accepted by transducer T. We start by associating each state Q of transducer T with a transition counter C Q , which maps symbol-pairs x:y to counts C Q (x:y), and a finality count f Q .",73,74
8629,218974512,"If string p=x 1 :y 1 , …, x n :y n is accepted by transducer T , then each symbol-pair x i :y i corresponds to a unique accepting state Q i because transducer T is deterministic.",25,26
8630,221818642,"Traditionally, the first input of a decoder is an embedding that corresponds to a start symbol S .",16,17
8631,221818642,"To generate the k th hypothesis, we perform decoding by feeding e k as embedding of the start symbol.",19,20
8632,218973729,"On one hand, English was still a symbol for modernization.",8,9
8633,218973729,"The attitude of Egyptians towards the use of English has shifted from post-colonization state, where English shop signs were criticized as ""Arabic in the Valley of Neglect"", ""Winds of Foreignization Sweep the Egyptian Street"" and ""Before Arabenglish Spreads"" (Simpson and others, 2008) , to being accepted by the majority as a symbol of modernization.",64,65
8634,218487395,The placeholder is a special symbol such as <UNK>.,5,6
8635,218974223,These methods rely on some alignment between the grapheme and phoneme strings because they gather statistics about symbol alignments.,17,18
8636,1336495,"As discussed in Section 2, metrics of isomorphism measure similarities between the distribution of proposed morphemes and the distribution of answer morphemes, where proposed and answer morphemes may be disjoint symbol sets.",32,33
8637,212718077,"These models generate observations step-by-step, modeling the probability of the next symbol given the previously generated ones.",16,17
8638,212718077,6) preceded by an embedding module (learnable representation for symbols and their positions) and followed by a logistic classification module (learnable linear classifier to predict the next symbol).,31,32
8639,1131864,"There is also a distinguished root node TOP in each forest, denoting the goal item in parsing, which is simply S 0,l where S is the start symbol and l is the sentence length.",29,30
8640,52119281,"However, it results in longer sequences in which each symbol contains less information, creating both modeling and computational challenges.",10,11
8641,17330608,9) Each such rule inserts the symbol # x immediately after an occurrence x in the input string.,7,8
8642,1160193,"Notice that the virtual nonterminal NP-PP representing the intermediate symbol is discontinuous with two spans on the target (English) side, because this binarization scheme completely ignores the reorderings of nonterminals.",11,12
8643,1160193,"The first pass is as above, and the second pass traverses the first-pass forest, assigning to each node v a set of augmented items, which we call +LM items, of the form (v a⋆b ), where a and b are target words and ⋆ is a placeholder symbol for an elided part of a target-language string.",56,57
8644,174802397,"For clarity, we denote the input and output in the decoder as z and y. z is a shifted copy of y in the standard NMT model, i.e. z = sos , y 1 , • • • , y J−1 , where sos is a start symbol.",49,50
8645,14770371,"A path in a translation hypergraph induces a translation hypothesis E along with its sequence of SCFG rules D = r 1 , r 2 , ..., r K which, if applied to the start symbol, derives E. The sequence of SCFG rules induced by a path is also called a derivation tree for E. Minimum Error Rate Training Given a set of source sentences F S 1 with corresponding reference translations R S 1 , the objective of MERT is to find a parameter set λM 1 which minimizes an automated evaluation criterion under a linear model: λM 1 = arg min λ M 1  S X s=1 Err `Rs, Ê(Fs; λ M 1 ) ´ff Ê(Fs; λ M 1 ) = arg max E  S X s=1 λmhm(E, Fs) ff .",37,38
8646,14770371,"To illustrate which operator is applied when, we transform H = V, E into a regular graph with typed nodes by (1) marking all vertices v ∈ V with the symbol ∨ and (2) replacing each hyperedge e ∈ E, |e| > 1, with a small subgraph consisting of a new vertex v ∧ (e) whose incoming and outgoing edges connect the same head and tail nodes Algorithm 1 ∧-operation (Sum) input: associative map a: V → Env(V), hyperarc e output: Minkowski sum of envelopes over T (e) for (i = 0; i < |T (e)|; ++i) { v = Ti(e); pq.enqueue( v, i, 0 ); } L = ∅; D = e, ε1 = .D; if (0 < j) { L.back().y -= A[v][j-1].y; L.back().m -= A[v][j-1].m; } } if (++j < A[v].size()) pq.enqueue( v, i, j ); } return L; in the transformed graph as were connected by e in the original graph.",34,35
8647,14770371,"Because the right-hand side of r e has n nonterminals, the arity of e is |e| = n. Let T (e) = {v 1 , ..., v n } denote the tail nodes of e. We now assume that each tail node v i ∈ T (e) is associated with the upper envelope over all candidate translations that are induced by derivations of the corresponding nonterminal symbol X i .",75,76
8648,52979524,"A round symbol represents a SQL tokens, a table column, etc.",2,3
8649,52979524,A square symbol indicates a module that predicts the next SQL token from its corresponding token instances with the same color.,2,3
8650,52979524,"A round symbol represents a SQL token, such as SELECT, WHERE, a table column, etc.",2,3
8651,52979524,A square symbol indicates a module that predicts the next SQL token from its corresponding token instances with the same color.,2,3
8652,16775609,"So we have the following stack s = [ NP @1 VP @2 ], where the dot indicates the next symbol to process in the English word-order.",21,22
8653,16775609,"Since the symbol right after the dot in the top rule is a word, we immediately grab it, and append it to the current hypothesis, which results in the new stack [ NP @1 VP @2 ] [Bush ].",2,3
8654,16775609,"The symbol after the dot in the top rule is called the next symbol, since it is the symbol to expand or process next.",1,2
8655,16775609,"The symbol after the dot in the top rule is called the next symbol, since it is the symbol to expand or process next.",13,14
8656,16775609,"The symbol after the dot in the top rule is called the next symbol, since it is the symbol to expand or process next.",19,20
8657,16775609,"Depending on the next symbol a, we can perform one of the three actions: • if a is a node η, we perform a Predict action which expands η using a rule r that can patternmatch the subtree rooted at η; we push r is to the stack, with the dot at the beginning; • if a is an English word, we perform a Scan action which immediately adds it to the current hypothesis, advancing the dot by one position; • if the dot is at the end of the top rule, we perform a Complete action which simply pops stack and advance the dot in the new top rule.",4,5
8658,16775609,"In fact, since both of them are deterministic operations, they are treated as ""closure"" operators in the real implementation, which means that after a prediction, we always do as many scanning/completion steps as possible until the symbol after the dot is another node, where we have to wait for the next prediction step.",44,45
8659,3510512,"Similarly, a +LM item in SCFG-based models has the form (v a⋆b ), where a and b are boundary words of the hypothesis string, and ⋆ is a placeholder symbol for an elided part of that string, indicating that a possible translation of the part of the input spanned by v starts with a and ends with b. An example +LM version of Deduction (2) is: (PP with ⋆ Sharon 1,3 ): (w 1 , t 1 ) (VP held ⋆ talk 3,6 ): (w 2 , t 2 ) (VP held ⋆ Sharon 1,6 ): (w, t 2 t 1 ) where w = w 1 + w 2 + c ′ − log P lm (with | talk) with a similar combination cost formed in combining adjacent boundary words of antecedents.",36,37
8660,989542,"We view each left-hand side subtree as a monolithic nonterminal symbol and factor each transducer rule into two SCFG rules: one from the root nonterminal to the subtree, and the other from the subtree to the leaves.",12,13
8661,989542,"The same algorithm can be used to find optimal strategies for synchronous parsing or for m-gram decoding: for parsing, the variables a, b, and c in Line 9 refer to the total number of spans of A, B, and C (Equation ( 16 )), Figure 17 Left, example spans for a ternary rule decomposition X → A B C. Each symbol represents a subset of nonterminals from the original SCFG rule, and the subsets may cover discontinuous spans in either language.",72,73
8662,195750845,"For data mixing, they used a special symbol on the source side to indicate the data domain.",8,9
8663,195750845,They include out-ofdomain clean data during this step and differentiate data types with a special symbol on the target side.,17,18
8664,195750845,Emojis were replaced with a special symbol.,6,7
8665,1675036,Each transition has associated an input symbol and an output string.,6,7
8666,1675036,The condition of determinism implies that no two distinct transitions departing from a given state have the same input symbol.,19,20
8667,1675036,"Every time an input symbol is accepted, the string associated to the corresponding transition is output and a new state is reached.",4,5
8668,201635822,One modification was necessary to run the pipeline for Nepali due to the end-of-sentence symbol of the script that was previously not recognized by the sentence splitter.,18,19
8669,207911212,Excluded features are indicated by the minus (-) symbol.,10,11
8670,207926546,I suggest we put on white stripes on our arms as a symbol of honest elections.,12,13
8671,311696,This gives us the following stack: s = [ NP @1 VP @2 ] The dot ( ) indicates the next symbol to process in stack hyp.,22,23
8672,6120662,"If a user's guess ê does not match e * and is not in the dictionary, we replace it with • the special symbol <COPY>, if ê appears to be a copy of the German source word f (meaning that its Levenshtein distance from f is < 0.2 • max(|ê|, |f |)); • else, the closest word in the dictionary 4 as measured by Levenshtein distance (breaking ties alphabetically), provided the dictionary has a word at distance ≤ 2; • else <BLANK>, as if the user had not guessed.",25,26
8673,53080329,"In general, we can observe that the macro-f1 scores of in-language and cross-language approaches are very similar and, in some settings, cross-language macro F1-scores are equal or above in-language scores (marked with the symbol in Table 4 ).",49,50
8674,248780544,The @ symbol denotes the time reference for an atom.,2,3
8675,248780544,"Starting from a template-based categorization of the rules (Table 1 ), we transform each pair (⟨claim NL, rule DL⟩) into a full-text specification: NL is prefixed by ""datalog translation:"", which identifies the downstream translation task; for DL we safely remove the symbol ⊥ appearing in all queries (thus non necessary to predict) and replace math symbols with text snippets.",56,57
8676,6099247,"Empty state is denoted using the customary symbol for empty string, .",7,8
8677,237940562,"An LP MLN program Π is a finite set of weighted rules of the form: w : A ← B (1) where A is a disjunction of atoms of σ, B is a conjunction of literals (atoms and negated atoms) of σ, and w is a real number or the symbol α.",57,58
8678,16386528,"Each edge is labelled with either a symbol from the source terminal vocabulary or a generic gap symbol, and the trie is constructed such that for any path originating at the root vertex, the sequence of edge labels represents the prefix of a rule's source righthand-side (RHS s , also referred to as a rule pattern).",7,8
8679,16386528,"Each edge is labelled with either a symbol from the source terminal vocabulary or a generic gap symbol, and the trie is constructed such that for any path originating at the root vertex, the sequence of edge labels represents the prefix of a rule's source righthand-side (RHS s , also referred to as a rule pattern).",17,18
8680,16386528,Edges in the secondary trie are labelled with the matching symbol and the position of the word in the input sentence (or a null position for gap labels).,10,11
8681,4706971,It outputs either a character or an advance symbol (to advance the focus of attention for the next time step).,8,9
8682,5575191,We use ⊥ to denote a symbol of rank 0 that is not in any ranked alphabet used in this paper.,6,7
8683,13848633,Most fixed-vocabulary language models employ a distinguished symbol UNK that represents all words not present in V; these words are termed out-of-vocabulary (OOV).,9,10
8684,13848633,"Let ∪ denote disjoint union, i.e., A ∪ B = C iff A ∪ B = C and A ∩ B = ∅. Let Σ be a discrete alphabet of characters, including a dis- tinguished unknown-character symbol .",41,42
8685,13848633,"2 A char- acter LM then defines p(c) = |c|+1 i=1 p(c i | c <i ), where we take c |c|+1 to be a distinguished end-ofstring symbol EOS.",32,33
8686,13848633,"Symbols are generated from a multinomial given the history h, leading to a new history h that now includes the symbol and is truncated to the Markov order.",21,22
8687,53615918,"To parse a CSV file, for each possible separator "";,|^\t "" and for each possible quote symbol """"'~"", we attempt to parse a first row of the CSV according to these separators and quote symbols and determine how many columns exist in the first row.",20,21
8688,53615918,"If the parser finds that any row has a different number of columns, then the parsing of the file with that separator and quote symbol fails, and we then try any remaining unattempted combinations of separators and quote symbols.",25,26
8689,5479016,"We denote ρ(t) to be the root symbol of tree t. When writing these rules, we avoid notational overhead by introducing a short-hand form from Galley et al. (",8,9
8690,5479016,"Following TSG terminology (see Figure 1 ), we call these ""variable nodes"" such as x 2 :NP-C substitution nodes, since when applying a rule to a tree, these nodes will be matched with a sub-tree with the same root symbol.",50,51
8691,5479016,"If r = (t, s, φ) is a rule, and d i is a (sub-) derivation with the root symbol of its source projection matches the corresponding substitution node in r, i.e., ρ(E( d i )) = φ(x i ), then d = r(d 1 , . . . ,",26,27
8692,5479016,"Similarly, we now marginalize over all derivations D(τ * ) = {d | E(d) = τ * } that translates English tree τ into some Chinese string and apply the Viterbi approximation again to search for the best derivation d * : c * = C(d * ) = C(argmax d∈D(τ * ) Pr(d)) (6) Assuming different rules in a derivation are applied independently, we approximate Pr(d) as Pr(d) = r∈d Pr(r) (7) where the probability Pr(r) of the rule r is estimated by conditioning on the root symbol ρ(t(r)): Pr(r) = Pr(t(r), s(r) | ρ(t(r))) = c(r) r :ρ(t(r ))=ρ(t(r)) c(r ) (8) where c(r) is the count (or frequency) of rule r in the training data.",102,103
8693,9009357,"That is, the words that were not in the dictionary were replaced with the unknown word symbol <UNK> before counting the bigrams.",17,18
8694,226283899,All the above methods have a finite vocabulary size and use a 'UNK' symbol to represent OOV words.,15,16
8695,226283899,OOV non-numerical words are replaced with symbol UNK word and OOV numerals are replaced with symbol UNK num .,8,9
8696,226283899,OOV non-numerical words are replaced with symbol UNK word and OOV numerals are replaced with symbol UNK num .,17,18
8697,8866458,The wildcard symbol * means it can be any word or relation.,2,3
8698,2863491,"3 ) is computed based on the previous hidden state z t−1 , previous generated symbol y t−1 and the computed context vector c t .",15,16
8699,1153327,"These two requirements are related to grammar refinement by annotation (Johnson, 1998) , where annotations must be bounded and monotonic: for example, one cannot refine a grammar by only remembering the grandparent but not the parent symbol.",42,43
8700,218486915,Model Our model reasons about the printed appearances of a symbol (say majuscule F) in a document via a mixture model whose K components correspond to different metal stamps used by a printer for the document.,10,11
8701,18028226,"Bracketing Transduction Grammar (BTG) (Wu, 1997) is a binary synchronous context-free grammar with only one non-terminal symbol, and has three types of rules (Figure 2 ): Straight which keeps the order of child nodes, Inverted which reverses the order, and Terminal which generates a terminal symbol.",25,26
8702,18028226,"Bracketing Transduction Grammar (BTG) (Wu, 1997) is a binary synchronous context-free grammar with only one non-terminal symbol, and has three types of rules (Figure 2 ): Straight which keeps the order of child nodes, Inverted which reverses the order, and Terminal which generates a terminal symbol.",58,59
8703,17525780,"On the one hand, the output symbol for a given transition could contain more than one word.",7,8
8704,543551,We tried Gaussian initialization (as in the rest of our model) and initializing the model by clustering rules either randomly or according to their parent symbol.,27,28
8705,3065236,"For simplicity, we assume that the training data is a single very long string, w 1 • • • w N , where w N is a special stop symbol, </s>.",31,32
8706,3065236,"We write u i for w i−n+1 • • • w i−1 , where, for i ≤ 0, w i is a special start symbol, <s>.",26,27
8707,30601989,"Once you find a completed hypothesis (by generating the </s> symbol), there are still other active hypotheses in the beam that can continue to grow, which might lead to better scores.",13,14
8708,219307532,"That is, the words that were not in the dictionary were replaced with the unknown word symbol <UNK> before counting the bigrams.",17,18
8709,207468,Note that each sentence ends up with a special end-ofsentence symbol <end>.,12,13
8710,11639083,"The process creates one xRS rule that is headed by a pseudo, non-syntactic nonterminal symbol that subsumes the target phrase and corresponding multi-headed syntactic structure; and one sibling xRS rule that explains how the non-syntactic nonterminal symbol can be combined with other genuine nonterminals in order to obtain genuine parse trees.",17,18
8711,11639083,"The process creates one xRS rule that is headed by a pseudo, non-syntactic nonterminal symbol that subsumes the target phrase and corresponding multi-headed syntactic structure; and one sibling xRS rule that explains how the non-syntactic nonterminal symbol can be combined with other genuine nonterminals in order to obtain genuine parse trees.",44,45
8712,891605,"With the model of Uszkoreit and Brants (2008) , the likelihood of a sequence of word tokens, w = w i m i=1 , with w i ∈ V ∪ {S}, where S is a designated start-ofsegment symbol, factors as L(w; C) = m i=1 p(w i |C(w i ))p(C(w i )|w i−1 ) . (",45,46
8713,13331080,And the predicted action for the START symbol is always s. Matching.,7,8
8714,15830005,"The goal of 1-best PCFG parsing is to compute the Viterbi inside score of the goal edge (TOP, 0, n) where TOP is a special root symbol.",32,33
8715,15830005,Figure 2 shows a part of the hierarchical symbol definition.,8,9
8716,15830005,"m − 1], we call an element in N i i-th layer shrinkage symbol.",17,18
8717,15830005,"For some we define a mapping π i→j : N i → P(N j ) where P(•) is the power set of •. Taking a symbol HP in Figure 2 as an example, π 0→1 (HP) = {S , N }.",26,27
8718,15830005,"When i = j, for some i-th layer shrinkage symbol A ∈ N i , π i→j (A) returns a singleton {A}.",12,13
8719,15830005,"If the best goal derivation d in the coarse chart does not include any shrinkage symbol, it is equivalent to the best goal derivation in the original chart.",15,16
8720,15830005,The deterministic parsing keeps only one non-terminal symbol with the highest score per chart cell and removes the other non-terminal symbols.,9,10
8721,15830005,"If all of the K-best derivations do not contain any shrinkage symbol, it returns them and terminates.",13,14
8722,8570505,"Note that the other two kinds of poetry can also be fit by this structure, if one permits another symbol (we use *) to stand for the syllables whose stress is not important, e. For this type of genre, we need to obey the same two constraints as in the line-based poetry, but also to ensure that rhyming constraints hold.",20,21
8723,6278207,"Parser Operation The dependency parser is initialized by pushing the words and their representations (we discuss word representations below in §3.3) of the input sentence in reverse order onto B such that the first word is at the top of B and the ROOT symbol is at the bottom, and S and A each contain an emptystack token.",47,48
8724,6278207,"Processing completes when B is empty (except for the empty-stack symbol), S contains two elements, one representing the full parse tree headed by the ROOT symbol and the other the empty-stack symbol, and A is the history of operations taken by the parser.",13,14
8725,6278207,"Processing completes when B is empty (except for the empty-stack symbol), S contains two elements, one representing the full parse tree headed by the ROOT symbol and the other the empty-stack symbol, and A is the history of operations taken by the parser.",31,32
8726,6278207,"Processing completes when B is empty (except for the empty-stack symbol), S contains two elements, one representing the full parse tree headed by the ROOT symbol and the other the empty-stack symbol, and A is the history of operations taken by the parser.",39,40
8727,17378258,"In order to make the boundaries of quotes more explicit and easy to recognize, we add a single node with the symbol ""QUOTE"" showing the boundaries of a quote, as is done for parenthesized expressions.",22,23
8728,8229552,"For control, the sequences also contained symbol strings.",7,8
8729,8229552,"Crucially, the activation was significantly stronger for letter than symbol strings of equal length.",10,11
8730,7383015,The alphabet Σ and start symbol S retain their usual interpretations.,5,6
8731,7383015,"More precisely, there is a single nonterminal (Nil), rather than a nonterminal symbol characterizing the phrase it is subsuming (Nil-the, Nil-a, etc.).",16,17
8732,3541198,"Although the structure of Θ is unspecified, in practice, most models that follow these generative stories contain a word translation table (t-table) denoted t, with each parameter t( f | e) representing the conditional probability of mapping a given source symbol e to a target symbol f .",49,50
8733,3541198,"Although the structure of Θ is unspecified, in practice, most models that follow these generative stories contain a word translation table (t-table) denoted t, with each parameter t( f | e) representing the conditional probability of mapping a given source symbol e to a target symbol f .",54,55
8734,3541198,"In particular, each diagonal entry T ii holds the probability of mapping a source symbol back onto itself, a quantity we intuitively believe should be high.",15,16
8735,3541198,"Specifically, let z n denote the missing data, where, in the parallel data setting, only the alignment is missing (z n k = a n k ) and in the non-parallel data setting, both alignment and source symbol are missing (z n 1 = (a n 1 , e n ), z n 2 = (a n 2 , f n )).",45,46
8736,3541198,"That is, C e, f k denotes the expected number of times a source-symbol type e is seen aligned to a target-symbol type f according to the posterior q k .",17,18
8737,3541198,"That is, C e, f k denotes the expected number of times a source-symbol type e is seen aligned to a target-symbol type f according to the posterior q k .",27,28
8738,1194739,"Roughly speaking, the upper-case tokens are variables, and the "":-"" symbol means that the left-hand side (the head of a rule) is implied by the conjunction of conditions on the right-hand size (the body).",17,18
8739,10692288,that was borrowed last week from Mary s0 is a non-terminal symbol the serves as a placeholder of the relative clause.,13,14
8740,10692288,"Using the example above, we replace the nonterminal symbol s0 with the second clause and obtain the Japanese sentence: watashi wa tom ga kino susume ta zasshi o kat ta .",9,10
8741,10692288,"2 ) by the Bonferroni method, where the symbol A ≫ B means ""A's improvement over B is statistically significant.""",9,10
8742,14190520,An SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data.,15,16
8743,14190520,We aim to provide a unified model where TSG rules and symbol refinement are learned from training data in a fully automatic and consistent fashion.,11,12
8744,14190520,"On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al.,",15,16
8745,14190520,"As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010) , large tree fragments and symbol refinement work complementarily for syntactic parsing.",24,25
8746,14190520,"For example, Bansal and Klein (2010) have reported that deterministic symbol refinement with heuristics helps improve the accuracy of a TSG parser.",13,14
8747,14190520,SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data.,14,15
8748,14190520,Our work differs from previous studies in that we focus on a unified model where TSG rules and symbol refinement are learned from training data in a fully automatic and consistent fashion.,18,19
8749,14190520,"We firstly review the Bayesian TSG model used in that work, and then present related work on TSGs and symbol refinement.",20,21
8750,14190520,"A TSG consists of a 4-tuple, G = (T, N, S, R), where T is a set of terminal symbols, N is a set of nonterminal symbols, S ∈ N is the distinguished start nonterminal symbol and R is a set of productions (a.k.a.",46,47
8751,14190520,It starts with a root symbol and rewrites (substi-tutes) nonterminal symbols with elementary trees until there are no remaining frontier nonterminals.,5,6
8752,14190520,"is a sequence of elementary trees used for the derivation, x = root (e) is the root symbol of e, and p (e |x ) is the probability of generating e given its root symbol x. As in a PCFG, e is generated conditionally independent of all others given x. The posterior distribution over elementary trees given a parse tree t can be computed by using the Bayes' rule: p (e |t ) ∝ p (t |e ) p (e) .",20,21
8753,14190520,"is a sequence of elementary trees used for the derivation, x = root (e) is the root symbol of e, and p (e |x ) is the probability of generating e given its root symbol x. As in a PCFG, e is generated conditionally independent of all others given x. The posterior distribution over elementary trees given a parse tree t can be computed by using the Bayes' rule: p (e |t ) ∝ p (t |e ) p (e) .",39,40
8754,14190520,Several studies have combined TSG induction and symbol refinement.,7,8
8755,14190520,"2007a ) is a sort of nonparametric Bayesian TSG model with symbol refinement, and is thus closely related to our SR-TSG model.",11,12
8756,14190520,"The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task.",2,3
8757,14190520,"As mentioned in the introduction, our model focuses on the automatic learning of a TSG and symbol refinement without heuristics.",17,18
8758,14190520,Our SR-TSG model is an extension of the conventional TSG model where every symbol of the elementary trees can be refined to fit the training data.,15,16
8759,14190520,"That is, we wish to infer the symbol subcategories of every node and substitution site (i.e., nodes where substitution occurs) from parse trees.",8,9
8760,14190520,One major issue as regards modeling an SR-TSG is that the space of the grammar rules will be very sparse since SR-TSG allows for arbitrarily large tree fragments and also an arbitrarily large set of symbol subcategories.,39,40
8761,14190520,"e |x k ∼ G x k G x k ∼ PYP d x k , θ x k , P sr-tsg (• |x k ) , where x k is a refined root symbol of an elementary tree e, while x is a raw nonterminal symbol in the corpus and k = 0, 1, . . .",37,38
8762,14190520,"e |x k ∼ G x k G x k ∼ PYP d x k , θ x k , P sr-tsg (• |x k ) , where x k is a refined root symbol of an elementary tree e, while x is a raw nonterminal symbol in the corpus and k = 0, 1, . . .",50,51
8763,14190520,is an index of the symbol subcategory.,5,6
8764,14190520,"Suppose x is NP and its symbol subcategory is 0, then x k is NP 0 .",6,7
8765,14190520,The backoff probability P sr-tsg (e |x k ) is given by the product of symbol-refined CFG (SR-CFG) rules that e contains as follows.,18,19
8766,14190520,"s c is the probability of stopping the expansion of a node labeled with c. SR-CFG rules are CFG rules where every symbol is refined, as shown in Table 1 .",24,25
8767,14190520,"The RU-CFG rule is a CFG rule where the root symbol is unrefined and all leaf nonterminal symbols are refined, as shown in Table 1 .",12,13
8768,14190520,The inference of the SR-TSG derivations corresponds to inferring two kinds of latent variables: latent symbol subcategories and latent substitution sites.,18,19
8769,14190520,"We first infer latent symbol subcategories for every symbol in the parse trees, and then infer latent substitution sites stepwise.",4,5
8770,14190520,"We first infer latent symbol subcategories for every symbol in the parse trees, and then infer latent substitution sites stepwise.",8,9
8771,14190520,"During the inference of symbol subcategories, every internal node is fixed as a substitution site.",4,5
8772,14190520,"Inference of Symbol Subcategories For the inference of latent symbol subcategories, we adopt split and merge training (Petrov et al.,",9,10
8773,14190520,"In each split-merge step, each symbol is split into at most two subcategories.",8,9
8774,14190520,"For example, every NP symbol in the training data is split into either NP 0 or NP 1 to maximize the posterior probability.",5,6
8775,14190520,"After convergence, we measure the loss of each split symbol in terms of the likelihood incurred when removing it, then the smallest 50% of the newly split symbols as regards that loss are merged to avoid overfitting.",10,11
8776,14190520,"In this work, we apply it to the training of symbol splitting.",11,12
8777,14190520,"Inference of Substitution Sites After the inference of symbol subcategories, we use Gibbs sampling to infer the substitution sites of parse trees as described in (Cohn and Lapata, 2009; Post and Gildea, 2009) .",8,9
8778,14190520,"During the inference, our sampler ignores the symbol subcategories of internal nodes of elementary trees since they do not affect the derivation of the SR-TSG.",8,9
8779,14190520,"Training and Parsing For the inference of symbol subcategories, we trained our model with the MCMC sampler by using 6 split-merge steps for the full training set and 3 split-merge steps for the small training set.",7,8
8780,14190520,"Therefore, each symbol can be subdivided into a maximum of 2 6 = 64 and 2 3 = 8 subcategories, respectively.",3,4
8781,14190520,"In each split-merge step, we initialized the sampler by randomly splitting every symbol in two subcategories and ran the MCMC sampler for 1000 iterations.",15,16
8782,14190520,This result suggests that the conventional TSG model trained from the vanilla treebank is insufficient to resolve Model F1 (≤ 40) F1 (all) TSG (no symbol refinement) Post and Gildea (2009) 82.6 - The performance of the SR-TSG parser was strongly affected by its backoff models.,30,31
8783,14190520,"This shows that an SR-TSG can use various large tree fragments depending on the context, which is specified by the symbol subcategories.",23,24
8784,14190520,Our model can be viewed as an extension of Cohn's work by the incorporation of symbol refinement.,16,17
8785,14190520,"Therefore, this result confirms that a TSG and symbol refinement work complementarily in improving parsing accuracy.",9,10
8786,14190520,"Compared with a symbol-refined CFG model such as the Berkeley parser (Petrov et al.,",3,4
8787,14190520,as latent variables and the search space is larger than that of a TSG when the symbol refinement model allows for more than two subcategories for each symbol.,16,17
8788,14190520,as latent variables and the search space is larger than that of a TSG when the symbol refinement model allows for more than two subcategories for each symbol.,27,28
8789,14190520,"Conclusion We have presented an SR-TSG, which is an extension of the conventional TSG model where each symbol of tree fragments can be automatically subcategorized to address the problem of the conditional independence assumptions of a TSG.",20,21
8790,6166308,"ITG in Chomsky normal form consists of unary production rules that are responsible for generating word pairs: X → e/f X → e/ X → /f where e is a source language word, f is a foreign language word, and means the null token, and binary production rules in two forms that are responsible for generating syntactic subtree pairs: X → [Y Z] and X → Y Z The rules with square brackets enclosing the right-hand side expand the left-hand side symbol into the two symbols on the right-hand side in the same order in the two languages, whereas the rules with angled brackets expand the left hand side symbol into the two right-hand side symbols in reverse order in the two languages.",94,95
8791,6166308,"ITG in Chomsky normal form consists of unary production rules that are responsible for generating word pairs: X → e/f X → e/ X → /f where e is a source language word, f is a foreign language word, and means the null token, and binary production rules in two forms that are responsible for generating syntactic subtree pairs: X → [Y Z] and X → Y Z The rules with square brackets enclosing the right-hand side expand the left-hand side symbol into the two symbols on the right-hand side in the same order in the two languages, whereas the rules with angled brackets expand the left hand side symbol into the two right-hand side symbols in reverse order in the two languages.",125,126
8792,17862825,"We therefore propose the Skip-node (SN) space, which represents a generalized space of tree fragments, where some nodes can be ""skipped"" or relabeled to a special symbol ' * ' that would match nodes of any label.",34,35
8793,17862825,A restriction on this space is that each skip symbol must connect two non-skip (regular) nodes.,9,10
8794,17862825,"The SN space consists of rooted ordered trees where some nodes are labeled with a special skip symbol ' * ', such that the number of regular nodes (not marked with ' * ') is at most S, and each skip node is within a distance of L from a non-skip node.",17,18
8795,14689761,"In each cell of the CKY matrix, we store the non-terminal symbol at the root of the English tree being built up.",14,15
8796,14689761,"Choose a rule r (of size s) to replace the start symbol with probability P rule (r|s, v).",13,14
8797,1465459,We extend the model in the previous section by introducing a noise symbol and state-based probability calculation.,12,13
8798,1465459,Noise-aware alignment model We introduce a noise symbol to handle partial noise in the many-to-many alignment model.,9,10
8799,1465459,"noise"" stands for the noise symbol and ""sp"" stands for a white space.",6,7
8800,2779302,"Based on inspection of incorrectly tokenized output, we therefore include a post-tokenization phase where we split all tokens that include a punctuation symbol into the individual one or two alphanumeric tokens (on either side of the punctuation symbol), and the punctuation symbol 1 .",25,26
8801,2779302,"Based on inspection of incorrectly tokenized output, we therefore include a post-tokenization phase where we split all tokens that include a punctuation symbol into the individual one or two alphanumeric tokens (on either side of the punctuation symbol), and the punctuation symbol 1 .",41,42
8802,2779302,"Based on inspection of incorrectly tokenized output, we therefore include a post-tokenization phase where we split all tokens that include a punctuation symbol into the individual one or two alphanumeric tokens (on either side of the punctuation symbol), and the punctuation symbol 1 .",47,48
8803,15842085,"Additionally let the symbol denote the empty antecedent, to which we will view x as referring when x is non-anaphoric.",3,4
8804,13933041,"Restaurant names, phone numbers, and other ""non-enumerable"" properties are abstracted -replaced by an ""X"" symbol -throughout the generation process.",22,23
8805,6187812,"A pomset mcfg G is a 7-tuple (Σ,N,O,P,F,R,S) such that Σ is a finite non-empty set of atoms, i.e. terminal symbols, N is a finite non-empty set of nonterminal symbols, where N∩Σ=∅, O is a set of valid pomset operators, P is a set of i-tuples of pomsets labelled by Σ∪N, F is a finite set of pomset rewriting functions from tuples of elements of P into elements in P, F⊆{ g:P n →P | n>0 }, R is a finite set of rewrite rules which pair n-ary elements of F with n+1 nonterminals, and S∈N is the start symbol, and d(S) = 1.",134,135
8806,6187812,"The start vertex is then labelled with the empty string, , and the end vertex is labelled with σ', a symbol not in Σ. Given a pomset p= (V p ,Σ, ,µ p ), a pomset graph for p is a vertex labelled graph γ(p) = (V γ ,E,µ γ ) where V γ and E are a finite set of vertices and edges, where V γ =V p ∪{v s ,v e } and E= ∪v s ×V min ∪V max ×v e , Σ γ =Σ∪{ ,σ'}, where σ' is a symbol not in Σ, and µ γ =µ p ∪{(v s , ),(v e ,σ')} is the vertex labelling function.",23,24
8807,6187812,"The start vertex is then labelled with the empty string, , and the end vertex is labelled with σ', a symbol not in Σ. Given a pomset p= (V p ,Σ, ,µ p ), a pomset graph for p is a vertex labelled graph γ(p) = (V γ ,E,µ γ ) where V γ and E are a finite set of vertices and edges, where V γ =V p ∪{v s ,v e } and E= ∪v s ×V min ∪V max ×v e , Σ γ =Σ∪{ ,σ'}, where σ' is a symbol not in Σ, and µ γ =µ p ∪{(v s , ),(v e ,σ')} is the vertex labelling function.",114,115
8808,8036724,"The probability distribution over initial trees is defined by using a Pitman-Yor process prior (Pitman and Yor, 1997) , that is, e |X ∼ G X G X |d X , θ X ∼ PYP (d X , θ X , P 0 (• |X )) , where X is a nonterminal symbol, e is an initial tree rooted with X, and P 0 (• |X ) is a base distribution over the infinite space of initial trees rooted with X. d X and θ X are hyperparameters that are used to control the model's behavior.",61,62
8809,8036724,An auxiliary tree includes a special nonterminal leaf node labeled with the same symbol as the root node.,13,14
8810,8036724,"2 , all the derivation information is embedded in each symbol.",10,11
8811,8036724,"That is, NP (NP (DT the) (N girl)) is a root symbol of the initial tree ""(NP (DT the) (N girl))"", which generates two child nodes: DT (DT the) and N (N girl) .",18,19
8812,454696,"3 is reduced to one of the following forms: play ( − − → kids ⊗ − −−− → games) , − − → kids (play × − −−− → games) ( − − → kids T × play) − −−− → games (4) where symbol denotes element-wise multiplication and play is a matrix.",52,53
8813,14328976,"Choose a rule r (of size s) to replace the start symbol with probability p rule (r|s, v).",13,14
8814,312883,"Formally, a bottom-up tree recognizer is defined by : 1 is a finite set of states, Figure 2 : Visualization of a bottom-up tree recognizer is a ranked alphabet, is the initial state, is a set of final states, and is a finite set of transitions from a vector of states to one state that reads a -ary symbol.",67,68
8815,312883,This is the key difference in visualization between string and tree automata -to capture the arity of the symbol being read we must visualize the automata as an ordered hypergraph.,18,19
8816,312883,"Formally, a bottom-up tree-to-weight transducer is defined by where , , , and are defined as for recognizers, and: is a finite set of transitions from a vector of states to one state, reading a -ary symbol and outputting some weight is the initial weight function mapping to is the final weight function mapping to .",46,47
8817,15615944,"For example, if we extract features only from the root symbol of s 0 , each state looks the same as a span of PCFGs.",11,12
8818,15615944,"This is similar to the hierarchical A* for PCFGs with multilevel symbol refinements (Pauls and Klein, 2009) .",12,13
8819,196181767,"It refines the attention for an output symbol by integrating that of each head, and consists of two hops.",7,8
8820,196181767,"In this paper, we define a hop as a computational step which could be performed for an output symbol many times.",19,20
8821,196181767,"By ""multi-hop attention"", we mean that some kind of attention is calculated many times for generating an output symbol.",23,24
8822,14333463,"4 , rewrites a nonterminal symbol into a graph.",5,6
8823,15371894,"In what follows, the starting point of the random walk calculation is indicated at the left side of the arrow symbol; i.e., P (s → t; π) denotes the probability of reaching t from s computed using forward random walks, and P (t ← s; π) denotes the same probability, computed in a backward fashion.",21,22
8824,9531504,"Evaluation and Analysis Experimental Setup CCGbank is a translation of the Penn Treebank into a corpus of CCG derivations (Hockenmaier All experiments were performed using automatically assigned POS-tags that are generated by a symbol-refined generative HMM tagger 1 (Huang et al.,",36,37
8825,207847212,"When the encoder observes the entire source sentence, a special symbol <eos> was feed into the encoder, and the decoder continue to generate spectrogram word by word.",11,12
8826,2554349,This increase coverage at the expense of increasing data sparsity as the non-terminal symbol set increases dramatically.,15,16
8827,2554349,Decorated non-terminals in rules must match the constituent span in the input sentence but the undecorated X symbol can match any span.,19,20
8828,2554349,"Where there is no constituency spans, the default symbol X is used to denote an undecorated non-terminal.",9,10
8829,7787351,This technique can be used for any language but its disadvantage is that there are so many linguistically meaningless candidates that it takes too long to calculate the probabilities of all combinations of the EN JP CN KR alphabet symbol number alphabet symbol number kanji hiragana katakana alphabet symbol number kanji alphabet symbol number kanji hangul candidates in the following analytical process.,39,40
8830,7787351,This technique can be used for any language but its disadvantage is that there are so many linguistically meaningless candidates that it takes too long to calculate the probabilities of all combinations of the EN JP CN KR alphabet symbol number alphabet symbol number kanji hiragana katakana alphabet symbol number kanji alphabet symbol number kanji hangul candidates in the following analytical process.,42,43
8831,7787351,This technique can be used for any language but its disadvantage is that there are so many linguistically meaningless candidates that it takes too long to calculate the probabilities of all combinations of the EN JP CN KR alphabet symbol number alphabet symbol number kanji hiragana katakana alphabet symbol number kanji alphabet symbol number kanji hangul candidates in the following analytical process.,48,49
8832,7787351,This technique can be used for any language but its disadvantage is that there are so many linguistically meaningless candidates that it takes too long to calculate the probabilities of all combinations of the EN JP CN KR alphabet symbol number alphabet symbol number kanji hiragana katakana alphabet symbol number kanji alphabet symbol number kanji hangul candidates in the following analytical process.,52,53
8833,7787351,"In Japanese, a space is always recognized as one word, a symbol.",13,14
8834,7787351,"Let the morpheme sequence be (1) if 1 − ≠ i i NC NC ) , | ( ) , | ( ) , ( 1 1 1 − − − × = i i i i i i i i NC NC w P w NC NC P NC w P (2) if 1 − = i i NC NC and 1 + = i i NC NC ) , | ( ) , ( 1 i i i i i NC w w P NC w P − = (3) if 1 − = i i NC NC and 1 + ≠ i i NC NC ) , | ( ) , | ( ) , ( 1 i i i i i i i NC w end P NC w w P NC w P > < × = − Here, the special symbol > < end indicates the end of an NC sequence.",153,154
8835,10074346,Hashtags were annotated according to the text following the # symbol.,10,11
8836,10074346,"If the word was a Twitter user handle, URL, emoticon, symbol or punctuation, it assigned the Other category.",13,14
8837,10074346,">, @____)), and the {symbol} tokens should all be labeled as 'None of the above'.",13,14
8838,8505552,"If strings are generated by the character unigram model, the sum of the probabilities of all length k strings equals to the probability of the event that the end of word symbol <eow> is selected after a character other than <eow> is selected k -1 times.",32,33
8839,8505552,This symbol typically appears at the end of transliterated Western origin words written in katakana.,1,2
8840,202718954,"In the sentence feature, the mention span is replaced by a special symbol.",13,14
8841,20584726, dates are converted to a symbol.,6,7
8842,20584726, all ratios are replaced by a special symbol.,8,9
8843,20584726,"Most messages have a cashtag, which is a stock symbol, such as $aapl, to specify the entity (stock) this message is about.",10,11
8844,20584726," all mentions are converted to a special symbol, for privacy reason.",8,9
8845,20584726," all cashtags are replaced by a special symbol, to avoid cashtags to gain a polarity value related to a particular time period.",8,9
8846,8677917,The symbol represents an elementwise multiplication.,1,2
8847,216562811,"The process of reading and returning a new source token is denoted by READ(), and expression x • x represents to append an element x to the end of sequence x. We denote by <s> and </s> the start symbol and end symbol of a sequence.",46,47
8848,216562811,"The process of reading and returning a new source token is denoted by READ(), and expression x • x represents to append an element x to the end of sequence x. We denote by <s> and </s> the start symbol and end symbol of a sequence.",49,50
8849,33705584,"Most messages have a cashtag, which is a stock symbol, such as $aapl, to specify the entity (stock) this message is about.",10,11
8850,33705584,"-All mentions are converted to a special symbol, for privacy reason.",7,8
8851,33705584,"-All cashtags are replaced by a special symbol, to avoid cashtags to gain a polarity value related to a particular time period.",7,8
8852,33705584,"These symbols reflect the value range of these numbers, and the range of the number determines which symbol it will be converted to.",18,19
8853,6971327,For each node n in the AMR tree we create a corresponding node m with the all-purpose symbol 'X' in the SBMT-like tree.,19,20
8854,15095390,A simple and sufficient way to ensure proper normalization of the model is to decompose the sentence probability according to the chain rule and make sure that the end-of-sentence symbol </S> is predicted with non-zero probability in any context.,33,34
8855,195218547,"In the final translation model, a special ""domain"" symbol is added to indicate which domain the source sentence belonging to.",11,12
8856,195218547,We illustrated that adding a domain symbol in source sentence improves the robustness of the model.,6,7
8857,195218547,"To differentiate the data from different domain, we use different start symbol in source side.",12,13
8858,195218547,"Similar to previous domain sensitive method, we include out-of-domain clean data during the training of this noisy translation model and differentiate them by different start symbol int target side.",30,31
8859,195218547,"The domain insensitive method simply mix the clean and noisy dataset in training while the domain sensitive method differentiate the clean and noisy dataset in target side by starting with different symbol (e.g. < clean s >, < noisy s >).",31,32
8860,195218547,"2018a) to differentiate multiple tasks, we simply assign different starting symbol for multiple tasks (Lample et al.,",12,13
8861,195218547,We also distinguish the domain of source sentences by inserting a domain symbol into source sentences.,12,13
8862,52118537,"Therefore, it becomes easier for the search algorithm to find the </eos> symbol.",15,16
8863,52118537,"The beauty of this method appears when we drop the logarithmic symbol in (12): exp( Ŝbp (x, y)) = bp • |y| i=1 p(y i |...) 1/|y| = bp •exp 1 |y| |y| i=1 log p(y i |...) which is in the same form of BLEU score (3).",11,12
8864,219307940,"Arithmetic coding separates modeling and compression, making our job similar to language modeling, where we use try to use context to predict the next symbol.",26,27
8865,1875724,"An example rule of the synchronous grammar is r 8 : X → X 1 qian X 2 , X 2 before X 1 , where the left-hand-side is a non-terminal symbol, and the right-hand-side is a pair of source and target strings, each of which consists a sequence of lexical terminals and non-terminals.",37,38
8866,4319940,"Notice that in conventional incremental parsing, the stack stores the subtrees  k Some text and the symbol or scaled j .",18,19
8867,4319940,"But different from Cross and Huang (2016) , after a structural action, we choose to keep the last branching point k, i.e., i Some text and the symbol or scaled k j (mostly for combine, but also trivially for shift).",32,33
8868,4319940,"This midpoint k disappears after a label action; therefore we can use the shape of the last span on the stack (whether it contains the split point, i.e., i xt and the symbol or scaled k j or i Some text and the symbol or scaled j ) to determine the parity of the step and thus no longer need to carry the step z in the state as in Cross and Huang (2016) .",36,37
8869,4319940,"This midpoint k disappears after a label action; therefore we can use the shape of the last span on the stack (whether it contains the split point, i.e., i xt and the symbol or scaled k j or i Some text and the symbol or scaled j ) to determine the parity of the step and thus no longer need to carry the step z in the state as in Cross and Huang (2016) .",47,48
8870,2506060,2 We define the language L(G) produced by an SCFG G as the pairs of terminal strings produced by rewriting exhaustively from the start symbol.,25,26
8871,2506060,"We view each left-hand side subtree as a monolithic nonterminal symbol and factor each transducer rule into two SCFG rules: one from the root nonterminal to the subtree, and the other from the subtree to the leaves.",12,13
8872,218487602,"For simplicity, we design where pad is a padding symbol in case b is shorter than a. Experiments Datasets and Implementation We evaluate our work on Chinese-to-English and English-to-Chinese simultaneous translation tasks.",10,11
8873,797950,"The two most dissimilar types (the last to be included in any existing cluster) were the symbol ""@"" and the question mark.",18,19
8874,173611,"These classes are: • SHIFT: a shift action is taken; • REDUCE-UNARY-XX: a unary reduce action is taken, and the root of the new subtree pushed onto S is of type XX (where XX is a non-terminal symbol, typically NP, VP, PP, for example); • REDUCE-LEFT-XX: a binary reduce action is taken, and the root of the new subtree pushed onto S is of non-terminal type XX.",49,50
8875,219966609,"The decoder in the NMT model acts as a conditional language model that operates on a shifted copy of y, i.e., sos , y 0 , ..., y |y|−1 where sos is a start symbol of a sentence and representations of x learned by the encoder.",37,38
8876,8563794,"We further require one of the NPs to end (ensured by the $ symbol) with a chemical, and the other (the disease) to end with a MeSH term from the C subtree.",14,15
8877,552136,The symbol * indicates a wildcard position where we allow up to three intervening words.,1,2
8878,51872928,"This is all but trivial, particularly due to the fact that the creation of a discussion is solely done by the users; although Wikipedia describes the required format of the different parts of a discussion in detail, not all users follow the format, often forgetting required symbols or mistakenly confusing a symbol with another one.",55,56
8879,225066984,"Therefore, we adopt the terminology of quasi-synchronous context-free grammars (Smith and Eisner, 2006) , or QCFGs, to refer to our induced grammar G. 5 Our grammar G contains a single non-terminal symbol, N T .",42,43
8880,225066984,"For T5-Base on the GEO-QUERY TMCD split, in 52 of the 201 incorrect predictions (26%), the first incorrectly predicted symbol occurs when the gold symbol has 0 probability under a trigram language model fit to the training data.",28,29
8881,225066984,"For T5-Base on the GEO-QUERY TMCD split, in 52 of the 201 incorrect predictions (26%), the first incorrectly predicted symbol occurs when the gold symbol has 0 probability under a trigram language model fit to the training data.",33,34
8882,225066984,"We write SCFG rules as: S → α, β Where S is a non-terminal symbol, and α and β are strings of non-terminal and terminal symbols.",18,19
8883,225066984,16  SPLIT can be implemented by considering pairs of sub-strings in α and β to replace with a new indexed non-terminal symbol.,26,27
8884,220047209,"At each output step, the decoder generates a distribution over possible actions, including selecting a symbol from the output vocabulary, and copying a token from the input x. We only allow copying of certain token types, and mask out invalid copying actions, including independent wordpieces from u and TAB and column-type tokens.",17,18
8885,160010264,"Generating Symbols We can generate a symbol, denoted Generate[i], P (a j =Generate[i] | x, y 1:j−1 ) ∝ exp(z j w out i ), (7) where w out i is a learned embedding vector for the element ∈ V out with index i. If a j = Generate[i], then y j will be the element ∈ V out with index i. Copying Entities We can also copy an entity candidate, denoted CopyEntity[i], P (a j = CopyEntity[i] | x, y 1:j−1 ) ∝ exp((z j W e ) w (Nenc) e i ), (8) where W e is a learned matrix, and i ∈ {1, . . . ,",6,7
8886,160010264,"When copying tokens, the decoder determines the type of the entity using an additional output symbol.",16,17
8887,2916543,"Let G = (V, Σ, R, S) be the formal grammar with non-terminals V , terminal vocabulary Σ, productions R and start symbol S. E represents a source sentence, and D, a formal derivation tree for that sentence.",30,31
8888,2916543,"Given a search node with a non-terminal as its first symbol on the stack, we expand with any production for that symbol, putting its yield onto the stack and updating the node cost to include its derivation score: [X, α] , p (X → β) ∈ R [β, α] , p + log P (X → β|E) If the first stack item is a terminal, it is scanned: [a, α] , p a ∈ Σ [α] , p Using these inference rules, we utilize a simple greedy approach that only accounts for the productions included in the derivation.",12,13
8889,2916543,"Given a search node with a non-terminal as its first symbol on the stack, we expand with any production for that symbol, putting its yield onto the stack and updating the node cost to include its derivation score: [X, α] , p (X → β) ∈ R [β, α] , p + log P (X → β|E) If the first stack item is a terminal, it is scanned: [a, α] , p a ∈ Σ [α] , p Using these inference rules, we utilize a simple greedy approach that only accounts for the productions included in the derivation.",24,25
8890,2916543,The completion-cost estimate for a given symbol is defined recursively.,8,9
8891,11202498,"In this paper, we propose a more fine-grained model named TransD, which is an improvement of TransR/CTransR. In TransD, we use two vectors to represent a named symbol object (entity and relation).",34,35
8892,11202498,"In the past decade, much work based on symbol and logic has been done for knowledge graph completion, but they are neither tractable nor enough convergence for large scale knowledge graphs.",9,10
8893,11202498,"TransD Model In TransD, each named symbol object (entities and relations) is represented by two vectors.",7,8
8894,11202498,"TransD has less complexity and more flexibility than TransR/CTransR. When learning embeddings of named symbol objects (entities or relations), TransD considers the diversity of them both.",16,17
8895,4421747,"For tracks 1-6, the top ranking result is marked with a • symbol and results in bold have no statistically significant difference with the best result on a track, p > 0.05 Williams' t-test (Diedenhofen and Musch, 2015) .",15,16
8896,43939290,"If we predict a multi-sentence graph, but there is no punctuation symbol that splits sentences, we post-process the graph and transform the root node into an AND node.",14,15
8897,1889321,"If an elementary tree has exactly one terminal symbol, that symbol is called its lexical anchor.",8,9
8898,1889321,"If an elementary tree has exactly one terminal symbol, that symbol is called its lexical anchor.",11,12
8899,4718302,The symbol ⊕ means concatenation.,1,2
8900,2851495,Let U be the set of units in the corpus and let r S (u) be the relation with unit u as a base in the system output or a special no-relation-symbol ⊥ if no such relation exists.,37,38
8901,6955865,For example the above plaintext could be enciphered to ABCDECF when using the homophonic substitution a b c d A 1 0 0 0 B 0 1 0 0 C 1 0 0 0 D 0 0 0 1 E 0 0 1 0 F 0 1 0 0 We will use the definition n max = max e f s(e|f ) (2) to characterize the maximum number of different cipher symbols allowed per plaintext symbol.,77,78
8902,6955865,"Here, the function EXT ORDER chooses which cipher symbol is used next for extension, EXT LIMITS decides which extensions are allowed, and SCORE scores the new partial hypotheses.",9,10
8903,6955865,"Extension Order (EXT ORDER) For the choice which ciphertext symbol should be fixed next during search, several possibilities exist: The overall goal is to choose an extension order that leads to an overall low error rate.",11,12
8904,6955865,In each step it greedily chooses the symbol that will maximize the number of fixed ciphertext n-grams.,7,8
8905,6955865,We create the ciphertext using a 1:1 substitution cipher in which we fix the mapping of the space symbol ' '.,18,19
8906,6955865,Note that fixing the ' ' symbol makes the problem much easier: The exact methods show much higher computational demands for lengths beyond 256 letters when not fixing the space symbol.,6,7
8907,6955865,Note that fixing the ' ' symbol makes the problem much easier: The exact methods show much higher computational demands for lengths beyond 256 letters when not fixing the space symbol.,31,32
8908,6955865,For the cipher length of 32 we obtain a symbol error rate of just 4.1% where the optimal solution (i.e. without search errors) for a 3-gram language model has a symbol error rate as high as 38.3%.,9,10
8909,6955865,For the cipher length of 32 we obtain a symbol error rate of just 4.1% where the optimal solution (i.e. without search errors) for a 3-gram language model has a symbol error rate as high as 38.3%.,35,36
8910,6955865,"To summarize: Our final decipherment-for which we only use a 6-gram language model-has a symbol error rate of only 2.0%, which is slightly better than the best decipherment reported in (Ravi and Knight, 2011a) .",21,22
8911,6955865,They used an n-gram language model together with a word dictionary and obtained a symbol error rate of 2.2%.,16,17
8912,6955865,Their final decipherment has a symbol error rate of 7.8%.,5,6
8913,6955865,Our algorithm obtains 6.0% symbol error rate.,5,6
8914,6955865,It should be noted that the improvements of 1.8% symbol error rate correspond to a larger improvement in terms of mapping error rate.,10,11
8915,6955865,This can also be seen when looking at Table 3 : An improvement of the symbol error rate from 6.51% to 5.96% corresponds to an improvement of mapping error rate from 21.78% to 19.40%.,15,16
8916,10092658,"pm , q ), which is a transition from the state q to the state q , with the source symbol w and producing the substrings (p 1 , . . . ,",21,22
8917,10092658,Each extended symbol consists of a token from the source language plus zero  or more words from each target language in their turn.,2,3
8918,6665740,where J is the length of the input sentence and $ is a symbol denoting the sentence end.,13,14
8919,613292,We use the symbol Pr(.),3,4
8920,613292,"In contrast, for model-based probability distributions, we use the generic symbol p(.).",14,15
8921,506063,"The left-hand side of the rule is a non-terminal symbol X ∈ N , and the ∼ relation denotes a one-to-one correspondence between the non-terminals in α and in β.",13,14
8922,506063,"Indexing α with j, i.e. the symbol α j , 1 ≤ j ≤ J α , denotes the j-th terminal symbol on the source side of the phrase pair α, β , and analogous with β i .",7,8
8923,506063,"Indexing α with j, i.e. the symbol α j , 1 ≤ j ≤ J α , denotes the j-th terminal symbol on the source side of the phrase pair α, β , and analogous with β i .",24,25
8924,19368200,"Grammatical information covers grammatical class, selective categories and inflection pattern symbol.",11,12
8925,7996502,In all three languages numbers and URLs were replaced by a category symbol.,12,13
8926,681862,The symbol # stands for the whole set of parameters to be estimated.,1,2
8927,227230345,"The order of the annotations separated by the pipe symbol remained the same for the entire line, meaning that the order of annotations in pipe-separated columns is always the same.",9,10
8928,16313623,"The left-hand side of the rule is a non-terminal symbol X ∈ N , and the ∼ relation denotes a oneto-one correspondence between the non-terminals in α and in β.",13,14
8929,16313623,"Indexing α with j, i.e. the symbol α j , 1 ≤ j ≤ J α , denotes the j-th terminal symbol on the source side of the phrase pair α, β , and analogous with β i , 1 ≤ i ≤ I β , on the target side.",7,8
8930,16313623,"Indexing α with j, i.e. the symbol α j , 1 ≤ j ≤ J α , denotes the j-th terminal symbol on the source side of the phrase pair α, β , and analogous with β i , 1 ≤ i ≤ I β , on the target side.",24,25
8931,11644259,We use the symbol Pr(.),3,4
8932,11644259,"In contrast, for modcl-t)ased prol)-ability distributions, we use the generic symbol v(.).",14,15
8933,15946485,Stack items are labeled with either a preterminal symbol or a nonterminal (a syntactic category).,8,9
8934,15946485,"The reduce(n, cat) action pops n stack symbols (n ≥ 1), makes them children of a new symbol labeled cat, and pushes that symbol on the stack.",22,23
8935,15946485,"The reduce(n, cat) action pops n stack symbols (n ≥ 1), makes them children of a new symbol labeled cat, and pushes that symbol on the stack.",29,30
8936,15946485,The SR parser terminates when the input is consumed and the stack contains the special symbol top.,15,16
8937,15946485,"The stack is popped, one input symbol is dequeued, and the popped stack item is replaced in the tree by the input item. (",7,8
8938,15946485,"Each feature is a node label, either a non-terminal (POS tag) or a terminal symbol (syntactic category).",19,20
8939,4358268,"For example, to construct simple bilexical and bi-POS inference rules to model the dependency of an adjacent head and a modifier, one can write firstorder clauses such as:  Here, we associate a feature vector φ c with each clause, which is annotated using the # symbol after each clause in the theory set.",53,54
8940,6902727,"Numbers are replaced with a single category symbol in a separate preprocessing step and we apply the long-range part-of-speech based reordering rules proposed by (Popović and Ney, 2006) .",7,8
8941,3160983,"The unigram language model S f is defined as S f = log p(f ) (8) which implies that (a) S f are real numbers with S f ∈ [−∞, 0] (9) (b) The following normalization constraint holds: f ∈V f exp(S f ) = 1 (10) Similarly for the bigram language model matrix S f f , we define S f f = log p(f |f ) (11) This definition implies that (a) S f f are real numbers with S f f ∈ [−∞, 0] (12) (b) For the sentence boundary symbol, it holds that S $$ = −∞ (13) (c) For all f ∈ V f the following normalization constraint holds: f ∈V f exp(S f f ) = 1 (14) 4 Decipherment Using Unigram LMs Problem Definition When using a unigram language model, Equation 2 simplifies to finding φ = arg max φ N i=1 p(φ(f i )) (15) which can be rewritten as φ = arg max φ f ∈V f N f S φ(f ) (16) When defining c f f = N f log p(f ), for f, f ∈ V f , Equation 16 can be brought into the form of φ = arg max φ f ∈V f c f φ(f ) (17) Figure 1 shows an illustration of this problem.",117,118
8942,6875913,"Using this example we describe the strategies we used for special cases in the transformation from Figure 1 (b) to Figure 1(c): • ignore the unaligned target word, e.g. e 1 • the unaligned source word should follow its preceding word, the unaligned feature is kept with a * symbol, e.g. f * 2 is after f 1 • when one source word is aligned to multiple target words, only keep the alignment that links the source word to the first target word, e.g. f 4 is linked to e 5 and e 6 , only f 4 − e 5 is kept.",54,55
8943,13753208,"Frequent substrings will be joined early, resulting in common words remaining as one unique symbol.",15,16
8944,13753208,"One downside is, however, that BPE is based on a greedy and deterministic symbol replacement, which can not provide multiple segmentations with probabilities.",15,16
8945,13753208,"According to Shannon's coding theorem, the optimal code length for a symbol s is − log p s , where p s is the occurrence probability of s. This is essentially the same as the segmentation strategy of the unigram language model described as (7) .",13,14
8946,13753208,"In this model, out-ofvocabulary words are not collapsed into a single UNK symbol, but converted into the sequence of characters with special prefixes representing the positions in the word.",15,16
8947,12501197,"γ ŷ,M ), γ ŷ,y = β + τ δ y,ŷ , where δ y,ŷ is the Kronecker symbol, β and τ are nonnegative scalar parameters.",27,28
8948,2224090,The probability function for P r (and by analogy P l ) is now: The intuition behind this approach is that the model will learn that the stop symbol is more likely to follow phrases with many sisters.,30,31
8949,13140545,"The remaining productions allow all possible binary structures, similar to dependency-style grammars for unsupervised parsing (Carroll and Charniak, 1992) : • PoS → w • PoS → PoS Y | Y PoS | PoS Y | Y PoS where PoS is a part-of-speech symbol, w is a terminal symbol and Y is any non-terminal in the grammar.",53,54
8950,13140545,"The remaining productions allow all possible binary structures, similar to dependency-style grammars for unsupervised parsing (Carroll and Charniak, 1992) : • PoS → w • PoS → PoS Y | Y PoS | PoS Y | Y PoS where PoS is a part-of-speech symbol, w is a terminal symbol and Y is any non-terminal in the grammar.",59,60
8951,272757,This step is repeated at each iteration to constrain the search for all possible symbol sequences observed in the training corpus. •,14,15
8952,272757,"The most used measure for the interdependence of two events is the mutual information MI(z,y) = log P¢~4,1 However, in this experi- P(z)P(y) "" ment, we use a correlation coefficient that has provided the best convergence speed for the optimization procedure: P(~' Y) (1) P~'~ -P(z) + P(y) where P(z) is the probability of symbol z. The coefficient p~,y (0 _< p~,~ _< 0.5) is easily extended to define Pz~,z2 ..... z. for the n-tuple (xl, x2 ..... zn) (0 _< p~, ......... _< l/n).",68,69
8953,272757,"In MI(z,y) are such that P(z,y) _ lThe perplexity PP(T) of a corpus 7"" is PP(T) = exp(-~ log P(T)), where n is the number of words in T. :aWe ranked symbol pairs and increased the phrase length by successive iteration.",44,45
8954,272757,An additional speed up to the algorithm could be gained by ran.king symbol k-tuple$ (k > 2) at each iteration.,12,13
8955,272757,The whole symbol space is then given by V = Ui v/ as shown in Figure 6 and the problem of learning 193 the best feature set is then decomposed into two subproblems: to find the oplimalsubset of V (first learning strategy) that gives us the best features (second learning strategy) generated by a given set V/. v0 Figure 6 : The sequence of symbol sets V/ generated by successive clustering steps.,2,3
8956,272757,The whole symbol space is then given by V = Ui v/ as shown in Figure 6 and the problem of learning 193 the best feature set is then decomposed into two subproblems: to find the oplimalsubset of V (first learning strategy) that gives us the best features (second learning strategy) generated by a given set V/. v0 Figure 6 : The sequence of symbol sets V/ generated by successive clustering steps.,69,70
8957,272757,"However, training context dependent probabilities as shown in Equation 7 delivers a stochastic separation between the correct and incorrect phrases: P(A T and T) logp(A T but T)=5""7 (8) Given a set of phrases containing terminal and non terminal symbols, the goal of large vocabulary stochastic language modeling for speech recognition and understanding is to assign a probability to all terminal symbol sequences.",66,67
8958,6098385,a source symbol (a word from the source language vocabulary); 2.,2,3
8959,6098385,-Substitution: Each arc in the word graph is treated as a substitution edge whose edit cost is proportional to the levenshtein distance between the symbol associated with this arc and the word prefix employed to traverse this arc during the search.,25,26
8960,6098385,This substitution cost is zero when the word prefix matches the symbol in the word graph arc.,11,12
8961,6835107,"Any sequence of one or more numbers was replaced by the symbol ""[number]"".",11,12
8962,6835107,"For the Commit act only, references to the first person were removed from the symbol [person]i.e., [",15,16
8963,235645,"We use the symbol GRU with different subscripts to represent GRU functions using different sets of parameters (for example, we used the enc f and enc b subscripts to denote the parameters of the forward and backward word-level encoder units.)",3,4
8964,235645,"For example, a three word input sentence where the last term is out-of-vocabulary will be represented as the following vector of embeddings in the word-level model: x = (x 1 , x 2 , x 3 ), where x 3 would be the embedding for the UNK symbol.",57,58
8965,257013,"An interpretation specifies which objects, functions and relations in the domain are represented by which symbol.",16,17
8966,52112703,"By randomly replacing a particular word in the question sentence with a blank symbol, a single test case is created.",13,14
8967,8752606,"In this case, symbol Γ makes reference only to a set of components.",4,5
8968,221097218,"The encoder maps an input sequence of symbol representations (x 1 , . . . ,",7,8
8969,4406304,"We were able to extract this information from our analyzer and we represented it in the features, using a special symbol in place of the elliptic word.",21,22
8970,246567,"Such a formulation leads to a formally syntax-based translation approach, where translation is viewed as a parallel parsing problem over a grammar with one non-terminal symbol.",30,31
8971,237010903,The MAs are indicated by the symbol ' '.,6,7
8972,10564228,"For example, two trees in Figure 1(a) sharing the same structure except for one terminal symbol are deemed at most 67% similar by the conventional tree kernel (PTK) (Moschitti, 2006 ).",17,18
8973,10564228,"where term(n) is the terminal symbol under n; Rule 3: Else if n 1 and n 2 have the same labels and they are not both pre-terminals, then: C(n 1 , n 2 ) = 1 + n i ∈children(n 1 ) n j ∈children(n 2 ) C(n i , n j ) where children(m) are the child nodes of m. As in other tree kernel approaches (Collins and Duffy, 2001; Moschitti, 2006) , we use a discount parameter λ to control for the disproportionately large similarity values of large tree structures.",6,7
8974,2506085,"Therefore, in the search algorithm, the end-of-sentence symbol Model Iter ¡ 0.0 0.2 0.4 0.6 0.8 1.0 baseline 0 13.0 13.1 13.1 13.1 13.0 13.4 13.7 PA 0 13.0 13.1 13.1 12.9 12.9 13.1 13.7 OP 0 12.8 12.7 12.8 12.8 12.7 13.1 13.7 OP+PA 0 13.1 13.3 12.9 13.0 12.9 13.1 13.7 h-2 0 12.5 12.7 12.5 12.6 12.9 13.2 13.7 h-2+ PA 0 12.7 12.8 13.0 12.7 12.7 13.0 13.7 h-2+ OP 0 12.3 12.3 12.4 12.6 12.7 12.8 13.7 h-2+ OP+ PA 0 12.6 12.6 12.4 12.5 12.7 12.9 13.7 Table 4 : N-best re-scoring WER(%) results can be predicted before the parse of the sentence is ready for completion 3 , thus completing the parse with a series of special CONSTRUCTOR moves (see (Chelba and Jelinek, 2000) for details).",13,14
8975,5817898,"To do that, we need one symbol for each word in the source and target languages, another symbol to separate the source and target sides in a phrase pair, and one additional symbol to distinguish between the different pairs in the phrase lexicon.",7,8
8976,5817898,"To do that, we need one symbol for each word in the source and target languages, another symbol to separate the source and target sides in a phrase pair, and one additional symbol to distinguish between the different pairs in the phrase lexicon.",19,20
8977,5817898,"To do that, we need one symbol for each word in the source and target languages, another symbol to separate the source and target sides in a phrase pair, and one additional symbol to distinguish between the different pairs in the phrase lexicon.",35,36
8978,5817898,"Assuming a uniform distribution over the K different symbols, each symbol would require − log 2 ( 1 K ) bits to encode.",11,12
8979,26917344,"Finally, the following DP recursion equations allow us to retrieve the path of maximum score in such a search graph: Q(∅, """") = 0 Q(N e , y e ) = max y∈Σ∪{$}: ∀(N p,yp ), y e =y p y N e=N p ∪ Θ(y p ,y) B(y e , E P (y |x) [# w (y )]) Q = max ∀(N p,yp ) ŷ=y p $ B(ŷ, E P (y |x) [# w (y )]) where the end-of-sentence symbol, $, denotes a complete translation, and function Θ(y p , y) returns the new n-grams generated when expanding hypothesis y p with word y. For example, given the hypothesis y p =""we are faced with"" and the expansion word y=""enormous"", the expanded hypothesis y e =""we are faced with enormous"" contains four 2 n-grams more than y p : ""enormous"", ""with enormous"", ""faced with enormous"", and ""are faced with enormous"".",116,117
8980,8547006,To fix our example we could have modified our instructions to use the final symbol in Table 2 .,14,15
8981,2955580,Each sentence terminates with a special end-of-sentence symbol EOS.,11,12
8982,7287895,Each sentence concludes with a special end-of-sentence symbol EOS.,11,12
8983,11430034,"Our similarity modelling may also suggest approaches for more general symbol systems that lack adequate indexing schemes, for example heraldry.",10,11
8984,8884428,"Previous Utterance, q User Utterance, q Transition Update Action, a inserting page numbers q t 1 add a background ti → t¬i 2, reset-T, reset how to number pages q t 2 insert numbers on pages in margin ti → si.j 1.4, append-G, append page numbers q t 3 set a page number in a footer ti → ai.j.z 1.2.1, append-A, append page number a document q t 4 insert a comment ti → g¬i.j 21.1, reset-G, reset page number q t 5 add a comment ""redo"" ti → a¬i.j.z 21.2.1, reset-A, reset page x of y q g 1 add a border gi.j → t¬i 6, reset-T, reset format page x of x q g 2 enter text and page numbers gi.j → gi.¬j 1.1, override-G, override enter page x of y q g 3 page x of y in footer gi.j → ai.j.z 1.3.1, append-A, append inserting page x of y q g 4 setting a default font gi.j → g¬i.j 6.1, reset-G, reset showing page x of x q g 5 set default font and style gi.j → a¬i.j.z 6.4.1, reset-A, reset page numbers bottom q a 1 make a degree symbol ai.j.z → t¬i 13, reset-T, reset numbering at bottom page q a 2 insert page numbers ai.j.z → gi.¬j 1.1, override-G, override insert footer page numbers q a 3 page number design ai.j.z−1 → ai.j.z 1.2.2, append-A, append headers page number q a 4 comments in document ai.j.z → g¬i.j 21.1, reset-G, reset page number in a footer q a 5 changing initials in a comment ai.j.z → a¬i.j.z 21.2.1, reset-A, reset Table 3 : Example q and q queries for append, override and reset dialog state updates.",239,240
8985,855149,"A special STOP symbol is introduced (the n + 1 th and m + 1 th modifiers), which is generated when there are no more modifiers.",3,4
8986,11312031,"In order to keep the number of parameters low, we used the following model: F(class~llal2...ln) = ~F(class~llx ) n + ~ °tn-iF(cclassj[lll2""""li) i=2 where a, ~ E (0, 1), ~ having a small value The symbol F is used instead of P since we have raw distributions (frequencies) and a normalization step is needed to compute the final probability distribution.",53,54
8987,5664162,"The punctuation feature, P u(C 0 ), checks whether C 0 is a punctuation symbol (such as ""?"", ""-"", "","").",16,17
8988,17830435,"A Context Free Grammar (CFG) consists of (i) a set of terminals; (ii) a set of nonterminals {N k }; (iii) a designated start symbol ROOT; and (iv) a set of rules, {r = N i → ζ j }, where ζ j is a sequence of terminals and nonterminals.",35,36
8989,108462,The interface for alphabets is defined using a single constant: for each label index an alphabet reports it must ensure to always deliver the same symbol on request through getSymbol().,26,27
8990,108462,Mapping of arc labels is necessary as symbol indices may differ between alphabets.,7,8
8991,108462,"For technical reasons, -labels are represented by a $ symbol.",10,11
8992,18454506,"Rather than learning relations from a bilingual corpus, GIATI converts every sentence pair into an extended-symbol string, which is directly derived from a monotonous bilingual segmentation, in order to, straight afterwards, infer a language model from.",18,19
8993,18454506,"Every phrase pair has therefore to be considered only one symbol in a language modelling application, which will be put into practice through the estimation of n-gram backoff models together with their corresponding representation under a finite state framework.",10,11
8994,7829066,"p($ | e, e ) is the trigram language probability for predicting the sentence boundary symbol $.",16,17
8995,7829066,"Here, $ is the sentence boundary symbol, which is thought to be at position 0 in the target sentence.",7,8
8996,7829066,"The search starts with S = {($, $, {∅}, 0)}, where $ denotes the sentence start symbol for the immediate two predecessor words and {∅} denotes the empty coverage set, in which no source position is covered yet.",26,27
8997,14225429,This corpus can be written in matrix form where symbol indicates the transpose of a matrix or vector: X =    x 1 . . .,9,10
8998,13936127,"As a simple example, the #gram string kernel maps each string £ a X to a vector with dimensionality X and each element in the vector indicates the number of times the corresponding symbol from X occurs in £ .",35,36
8999,13936127,It improves on the # -gram kernel by better handling cases with repeated occurrences of the same symbol.,17,18
9000,13936127,"More specifically, the Repetition kernel is defined such that its vector space consists of all sequences from X composed of the same symbol.",23,24
9001,13936127,"2 serves to discount features for the occurrence of gaps, and 2 £ discounts longer symbol sequences.",16,17
9002,13936127,"In the following examples, the node labels are abbreviated as well; is a special symbol for end of path and e is a special symbol for start of path.",16,17
9003,13936127,"In the following examples, the node labels are abbreviated as well; is a special symbol for end of path and e is a special symbol for start of path.",26,27
9004,10776196,"Finite state models A stochastic finite-state automaton A is a tuple (Γ, Q, i, f, P ), where Γ is an alphabet of symbols, Q is a finite set of states, functions i : Q → [0, 1] and f : Q → [0, 1] refer to the probability of each state to be, respectively, initial and final, and parcial function P : Q × {Γ ∪ ε} × Q → [0, 1] defines a set of transitions between pairs of states in such a way that each transition is labelled with a symbol from Γ (or the empty string ε), and is assigned a probability.",117,118
9005,10776196,"Rather than learning translations, GIATI first converts every pair of parallel sentences in the training corpus into a corresponding extended-symbol string in order to, straight afterwards, infer a language model from.",22,23
9006,10776196,"Γ 2 Γ 1 Γ 0 Γ 0 Therefore, transitions are labelled with a symbol from Γ and every extended symbol in Γ is a translation pair coming from a phrase-based dictionary which is inferred from the parallel corpus.",15,16
9007,10776196,"Γ 2 Γ 1 Γ 0 Γ 0 Therefore, transitions are labelled with a symbol from Γ and every extended symbol in Γ is a translation pair coming from a phrase-based dictionary which is inferred from the parallel corpus.",21,22
9008,10776196,"Nevertheless, since trained extended-symbol n-gram events would typically include more than n source words, the verification of their presence or their absence inside the test set would imply hashing all the test-set word sequences, which is rather impractical.",6,7
9009,10776196,"In other words, from the n-gram point of view, backoff weights (or finite-state transitions) should only be employed if no transitions are found in the n-gram automaton for a current bilingual symbol.",41,42
9010,10776196,"Instead, input sequences have to be analysed in such a way as if they could be internally representing any possible bilingual symbol from the extended vocabulary that matches their source sides.",22,23
9011,10776196,"That way, bilingual symbols are considered to be a sort of input, so the backoff smoothing criterion is then applied to each compatible, bilingual symbol.",27,28
9012,10776196,"For phrase-based transducers, it means that for a successful transition (x, ȳ), there is no need to go backoff and find other paths consuming that bilingual symbol, but we must try backoff transitions to look for any other successful transition (x ′ , ȳ′ ), which is also compatible with the current position.",33,34
9013,10776196,"This constraint will cause that only one transition per bilingual symbol will be followed, and that it will be the highest in the hierarchy of history levels.",10,11
9014,5219389,We use the symbol Pr (•) to denote general probability distributions with (almost) no specific assumptions.,3,4
9015,5219389,"In contrast, for model-based probability distributions, we use the generic symbol p(•).",14,15
9016,5219389,The symbol B ρ(i) denotes the average of all elements in B ρ(i) . •,1,2
9017,2618953,Adding a symbol for interior of word produced a gain in accuracy.,2,3
9018,700574,Each extended symbol consists of a word from the source language plus zero or more words from each target language.,2,3
9019,1491127,"In the former case, the parse subtree is to be read left-to-right in both s and t, and in the latter case it is to be read left-to-right in s and right-to-left in t. A BTG contains only the start symbol S and one nonterminal symbol A, and each production rule consists of either a string of As or a terminal pair.",54,55
9020,1491127,"In the former case, the parse subtree is to be read left-to-right in both s and t, and in the latter case it is to be read left-to-right in s and right-to-left in t. A BTG contains only the start symbol S and one nonterminal symbol A, and each production rule consists of either a string of As or a terminal pair.",59,60
9021,1491127,"Algorithm For the calculation of the distance of two partial symbol sequences s i 1 i 0 and t j 1 j 0 , we have to determine the cost of the cheapest parse tree in all parses tree T (s i 1 i 0 , t j 1 j 0 ) that generate these sequences.",10,11
9022,1491127,The classical Levenshtein distance has been extended by block transpositions in order to allow for moves of symbol blocks at constant cost.,17,18
9023,1014562,The transitions associated with back-off are labeled with a special source symbol (not in the source vocabulary) and with an empty target string.,13,14
9024,1763317,"Also in machine translation, (Bangalore and Riccardi, 2002) defines a bilanguage corpus as consisting of source-target symbol pair sequences (s 1 : t 1 ) . . . (",22,23
9025,1763317,s n : t n ) such that s i ∈ L s ∪ {λ} and its aligned symbol t i ∈ L t ∪ {λ} where L s and L t are a couple of related languages.,20,21
9026,221949175,The (¬) symbol indicates the non-entailment subset.,4,5
9027,1871688,"The major difficulty is that the symbol used most often to indicate the end of the sentence, the dot, is also used with others purposes -to indicate abbreviations or as a part of a numerical expression.",6,7
9028,1871688,Each transition is labelled by a symbol from the input vocabulary and by a string of symbols that belong to the output vocabulary.,6,7
9029,59685705,"Formally a CFG is a quadruple g = (V N, Vr , R, S), where VN is the set of all nonterminals, Vr the set of terminals, R is the set of rules and S denominates the starting symbol that has to cover the analysed sentence.",45,46
9030,10288000,with a single symbol.,3,4
9031,10288000,"Since each of these chart cells with given source span [i, j] is identified by nonterminal symbol X and LM context e, we have at worst |N | * |T T | 2(g−1) such chart cells in a span.",19,20
9032,14203516,"With a prescribed tree visit order, our tree annotator model predicts a symbol l i , where l i takes value from a predefined finite set L, for each non-terminal node n i in a sequential fashion: P (l 1 , • • • , l |T | |T ) = |T | i=1 P (l i |l 1 , • • • , l i−1 , T ) (1) The visit order is important since it determines what are in the conditioning of Eq. (",13,14
9033,14203516,"That's why both feature values are NONE, a special symbol signifying that a node does not carry any function tag.",11,12
9034,2790420,"These statistics take the form of real-valued feature vectors for each rule as well as summary information collected over the corpus, such as the frequency of each nonterminal symbol, or unique rule source sides encountered.",31,32
9035,2790420,"We limit the total number of symbols in each rule to 8 (MaxSource/TargetSymbolCount) and require all rules to contain at least one source-side terminal symbol (noAllowAbstractRules, noAllowRulesWithOnlyTargetTerminals) since this reduces decoding time considerably.",30,31
9036,12721997,"e n , we associate a generic nonterminal symbol _X with this rule, allowing it to still take part in hierarchically motivated synchronous derivations.",8,9
9037,8666976,"All rules have at most two nonterminal symbols, which must be non-consecutive on the source side, and rules must contain at least one source-side terminal symbol.",31,32
9038,981985,"The most general case of the ITG family is the bracketing transduction grammar (BTG, Figure 1 ), which has only one nonterminal symbol.",25,26
9039,17335671,"Given co-occurrence matrices tabulated in this way, we perform an approximate maximization of t i i using a simulated annealing procedure in which each trial move takes a symbol 0 or H out of the cluster to which it is tentatively assigned and places it into another.",31,32
9040,11285313,"To minimize the number of symbol errors as is more suitable for a task like POS tagging, we show that another form of the Bayes decision rule can be derived.",5,6
9041,11285313,"The second is the measure for symbol errors, which is the more appropriate measure for POS tagging and also speech recognition with no insertion and deletion errors (such as isolated word recognition).",6,7
9042,11285313,"In practice, however, the empirical errors are counted at the symbol level.",12,13
9043,11285313,"Thus we have obtained the Bayes decision rule for minimum symbol error at position m = 1, ..., N : (w N 1 , m) → ĝm = arg max g P r m (g|w N 1 ) = arg max g P r m (g, w N 1 ) By construction this decision rule has the special property that it does not put direct emphasis on local coherency of the POS tags produced.",10,11
9044,11285313,"We will review two popular modelling approaches, namely the generative model and the direct model, and consider the associated Bayes decision rules for both minimum string error and minimum symbol error.",31,32
9045,11285313,"Symbol Error To apply the Bayes decision rule for minimum symbol error rate, we first compute the marginal probability p m (g, w N 1 ): p m (g, w N 1 ) = g N 1 : gm=g p(g N 1 , w N 1 ) = g N 1 : gm=g n p(g n |g n−1 n−2 ) • p(w n |g n ) Again, since the model is a second-order model, the sum over all possible POS tag strings g N 1 (with g m = g) can be computed efficiently using a suitable extension of the forward-backward algorithm (Bahl et al.,",10,11
9046,11285313,"Thus we obtain the decision rule for minimum symbol error at positions m = 1, ..., N : (w N 1 , m) → ĝm = arg max g p m (g, w N 1 ) Here, after the the marginal probability p m (g, w N 1 ) has been computed, the task of finding the most probable POS tag at position m is computationally easy.",8,9
9047,11285313,"Symbol Error For the minimum symbol error, the marginal (and posterior) probability p m (g|w N 1 ) has to be computed: p m (g|w N 1 ) = g N 1 : gm=g P r(g N 1 |w N 1 ) = g N 1 : gm=g n p(g n |g n−1 n−2 , w n+2 n−2 ) which, due to the specific structure of the model p(g n |g n−1 n−2 , w n+2 n−2 ), can be calculated efficiently using only a forward algorithm (without a 'backward' part).",5,6
9048,11285313,"Thus we obtain the decision rule for minimum symbol error at positions m = 1, ..., N : (w N 1 , m) → ĝm = arg max g p m (g|w N 1 ) As in the case of the generative model, the computational effort is to compute the posterior probability p m (g|w N 1 ) rather than to find the most probable tag at position m. The Training Procedure So far, we have said nothing about how we train the free parameters of the model distributions.",8,9
9049,11285313,"For the direct model, the overall tagging error rate increases on each of the two tasks (from 3.0 % to 3.3 % on WSJ and from 5.4 % to 5.6 % on MTP) when we use the symbol decision rule instead of the string decision rule.",40,41
9050,11285313,"For the generative model, these differences seem to occur at random, but for the direct model, some distinct tendencies can be observed For the German corpus, the string decision rule better handles demonstrative determiners (Rr) and subordinate conjunctions (Cs) whereas symbol decision rule is better for definite articles (Db).",48,49
9051,11285313,"The symbol decision rule typically tags the demonstrative determiner as definite article (Db) and subordinate conjunctions as interrogative adverbs (Bi), and the string decision rule tends to assign the demonstrative determiner tag to definite articles.",1,2
9052,11285313,"These typical errors for the symbol decision rule are shown in Table 5 , and for the string decision rule in Table 6 .",5,6
9053,11285313,"Conclusion So far, the experimental tests have shown no improvement when we use the Bayes decision rule for minimizing the number of symbol errors rather than the number of string errors.",23,24
9054,11285313,"The main purpose of this paper has been to show that, in addition to the widely used decision rule for minimizing the string errors, it is possible to derive a decision rule for minimizing the number of symbol errors and to build up the associated mathematical framework.",39,40
9055,11285313,Can we come up with a training criterion tailored to the symbol error rate?,11,12
9056,1115630,"All rules have at most two nonterminal symbols, which must be non-consecutive on the source side, and rules must contain at least one source-side terminal symbol.",31,32
9057,11383732,C ( Pu 0 A punctuation symbol is usually a good indication of a word boundary.,6,7
9058,11383732,"This feature checks whether the current character is a punctuation symbol (such as ""。"", ""-"", ""，""). : )",10,11
9059,917915,"For example, VP → ne VB 1 pas # do not VB 1 represents the discontiguous translation of the French words ""ne"" and ""pas"" to ""do not"", in the context of the labeled nonterminal symbol ""VB"" (representing syntactic category ""verb"").",42,43
9060,917915,"Two of these derivations would produce S as a root symbol, while one derivation would produce SBAR.",10,11
9061,917915,"Each h i ∈ H. When N , H are defined to include just a single generic symbol as in (Chiang, 2005) , we produce the unlabeled grammar discussed above.",17,18
9062,201667895,"For example, a hard rule may eliminate a complete sen-tence pair encountering even one ℃ symbol, while uniblock only gives a small penalization due to the fact that one occurrence of the symbol ℃ does not alter the count-based feature vector e much.",18,19
9063,201667895,"For example, a hard rule may eliminate a complete sen-tence pair encountering even one ℃ symbol, while uniblock only gives a small penalization due to the fact that one occurrence of the symbol ℃ does not alter the count-based feature vector e much.",36,37
9064,16425844,Performance has been measured in terms of both semantic-symbol error and fUll-sentence matching rates.,10,11
9065,16425844,Semantic-symbol error rates are much lower in the categorized experiments than in the uncategorized ones.,2,3
9066,16425844,"We can also appreciate a remarkable decrease in semantic-symbol error rates of Error Correcting with respect to Viterbi translations, specially for smaller training corpus.",10,11
9067,16425844,Semantic-symbol error rates.,2,3
9068,16425844,"In our task, we got a 3% in semantic-symbol error rate for a training set of approximately 6000 pairs, while for the same level of performance using the standard Viterbi algorithm requires some 10000 training pairs.",12,13
9069,7020283,The SAMT toolkit can be used to generate an SAMT grammar as well as a purely hierarchical grammar that uses a single generic nonterminal symbol X [10] .,24,25
9070,7020283,The architecture treats the tab symbol as a key-value separator.,5,6
9071,1258901,The system described in Zollmann and Venugopal (2006) can be used to create a Syntax Augmented grammar as well as a purely hierarchical grammar that uses a single generic nonterminal symbol Chiang (2005) .,32,33
9072,1258901,"We show the estimated counts on these rules as well as their source, target and lefthand-side nonterminal symbol.",20,21
9073,245838425,"In most of them, the modified word is in fact a currency's name or symbol (e.g. $25 million, vijfhonderd zestig miljoen gulden /five hundred and sixty million guilders/, 90 млрд рублей /90 billion rubles/ ).",16,17
9074,245838425,Let us look into mentions with a $ symbol as their syntactic head ($ mention) in English-ARRAU.,8,9
9075,5856074,"Because we are Operation Parameterization elementary tree grouping P elem (t a |ε a ⇒ children(ε a )) re-order P align (α|ε a ⇒ children(t a )) insertion α can include ""insertion"" symbol lexical translation P t (f |e) cloning P makeclone (ε) α can include ""clone"" symbol Table 1 : The probabilistic tree-to-tree model ultimately interested in predicting the correct target string, regardless of its structure, we do not assign probabilities to these steps.",40,41
9076,5856074,"Because we are Operation Parameterization elementary tree grouping P elem (t a |ε a ⇒ children(ε a )) re-order P align (α|ε a ⇒ children(t a )) insertion α can include ""insertion"" symbol lexical translation P t (f |e) cloning P makeclone (ε) α can include ""clone"" symbol Table 1 : The probabilistic tree-to-tree model ultimately interested in predicting the correct target string, regardless of its structure, we do not assign probabilities to these steps.",61,62
9077,52119752,"For training, we delimit the simple sentences with a special symbol.",11,12
9078,5490143,"Starting from a dummy start state in which the dot is just to the left of the grammar's start symbol, predict adds new states for rules which could expand the start symbol.",20,21
9079,5490143,"Starting from a dummy start state in which the dot is just to the left of the grammar's start symbol, predict adds new states for rules which could expand the start symbol.",33,34
9080,5490143,"After prediction, scan checks the input string: if the symbol immediately following the dot matches the current word in the input, then the dot is moved rightward, across the symbol.",11,12
9081,5490143,"After prediction, scan checks the input string: if the symbol immediately following the dot matches the current word in the input, then the dot is moved rightward, across the symbol.",33,34
9082,5490143,"If, as a result of scanning, any states are now present in which the dot is at the end of a rule, then the left hand side of that rule has been recognized, and any other states having a dot immediately in front of the newly-recognized left hand side symbol can now have their dots moved as well.",55,56
9083,5490143,Parsing finishes when the dot in the dummy start state is moved across the grammar's start symbol.,17,18
9084,6246442,"Rose and Riccardi modeled word fragments (using a single word fragment symbol frag) in their system ""How May I Help You"" (Rose and Riccardi, 1999) .",12,13
9085,210063691,"As explained above in Section 2.2, we apply nearest neighbor mapping to the embedding output of the model to map its prediction back onto an entity symbol.",27,28
9086,7655208,To keep the descriptions here concise we have used the symbol '?',10,11
9087,333563,"For example, the original MRG contained the following production rules for generating an infinite number of travel actions from the root symbol S. While this process of expanding the production rules resulted in many more rules, these expanded rules usually correspond better with words or phrases in natural language.",22,23
9088,5709943,"For simplicity, stop fragments are omitted in Figure 2 and 4 and Y is attached to the stop symbol ( Y ).",19,20
9089,5709943,"Stop(#) is a special type of forward substitution and applies to donor states, but only when the input word is the terminal symbol #.",26,27
9090,224724349,"Neural PCFGs A probabilistic context-free grammar (PCFG) consists of a 5-tuple grammar G = (S, N , P, Σ, R) and rule probabilities π = {π r } r∈R , where S is the start symbol, N is a finite set of nonterminals, P is a finite set of preterminals, Σ is a finite set of terminal symbols, and R is a finite set of rules associated with probabilities π.",47,48
9091,431369,"To obtain the uncertainty about infinite PCFG languages, a recursive relation due to Grenander (1967) can be used to calculate the entropy of the start symbol S which begins all derivations.",28,29
9092,431369,"If L(G) is the language of the grammar G, parsing an initial substring w is the intersection depicted in 3 where the period denotes any terminal symbol of G and the Kleene star indicates any number of repetitions.",28,29
9093,431369,"The uncertainty associated with the the start symbol of this new, resultant grammar is the conditional entropy H(S|w 1 , w 2 , • • • w n ).",7,8
9094,124264466,"In HMM-LR, the LR parser is used as a language source model for symbol predic- tion/ generation.",16,17
9095,124264466,"When merging two grammars, the start symbol of the phonemic grammar is replaced with pre-terminal names that might include unknown words (in our ex periments, proper-noun is allowed to include unknown words) .",7,8
9096,17950510,"Although the symbol (dot) that is used to represent period is ambiguous because it is also used as the decimal point or in abbreviations, its resolution only requires local context.",2,3
9097,17950510,"Chinese also uses periods (albeit with a different symbol), question marks, and exclamation marks to indicate sentence boundaries.",9,10
9098,15523153,"For each original symbol in the grammar such as NP, we consider two latent refinements: NP 0 and NP 1 .",3,4
9099,15523153,Final grammar estimation: The EM procedure used during split and merge assigns fractional counts c(• • • ) to each refined symbol X i and each production X i → Y j Z k .,22,23
9100,15523153,"If the fractional count of a word given a pre-terminal symbol falls below a threshold k, then we consider that instance rare and reserve a fraction of its probability mass for unseen words.",12,13
9101,16872779,Thus each verb will belong to a class with a symbol representing each of the three dimensions.,10,11
9102,52053741,"8 )) 10: Run PG decoder for one step to get {w t } 11: Summary ← Summary + {w t } 12: if w t is the period symbol then 13: R(S i ) ← Sim(S i , Summary), ∀i 14: MMR(S i ) ← λI(S i ) −(1 − λ)R(S i ), ∀i 15: end if 16: end while tences and documents, they are data-driven and not restricted to a particular domain.",35,36
9103,52051402,A typical RNN text generator unrolls the sequence X one word at a time until an end-ofsentence symbol (EOS) is reached.,19,20
9104,236179526,The symbol '=' represents the attachment of pronouns and other cases.,1,2
9105,236179526,"During training, all reduplication prefixes are replaced with a special symbol and treated as one type of the atomic prefixes.",11,12
9106,236179526,The input is the prefix string where letters and the symbol '-' are denoted by one-hot encoding.,10,11
9107,20336168,"As the morph features consist of several fields separated by a special symbol, we treat the prediction of each field as an independent task, and then combine the predictions from different models.",12,13
9108,173990267,"We assign (r(w i ), l(w i )) to the w i (0 < i ≤ n−1) and leave a padding symbol N to the w n .",26,27
9109,13419043,We terminate translation if the best scoring partial hypothesis ends with the sentence end symbol.,14,15
9110,14825753,Lexicon can be seen as a set of symbols with each symbol representing certain meanings.,11,12
9111,14825753,A bilingual dictionary easily enables us to map from one symbol set to another.,10,11
9112,5515513,At each level a decision about the n-th symbol is made.,10,11
9113,6783623,At each level a decision about the nth symbol is made.,8,9
9114,6783623,"Here, the function EXT ORDER (see Section 5) chooses which cipher symbol is used next for extension, EXT LIMITS decides which extensions are allowed, and SCORE (see Section 4) scores the new partial hypotheses.",14,15
9115,6783623,Note that this is not a simple linear interpolation of different n-gram trivial scores: Each symbol is scored only using the maximum amount of context available.,18,19
9116,6783623,"2013) , two strategies are presented: One which at each step chooses the most frequent remaining cipher symbol, and another, which greedily chooses the next symbol to maximize the number of contiguously fixed n-grams in the ciphertext.",19,20
9117,6783623,"2013) , two strategies are presented: One which at each step chooses the most frequent remaining cipher symbol, and another, which greedily chooses the next symbol to maximize the number of contiguously fixed n-grams in the ciphertext.",29,30
9118,6783623,"Here n is the n-gram order, w n the weight for order n, and # n the number of positions whose maximum context is of size n. We perform a beam search over all possible enumerations of the cipher vocabulary: We start with fixing only the first symbol to decipher.",52,53
9119,6783623,We then continue with the second symbol and evaluate all resulting extension orders of length 2.,6,7
9120,6783623,"Compared to the Zodiac-408, which has length 408 while having 54 different symbols (7.55 observations per symbol), part two of the Beale ciphers has length 762 while having 182 different symbols (4.18 observations per symbol).",18,19
9121,6783623,"Compared to the Zodiac-408, which has length 408 while having 54 different symbols (7.55 observations per symbol), part two of the Beale ciphers has length 762 while having 182 different symbols (4.18 observations per symbol).",39,40
9122,6783623,Here we run our algorithm with a beam size of 10M and achieve a decipherment accuracy of 157 out of 185 symbols correct yielding a symbol error rate of less than 5.4%.,26,27
9123,6783623,"We also ran our algorithm on the other parts of the Beale ciphers: The first part has a length 520 and contains 299 different cipher symbols (1.74 observations per symbol), while part three has length 618 and has 264 symbols which is 2.34 observations per mapping.",31,32
9124,3544821,"2007) , and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (",24,25
9125,237558782,"However, we employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system.",16,17
9126,1400617,We insert a special end of sentence symbol at sentence boundaries so that the features looking forwards and backwards are always defined.,7,8
9127,15916545,"1977) : θf|e = n:fn=f p n (e|f N 1 , ϑ) f n:fn=f p n (e|f N 1 , ϑ) (5) with p n (e|f N 1 , ϑ) = [e N 1 :en=e] p(e N 1 |f N 1 , ϑ) (6) being the posterior probability of observing the plaintext symbol e at position n given the ciphertext sequence f N 1 and the current parameters ϑ. p n (e|f N 1 , ϑ) can be efficiently computed using the forward-backward algorithm.",77,78
9128,15916545,"For the full vocabulary of 3661 words, preselection EM using a 4-gram LM needs less than 7% of the time of beam EM with a 3-gram LM and performs by 1% better in symbol accuracy.",40,41
9129,1334893,The root of the tree is a special symbol denoted by x 0 which has exactly one modifier.,8,9
9130,158396,"Using this example we describe the strategies we used for special cases in the transformation from Figure 1(b) to Figure 1(c): • ignore the unaligned target word, e.g. e 1 • the unaligned source word should follow its preceding word, the unaligned feature is kept with a * symbol, e.g. f * 2 is after f 1 • when one source word is aligned to multiple target words, only keep the alignment that links the source word to the first target word, e.g. f 4 is linked to e 5 and e 6 , only f 4 − e 5 is kept.",52,53
9131,5193894,Definitions and Notation Assume a given alphabet Σ of terminal symbols and a disjoint finite alphabet N of nonterminal symbols that includes the special symbol ROOT.,24,25
9132,5193894,"This PCFG G generates only parses of w: that is, it assigns weight 0 to any derivation with fringe = w. G uses the original terminals Σ, but a different nonterminal set-namely the anchored nonterminals N = {A k i : A ∈ N , 0 ≤ i < k ≤ n}, with root symbol ROOT = ROOT n 0 .",62,63
9133,51860864,The program or editor writes the code symbol designating the function it serves next to each occurrence.,7,8
9134,51860864,"Next to each function symbol is a reference number, identifying a section in the worksheet.",4,5
9135,51860864,But now a category symbol can be assigned to each morph occurrence in the text and a search for constructions started.,4,5
9136,51860864,"Every occurrence in text of a tentative construction is replaced with an arbitrary symbol standing for the construction, and every occurrence of a morph in a tentative class is replaced with an arbitrary symbol standing for the class.",13,14
9137,51860864,"Every occurrence in text of a tentative construction is replaced with an arbitrary symbol standing for the construction, and every occurrence of a morph in a tentative class is replaced with an arbitrary symbol standing for the class.",34,35
9138,1332764,"The compound boundary is treated as a separate token, represented by the special symbol +GEN+, to signal to the syntactic analysis component that genitive compounding has taken place.",14,15
9139,53081957,"Intra-sentential coreference occurs when an instance refers to an antecedent, where we replace the instance with a special symbol ""•"" and add a COREF link between ""•"" and its antecedent.",21,22
9140,53081957,"Brackets, parentheses, and the special symbol ""•"" are all considered as tokens in this representation.",7,8
9141,53081957,"represents a scenario, where the token is not a special symbol ""•"", and it refers to none of the preceding tokens.",11,12
9142,53081957,"b) We preprocess the data by replacing the special symbol ""•"" with the syntactic head of its antecedent.",10,11
9143,236486149,"The removal of mentions (i.e., including a username with the @ symbol inside a tweet), URLs and non-ASCII characters were found to be the biggest contributor to document length reduction.",13,14
9144,195584320,"Probabilistic Context-Free Grammars We consider context-free grammars (CFG) consisting of a 5-tuple G = (S, N , P, Σ, R) where S is the distinguished start symbol, N is a finite set of nonterminals, P is a finite set of preterminals, 1 Σ is a finite set of terminal symbols, and R is a finite set of rules of the form, S → A, A ∈ N A → B C, A ∈ N , B, C ∈ N ∪ P T → w, T ∈ P, w ∈ Σ. A probabilistic context-free grammar (PCFG) consists of a grammar G and rule probabilities π = {π r } r∈R such that π r is the probability of the rule r. Letting T G be the set of all parse trees of G, a PCFG defines a probability distribution over t ∈ T G via p π (t) = r∈t R π r where t R is the set of rules used in the derivation of t. It also defines a distribution over string of terminals x ∈ Σ * via p π (x) = t∈T G (x) p π (t), where T G (x) = {t | yield(t) = x}, i.e. the set of trees t such that t's leaves are x. We will use p π (t | x) p π (t | yield(t) = x) to denote the posterior distribution over latent trees given the observed sentence x. Parameterization The standard way to parameterize a PCFG is to simply associate a scalar to each rule π r with the constraint that they form valid probability distributions, i.e. each nonterminal is associated with a fully-parameterized categorical distribution over its rules.",39,40
9145,195584320,"We associate embeddings with each symbol, introducing input embeddings w N for each symbol N on the left side of a rule (i.e. N ∈ {S} ∪ N ∪ P).",5,6
9146,195584320,"We associate embeddings with each symbol, introducing input embeddings w N for each symbol N on the left side of a rule (i.e. N ∈ {S} ∪ N ∪ P).",14,15
9147,195584320,"We will use E G = {w N | N ∈ {S} ∪ N ∪ P} to denote the set of input symbol embeddings for a grammar G, and λ to refer to the parameters of the neural network used to obtain the rule probabilities.",26,27
9148,195584320,"In the neural PCFG, the global rule probabilities π = π S ∪ π N ∪ π P are the output from a neural net run over the symbol embeddings E G , where π N are the set of rules with a nonterminal on the left hand side (π S and π P are similarly defined).",29,30
9149,195584320,"In the compound PCFG, we have per-sentence rule probabilities π z = π z,S ∪ π z,N ∪ π z,P obtained from running a neural net over a random vector z (which varies across sentences) and global symbol embeddings E G .",48,49
9150,195584320,"In this work, we study compound probabilistic context free grammars whose distribution over trees arises from the following generative process: we first obtain rule probabilities via z ∼ p γ (z), π z = f λ (z, E G ), where p γ (z) is a prior with parameters γ (spherical Gaussian in this paper), and f λ is a neural network that concatenates the input symbol embeddings with z and outputs the sentence-level rule probabilities π z , π z,S→A ∝ exp(u A f 1 ([w S ; z])), π z,A→BC ∝ exp(u BC [w A ; z]), π z,T →w ∝ exp(u w f 2 ([w T ; z])), where [w; z] denotes vector concatenation.",79,80
9151,195584320,"Hyperparameters Our PCFG uses 30 nonterminals and 60 preterminals, with 256-dimensional symbol embeddings.",14,15
9152,195584320,A Appendix A.1 Model Parameterization Neural PCFG We associate an input embedding w N for each symbol N on the left side of a rule (i.e. N ∈ {S} ∪ N ∪ P) and run a neural network over w N to obtain the rule probabilities.,16,17
9153,10132752,One grammatical category (non-terminal symbol) is selected as the one being of particular interest for the moment.,7,8
9154,44097146,"Experimental Details In experiments, instead of using the full vocabularies shown in table 1, we set a minimum count threshold for each dataset, to replace the rare words by a special out-of-vocabulary symbol.",39,40
9155,8368494,To do this we treat each literal as a symbol.,9,10
9156,8368494,This means each verb and attribute predicate with its respective arguments is taken to denote a single symbol.,17,18
9157,222272091,"2019) , we produce one instance by concatenating the source sentence with a context sentence, and adding a special symbol "" DCS "" at the beginning and a separator token "" SEP "" in between.",21,22
9158,222272091,"We believe the special symbol "" DCS "" can encode the information of source-context sentence pairs well by the self-attention.",4,5
9159,20480670,"A simple choice for H y is a Markov process that emits characters in Σ ∪ {$}, where $ is a distinguished stop symbol that indicates the end of the string.",26,27
9160,20480670,"Each H y,u is a distribution over Σ, defined recursively as H y, ∼ PYP(d , α , U Σ ) (3) H y,u ∼ PYP(d |u| , α |u| , H y,σ(u) ) where is the empty sequence, U Σ is the uniform distribution over Σ ∪ {$}, and σ(u) drops the first symbol from u. The discount and concentration parameters (d |u| , α |u| ) are associated with the lengths of the contexts |u|, and should generally be larger for longer (more specific) contexts, implying stronger backoff from those contexts.",70,71
9161,20480670,"To allow the x t to be a multi-word string, we simply augment the character set with a distinguished space symbol ∈ Σ that separates words within a string.",23,24
9162,20480670,"For instance, New York would be generated as the 9-symbol sequence New York$. Although the model emits New York all at once, we still formulate our inference procedure as a particle filter that proposes one tag for each word.",12,13
9163,10774106,"We could namely develop monoglot dictionaries -that is dictionaries entirely in one of the foreign languages with MT value -which, in the light of the semantic peculiarities of each of the target languages, explain every one of the multiple meanings carried by the meaningful constituents of foreign language texts and which indicate each incident meaning by a distinctive symbol.",60,61
9164,10774106,The buyers of MT would then use these dictionaries and select from them the supplementary symbol indicated by the incident meaning concerned.,15,16
9165,10774106,The pre-editor would then select the meaning required by the context and dial the distinctive graphic symbol representative of this meaning and supplied by the dictionary entry.,18,19
9166,10774106,The dialling of the supplementary graphic symbol would then release the section concerned of the foreign text for the next stage in the translation process.,6,7
9167,174797789,"The publication in 1845 (republished in 1886) of Alfred Toussenel's Les Juifs rois de l'epoque caused especially the rise of the so-called economic antisemitism, which accused the Jews of an increasing economic and financial influence, of which the Rothschilds were considered the protagonists and became a symbol.",53,54
9168,244077677,"This ten rules roughly spell out the semantic ideal of ""one-word-one meaning"" or maximum one-to-one correspondence of a symbol and its referent.",28,29
9169,244077677,"4one word order The overall semantic rule of maximizing the one-to-one correspondence of symbol and referent mean that the position or order of the words in a sentence should tell whether each word be the subject or verb or object or modifier, etc.",17,18
9170,244077677,"-one meaning for each word The semantic rule, ""one symbol, one meaning"" be an ideal which can be approached but seldom can be perfectly and permanently achieved.",11,12
9171,244077677,"The supplemental codings in effect carry out the rule of ""one symbol, one referent.""",12,13
9172,244077677,"The semantic rule of seeking the most full one-to-one correspondence of symbol and referent will call for the forms of grammar to be completely analyzed into invariant particles, each with constant meaning.",15,16
9173,244077677,All the rules here spell out the general semantic ideal of just one meaning for any symbol.,16,17
9174,203688974,"We use the pipe ""|"" symbol to mark morpheme boundaries, as inspired by Beesley and Karttunen (2003) , who use ""TB"" to mark the token boundary. """,7,8
9175,1949831,"Two changes are required: (i) there is no input buffer of unprocessed words, rather there is an output buffer (T ), and (ii) instead of a SHIFT operation there are GEN(x) operations which generate terminal symbol x ∈ Σ and add it to the top of the stack and the output buffer.",44,45
9176,1949831,The REDUCE operation can only be applied if the top of the stack is not an open nonterminal symbol and n ≥ 1.,18,19
9177,17441119,"This leads to the realization that a symbol may both subsume, and be subsumed by, another symbol in the same coupling step; it is not clear how to apply the above redistribution technique to our situation.",7,8
9178,17441119,"This leads to the realization that a symbol may both subsume, and be subsumed by, another symbol in the same coupling step; it is not clear how to apply the above redistribution technique to our situation.",18,19
9179,17441119,"Then, as an extreme, we replace all POS tags with the same symbol ""X,"" to investigate what predicate/argument relationships can be derived: we Fragment Analysis In this section we analyze hand-selected preliminary fragments and lexical clusterings our system learns.",14,15
9180,3133211,All out-of-vocabulary events are converted to (and scored as) the symbol UNK.,16,17
9181,15746905,"To capture the small bits of ""phonetic glue"" (e.g., the n that occurs between banane and markt in the compound bananenmarkt) that may occur when generating compound words, we insert a special SUF symbol in between each pair of source words.",39,40
9182,15746905,This symbol will allow us to insert a small suffix in between the translations of source words.,1,2
9183,15746905,"Finally, we insert a special END symbol at the end of each source phrase.",7,8
9184,15746905,"This symbol will allow the model to generate morphological variants due to suffixes indicating case, number, and agreement that only occur at the end of a whole compound word, but not in between the individual pieces.",1,2
9185,11015303,"Theron & Cloete additionally repeat an out-of-bounds symbol outside the word, whereas we dispense with these marks.",11,12
9186,11015303,1 3 4 5 6 7 8 x c i s e e $=e Each of these contexts is unaccented (it is meant to be matched with representations of unaccented words) and the original form of the pivot letter is associated to the context as an output (we use the symbol '=' to mark this output).,55,56
9187,26855299,"The target population is then taught to recognise the words rather as one would recognise a symbol, then associate it with the defined meaning.",16,17
9188,15188277,"Input and output strings are augmented with a trailing (""end-of-string"") symbol that is seen by the single-character lookahead.",18,19
9189,9787203,"For a sequence of icon containing more than one predicative symbol, the calculus of the assignments is done for every one of them.",10,11
9190,224814163,The intuition is to encourage any lost character to be mapped to exactly one 5 known IPA symbol.,17,18
9191,820272,"w M is p(w) = M i=1 p(c i | c i−1 ) × p(w i | c i ), (2.1) where c 0 is a special start symbol.",32,33
9192,15108580,"Note that ""•"" indicates a Hadamard product, ξ is an error signal vector, the prime superscript indicates a derivative (i.e., σ means derivative function of the sigmoid), and z is the symbol for linear pre-activation values.",39,40
9193,1006112,Such aligned subphrases are used to generalize their parent phrases by being substituted as a single nonterminal symbol [X].,17,18
9194,1255985,"When extracting phrases from the matched phrase trees, we ignored tokens with part-ofspeech tags as pronoun, number, determiner, punctuation or symbol, and removed all subtrees in the matched phrase trees that had either relativeclause-modifier or clausal-complement dependency with their parents since, even though we want full phrases, including these sub-trees introduced extraneous phrases and clauses.",26,27
9195,7655549,"Over time, participants develop common standards codifying those concepts: they develop a system of meaning-symbol pairs, or, signs.",18,19
9196,15954468,"However, since the only information a constituent carries in context-free grammar is the grammar symbol, the spurious derivations only produce exactly the same results as the normal ones.",17,18
9197,15954468,"INITIAL ITEM: PREDICTION: (id, nil, (O,O, po, mm( ~o) ,O) ) ' where id is a new symbol (id, pid, (i,j ,p = (a, ~),M,d) ) (id', id, (j,j, p', p(M/ (d+l) ) U mm(~I,'), 0))' where id I is a new symbol, and d ( a and pl = (ar,~t) C P SCANNING: COMPLETION: (id, pid, (i,j,p = (a, ~) ,M,d) ) (id, pid, (i,j+l,p ,M U mm( ~') \ (d+l),d+l)) ' where d < a and (wj+D ~') E P (id, pid,(i,j,p,M,d) ) (id"",id,(j,k,p',M',a')) where d < a (ia, pie, (i,k,p , UU (U' \ (d+l)),d+l)) ' Figure 6 Shieber's parsing operations modified.",33,34
9198,15954468,"INITIAL ITEM: PREDICTION: (id, nil, (O,O, po, mm( ~o) ,O) ) ' where id is a new symbol (id, pid, (i,j ,p = (a, ~),M,d) ) (id', id, (j,j, p', p(M/ (d+l) ) U mm(~I,'), 0))' where id I is a new symbol, and d ( a and pl = (ar,~t) C P SCANNING: COMPLETION: (id, pid, (i,j,p = (a, ~) ,M,d) ) (id, pid, (i,j+l,p ,M U mm( ~') \ (d+l),d+l)) ' where d < a and (wj+D ~') E P (id, pid,(i,j,p,M,d) ) (id"",id,(j,k,p',M',a')) where d < a (ia, pie, (i,k,p , UU (U' \ (d+l)),d+l)) ' Figure 6 Shieber's parsing operations modified.",93,94
9199,15954468,"By generating a new symbol for the self index in every Prediction item (id'), parent pointers in those items are threaded to form a prediction path.",4,5
9200,1233867,"For hierarchical phrase-based translation (HPBT), the set consists of only two symbols, X and a goal symbol S. set R = R 0 ∪ R 1 ∪ R 2 : • Arity 2 (binary) rules (R 2 ): a(h 1 ) → α 1 b(h 2 )α 2 c(h 3 )α 3 , β 1 b(h 2 )β 2 c(h 3 )β 3 or a(h 1 ) → α 1 b(h 2 )α 2 c(h 3 )α 3 , β 1 c(h 2 )β 2 b(h 3 )β 3 where a, b, c ∈ N , h 1 , h 2 , h 3 ∈ [m], α 1 , α 2 , α 3 ∈ [n s ] * and β 1 , β 2 , β 3 ∈ [n t ] * . •",22,23
9201,9135033,"If r = (t, s, φ) is a rule, and d i is a (sub)derivation with the root symbol of its source projection maching the corresponding substition node in r, i.e., root(S( d i )) = φ(x i ), then d = r(d 1 , . . . ,",24,25
9202,12858058,"This is straightforward to do: each node becomes a unique non-terminal symbol, with its incoming edges corresponding to different ways of rewriting it.",14,15
9203,7419876,"In the special case where x is a first mention of x.e, x.p is the special symbol ♦, x.e is a newly allocated entity of some appropriate type, and the name x.n is generated from scratch.",17,18
9204,7419876,"When â is the special end-of-string symbol #, the only allowed edits are the insertion ( b ) and the substitution ( # # ).",10,11
9205,7419876,"We use the same model, taking ♦.n to be the empty string (but with # ♦ rather than # as the end-ofstring symbol).",27,28
9206,1649509,The position of t in c is denoted by a hole symbol '@'.,11,12
9207,19778762,"For every end-pattern, ep i , we construct a classifier and ' |' is the pipe symbol signifying that, the output at the left of the operator is used as input to the right of the operator.",20,21
9208,44112954,"Therefore, this analysis leads us to think that q discriminates a from b, and that q is an attribute of b rather than of a. 1 The symbol '→' denotes tendency (""tends to"").",29,30
9209,201676402,"b) First-character embedding: Similar to the part-of-speech tag embedding, we extracted the first character of each token in a tweet and generated four binary features depending on whether the first character was an uppercase letter, a lowercase letter, an integer, or a symbol / special character. (",54,55
9210,2902239,We induce 36 tags for English and 37 for Chinese to match the number of tags present in the treebanks (excluding symbol and punctuation tags).,22,23
9211,13256575,"5  2) For each parse tree: Generate root node z TOP ∼ Binomial(θ TOP ) For each node i in the parse tree: Choose rule type t i ∼ Multinomial(θ T zi ) If t i == Lex: Emit terminal symbol x i ∼ Multinomial(φ L zi ) If t i == Left/Right/Unary: Generate argument category y i ∼ Multinomial(φ Y zi ) Generate combinator c i ∼ Multinomial(θ C zi,yi ) Deterministically create z L(i) (and z R(i) if binary) z i y i c i z L(i) z R(i) x L(i) x R(i) z ∞ ∞ y φ Y θ T θ C φ L β Y β L Because we are working with CCG, the parent z i , argument y i and combinator c i uniquely define the two children categories (z L(i) , z R(i) ).",46,47
9212,13256575,"For every grammar symbol, an argument distribution and emission distribution is drawn from the corresponding Dirichlet Processes.",3,4
9213,13256575,"In addition, there are several MLE distributions tied to a given symbol for generating rule types, combinators and lexical tokens.",12,13
9214,300249,"Category induction We assume there are two atomic categories, N (nouns or noun phrases) and S (sentences), a special conjunction category conj, and a special start symbol TOP.",33,34
9215,226283502,"The authors' architecture works as follows: for each input x (for our purposes an OIA etymon), a latent representation h enc j ∈ R 2D is learned for each time step j ∈ {1, ..., |x|} via a bidirectional LSTM on the basis of the input symbol at time step j. For each output y (for our purposes a medieval/modern Indo-Aryan reflex), a latent representation h dec i ∈ R D is learned via a forward LSTM for each time step i ∈ {1, ..., |y|} on the basis of the output symbol at time step i − 1.",55,56
9216,226283502,"The authors' architecture works as follows: for each input x (for our purposes an OIA etymon), a latent representation h enc j ∈ R 2D is learned for each time step j ∈ {1, ..., |x|} via a bidirectional LSTM on the basis of the input symbol at time step j. For each output y (for our purposes a medieval/modern Indo-Aryan reflex), a latent representation h dec i ∈ R D is learned via a forward LSTM for each time step i ∈ {1, ..., |y|} on the basis of the output symbol at time step i − 1.",112,113
9217,226283502,"The probability that the output is aligned with the jth input symbol at time i is equal to softmax(h dec i T h enc j ), where T ∈ R D×2D is a learned parameter.",11,12
9218,226283502,"The emission probability of the output symbol at time i given such an alignment is proportional to exp(W tanh(S[h dec i ; h enc j ])), and is hence also dependent on the previous output symbols (W ∈ R Σy×3D and S ∈ R 3D×3D are learned parameters).",6,7
9219,224707565,"Therefore, we expect an input symbol to receive a nonzero attribution score if and only if it comprises a subsequence.",6,7
9220,224707565,"The first four counters record how many occurrences of each symbol have been observed at time step t. The next three counters record the number of bs, cs, and ds that form one of the four distinguished subsequences with an earlier symbol.",10,11
9221,224707565,"The first four counters record how many occurrences of each symbol have been observed at time step t. The next three counters record the number of bs, cs, and ds that form one of the four distinguished subsequences with an earlier symbol.",43,44
9222,224707565,"The hidden state h (t) is a onehot representation of the pair ⟨ q, x (t) ⟩ , which encodes both the current state of A and the most recent input symbol.",36,37
9223,224707565,"Since the FSA undergoes a state transition with each input symbol, the forget gate always clears c (t) , so that information written to the cell state does not persist beyond a single time step.",10,11
9224,224707565,"When the current input symbol is a closing bracket, the highest item of positions 1 through k − 1 is deleted and copied to position k, popping the top item from the stack.",4,5
9225,224707565,"Thus, g (t) k contains the stack encoding of the current input symbol if it is an opening bracket.",15,16
9226,224707565,"If the current input symbol is a closing bracket, then α (t) = 0, so the sign of u (t) is determined by the highest item of h (t−1) :k−1 .",4,5
9227,224707565,"When the number of as is different from the number of bs, occlusion assigns a lower-magnitude score to the symbol with fewer instances.",22,23
9228,224707565,"Heatmaps #7-10 show that LRP fails to assign positive scores to the first symbol of each subsequence, while the other methods generally do not.",16,17
9229,224707565,"For the FSA-based network, saliency, G × I, and LRP assign only the last symbol a nonzero score when the relevance output class c matches the network's predicted class.",19,20
9230,224707565,"When the matching bracket is not the last symbol of the input, the other unclosed brackets are also highlighted, with progressively smaller magnitudes, and with brackets of the opposite type from c receiving negative scores.",8,9
9231,224707565,"5  Given an input string classified as True, we iteratively remove the symbol with the highest relevance score, recomputing heatmaps at each iteration, until the string no longer contains any of the four subsequences.",14,15
9232,224707565,A peculiar property of the SP task is that removing a symbol preserves the validity of input strings.,11,12
9233,224707565,"Saliency, G × I, and LRP perform close to the random baseline on the FSA network; this is unsurprising, since these methods only assign nonzero scores to the last input symbol.",34,35
9234,224707565,"A.3 FSA Network Here we describe a general construction of an LSTM simulating an FSA with states Q, accepting states Q F ⊆ Q, alphabet Σ, and transition function δ : Q × Σ → Q. Recall that h (t) contains a one-hot representation of pairs in Q × Σ encoding the current state of the FSA and the most recent input symbol.",69,70
9235,248780351,"For all languages, we replace any other special characters with the symbol C. B Datasets This section provides further information about the datasets used to extract the perceptual representations.",12,13
9236,209443915,"To remedy this, Baek proposes that the decision to project a symbol to the tier may be conditioned by local contextual information such as the presence of word boundaries.",12,13
9237,209443915,"Intuitively, homomorphisms are functions that replace each symbol of ⌃ with a string in ⇤ .",8,9
9238,209443915,"Heavy syllables are represented by the symbol H, while light syllables are represented by the symbol L. Definition 15.",6,7
9239,209443915,"Heavy syllables are represented by the symbol H, while light syllables are represented by the symbol L. Definition 15.",16,17
9240,209443915,"Thus, every symbol of ⌃ is projected to tier ⌧ , so C LHOR is k-SL.",3,4
9241,209443915,"In other words, ⌧ is the same as , except the last symbol of the input is always projected.",13,14
9242,209443915,"To represent stacks of asterisks, I annotate alphabet symbols with a subscript indicating the number of asterisks above that symbol.",20,21
9243,209443915,"Symbols in ⌃ i [ ⌃ i 1 represent syllables with an asterisk on level i 1; symbols in ⌃ i represent syllables with an asterisk on both level i 1 and level i. If the first symbol of x in ⌃ i [ ⌃ i 1 is of the form i 1 2 ⌃ i 1 , then this symbol is incremented to i .",38,39
9244,209443915,"Symbols in ⌃ i [ ⌃ i 1 represent syllables with an asterisk on level i 1; symbols in ⌃ i represent syllables with an asterisk on both level i 1 and level i. If the first symbol of x in ⌃ i [ ⌃ i 1 is of the form i 1 2 ⌃ i 1 , then this symbol is incremented to i .",61,62
9245,209443915,"If the first symbol of x in ⌃ i [ ⌃ i 1 is of the form i 2 ⌃ i , then this symbol is left unchanged: ER(i, ) is still understood to add an asterisk on the ith level, but an asterisk has already been added there by another rule.",3,4
9246,209443915,"If the first symbol of x in ⌃ i [ ⌃ i 1 is of the form i 2 ⌃ i , then this symbol is left unchanged: ER(i, ) is still understood to add an asterisk on the ith level, but an asterisk has already been added there by another rule.",24,25
9247,209443915,"With the three-letter Dybo's Rule, the state needs to record the identity of the most recent input symbol in order to delay the output by one time step.",21,22
9248,209443915,The only way to do this with an I-TISL SFST is to project the most recent input symbol to the tier.,19,20
9249,209443915,"Let ⌧ be the 2-ISL tier projection that projects • all instances of D, É, and Ŕ; • all instances of DR, D Ŕ, ER, E Ŕ, DR, D Ŕ, ÉR, and É Ŕ; and • the last symbol of the input.",51,52
9250,69345275,"The start symbol is S. S(xy) ← T (x, y) (2a) T (ax, ay) ← T (x, y) T (bx, by) ← T (x, y) (2c) T (ε, ε) ← (2d) A rule of the form A(y) ← B 1 (x 1 )B 2 (x 2 ) . . .",2,3
9251,69345275,"By rule (2d), T generates ⟨ε, ε⟩. By (2c), T generates ⟨b, b⟩. By (2b), T generates ⟨ab, ab⟩. By (2a), the start symbol S generates abab.",38,39
9252,69345275,"To that end, observe that for any context γ and symbol i ∈ {a, b, c}, |γ ⊙ y| i = |γ ⊙ x| i − |x| i + |y| i . (",11,12
9253,201627026,"2011) , for each symbol u ∈ Σ, either all tokens of u must be projected to the tier, or no tokens of u may be projected.",5,6
9254,189927986,"Intuitively, strictly local functions are functions computed by SFSTs in which each state represents the i − 1 most recent symbols in the input stream and the j − 1 most recent symbols in the output stream along with the current input symbol, for some parameter values i, j fixed.",43,44
9255,189927986,"Such functions are ""local"" in the sense that the action performed on each input symbol depends only on information about symbols in the input and output streams within a bounded distance.",16,17
9256,189927986,"There, the special symbol @, which is not part of the input alphabet, indicates the location of a reduced vowel, effectively recording the previous action in the output.",4,5
9257,189927986,"Since there is no way to mark the location of a deleted symbol, the SFST in Figure 2 explicitly records its previous action in its state names.",12,13
9258,189927986,"V : λ V : V C : C V : λ C : C V : V C : C V : λ Recall that at each time step, an SFST must read exactly one input symbol while producing an output string of any length.",38,39
9259,189927986,"These actions do not record any output symbols, but they must be kept on the tier in a TSSL implementation so that the input symbol x can be recovered.",25,26
9260,189927986,T always copies the first symbol of its input to the 4 The angle brackets are omitted from the state names.,5,6
9261,189927986,"Thereafter, T behaves as follows: all as are deleted; a b is changed to a c if the most recent input symbol is the same as the first input symbol; a b is changed to a d otherwise.",24,25
9262,189927986,"Thereafter, T behaves as follows: all as are deleted; a b is changed to a c if the most recent input symbol is the same as the first input symbol; a b is changed to a d otherwise.",32,33
9263,189927986,"As discussed in the proof, an onward SFST computing f must copy the current input symbol to the output stream during each time step.",16,17
9264,189927986,"At the end of the computation, the final output function is responsible for adding the first input symbol to the end of the output string.",18,19
9265,189927986,"Any onward TSSL SFST that attempts to compute f will eventually forget the identity of the first input symbol, so the final output function cannot determine what to add to the output.",18,19
9266,189927986,"If the first symbol of its input is an a, then T behaves in an onward manner, copying the current input symbol at each time step.",3,4
9267,189927986,"If the first symbol of its input is an a, then T behaves in an onward manner, copying the current input symbol at each time step.",23,24
9268,189927986,"If the first symbol of T 's input is a b, then T alternates between producing no output and producing two symbols of output.",3,4
9269,189927986,"Every time T performs a non-deleting action x : y, y contains both the symbol that the onward SFST would produce at the current time step and the symbol that the onward SFST would have produced at the previous time step.",17,18
9270,189927986,"Every time T performs a non-deleting action x : y, y contains both the symbol that the onward SFST would produce at the current time step and the symbol that the onward SFST would have produced at the previous time step.",31,32
9271,189927986,"This way, T encodes the identity of the first symbol of its input using the manner in which it produces output-if T produces output at every time step, then the first symbol is an a, and if it produces output every two time steps, then the first symbol is a b. In general, this kind of encoding trick can be applied to a wide range of SFSTs, including all SFSTs V that do not perform deletions.",10,11
9272,189927986,"This way, T encodes the identity of the first symbol of its input using the manner in which it produces output-if T produces output at every time step, then the first symbol is an a, and if it produces output every two time steps, then the first symbol is a b. In general, this kind of encoding trick can be applied to a wide range of SFSTs, including all SFSTs V that do not perform deletions.",35,36
9273,189927986,"This way, T encodes the identity of the first symbol of its input using the manner in which it produces output-if T produces output at every time step, then the first symbol is an a, and if it produces output every two time steps, then the first symbol is a b. In general, this kind of encoding trick can be applied to a wide range of SFSTs, including all SFSTs V that do not perform deletions.",53,54
9274,5476154,Deleting the least frequent symbol from a string is non-finite-state because such a mapping requires counting the number of occurrences of each symbol.,4,5
9275,5476154,Deleting the least frequent symbol from a string is non-finite-state because such a mapping requires counting the number of occurrences of each symbol.,26,27
9276,5476154,"In HS, GEN only produces candidates that differ from the input by one symbol.",14,15
9277,5476154,"The ab sequence in the input aaabb cannot be destroyed by deleting only one symbol, so MAX simply chooses the faithful candidate.",15,16
9278,5476154,"A single change to a string is defined as insertion, substitution, or deletion of a single symbol in that string.",18,19
9279,5476154,"In particular, I have assumed that ""one change at a time"" means insertion, deletion, or substitution of a single symbol.",24,25
9280,227231595,"We created an alphabet of 76 symbols representing all the characters that appear at least 50 times in the training data, plus a 'NA' symbol, and then we used one-hot encoding vectors as input to the CNN.",27,28
9281,2082777,Each symbol corresponds to a word token in the text segment.,1,2
9282,2082777,"θ k |k denotes the probability of transitioning to state k given that the preceding state is k. φ v|k denotes the probability that a particular symbol emitted during a visit to state k is the word v. As in standard treatments, we assume an extra final state at the end of the sequence that emits a stop symbol.",26,27
9283,2082777,"θ k |k denotes the probability of transitioning to state k given that the preceding state is k. φ v|k denotes the probability that a particular symbol emitted during a visit to state k is the word v. As in standard treatments, we assume an extra final state at the end of the sequence that emits a stop symbol.",59,60
9284,2082777,"During estimation, we concatenate all segments into a single sequence, delimited by a special boundary symbol.",17,18
9285,865335,"Note the difference from traditional HMMs, in which a single observation symbol is drawn at each time step.",12,13
9286,52011869,"The abstraction of a sentence is [f 1 , dis 1 , f 2 , • • • , o] where f i denotes semantic frames, dis i denotes discourse markers and o denotes period symbol.",38,39
9287,3079803,The • symbol denotes composition.,2,3
9288,174801632,"From this, we create training data that contains relation statements in which the entities have been replaced with a special [BLANK] symbol, as illustrated in Figure 1 .",24,25
9289,174801632,"r N , e N 1 , e N 2 )] where each ri = (x i , s i 1 , s i 2 ) contains a relation statement in which one or both entity mentions may have been replaced by a special [BLANK] symbol.",49,50
9290,174801632,"Otherwise, the span has been replaced with a single [BLANK] symbol.",13,14
9291,2853512,A ground atom is an expression formed by applying a predicate symbol to the correct number of constant symbols as arguments.,11,12
9292,2853512,A variable atom is an expression formed by applying a predicate symbol to the correct number of variables as arguments.,11,12
9293,2853512,Each node is a word or predicate symbol and there is an edge for each pair of nodes.,7,8
9294,14940654,"We found that using one past symbol (bigram model) works better than other n-gram based methods for English to Persian transliteration (Karimi et al.,",6,7
9295,14940654,A special symbol is used to indicate the start and/or end of each word if the beginning and end of the word is a consonant respectively.,2,3
9296,14940654,"Therefore, for the words starting or ending with consonants, the symbol ""#"" is added, which is treated as a consonant and therefore grouped in the consonant segment.",12,13
9297,233025289,"This makes our method different from traditional beam search, the latter terminates when an end-of-sequence symbol [SEP] is generated for the summary.",20,21
9298,233025289,"We obtain the predicted length L pred using a baseline abstractive summarizer, which takes the source text as input and greedily decodes a summary in a left-to-right manner until an end-of-sequence symbol is predicted; L pred is the length of the decoding sequence.",40,41
9299,244077684,"In order that a meaningful sequence of symbols could be detected regardless of the successive columns of the card in which they are punched, IBM was recently persuaded to develop a new type of punching worked out in such a way that a predetermined combination of five holes in any column is used to indicate some symbol, e.g. a letter or numeral.",57,58
9300,1283622,"2005) , the BKY algorithm uses EM to estimate probabilities on symbols that are automatically augmented with latent annotations, a process which can be viewed as symbol splitting.",28,29
9301,1283622,"In the case of AUTO-CATLEMMA, morphological ambiguity is totally absent in training set: each terminal symbol is the gold POS+lemma pair, and hence appears with a unique part-of-speech in the whole training set.",19,20
9302,1283622,"More precisely, for instance about 30% of the original unseen in the dev set, are replaced by a UNKC* symbol, which means that 70% are replaced by a cluster-based symbol and are thus ""connected"" to the known vocabulary.",23,24
9303,1283622,"More precisely, for instance about 30% of the original unseen in the dev set, are replaced by a UNKC* symbol, which means that 70% are replaced by a cluster-based symbol and are thus ""connected"" to the known vocabulary.",37,38
9304,27957304,"A simple multiplication of the fractions, which results in the cancellation of like quantities in the numerator and denominator, results in a unique symbol indicative of the functions of the word block so analyzed.",25,26
9305,42802688,"Eventually, in response to the needs of computational linguists, procedural languages such as COMIT were developed; in addition, other higher-level symbol manipulation languages such as LISP and SNOBOL proved useful for linguistic applications in mind.",26,27
9306,61207035,"The word ""process"" means that a theory of understanding should be stated in a symbol-processing manner, one in which most linguistic theories are not.",16,17
9307,244077656,"As to the logical interpretation, the theory does not define the relation of predication (though it can cope with the distinction between monadic, dyadic etc, relations), but it does explain why, however predicative logic is developed, the symbol P remains unique and unchanged; for this point, our Z.Z, is one of the four vertices of the lattice which can be transformed, by inversion of factors in the lattice, into the upper or the lower bound, just as in logic P is the point from which, no matter how far the x -sequence is extended the whole system of relation always hangs.",45,46
9308,1485865,"In this model, given a left-hand side symbol, the head and its arguments are first generated and then the MNT are generated from the head outward.",10,11
9309,36728371,"We fomalise these supplementary replacement restrictions as follows: Creation of Replacement-Variables in T (i) Let the restriction, ""The pattern of replacement is as for the earlier position, X, in p, except that the same replacement must be used"", be formalised as follows: We insert the symbol X 1 after the K-point for position X, and X 2 of the K-point for the position which, in the replacement-specification, carries the instruction.",58,59
9310,36728371,"If more than one position carries this instruction, the symbols W and V can be used, in addition to the symbol X. Let the replacement-restriction just specified above be called Restriction A. (ii) Let the restriction, ""The pattern of replacement is as for the earlier position, Y, in p except that the same replacement must not be used"", be formalised as follows: We insert the symbol Y after the K-point for position Y, and the symbol -Y (""not-Y"") after the K-point for the position which, in the replacement pattern, carries the instruction.",22,23
9311,36728371,"If more than one position carries this instruction, the symbols W and V can be used, in addition to the symbol X. Let the replacement-restriction just specified above be called Restriction A. (ii) Let the restriction, ""The pattern of replacement is as for the earlier position, Y, in p except that the same replacement must not be used"", be formalised as follows: We insert the symbol Y after the K-point for position Y, and the symbol -Y (""not-Y"") after the K-point for the position which, in the replacement pattern, carries the instruction.",78,79
9312,36728371,"If more than one position carries this instruction, the symbols W and V can be used, in addition to the symbol X. Let the replacement-restriction just specified above be called Restriction A. (ii) Let the restriction, ""The pattern of replacement is as for the earlier position, Y, in p except that the same replacement must not be used"", be formalised as follows: We insert the symbol Y after the K-point for position Y, and the symbol -Y (""not-Y"") after the K-point for the position which, in the replacement pattern, carries the instruction.",91,92
9313,245118156,The '+' sign is used when a sentence is to be regarded as true; '-' is a symbol for a false sentence.,22,23
9314,244077624,"Following Pollard, the first operation to be carried out on a symbol-sequence recognised as a unit of analysis might be the count for the number of commas in this unit.",12,13
9315,27354720,"The strings ,  are defined on a vocabulary V = V N UV T , where V N is the non-terminal vocabulary which includes an initial symbol S meaning sentence.",30,31
9316,27354720,"The storage tape vocabulary is V 0 = eU{A k }U, where e is the null element, and  a special symbol which is never printed out.",24,25
9317,27354720,The squares on which the strings are written are occupied simultaneously by both a i (or A k or ) and e; either infinite side of the string can be thought of as filled with the blank symbol #.,40,41
9318,27354720,"A situation  of the PDS automaton is a triplet  = (a i , S j , A k ); if the PDS is in the situation  it is also in the situations where the elements a i or A k or both are replaced by e. There is an initial situation (a i , S 0 , ) where the input head is positioned on the leftmost square of the input string, and the storage head on the symbol . A computation starts in an initial situation and is directed by a finite number of instructions I, (S r , x); when the automaton returns to the initial situation the first time, x = . From the situation , where the automaton is in state S j , the automaton switches into the state S r and moves its input tape one square left if the first element of  is an a i , otherwise (for e) the tape is not moved.",86,87
9319,27354720,"If x =  the storage tape is moved one square right, nothing is printed and the square A k previously scanned is replaced by the blank symbol #.",28,29
9320,27354720,"After an instruction I has been carried out, the automaton is in the new situation ' whose first element is the symbol of the input string now being scanned, the second element is S r , the third element can be either the same A k if x = e or A m if x = A m , or if x = , the rightmost symbol of A k written on the storage tape.",23,24
9321,27354720,"After an instruction I has been carried out, the automaton is in the new situation ' whose first element is the symbol of the input string now being scanned, the second element is S r , the third element can be either the same A k if x = e or A m if x = A m , or if x = , the rightmost symbol of A k written on the storage tape.",70,71
9322,27354720,"The symbol S, meaning sentence and  meaning period (end of sentence) are P i 's. The set I of instructions contains: I 1 : (e, S 0 , a)  (S 1 , e) I 2 : (e, S 1 ,e)  (S 2 , S) I 3 : (s j , S 2 , P k ) (S Pk ,) I 4 : (e, S Pk , e)  (S 2 , x) I F : (s j , S 2 , )  (S 0 ,) The set of the states is {S 0 , S 1 , S 2 , [S Pk | P k  P]}.",1,2
9323,46532082,"Not to mention, of course, that all-time status symbol, the bumper sticker carrying the same text and serving to fatten the pockets of some enterprising graduate student, while providing the more well-heeled members of the trade with a convenient shibboleth.",12,13
9324,207974610,"In particular, a context-free phrase structure grammar, a CF grammar for short, may be defined, again in slight variation from Chomsky's original definition, as an ordered quadruple < V, T, S, P>, where V is the (total) vocabulary, T (the terminal vocabulary) is a subset of V, S (the initial symbol) is a distinguished element of V-T (the auxiliary vocabulary), and P is a finite set of production rules of the form X x, where XV-T and x is a string over V. We say that a string x directly generates y, if y results from x by one application of one of the production rules, and that x generates y, if y results from x by finitely many applications of these rules (more exactly, if there exist sequences of strings z 1 ,z 2 ,...,z n such that x = z 1 , y = z n and z i directly generates z i+1 , for i = 1,..., n-1).",70,71
9325,207974610,"These grammars are equivalent to grammars in which all categories have the form \/ where ,, and  are fundamental categories and where  and  may be empty (in which case the corresponding diagonal will be omitted, too, from the symbol).",45,46
9326,207974610,Each proper symbol is well-formed (wf) F2.,2,3
9327,207974610,"With 'C' as the only improper symbol and F2 changed to: Whenever  and  are wf, so is C, expansion of  (though not of ) causes inner branching.",8,9
9328,207974610,"Surely computers can manipulate symbols if given the proper instructions and they do it splendidly, many times quicker and safer than humans, but the distance from symbol manipulation to linguistic understanding is enormous, and loose talk will not diminish it.",28,29
9329,245118159,"This is not to say that formal inquiry, when it confines its interest to techniques of symbol manipulation, somehow escapes the same vice of specialization.",17,18
9330,245118159,"The symbol ""act"" would stand for any elemental or composite constituent of a whole but unique universe, one among others named by the symbol ""mind,"" whose personal and partly intimate point of view would be felt as the very direction of the act.",1,2
9331,245118159,"The symbol ""act"" would stand for any elemental or composite constituent of a whole but unique universe, one among others named by the symbol ""mind,"" whose personal and partly intimate point of view would be felt as the very direction of the act.",26,27
9332,245118159,The initial grammar consisted of rules which placed each graphic symbol of the text in a one-member class.,10,11
9333,245118159,The machine would in fact create such a rule for any new symbol it came across in the text.,12,13
9334,245118159,"The events monitored for vertical classifications will be rule substitutions in perceptual experience, as jointly given by the symbol being substituted and the symbol at the place of substitution.",19,20
9335,245118159,"The events monitored for vertical classifications will be rule substitutions in perceptual experience, as jointly given by the symbol being substituted and the symbol at the place of substitution.",24,25
9336,245118159,The clusters of symbols being substituted will then be matched to clusters of places of substitution to detect those concentrations of affinity which will define more specialized classes to be named by new symbol It will be found that these vertical classifications can be carried out for the substitutable symbols and the places of substitution instancing the name of a single class.,33,34
9337,245118159,"Semantic synthesis, starting from a given symbol naming a class of syntactic segments, will substitute semantic rules in order to construct a member of that class.",7,8
9338,36592297,One of the consonants has a special symbol when it represents a feminine ending (this is a compromise between two pronunciations of the feminine ending).,7,8
9339,37143806,"The structural changes that may be performed by transformations as we have formalized them are limited to the substitution of a sequence of trees (including possibly the null sequence) for a single tree, a process which is followed by erasure of all nonterminal nodes that dominate no terminal symbol.",51,52
9340,37143806,"Further reversing the generative procedure, it remains only to determine which elements of this set are analyzable as the sentence symbol 331 with respect to G. Every deep structure of S with respect to the given transformational grammar must be included in the set of trees thus obtained.",21,22
9341,34140950,"The simplest conceivable written language would have one symbol per idea, together with appropriate rules, possibly involving extra symbols for syntactical relations.",8,9
9342,16551891,Each edge of an SST has an associated input symbol and output string.,9,10
9343,11177932,"In determining the initial node, only central contexts are tailored by replacing one or more contained words with ∅ symbol, which in turn can match to any word.",20,21
9344,15179147,"Since hashtags are considered as good indicators of topics in the tweets, we investigated two different ways of using hashtags as features: first, considering hashtags in the same way as words, and second, removing the # symbol and treating hashtags as normal words.",41,42
9345,15179147,The bottom part of the table shows the clustering results by removing the # symbol and treating hashtags as normal words.,14,15
9346,15179147,We believe that this improvement is because removing the # symbol contributes to increasing the term frequency of the same topic word in the tweets.,10,11
9347,52184953,"To illustrate, let us construct a PDT that computes the function f w# |w| = # |w| w R , where w R is the reverse of w and # |w| is a sequence of #s of the same length as w. We can begin to compute f using a single state q 0 by pushing each symbol of w onto the stack while emitting #s as output.",60,61
9348,52184953,"Since in the general case, correctly producing w R requires recording w in the stack, we evaluate the network solely based on the portion of its output where w R should appear, immediately after reading the last symbol of w. XOR Evaluation We consider two tasks that require the network to implement the XOR function.",40,41
9349,52184953,"The Delayed XOR Evaluation task is similar, except that the most recent input symbol is excluded from the XOR computation.",14,15
9350,52184953,"However, in the Delayed XOR Evaluation task, the delay between reading an input symbol and incorporating it into the XOR gives the network two linear layers to compute XOR when unravelled through time.",15,16
9351,52184953,"At each time step t, the network reads the tth symbol of some string and must attempt to output the (t + 1)st symbol.",11,12
9352,52184953,"At each time step t, the network reads the tth symbol of some string and must attempt to output the (t + 1)st symbol.",25,26
9353,52184953,"S → S S ∨ | S S ∧ S → T | F At each time step, the network must output the truth value of the longest sub-formula ending at the input symbol.",36,37
9354,52184953,Note that the vectors pushed onto the stack in the presence of input symbol 1 vary between two possible values that represent the current parity.,13,14
9355,1261609,The three inference rules are: 1) match a terminal symbol and move across one edge in the lattice 2) move across an -edge without advancing the dot in an incomplete rule 3) advance the dot across a nonterminal symbol given appropriate antecedents.,11,12
9356,1261609,The three inference rules are: 1) match a terminal symbol and move across one edge in the lattice 2) move across an -edge without advancing the dot in an incomplete rule 3) advance the dot across a nonterminal symbol given appropriate antecedents.,42,43
9357,174799037,Other words were replaced by a special symbol UNK.,7,8
9358,235313334,Our assumption is that the same intruder symbol may be repeated in one word.,7,8
9359,222124957,MLM replaces some tokens with a [mask] symbol and provides both right and left contexts (bidirectional context) for predicting the masked tokens.,9,10
9360,3829104,The symbol tilde (~) indicates that the suffix string followed by the tilde (~) notation makes another new word concatenating with the original entry word.,1,2
9361,3097631,"If the lexical item is a verb, the corresponding tree is a skeleton for an entire sentence with the verb already present, anchoring the tree as a terminal symbol.",30,31
9362,2734893,"All punctuation in the word containing the location had to be removed, including the hash symbol (#).",16,17
9363,231942445,The symbol before ?,1,2
9364,222291230,We choose the inserted symbol randomly but in case of multiple insertions into one word keep the symbol identical.,4,5
9365,222291230,We choose the inserted symbol randomly but in case of multiple insertions into one word keep the symbol identical.,17,18
9366,222291230,"For each two characters, φ indicates how likely the insertion of a symbol between them is.",13,14
9367,8480105,"The variables S, he, eats, rice are symbol IDs obtained from a vocabulary (not shown here).",10,11
9368,13898507,"Finally, Text Under Graphic is the text under a graphic which usually starts with a marker symbol (such as *) and is essentially a footnote (such as U.S. only, one available seat flown one mile, year ending June 2002 in Figure 7b ).",17,18
9369,9936276,"The search space over which we minimize is implicitly represented as the Recursive Transition Network H (see equation ( 2 )), where Π x is encoded as a weighted FSA that represents the set of permutations of x with their associated distance costs, and LS is the one-state Levenshtein transducer whose output weight for a string pair (x,y) is the Levenshtein distance between x, and y, and the symbol • denotes composition.",80,81
9370,6277509,"1 x 0 is a special wall symbol, $, on the left.",7,8
9371,18559393,"∃X∃Y( çocuk(X) ∧ zeytin(Y) ∧ ye(Event, X,Y, In the Prolog program, existentially quantified expressions like this one have the form some(X,Restrictor,Scope), where X is the quantified variable, and Restrictor and Scope are the two sides of the conjunction (Covington, 1994 ) some(X,çocuk(X),some(Y,zeytin(Y),ye (EventMarker, X,Y,Location,Time,Goal,Source,Instrument, definite_past,none,positive) )) The successful DCG parsing of a sentence also results in a field being instantiated to a symbol representing the sentence's mood.",104,105
9372,14200458,is given by the probability of its root symbol times the conditional probabilities of the rules.,8,9
9373,240707265,"For it is what we get if we express the chain of determination of the occurrence without citation of specific substituents, but replace each by some symbol expressive only of their syntactic function as determined by equipollence. (",27,28
9374,240707265,"In the recursive/conjunct type, the new OP introduced by the governor needs a new symbol; I shall write it ZC.",17,18
9375,240707265,"In the exocentric type, in addition to S for the dependents we shall need a new symbol for the OP of the governor, and this I shall write O (for operative).",17,18
9376,240707265,"If the OP of the compound is X, I shall write that of its recursive governor as ZC.X. In the conjunct type, since the OP of the compound is indicated by that of the dependents, we only need one symbol for that of the governor, for which I choose IC (standing for indeterminate conjunction).",42,43
9377,240707265,"An endocentric compound requires a new symbol for the OP of its dependent; if its own OP is X, I shall write that of its dependent as XA.",6,7
9378,790934,"This is the ""symbol grounding problem"".",4,5
9379,790934,"This is a problem of infinite regress, called the ""symbol grounding problem"" (Harnad, 1990; Harnad, 2003) : the meanings of words in dictionary definitions are, in and of themselves, ungrounded.",11,12
9380,790934,"The remainder of this paper is organized as follows: In Section 2, we introduce the graph-theoretical definitions and notations used for formulating the symbol grounding problem in Section 3.",27,28
9381,790934,"Note that (not, good, eatable, fruit) is a path of G while (good, bad, good) is a cycle (as well as a path) of G. A Graph-Theoretical Formulation of the Problem We are now ready to formulate the symbol grounding problem from a mathematical point of view.",51,52
9382,790934,"Therefore, in studying the symbol grounding problem in dictionaries, we can restrict Let W be the set of vertices of G 5: U ← {v ∈ W | N + G (v) = ∅} 6: G ← G [W − U ] 7: until U = ∅ 8: return G 9: end function ourselves to the grounding kernel of the graph G corresponding to D. This phenomenon is interesting because every dictionary contains many words that can be recursively removed without compromising the understanding of the other definitions.",5,6
9383,790934,"K G = {bad, Grounding Sets and the Mental Lexicon In Section 3, we introduced all the necessary terminology to study the symbol grounding problem using graph theory and digital dictionaries.",25,26
9384,790934,A dictionary is a formal symbol system.,5,6
9385,790934,"In cognitive science, this is the basis of computationalism (or cognitivism or ""disembodied cognition"" (Pylyshyn, 1984) ), according to which cognition, too, is a formal symbol system -one that can be studied and explained independently of the hardware (or, insofar as it concerns humans, the wetware) on which it is implemented.",35,36
9386,790934,"Although computationalism and symbol grounding provide the background context for our investigations and findings, the present paper does not favor any particular theory of mental representation of meaning.",3,4
9387,790934,A dictionary is a symbol system that relates words to words in such a way that the meanings of the definienda are conveyed via the definientes.,4,5
9388,790934,Future Work The main purpose of this paper was to introduce a formal approach to the symbol grounding problem based on the computational analysis of digital dictionaries.,16,17
9389,236486140,"Moreover, we attempt to identify the linguistic features of our parallel corpus, including # of sents/tokens/words (number of sentences/tokens/words); avg of SL/WS/SS (average of sentence length/words/spaces per sentence); and * # of K/E/S-toks (number of Korean/English/special-symbol letter tokens).",72,73
9390,236486140,"In addition, we remove pairs of sentences that are identical or which consist of special symbol tokens comprising more than 50% of the total tokens , as these sentences may not be inconsistent with the learning method.",16,17
9391,236486140,"The source sentences in the training, validation, and test datasets lost 320,924, 6,141, and 6,799 special symbol tokens, respectively.",20,21
9392,236486140,Such special symbol tokens may sometimes contain actual colloquial tones or emotions that are not represented by the text adequately.,2,3
9393,236486140,"Thus, excessive omission of special symbol tokens is equivalent to the loss of rich representation information of colloquial forms that are different from written ones.",6,7
9394,236486140,"32 times, the percent sign (""%"") 33 times, and the dollar symbol (""$"") 1 time.",17,18
9395,13036650,The symbol ~i specifies the variety and the abstracting level of each unit.,1,2
9396,5623056,"Because these human-generated expressions have all been pre-annotated, we treat language and logic interchangeably and refer to both with the symbol e. We write e(W ) for the expression generated by a human for a particular world W , and e W for the result of evaluating the logical form e against W .",25,26
9397,215516088,"In addition, we include up to four initial tree pairs rooted in the start symbol of the grammar and with a single adjunction sites into which the auxiliary trees can adjoin.",15,16
9398,23942431,This assumes that the root symbol is <S>.,5,6
9399,2089367,A symbol with an exclamation mark (!),1,2
9400,7102676,"The words which denote values of domains,i.e, which appear in the content of the data base, are preceded and followed by the special symbol"" in the input sentence and remain unaltered in the internal representation.",27,28
9401,764565,"2) An SML expression is generated, by introducing a new variable symbol in the form 'SYS**', whenever a partial result of parsing becomes sufficient to do so. (",13,14
9402,33303205,"Linear programming aim: explore the connections between dual decomposition and linear programming • basic optimization over the simplex • formal properties of linear programming • full example with fractional optimal solutions • tightening linear program relaxations Simplex define: • ∆ y is the simplex over Y where α ∈ ∆ y implies α y ≥ 0 and y α y = 1 • ∆ z is the simplex over Z • δ y : Y → ∆ y maps elements to the simplex example: Y = {y 1 , y 2 , y 3 } vertices • δ y (y 1 ) = (1, 0, 0) • δ y (y 2 ) = (0, 1, 0) • δ y (y 3 ) = (0, 0, 1) such that for all i, t y α y y (i, t) = z β z z(i, t) Lagrangian Lagrangian: M(u, α, β) = y α y f (y ) + z β z g (z) + i,t u(i, t) y α y y (i, t) − z β z z(i, t) = y α y f (y ) + i,t u(i, t) y α y y (i, t) + z β z g (z) − i,t u(i, t) z β z z(i, t) Lagrangian dual: M(u) = max α∈∆ y ,β∈∆ z M(u, α, β) Strong duality define: • α * , β * is the optimal assignment to α, β in the linear program Primal relationship define: • Q ⊆ ∆ y × ∆ z corresponds to feasible solutions of the original problem Q = {(δ y (y ), δ z (z)): y ∈ Y, z ∈ Z, y (i, t) = z(i, t) for all (i, t)} • Q ⊆ ∆ y × ∆ z is the set of feasible solutions to the LP Q = {(α, β): α ∈ ∆ Y , β ∈ ∆ Z , y α y y (i, t) = z β z z(i, t) for all (i, t)} • Q ⊆ Q solutions: max q∈Q h(q) ≤ max q∈Q h(q) for any h Concrete example • Y = {y 1 , y 2 , y 3 } • Z = {z 1 , z 2 , z 3 } • ∆ y ⊂ R 3 , ∆ z ⊂ R 3 Advanced examples aim: demonstrate some different relaxation techniques • higher-order non-projective dependency parsing • syntactic machine translation Higher-order non-projective dependency parsing setup: given a model for higher-order non-projective dependency parsing (sibling features) problem: find non-projective dependency parse that maximizes the score of this model difficulty: • model is NP-hard to decode • complexity of the model comes from enforcing combinatorial constraints strategy: design a decomposition that separates combinatorial constraints from direct implementation of the scoring function Non-projective dependency parsing structure: • starts at the root symbol * • each word has a exactly one parent word • produces a tree structure (no cycles) such that for all i, j y (i, j) = z(i, j) Algorithm step-by-step [Animation] Syntactic translation decoding setup: assume a trained model for syntactic machine translation problem: find best derivation that maximizes the score of this model difficulty: • need to incorporate language model in decoding • empirically, relaxation is often not tight, so dual decomposition does not always converge strategy: • use a different relaxation to handle language model • incrementally add constraints to find exact solution Syntactic translation example [Animation] Summary presented dual decomposition as a method for decoding in NLP formal guarantees • gives certificate or approximate solution • can improve approximate solutions by tightening relaxation efficient algorithms • uses fast combinatorial algorithms • can improve speed with lazy decoding widely applicable • demonstrated algorithms for a wide range of NLP tasks (parsing, tagging, alignment, mt decoding) Simple solution • α (1) = (0, 0, 1) ∈ ∆ y is representation of  • α (2) = (0.5, 0.5, 0) ∈ ∆ y is combination of y 1 and y 2 Optimal solution weights: • the choice of f and g determines the optimal solution ), the optimal solution is fractional Tightening (Sherali and Adams, 1994; Sontag et al.,",599,600
9403,2707891,"While that parser uses heavily refined PCFGs with rule probabilities defined at the refined symbol level, we interact with its posterior distribution via posterior marginal probabilities over unrefined symbols.",14,15
9404,2707891,We determine types at both a coarse (more collapsed than Treebank symbols) and fine (Treebank symbol) level.,18,19
9405,5782458,"Using a built-in backtracking algorithm, ROBRA finds the first possible traversal of the control graph leading to an exit (&NUL symbol), thereby applying each traversed TG to the object tree.",25,26
9406,59777820,The node name indicates a semantic symbol.,6,7
9407,18455650,"Similarly, the posteditor can add a syntactic symbol such as a part of speech a technical term a compound and an idiom to a word group involved in the given new word sequence if necessary.",8,9
9408,18455650,<:NP(t)> and <INF(t)> denote the non-ternfinal sym~ bolz of ~z noun phrase and an infinitive phrase correspondirq~ to a term expression o1' an intermediate form t. The symbol * denotes the term prefixed to a frame which includes and modifies the symbol *.,37,38
9409,18455650,<:NP(t)> and <INF(t)> denote the non-ternfinal sym~ bolz of ~z noun phrase and an infinitive phrase correspondirq~ to a term expression o1' an intermediate form t. The symbol * denotes the term prefixed to a frame which includes and modifies the symbol *.,51,52
9410,19401538,Kana is a phonetic symbol and kanji is an ideograph.,4,5
9411,19401538,Example i. Letter Meaning 'X [-.......... 1 74 X dash K one (3 symbol for long vowel The selection from these resembling patterns depends on their context. ',17,18
9412,45489201,"IIowever, the un~ fJcatJon procedure is confined to a unilateral unification from a module to a specification in which each symbol is J nterpreted as a constant under the condition that any substitution for the symbol Jn the specifications is forbidden.",21,22
9413,45489201,"IIowever, the un~ fJcatJon procedure is confined to a unilateral unification from a module to a specification in which each symbol is J nterpreted as a constant under the condition that any substitution for the symbol Jn the specifications is forbidden.",36,37
9414,13900501,"The Twitter clean version removes HTML code, URLs, user mentions(@), the # symbol of hashtags, and all the retweeted tweets.",16,17
9415,237295802,"It sounds marvellous, and it is, but it has limitations: the OCR will only read a limited number of typewriter faces, and may even balk at an equivalent typeface; it will only read portrait, not landscape; if it cannot read a letter, it transmits a block symbol, which is helpful, but it may also think it reads a character correctly and be wrong.",55,56
9416,46111577,The applied rule attaches a semantic symbol to the new node and determines the semantic relation between two nodes in the analysis window.,6,7
9417,221970201,"Input Output Input Output Input Output AOR (N = 10, L = 6) 6 10 9 5 2 3 − 6 / 10 + 9 / 5 * 2 == 3 2 2 4 8 2 4 2 * 2 − 4 + 8 / 2 == 4 2 2 4 8 2 4 − 2 + 2 / 4 * 8 + 2 == 4 AES (N = 10, L = 4) − 3 + 10 / 2 == 2 − 3 + 10 / 2 == 2 (− 2 + 4 ) / 7 * 7 == 2 2 / 7 * 7 == 2 2 / 7 * ( 11 − 4 ) == ( 4 − 2 ) 2 / 7 * 7 == 2 AEC (N = 10, L = 5) 4 − 3 / 6 * 4 == 2 4 − 3 / 6 * 4 == 2 6 7 * + / 7 + / 7 == 2 − 7 * 5 / 7 + 7 == 2 − 6 5 + 11 − 2 − 6 + 5 + 11 − 8 == 2 Editing Actions An editing action contains (i) the type of editing operation, (ii) the position the editing occurs, and (iii) a text symbol.",246,247
9418,221970201,"Some operations, such as deletion, do not need a symbol input, so, the symbol component can also be omitted.",11,12
9419,221970201,"Some operations, such as deletion, do not need a symbol input, so, the symbol component can also be omitted.",17,18
9420,237295783,"Typically the ruler line is a row of dots with a number instead of every tenth dot, and some other symbol such as a colon at the intervening fifth dots: 0….:….1….:….2…..:….3…..:…..4….. etc.",21,22
9421,21973259,"Especially, the former command brings easy achievement of tabulations or mathematics, like a horizontal line in a fraction or a root symbol.",23,24
9422,738216,"The next example shows some Japanese sentences which have the main verb illustrated in Table 2 , the internal expressions and the corresponding English sentences , where the postpositions in Japanese sentences are enclosed with parentheses, and the symbol "" * "" in the internal expressions denotes the term which is in front of the f~ame including the s~r~ol.",39,40
9423,6622721,where C denotes a certain case for the outside governor and the symbol * denotes the prefixed term to the case frame containing it.,12,13
9424,6622721,"Otherwise, ordinary mode of parsing restarts from term t I V 3 (FKz-62 : ~2' K3-C3 :ts ' VP£ ) --~ V% (]K~-~z : $2' VP{ ) V 5 (K3-C 3 : t 3 ) (4) where V{ ( ~ =i ~ 5) denotes a non-terminal symbol, ~-~ : ~ a sequent of several pai~s of a case label and a category followed by a term, and VPL a label of a verb pattern.",67,68
9425,6622721,"In 'Fable 5, the words with a symbol * denote the equivalent Written in Japanese• At the execution of case transformation, the routine designated by the code is called and carries out the necessary processing.",9,10
9426,237295821,"As I said, the item is identified by up to 16 alphanumeric characters which are enclosed by a format symbol at either end.",20,21
9427,1001406,Information for writing kanji is assumed to be input to/output from the memory device of the cerebrum as a symbol string of kanji-forming stroke vectors.,21,22
9428,1001406,"At the same time, as a graded structure is observed in the stroke symbol string, transinformation content between compound events and possibility of compression are also calculated by setting each previous section as an encoding element, in which a close coupling relation is found between them, such as the transinformation amount between compound elements of 3-strokes, is about ten times as much as in case of sectioning.",14,15
9429,10388702,"In effect, the VENUS interlingua becomes a TL transfer intermediate representation for the purposes of this experiment Additionally, a device essentially the same as a transfer dictionary was employed, which, however, instead of transferring words, transfers word concepts (i.e, the metalinguistic symbol representation of the reference of the lexeme, rather than a canonical morpheme representing the name of the word).",49,50
9430,16922608,The remaining four alphabetic symbols used to represent the spelling have the following properties:- The first symbol can be any alphabet from A to Z (except V).,16,17
9431,16922608,"The second symbol can be a blank, A, E, H, I, M, N, O, R, U or V, a total of eleven possibilities.",2,3
9432,16922608,"The third symbol can be a blank, A, E, G, I, N, O, or U, a total of eight possibilities.",2,3
9433,16922608,"The fourth symbol can be a blank, A, G, I, N, O, or U, a total of seven possibilities.",2,3
9434,1960126,The number of nodes in the graph is equal to the number of word form tokens in the sentence plus that of punctuation signs and a symbol for the sentence as such (the root of the tree).,26,27
9435,2838871,"However, in order to make the computer process Kana sentences easy, it would be necessary to put a space as a segmental symbol between words or some units in sentences.",24,25
9436,2838871,"In the above example, an underline denotes a word and a slant does a segmental symbol between Bunsetsu.",16,17
9437,9882071,"If r6H then r is said to be an n-1 -ary function symbol, if r~H then r is said to be an n-ary relation symbol.",12,13
9438,9882071,"If r6H then r is said to be an n-1 -ary function symbol, if r~H then r is said to be an n-ary relation symbol.",29,30
9439,5849325,The symbol of '~' represents a KANJI.,1,2
9440,9955107,17 is a special symbol.,4,5
9441,16505774,"The parsing strategy is fundamentally a bottom-up strategy, which may be defined recursively as follows : For each syntactic unit (except Words), execute the following steps : -the segmentation of this unit into its own internal syntactic units, -the parsing of these internal units according to a definite order, -the determination of the semantic relationships between the internal units, -the substitution of the given unit, at the next higher level, by a special symbol, which represents the analyzed unit.",84,85
9442,16505774,"f) ((SB) (PP) (SB)) Figure 3 : segmentation of a sentence a) the original French sentence b) the English translation c) the input of the segmentation procedure d) the state of the sentence after the analysis of the relative clause ""auxquels vous vous attachez"", which is replaced by the special symbol PR e) the state of the sentence after the analysis of the relative clause ""qui vous rendent de l'affection"", which is replaced by PR f) final state of the sentence : the main clause was found and replaced by PP concerning the order for parsing the internal units at a given level, two strategies are applied, one for clauses, and one for groups.",66,67
9443,2744607,"In this definition symbol ,+t is used to designate the [ j s01] [ is01 ]))]+]', [%R(s02...S0n)]'=lj~i(i-A(~il...),is02...~i m (~ j s02(vJ[ i I ]...[ j s02]""'[ i m] ^ [%R(s03...S0n)]' [Js02][is02])))), [%R(S0n)]'= ljNi(i"" A(%i l...Xis0...~im(~-js0n( vj[il]...[Js0n]...[im] A iS0 n"" A( %ii... )~iSll...lis2 I...%im(3 j Sl 1 (Vj SOn[ il ]... [Js11]"""" [is21]'""[i m] ^ ([$R(s22...S2q)]' [is2 I] ÷ [*R(Sl2...Slk)]'[js ][i s ], • 1.1 11 Zs11""JSll)))))))), .[im])]+] ' • where we assumed t=<t I, t 2, ... ,.",3,4
9444,8823877,"The solution of Manber and Myers (1990) exploits this fact along with the observation that each comparison in binary search is carried out according to a fixed recursion scheme: a query is only ever compared against a specific suffix M for a single range of suffixes bounded by some fixed L and R. Hence if we know the longest common prefix between M and each of its corresponding L and R according to the fixed recursions in the algorithm, we can maintain a bound on h and reduce the aggregate number of symbol comparisons to O(|Q| + log |T |).",96,97
9445,1841979,"The object has processes for (a) advancing the analysis with a new input word, (b) for advaacing the analysis when a subparse has been completed (the parse can only be immediately advanced when the next element of the rule-tail is a terminal symbol; otherwise it has to create another object to process a rule expanding the non-terminal category), and (c) for merging with another path.",50,51
9446,231642386,"Two transpositions are possible from a verb context to a nominalised phrase (denoted by the symbol ⇒): • krz ątał się gor ączkowo 'he bustled frantically' ⇒ gor ączkowa krz ątanina 'frantic bustle', • jest zimno na ulicy 'it is cold in the street' ⇒ zimna ulica 'cold street'.",16,17
9447,12272848,Individual tlinks are created by instantiating the input paths by non-default inheritance (indicated by the symbol <=) from entries in the monolingual lexicons. (,18,19
9448,49417344,"bidirectional LSTM provides a prediction for each word, either O if the word is not used in the final query, or a symbol such as city1 to indicate that it fills a slot.",24,25
9449,756123,An empty symbol of a gap is encoded as @gap. † †,2,3
9450,248780232,"We say that a representation is speculative when a symbol in the representation encodes a commitment to a certain syntactic decision, but the evidence for that decision is not present in the corresponding prefix of the input.",9,10
9451,248780232,"We further provide an analysis of the symbols learned by our model, including explorations of the linguistic features captured by the symbol set, the information content of our incremental representation for prefixes of a full utterance, and the system's ability to defer resolution of attachment ambiguities.",22,23
9452,248780232,"By contrast, our analysis has a lesser focus on the attention distribution between tokens, and a greater focus on the features and syntactic decisions captured by each individual symbol.",30,31
9453,248780232,"The decision to label each token with a single symbol is partially rooted in prior research providing evidence that syntactic decisions among human speakers adhere to the uniform information density hypothesis, thus each token may convey similar amounts of syntactic information (Levy and Jaeger, 2006) .",9,10
9454,248780232,"Each projected vector is then converted into a single discrete symbol via vector quantization (van den Oord et al.,",10,11
9455,248780232,"After discretization, each symbol from the sequence is associated with a learned embedding, as specified in the vector quantization codebook.",4,5
9456,248780232,"For example, the discrete symbol associated with a word may help determine a syntactic attachment that concerns previous words that have already been assigned their own symbols.",5,6
9457,248780232,This model uses only 8 bits per token (256 symbols) to define the discrete symbol set using a unidirectional pretrained model (GPT2-medium).,16,17
9458,248780232,"To do this, we trained a number of models while varying the size of the symbol set.",16,17
9459,248780232,"Entropy of Symbol Distribution In our parsing scheme, each new token is assigned a single syntactic symbol based on all tokens up to the current.",17,18
9460,248780232,"At this point, the characteristics of each symbol also roughly stabilize.",8,9
9461,248780232,"Thus, by analyzing the features captured by differently sized symbol sets, we can deduce a rough hierarchy of distinct features that are relevant to the incremental parsing task.",10,11
9462,248780232,"This feature of our symbol set suggests that our tags capture structural context beyond the current word, and the features learned by these tags can have human-interpretable meanings upon analysis.",4,5
9463,248780232,"However, the final token offers additional information that may influence the attachment location, suggesting that the symbol sequence up to the preposition does not eliminate either possible structure, but rather encodes the locations of other likely attachments.",18,19
9464,248780232,"The encoder's decision over whether to mark the final token as symbol 11 or 16 allows the final tree to have an attachment to the verb phrase, rather than adhering to the partial interpretation of targeting the noun phrase.",12,13
9465,248780232,"Conclusion In this paper, we present an approach to inducing syntactic representations that associate each token in the input with a discrete symbol from an arbitrarily-sized vocabulary, where the representations can be predicted incrementally in a strictly append-only manner.",23,24
9466,10512592,"a n ] whose elements a, are terms, record-like structures consisting of a head, a type symbol, and a body, a list of attribute-value pairs : attribute => value .",21,22
9467,14948394,"Thus, the following are two distinct productions in an ITG: C → [AB] C → 〈AB〉 Consider each nonterminal symbol to stand for a pair of matched strings, so that for example (A 1 , A 2 ) denotes the string-pair generated by A. The operator [] performs the ""usual"" pairwise concatenation so that [AB] yields the string-pair (C 1 ,C 2 ) where C 1 = A 1 B 1 and C 2 = A 2 B 2 .",25,26
9468,14948394,"Either or both x and y may take the special value ∈ denoting an empty string, allowing a symbol of either language to have no counterpart in the other language by being matched to an empty string.",19,20
9469,4985773,"Define a binary 1 context-free grammar (CFG) as a 4-tuple (N , G, T , r) where N is a set of nonterminal symbols (e.g. NP, VP), T is a set of terminal symbols, consisting of the words in the language, G is a set of binary rules of the form A → β 1 β 2 , and r ∈ N is a distinguished root nonterminal symbol.",82,83
9470,4985773,"n}, where 0 is a special symbol indicating the pseudo-root of the sentence.",8,9
9471,241583519,9 https://catalog.ldc.upenn.edu/docs/ LDC2013T19/OntoNotes-Release-5.0.pdf • PERCENT: Percentage expressions with % symbol or the word 'percent'. •,14,15
9472,241583519,PNT: Percentage expressions with % symbol or the word 'percent'. •,6,7
9473,141413861,Is it going to be achieved by a symbol-processing or by a connectionist approach?,8,9
9474,7227918,"v n , it is the case that: (1) v 1 = 2 and v n = 3; (2) the leaves 2 and 3 cannot appear at any other position in the strings s(y) for y ∈ Y; (3) l(2) = <s> where <s> is the start symbol in the language model; (4) l(3) = </s> where </s> is the end symbol.",63,64
9475,7227918,"v n , it is the case that: (1) v 1 = 2 and v n = 3; (2) the leaves 2 and 3 cannot appear at any other position in the strings s(y) for y ∈ Y; (3) l(2) = <s> where <s> is the start symbol in the language model; (4) l(3) = </s> where </s> is the end symbol.",85,86
9476,11642874,"To handle the case where a is not an entity, we add a special symbol ∅ to every E(a).",15,16
9477,6359641,"Multilingual Neural Machine Translation Existing machine translation systems, mostly based on a phrase-based system or its variants, work by directly mapping a symbol or a subsequence of symbols in a source language to its corresponding symbol or subsequence in a target language.",26,27
9478,6359641,"Multilingual Neural Machine Translation Existing machine translation systems, mostly based on a phrase-based system or its variants, work by directly mapping a symbol or a subsequence of symbols in a source language to its corresponding symbol or subsequence in a target language.",39,40
9479,6359641,"Each symbol in both source and target sentences, x t or y t , is an integer index of the symbol in a vocabulary.",1,2
9480,6359641,"Each symbol in both source and target sentences, x t or y t , is an integer index of the symbol in a vocabulary.",21,22
9481,6359641,"1) The decoder, which is implemented as an RNN as well, generates one symbol at a time, the translation of the source sentence, based on the context set returned by the encoder.",16,17
9482,6359641,"At each time step t in the decoder, a time-dependent context vector c t is computed based on the previous hidden state of the decoder z t−1 , the previously decoded symbol ỹt−1 and the whole context set C. This starts by computing the relevance score of each context vector as e t,i = f score (h i , z t−1 , E y [ỹ t−1 ]), (2) for all i = 1, . . . ,",34,35
9483,6359641,This relevance score measures how relevant the i-th context vector of the source sentence is in deciding the next symbol in the translation.,21,22
9484,6359641,"4) With this time-dependent context vector c t , the previous hidden state z t−1 and the previously decoded symbol ỹt−1 , the decoder's hidden state is updated by z t = Ψ dec (z t−1 , E y [ỹ t−1 ] , c t ) , (5) where Ψ dec is a recurrent activation function.",22,23
9485,6359641,"The probability distribution for the next target symbol is computed by p(y t = k|ỹ <t , X) ∝ e g k (zt,ct,E[ỹ t−1 ]) , (7) where g k is a parametric function that returns the unnormalized probability for the next target symbol being k. Training this attention-based model is done by maximizing the conditional log-likelihood L(θ) = 1 N N n=1 Ty t=1 log p(y t = y (n) t |y (n) <t , X (n) ), (8) where the log probability inside the inner summation is from Eq. (",7,8
9486,6359641,"The probability distribution for the next target symbol is computed by p(y t = k|ỹ <t , X) ∝ e g k (zt,ct,E[ỹ t−1 ]) , (7) where g k is a parametric function that returns the unnormalized probability for the next target symbol being k. Training this attention-based model is done by maximizing the conditional log-likelihood L(θ) = 1 N N n=1 Ty t=1 log p(y t = y (n) t |y (n) <t , X (n) ), (8) where the log probability inside the inner summation is from Eq. (",53,54
9487,6359641,Each decoder exposes a parametric function ϕ m att that transforms its hidden state and the previously decoded symbol to be compatible with a shared attention mechanism.,18,19
9488,6359641,"This transformer is a parametric function that takes as input the previous hidden state z m t−1 and the previous symbol ỹm t−1 and returns a vector for the attention mechanism: zm t−1 = ϕ m att z m t−1 , E m y ỹm t−1 (12) which replaces z t−1 in Eq.",20,21
9489,6359641,"Given the previous hidden state z m t−1 , previously decoded symbol ỹm t−1 and the time-dependent context vector c m t , which we will discuss shortly, the decoder updates its hidden state: z t = Ψ dec z m t−1 , E m y ỹm t−1 , f m adp (c m t ) , where f m adp affine-transforms the time-dependent context vector to be of the same dimensionality as the decoder.",11,12
9490,6359641,"Once the hidden state is updated, the probability distribution over the next symbol is computed exactly as for the pair-specific model (see Eq. (",13,14
9491,6359641,"The relevance score of each context vector h n t is computed based on the decoder's previous hidden state z m t−1 and previous symbol ỹm t−1 : e m,n t,i =f score hn t , zm t−1 , ỹm t−1 These scores are normalized according to Eq. (",25,26
9492,6359641,"Model Architecture Each symbol, either source or target, is projected on a 620-dimensional space.",3,4
9493,18352846,"label NT (v) = label A (u, v) • In case |span(v)| > 1 add a new node u as a daughter designating the lexical head, labeled with the wildcard symbol *: V := V ∪ {u} A := A ∪ {(v, u)} label NT (u) = * • For each node v such that |span(v)| = 1, add a new node u as a daughter, labeled with its own terminal: V := V ∪ {u} A := A ∪ {(v, u)} if (label NT (v) = * ) label T (u) := label V (v) else label T (u) := label V (parent(v)) That is to say, we label all nodes with spans greater than 1 with the grammatical function of their head, and for each node we add a new daughter u designating the head word, labeled with its grammatical function.",36,37
9494,248780379,"The Universal POS tagset contains the following core part-of-speech categories that can be used for any UD language: adjective, adposition, adverb, auxiliary, coordinating conjunction, determiner, interjection, noun, numeral, particle, pronoun, proper noun, punctuation, subordinating conjunction, symbol, verb and other.",54,55
9495,237592862,"A.3 Heatmap of Word Importance To study how each word in the sentence impacts the prediction of the model, we define word importance as follows: • For an original word, its importance is calculated as the difference between the log likelihood of a gold label before and after the original word is replaced with a special ""unknown"" symbol (<unk>). •",62,63
9496,247450850,"Notation: X is the speech input; L max is the maximum length of the hypotheses to be searched, we set it to T ; C is the decoded symbol sequence; [b] denotes [blank] .",31,32
9497,247450850,The CTC prefix probability is defined as the cumulative probability of all label sequences that have h as their prefix: where v denotes all possible symbol sequences except the empty.,26,27
9498,18057552,"Example : Semantic structure associated to the advice ""Do not expose to rain"" in a user's manual : Among the ""generation models"" of the system, the following is Functionnaly Unifiable to the above structure : ] ] Remark : the symbol * means that the rule may be repeated.",46,47
9499,51873882,It also assumes that every nonterminal category X 1 in G 1 has a corresponding non-terminal category X 2 in G 2 and that every ter-minal symbol (or word) w 1 in G 1 has a corresponding terminal symbol w 2 in G 2 .,30,31
9500,51873882,It also assumes that every nonterminal category X 1 in G 1 has a corresponding non-terminal category X 2 in G 2 and that every ter-minal symbol (or word) w 1 in G 1 has a corresponding terminal symbol w 2 in G 2 .,44,45
9501,51873882,"Finally, it assumes that every production rule in L 1 has a corresponding rule in L 2 -i.e, the non-terminal categories on the left-hand side of the two rules correspond to each other, and every category/symbol on the right-hand side of one rule corresponds to a category/symbol on the right-hand side of the other rule.",44,45
9502,51873882,"Finally, it assumes that every production rule in L 1 has a corresponding rule in L 2 -i.e, the non-terminal categories on the left-hand side of the two rules correspond to each other, and every category/symbol on the right-hand side of one rule corresponds to a category/symbol on the right-hand side of the other rule.",59,60
9503,10014401,"We assume the sentences in L req have been generated by a context-free grammar G = N , Σ, S ∈ N , R , where N is a set of nonterminals, Σ is the aforementioned lexicon, S ∈ N is the start symbol and R is a set of context-free rules {A → α|A ∈ N , α ∈ (N ∪ Σ) * }.",48,49
9504,1145506,"The i th symbol of a permutation π will be denoted as π(i), and the inverse of the permutation π −1 is defined so that if π(i) = j then π −1 (j) = i. The identity, or monotone, permutation id is the permutation for which id(i) = i for all i. Table 1 shows the permutations associated with the example alignments in Figure 1 .",3,4
9505,7373647,"2 can be formally described using: • Set of states, S = s ∪ L ∪ X L ∪ e • Set of observations, O • Emission matrix (|S| × |O|) • Transition matrix (|S| × |S|) O consists of all seen events in the data, and a special symbol unk for all unseen events.",57,58
9506,7373647,"If the unigram, i.e., the token, is also unknown, then the observation of the symbol unk is used instead.",18,19
9507,11074199,"Outof-vocobulary tokens are represented by an UNK symbol in the word embedding layer, but treated normally by the character embedding layer.",9,10
9508,248779947,"Figure 2a shows that the male bartender ( ) profession word started to appear in the Arabic poems as a profession of serving alcohol generally, wine exclusively, as a symbol of love, passion, and adoration for women from the age of between Umayyad and Abbasid until the Modern age.",31,32
9509,233365022,"SALAD contains tweets annotated as Positive, Negative, Neutral, Conflict and ConflictTxtvsEm (tweets where the comments have a conflict in the text and the emoji symbol) .",28,29
9510,128010999,"The transition system maintains a stack of partiallyconstructed trees, where each element of the stack is one of the following: (a) a terminal symbol, i.e. a word; (b) a complete tree; or (c) a tree with a single empty slot, denoted by the special element ∅. An empty slot must be the rightmost leaf node in its tree, but may occur at any depth.",27,28
9511,6563702,"head, a type symbol, and 2.",4,5
9512,218596282,"We require that the number of open scope blocks equals the indentation level i l for each line l. Symbol Table Constraints Each scope block is associated with a symbol table (Aho et al.,",29,30
9513,218596282,"After checking these constraints, any variables declared by a given code piece will be added to the symbol table associated with the current scope.",18,19
9514,218596282,"These symbol table constraints are based on the semantic information of code pieces and are fundamentally different from previous AST-based syntactic constraints for code generation (Rabinovich et al.,",1,2
9515,218596282,"First, we can efficiently compute whether a program prefix can possibly lead to a full program that satisfies the constraints by using an incremental parser (Ghezzi and Mandrioli, 1979) and checking the symbol tables.",36,37
9516,218596282,We rely on the following heuristic assumptions to parse the code pieces generated by the model: (1) a code piece belongs to only one variable scope; (2) the generation of every primary expression terminal symbol lies in one line.,40,41
9517,218596282,Symbol Table Constraints : both the syntactic constraints and the symbol table constraints described in section 3.2.,10,11
9518,218596282,Rejection by Constraints In this section we give representative examples on what program candidates are rejected by our syntactic and symbol table constraints.,20,21
9519,218596282,"This means the symbol on the top of the stack, the state, or the transition rule need to have full information of about whether each variable has been declared, which contains exponentially many possibilities w.r.t.",3,4
9520,218596282,Lemma 2 Let S be the start symbol of the CFG.,7,8
9521,218596282,"Then for all w ∈ L, there exists a symbol A with In other words, for any member of the language, we can find a symbol in the derivation responsible for between 1/3 and 2/3 of the final yield.",10,11
9522,218596282,"Then for all w ∈ L, there exists a symbol A with In other words, for any member of the language, we can find a symbol in the derivation responsible for between 1/3 and 2/3 of the final yield.",28,29
9523,218596282,"Let P K be all sequences of permutations of the K variables and thus P K ⊂ L. Then by Lemma 2, for every permutation π ∈ P K we can find yield y π that is yielded by a single symbol such that 1 3 K ≤ |y π | ≤ 2 3 K. Now we consider two permutations π 1 and π 2 .",42,43
9524,218596282,"If y π 1 and y π 2 are yielded by the same symbol, then they must have the same length (this is the part where the proof is slightly different from Ellul et al. (",13,14
9525,218596282,X is a generic symbol.,4,5
9526,219300983,Eventually the parser will reach a point at which it becomes cer tain of the category of the original symbol.,19,20
9527,219300983,The transformation system also has the advantage over LR(k) style parsers in that no lookahead is required to parse such grammars; the parser only needs to examine the current input symbol in order for a decision to be made.,32,33
9528,219300983,The parser checks the first input symbol against all of the possibilities below the top or node in the transformed grammar.,6,7
9529,219300983,4 In addition looka head gates can be added to and nodes under an or node to prevent the parser needlessly descending a chain of nodes to match the left-most symbol.,32,33
9530,7888734,"Precisely speaking, different numbers should be considered to denote different meaning, but to avoid data sparseness problem, we abstract numbers into a special symbol num .",26,27
9531,15785388,Given a NDFSM SKand a sequence of symbols .SS • all the paths are followed; • the longest matching sequence 3t1 ..i] is considered the result of the application; • the actions corresponding to all the longest paths are executed ; • a single output symbol (i .e .,49,50
9532,1356465,"The Berkeley parser induces a latent, statesplit PCFG in which each symbol V of the (observed) X-bar grammar is refined into a set of more specific symbols {V 1 , V 2 , . . .}",12,13
9533,42016700,"It was found that a package for finding and correctly linking the majority of person descriptors could be written in about a week by incorporating the information that Brill provides with that provided by the NLToolset, i.e. symbol name, semantic category, and possible parts of speech as found in the NLToolset's lexicon.",38,39
9534,8313435,"For clarity of presentation, we restrict the grammar to be in Chomsky normal form (CNF), where all rules in the grammar are of the form non-terminal symbols, and a is some terminal symbol.",39,40
9535,694926,"Figure 3: LR table and Connection Constraints The N-gram model is the most commonly used probabilistic language model, and it assumes that a symbol sequence can be described by a higher order Markov process.",27,28
9536,694926,"A in state m with lookahead symbol a that has not been assigned a probability, where n is the number of conflict actions in state m with lookahead symbol a. Step 8 Return the LR table T produced at the completion of Step 7 as the Bi#ram LR table.",6,7
9537,694926,"A in state m with lookahead symbol a that has not been assigned a probability, where n is the number of conflict actions in state m with lookahead symbol a. Step 8 Return the LR table T produced at the completion of Step 7 as the Bi#ram LR table.",29,30
9538,694926,"From the CFG given in Figure 4 , we can generate an LR In state 1 with a lookahead symbol bl, re6 is carried out after executing action shl in state 0, pushing al onto the stack.",19,20
9539,694926,"On the other hand, in the case of re6 in state 1 with lookahead symbol b$, as al can be followed by b2 (PConnect(al, b~) ~ 0), action re6 cannot be removed.",15,16
9540,694926,"In state 9, re6 with lookahead symbol $ has already been removed in Step 2, and there is no succeeding action for shg.",7,8
9541,694926,"As a result, re3 in state 8 with lookahead symbol al is removed in Step 3.",10,11
9542,694926,"Similarly, re9 in state 7 with lookahead symbol al is also removed in Step 3.",8,9
9543,694926,Consider the beginning symbol a2 of a sentence.,3,4
9544,13362841,We define a new representation in which each non-terminal symbol is associated with a specific word (the head).,11,12
9545,13362841,"If a symbol occurs more than once in a spine, we use indices to distinguish instances.",2,3
9546,12601888,"If an organization changed-or plans to change-its name, the old or future name is linked to th e current name, and the system symbol for the current name is used by all references to eithe r name .",29,30
9547,12601888,Assigning the same symbol name to each instance of a template element greatly simplifies th e work of the subsequent ST system .,3,4
9548,9168640,"Our goal is to create a prediction procedure which, given an attachment problem, x, will return a derivation, d, which is a parse for x using the following mini CFG grammar whose initial symbol is ROOT: ( 5) a. ROOT → VP b. ROOT → NP c. VP → verb H NP PP * d. NP → baseNP H PP * e. PP → prep H NP The head of each XP is indicated with a subscripted H. All siblings to a head are called its modifiers.",38,39
9549,9168640,"All numeral strings of two or four digits were replaced with a symbol representing year, and all other numeral strings were replaced with a symbol representing numeric value.",12,13
9550,9168640,"All numeral strings of two or four digits were replaced with a symbol representing year, and all other numeral strings were replaced with a symbol representing numeric value.",25,26
9551,9168640,"Following the reasoning of Collins (1999, p. 46) , we imagine that each head's final modifier is a special STOP symbol.",24,25
9552,9168640,"Then, the probability of a problem-derivation pair, conditioned on 1 d∈AD , is estimated as p G Ind (x, d | 1 d∈AD ; θ G Ind ) = h∈heads(d) p∈deps(head) p G Ind (prep | head; θ G Ind ) Here heads(d) is the set of head of N P and V P phrases in d, and deps(head) is the list of modifiers of head in d, including the STOP symbol.",85,86
9553,9168640,"But, rather than put the word itself, each direct object is represented by a special symbol, DO.",17,18
9554,199661737,"At that time, the goto process uses both an incomming nonterminal symbol and a state number on the top of the stack.",12,13
9555,2717798,"If we say that the notation of V ( R) is to marginalize out all other variables in β except η (R) , and P (y (R) d ) is the prior for the region prediction task, we can predict the expected region value ŷ(R) d of a document d: ŷ(R) d ∝ arg max ŷ(R) d exp V ( R) log β + log P (y (R) d ) = arg max ŷ(R) d exp V ( R) m + η (T ) z (d) n + λ (R) η (R) y (R) d + λ (Q) η (Q) y (Q) d + η (I) y (R) d ,y (Q) d ,z (d) n P (y (R) d ) If the symbol δ is the hyperprior for the learning rate and ẏ(R) d is the true label, the update procedure for the weights becomes: λ (R ) d = λ (R) d + δ( ẏ(R) d − ŷ(R) d ) Similarly, we derive the λ (Q) parameter using the above formula.",167,168
9556,18285941,The symbol 'sh N' in some entries indicates that the generalized LR parser has to push a look-ahead preterminal on the LR stack and go to 'state N'.,1,2
9557,18285941,The symbol 're N' means that the generalized LR parser has to reduce from the top of the stack the number of elements equivalent to that of the right-hand side of the rule numbered 'N'.,1,2
9558,18285941,The symbol 'ace' means that the generalized LR parser has success-Nlly completed parsing.,1,2
9559,18285941,3 ) functions as a cut symbol of Prolog.,6,7
9560,222141054,"For text classification, we use encoded representation of the special beginning-of-document symbol [CLS]as our key.",16,17
9561,237213465,"For the hybrid method, statistically significant improvements over tuned BM25 are denoted with the symbol † based on paired t-tests (p < 0.01).",15,16
9562,5925661,There we should be able to distinguish between different occurrences of ~he same symbol in a tree.,13,14
9563,5925661,"Now a production p E ~ is a triple ([al],SO,~) with a q ~, e e (4 U [EIND])*, and sc is a structural condition which contains the symbol a I .",38,39
9564,1163931,"Section B lists the notation specific to sonic rhymes, including the symbol indicating a syllable break -, as well as support for phonetic rhymes.",12,13
9565,12617502,"Then, an atom is defined as P(t~, t2,..., t~) where P is a n-place predicate symbol and t~ (i = l,...,n) is a term.",23,24
9566,12617502,"In the NTA] definition, V denotes a predicate symbol.",10,11
9567,12617502,"These facts are represented by formulas in the standard form Q (~Fv G ) (I0) Where Q represents quantification of variables, taking the form (q, V~/d, )(qzvz/d~ )... (qKv,/dK), in which q~ denotes a quantifier symbol V or 3, vz is a variable name and dz indicates the domain of the variable VZ.",50,51
9568,12617502,"TIC(Test for Implicative Condition) After the candidate literal to be resolved upon is decided, axioms which contain the literals with the same predicate symbol as the candidate literal's but with the opposite sign are retrieved in the knowledge base.",25,26
9569,12617502,"The syntax of the system language is based on the tuple (V0, Vi,... , V n) where V 0 is either a system command name, a predicate symbol or a variable whose domain is a PREDICATE; Vi(i ¥0 ) is a term as which a string and a numerical constant and a formula are permitted.",33,34
9570,12617502,"For example, [AX/MAN][EY*/WOMAN] where A and E denote the universal and existential quantifier respectively, and the symbol * attached to Y denotes that Y is a query variable.",22,23
9571,12617502,A predicate symbol itself can be a variable.,2,3
9572,12617502,where the symbol $ attached to X denotes that X is a variable and the symbol ?,2,3
9573,12617502,where the symbol $ attached to X denotes that X is a variable and the symbol ?,15,16
9574,12617502,"Then, grouping of words in the sentence is made according to the pattern ""PERSON1 meet PERSON2"", resulting in ; an empty group where the words marked with the star symbol * denote the instances of PERSON1 and PERSON2.",33,34
9575,235359181,"2017) , we apply lowercasing, tokenization and replacing digits with digit symbol to preprocess all the datasets.",13,14
9576,16056702,For the symbol NG there is a TR-rule too.,2,3
9577,16056702,"In the TR-rules, however, attempt is made to describe and deal with all possible occurrences of a symbol.",21,22
9578,207914581,"2017) proposed Neural Phrase MT (NPMT) that is built upon Sleep-WAke Network (SWAN), a segmentation-based sequence modeling technique, which automatically discovers phrases given the data and appends the special symbol $ to the source and target data.",40,41
9579,235390653,The syllable weight of a syllable can either be laghu (light)(represented by the symbol ।),14,15
9580,235390653,or guru (heavy)(represented by the symbol ऽ).,6,7
9581,3045083,"As a usual dependency structure, each node is not labelled by a category symbol like NP, VP, PP etc.,",14,15
9582,3045083,"These case labels have their own entries in the bi-lingual dictionary, in which rules for selecting appropriate prepositions are described, The main difference is that; each node in the tree is characterized not only by a category symbol like S, NP, VP, etc.,",42,43
9583,220962184,"Conventionally, hashtags are prefixed with the # symbol followed by phrases concatenated without whitespaces.",8,9
9584,220962184,We preserved the # symbol in the beginning of segmented words to help the model differentiate hashtags from plain texts.,4,5
9585,5208036,Two more arguments are added to each predicate which denotes nontenninal symbol in figure 4 .,11,12
9586,1455285,This tree allows the corresponding clause number terminal symbol to be recognized by an appropriate variable instance.,8,9
9587,1455285,"We use a set of nonterminal symbols {S, A}, with S being the start symbol.",18,19
9588,1455285,The material between them is replaced with a single new node with a fresh nonterminal symbol and a fresh link.,15,16
9589,1455285,We can therefore use an atomic symbol to name each signature (perfect hashing).,6,7
9590,3068838,"Ic~'~) , is concatination symbol.",5,6
9591,16624496,"If its morphological tag contains the symbol for negative verb, a new node is created with the lexical (trlemma) value 'Neg' and functor 'RHEM' (rhematizer, i.e. focus sensitive particle).",6,7
9592,15735989,"In this paper, input sentences are given in colloquial style in which a spacing symbol is placed between two successive B-phrases.",15,16
9593,15735989,"ai,J,aj,~,P) I (ai•J,aj, ,P) where, N~i > j~l, aie DI(i), ajE DI(j), ce ~, P~ and $ is a specially introduced symbol.",43,44
9594,15735989,"Form of items in PDSL (i,j,a~,a~,P#) where, Nhi ~j ~i a# ~ DI(i) ~ {#}, ~ % DI(j) U {~, P~ i~ a subset of C or #Oand# is specially introduced symbol.",49,50
9595,51786318,"Here, the encoder maps an input sequence of symbol representations (x 1 , ..., x n ) to a sequence of continuous representations z = (z 1 , ..., z n ).",9,10
9596,6273860,The symbol -*means non-exsistent filler.,1,2
9597,2868925,The morphological component relates these leaves to actual symbol strings.,8,9
9598,27120,"The entire one million words of the Corpus have been ""tagged"", with each word given a specific grammatical symbol.",21,22
9599,27120,"The ""tagging"" procedure, which was semiautomatic, assigned to each running word an unambiguous symbol based on a taxonomy of 82 grammatical categories.",17,18
9600,27120,"Headlines and headings, which are identified by a special symbol in the tagged Corpus, were not included because of the particular nature of English ""headline grammar"", which often omits verbs entirely, e.g. Actor in Critical Condition after Explosion, or omits some verb form, particularly the finite one, e.g. President to Meet Brezhnev in Vienna.",10,11
9601,27120,"The symbol X, in this case, thus designates any tag outside of those that may appear in a verbal group.",1,2
9602,27120,"The results are given in Table 3 , where the symbol F and NF stand for finite and non-finite predications respectively.",10,11
9603,47759232,"The generative procedure for TRs is based on the following points: (i) To generate a node n means (a) to create the node n either as the root of a TR, or as a node that is dependent on another one and is placed to the right of all its sister nodes, and also (b) to choose n's lexical value and the values of its grammatemes, taking into account the subcategorization conditions of the mother node of n and the restrictions on the combinations of grammatemes (specified in the lexical entry of the head or in the data concerning the respective word class); the technique used to realize these conditions and restrictions is unification; e.g. if n is the Objective of a verb that subcategorizes its Objective as a verb, then the lexical unit in the label of n has to be accompanied by the symbol identifying its word class as verb; (c) if n is a root, the lexical part of its label is a verb, and its grammatemes determine it as a finite verb form of the main clause; n is then specified either as CB (i.e., as belonging to the topic) or, in the non-marked case, as NB (i.e., as belonging to the focus); (ii) if the symbol of a complementation (argument or adjunct) is present in the frame of the node n, then it is possible to generate either a left or a right daughter of n; (ii)(a) in case a left daughter is being generated, it is assigned a CB marker and a complementation value chosen from the frame of n; (ii)(b) if a right daughter is being generated, it is assigned a NB marker and a complementation from the left end of the frame of n. (iii) If the chosen complementation is an argument, it is deleted in the frame of n (as having been saturated).",161,162
9604,47759232,"The generative procedure for TRs is based on the following points: (i) To generate a node n means (a) to create the node n either as the root of a TR, or as a node that is dependent on another one and is placed to the right of all its sister nodes, and also (b) to choose n's lexical value and the values of its grammatemes, taking into account the subcategorization conditions of the mother node of n and the restrictions on the combinations of grammatemes (specified in the lexical entry of the head or in the data concerning the respective word class); the technique used to realize these conditions and restrictions is unification; e.g. if n is the Objective of a verb that subcategorizes its Objective as a verb, then the lexical unit in the label of n has to be accompanied by the symbol identifying its word class as verb; (c) if n is a root, the lexical part of its label is a verb, and its grammatemes determine it as a finite verb form of the main clause; n is then specified either as CB (i.e., as belonging to the topic) or, in the non-marked case, as NB (i.e., as belonging to the focus); (ii) if the symbol of a complementation (argument or adjunct) is present in the frame of the node n, then it is possible to generate either a left or a right daughter of n; (ii)(a) in case a left daughter is being generated, it is assigned a CB marker and a complementation value chosen from the frame of n; (ii)(b) if a right daughter is being generated, it is assigned a NB marker and a complementation from the left end of the frame of n. (iii) If the chosen complementation is an argument, it is deleted in the frame of n (as having been saturated).",245,246
9605,219306932,"A Vector-TAG (V-TAG) is a 5-tuple (N, T, S, V ) where N and T are disjoint sets of nonterminal and terminal symbols, respectively; S ∈ N is the start symbol; and V is a set of sets of trees, called vectors.",44,45
9606,2410236,"Here, the symbol o(@)s is a dummy element standing for the noun phrase, carrying the same subscripts, at the head of NP rl.",3,4
9607,2410236,"Next, the symbol SI.i, I~ representing a possibly em-"" pro,j ' bedded sentence, can be developed by the rules: • s3.l,k (20)a sl'l'kpro,J ÷ N:j de V32 pro,j' b SI'I,.",3,4
9608,6932851,"The LE for an adjective prepositional phrase is constructed using a special symbol *ape E<<<<0,1>,<O,i>>,i>,<<0,0>,i>>.",12,13
9609,6932851,Interrogative is denoted with a symbol #QUES. #,5,6
9610,6932851,"For YES-NO questions, a symbol (whether) is used which maps a sentence into a noun clause.",7,8
9611,6932851,"~#QUES(who(#ante(run))) E E0 where, the symbol (who) maps a sentence into a noun clause.",9,10
9612,6932851,-Imperative is denoted with a symbol #IMP. #,5,6
9613,6932851,A distinguished symbol 'which' announces the occurence of a relative clause and sends the denotation of the antecedent as a message.,2,3
9614,6932851,"A noun clause is interpreted as follows: I fun(sentence)~DEF[?X; fun*(?X,ml)], where T(~l,sentence*), where 'fun' stands for a symbol such as 'that', 'whether' ... etc.",31,32
9615,218606769,The material between them is replaced with a single new node with a fresh nonterminal symbol and a fresh link number.,15,16
9616,12781912,"A small letter, such as n, v, and v L, stands for a linguistic expression and a capital letter, such as N, V, and V L, stands for the predicate symbol corresponding to the linguistic expression represented by its small letter.",38,39
9617,1918428,"We can write this as: log p(y|x; θ) ≈ N −1 i=0 log p(y i+1 |x, y c ; θ), where we make a Markov assumption on the length of the context as size C and assume for i < 1, y i is a special start symbol S .",54,55
9618,226226530,"1) a. Mary runs → RUN(MARY) b. John sees Bob → SEE (JOHN, BOB) Even for simple sentences like those in (1), which represent the smallest representations of object reflexives in English, the network must learn lexical semantic correspondences (e.g., the input symbol Mary is mapped to the output MARY and runs is mapped to RUN) and a mode of composition (e.g., for an intransitive sentence, the meaning of the subject is surrounded by parentheses and appended to the meaning of the verb).",53,54
9619,226226530,"However, unlike Marcus' task, this subject and its corresponding semantic symbol did occur in other (non-reflexive) contexts in the training data, and therefore was in the realm of possible inputs and outputs for the network.",13,14
9620,23172329,"If interjectional sentences, vocative sentences and pseudosentences consisting onl~ in a noun phrase ere not discussed, then it can be stated that the root of every tree of the mentioned kind is labelled by a symbol the lexical part of which belongs to the word class of verbs.",37,38
9621,10175429,"That is by pointing or otherwise focussing on a real or abstract object in U and indicating the degree -on the scale from 0 to I -to which it is compatible with the symbol in question"".",33,34
9622,221949448,"NULL is the symbol denoting to delete the slot (d i , s j ) from B t−1 .",3,4
9623,221949448,"8 ) This function simply update the B t−1 when new slot-value pairs appear in Lev t , and it delete the corresponding slot-value when the NULL symbol is generated.",31,32
9624,1573022,"2 The structure of adjective concept 2.1 Structural Patterns Adjective concepts can be regarded as the concept of a relation among constituents which produce difference and can be represented as follows: Ad (C, e, o, of, Or, o m, os, i, l, t, r, ...) (1) where each symbol in parentheses stands for a constituent.",64,65
9625,18076328,This yields the following kinds of analyses: The symbol VSUP in Fig.,9,10
9626,1320293,"~le operations necessary to obtain this component of the CSR ~len analyzing the initial Japanese text will evidently comprise isolating separate word forms and determining their internal structure (in terms of lexemes and morphologic markers), resolving ~nbi~ities for all units established; eliminating synonymy where it is manifested as supplementary distribution or free variation of morphologic units; detecting phraseological word combinations ~d reducing them to a one-word symbol; giving special labels to those word forms or parts of word forms which play ~ auxiliary role in the text analyzed and require no special translation equivalents; filling in the units omitted in the source text if their absence obscures its structure and hinders the translation process (due to the differences between the rules of linguistic ellipsis in the two lmlguages), etc.",75,76
9627,226262196,We reset the state distribution to p(z 1 | z 0 ) after encountering the EOS symbol.,16,17
9628,6591541,"However, rather than devising linguistically motivated features or splits, latent variable parsing takes a fully automated approach, in which each symbol is split into unconstrained subcategories.",23,24
9629,219305994,"For premodifiers, Mk is the symbol nil, since no lexical item links the premodifier to the head.",6,7
9630,219305994,"For appositives, Mk is the symbol appos.",6,7
9631,15073679,A Wild Card symbol to match with any word.,3,4
9632,1265586,"Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symbol splitting (Johnson, 1998; Klein and Manning, 2003) to full lexicalization and intricate smoothing (Collins, 1999; Charniak, 2000) .",24,25
9633,1265586,A manual approach might take the symbol NP and subdivide it into one subsymbol NPˆS for subjects and another subsymbol NPˆVP for objects.,6,7
9634,1265586,"However, rather than devising linguistically motivated features or splits, we take a fully automated approach, in which each symbol is split into unconstrained subsymbols.",21,22
9635,1265586,"In each iteration, we initialize EM with the results of the previous round's grammar, splitting every previous symbol in two and adding a small amount of randomness (1%) to break the symmetry between the various subsymbols.",20,21
9636,1265586,"Our refined grammars G are over symbols of the form X-k where X is an evaluation symbol (such as NP) and k is some indicator of a subsymbol, which may encode something linguistic like a parent annotation context, but which is formally just an integer.",18,19
9637,1265586,"Two pass parsing In the previous experiments, we conflated the phrasal categories and grammatical functions into single initial grammar symbol.",20,21
9638,7019269,"The ""+"" symbol states that the corresponding form or semantic distribution is allowed. ""-""",4,5
9639,7019269,"In our system, the condition slot of the symbol ToVinf0 states that the construction [ Verb in the complete infinitive form + complements ] is allowed if the subject of the current SC equals the subject of the main clause i.e. of the parent SC.",9,10
9640,7019269,"Most of the time, the sub-SCs are generated using LD and lexicon-grammar as stated by the symbol linguistic_synthesis.",21,22
9641,1123594,"First, consider the problem of calculating the expected counts of a symbol X in a tree distribution given by a grammar G, ignoring the issue of projection.",12,13
9642,1123594,"These expected counts obey the following onestep equations (assuming a unique root symbol): c(root) = 1 c(X) = Y →αXβ P (αXβ|Y )c(Y ) Here, α, β, or both can be empty, and a rule X → γ appears in the sum once for each X it contains.",13,14
9643,1123594,"1 In our experiments, we solve this system iteratively, with the following recurrences: c 0 (X) ← 1 if X = root 0 otherwise c i+1 (X) ← Y →αXβ P (αXβ|Y )c i (Y ) Note that, as in other iterative fixpoint methods, such as policy evaluation for Markov decision processes (Sutton and Barto, 1998) , the quantities c k (X) have a useful interpretation as the expected counts ignoring nodes deeper than depth k (i.e. the roots are all the root symbol, so c 0 (root) = 1).",101,102
9644,1123594,"G −1 G 0 =X-bar G 1 G 2 G 3 G 4 G 5 (G 6 =G) Output Objective Functions for Parsing A split PCFG is a grammar G over symbols of the form X-k where X is an evaluation symbol (such as NP) and k is some indicator of a subcategory, such as a parent annotation.",49,50
9645,216804363,Internal nodes in the elementary trees are labeled with a nonterminal symbol.,11,12
9646,216804363,The substitution operation occurs when an elementary tree rooted in a nonterminal symbol A replaces a substitution node with the same nonterminal symbol.,12,13
9647,216804363,The substitution operation occurs when an elementary tree rooted in a nonterminal symbol A replaces a substitution node with the same nonterminal symbol.,22,23
9648,1346130,"Interpretation strategy of segmented word component Data structure In comparison with other symbol system, every hmlan language has a remarkable characteristics; namely, the structure of segmented words.",12,13
9649,219300685,"F i r s t , t h e symbol system used must be more d i r e c t , i .",9,10
9650,14597127,"Lexical methods split each pre-terminal symbol into many subsymbols, one for each word, and then focus on smoothing sparse lexical statis-tics (Collins, 1999; Charniak, 2000) .",7,8
9651,14597127,"Unlexicalized methods refine the grammar in a more conservative fashion, splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols (Klein and Manning, 2003; Matsuzaki et al.,",20,21
9652,14597127,We apply our HDP-PCFG-GR model to automatically learn the number of subsymbols for each symbol.,18,19
9653,14597127,We consider each grammar symbol as a mixture component whose parameters are the rule probabilities for that symbol.,4,5
9654,14597127,We consider each grammar symbol as a mixture component whose parameters are the rule probabilities for that symbol.,17,18
9655,14597127,"HDP-PCFG β ∼ GEM(α) [draw top-level symbol weights] For each grammar symbol z ∈ {1, 2, . . . }:",12,13
9656,14597127,"HDP-PCFG β ∼ GEM(α) [draw top-level symbol weights] For each grammar symbol z ∈ {1, 2, . . . }:",18,19
9657,14597127,"−φ T z ∼ Dirichlet(α T ) [draw rule type parameters] −φ E z ∼ Dirichlet(α E ) [draw emission parameters] −φ B z ∼ DP(α B , ββ T ) [draw binary production parameters] For each node i in the parse tree: −ti ∼ Multinomial(φ T z i ) [choose rule type] −If ti = EMISSION: −−xi ∼ Multinomial(φ E z i ) [emit terminal symbol] −If ti = BINARY-PRODUCTION: −−(z L(i) , z R(i) ) ∼ Multinomial(φ B z i ) [generate children symbols] β φ B z φ T z φ E z z ∞ z 1 z 2 x 2 z 3 x 3 T Parameters Trees Figure 2 : The definition and graphical model of the HDP-PCFG.",77,78
9658,14597127,"We use L(i) and R(i) to denote the left and right children of node i. In the HMM, the transition parameters of a state specify a distribution over single next states; similarly, the binary production parameters of a grammar symbol must specify a distribution over pairs of grammar symbols for its children.",44,45
9659,14597127,"Therefore, each grammar symbol must also have a distribution over the type of rule to apply.",4,5
9660,14597127,"To summarize, the parameters of each grammar symbol z consists of (1) a distribution over a finite number of rule types φ T z , (2) an emission distribution φ E z over terminal symbols, and (3) a binary production distribution φ B z over pairs of children grammar symbols.",8,9
9661,14597127,"An alternative definition of an HDP-PCFG would be as follows: for each symbol z, draw a distribution over left child symbols l z ∼ DP(β) and an independent distribution over right child symbols r z ∼ DP(β).",15,16
9662,14597127,This also yields a distribution over symbol pairs and hence defines a different type of nonparametric PCFG.,6,7
9663,14597127,"In this scenario, the set of symbols is known, but we do not know how many subsymbols to allocate per symbol.",22,23
9664,14597127,"The essential difference is that now we have a collection of HDP-PCFG models for each symbol s ∈ S, each one operating at the subsymbol level.",17,18
9665,14597127,"HDP-PCFG for grammar refinement (HDP-PCFG-GR) For each symbol s ∈ S: −β s ∼ GEM(α) [draw subsymbol weights] −For each subsymbol z ∈ {1, 2, . . . }:",15,16
9666,14597127,"−−φ T sz ∼ Dirichlet(α T ) [draw rule type parameters] −−φ E sz ∼ Dirichlet(α E (s)) [draw emission parameters] −−φ u sz ∼ Dirichlet(α u ) [unary symbol productions] −−φ b sz ∼ Dirichlet(α b ) [binary symbol productions] −−For each child symbol s ∈ S: −−−φ U szs ∼ DP(α U , β s ) [unary subsymbol prod.]",37,38
9667,14597127,"−−φ T sz ∼ Dirichlet(α T ) [draw rule type parameters] −−φ E sz ∼ Dirichlet(α E (s)) [draw emission parameters] −−φ u sz ∼ Dirichlet(α u ) [unary symbol productions] −−φ b sz ∼ Dirichlet(α b ) [binary symbol productions] −−For each child symbol s ∈ S: −−−φ U szs ∼ DP(α U , β s ) [unary subsymbol prod.]",49,50
9668,14597127,"−−φ T sz ∼ Dirichlet(α T ) [draw rule type parameters] −−φ E sz ∼ Dirichlet(α E (s)) [draw emission parameters] −−φ u sz ∼ Dirichlet(α u ) [unary symbol productions] −−φ b sz ∼ Dirichlet(α b ) [binary symbol productions] −−For each child symbol s ∈ S: −−−φ U szs ∼ DP(α U , β s ) [unary subsymbol prod.]",55,56
9669,14597127,"−−For each pair of children symbols (s , s ) ∈ S × S: −−−φ B szs s ∼ DP(α B , β s β T s ) [binary subsymbol] For each node i in the parse tree: −ti ∼ Multinomial(φ T s i z i ) [choose rule type] −If ti = EMISSION: −−xi ∼ Multinomial(φ E s i z i ) [emit terminal symbol] −If ti = UNARY-PRODUCTION: −−s L(i) ∼ Multinomial(φ u s i z i ) [generate child symbol] −−z L(i) ∼ Multinomial(φ U s i z i s L(i) ) [child subsymbol] −If ti = BINARY-PRODUCTION: −−(s L(i) , s R(i) ) ∼ Mult(φs i z i ) [children symbols] −−(z L(i) , z R(i) ) ∼ Mult(φ B s i z i s L(i) s R(i) ) [subsymbols] Variational inference We present an inference algorithm for the HDP-PCFG model described in Section 2.4, which can also be adapted to the HDP-PCFG-GR model with a bit more bookkeeping.",74,75
9670,14597127,"−−For each pair of children symbols (s , s ) ∈ S × S: −−−φ B szs s ∼ DP(α B , β s β T s ) [binary subsymbol] For each node i in the parse tree: −ti ∼ Multinomial(φ T s i z i ) [choose rule type] −If ti = EMISSION: −−xi ∼ Multinomial(φ E s i z i ) [emit terminal symbol] −If ti = UNARY-PRODUCTION: −−s L(i) ∼ Multinomial(φ u s i z i ) [generate child symbol] −−z L(i) ∼ Multinomial(φ U s i z i s L(i) ) [child subsymbol] −If ti = BINARY-PRODUCTION: −−(s L(i) , s R(i) ) ∼ Mult(φs i z i ) [children symbols] −−(z L(i) , z R(i) ) ∼ Mult(φ B s i z i s L(i) s R(i) ) [subsymbols] Variational inference We present an inference algorithm for the HDP-PCFG model described in Section 2.4, which can also be adapted to the HDP-PCFG-GR model with a bit more bookkeeping.",97,98
9671,14597127,"Structured mean-field approximation We denote parameters of the HDP-PCFG as θ = (β, φ), where β denotes the top-level symbol probabilities and φ denotes the rule probabilities.",29,30
9672,14597127,"Top-level symbol probabilities q(β): Recall that we restrict q(β) = δ β * (β), so optimizing β is equivalent to finding a single best β * .",3,4
9673,14597127,"Unlike q(φ) 3 Because we have truncated the top-level symbol weights, the DP prior on φ B z reduces to a finite Dirichlet distribution.",12,13
9674,14597127,"We binarize the trees in the treebank as follows: for each non-terminal node with symbol X, we in- troduce a right-branching cascade of new nodes with symbol X. The end result is that each node has at most two children.",17,18
9675,14597127,"We binarize the trees in the treebank as follows: for each non-terminal node with symbol X, we in- troduce a right-branching cascade of new nodes with symbol X. The end result is that each node has at most two children.",32,33
9676,14597127,"Our goal is to learn a refined grammar, where each symbol in the training set is split into K subsymbols.",11,12
9677,226262386,"For example, the Devanagari script does not have a grapheme for the glottal stop -as a solution, printed texts in the Yakkha language use the IPA symbol 'P' (Schackow, 2015) .",28,29
9678,226262386,"As seen in Figure 2 , the system does not recognize the Cyrillic script character χ in Griko and the IPA symbol P in Yakkha.",21,22
9679,1099968,"Retrieval Examples We will show here ETOC retrievals with the following characteristic features: long-distance dependency, idioms, the ellipsis symbol, aspect and finally a rather problematic one, semantic ambiguity.",23,24
9680,1112671,"For premodifiers, Mk is the symbol nil, since no lexical item links the premodifier to the head.",6,7
9681,1112671,"For appositives, Mk is the symbol appos.",6,7
9682,6684426,"Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symbol splitting (Johnson, 1998; Klein and Manning, 2003) to full lexicalization and intricate smoothing (Collins, 1999; Charniak, 2000) .",24,25
9683,6684426,"For example, the symbol NP might be split into the subsymbol NPˆS in subject position and the subsymbol NPˆVP in object position.",4,5
9684,6684426,2005) and also Prescher (2005) exhibited an automatic approach in which each symbol is split into a fixed number of subsymbols.,15,16
9685,6684426,"In each iteration we initialize EM with the results of the smaller grammar, splitting every previous annotation symbol in two and adding a small amount of randomness (1%) to break the symmetry.",18,19
9686,6684426,Hierarchical splitting leads to better parameter estimates over directly estimating a grammar with 2 k subsymbols per symbol.,17,18
9687,6684426,It should be noted that simple frequency statistics are not sufficient for determining how often to split each symbol.,18,19
9688,6684426,"Therefore, we go in the opposite direction; that is, we split every symbol in two, train, and then measure for each annotation the loss in likelihood incurred when removing it.",15,16
9689,6684426,"Therefore the inside score of A is: P IN (r, t, A) = p 1 P IN (r, t, A 1 ) + p 2 P IN (r, t, A 2 ) Since A can be produced as A 1 or A 2 by its parents, its outside score is: P OUT (r, t, A) = P OUT (r, t, A 1 ) + P OUT (r, t, A 2 ) Replacing these quantities in (2) gives us the likelihood P n (w, T ) where these two annotations and their corresponding rules have been merged, around only node n. We approximate the overall loss in data likelihood due to merging A 1 and A 2 everywhere in all sentences w i by the product of this loss for each local change: ∆ ANNOTATION (A 1 , A 2 ) = i n∈Ti P n (w i , T i ) P(w i , T i ) This expression is an approximation because it neglects interactions between instances of a symbol at multiple places in the same tree.",201,202
9690,6684426,The numbers of splits learned turned out to not be a direct function of symbol frequency; the numbers of symbols for both lexical and nonlexical tags after 4 SM cycles are given in Table 2 .,14,15
9691,6684426,"Smoothing the productions of each subsymbol by shrinking them towards their common base symbol gives us a more reliable estimate, allowing them to share statistical strength.",13,14
9692,6684426,"For a given sentence, we first run the inside-outside algorithm using the baseline (unannotated) grammar, producing a packed forest representation of the posterior symbol probabilities for each span.",29,30
9693,6684426,"For example, one span might have a posterior probability of 0.8 of the symbol NP, but e −10 for PP.",14,15
9694,6684426,"This approach gives parsing accuracies of up to 90.7% on the development set, substantially higher than previous symbol-splitting approaches, while starting from an extremely simple base grammar.",19,20
9695,6684426,"In our model, POS tags are split just like any other grammar symbol: the subsymbols for several tags are shown in Table 1 , along with their most frequent members.",13,14
9696,6684426,A nonterminal split can be used to model an otherwise uncaptured correlation between that symbol's external context (e.g. its parent symbol) and its internal context (e.g. its child symbols).,14,15
9697,6684426,A nonterminal split can be used to model an otherwise uncaptured correlation between that symbol's external context (e.g. its parent symbol) and its internal context (e.g. its child symbols).,22,23
9698,6684426,"A particularly clean example of a split correlating external with internal contexts is the inverted sentence category (SINV), which has only two subsymbols, one which usually has the ROOT symbol as its parent (and which has sentence final puncutation as its last child), and a second subsymbol which occurs in embedded contexts (and does not end in punctuation).",33,34
9699,6684426,"As one example, the subsymbol S-12 (matrix clauses) occurs only under the ROOT symbol.",16,17
9700,6684426,"This same propagation occurs even more frequently in the intermediate symbols, with, for example, one subsymbol of NP symbol specializing in propagating proper noun sequences.",21,22
9701,1419832,"For technical convenience we add the symbol ""$ <"" at the beginning of tag images and ""> $~ at the etad.",6,7
9702,8922993,i) Quotated words or phrases from Pali language are skipped by inserting special symbol to indicate there are skipped words.,14,15
9703,8922993,3 are attached special symbol after and before the word.,4,5
9704,8922993,"Statistical data Total number of letters in the machine readable text is 1362602 which include special symbols such as separator,skip symbol,comma,etc.",22,23
9705,1364249,"A dependency structure D over a sentence is a set of dependencies (arcs) which form a planar, acyclic graph rooted at the special symbol ROOT, and in which each word in s appears as an argument exactly once.",26,27
9706,1364249,"Existing generative dependency models intended for unsupervised learning have chosen to first generate a word-free graph G, then populate the sentence s conditioned on G. For instance, the model of Paskin (2002) , which is broadly similar to the semiprobabilistic model in Yuret (1998) , first chooses a graph G uniformly at random (such as figure 2 ), then fills in the words, starting with a fixed root symbol (assumed to be at the rightmost end), and working down G until an entire dependency structure D is filled in (figure 1a ).",79,80
9707,1364249,"Even with a grammar in which the productions are initially uniform, a symbol X can only possibly have non-zero posterior likelihood over spans which contain a matching terminal X. Therefore, one can start with uniform rewrites and let the interaction between the data and the model structure break the initial symmetry.",13,14
9708,1364249,"Moreover, in a head-outward model, it is natural to model stop steps, where the final argument on each side of a head is always the special symbol STOP.",31,32
9709,1080545,"In practice, we have a special stop symbol in our n-gram counts, and the probability of emitting a space from a final state is the probability of the n-gram having chosen the stop character.",8,9
9710,6422949,"Edmonton An A* Algorithm An agenda-based PCFG parser operates on parse items called edges, such as NP:[0,2], which denote a grammar symbol over a span.",27,28
9711,6422949,"The outside portion is an outside parse: formally, an outside parse of an edge X:[i, j] in sentence s = 0 w n is a derivation from G's root symbol to w 0i Xw jn .",34,35
9712,6422949,"A* Estimates for Parsing When parsing with a PCFG G, each edge e = X:[i, j] spans some interval [i, j] of the sentence and is labeled by some grammar symbol (or state) X. Our presentation assumes that G is a binarized grammar, and so in general X may be either a complete state like NP that was in an original n-ary grammar, or an intermediate state, like an Earley dotted rule, that is the result of implicit or explicit grammar binarization.",37,38
9713,18019987,"The well-formedness is understood as a purely syntactical feature, Rule I. <text>÷#<t-section>(><t-section>)*# Here #~ a terminal boundary symbol, ~ means ""then"", ""and after that"", * is Kleene's star operator; the parentheses are neither terminal nor nonterminal symbols, they only mean that the sequence ><t-section> is to be repeated as a whole, A t-section represents an event.",30,31
9714,18019987,"> < The symbol over a term may be rewritten as ~ -~,~_mean For a term x the expressions T, respectiyely, that the occurrence of a predicate, with x, ~, x filling up a slot, is bad for x, good for x, or involves a hope for the good for x. > < may also be omitted.",3,4
9715,11495042,"For variable horizontal histories, we did not split intermediate states below 10 occurrences of a symbol.",16,17
9716,11495042,"For example, if the symbol VP: [VBZ] . . .",5,6
9717,11495042,These symbol counts include all the intermediate states which represent partially completed constituents.,1,2
9718,11495042,"In the remaining sections, we discuss other annotations which increasingly split the symbol space.",13,14
9719,11495042,"Second, the VP symbol is very overloaded in the Penn treebank, most severely in that there is no distinction between finite and infinitival VPs.",4,5
9720,18749678,"Each time a new non-terminal is derived in the expansion of a rule, this non-terminal is added to the cancellation set and the resulting set is passed on to the next symbol in the expansion.",36,37
9721,13508558,All symbols of the form 'c-xxx' represent the Chinese equivalent of the English symbol 'xxx'.,17,18
9722,59759783,"This conditional probability denotes the probability that a non-terminal symbol a , having appeared in the sentential form during the sentence derivation process, will be replaced with a sequence of term inal and non-terminal symbols /? .",11,12
9723,59759783,p Processes of sentence generation from a sentence symbol 5 by a probabilistic context-free grammar will be carried out in an identical m anner to the usual non-probabilistic context-free grammar.,8,9
9724,59759783,"VP can be reached from the state 5 by applying the rule 5 -* hP.VP to a start symbol 5, the state ART.NOUN.VP can be reached from the state NP.VP by applving the rule NP -* ART.NOUN to the first NP of the sentential form NP.VP, and so on.",19,20
9725,59759783,"In a context-freegrammar, each replacement of a non-terminal symbol occurs independently of the context.",13,14
9726,59759783,Normalize the count so that the total count for rules with same left hand side non-terminal symbol a becomes 1.,18,19
9727,59759783,"This triplet shows that a sequence of fi.y which spans substring(i,j) is replaced with a non-terminal symbol a. (/?:",21,22
9728,59759783,"The top-level of the two-level list corresponds to a left hand side non-term inal symbol, called as LHS symbol hereinafter.",20,21
9729,59759783,"The top-level of the two-level list corresponds to a left hand side non-term inal symbol, called as LHS symbol hereinafter.",25,26
9730,59759783,All com binations of left and right subordinate structures are kept in the sub-list of the LHS symbol.,19,20
9731,59759783,"y) to the sub-list of a. In addition to the sub-list, a LHS symbol is associated with two variables -MaxP and SumP. These This procedure is similar to that of Viterbi algorithm [4] and maintains the maximum probability and the total probability in M axP and SumP respectively.",19,20
9732,11613407,"The practical value of the incorporation of an explanation component lies essentially in the fact that, as Stallman and Sussman have put it, '~such programs are more convincing when right and easier to debug when wrong"".~5 Figure I provides an overview and comparison of the explanation components which have been implemented to date: BLAH 22, DIGITALIS ADVISOR 16, EL Is, EXPOUND ~, HAM-RPMI~, 21, LUIG113, MYCIN12, ~, NOAH 11, PROSPECTOR 7, SHRDLU ~, TKP2 I° (The symbol ""-"" signifies that the attribute in question is not applicable to the given system).",96,97
9733,215826656,Generation continues until the <eos> symbol is generated.,7,8
9734,61030,"For example, the non-terminal symbol NP can be thought of as a cluster of (terminal) sequences which can be generated starting from NP.",7,8
9735,352962,"In other words, for each symbol a in the vocabulary of some regular set R, let R b be a particular regular set.",6,7
9736,352962,"Where N and N' are nonterminals, T k , T j , T k , and T j , are terminals, sem-k is semantic features, P and P' are production rules, and S is the start symbol.",44,45
9737,5897247,"First, the input text (= the input sequence of k~na and kanjl kodes) is broken up into fragments by conteztual delimiters eertMn to denote word or morph boundaries (e.g., punctuation marks, the occurrence of a k~tat~na symbol after a hlragana one or vice verB% etc,).",47,48
9738,3543642,"For the transcriptions created by the Englishspeaking participants, and since most of the wordlike units of the transcriptions do not exist in any English pronunciations lexicon, we use the LO-GIOS Lexicon Tool (SpeechLab, 2007) that uses some simple letter-to-sound rules to produce a phonetic transcription in the ARPAbet symbol set.",59,60
9739,15641974,"The utility of comparing part-of-speech symbol strings (set 12) is very limited (what use would be the match of ""Queen of England"" with ""rate of exchange,"" both N Prep N?)",9,10
9740,35225391,"For example, the straight double quote used by Excel in English is also the symbol for inches and can cause problems in Excel glossaries.",15,16
9741,237295819,"The limitations of our data records to a fixed length had the greatest impact on our work 40 characters for field 1 (I 1 : foreign language term) 34 characters for field 2 (I 2 : explanatory addition to or rearrangement of I 1 ) 45 characters for field 4 (I 4 : German equivalent) 39 characters for field 5 (I 5 : explanatory addition to or rearrangement of I 4 ) 13 characters for field 3 (I 3 : code group (which was increased by 2 characters): language symbol, subject field code, source, quality symbols) and I 6 -an 8-digit number allocated (automatically) to each data record as the storage address and for parity check.",98,99
9742,237295819,A symbol prevents the output of such data records -in this instance in the direction English-German.,1,2
9743,237295819,"We do not consider our System to be as sophisticated as it could be, but it is easy to operate it is very fast and it offers the translator only as much information as he can handle and requires for his work, i.e. the foreign-language term and its German equivalent(s), subject field code, source, quality symbol and, on demand only, definitions and context.",63,64
9744,29597766,"They are represented in the thesaurus with the symbol UP/UF (Usado Por/Used For): ""violencia sexual"", ""violación"", ""acoso sexual"".",8,9
9745,13320514,"Localization of basic values such as data, time, decimal symbol and basic measurements conversion is also automated.",11,12
9746,17669202,"As a result, people do not seem to be upset or offended by having to click on a flag that is not that of their own country, when they know that this is a symbol that stands for a widely spoken language, thereby showing a fairly pragmatic attitude.",36,37
9747,36498575,"We do this by traversing the internal target nodes in int i in target derivation order: for each internal target node w j in int i , we (a) generate a sequence added j of added target adjunct phrases one at a time, until the special stop symbol STOP is generated, and (b) assign landing sites to the generated target adjunct phrases in the process.",51,52
9748,236999865,"Each separate information category is stored in its own field, identified by a pre-defined field code, consisting of a language symbol (lower case) and a field type code (upper case) or just a field type code.",24,25
9749,5260380,"Hence, type unification costs are minimized: two symbol table lookups, one addition, one multiplication, and a hash table lookup.",9,10
9750,41201999,"We still sometimes occasionally encounter problems with extended characters in RTF files becoming corrupted between Windows and Mac OSX applications, and with issues of unsupported or differently encoded glyphs in typefaces for DTP, the euro symbol and typographers' quotes being good examples of the latter issues.",37,38
9751,237055463,In some East African cultures for example the importance of blood as a symbol for life itself required more entries to be added to the original English list for blood.,13,14
9752,15931390,Therefore they must be a part of some other symbol system which is not explicitly introduced.,9,10
9753,237417649,"The unit symbol is ""l"".",2,3
9754,473459,in parallelogram are the operations defined in section 3.2.1; and # is a fictitious symbol that indicates the end of the automaton's input. {,15,16
9755,43622537,"Considering the use of symbols form AAC (augmentative and alternative communication) designed for speech-impaired disabled users by non-English speaking patients, we noticed that AAC symbol sets have a systematic iconicity that regular users learn, but which may be opaque to first-time (or one-time) untrained users (Johnson, 2004) .",31,32
9756,42996569,"Having a termbank is rapidly becoming a status symbol, as pointed out by Thomas Schneider, of Siemens: 'other [ termbanks] will be designed mainly for prestige purposes, and that implies: be different from the other banks, and incompatible.",8,9
9757,237055460,"To search for a term, the user enters a string-search of one, two or three letters or a wild card symbol (every additional letter reduces the number of retrievable records) at the beginning of the appropriate language line (normally, the line corresponding to the source language) and then presses F2.",24,25
9758,9181084,"Filtering removes the following elements from the tweets: URLs, tokens starting with the ""@"" symbol (Twitter user names), and tokens starting with the ""#"" symbol (Hashtags). •",18,19
9759,9181084,"Filtering removes the following elements from the tweets: URLs, tokens starting with the ""@"" symbol (Twitter user names), and tokens starting with the ""#"" symbol (Hashtags). •",33,34
9760,237261511,"Apart from the language pair, the data record contains a code group consisting of the language symbol, subject field code, source code and quality symbols, as well as optional short comments in brackets.",17,18
9761,237418560,"In retrospect, we made some mistakes: we recognised that some people would always use two letters for a currency abbreviation such as Fr so an alternative single symbol Fr would only make life complicated, but we yielded to people who had seen ij, oe and ŉ on Dutch, French and Afrikaans typewriter keyboards and insisted they were language-specific single characters.",29,30
9762,237417614,"Figure 1 shows a typical thesaurus entry, in which the term 'natural gas' has been preferred to 'condensate gas' or 'formation gas', and so these are listed with the symbol 'U[sed] F[or]'.",37,38
9763,237256847,"Then comes the 'reference': this is the document symbol, title of document or publication in which the term was found.",11,12
9764,237256847,One convenient short cut which provides a way of finding a term which can be written in several ways is to use the command 'at' which is the symbol on the keyboard consisting of a small 'a' surrounded by a circle which is used to mean 'at' in certain accounting and other contexts.,30,31
9765,237256847,"For right truncation, which is the most commonly used, a word or part of a word is typed in and the 'at' symbol is then typed.",26,27
9766,7698334,Next to splitting on the basis of the maximal cosine distance between two possible parts of the context we also ran the WSD system with contexts that were split in half and with contexts that were split at the first punctuation symbol.,41,42
9767,16648361,A rule takes as input a pair of a lexical stem and a complex symbol representing the inflectional categories associated to the former.,14,15
9768,5873462,Yet this algorithm can be applied to arbitrary symbol chains.,8,9
9769,18178933,The colon in Figure 2 is the symbol for preseleetlon.,7,8
9770,17541304,"In order to obtain generalized dependency rules from examples, one must replace each noun in the examples by some kind of symbol, such as semantic marker or semantic features.",22,23
9771,237204597,"In this case a special symbol (<XX>) is displayed which is treated by the engine like a normal noun: Formatting information which is relevant for translation such as bold, italic etc.",5,6
9772,17263523,The pre-editor would then select the meaning required by the context and dial the distinctive graphic symbol representative of this meaning and supplied by the dictionary. [,18,19
9773,51990022,"Stock market returns For each 100 stock ticker symbol mentioned in the tweet dataset, the stock closing price were downloaded.",8,9
9774,237055445,"A "" §"" (symbol to denote a section of a legal document, as used in several European countries) converts to ( õ ) (lower case letter o with a tilde).",5,6
9775,237055445,Similar problems have plagued UNOV in terms of printing from the custom symbol set on the mainframe.,12,13
9776,27378030,"We assumed that words after a negative word occur in a negative context, that finishes at the end of the tweet unless a punctuation symbol occurs before.",25,26
9777,237295822,"The IEC had the largest collection in the world of symbols for use in circuit diagrams and for use on equipment such as a symbol for 'press', and a series of international symbols for use in railway stations, airports and traffic signs is being developed.",24,25
9778,237295775,"If, within a single linguistic group, one country uses terms peculiar to it, they are mentioned, preceded by the appropriate symbol.",24,25
9779,237295818,"Multiplication was named differently, the equation symbol carried another name, and the same plants had different nomenclature.",7,8
9780,8035278,"The use of these metavariables can be seen in the following LFG grammar rules, where the symbol that introduces the controller constituent has among its equations one ofthe form $ = $ $ and the empty word is associated with the equation ~ = ~ ~. S' NP S (1"" Q-FOC) = $ t = ~=~ NP -~ e t=tt The instantiation procedure for these bounded domination variables thereby is defined such that it identifies the f-structures of the controller and controllee in a way which is shown in the following schematical description in figure 1 for a sentence like (2), leading to f-structures with shared substructures.",17,18
9781,9705354,"Also associated with the NIKL system, though not a part of the core language definition, is a symbol table which associates atomic names with the roles or concepts they denote, and concepts and roles with the names denoting them.",19,20
9782,9705354,"If a concept or role does not have a name, the symbol table is able to create and install one for it when demanded.",12,13
9783,9705354,"Similarly, the NIKL system's creating a new concept or role, and creation of a name in the symbol table to stand for it, furnishes us with a new non-J0gical constant.",20,21
9784,9705354,"The symbol ""<rest>"" denotes a (possibly empty) sequence of formulae.",1,2
9785,9705354,"The symbol ""<quant>"" is to be understood as being replaced by either the operator SETOF or the quantifier EXISTS.",1,2
9786,30921078,"The JIS X 0201 Roman character set is very similar to the ASCII character set but the backslash has been replaced by the Yen symbol and the tilde has been replaced by the ""overline"".",24,25
9787,30921078,"For anyone who has used Japanese DOS or Japanese Windows this explains why the path separator ""\"" displays as a Yen symbol.",23,24
9788,15653259,"For every root symbol created, a target tree is built following the structure of the source tree and only altering it within the scope of a single node at a time.",3,4
9789,15653259,"Figure 1: Strategies of MT solutions The Translation Description Language Every terminal and non-terminal symbol (or what is equivalent, the corresponding node in the syntax tree under construction) has a well-defined, type-specific set of features.",17,18
9790,15653259,"The difference, however, between this and our system is that in our system every symbol that is created and is not eliminated by an overriding pattern is retained even if it does not form part of a correct sentence's syntax tree.",16,17
9791,67741411,The first symbol in each expr~on is a label indicating the function of that item within the plan; embedded items appearing in angle brackets are information units from the current-events knowledge base.,2,3
9792,63727371,"Continuous speech Technology problems -The sound-to-symbol mapping is imperfectly realised -context and individual differences have profound and complex effects, which we cannot yet capture in our computer systems.",9,10
9793,281834,"The justices make the basic fact situation weaker and stronger along a dimension that might be called focus-of-attention: they remove all of the secular images leaving only the religious one, they physically shrink the symbol to an extreme and relegate it to a corner, they remove the religious symbols and leave the secular ones.",40,41
9794,8248280,"Since, by definition, no auxiliary trees are adjoinable to a node labelled by a terminal symbol, •o co•straint ha8 to be stated for node labelled by a terminal.",17,18
9795,8248280,"If ~ is an auxiliary tree, 3` 6 D(~) and the frontier of 3` is w s X w 2 (X is • nonterminal,Wl,W 2 6 L ~') the• the leaf node having this non-terminal symbol X at the frontier is called the foot of 3`.",48,49
9796,8248280,"once we note that there are no auxiliary trees in G rooted with the symbol S, and that N 1 13 N z == as.",14,15
9797,8248280,"Let S be a symbol ant in Ni, and let N == N t U (S).",4,5
9798,8248280,"The OA and NA constraints at X are treated similar to the previous cases, and so is the case if either Y or Z is labelled by a terminal symbol.",30,31
9799,8248280,"We will associate with Y the quadruple (p,q20q~,s) and the constraint that root of the tree which can be adjoined at X should have the quadruple (ql,P,e,q4) associated with it among the trees which were allowed in the original grammar, if it is to be adjoined at X. The cases where the original grammar had null or obligatory ""constraint associated with this •ode or Y is labelled with a terminal symbol, are treated similar to how we dealt with them ia the previous cases.",84,85
9800,8248280,"It k more convinient for us to think of the headed string (i,a,.:.aa) as the string al...a a with the head pointing in between the symbok a I and el+ , rather than at the symbol t 1.",41,42
9801,26927490,It is a formal system that depicts a language by describing how a legal text can be derived from symbols called an axiom or sentence symbol [ELI2001].,25,26
9802,26927490,"> A start symbol, which is a special non-terminal symbol that appears in the initial string generated by the grammar [NELS2001].""",3,4
9803,26927490,"> A start symbol, which is a special non-terminal symbol that appears in the initial string generated by the grammar [NELS2001].""",12,13
9804,26927490,"To generate a string of terminal symbols from a Context-free grammar, the following steps are done: • First, begin with a string that consists of the start symbol; • Apply one of the productions with the start symbol on the left hand side, replacing the start symbol with the right hand of the production; • Repeat the process of choosing non-terminal symbols in the string, and replacing them with the right hand side of some corresponding production, until all the non-terminal symbols are replaced by terminal symbols [NELS2001] .",32,33
9805,26927490,"To generate a string of terminal symbols from a Context-free grammar, the following steps are done: • First, begin with a string that consists of the start symbol; • Apply one of the productions with the start symbol on the left hand side, replacing the start symbol with the right hand of the production; • Repeat the process of choosing non-terminal symbols in the string, and replacing them with the right hand side of some corresponding production, until all the non-terminal symbols are replaced by terminal symbols [NELS2001] .",43,44
9806,26927490,"To generate a string of terminal symbols from a Context-free grammar, the following steps are done: • First, begin with a string that consists of the start symbol; • Apply one of the productions with the start symbol on the left hand side, replacing the start symbol with the right hand of the production; • Repeat the process of choosing non-terminal symbols in the string, and replacing them with the right hand side of some corresponding production, until all the non-terminal symbols are replaced by terminal symbols [NELS2001] .",53,54
9807,26927490,> Start symbol: special non-terminal symbol that appears in the initial string generated by the grammar.,2,3
9808,26927490,> Start symbol: special non-terminal symbol that appears in the initial string generated by the grammar.,8,9
9809,2248442,"APP-TERM ""B J-"" *9) denotes that a new symbol concatenating ""B J-"" and the value of *9 should be created. """,14,15
9810,1222366,"Finally, we also consider a version of the data set where we filtered out the hash-tag symbol from the Twitter news sentence pairs.",19,20
9811,1222366,"Finally, Run3, submission correction phase (0.7901), used a different data set were we filteredout hash-tag symbol from Twitter-news sentence pairs.",22,23
9812,237217076,The national language soon became a critical symbol of the central power.,7,8
9813,6637772,"An example of the effect of tokenization can be observed for the QUESTION class, which improved largely because of the question mark symbol '?'",23,24
9814,63759597,The symbol @ covers a specified range of words.,1,2
9815,63759597,"The third option in the advanced search dialogue box is TAG SEARCH, which allows the user to specify a search query consisting of a combination of words and part-of-speech tags, with the special symbol & being used to separate words from tags in the search query.",39,40
9816,17603302,"Table 1 describes selected symbols from the ARPAbet symbol set (Shoup, 1980) used for representing phonemes and phones in TIMIT.",8,9
9817,17603302,"During this process, we consider only potential phone/phoneme matches, and we ignore any match involving a ""-"" symbol (which indicates an insertion or a deletion).",23,24
9818,250390823,"CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating questions/answers).",5,6
9819,12355446,"Inflection information is coded through the <s> (symbol ) XML tag, the first one being the part-of-speech.",10,11
9820,32482544,Cashtag: Cashtag provides the stock ticker symbol to which the span and sentiment are related.,7,8
9821,42963593,"In the tweet, ""Oil To Break Out: Adding Chevron https://t.co/IrZkAVxjiE $AXP $CLGRF $CSCO $ERX $IBM $MCD $SSRI $VLO $WMT $XOM $CVX"", all companies denoted by the cashtag $ticker-symbol share only one description.",46,47
9822,29382717,Every tweet had a sentiment score between -1 to 1 and it showed its sentiment toward the stock symbol that was assigned to that tweet.,18,19
9823,6852855,"Introduction A Hidden Markov Model (HMM) is a finite state automaton with stochastic state transitions and symbol emissions (Rabiner, 1989) .",18,19
9824,6852855,"The automaton models a random process that can produce a sequence of symbols by starting from some state, transferring from one state to another state with a symbol being emitted at each state, until a final state is reached.",28,29
9825,6852855,"Formally, a hidden Markov model (HMM) is specified by a five-tuple (S, K, Π, A, B), where S is a set of states; K is the alphabet of observation symbols; Π is the initial state distribution; A is the probability distribution of state transitions; and B is the probability distribution of symbol emissions.",67,68
9826,6852855,"To train an HMM for extracting fillers for a specific slot, maximum likelihood estimation is used to determine the probabilities (i.e., the initial state probabilities, the state transition probabilities, and the symbol emission probabilities) associated with each HMM from labelled texts.",36,37
9827,6852855,"The probability of s following this particular background state path Q bg can be easily calculated with respect to the HMM λ as follows: P (s, Q bg |λ) =π q bg b q bg (O 1 )a q bg q bg b q bg (O 2 ) • • • a q bg q bg b q bg (O T ), where π i is the initial state probability for state i, b i (O t ) is the emission probability of symbol O t at state i, and a ij is the state transition probability from state i to state j. We know that the probability of observing s given the HMM λ actually sums over the probabilities of observing s on all the possible state sequences given the HMM, i.e., P (s|λ) = all Q P (s, Q|λ) Let Q f iller denote the set of state sequences that pass through any filler states.",95,96
9828,17063549,"Often we drop the superscripts N and T over the level symbol, assuming some lower level L_ 1, whenever it does not lead to ambiguity.",11,12
9829,20731062,Cashtag refers to a stock symbol that uniquely identifies a company.,5,6
9830,20731062,For e.g. $AAPL represents stock symbol for the company Apple Inc. Every instance of microblogs messages also include a span which indicates a part of text from where prediction should be derived.,6,7
9831,56549841,"In a similar manner to the product Kite Mark (see earlier in this paper), there is a similar recognised symbol for accreditation to BS 5750.",22,23
9832,237155051,The same sort of translation ensures that a Norwegian 'A with circle' displayed on the Epson (but which is actually transmitted as a bracket symbol) is turned back into an 'A with circle' on the Sony display.,27,28
9833,477974,"Hyperonyms in NLG systems In contrast to psycholinguistics-inspired work, the vast majority of natural language generation systems uses computations based on symbol manipulation, often connected with symbolic knowledge representation and reasoning techniques.",24,25
9834,6037458,"To construct a reference to a particular entity, the algorithm takes as input a symbol corresponding to the intended referent and a list of symbols corresponding to other entities in focus, known as the contrast set.",15,16
9835,13307193,This was bccause the parser was run with the start symbol set to the labe!,10,11
9836,219303746,"In equations ( 5 ) and ( 6 ) the contribution of a node N is determined by the combination of the inside probabilities of N's children and by all possible adjunctions at N. In (7) we recognize some terminal symbol if it occurs in the prefix, or ignore its contribution to the span if it occurs after the last symbol of the prefix.",43,44
9837,219303746,"In equations ( 5 ) and ( 6 ) the contribution of a node N is determined by the combination of the inside probabilities of N's children and by all possible adjunctions at N. In (7) we recognize some terminal symbol if it occurs in the prefix, or ignore its contribution to the span if it occurs after the last symbol of the prefix.",64,65
9838,219302769,"For each leaf A E V, label(A) is an element from E U {e}, and for each other node A, label(A) is an element from N. S is an element from N which is a distinguished start symbol.",44,45
9839,5705028,"In what follows, we use a,ß, ... to denote strings of nonterminals associated with empty stacks of indices, x,y,v,w,z, ... to denote strings of terminal symbols, and a to denote a terminal symbol.",47,48
9840,5705028,"For example, the condition VaVß V-yVS =/:t in step (4) needs to be reformulated to the effect that at least one symbol from VaVßV-yVS should belang to the input, i.e. the prefix.",27,28
9841,18289778,"To solve this problem, we require that the last (meta) symbol of the preceding tokenization must equal to the first (meta) symbol of the following tokenization in order to concatenate the two.",13,14
9842,18289778,"To solve this problem, we require that the last (meta) symbol of the preceding tokenization must equal to the first (meta) symbol of the following tokenization in order to concatenate the two.",26,27
9843,57916,"In equations ( 5 ) and ( 6 ) the contribution of a node N is determined by the combination of the inside probabilities of N's children and by all possible adjunetions at N. In (7) we recognize some terminal symbol if it occurs in the prefix, or ignore its contribution to the span if it occurs after the last symbol of the prefix.",43,44
9844,57916,"In equations ( 5 ) and ( 6 ) the contribution of a node N is determined by the combination of the inside probabilities of N's children and by all possible adjunetions at N. In (7) we recognize some terminal symbol if it occurs in the prefix, or ignore its contribution to the span if it occurs after the last symbol of the prefix.",64,65
9845,1924466,"For each leaf A E V, label(A) is an element from E U {e}, and for each other node A, label(A) is an element from N. S is an element from N which is a distinguished start symbol.",44,45
9846,16964246,"means any symbol, i.e., ?",2,3
9847,195700014,"From BERT's perspective, the artificial word is a period symbol.",11,12
9848,13539149,"Harris [1955] proposed an approach for segmenting words into morphemes that, although it did not use entropy, was based on an intuitively similar concept: Every symbol of a word is annotated with the count of all possible successor symbols given the .substring that ends with the current symbol, and with the count of all possible predecessor symbols I Such a corpus can be electronically encoded with arbitrarily defined symbol codes.",30,31
9849,13539149,"Harris [1955] proposed an approach for segmenting words into morphemes that, although it did not use entropy, was based on an intuitively similar concept: Every symbol of a word is annotated with the count of all possible successor symbols given the .substring that ends with the current symbol, and with the count of all possible predecessor symbols I Such a corpus can be electronically encoded with arbitrarily defined symbol codes.",52,53
9850,13539149,"Harris [1955] proposed an approach for segmenting words into morphemes that, although it did not use entropy, was based on an intuitively similar concept: Every symbol of a word is annotated with the count of all possible successor symbols given the .substring that ends with the current symbol, and with the count of all possible predecessor symbols I Such a corpus can be electronically encoded with arbitrarily defined symbol codes.",74,75
9851,13539149,given the tail of the word that starts with the current symbol.,11,12
9852,13539149,"The probability of a symbol s with respect to this model M and to a context c can be estimated by: f(s, M, c) p(slM, c) = (4) ](U,c) The inIormation of a symbol s with respect to the model M and to a context c is defined by: I(s]U, c) = -log2 p(slU, c) (5) Intuitively, information can be considered as the surprise of the model about the symbol s after having seen the context c. The more the symbol is unexpected from the model's experience, the higher is the value of information [Shannon and Weaver, 1949] • The entropy of a context c with respect to this model M expresses the expected value of information, and is defined by: g(Af, c) = Zp(s]M,c ) I(slM, c) (6) sEE Monitoring entropy and information across a corpus shows that maxima often correspond with word 3Note that blanks become ""BL"" and new-lines become ""NL"".",4,5
9853,13539149,"The probability of a symbol s with respect to this model M and to a context c can be estimated by: f(s, M, c) p(slM, c) = (4) ](U,c) The inIormation of a symbol s with respect to the model M and to a context c is defined by: I(s]U, c) = -log2 p(slU, c) (5) Intuitively, information can be considered as the surprise of the model about the symbol s after having seen the context c. The more the symbol is unexpected from the model's experience, the higher is the value of information [Shannon and Weaver, 1949] • The entropy of a context c with respect to this model M expresses the expected value of information, and is defined by: g(Af, c) = Zp(s]M,c ) I(slM, c) (6) sEE Monitoring entropy and information across a corpus shows that maxima often correspond with word 3Note that blanks become ""BL"" and new-lines become ""NL"".",46,47
9854,13539149,"The probability of a symbol s with respect to this model M and to a context c can be estimated by: f(s, M, c) p(slM, c) = (4) ](U,c) The inIormation of a symbol s with respect to the model M and to a context c is defined by: I(s]U, c) = -log2 p(slU, c) (5) Intuitively, information can be considered as the surprise of the model about the symbol s after having seen the context c. The more the symbol is unexpected from the model's experience, the higher is the value of information [Shannon and Weaver, 1949] • The entropy of a context c with respect to this model M expresses the expected value of information, and is defined by: g(Af, c) = Zp(s]M,c ) I(slM, c) (6) sEE Monitoring entropy and information across a corpus shows that maxima often correspond with word 3Note that blanks become ""BL"" and new-lines become ""NL"".",90,91
9855,13539149,"The probability of a symbol s with respect to this model M and to a context c can be estimated by: f(s, M, c) p(slM, c) = (4) ](U,c) The inIormation of a symbol s with respect to the model M and to a context c is defined by: I(s]U, c) = -log2 p(slU, c) (5) Intuitively, information can be considered as the surprise of the model about the symbol s after having seen the context c. The more the symbol is unexpected from the model's experience, the higher is the value of information [Shannon and Weaver, 1949] • The entropy of a context c with respect to this model M expresses the expected value of information, and is defined by: g(Af, c) = Zp(s]M,c ) I(slM, c) (6) sEE Monitoring entropy and information across a corpus shows that maxima often correspond with word 3Note that blanks become ""BL"" and new-lines become ""NL"".",101,102
9856,13539149,"Here, an information value is assigned to every symbol.",9,10
9857,13539149,This value expresses the information of the symbol in a given left or right context.,7,8
9858,13539149,"It expresses the model's uncertainty after having seen the left or right context, but not yet the symbol• When going from left to right, all end of a separator, is often marked by a maximum in entropy because the next word to the right can start with almost any symbol, and the model has no ""idea"" what it will be.",53,54
9859,13539149,There is also a maximum in information because the first symbol of the word is (more or less) unexpected; the model has no particular expectation.,10,11
9860,13539149,"Similarly, when going from right to left, a beginning of a separator is often marked by a maximum in entropy because the word next to the left call end with almost any symbol.",34,35
9861,13539149,There is also a maximum in information because the last symbol of the word is (more or less) unexpected; the model has no particular expectation.,10,11
9862,13539149,"It also may be observed that the maxima in one direction are bigger then the maxima in the other direction due to the fact that a particular language may have e.g. stronger constraints on endings than on beginnings of words: A language may employ suffixes with most words in a corpus, which limits the number of endings, but rarely use prefixes, which allows a word to start with almost any symbol Thresholds Not all maxima correspond with word boundaries.",74,75
9863,13539149,"5As context of a separator, we consider the preceding and the following symbol.",13,14
9864,3265110,"Thus we obtain a tagged b-type sequence s. "" (14) -c_/~+l .-.C_ 2 C0:~0 C2-'""~a-1 ta stating that to is the most probable tag in the class co if it is preceded by t B~ cS(Z-z)...cB2 c m and followed by c al cA:.. .c A(~-I) ta% In expression 14 the subscripts --/3 -B+I.. Boundaries are denoted by a particular symbol and can occur at the edge of the look-back and lookahead sequence: t B~ c s(t~-l) ...c B2 c B1 c:t c Ax c A1 ...c A(a-1) #An (16) t s# c ~(~-l) ...c ~ c B1 c:t c A1 c A1 ...#A(~--Z) (17) #Be C~(~-Z) ...CB2 cBZ c:t #AZ (18) #BZ c:t #AZ (19) #B2 cBl c:t c A' c ~I ...cA(°-I) t a~ (20) For example: ~-B2 [DET, PRONI-B1 [ADJ, NOUN, V~B]: ADJ ( 21 ) SRegular expression operators used in this article are explained in the annex.",73,74
9865,3265110,"We rather traverse the network, and overwrite every symbol r with the empty string symbol e. In the following determinization of the network, all ~ are eliminated.",9,10
9866,3265110,"We rather traverse the network, and overwrite every symbol r with the empty string symbol e. In the following determinization of the network, all ~ are eliminated.",15,16
9867,3265110,Any symbol in the known alphabet and its extensions,1,2
9868,655403,A finite set of attributes is associated with each non-terminal symbol.,12,13
9869,48356442,Part-of-Speech (POS) tagging assigns a category from a given symbol set to each token in an input string.,15,16
9870,1418840,"Like in an HMM, we take into account initial probabilities ~r, transition probabilities a and class (i.e. observation symbol) probabilities b. We do, however, not estimate probabilities over paths.",21,22
9871,1418840,"Every arc is labelled with the same symbol pair as its destination state, with the class symbol in the upper language and the tag symbol in the lower language.",7,8
9872,1418840,"Every arc is labelled with the same symbol pair as its destination state, with the class symbol in the upper language and the tag symbol in the lower language.",17,18
9873,1418840,"Every arc is labelled with the same symbol pair as its destination state, with the class symbol in the upper language and the tag symbol in the lower language.",25,26
9874,1418840,"The final subsequence of a sentence is equivalent to a middle one, if we assume that the sentence end symbol (.",20,21
9875,1418840,Any symbol in the known alphabet and its extensions,1,2
9876,3223962,"The heuristics derived from the above discussions are summarised below: Heuristic 4 Preferences among features for embedding and center transition: Abrupt shifting + Normal embedding Good embedding > Continuation + Joint Conjunct > Good embedding The '+' symbol can be interpreted in different ways, depending on how the features are used in an NLG system.",41,42
9877,3223962,"In a system using numbers for planning, it can have the same meaning as the arithmetic symbol.",17,18
9878,10685297,"it is called observation symbol prolmbility (lexical probability) and can be estimated by: f(wl ll) t,(,,,~lt~) --f(t~) (2) The second order state transition probability (contextual probability) 1,(t~ I t~-2 re-.t) in formula (l) expresses how probable it; is that the tag tl appears in the context of its two preceding tags li-', all(] ti-].",4,5
9879,195767584,"Weighted Grammars Formally, a sentiment grammar is defined as G = (N, S, Σ, R t , R e , W t , W e ), where N = {A, B, C, ...} is a finite set of sentiment polarities, S ∈ N is the start symbol, Σ is a finite set of terminal symbols representing words such that N ∩ Σ = ∅, R t is the transition rule set containing production rules of the form X α where X ∈ N and α ∈ N + ; R e is the emission rule set containing production rules of the form X w where X ∈ N and w ∈ Σ + .",58,59
9880,195767584,"The posterior (or expected count ) of A BC, i, k, j can be calculate as follows: q(A BC, i, k, j) = s(A , BC, i, k, j) s I (S, 1, n) , (16) where s I (S, 1, n) is the inside score for the start symbol S over the whole sentence w 1:n .",72,73
9881,59860008,"Definition 4 (Discontinuous Phrase Structure Grammar) A Discontinuous Phrase Structure Grammar (DPSG) is a quadruple (VN, Vr ,S, R), where • VN is a fi.nite set of nonterminal symbols; • Vr is a fi.nite set of terminal symbols; Jet V denote V N U Vr ; • S E V N is the distinguished start symbol of the grammar; 3 The Parsing Algorithm Bunt (1991) outlined an active ch art parsing algorithm for DPSG that constructs discotrees bottom up (see also (van der Sloot, 1990) ).",67,68
9882,59860008,"To this end, each edge is additionally asso ciated with two bit-strings (fields covers and ctxt_covers ) Later on, it also builds the edge [O, 4, S ➔ P • Q f, 110100, 000000] , which is called active since it still needs to be combined with consd.tuents for Q and f. Notice that the input symbol c is not yet dominated by a direct or a context daughter of the edge .",67,68
9883,59860008,The left-hand side category of ie (ie.ihs) must match the symbol to the right of the dot on ae's right-hand side (ae.next_cat) .,14,15
9884,59860008,"Furthermore , in order to ensure that the constituents corresponding to the right-hand side symbols of ae form an adjacency sequence, the next edge to be combined with ae has to start at the position of the first terminal symbol within ae's span that is neither dominated by a direct nor by a context daughter of ae.",42,43
9885,59860008,The algorithm maintains a set of accumulators c> n (Y) for each symbol YE V and each node n in the parse tree .,15,16
9886,59860008,"We instead use two bit-strings for this purpose, as explained in the previous Initialization: { We initialize the algorithm by assigning a value of 1 to c> b &(Z) for each terminal symbol Wt -I = Z (1 :'.S t :'.ST) in the input, such that b is a bit-string in which only the t-th bit is set and b is an all-zero bit-string .",40,41
9887,59860008,"After the values for all & b, b(X i ) have been computed, the probability of the MPP is that of the accumulator for parse trees headed by the start symbol of the input grammar (X 1 ) that dominate all terminal symbols in the input (b = 1 T , b = o T ).",32,33
9888,59860008,"Finally, when the agenda is empty, the edge that is headed by the start symbol of the grammar and that spans the entire input represents the Most Probable Parse for the given input sentence.",16,17
9889,21366937,2016) removes only the # symbol and treats the hashtag as a unigram.,6,7
9890,174798001,"The decoder is another RNN that takes this vector as input and decodes it sequentially, outputting one symbol at each timestep (here, the phonemes of the past tense form).",18,19
9891,12586316,Unknown characters are mapped to a special symbol that is not used elsewhere. . . .,7,8
9892,15515338,"Table 2 summaries the patterns listed by different aspects, with a symbol '+' and '-' representing the polarity being positive or negative the pattern implies.",12,13
9893,2360770,"To extract the interaction footprint of students before they drop out of the course, we extract the following features: a)N-grams of length 4,5 and sequence length from ""Engagement trajectory"", ""Video Play Proportion trajectory"" and ""IPI trajectory"" of students for the videos watched from 0 to (n-1)th instant, b)Engagement, Video Play Proportion and IPI trajectories for the nth instance (attribute for the last video lecture watched before dropping out), c)Proportion of different symbol representations in the trajectories (for example, in a trajectory such as HLLHH, proportion(H)=60%, proportion(L)=40%.",88,89
9894,49870417,All of these can be split by the '+' symbol and transformed into a root and tag sequence.,11,12
9895,174803587,"In our GCDT, the character sets consist of all unique characters in datasets besides the special symbol ""PAD"" and ""UNK"".",17,18
9896,32457473,"9 In order to unify the notation for merge and move, we adopt the convention that all diacritics appear on the side of the Part of Speech (PoS) symbol on which selection occurs; hence x= indicates rightward selection, =x leftward selection.",31,32
9897,32457473,"31 If we view the syntactic part of the head chain plus the α f s as a single atomic category symbol, then all we are saying in effect here is that combining a category of type A with a category of type B results in a category of type C, which is no different from any other MCFG rule.",21,22
9898,48356464,"In a directional MG, such as that presented in T&S, the = symbol on the selector can appear on either side of the x category symbol, and this indicates whether selection is to the left or to the right.",14,15
9899,48356464,"In a directional MG, such as that presented in T&S, the = symbol on the selector can appear on either side of the x category symbol, and this indicates whether selection is to the left or to the right.",27,28
9900,48356464,"symbol: The +ACC selectional requirement on the V head's +CASE licensor specifies that the object's licensee feature must bear an ACC selectional property, while +NOM on the T(ense) head indicates that the subject's licensee must have a NOM property.",0,1
9901,155099789,"The decoder is an LSTM trained as a conditional language model, i.e. its initial hidden state is the output of the encoder and its input at each step is the embedding of previous output symbol.",35,36
9902,7568858,"The GRADE algorithm can be applied to any generative grammar i.e., any grammar which uses a start symbol and a set of production rules to generate the sentences of the language described by that grammar.",18,19
9903,7568858,"It is grammar neutral in that it can be applied to any generative grammar i.e., any grammar which includes a start symbol and a set of production rules.",22,23
9904,7568858,"Starting from the string consisting of the start symbol, the GRADE algorithm recursively applies grammar rules replacing one oc-1 Although some semantic input is possible.",8,9
9905,7568858,currence of its left-hand side in the string by its righthand side until a string that contains neither the start symbol nor designated nonterminal symbols is produced.,22,23
9906,7568858,"The type of a recursive rule is simply the main category expanded by that rule namely, N, NP, V, VP and S. In addition, whenever a rule is applied, the GRADE algorithm arbitrarily divides up the recursion quotas of a symbol among the symbol's children.",46,47
9907,7568858,"The type of a recursive rule is simply the main category expanded by that rule namely, N, NP, V, VP and S. In addition, whenever a rule is applied, the GRADE algorithm arbitrarily divides up the recursion quotas of a symbol among the symbol's children.",49,50
9908,7568858,"Second, the ""root rule"" i.e., the rule that is used to expand the start symbol can be constrained in several ways.",18,19
9909,195820682,The complete language symbol which is a complex type feature structure represented by * Corresponding author.,3,4
9910,196187316,"For preprocessing, we first replace mentions with a certain symbol user .",10,11
9911,196187316,"Finally, words appearing only once in the training set and words not appearing in the training set but appearing in the development set or test set are replaced with a certain symbol unk .",32,33
9912,13115581,"This new item is added to the prefix at the next iteration to predict another item, until an end-of-sequence symbol is reached.",24,25
9913,174797942,"This is possible for dependency parsing because each word has a unique head word, including the root of the sentence, which is attached to an artificial root symbol.",29,30
9914,174797942,Embeddings of POS tags and other special symbol are also randomly initialized.,7,8
9915,5801103,"In this paper, we define an ǫ-extension Hidden Markov Model that allows us to align source and target language strings such that the characters in the source string may be optionally aligned to the ǫ symbol.",38,39
9916,5801103,"1 We also assume both languages have a special null symbol ǫ, that is ǫ ∈ E and ǫ ∈ F. Our alignment model is a Hidden Markov Model H(X, Y, S, T, P s ), where • X is the start state and Y is the end state. •",10,11
9917,5476592,symbol.,0,1
9918,209387740,"LM-based Text Generation We generate text by sampling from a LM implemented on top of LSTM Networks (Hochreiter and Schmidhuber, 1997) trained to predict the next symbol y in the sequence given the history using the following definition: P (y 1 . . .",31,32
9919,209387740,Let x k be the activation for the k th vocabulary symbol at the penultimate layer (i.e. before the output softmax layer).,11,12
9920,209387740, We trained all models with a cross-entropy objective targeted at predicting the next symbol in the sequence.,16,17
9921,8725213,A label-string consists of a grammatical class symbol (see Table 1 ) and a unique ordinal number for each distinct string.,9,10
9922,166228565,"As shown in Figure 1 , we observe that the infilled words should be conditioned on past and future information around the missing part, which is contrary to the popular learning paradigm, namely, each output symbol is conditioned on all previous outputs during inference by using unidirectional Beam Search (BS) (Och and Ney, 2004) .",38,39
9923,231847070,"  We perform statistical significance tests of our method (wmd-EMAP) against all other methods, for a given dataset, and denote the outcomes by ∨ when the compared method is worse and ∧ when our method is worse, while the absence of a symbol indicates insignificant differences.",49,50
9924,235732200,"Specifically, we generate lyrics in the reverse order with rhyme representation and constraint for rhyme enhancement and insert a beat symbol into lyrics for rhythm/beat modeling.",21,22
9925,235732200,"Therefore, we model and generate rhythmic beats along with the lyrics with a specific symbol: we regard beat as a special token [BEAT] Rap usually contains different beat frequencies, i.e., the ratios between the total number of words and the total number of beats in a rap song.",15,16
9926,219300650,"An agent may insert an anchor symbol (e.g. "">>Agent[01]"") at the beginning of its talk, in order to specify which agent to speak to.",6,7
9927,236460317,CLS] and [SEP] are classification symbol and segment separation symbol respectively.,8,9
9928,236460317,CLS] and [SEP] are classification symbol and segment separation symbol respectively.,12,13
9929,14734250,"The relative scope of two quantifiers is only considered for variables We adopt the convention of naming variables with the first letter of the head noun with which they are associated (R=representative, C=company, S=sample) and using the symbol '>' to denote relative scope.",47,48
9930,9064419,"First, those trees d s h which are not rooted on the distinguished symbol and for which there is no g s A He cW such that d can be composed into g .",14,15
9931,9064419,Substitution nodes are indicated with a '+'; foot nodes are indicated with a '*'; the distinguished symbol is 's'.,22,23
9932,9064419,Recall that a feature structure attached to a nonterminal symbol in some rule (tree in the case of TAG) of a grammar is an abbreviation for several similar rules.,9,10
9933,247922348,"2016; Kudo and Richardson, 2018 ) vocabulary V and end with a special end-of-sentence symbol </s>: x, y ∈ {z • </s>|z ∈ V * } (3) where V * is the Kleene closure over V which includes the empty sequence .",20,21
9934,247922348,"5) Stahlberg and Byrne (2019) complete (i.e. ending with the end-of-sentence symbol </s>) hypothesis score during search, and use it to safely prune entire subspaces using Eq.",19,20
9935,219301994,"This symbol grounding approach [Harnad, 1990] represents word meanings as functions which take as input a perceptual representation of a grounding and return whether it matches words in the language.",1,2
9936,174798391,"All input text is lower-cased and tokenised, numbers with 5 or more digits get their digits replaced by a wildcard symbol #, while words longer than 16 characters are replaced by a wildcard token LONG-WORD.",23,24
9937,8107044,"The output interpretation y is described using a subset of first order logic, consisting of typed constants (e.g., robotic soccer player), functions capturing relations between entities, and their properties (e.g., pass(x, y), where pass is a function symbol and x, y are typed arguments).",48,49
9938,8107044,"We encode a first-order decision as α cs , a binary variable indicating that constituent c is aligned with the logical symbol s. A second-order decision β cs,dt , is encoded as a binary variable indicating that the symbol t (associated with constituent d) is an argument of a function s (associated with constituent c).",23,24
9939,8107044,"We encode a first-order decision as α cs , a binary variable indicating that constituent c is aligned with the logical symbol s. A second-order decision β cs,dt , is encoded as a binary variable indicating that the symbol t (associated with constituent d) is an argument of a function s (associated with constituent c).",44,45
9940,8107044,Φ 1 depends on lexical information: each mapping of a lexical item c to a domain symbol s generates a feature.,17,18
9941,8107044,In addition each combination of a lexical item c and an symbol type generates a feature.,11,12
9942,8107044,"We assume that these labels correspond to a binding to some logical symbol, and encode it as a constraint forcing the relations between the two models.",12,13
9943,8107044,"Given the training set {(x, {y 1 , .., y k })}, every word in x is aligned to every symbol in every y that is aligned with it.",28,29
9944,8107044,level matching to a logical symbol.,5,6
9945,2705641,"A head is typica.lly introduced 5 in pretermina.1 rules such a.s lea~e ~ V V ~-partir where, two w~rbs, '""leave"" a.nd ""partir,"" are associated with the heads of the nonterminal symbol V. This is equiwdently expressed as lea~e:l --* V:I V:I +--pa.rtir:l which is physica.lly implemented as a.n entry of a. lexicon.",42,43
9946,2705641,"It, addition, they ca.n be a.ssociated with a.n explMt nonterminal symbol such a.s ""V:*"" or ""A])JP:*"" (e.g., '""leave:V:*"").",12,13
9947,196177052,"At each generation step, it makes a decision as to whether a new token needs to be generated from the current head token h in the dir direction by sampling a STOP or CONTINUE symbol dec from the CHILD distribution p(dec|h, dir, val) where val is an indicator representing whether token h has already generated a token before.",35,36
9948,10956433,"Each model had random interpretations for ten different propositional and four relational symbols; each individual had a 10% chance to be in the extension of each propositional symbol, and each pair of individuals had a 10% chance to be related by a relational symbol.",29,30
9949,10956433,"Each model had random interpretations for ten different propositional and four relational symbols; each individual had a 10% chance to be in the extension of each propositional symbol, and each pair of individuals had a 10% chance to be related by a relational symbol.",47,48
9950,16981400,In other words: the output symbol of any level does not occur in the actual or lowcr level dictionaries.,6,7
9951,16981400,"It is clear that no backtracking is possible in our runtime system, that is, a meta-word cannot be categorized by a symbol that is a recta-letter of meta-words on the same or lower level.",26,27
9952,202781416,"We also exploit this in our model, as BERT supports inputs with multiple sentences separated by a special [SEP] symbol.",22,23
9953,5492372,"If the lexical item is a verb, the corresponding tree is a skeleton for an entire sentence with the verb already present, anchoring the tree as a terminal symbol.",30,31
9954,196199844,"Similarly, when computing a modified variant of skip-grams based on n-grams of size 3, where only consonants are taken into account, and in which we allow to replace up to one segment systematically by a gap-symbol (""-""), we can see from Table 1 that the structure of matching ngrams directly reflects the cognate relations, with Greek çEri ""hand"" opposed to German Hand and English hand (both cognate), as well as Russian [ruka], Polish rẼNka (both cognate).",44,45
9955,8843191,"In this fashion each sentence was broken down into a set of such category-patterns, resulting in a set of different categoryq)atterns for each punctuation symbol.",27,28
9956,5089103,Introduction A replacement expression specifies that a given symbol or a sequence of symbols should be replaced by another one in a certain context or contexts.,8,9
9957,5089103,"An Nt pair a : b can be thought of as tim crossproduct of a and b, the minimal relation consisting of a (the upper syml)ol) and b (the lower symbol).",34,35
9958,5089103,"For this purpose, we introduce a special symbol, .#. (",8,9
9959,5089103,"upper side of a replacement expression that presupposes a strict alternation of empty substrings and non-empty substrings of exactly one symbol: e x e y e z e ... [12] In applying this to the above example, we obtain [.",22,23
9960,16909656,"A category (I, F, A) is represented as: 3 e A categorial identifier I, which is a symbol identifying the category.",23,24
9961,11643531,"When the ENDEXPR expressions, which are listed in the set of the speech act expressions (represented as Set-ENDEXPR), are defined as a symbol ENDEXPR, we can approximate the speech act types as SPEECH ACT=ENDEXPR.",28,29
9962,383997,⇒ t and t ∈ /w/ (S being the start symbol of G).,11,12
9963,383997,"C[y]); ) (where S is the start symbol of the TAG G. General items Conclusion and perspective In this paper, we have illustrated the use for TAGs of general and abstract tools, syntactic descriptions, which can be used to parse linear λ-terms.",11,12
9964,196175723,"Using the chain rule, such a distribution can be factorized as the product of the probability distribution of a character conditioned on its history: p(c 1 , ..., c T ) = T t=1 p(c t | c 0 , ..., c t−1 ), where c 0 is a special symbol indicating the beginning of the sequence.",55,56
9965,196175723,"2) This reformulation is exact under the conditional independence assumption, i.e., that the symbol at position t in the character sequence is independent of the segmentation, given the characters up to time t−1.",16,17
9966,196175723,"We learn a character level neural network to encode the context with hidden representation h t for each character t. The probability distribution of the next symbol, either a character or a n-gram, is obtained by taking the softmax over the full vocabulary, which includes both characters and longer elements: p(• | c 0 , ..., c t−1 ) = softmax(Wh t ).",26,27
9967,209387559,"5 ), starting from a designated start symbol.",8,9
9968,209387559,"We write L S (G) for the set of derivation trees which can be derived from the start symbol S. If we assume that RefBlock is the start symbol, the tree in Fig.",20,21
9969,209387559,"We write L S (G) for the set of derivation trees which can be derived from the start symbol S. If we assume that RefBlock is the start symbol, the tree in Fig.",30,31
9970,209387559,"Given a start symbol S, we define L(G, S) = {( t S , t R ) | t ∈ L S (G)}.",3,4
9971,3879624,In [10] it is affirmed that this can be done associating each elementary structure in a grammar with a lexical item (terminal symbol in the context of formal grammars).,25,26
9972,3205220,"The special symbol * indicates an empty context, which is always satisfied.",2,3
9973,3205220,"If the rules were to be compiled into automata, a genuine symbol, e.g. 0, must be introduced by the rule compiler.",12,13
9974,3205220,"Rule ]/,3 maps an atlix symbol from the fourth tap(', to the surface.",6,7
9975,2994972,h) A sequence of elements is grammatical if there exists at least one wordfor-word translation of it into grammatical category symbols which yields a symbol sequence that is permitted according to the set of combination rules.,27,28
9976,2994972,We say that a symbol sequence which agrees with the combination rules is a grammatical symbol sequence.,4,5
9977,2994972,We say that a symbol sequence which agrees with the combination rules is a grammatical symbol sequence.,15,16
9978,2994972,Z. Shrinking Procedure We assume that it is possible to verify the grammaticality of a symbol sequence by reduction of it to simpler and shorter sequences step by step.,15,16
9979,2994972,In each step one or more symbols in the sequences are replaced by one new symbol.,15,16
9980,2994972,The string replaced by one other symbol will -to begin with without linguistic interpretation -be called a syntagrn; the replacing symbol will be called the name of the syntagm.,6,7
9981,2994972,The string replaced by one other symbol will -to begin with without linguistic interpretation -be called a syntagrn; the replacing symbol will be called the name of the syntagm.,21,22
9982,2994972,"By successive application of rewriting rules, the original sequence is shrunk to a no longer reducible residuej whichmay be just one symbol.",22,23
9983,2994972,b) Combinatorics is condensed to the following a) a sym-bol sequence is a grammatical syntagm Of type t if and only if it can be reducedtot by successive application of one of-the following t~o cancellation rules for contracting two neighbouring symbols of theoriginal or the so far reduced -sequence into one symbol: x/y y~ x y y\x-* x where x and Z are atomic or complex symbols.,59,60
9984,2994972,"Substituting Complex Symbols We make a preliminary simplification of the problem by replacing every complex denominator in the sequence by a new, arbitrary atomic symbol.",25,26
9985,2994972,We then do perform it in more than one way~ thus generating a number of alternative symbol strings to be processed.,16,17
9986,2994972,Instead we have made the symbol qelection procedure more difficult.,5,6
9987,2994972,We first consider the simpler problem of analyzing a given string of category symbols and see if it can be reduced to s. We join the (not underlined symbol) s followed by space to the beginning of the given symbol string.,29,30
9988,2994972,We first consider the simpler problem of analyzing a given string of category symbols and see if it can be reduced to s. We join the (not underlined symbol) s followed by space to the beginning of the given symbol string.,41,42
9989,2994972,"The same holds for the rest of the original string, after the two paired atoms and everything between them has been removed: To analyze a string of words we assign to each word one symbol of the type ABCDE, where C is the set of numerators in all the word ts category symbols, B is the set of nominator atoms appearing immediately before the numerator in any one of the word's category symbols, etc.",36,37
9990,52009420,"Each operator symbol admits a fixed number of arguments called its rank, e.g., the ranks of f 0 , f 1 , and f 2 are 0, 1, and 2, respectively.",2,3
9991,52009420,"The operator symbol at the position π in ξ, denoted by ξ(π), is f if π = ε and ξ i (π ) if π = iπ .",2,3
9992,52009420,"A Σalgebra A consists of a set A (domain) and, for each operator symbol f in Σ of rank k, an operation f 1 shows the operations of the Σ ex -algebras A s and A t with the set of strings and parse trees as domains, respectively.",16,17
9993,52009420,"Grammars for discontinuous parsing We use the following straightforward representation of LCFRSs as IRTGs: Σ is the set of yield function symbols, A s is the set of tuples over strings, and the string algebra A s interprets each yield function symbol by the respective yield function.",44,45
9994,52009420,For each rule we remember the grammatical function symbol of the left-hand side nonterminal to its parent in the algebra A t .,8,9
9995,52009420,"Also, we replace words with less than 4 occurrences in the training corpus by a fresh ""UNKNOWN"" symbol.",20,21
9996,52009420,"In addition, the training set and the validation set are enriched by copies of the constituent trees where every word is replaced by the ""UNKNOWN"" symbol.",28,29
9997,12135344,"There is a finite set of (up to isomorphism unique) tinite models {M1, ..,Mu} of MP such that each other finite model M~ of MP can successively be reduced to a model M G [Mk] by a chain of models M = M E -< M~ -< .. ~ M~. ~ of MP such that for each pair of models /VI~ = (lal',%:i), M~ +1 = (L/i+1,53 :I+1} dmre is a (partial) isomorphism f from l, ti+l\bli in la # such that ,9~(R) is the set of tuples (at,.., a,,~) with (bl, .., b,,~) 6 c3:{+1 (R), and al = bl if bt C b¢ i, and at = f(bt) if bt G U~+~\U ~, for every relation symbol R. Since the infinite models of MP correspond to unions of infinite chains of such models (i.e. MP is a rather restricted W-theory), we can reduce the test of M~. p q~ tbr each model M~. of Me to a test of Mk ~ (b. Thus, we can decide MP L-~/) by checking M [=-q5 for all models M E {M1, .., Mn}.",157,158
9998,18621877,"In doing that, we introduce a don't care symbol representing any possible word.",10,11
9999,11358044,"T is a set of dependency rules of the form X(Y1 Y2 ... Yi-1 # Yi+l ... Ym), where XGC, Y1GC, .... Ym@C, and # is a special symbol that does not belong to C. (see fig.",33,34
10000,11358044,3 ); 4) the root is a unique symbol as such that As: as E L and As~S. The condition of projectivity limits the expressive power of the formalism to be equivalent to the context-free power.,10,11
10001,11358044,"3,1 Transition graphs and parse tables A transition graph is a pair (V, E), where V is a set of vertices called states, and E is a set of directed edges labelled with a syntactic category or the symbol #.",43,44
10002,11358044,The expansion of a state s takes into account each symbol Y that ilmnediately follows a dot (Y C C U {#}).,10,11
10003,11358044,"A dotted string of the form .Y'13 is treated as a pair of dotted strings {.Y'13, .[3}, so to allow a number of iterations (one or more Y's follow) or no iteration (the first symbol in [~ follows) in the next step.",42,43
10004,11358044,"The function ""star"" takes into account these cases; the repeat loop accounts for the case when the first symbol of 13 is starred too.",21,22
10005,219309407,"Let G = Σ, N, I, A, S be a tree-adjoining grammar, where Σ, N , I, A, and S are the set of terminal symbols, the set of non-terminal symbols, the set of initial trees, the set of auxiliary tree, and the distinguished non-terminal symbol, respectively.",63,64
10006,219309407,"for each non-terminal symbol α, constants α i : τ −• • • • τ i times −• τ for 1 ≤ i ≤ k, where k is the maximal branching of the interior nodes labelled with α that occur in the initial and auxiliary trees of G. Clearly, the terms of type τ that can be built by means of the above set of constants correspond to trees whose frontier nodes are terminal symbols and whose interior nodes are labelled with non-terminal symbols.",5,6
10007,52013038,"8 Lions are vanishing in Africa, where they have long been a symbol of the continent's wild beauty, power and freedom.",13,14
10008,52009850,It is a common approach to enable an EncDec model to work with OOVs that utilizing a special and universal symbol unk to replace all OOVs.,20,21
10009,52009850,"c Nx , c eos ) where N x is the character number of x and c eos is a special symbol to indicate the end of characters.",21,22
10010,52009850,"HL-Decoder has a vocabulary V d and a corresponding embedding matrix V d ∈ R |V d | * dw , which records all characters of the target language, a connector symbol and some HF words.",34,35
10011,343885,"A skeletal grammar (over FEATS, ATOMS and CATS) is a tuple 8 § E where is a finite set of rules, each of which is an MRS of length (with a designated first element, the head of the rule), and a sequence of length of categories; is a lexicon, which associates with every terminal d (over a fixed finite set e of terminals) a finite set of extended categories ¦ d ¨; E is the start symbol (an extended category).",89,90
10012,343885,"q nor ©¤ ¾ ; it may generate arbitrarily deep derivation trees (containing lists of increasing length) whose frontier consists of only one symbol, and thus there exists no finite-ranged function mapping each FS on such a derivation to a finite set of FSs.",26,27
10013,343885,The feature TEMP represents the number of derivation steps before generating the next È symbol.,14,15
10014,3506962,"A local scattered context grammar 2 is a tuple G = (N, T, P, S ), where • N and T are finite, disjoint sets of nonterminal symbols and terminal symbols, respectively, • S ∈ N is the start symbol, and • P is a finite set of productions of the form (A 1 , . . . ,",47,48
10015,5267255,"The # symbol is required by SGX to identify the semantic, head of ~ chain rule for SlID generation.",2,3
10016,10112182,"Notice that the symbols representing the circumfix in the PR pattern level (i.e., the occurrences of the symbol 'P'), the circumfix symbols in the circumfix level, and the circumfix symbols in the surface level are located in correlating places in the different levels.",19,20
10017,10112182,"In addition to being associated with an alphabet symbol, each arc is also associated with an action on the registers.",8,9
10018,10112182,"The read action, denoted R, allows traversing an arc only if a designated register contains a specific symbol.",19,20
10019,10112182,"The write action, denoted W, allows traversing an arc while writing a specific symbol into a designated register.",15,16
10020,10112182,r Γ is a finite alphabet including the symbol '#' (the registers alphabet).,8,9
10021,10112182,"Given a symbol α ∈ Σ ∪ { } and an FSRA A, we say that a configuration (s, u) produces a configuration (t, v), denoted (s, u) α,A (t, v), iff either one of the following holds: r There exists i, 0 ≤ i ≤ n − 1, and there exists γ ∈ Γ, such that (s, α, R, i, γ, t) ∈ δ and u = v and u i = v i = γ; or r There exists i, 0 ≤ i ≤ n − 1, and there exists γ ∈ Γ, such that (s, α, W, i, γ, t) ∈ δ and for all k, k ∈ {0, 1, ..., n − 1}, such that k = i, u k = v k and v i = γ.",2,3
10022,10112182,"If the register operation is R, then the contents of the registers in the two configurations must be equal, and in particular the contents of the designated register in the two configurations should be the expected symbol (γ).",38,39
10023,10112182,"If the register operation is W, then the contents of the registers in the two configurations is equal except for the designated register, whose contents in the produced configuration should be the expected symbol (γ).",35,36
10024,10112182,"Thus, (s, σ, a 1 , ..., a i , t) ∈ δ where i ≤ k implies that if A is in state s, the input symbol is σ and all the register operations a 1 , ..., a i are executed successfully, then A may enter state t. Definition Given a ∈ Actions Γ n we define a relation over Γ n , denoted u a v for u, v ∈ Γ n .",34,35
10025,10112182,"Given a symbol α ∈ Σ ∪ { } and an FSRA-k A, we say that a configuration (s, u) produces a configuration (t, v), denoted (s, u) α,A (t, v), iff there exist a 1 , . . . ,",2,3
10026,10112182,The first transition in the series deals with the new input symbol and the rest are -transitions.,11,12
10027,10112182,The register alphabet consists of the states of A and the symbol '#'.,11,12
10028,10112182,"In FSRAs, traversing an arc depends not only on the input symbol but also on satisfying the series of register operations.",12,13
10029,10112182,"If the last write operation writes into the register the same symbol that the read operation required, then the write is redundant; leave only the read operation.",11,12
10030,10112182,"In FSRAs, in contrast, a functional transition relation does not guarantee linear recognition time, since multiple possible transitions can exist for a given state and a given input symbol.",31,32
10031,10112182,"However, they do imply that for the state q and for the same input symbol (σ), more than one possible arc can be traversed.",15,16
10032,10112182,"Generally, a FSRA is linearized if it is optimized, -free, and given a current state and a new input symbol, and at most one transition can be traversed.",22,23
10033,10112182,Traverse the arc only if the scanned symbol is the content of !,7,8
10034,10112182,"Then, when suffixing the lexicon base words, the variable V uses the the content of register 1 to determine which of the symbols 'i', 'u' should be scanned and allows traversing the arc only if the correct symbol is scanned.",44,45
10035,10112182,"The extended model, FSRA*, is obtained from the FSRA-1 model by adding a new symbol, '*', assumed not to belong to Σ, and by forcing Γ to be equal to Σ. The '*' indicates equality between the input symbol and the designated register content, eliminating the need to duplicate paths for different symbols.",17,18
10036,10112182,"The extended model, FSRA*, is obtained from the FSRA-1 model by adding a new symbol, '*', assumed not to belong to Σ, and by forcing Γ to be equal to Σ. The '*' indicates equality between the input symbol and the designated register content, eliminating the need to duplicate paths for different symbols.",48,49
10037,10112182," the FSRA of Figure 8 , the definite Arabic article al is not scanned as one symbol but as two separate symbols.",17,18
10038,10112182,The extension is done by adding to each transition an output symbol.,11,12
10039,10112182,"r (s, σ, R, i, * , t) ∈ δ and (s, * , R, i, σ, t) ∈ δ for σ = imply that if the automaton is in state s, the input symbol is σ and the content of the i-th register is the same σ, then the automaton may enter state t. r (s, σ, W, i, * , t) ∈ δ and (s, * , W, i, σ, t) ∈ δ for σ = imply that if the automaton is in state s and the input symbol is σ, then the content of the i-th register is changed to σ, and the automaton may enter state t. r (s, * , R, i, * , t) ∈ δ implies that if the automaton is in state s, the input symbol is some σ ∈ Σ and the content of the i-th register is the same σ, then the automaton may enter state t. r (s, * , W, i, * , t) ∈ δ implies that if the automaton is in state s and the input symbol is some σ ∈ Σ, then the content of the i-th register is changed to the same σ, and the automaton may enter state t. With this extended model we can construct an efficient registered automaton for L n : The number of registers is n+1.",47,48
10040,10112182,"r (s, σ, R, i, * , t) ∈ δ and (s, * , R, i, σ, t) ∈ δ for σ = imply that if the automaton is in state s, the input symbol is σ and the content of the i-th register is the same σ, then the automaton may enter state t. r (s, σ, W, i, * , t) ∈ δ and (s, * , W, i, σ, t) ∈ δ for σ = imply that if the automaton is in state s and the input symbol is σ, then the content of the i-th register is changed to σ, and the automaton may enter state t. r (s, * , R, i, * , t) ∈ δ implies that if the automaton is in state s, the input symbol is some σ ∈ Σ and the content of the i-th register is the same σ, then the automaton may enter state t. r (s, * , W, i, * , t) ∈ δ implies that if the automaton is in state s and the input symbol is some σ ∈ Σ, then the content of the i-th register is changed to the same σ, and the automaton may enter state t. With this extended model we can construct an efficient registered automaton for L n : The number of registers is n+1.",118,119
10041,10112182,"r (s, σ, R, i, * , t) ∈ δ and (s, * , R, i, σ, t) ∈ δ for σ = imply that if the automaton is in state s, the input symbol is σ and the content of the i-th register is the same σ, then the automaton may enter state t. r (s, σ, W, i, * , t) ∈ δ and (s, * , W, i, σ, t) ∈ δ for σ = imply that if the automaton is in state s and the input symbol is σ, then the content of the i-th register is changed to σ, and the automaton may enter state t. r (s, * , R, i, * , t) ∈ δ implies that if the automaton is in state s, the input symbol is some σ ∈ Σ and the content of the i-th register is the same σ, then the automaton may enter state t. r (s, * , W, i, * , t) ∈ δ implies that if the automaton is in state s and the input symbol is some σ ∈ Σ, then the content of the i-th register is changed to the same σ, and the automaton may enter state t. With this extended model we can construct an efficient registered automaton for L n : The number of registers is n+1.",171,172
10042,10112182,"r (s, σ, R, i, * , t) ∈ δ and (s, * , R, i, σ, t) ∈ δ for σ = imply that if the automaton is in state s, the input symbol is σ and the content of the i-th register is the same σ, then the automaton may enter state t. r (s, σ, W, i, * , t) ∈ δ and (s, * , W, i, σ, t) ∈ δ for σ = imply that if the automaton is in state s and the input symbol is σ, then the content of the i-th register is changed to σ, and the automaton may enter state t. r (s, * , R, i, * , t) ∈ δ implies that if the automaton is in state s, the input symbol is some σ ∈ Σ and the content of the i-th register is the same σ, then the automaton may enter state t. r (s, * , W, i, * , t) ∈ δ implies that if the automaton is in state s and the input symbol is some σ ∈ Σ, then the content of the i-th register is changed to the same σ, and the automaton may enter state t. With this extended model we can construct an efficient registered automaton for L n : The number of registers is n+1.",226,227
10043,52009569,"The ""[]"" symbol denotes the concatenation operation.",5,6
10044,1188956,"A lexical dependency grammar (LDG) is a quadruplet ¡ £¢ ¥¤ §¦ ¨¤ © ¤ , where ¢ is the set of terminal symbols, ¦ is the set of non-terminal symbols, ¦ is the start symbol, and © is the set of productions.",44,45
10045,1188956,The terminal symbol % is called the head of the production.,2,3
10046,1188956,"Function TGb (¤ § d Inputs : is a dependency tree, is a non-terminal symbol.",18,19
10047,1188956,"Kanazawa (Kanazawa, 1998) studies inference of several classes of categorial grammars from functor structures, based on counting the number of categories associated to a terminal symbol.",29,30
10048,869387,"4 This is shown in Figure 2 (the subscripts indicate the relation between traces and moved elements within elementary trees, while the superscripts, as before, indicate which clause a terminal symbol belongs to).",34,35
10049,232453,and a start symbol A s that is a TFS.,3,4
10050,232453,"Also, notice that the goal is to deduce a TFS which is subsumed by the start symbol, and when TFSs can be cyclic, there can be infinitely many such TFSs (and, hence, goals) -see Wintner and Francez (1999) .",17,18
10051,232453,"Note also that the start symbol A ' does not play a role in this definition; this is equivalent to assuming that the start symbol is always the most general TFS, _k. The most natural observable for a grammar would be its language, either as a set of strings or augmented by TFSs.",5,6
10052,232453,"Note also that the start symbol A ' does not play a role in this definition; this is equivalent to assuming that the start symbol is always the most general TFS, _k. The most natural observable for a grammar would be its language, either as a set of strings or augmented by TFSs.",25,26
10053,3111523,"Preliminary notions A CFG is a four-tuple G cf = V N , V t , R cf , S where V t is a set of terminals, V N is a set of non-terminals, including the start symbol S, and R cf is a set of productions, assumed to be in a normal form where each rule has either (zero or more) non-terminals or a single terminal in its body, and where the start symbol never occurs in the right hand side of rules.",44,45
10054,3111523,"Preliminary notions A CFG is a four-tuple G cf = V N , V t , R cf , S where V t is a set of terminals, V N is a set of non-terminals, including the start symbol S, and R cf is a set of productions, assumed to be in a normal form where each rule has either (zero or more) non-terminals or a single terminal in its body, and where the start symbol never occurs in the right hand side of rules.",88,89
10055,3111523,"l n ], where A is a nonterminal, each l i is a stack symbol, and l 1 is the top of the stack.",16,17
10056,3111523,"A unification grammar is a tuple G u = R u , A s , L where R u is a finite set of rules, each of which is an MRS of length n ≥ 1, L is a lexicon, which associates with every word w ∈ WORDS a finite set of feature structures, L(w), and A s is a feature structure, the start symbol.",71,72
10057,3111523,"When a feature structure which is represented as an unbounded list (a list that is not terminated by elist) is unifiable with an image of a LIG symbol, the former is a prefix of the latter.",29,30
10058,3111523,"To simulate LIGs with UGs we represent each symbol in the LIG as a feature structure, encoding the stack of LIG non-terminals as lists.",8,9
10059,3111523,"Rule application In UG a rule is applied by unification in context of the rule and a sentential form, both of which are MRSs, whereas in LIG, the head of a rule and the selected element of a sentential form must have the same non-terminal symbol and consistent stacks.",50,51
10060,3111523,"The first step is to have exactly one non-terminal symbol (in addition to the start symbol); when all non-terminal symbols are identical, only the content of the stack has to be taken into account.",11,12
10061,3111523,"The first step is to have exactly one non-terminal symbol (in addition to the start symbol); when all non-terminal symbols are identical, only the content of the stack has to be taken into account.",18,19
10062,3111523,"There are exactly two non-terminal symbols, S (the start symbol) and N .",13,14
10063,3111523,"The start rule only applies once in a derivation, simulating the situation in UGs of a rule whose head is unifiable with the start symbol.",25,26
10064,8968727,"In (Foret and Nir, 2002a) it was shown that rigid grammars (grammars that assign only one type to any particular symbol) in L are not learnable from strings.",24,25
10065,8968727,"We prove that for any it e 11+, L(G,"") g lks, a, C i+l)perm 0 < n } : For a string a to be included in a language generated by an LP grammar G, G must assign a type T31 to a symbol in a that has s as range subtype.",51,52
10066,8968727,"For any G, assigns such a type only to the symbol s. Furthermore, s occurs only once, as range subtype, in this type.",11,12
10067,8968727,"This tree can begin with a sequence of applications of the ass and comm rules (which only makes sense if a is not a single symbol), there are some possibilities after this: (a) since G"",n > 1 assigns this type to c, a c, (b) use of [\/1 1 .",26,27
10068,8968727,"Since Tri has only one other domain subtype TM, = a"" • a"" ...a every n times sentence in L(CT ) must contain at least one symbol to which Gn assigns a type with a as range subtype, the only symbols that qualify are a and c. Given that there are no range subtypes TD,7 to be found in an , Treeb must be of the form6 Tree,, i Tree,, 7,, iHa 7,1- [•I] Tree' 71 1-a"" T2 0 . . .",30,31
10069,8968727,"This case corresponds with all trees Treel...Treen being of the form TreeLift where the hypothesis hypo is cancelled (together with n -1 other hypotheses) lower in the tree by n times application of [•/] where the last application has argument a H a•a a. ti times Since a"" = a/(a \a) (the case a' = (a/a) \a can be dealt with in similar fashion), any Treei is either of the form [ro H a\a] 1 ass, comm, [.E] H a/(a\a) which given the type-assignments in Gn>1 can only be a (non-normal form) variant of TreeLift, or symbol H al (a\a) which, given tile type-assignments in G"">1, is only compatible with the derivation TreeCElim.",126,127
10070,8968727,"We show that L(G+ ) C (s, a, c+)Perm: For a string a to be included in a language generated by an LP grammar G, G must assign a type T+ to a symbol in a that has s as subtype.",38,39
10071,8968727,"Grammar G+ assigns such a type only to the symbol s. Furthermore, 8 occurs only once, as range subtype, in this type.",9,10
10072,8968727,"Since T+ has only two domain subtypes TMp = a and TMF = cic, every sentence in L(G ± ) must contain at least one symbol to which G+ assigns a type with a as range subtype, the only symbol that qualifies is a. Thus all derivations for a string in this language must start Tree+ sH (sla)I(elc) a' I-ale s [1E] (a') H 8Ia a H a [1E1 (s 0 (al) 0 a H 8 with ass, comm,[4•E] a"" a s o-"" I- where a' o a is some permutation of a"" +a"" (a"" and 0-""' may be empty).",26,27
10073,8968727,"Since T+ has only two domain subtypes TMp = a and TMF = cic, every sentence in L(G ± ) must contain at least one symbol to which G+ assigns a type with a as range subtype, the only symbol that qualifies is a. Thus all derivations for a string in this language must start Tree+ sH (sla)I(elc) a' I-ale s [1E] (a') H 8Ia a H a [1E1 (s 0 (al) 0 a H 8 with ass, comm,[4•E] a"" a s o-"" I- where a' o a is some permutation of a"" +a"" (a"" and 0-""' may be empty).",41,42
10074,746639,"w $A} maximum one containment A B x A By concatenation Aˆn n-ary concatenation Aˆ| n k} n to k concatenations of A Aˆ n more than n concatenations of A Aˆ n less than n concatenations of A A x B A x B crossproduct A o B A o B composition A Aˆoptionality a : b a : b symbol pair [ ] ( ) order control R P Q | R domain Q ¤ domain R u o Q} upper-side priority union R p Q | R Q o range Q ¤ range R u n} lower-side priority union R Q u Q domain R ¤ domain Q u o R upper-side minus R Q l Q R o range R ¤ range Q u lower-side minus A B $x B Ay A before B A B $x A By A after B A r reverse A reverse R u or R 1 domain R upper language of the regular relation R R l or R 2 range R lower language of the regular relation R R i invert R or inverse R regular relation inverse A.3 Restriction XFST restriction rules are not provided by FSA, nor did we implement them.",66,67
10075,746639,"A.6 Boundary symbol for restriction and replacement In the restriction, £ , and conditional replacement, V G V V G p V G V p @ p @ @ @ expressions we can use a special boundary marker, # , to refer to the beginning or to the end of a string.",2,3
10076,746639,Construction: We do not deal with all the cases where the boundary symbol # can be used.,13,14
10077,52010243,"For example, the decoder outputs ""VP →S→ </L>"" for the span [4, 4] and ""NP→ </L>"" for the span [0,2] in Figure 1c where </L> is a stopping symbol.",44,45
10078,52010243,"In particular, we append a starting symbol <s> and an ending symbol </s> to the left-to-right LSTM and the right-to-left LSTM, respectively.",7,8
10079,52010243,"In particular, we append a starting symbol <s> and an ending symbol </s> to the left-to-right LSTM and the right-to-left LSTM, respectively.",14,15
10080,51681963,"Still, this labeling essentially associates each symbol (usually a word) in a sequence (usually a sentence) with a label.",7,8
10081,51681963,"Our approach to reduce string transduction to sequence labeling relies on a simple observation: in most cases in transduction, it is easier to delete a symbol than to insert a symbol.",27,28
10082,51681963,"Our approach to reduce string transduction to sequence labeling relies on a simple observation: in most cases in transduction, it is easier to delete a symbol than to insert a symbol.",32,33
10083,51681963,"This is true because insertion requires identifying both the point of insertion and the character that needs to be inserted, while deletion is a ""binary"" decision -either the decoding algorithm chooses to delete a specific symbol or not to.",38,39
10084,51681963,"The prediction problem would be much simpler if the insertion positions were in place, because the model would only need to decide which symbol goes in each position.",24,25
10085,51681963,It is an easier task to delete a symbol than to insert one for string transduction.,8,9
10086,51681963,"and therefore the string we would try to transduce to said would be the string sayε, where ε is a special symbol marking potential insertion.",22,23
10087,51681963,"Learning the Labeling Model with σ Once we have the σ function applied to all aligned training data, we mark in the output sequences in the training data the spurious insertions that should not be in the input sequence with a special symbol D (for deletion).",43,44
10088,51681963,"For example, when we experimented with a constant context function that adds an additional ε symbol after every character during training and decoding, the accuracy on the test set was much lower, whether we used a dictionary or not.",16,17
10089,5754213,are wildcards and match any variable or symbol in the pre-SPL): 1.,7,8
10090,16539643,Any Supertag becomes an individual state and any terminal an individual output symbol.,12,13
10091,3259278,is the default symbol.,3,4
10092,16293576,"T 1 A 1 ↓ 1 T 2 A 2 ↓ 2 T k A k ↓ k U k ↓ k+1 β k, syn : S A 1 ↓ 1 T 2 A 2 ↓ 2 T k A k ↓ k U k ↓ k+1 σ k, syn : U k V k T 1 ↓ γ k, syn : U k τ k, syn : A i ↑S a i α i, syn : t e 1 ( e t ) e 2 ( e k−1 t ) e k ( e k t ) k+1 β k, sem : t e 1 ( e t ) e 2 ( e k−1 t ) e k ( e k t ) k+1 σ k, sem : ( e k t ) ( t ( e k t )) t γ k, sem : ( e k t ) τ k, sem : (( e t ) t ) is the start symbol of G k .",175,176
10093,193506,"To demonstrate the usefulness of finite-state automata in natural language applications, some operations on automata are directly defined, includ- Note that w is deterministic: for any state and alphabet symbol there is at most one possible transition.",34,35
10094,15652582,"An ELLIG (respectively, ERLIG) is an LIG, ¨, such that for each nonterminating production © , the distinguished symbol on the righthand side (rhs) is the leftmost (respectively, rightmost) nonterminal, i.e., © is of the form ¡ ¢ ! "" #",23,24
10095,15652582,"If, in addition, for each such nonterminating rule © , no terminal symbol appears to the left (respectively, right) of the distinguished nonterminal symbol of the rhs, i.e., if is always the empty string, then ¨is simply referred to as an LLIG (respectively, RLIG).",14,15
10096,15652582,"If, in addition, for each such nonterminating rule © , no terminal symbol appears to the left (respectively, right) of the distinguished nonterminal symbol of the rhs, i.e., if is always the empty string, then ¨is simply referred to as an LLIG (respectively, RLIG).",28,29
10097,15652582,"18 A nonterminal in our weakly equivalent TAG, W ¡ , is either the start symbol, ¢ , or a pair y ¤£ with y being a basic category from W , and with There is a single initial tree (cf.",16,17
10098,837464,We use a specific symbol to mark out-of-vocabulary words (OOVs).,4,5
10099,11280500,We limit language models to a fixed vocabulary and map out-of-vocabulary (OOV) tokens to a unique symbol to better control the OOV rates among various corpora.,22,23
10100,11338792,Exactly which symbol is emitted is determined by a probability distribution that is specific to each state.,2,3
10101,11338792,"B = {bj(k)) , the observation symbol probability distribution.",9,10
10102,11338792,"The probability bj(k) is the probability that the k-th output symbol will be emitted when the model is in state j. For part-of-speech tagging, this is the probability that the word Wk will be emitted when the system is at tag tj (i.e., P(wkltj)).",13,14
10103,11338792,"The A matrix contains state transition probabilities, the B matrix contains output symbol distributions, and the C matrix contains unknown word distributions.",13,14
10104,219304081,"Assign types q '"" ¦"" q "" , q "" ¦"" ¢¨ "" and q $ I"" q $ , q $ I"" ¢¨ "" to symbol v .",31,32
10105,219304081,Parsing such a sentence will yield a solution: one can collect the assignments to the symbol node used in the derivation to obtain a minimum node cover.,16,17
10106,219304081,"Assign types q '"" ¦"" q "" and q $ $"" q $ to symbol e .",17,18
10107,196203177,"A Non-Deterministic Weighted Automaton (WA) with k states is defined as a tuple: A = α 0 , α ∞ , {A σ } σ∈Σ with: α 0 , α ∞ ∈ R k are the initial and final weight vectors; and A σ ∈ R k×k are the transition matrices associated to each symbol σ ∈ Σ. The function f A : Σ → R realized by an WA A is defined as: f A (x) = α 0 A x 1 • • • A xn α ∞ . (",62,63
10108,196203177,"Given a function f computing substring expectations, the interpolation is: g(x 1:n , σ) = exp    n−1 j=0 w σ,j log f (x n−j:n • σ)    (3) where x 1:n is a context of size n, σ is the output symbol, and w σ,j are the interpolation weights, with one parameter per output symbol σ and context length j, with 0 ≤ j < n. As it is standard with interpolation models, we train the weights by maximizing the conditional log-likelihood of the development set.",62,63
10109,196203177,"Given a function f computing substring expectations, the interpolation is: g(x 1:n , σ) = exp    n−1 j=0 w σ,j log f (x n−j:n • σ)    (3) where x 1:n is a context of size n, σ is the output symbol, and w σ,j are the interpolation weights, with one parameter per output symbol σ and context length j, with 0 ≤ j < n. As it is standard with interpolation models, we train the weights by maximizing the conditional log-likelihood of the development set.",79,80
10110,196203177,"Following the standard, the goal is to learn a language model that predicts the next symbol given a sentence prefix, including the prediction of sentence ends.",16,17
10111,11178029,"Given a CFG, the left corner of a non-terminal symbol Q is the terminal or non-terminal symbol R if and only if there exists a production Q TS UR HV in the grammar, where V is a sequence of symbols.",12,13
10112,11178029,"Given a CFG, the left corner of a non-terminal symbol Q is the terminal or non-terminal symbol R if and only if there exists a production Q TS UR HV in the grammar, where V is a sequence of symbols.",21,22
10113,11178029,We use a dummy symbol f u t for denoting adjoining constraints.,4,5
10114,11178029,9 A@ CB 2 2 S E TE U VD WE G IH 2 2 5P B Q D WE G IH The recognition process starts by predicting every initial tree: 2 & X( Y0 ¥ 5 q CS ¢ y s ( 9 i( 9 `¦ )@ ( @ E x u s Scanner deduction steps can be applied when the recognition reaches a node c whose label is the empty string or a terminal symbol which matches the current symbol in the input string: 2 4 a6 ¥ 5 m S ¡ b¢ c V ©( 8 g( ¤ e( ( hE ( 5 label% c 3 ( ¥¤ D( ¥¤ dc e¦ label% Fc 3 Y¦ E 5 m S ¡ c ¢ V 7( 98 g( ¤ dc e¦ label% Fc 3 Y¦ ( V¨( hE where ¦ ¤ 5¦ denotes the length of the ¤ word.,81,82
10115,11178029,9 A@ CB 2 2 S E TE U VD WE G IH 2 2 5P B Q D WE G IH The recognition process starts by predicting every initial tree: 2 & X( Y0 ¥ 5 q CS ¢ y s ( 9 i( 9 `¦ )@ ( @ E x u s Scanner deduction steps can be applied when the recognition reaches a node c whose label is the empty string or a terminal symbol which matches the current symbol in the input string: 2 4 a6 ¥ 5 m S ¡ b¢ c V ©( 8 g( ¤ e( ( hE ( 5 label% c 3 ( ¥¤ D( ¥¤ dc e¦ label% Fc 3 Y¦ E 5 m S ¡ c ¢ V 7( 98 g( ¤ dc e¦ label% Fc 3 Y¦ ( V¨( hE where ¦ ¤ 5¦ denotes the length of the ¤ word.,86,87
10116,11178029,"Informally, left corner relation for TAGs goes down on nodes of elementary trees starting on a node labeled with a non-terminal symbol and ending on an adjunction node, i.e, nodes where an adjunction can be performed.",24,25
10117,11178029,"When there not exists such adjunction node, the left corner relation can also end in a w node or a node whose label is a terminal symbol or the empty word X .",27,28
10118,11178029,"¢ In the case that ¤ ¦ and the left-most daughter of is labeled with a terminal symbol or X , we can go down on the tree directly to that node: 2 58 !",19,20
10119,11178029,"9 A@ CB D D ¥ 5 m S ¡ b¢ V 7( 98 g( ¤ e( ( hE ( 5 label% c 3 ( ¥¤ e( ¤ dc e¦ label% Fc 3 Y¦ E 5 S nc ¢ f ( ¤ e( ¤ c e¦ label% c 3 1¦ '( @ ( @ E ¢ In the case that ¤ ¦ and is a node labeled with a non terminal symbol whose left-most daughter is an adjunction node, we will stop at that node: 2 8 !",82,83
10120,11178029,"¢ When q ¤ ¦ and the left-most daughter of is a node c labeled with a terminal symbol or X , we will apply: 2 'P B RQ 8 !",20,21
10121,11178029,These new deduction steps filter predictions on nodes belonging to the auxiliary tree and to the elementary tree x where the adjunction is performed: ¢ When ¤ ¦ and the left-most daughter of is a node c labeled with a terminal symbol or X we will apply: 2 5S E TE U 8 !,44,45
10122,2184377,"A grammar is defined over a concrete type signature and is a structure including a set of rules (each constructed from a series of TFSs), a lexicon mapping words to sets of TFSs and a start symbol which is a TFS.",39,40
10123,8813112,When dealing with partial fu nctions the symbol 'f ( x) 1' means that f is defined for the value x and the symbol 'j' means undefinedness.,7,8
10124,8813112,When dealing with partial fu nctions the symbol 'f ( x) 1' means that f is defined for the value x and the symbol 'j' means undefinedness.,26,27
10125,8813112,"Definition 4.3 (Grammars} A grammar G = (R, A s ) is a finite set of rules n and a start symbol A s that is a TFS.",25,26
10126,8813112,"w = W t •• •W n E Figure 3 shows a sequence of derivations, starting from some feature structure that is more specific than the initial symbol and ending in a sequence of structures that can stand for the string ""John loves fish"" , based upon the example grammar.",28,29
10127,8813112,"p re di c t; li ex contains three items: the pre-terminals corresponding to ""John"" , ""loves"" and ""fish"" with the dot set to 0 in entries (0, 1), (  {11) This complete edge can now be used with edge 10 to form, in S 3 , the following edge in (0, 3): HEAD • tij {12) and since the head of this complete edge, which spans the entire input string, is more specific than the initial symbol, the string ""John loves fish"" is accepted by the parser.",102,103
10128,15080006,"4 For this reason, we use the following alternative control algorithm in this paper, which is designed to reject the input as soon as the next input symbol no longer immediately contributes to deriving new facts.",29,30
10129,15080006,"In the Datalog program representing an MCFG, occurrences of variables in the body of a rule come in pairs, with each pair corresponding to an occurrence of a symbol (terminal or string variable) in the head of the corresponding MCFG rule.",30,31
10130,15080006,We will rewrite the Datalog program in such a way that the modified program satisfies the following property: • The order of (the first occurrences of) the pairs of variables in the body of a rule correspond to the order of the corresponding symbol occurrences in the MCFG rule.,46,47
10131,15080006,"For each such node M, the 2-MCFG has a distinct nonterminal symbol M, whose arity is either 2 or 1 depending on whether the node M dominates the foot node or not.",14,15
10132,1565364,"For our formalism, this means that the regular expression in a production is such that each string in its denoted language contains at least one terminal symbol.",27,28
10133,1565364,"The right-hand side symbols represent the active valency of the lexical head of the rule, i.e., the categories that must or may (depending on whether the symbol is optional) be dependents on this lexical head for it to be the root of a well-formed subtree.",31,32
10134,1565364,"Completion: If ¡ 1¤ 2¦ ¨contains either the input symbol or an item © such that is a final state of , and is a -FSM, then add to ¡ ¥¤ §¦ ¨all © 3 54 § 64 7 such that 84 is a rule-FSM which transitions from a start state to state 4 on input or .",11,12
10135,1565364,"is in ¡ ¤ 2¦ C , and ¡ C ED 9 ¦ ¨contains either the input symbol or the item © @F G !",18,19
10136,10201642,"We convert the input string into the regular set of binary trees whose yield equals the input string (using c as the sole symbol of rank 2), and turn the grammar into a tree grammar, replacing all instances of string concatenation in the grammar with the tree operation t 1 , t 2 → c(t 1 , t 2 ).",24,25
10137,52011042,"where EOS is an additional symbol indicating the end of the predicted sequence, L is the length of the input sequence, cycle(•) is the cycle detected function which returns True if there exists a cycle on the input path, nproj(•) is an indicator of whether the resulting tree is projective.",5,6
10138,705,LTAG An LTAG is a set of trees (elementary lrees) which have at least one terminal symbol on its frontier called the anchor.,18,19
10139,153313159,"y T , one at a time, terminated by a stop symbol.",12,13
10140,16558741,"Here, ""omissible"" is a linguistic pattern modification operator [10] , and the special symbol, ""*"", ill a linguistic pattern, is a CAPIT's pattern definition notation, which means that it matches with any sequence of words.",18,19
10141,52010148,Entity: Which animal serves as a symbol throughout the book?,7,8
10142,52010148,Entity (Animal): Which animal serves as a symbol throughout the book?,9,10
10143,51974493,"Then, we replace the entity tokens in the input question with a special entity symbol e and apply DCNN on it again to get a second representation for the question.",15,16
10144,51974493,We produce two representations for the core relation: one that includes the label of the attached entity and one that includes the entity symbol.,24,25
10145,3714725,"1 ) accordingly: D(T I Tt,T~ ) = cr(Tt,T~) -~ f(T I Tz,T~ ) o'(r/,Tr) -1 q-1 -~ I P(TIT0 + P(TIT~ ) +-2 ~(T,,T~)-~ + i and P(T I T~) P(TIT,) a(Ti) -1 f(T I T~) + P(T) a(Tz) -1 + 1 ~r(Tr) -1 f(T ITs) + P(T) ~(T~) -I + 1 1Or really, P (T i I 1o, 11,..., ln) where lo is a special symbol indicating the beginning o1 the word.",95,96
10146,189897915,"2005) , and that it facilitates mental processing compared to an icon-based symbol system, in that the word-meaning map can be direct (Lupyan and Thompson-Schill, 2012) .",15,16
10147,5171045,"The following is an example of the sort of grammar rule we assume: s:[stype=decl] --> np: [prsn=P,num=N] vp: [vtype=tensed,prsn=P,num=N] The notation is that of augmented phrase structure rules, where nonterminals are complex category expressions having the form of a major category symbol followed by a list of feature constraints.",69,70
10148,5171045,"To extend the formalism to incorporate semantic specifications assigning an LF to each phrase, we augment the nonterminals with an LF specification, separated by the symbol ""/"": s:[stype=decl]/VP_sem --> np:[prsn=P,num=N]/NP_sem vp: [vtype=tensed,prsn=P,num=N, sub=NP_sem] /VP_sem The rule now says that the LF of the sentence is the same as the LF of the verb phrase, and the LF of the noun phrase is unified with the sub (""subject"") feature of the verb phrase.",27,28
10149,224803723,"With this consideration, we identify two types of representation learning objectives: Embed Symbol / Explain Data: Aligns the embedding of symbolic entities and raw entities, grounding the symbol in the raw data, and using the symbol embedding to explain properties of previously unseen raw-entity instances.",31,32
10150,224803723,"With this consideration, we identify two types of representation learning objectives: Embed Symbol / Explain Data: Aligns the embedding of symbolic entities and raw entities, grounding the symbol in the raw data, and using the symbol embedding to explain properties of previously unseen raw-entity instances.",40,41
10151,224803723,"In the context of LGBT issues, we find that statements closest to the liberal symbol are those that support the legalization of samesex marriage, and frame it as a constitutional issue.",15,16
10152,224803723,"On the other hand, the statements closest to the conservative symbol, frame homosexuality and same-sex marriage as a moral or religious issue, and we find statements both supporting and opposing same-sex marriage.",11,12
10153,184488346,"6 We restrict the set of characters to those that we see at least 25 times in the training set, replacing all others with a new symbol , as is common and easily defensible in openvocabulary language modeling (Mielke and Eisner, 2018).",27,28
10154,189762150,"For each sentence, we assume that it is written according to some semantic sketch, which is also denoted by a symbol sequence.",22,23
10155,189762150,"Aspect-Aware Sketch Generation A sketch is a symbol sequence describing the skeleton of a sentence, where each symbol denotes a semantic symbol such as a POS tag or a bi-gram.",9,10
10156,189762150,"Aspect-Aware Sketch Generation A sketch is a symbol sequence describing the skeleton of a sentence, where each symbol denotes a semantic symbol such as a POS tag or a bi-gram.",20,21
10157,189762150,"Aspect-Aware Sketch Generation A sketch is a symbol sequence describing the skeleton of a sentence, where each symbol denotes a semantic symbol such as a POS tag or a bi-gram.",24,25
10158,189762150,"Let h S j,t ∈ R d H S denote a d H S -dimensional hidden vector at time step t for the j-th sketch, which is computed via: h S j,t = GRU(h S j,t−1 , x S j,t ), (8) where x S j,t is further defined as xj,t = vs j,t−1 va j , (9) where v s j,t−1 ∈ R d S denotes the embedding for the previous sketch symbol s j,t−1 , v a j denotes the embedding of the current aspect, and "" "" denotes the element-wise product.",97,98
10159,189762150,"Then we transform it into an intermediate vector with the dimensionality of the vocabulary size: z = tanh(W7[ hY j,t ; vs j,t ] + b3), (13) where v s j,t is the embedding of the sketch symbol s j,t .",49,50
10160,189762150,"However, the length of the sketch is not necessarily equal to that of the generated sentence, since a sketch symbol can correspond to a multiterm phrase.",21,22
10161,189762150,"Once we generate the END symbol, the generation process will be stopped.",5,6
10162,2639682,"4 RA(v,nl,p,n2) = f (,,vl,,,,,I,v,,ce) wl,,,,,1,v,,,u ) (1) f(v,nl ,p,n2) ,~ f ( vPlv ,,~ L ,p,u2) -/(,,vl ..... I,v,-2)+y(, In (1), the symbol f denotes frequency of a parti('ular tuple in the training data.",58,59
10163,58981822,"We compose an alphabet of 105 characters that includes uppercase and lowercase characters, Moldavian and Romanian diacritics (such as ȃ, â, î, s ¸and t ¸), digits, and 33 other symbol characters.",38,39
10164,52011451,"Depending on the context, I will use the terms gappy trigram and extended bigram interchangeably for trigrams where one of the three symbols (the ""gap"") is a wildcard, i.e. can be replaced by any symbol.",40,41
10165,52011451,"To also be able to define information content values for segments at the start and the end of an IPA representation, the word boundary symbol # is used for expanding a string a of length k to the positions a −1 , a −0 , a k+1 , and a k+2 .",25,26
10166,52011451,"Also, no special treatment was needed for the gap symbol, which occurs in correspondences whenever insertions or deletions were inferred in the optimal alignment, and models the costs for inserting or deleting the respective phoneme in the inferred matrices.",10,11
10167,52011451,"The probabilities are not directly based on counts of the number of times each symbol pair was aligned, but each instance in a candidate cognate pair only counts with its combined information content.",14,15
10168,989998,"attempt to connect sentiment labels with ratings by Kronecker symbol, but this method only applies to three sentiment polarities: −1(negative), 0(neutral), +1(positive), and it does not explore the word-level lexicon, which is also an important source of knowledge.",9,10
10169,5911629,N is a total function that assigns every symbol 2 ˙a (positive) rank.,8,9
10170,5911629,Definition 3 A coupled context-free grammar is a tuple G D .N; T; P; S/ where: N is a ranked alphabet of non-terminal symbols; T is an unranked alphabet of terminal symbols; P is a ranked rewriting system over ESD.N; T /; S 2 N OE1 is a start symbol.,61,62
10171,5911629,"The dependency view on CCFG A CCFG G is strongly lexicalized, if each production p contains exactly one terminal symbol, written as anchor.p/. Just as in the case of TAG, a strongly lexicalized CCFG G can be interpreted as a dependency grammar: Let .T ] ; T [ / be a derivation in G. Since G is strongly lexicalized, there is a one-to-one mapping between the nodes of the derivation tree T ] (labelled with productions) and the leaves of the derived tree T [ (labelled with terminals); we refer to this mapping by the name f L .",20,21
10172,9018481,"An SMCFG is a 5tuple G = (N, T, F, P, S) where N is a finite set of nonterminals, T is a finite set of terminals, F is a finite set of functions, P is a finite set of (production) rules and S ∈ N is the start symbol.",60,61
10173,9018481,"The type of W v is denoted by type(v) and we prede- fine type(1) = S, that is, W 1 is the start symbol.",27,28
10174,9018481,"Also, let êv (a) and êv (a, b) be re-estimated emission probabilities that W v emits a symbol a and two symbols a and b respectively.",25,26
10175,5290544,"∨ (v(w 1,n−1 ) ∧ v(w n,n )) where v(x) is the variable corresponding to the congruence class [x] and w i,j is the substring of w from the i th to the j th symbol of w. This statement is representing the fact that if a congruence class [w] is chosen as a non-terminal then for each string in w ∈ [w], there must be at least one CNF rule A → BC that generates w and thus there must be at least one division of w into w 1,k w k+1,n such that B corresponds to [w 1,k ] and C corresponds to [w k+1,n ].",44,45
10176,1949491,"Word translation probabilities are placed onto arcs emitting the word as an output symbol (in the figure, note the arcs emitting ""committee"", ""the"", etc.).",13,14
10177,2814545,"An alternative often used is to introduce a special symbol (♯) and to only consider the strings terminating with ♯: the distribution is then over Σ ⋆ ♯. Equivalence results between HMMs and PFA can be found in (Vidal et al.,",9,10
10178,2814545,Each transition in a PFST has attached a symbol from the source alphabet (or λ) and a string (possible empty string) of symbols from the target alphabet.,8,9
10179,17325482,"Identifying 'Phrase-like' chunks Having detected a signal which satisfies criteria indicating language-like structures at a physical level (Elliott and Atwell, 2000; Elliott and Atwell, 1999) , second stage analysis is required to begin the process of identifying internal grammatical components, which constitute the basic building blocks of the symbol system.",60,61
10180,9399737,"R g refers to the glue rules that rewrite the start symbol S: S → <X, X> (1) S → <SX, SX> (2) R is the set of finite production rules in G and has two types, viz.",11,12
10181,2125749,"2007) works as follows: firstly, train an n-gram language model based on the translation N-best list or translation forest; secondly, expand each partial hypothesis by appending a word via overlapped (n-1)-grams until the partial hypothesis reaches the sentence ending symbol.",49,50
10182,6844025,"That is, the special symbol may be a separator or content of cells.",5,6
10183,15371205,"For example, Figure 1 shows a dependency tree for the sentence, Economic news had little effect on financial markets, with the sentence's root-symbol as its root.",28,29
10184,12366433,"The NULL symbol is treated quite differently under and , and this leads to a large mismatch between the MBR-GAE decoders and the AER metric.",2,3
10185,710628,X I ∈ N is the start-symbol of G. • R is a finite set of production rules.,8,9
10186,710628,We will call every subsequence parsed to a grammar symbol a phrase.,9,10
10187,710628,"We name this formula the semantic value of the grammar symbol and denote it as [[α] ] i,j .",10,11
10188,710628,"Semantics for a terminal symbol α i,j is a one-argument predicate whose name is α and whose argument is variable a α,i,j . ) [[",4,5
10189,710628,"We represent the semantic value of a symbol generated by an accumulation rule as a graph, whose vertices are constants that are arguments of the predicate.",7,8
10190,710628,Upon the end of the parsing process we obtain an edge labelled by the start symbol of the grammar.,15,16
10191,710628,"There are three types of rules that occur during the corpus processing: • Rules that make up the lexicon Name ::= ur mes Name(varName, varur, varmes) • Rules that describe the document structure var Name is a variable, whose index is an edge labelled by the symbol Name.",53,54
10192,710628,val YearGod is a set of predicate names connected with the symbol YearGod.,11,12
10193,710628,"When a single sign is illegible, it is denoted in the document as x. The parser considers the x symbol as a wildcard that may be matched with any terminal symbol.",20,21
10194,710628,"When a single sign is illegible, it is denoted in the document as x. The parser considers the x symbol as a wildcard that may be matched with any terminal symbol.",31,32
10195,710628,Then we replace the [...] symbol by a sequence of x symbols.,7,8
10196,2082658,"In this table, the symbol ""+"" in the Features column means current configuration contains both the baseline features and new cluster-based features; the number is the total number of the clusters; the symbol ""+"" in the Data column means which portion of the Gigaword data is used to cluster words; the symbol ""S"" and ""SS"" in parentheses denote (s)upervised and (s)emi-(s)upervised word segmentation.",5,6
10197,2082658,"In this table, the symbol ""+"" in the Features column means current configuration contains both the baseline features and new cluster-based features; the number is the total number of the clusters; the symbol ""+"" in the Data column means which portion of the Gigaword data is used to cluster words; the symbol ""S"" and ""SS"" in parentheses denote (s)upervised and (s)emi-(s)upervised word segmentation.",39,40
10198,2082658,"In this table, the symbol ""+"" in the Features column means current configuration contains both the baseline features and new cluster-based features; the number is the total number of the clusters; the symbol ""+"" in the Data column means which portion of the Gigaword data is used to cluster words; the symbol ""S"" and ""SS"" in parentheses denote (s)upervised and (s)emi-(s)upervised word segmentation.",61,62
10199,2925458,"3) The sentences do not contain symbol characters, such as colon, dash etc, which tend to cause parse errors.",7,8
10200,14686530,"Under this model each phrase pair gets some probability distribution P hier ( e, f : θ x , θ t ), where θ x and θ t are the parameters of symbol distribution and phrase table respectively.",34,35
10201,6885919,"The NMT system can decide to place any symbol into a token sequence that would form a compound, even the ones which were never part of a compound in training.",8,9
10202,6885919,"When we segment compounds, we always place an indicator symbol before the initial part of the split compound token sequence, which can be either #L or #U. It specifies the original casing of the compound (lower or upper).",10,11
10203,11857801,"Table 1 gives an overview of the possible bigram combinations, using the three symbol tagset, plus sentence beginning and end markers, and their judgment as good, bad or neutral.",14,15
10204,14695911,"To the left of M is a sequence of one or more left labels L i (c) including the special termination symbol △ and similarly for the labels to the right, R i (c).",23,24
10205,53222571,"The decoder generates one target symbol y i at a time by computing the conditional probability p(y i |y 1 , y 2 , . . . ,",5,6
10206,658796,"The terminal symbol strings are first pre-processed by stripping punctuation and empty categories, which could not be expected from the output of a speech recognizer.",2,3
10207,51877763,"Given a morphologically fully underspecified lemma sequence, the task considered in this work consists of annotating each input symbol with the complete set of morphological features needed to generate the inflected word forms.",19,20
10208,51877763,"A Neural Architecture for Morphological Tagging of Lemmas We consider a tagging task in which, given a lemmatized input sequence x = {x 1 , x 2 , ..., x N }, each input symbol is assigned one out of a set T of predefined fine-grained tags, resulting in the output sequence y = {y 1 , y 2 , ..., y N }, where x and y have the same length N .",38,39
10209,51877763,"Given an input sequence of length N , in the first layer each input symbol is mapped to three one-hot vector representations, corresponding to the three features described above.",14,15
10210,51877763,"The lemma, the capitalization, and the suffix embeddings are then concatenated in order to obtain a single vector representation for each input symbol.",24,25
10211,7784644,"A context-free grammar G = (V, T, S † , P ) consists of: a set of non-terminal symbols V , including a special start symbol S † ; a set of terminal symbols T ; and a set of rule productions P of the form A → α for A ∈ V and α ∈ (V ∪ T ) * , i.e., a single non-terminal on the left-hand side of the rule production, and a sequence of 0 or more terminals or non-terminals on the right-hand side of the rule.",33,34
10212,137930,"A TAG is a tuple G = (N,~,I,A,S), where N, ~ are the finite sets of nonterminal and terminal symbols, respectively, I, A are the finite sets of initial and auxiliary trees, respectively, and S E N is the initial symbol.",53,54
10213,7733948,"If a sentence a is the topic of another sentence/3, this is formalised as a ~ /~.l This symbol also occurs in the graph, indicating a further SDRS condition.",19,20
10214,2466331,"Given a finite set Φ of formulas, or fluents, a symbol consists of a consistent subset of Φ, which non-exhaustively enumerates what holds true at some single point in time.",12,13
10215,2466331,"L & L = k≥1 {(σ1 ∪ σ 1 ) ... (σ k ∪ σ k ) | σ1...σ k ∈ L, σ 1 ... σ k ∈ L } A simple example (by Fernando), deriving the representation for the phrase ""rain from dawn to dusk"", should make clear the general idea ( stands for ∅-as-a-symbol, boxes replacing braces).",71,72
10216,2466331,"23) Λ 1 (V iter in an hour) = a, ¬o, time(m) a, ¬o + ¬a, o, time(n), hour(m,n) (24) Λ 2 (V iter in an hour) = a, ¬o, time(m) ¬a, o + ¬a, o, time(n), hour(m,n) In ( 23 ) the start of the phase of activity is postponed to the very last symbol.",84,85
10217,3137601,"For terminal nodes, a click on the lexical symbol in either of the lower frames will access the cgi interface of the dictionary and deliver the complete lexical information that is available for the respective entry.",9,10
10218,538616,To relax the prohibition it is sufficient to drop the symbol T throughout therules.,10,11
10219,1883764,"Let the dash symbol ""−"" be a gap in the alignment (corresponding to an insertion/deletion).",3,4
10220,321932,"The symbol SEM is used to abstract over specific content such as the set of points delimiting an area or the identifiers of selected objects (Johnston et al.,",1,2
10221,321932,The epsilon symbol (¢ ) indicates that a stream is empty in a given terminal.,2,3
10222,10406219,"A special part of speech, called segmentation word, which corresponds to the beginning or end symbol of clauses is introduced to express a sentence structure.",17,18
10223,6372198,This problem was implemented as an SRN learning taskto predict the symbol following the left context given to the input layer so far.,11,12
10224,6372198,"Words were applied to the network, symbol by symbol, which in turn were encoded orthogonally, that is, one node standing for one symbol (Fig.",7,8
10225,6372198,"Words were applied to the network, symbol by symbol, which in turn were encoded orthogonally, that is, one node standing for one symbol (Fig.",9,10
10226,6372198,"Words were applied to the network, symbol by symbol, which in turn were encoded orthogonally, that is, one node standing for one symbol (Fig.",26,27
10227,6372198,An extra symbol ('#') was used as a delimiter.,2,3
10228,6372198,The second approach computes the local proximity between the network response and a vector containing the empirical symbol probabilities that a given symbol would follow the current left context.,17,18
10229,6372198,The second approach computes the local proximity between the network response and a vector containing the empirical symbol probabilities that a given symbol would follow the current left context.,22,23
10230,6372198,"The root is empty, it does not represent a symbol.",10,11
10231,6372198,"In addition to the symbol at each node, we can keep additional information, for example the frequency of a word, if this node is the end of a word.",4,5
10232,6372198,"Another approach related to the practical implementation of a trained SRN is to search for a cue, giving an answer to the question whether given symbol can follow the context provided to the input layer so far.",26,27
10233,2658285,"Non-terminal symbols are in capitals and denote intermediate states; the terminal symbol α corresponds to all words seen in the training set, and g( f .v) is a function for generating integer numbers given the value of a field f .",14,15
10234,2658285,"All non-terminals, save the start symbol S, have one or more constraints (shown in parentheses), similar to number and gender agreement constraints in augmented syntactic rules.",8,9
10235,2658285,"Rule (1) denotes the expansion from the start symbol S to record R, which has the special start type (hence the notation R(start)).",10,11
10236,6434733,"A dependency grammar is a six-tuple <W, C, S, D, I, H>, where W is a finite set of symbols (words of a natural language); C is a set of syntactic categories (among which the special category E); S is a non-empty set of root categories (C ~ S); D is the set of dependency relations, e.g. SUB J, OBJ, XCOMP, P-OBJ, PRED (among which the special relation VISITOR1); I is a finite set of symbols (among which the special symbol 0), called u-indices; H is a set of dependency rules of the form x:X (<rlYlUl'Cl> ... <ri-1Yi-lUi-l'Ci-1 > # <a'i+lYi+lUi+fl;i+l> ... <rmYmum'~m>) 1) xe W, is the head of the rule; 2) Xe C, is its syntactic category; 3) an element <r i Yi ui xj> is a d-quadruple YeC. Finally, it holds that: I) For each ue I that appears in a u-triple <u, r, Y>~Uj, there exists exactly one d-quad <riYiu:]xi> in the same rule such that u=ui, i ~j.",112,113
10237,6434733,A triple consisting of a word w (a W) or the trace symbol e(~W) and two integers g and v is a word object of the grammar.,14,15
10238,6434733,The dot can advance across a d-quadruple <rjYiui'q> or across the special symbol #.,16,17
10239,6434733,"When an item P contains a dotted rule with the dot at its end and a T-stack with the empty set @ as the top symbol, the parser looks for the items that can advance the dot, .given the completion of the dotted dependency rule m P. Here is the algorithm.",27,28
10240,6434733,"Scanner: When the dot precedes the symbol #, the parser can scan the current input word wi (if y, the head of the item P, is equal to it), or pseudoscan a trace item, respectively.",7,8
10241,6434733,"In this example we neglect the problem of subject-verb agreement: it can be coded by inserting the AGR features in the category label (in a similar way to the +EX feature in the grammar G1); the comments on the right help to follow the events; the separator symbol I helps to keep trace of the sets in the stack; finally, we have left in plain text the d-quad sequence of the dotted rules; the other components of the items appear in boldface. (",55,56
10242,215514124,"To emphasize the arity of a symbol, we will write it as a parenthesized superscript, for instance f (n) for a symbol f of arity n. Analogously, we write F (n) for the set of symbols in F with arity n. Symbols with arity zero (F (0) ) are called NULLARY symbols or CON-STANTS.",6,7
10243,215514124,"To emphasize the arity of a symbol, we will write it as a parenthesized superscript, for instance f (n) for a symbol f of arity n. Analogously, we write F (n) for the set of symbols in F with arity n. Symbols with arity zero (F (0) ) are called NULLARY symbols or CON-STANTS.",25,26
10244,215514124,"A tree in T(F, X n ) will be called a context, typically denoted with the symbol C. For a context C ∈ T(F, X n ) and a sequence of n trees t 1 , . . . ,",18,19
10245,215514124,"Traditional presentations of TAG, which we will assume familiarity with, take the symbols in elementary and derived trees to be unranked; nodes labeled with a given nonterminal symbol may have differing numbers of children. (",30,31
10246,215514124,We will thus take the nodes of TAG trees to be labeled with symbols from a ranked alphabet F; a given symbol then has a fixed arity and a fixed number of children.,22,23
10247,215514124,"We will notate these elements, abusing notation, as e (n) , and make use of a function |•| to unrank symbols in F, so that |e (n) | = e. To handle foot nodes, for each non-nullary symbol e (i) ∈ F (≥1) , we will associate a new nullary symbol e * , which one can take to be the pair of e and * ; the set of such symbols will be notated F * .",47,48
10248,215514124,"We will notate these elements, abusing notation, as e (n) , and make use of a function |•| to unrank symbols in F, so that |e (n) | = e. To handle foot nodes, for each non-nullary symbol e (i) ∈ F (≥1) , we will associate a new nullary symbol e * , which one can take to be the pair of e and * ; the set of such symbols will be notated F * .",64,65
10249,215514124,"Finally, to allow null adjoining constraints, for each f ∈ F (i) , we introduce a symbol f / 0 also of arity i, and take F / 0 to be the set of all such symbols.",20,21
10250,215514124,"We will extend the function |•| to provide the unranked symbol associated with these symbols as well, so |e ↓ | = |e * | = |e (i) / 0 | = e. A TAG is then a quadruple F, S, I, A , where F is a ranked alphabet; S ∈ F is a distinguished initial symbol; I is the set of initial trees, a finite subset of T(F ∪ F / 0 ∪ F ↓ ); and A is the set of auxiliary trees, a finite subset of T(F ∪F / 0 ∪F ↓ ∪F * ).",10,11
10251,215514124,"We will extend the function |•| to provide the unranked symbol associated with these symbols as well, so |e ↓ | = |e * | = |e (i) / 0 | = e. A TAG is then a quadruple F, S, I, A , where F is a ranked alphabet; S ∈ F is a distinguished initial symbol; I is the set of initial trees, a finite subset of T(F ∪ F / 0 ∪ F ↓ ); and A is the set of auxiliary trees, a finite subset of T(F ∪F / 0 ∪F ↓ ∪F * ).",64,65
10252,215514124,"3 Tree Transducers, Homomorphisms, and Automata Tree Transducers Informally, a TREE TRANSDUCER is a function from T(F) to T(G) defined such that the symbol at the root ofthe input tree and a current state determines an output context in which the recursive images of the subtrees are placed.",28,29
10253,215514124,"By virtue of nondeterminism in the equations, multiple equations for a given state q and symbol f , tree transducers define true relations rather than merely functions.",16,17
10254,215514124,"A transducer is LINEAR if all such τ are linear; is COMPLETE if τ contains every variable in X n ; is ε -FREE if τ ∈ X n ; is SYMBOL-TO-SYMBOL if height(τ) = 1; and is a DELABELING if τ is complete, linear, and symbolto-symbol.",57,58
10255,215514124,"We can limit attention to bimorphisms in which the input or output homomorphisms are restricted to a certain type, linear (L), complete (C), epsilonfree (F), symbol-to-symbol (S), delabeling (D), or unrestricted (M).",35,36
10256,215514124,"We can limit attention to bimorphisms in which the input or output homomorphisms are restricted to a certain type, linear (L), complete (C), epsilonfree (F), symbol-to-symbol (S), delabeling (D), or unrestricted (M).",39,40
10257,8250857,"Moreover, let the symbol S be the start symbol of the grammar G. Following the notation of Lari and Young, we denote the inside probability as e(X, i,j), which represents the probability that a nonterminal X :~ wi...wj.",4,5
10258,8250857,"Moreover, let the symbol S be the start symbol of the grammar G. Following the notation of Lari and Young, we denote the inside probability as e(X, i,j), which represents the probability that a nonterminal X :~ wi...wj.",9,10
10259,8250857,"The base case is h(X,i,i) = -e(X, i, i) log2 (e(X, i, i)) since a non-terminal X can generate the symbol wi in exactly one way.",35,36
10260,7144001,"Unless specified differently, we assume that we are given a context-free grammar G N Σ S P with nonterminals N, terminals Σ, start symbol S and set of productions P. For Earley's algorithm we also assume a new start symbol S ¼ which is not in N. Each production is of the form A α with A ¾ N α ¾ ´N Σµ £ .",28,29
10261,7144001,"Unless specified differently, we assume that we are given a context-free grammar G N Σ S P with nonterminals N, terminals Σ, start symbol S and set of productions P. For Earley's algorithm we also assume a new start symbol S ¼ which is not in N. Each production is of the form A α with A ¾ N α ¾ ´N Σµ £ .",45,46
10262,7144001,"1 with the obvious sets of nonterminals and terminals, the start symbol S and productions P. It is left to the reader to calculate example derivations for the three algorithms for a sentence such as John hit the dog with the stick.",12,13
10263,7144001,"The new, propagated edge constraint spans the entire range of the positions of the daughters and is labeled with the (nonterminal) symbol of the LHS of the CF rule.",24,25
10264,3196275,"While the portability of Java was tempting, we eventually decided on Common LISP ((Steele, 1990) ) with its more powerful symbol and list manipulation facilities.",25,26
10265,3159994,b) Phonetic symbol translation is a prerequisite for speech translation. (,3,4
10266,8160425,"2012) that finitely ambiguous tree adjoining grammars cannot be transformed into a normal form (preserving the generated tree language), in which each production contains a lexical symbol.",31,32
10267,8160425,"In first-order substitution we replace leaves (elements of X), whereas in second-order substitution we replace an internal node (labeled by a symbol of Σ).",29,30
10268,8160425,"A (simple) context-free tree grammar [CFTG] is a system (N, Σ, S, P ) such that • N is a ranked alphabet of nonterminal symbols, • Σ is a ranked alphabet of terminal symbols, 7 σ α σ α α ε ← σ σ α x 2 σ x 1 α = σ σ α σ α α σ α α Figure 2 : Example second-order substitution, in which the boxed symbol σ is replaced. •",86,87
10269,8160425,"In particular, it eliminates projection productions A(x 1 ) → x 1 and unit productions, in which the right-hand side has the same shape as the lefthand side (potentially with a different root symbol and a different order of the variables).",38,39
10270,8160425,"In this contribution, we want to (strongly) lexicalize CFTG, which means that for each CFTG G such that G has finite ∆-ambiguity, we want to construct an equivalent CFTG such that each non-initial production contains at least one lexical symbol.",46,47
10271,8160425,"The goal is to make sure that non-initial monic productions (i.e., productions of which the right-hand side contains at most one nonterminal) contain at least one lexical symbol.",34,35
10272,8160425,"For every A ∈ N with A = S, we compute all monic sentential forms without a lexical symbol that are reachable from A(x 1 , . . . ,",19,20
10273,8160425,The basic idea of the construction is that we guess a lexical symbol for each non-∆-lexicalized production.,12,13
10274,8160425,The guessed symbol is put into a new parameter of a nonterminal.,2,3
10275,8160425,"It will be kept in the parameter until we reach a terminal production, where we exchange the same lexical symbol by the parameter.",20,21
10276,8160425,"We let N = N × ∆ be a new set of nonterminals such that rk( A, δ ) = rk(A) + 1 for every A ∈ N and δ ∈ ∆. Intuitively, A, δ represents the nonterminal A, which has the lexical symbol δ in its last (new) parameter.",49,50
10277,8160425,"Finally, we replace each non-initial non-∆-lexicalized production in G by new productions that guess a lexical symbol and add it to the new parameter of the (lexicographically) first nonterminal of N in the right-hand side.",19,20
10278,2607380,"For example, A((g .. h}, (i .. j), (k .. l})--+ B((g+1 .. h), (i+i .. where S is the start symbol.",31,32
10279,2607380,"Each terminal leaf has a single decoration which is its terminal symbol or e. Afterwards, we collect into a string dr the decorations gathered during a top-down left-to-right walk in r. If r is an auxiliary tree, let d~ and d~ be the part of dr gathered before and after the foot of r has been hit.",11,12
10280,2607380,"This is due to the fact that an RCG is a purely syntactic formalism in the sense that it only handles (part of) the source text, exclusive of any other symbol.",33,34
10281,2607380,"An RNRG is a labeled tree rewriting system that consists of a starting tree and a finite set of rewriting rules, A -a, where A is a nonterminal symbol and a is a tree structure, which specifies how a node 11, labeled A, can be rewritten.",30,31
10282,2607380,"Note that if an internal node is labeled by a terminal symbol, this node cannot be rewritten and its labe!",11,12
10283,2607380,The previous three-copy language can be described by an RNRG of rank 2 whose initial tree is and the set of rewrite rules for the node A is where t stands for an anonymous terminal symbol which labels non leaf nodes.,37,38
10284,219301636,"They do so by using wrapping auxiliary trees, which allow nonempty frontier nodes (i.e., leaf nodes whose labels are not the empty terminal symbol) on both sides of the foot node.",26,27
10285,219301636,"The symbol ""-"" denotes that the difference of scores between the models bears no statistical significance.",1,2
10286,59676530,"SHIFT(</>) moves 0 by one token to the right, i.e. SHIFT(a 0 /3) = bi, = a.NEXT(/3) 0 (/3 \ NEXT(/3))} where NEXT(</>) returns a set of all alternatives for the next symbol (Gorn address) .",45,46
10287,3536958,"Linear tree-adjoining grammars (TAGs), by analogy with linear context-free grammars, are treeadjoining grammars in which at most one symbol in each elementary tree can be rewritten (adjoined or substituted at).",26,27
10288,3536958,"Let Z 1 be the child that dominates the foot node; let V 1 be a fresh nonterminal symbol and insert V 1 nodes above Y and below Z 1 , and excise the segment between the two V nodes, leaving behind an active obligatory-adjunction node.",19,20
10289,3536958,"If Y has another child, call it Z 2 ; let V 2 be a fresh nonterminal symbol and insert a V 2 node above Z 2 , and break off the subtree rooted in V 2 , leaving behind a substitution node.",18,19
10290,3536958,"A marked string (as in Ogden's Lemma) over an alphabet Σ is a string over Σ × {0, 1}, where a symbol σ, 1 is marked and a symbol σ, 0 is not.",28,29
10291,3536958,"A marked string (as in Ogden's Lemma) over an alphabet Σ is a string over Σ × {0, 1}, where a symbol σ, 1 is marked and a symbol σ, 0 is not.",36,37
10292,3536958,"Moreover, we may use the symbol w i to refer to the occurrence of the ith member of the decomposition in w; for example, if w is a marked string, we may say that a symbol in w i is marked, or if w is generated by a TAG derivation, we may say that w i is generated by some set of nodes in the derivation tree.",6,7
10293,3536958,"Moreover, we may use the symbol w i to refer to the occurrence of the ith member of the decomposition in w; for example, if w is a marked string, we may say that a symbol in w i is marked, or if w is generated by a TAG derivation, we may say that w i is generated by some set of nodes in the derivation tree.",39,40
10294,3536958,"To show that it holds, we need to find a path P in the derivation tree of z that has a cycle that generates at least one marked symbol.",29,30
10295,9955259,"Since an input tagged corpus consists of leaf categories, all leaf categories must be replaced by corresponding categories that are elements of category set G. We put requirements on G, in order to ensure that all leaf categories have its ancestor category in G. We recursively define a function as follows: 6U,G (x) { x, x E G Su (P(x)), otherwise This equation means that (5 U' G (X) is an ancestor of x and it is an element of G. We require that the generated category set G must satisfy the following two requirements: (1) 6 U'G (X) is defined for all x E VM , (2) P(x) sg' G for all x E G. Environment of a Category We quantify the context of occurrence in a corpus H with a category set G by the terms pH (x; z) and plI R ' u' G (x; z) as follows: PL (X; Z) PH R '"" (X ; Z) fGrl'U (x z) fG ILU (z) II,U f G (Z • X) f G H'u CO • 1 N, NO, PN, RG, NT and SYM stand for noun, number, propernoun, region, nation and symbol, respectively.",248,249
10296,5908935,"Since the number of sorts of bunsetsu is enormous, considering it as a symbol to be predicted would surely invoke the datasparseness problem.",14,15
10297,5908935,"The terminal symbol is the attribute of a bunsetsu, represented by the product of the head of the content part and that of the function part.",2,3
10298,5908935,The simplest way is to select each bunsetsu as a terminal symbol.,11,12
10299,5908935,The attribute sequence of a sentence is generated through applications of these rewriting rules to the start symbol S. Each rewriting rule has a probability and the probability of the attribute sequence is the product of those of the rewriting rules used for its generation.,17,18
10300,5908935,"Solution Search Algorithm The stochastic context-free grammar used for syntactic analysis consists of rewriting rules (see formula (3)) in Chom~ky normal form (Hopcroft and Ullman, 1979) except for the derivation from the start symbol (formula (2)).",44,45
10301,5908935,It should be noted that it is necessary to multiply the probability of the derivation from the start symbol at the end of the process.,18,19
10302,687713,The linked non-terminals in the yield constitute the symbol pairs to which new rules (subtree pairs) are applied.,10,11
10303,2824224,"However for the $ symbol, the Penn treebank used SYM as a POS tag and the SR Task $, but the modification is not listed.",4,5
10304,2239085,"We use the B symbol (Begriff ) to bring out the links to Formal Concept Analysis (Ganter and Wille, 1997; Davey and Priestley, 2002) .",4,5
10305,2239085,"Consider a nonterminal N in a CFG with start symbol S. We can define C(N ) = {(l, r)|S * ⇒ lN r} and the yield Y (N ) = {w|N * ⇒ w}.",9,10
10306,1868,"They do so by using wrapping auxiliary trees, which allow nonempty frontier nodes (i.e., leaf nodes whose labels are not the empty terminal symbol) on both sides of the foot node.",26,27
10307,1868,"The symbol ""-"" denotes that the difference of scores between the models bears no statistical significance.",1,2
10308,18777142,"The aux.iliary symbol is on the root node, as these trees do not have a foot node.",2,3
10309,1877927,"We represent this 'local' order as a string over the alphabet N 0 , where the symbol 0 represents the singleton interval OEu; u, and a symbol i ¤ 0 represents the interval that corresponds to the yield of the i th direct dependent of u. An order-annotated tree is a tree labelled with pairs h ; !",18,19
10310,1877927,"We represent this 'local' order as a string over the alphabet N 0 , where the symbol 0 represents the singleton interval OEu; u, and a symbol i ¤ 0 represents the interval that corresponds to the yield of the i th direct dependent of u. An order-annotated tree is a tree labelled with pairs h ; !",30,31
10311,1877927,A symbol i in !,1,2
10312,1877927,".u/ in a tree t contains all and only the symbols in the collection f0g [ f i j ui 2 N.t / g, i.e., one symbol for u, and one symbol for every direct dependent of u. Property C2: The number of occurrences of a symbol i ¤ 0 in !",29,30
10313,1877927,".u/ in a tree t contains all and only the symbols in the collection f0g [ f i j ui 2 N.t / g, i.e., one symbol for u, and one symbol for every direct dependent of u. Property C2: The number of occurrences of a symbol i ¤ 0 in !",35,36
10314,1877927,".u/ in a tree t contains all and only the symbols in the collection f0g [ f i j ui 2 N.t / g, i.e., one symbol for u, and one symbol for every direct dependent of u. Property C2: The number of occurrences of a symbol i ¤ 0 in !",51,52
10315,1877927,"Formally, a (normalized) regular tree grammar is a construct G D .N G ; ˙G; S G ; P G /, in which N G and ˙G are finite sets of non-terminal and terminal symbols, respectively, S G 2 N G is a dedicated start symbol, and P G is a finite set of productions of the form A !",54,55
10316,1877927,"The tree language generated by G is the set of all terminal trees that can eventually be derived from the trivial tree formed by its start symbol: L.G/ D f t 2 T ˙G j S G ) G t g. Regular dependency grammars We call a dependency language regular, if its encoding as a set of trees over ˙ ˝forms a regular tree language, and write REGD for the class of all regular dependency languages.",26,27
10317,1877927,"Here is an example for a function: f .hx 1 1 ; x 2 1 i; hx 1 2 i/ D hax 1 1 ; x 1 2 x 2 1 i This function states that in order to compute the pair of strings that corresponds to a tree whose root node is labelled with the symbol f , one first has to compute the pair of strings corresponding to the first child of the root node (hx 1 1 ; x 2 1 i) and the single string corresponding to the second child (hx 1 2 i), and then concatenate the individual components in the specified order, preceded by the terminal symbol a. We call a function lexicalized, if it contributes exactly one terminal symbol.",58,59
10318,1877927,"Here is an example for a function: f .hx 1 1 ; x 2 1 i; hx 1 2 i/ D hax 1 1 ; x 1 2 x 2 1 i This function states that in order to compute the pair of strings that corresponds to a tree whose root node is labelled with the symbol f , one first has to compute the pair of strings corresponding to the first child of the root node (hx 1 1 ; x 2 1 i) and the single string corresponding to the second child (hx 1 2 i), and then concatenate the individual components in the specified order, preceded by the terminal symbol a. We call a function lexicalized, if it contributes exactly one terminal symbol.",120,121
10319,1877927,"Here is an example for a function: f .hx 1 1 ; x 2 1 i; hx 1 2 i/ D hax 1 1 ; x 1 2 x 2 1 i This function states that in order to compute the pair of strings that corresponds to a tree whose root node is labelled with the symbol f , one first has to compute the pair of strings corresponding to the first child of the root node (hx 1 1 ; x 2 1 i) and the single string corresponding to the second child (hx 1 2 i), and then concatenate the individual components in the specified order, preceded by the terminal symbol a. We call a function lexicalized, if it contributes exactly one terminal symbol.",134,135
10320,2391590,"the axiom symbol, and I and A the set of initial and auxiliary trees respectively.",2,3
10321,2391590,"With respect to deduction steps, we have that ,,.., _ '1""'1!ni U 'T'IE U 'T'llnc U 'T'>Conc U 'T'IFoot U ,,.., Adj U 'T'>Subs vdVH - v dVH vdVH vdV H vdVH vdVH vdV H vdVH The initializer steps deduce those items associated to productions whose right hand side includes a terminal that matches with an input symbol.",70,71
10322,2391590,"rn symbol, whenever an item [T -+ •Ra•, 0, n, -, -j is deduced.",1,2
10323,1767631,"A tree adjoining grammar (tag) is a 5-tuple G = (N, T, S, I, A) where N and T are finite sets of nonterminals and terminals respectively, S the start symbol, I a finite set of initial trees (center trees) and A a finite set of adjunct trees (auxiliary trees).",41,42
10324,1767631,"1987) is a 5-tuple G = (N, T, F, P, S) where N is a finite set of nonterminals, T a finite set of terminals, F a finite set of functions, P a finite set of (production) rules and S the start symbol.",56,57
10325,1767631,"For the start symbol S, dim(S) = 1.",3,4
10326,11897785,"We can take two lessons from this assessment: (1) since much of our data comes from naturally occurring speech, it may be useful to rerun our tests with an NP fragment as a valid root symbol in our grammars; (2) proper identification of auxiliary verbs is an important next step for improving our system.",39,40
10327,12863912,"We consider two dynamic grammar mechanisms: activation and deactivation of grammar rules, and the substitution of a new regular language for a terminal symbol when recognizing the next utterance.",25,26
10328,12863912,"Vx E ~, a(x) C Reg(A"" × R) where Reg(A* x R) denotes the set of weighted regular languages over the alphabet A. Thus a simply substitutes for each symbol a E ~ a weighted regular expression a(a).",35,36
10329,12863912,"For example, Figure 3 shows the weighted automaton for grammar G2 consisting of the last three rules of G1 with start symbol X. Reg( A * × R ).",22,23
10330,12863912,"The automaton M(X) that represents the language generated by nonterminal symbol X can be defined using K(S), where S is the strongly connected component containing X, X E S. For instance, when the subgrammar of S is right-linear, M(X)is the automaton that has the same states, transitions, and final states as K(S) and has the state corresponding to X as initial state.",11,12
10331,12863912,"The dynamic substitution of a terminal symbol a by a weighted automaton 9 aa is done by replacing the symbol a by the automaton aa, using the replacement operation discussed earlier.",6,7
10332,12863912,"The dynamic substitution of a terminal symbol a by a weighted automaton 9 aa is done by replacing the symbol a by the automaton aa, using the replacement operation discussed earlier.",19,20
10333,1388209,"Here, a token is the specific, situated (i.e., as appearing in the sentence) instantiation of a linguistic entity: a letter, symbol, sound, word, phrase, or another related entity.",27,28
10334,1388209,"Rubric: Suppose X is a linguistic entity in a sentence S. Construct sentence S' as follows: replace X in S with a phrase X' of the form ""that [item]"", where [item] is the appropriate term for X in the context of S (e.g., ""letter"", ""symbol"", ""word"", ""name"", ""phrase"", ""sentence"", etc.).",61,62
10335,1388209,"A set of 23 ""mentionsignificant"" words was gathered informally from the pilot corpus, consisting of nouns and verbs: Nouns: letter, meaning, name, phrase, pronunciation, sentence, sound, symbol, term, title, word Verbs: ask, call, hear, mean, name, pronounce, refer, say, tell, title, translate, write Instances of highlighted text were only promoted to the hand annotation stage if they contained at least one of these words within the three-word phrase directly preceding the highlighted text.",38,39
10336,1388209,"Corpus Composition As stated previously, categories for mentioned language were identified based on intuitive relationships among the substitution phrases created for the rubric (e.g., ""that word"", ""that title"", ""that symbol"").",39,40
10337,5486621,"In the case of a phrase structure rule and a lexical item, let a newly created symbol be the predicate and all the variables in the third element be the arguments.",17,18
10338,11282692,"A subtree can be identified by a pair (t, N), where t is an elementary tree and N is a node in that tree; the pair indicates the subtree of t rooted at N. The set of all trees needed by our algorithm is given by: T = IUAU{(t,N) I tEIUA, NEAf(t)} From here on, we will use the symbol t exclusively to range over I U A, and r to range over T in general.",72,73
10339,11282692,"We will use the symbol A to range over stacks and substrings of stacks, and the symbol X to range over elements from A4.",4,5
10340,11282692,"We will use the symbol A to range over stacks and substrings of stacks, and the symbol X to range over elements from A4.",17,18
10341,6112493,"While the model presented by (Wright & Wrigley, 1991) was equivalent to the standard PCFG (probabilistic context-free grammar, see (Charniak, 1993) ) model, which is not contextsensitive and thus has certain limitations in the precision that it can achieve, later work tried to implement slight context-sensitivity (as e.g. the probability of a shift/reduce-action in Briscoe and Carroll's model depends on the current and succeeding LR parser state and the look-ahead symbol).",92,93
10342,6112493,"We have chosen three contexts for evaluation (K1 and K2 also exist in our model but are irrelevant for this evaluation): ù K3: LR parser state and look-ahead symbol, ù K4: K3 plus phrase head of the top element of the LR parsing stack, ù K5: K4 plus look-ahead word.",33,34
10343,1857060,"In our experiments in this paper, we only use LTAG grammars where each elementary tree is lexicalized by exactly one word (terminal symbol) on the frontier.",24,25
10344,1857060,The spine is the path from a nonterminal lexicalized by a word to the terminal symbol on the frontier equal to that word.,15,16
10345,14992350,"In order to deal efficiently with multisets in the input, we use a slightly extended version of Earley parsing which overgenerates with respect to repetitions of the same input symbol.",30,31
10346,14992350,The reason is that we do not check here whether any symbol occurs more than once.,11,12
10347,14992350,"First, a s ubgrammar G' is constructed which only provides the Jexical frames of any input symbol w;, i=l ,."",n.",18,19
10348,14992350,"As the grammar is in Greibach normal form, one ordered symbol must equal the terminal in thc rul e. All other symbols may go to the finite set of promotion sites provided by the FSA.",11,12
10349,14992350,"In order to ultimately get all gaps filled, the initial symbol must have an empty GAP !",11,12
10350,14304029,"It has mappings from words to their pronunciations  and the current phoneme set contains 39 phonemes based on the ARPAbet symbol set, which has been developed for speech recognition uses.",21,22
10351,215746894,"For leaf nodes, the productions are of the form entityLabel → ce where ce, which stands for candidate entity, is the only terminal symbol in the grammar.",26,27
10352,11198005,Each word was randomly assigned an arbitrary 4-digit symbol.,10,11
10353,11198005,The training set for the ANN consists of 1449 word/symbol pairs.,11,12
10354,11198005,"The output of the network is a 4-digit symbol, represented by four 9-node distributed representations (total of 36 nodes).",10,11
10355,11411446,"The preprocessing stage for both operand automata consists of adding a transition with a special symbol x at every final state, going to itself, and with a weight of 1.",15,16
10356,11411446,"This will allow to match words of different lengths, as when one of the automata is ""exhausted,"" the x symbol will be added as long as the other automaton is not.",23,24
10357,11411446,"After the composition, the x symbol is replaced everywhere by .",6,7
10358,11411446,"The symbol ""?""",1,2
10359,11411446,"matches any symbol; ""x"" is a special espilonsymbol introduced in the final states of the operand automata at preprocessing.",2,3
10360,17865966,This problem was implemented as an SRN learning taskto predict the symbol following the left context given to the input layer so far.,11,12
10361,17865966,"Words were applied to the network, symbol by symbol, which in turn were encoded orthogonally, that is, one node standing for one symbol (Fig.",7,8
10362,17865966,"Words were applied to the network, symbol by symbol, which in turn were encoded orthogonally, that is, one node standing for one symbol (Fig.",9,10
10363,17865966,"Words were applied to the network, symbol by symbol, which in turn were encoded orthogonally, that is, one node standing for one symbol (Fig.",26,27
10364,17865966,An extra symbol ('#') was used as a delimiter.,2,3
10365,17865966,The second approach computes the local proximity between the network response and a vector containing the empirical symbol probabifities that a given symbol would follow the current left context.,17,18
10366,17865966,The second approach computes the local proximity between the network response and a vector containing the empirical symbol probabifities that a given symbol would follow the current left context.,22,23
10367,17865966,"The root is empty, it does not represent a symbol.",10,11
10368,17865966,"In addition to the symbol at each node, we can keep additional information, for example the frequency of a word, if this node is the end of a word.",4,5
10369,17865966,"Another approach related to the practical implementation of a trained SRN is to search for a cue, giving an answer to the question whether given symbol can follow the context provirtea to the input layer so far.",26,27
10370,2811209,"At last, several rules are used to recognize some proper names separated by coordinate characters like ""、"", ""和"", ""与"" and symbol ""•"" in foreign person names.",29,30
10371,1355040,"consider A!xXY 2 P i,1 ; as each source symbol occurs at most once in every word of L(S i,1 ), the same holds for LA hence the sets of source symbols occurring in LX and LY are disjoint.",9,10
10372,52895758,The model consists of six layers: Embed layer: The model first encodes each word and symbol in the input sequences I and m into fixed-length representations.,17,18
10373,52895758,"Note that o t in-cludes a value for each of the pre-defined behaviors in the graph m, as well as for a special ""stop"" symbol to identify the end of the output sequence.",31,32
10374,52895758,"The mask function returns a vector of the same dimensionality as the logits o t , but with zeros for the valid behaviors after the last location n t and for the special stop symbol, and − inf for any invalid predictions according to the connectivity of the behavioral navigation graph.",34,35
10375,8376056,"Different from western-like languages, handling oriented languages is far more difficult since there is no explicit boundary symbol to indicate what a word is in the text.",20,21
10376,14953119,"Identifying 'Phrases' Although alien brains may be more or less powerful than ours (Norris 1999) , it is reasonable to assume that all intelligent problem solvers are subject to the same ultimate constraints of computational power and storage and their symbol systems will reflect this.",44,45
10377,9230323,"y will refer to a directed, unlabeled dependency tree, which is a map y : {1, ..., n} → {0, ..., n} from child indices to parent indices; x 0 is the invisible ""wall"" symbol.",47,48
10378,14144148,"A symbol in Eu•,s is called an operator of type (tr, s), arity u:, sort s and rank jwj, where lwl denotes the Jength of w. Let X == { x 1 , x 2 , .r 3 , . .. }",1,2
10379,14144148,S E N is the start symbol.,6,7
10380,14144148,S E F 0 is the starting symbol and Pis a set of productions.,7,8
10381,14144148,"The function l just retums a triple of a. 1 , a 2 and a. 3 • The corresponding tree has a mother node Jabeled with a ternary tupling symbol and the three unary arguments of the mapping as daughters.",29,30
10382,14144148,Then we can easily construct appropriate transiti ons by basically reversing the arrow: the nonteITTJinals become state names and the mother node will be read as alphabet symbol.,28,29
10383,14144148,"All nonterminal nodes in T' are Jabeled by some c E C or a ""tupling"" symbol.",18,19
10384,14144148,"The terminal nodes in T' are either labeled by some "" linguistic"" symbol, a ""tupling"" symbol of the form ( )(k,OJ• i.e. the ""empty"" tuple, or by some ""projection"" symbol rrf.",14,15
10385,14144148,"The terminal nodes in T' are either labeled by some "" linguistic"" symbol, a ""tupling"" symbol of the form ( )(k,OJ• i.e. the ""empty"" tuple, or by some ""projection"" symbol rrf.",20,21
10386,14144148,"The terminal nodes in T' are either labeled by some "" linguistic"" symbol, a ""tupling"" symbol of the form ( )(k,OJ• i.e. the ""empty"" tuple, or by some ""projection"" symbol rrf.",43,44
10387,14144148,"Any ""linguistic"" node dominating anything in the intended tree is on some left branch in T', i.e., it is the left daughter of some c E C and the sister of a tupling symbol whose daughters evaluate to the intended daughters.",38,39
10388,14144148,"For any node v labeled with some ""projection"" symbol 7rf E ll in T1 there is a unigue node µ (labeled with some c E C) which properly dominates v and which immediately dominates a node Jabeled with a ""tupling"" symbol whose i-th daughter will eventually evaluate to the value of r.f.",10,11
10389,14144148,"For any node v labeled with some ""projection"" symbol 7rf E ll in T1 there is a unigue node µ (labeled with some c E C) which properly dominates v and which immediately dominates a node Jabeled with a ""tupling"" symbol whose i-th daughter will eventually evaluate to the value of r.f.",46,47
10390,14144148,"Moreover, µ will be the first node properly dominating v which is on a left branch and bears a composition symbol.",21,22
10391,14144148,"Then it has to go up the first branch, read a composition symbol and descend to its sister.",13,14
10392,14144148,"lf it reads a composition symbol, the automaton goes to the left daughter and tries again.",5,6
10393,14144148,"If it reads a tupling symbol, the automaton proceeds with its daughters.",5,6
10394,14144148,"On finding a projection symbol, it searches for the appropriate ""filler"" by going upwards until it is on a leftmost branch which is labeled with a composition symbol.",4,5
10395,14144148,"On finding a projection symbol, it searches for the appropriate ""filler"" by going upwards until it is on a leftmost branch which is labeled with a composition symbol.",30,31
10396,236486106,"For our IWSLT submission, we generate the extended prefixes for dynamic mask simply by appending UNK (i.e the unknown word symbol) to the prefix.",22,23
10397,8321561,"From the initial state on both the left and the right subtree we can either go to the state denoting ""found xl"" (al) if we read symbol 10 or to the state denoting ""found x2"" (a2) if we read symbol 01.",30,31
10398,8321561,"From the initial state on both the left and the right subtree we can either go to the state denoting ""found xl"" (al) if we read symbol 10 or to the state denoting ""found x2"" (a2) if we read symbol 01.",47,48
10399,9871580,"In a LIG, the stack associated with the LHS symbol X is copied to one of the RHS nonterminals Y , the top symbol can be popped off the stack of X, or a new symbol can be pushed onto the copy of the stack that is passed down to Y : 4 copy: X[α] → ...Y [α]... pop: X[αc] → ...Y [α]... push: X[α] → ...Y [αc]... We show that translating both LTAG and CCG directly into strongly equivalent indexed grammars which capture all dependencies in the extended domain of locality via nonterminal stacks reveals that CCG requires a LIG with registers which is not strongly equivalent to any LIG that can be obtained from a LTAG.",10,11
10400,9871580,"In a LIG, the stack associated with the LHS symbol X is copied to one of the RHS nonterminals Y , the top symbol can be popped off the stack of X, or a new symbol can be pushed onto the copy of the stack that is passed down to Y : 4 copy: X[α] → ...Y [α]... pop: X[αc] → ...Y [α]... push: X[α] → ...Y [αc]... We show that translating both LTAG and CCG directly into strongly equivalent indexed grammars which capture all dependencies in the extended domain of locality via nonterminal stacks reveals that CCG requires a LIG with registers which is not strongly equivalent to any LIG that can be obtained from a LTAG.",24,25
10401,9871580,"In a LIG, the stack associated with the LHS symbol X is copied to one of the RHS nonterminals Y , the top symbol can be popped off the stack of X, or a new symbol can be pushed onto the copy of the stack that is passed down to Y : 4 copy: X[α] → ...Y [α]... pop: X[αc] → ...Y [α]... push: X[α] → ...Y [αc]... We show that translating both LTAG and CCG directly into strongly equivalent indexed grammars which capture all dependencies in the extended domain of locality via nonterminal stacks reveals that CCG requires a LIG with registers which is not strongly equivalent to any LIG that can be obtained from a LTAG.",37,38
10402,9871580,Every dependent Y i that has scope over the lexical anchor is generated by a rule which pushes a symbol y i onto the stack.,19,20
10403,9871580,"Every adjunction node requires two additional unary rules which push and pop a node identifier: push: The effect of pop n,m Bounds n on composition B n and m on type-raising in CCG correspond thus to a LIG that allows all pop i,j operations for i ≤ n and j ≤ m. Given a category t : [c 1 ...c n+m+1 ], a standard LIG could pop the c i s only in the reverse order c n+m+1 ...c 1 , whereas a LIG with pop n,m could also pop off c m+1 as the first symbol.",110,111
10404,9706446,"To allow tapes of different length to be accepted, a symbol to be matched against one or other of the tapes to do a transition may be empty, in which case the corresponding tape is ignored.",11,12
10405,9706446,"Certainly this model does not depend on the processing direction from left to right, but at any time during processing it focusses on only one symbol on the input tape.",26,27
10406,9706446,A routine for locating the stem vowel and replacing it by a generic symbol; it is realized by a simple function.,13,14
10407,9706446,The generic symbol is replaced by the stem vowel indicated by the feature list using a single rewriting rule.,2,3
10408,9706446,"The stem vowel of the infinitive is replaced by a place holder, the stem vowel is added to the end of the form, separated frmn the stem by a hyphen: Stein vowels consisting of more than one character are encoded as a single symbol.",46,47
10409,220045475,"The other measure, DLG, computes wordhood of an n-gram s according to the change of the description length of a dataset D with and without treating that n-gram as a segment: DLG(s) = DL(D) − DL(D[r → s] ⊕ s) (11) where D denotes the original dataset and D[r → s]⊕s represents a new dataset by treating s as a new segment, replacing all the occurrences of s with a new symbol r (which can be seen as an index for newly identified segment s), and then appending s at the end.",85,86
10410,10631637,The terminal symbol e/f represents tokens output to the English and Foreign sentences respectively.,2,3
10411,10631637,"A redundant, headaware grammar is shown here: A → [M A] | M A | [AM ] | AM |H M → he/f | here/f | quickly/f H → ran/f (3) Note that two modifiers can never be combined without also including the A symbol, which always contains the head.",59,60
10412,5665391,The shift operation advances the input stream by one symbol and push the symbol into the stack; while the reduce operation applies some reduction rule to the topmost elements of the stack.,9,10
10413,5665391,The shift operation advances the input stream by one symbol and push the symbol into the stack; while the reduce operation applies some reduction rule to the topmost elements of the stack.,13,14
10414,5665391,"In our case, the input stream is the target string of the rule and the symbol is the corresponding source index of the elements of the target string.",16,17
10415,5665391,"Unless the symbol corresponds to an anchor, it tries to apply the reduce operation.",2,3
10416,5665391,"If the symbol read is a nonterminal, then we push the entire stack that corresponds to that nonterminal.",2,3
10417,2991968,"And secondly, it allows symbol-level compression of the structure with local, on-the-fly decompression as needed.",5,6
10418,2369967,"At each point in the top-down traversal, the transducer chooses a production to apply, based only on the current state and the current root symbol.",28,29
10419,4248450,We only use output alphabet symbols that are used for ≥ 5 form-lemma pairs and also add a special output symbol that indicates that the aligned input should simply be copied.,22,23
10420,309731,"We refer to a character alignment unit as ""multigram"" later on and represent it by the symbol ""q"".",18,19
10421,219302007,These are connected together by a symbol.,6,7
10422,219302007,These are adjacent to each other and not connected by a symbol.,11,12
10423,219302007,A word connected by a symbol represents a name of the plant part.,5,6
10424,219302007,"On the contrary, a word which is not connected by a symbol may represent any type of information.",12,13
10425,219302007,Rule for names of plant parts by using symbols] A word which is connected to the other element by a symbol is interpreted as a name of the plant part.,21,22
10426,14161026,"Each non-Chinese character c is replaced by a cluster representative symbol σ M , where c is in the cluster M. We refer to the string composed of all σ M as F. If the length of F is more than that of W, it will be shortened to W. The normalized sentence is then placed in one file, and the non-Chinese character sequence is placed in another.",12,13
10427,1540379,"The N-gram approximation of the joint probability can be defined in terms of multigrams q i as: p(q k 1 ) ≈ k+1 j=1 p(q j |q j−1 j−N +1 ) (1) where q 0 , q k+1 are set to a special boundary symbol.",49,50
10428,2903902,"Hindi, on the other hand, uses a special joining symbol between two characters to write conjuncts.",11,12
10429,2903902,If the joining symbol is used between two identical characters then it will be transliterated with a shadda in Urdu.,3,4
10430,2903902,"Assume the joining symbol is ""z"" and L is a character in Hindi.",3,4
10431,5744000,"The symbol (*) indicates that the run is a closed test, which only uses the training material from the training data for the particular corpus.",1,2
10432,10378418,"Next, we generate the final period in the OUT model and then the end-of-sentence symbol.",19,20
10433,3447105,The symbol ∅ represents the empty string.,1,2
10434,3447105,We randomly selected 100 word pairs that were incorrectly mined as transliteration pairs and clustered them into the following error types: (1) affix-based error: the words differ by one or two characters at the start or end of the word; (2) pronunciation error: a borrowed word that sounds slightly different in the original language; (3) punctuation errors: one word has an additional punctuation symbol that makes the word pair a non-transliteration; (4) gold standard error: errors in the gold standard; (5) worst errors: word pairs that are far from being considered as transliteration pairs.,77,78
10435,799077,The empty set symbol indicates that the NL string does not carry any meaning.,3,4
10436,9550077,The symbol SEM is used to abstract over specific content such as the set of points delimiting an area or the identifiers of selected objects.,1,2
10437,9550077,The epsilon symbol ( ) indicates that a stream is empty in a given terminal.,2,3
10438,2297378,These are connected together by a symbol.,6,7
10439,2297378,These are adjacent to each other and not connected by a symbol.,11,12
10440,2297378,A word connected by a symbol represents a name of the plant part.,5,6
10441,2297378,"On the contrary, a word which is not connected by a symbol may represent any type of information.",12,13
10442,2297378,Rule for names of plant parts by using symbols] A word which is connected to the other element by a symbol is interpreted as a name of the plant part.,21,22
10443,104744,"In general, a pseudo-punctuation symbol is the conjunction of the binned values of all of the prosodic features used in a particular run.",7,8
10444,104744,"When mapping from binned prosodic variables to pseudopunctuation symbols, some of the binned values can be represented by the absence of a pseudopunctuation symbol.",24,25
10445,104744,"The ranges are given below, and were chosen so that they align with bin boundaries and result in each type of pseudopunctuation symbol occuring on 40% of words.",23,24
10446,104744,Thus when a prosodic feature is used alone only 4 of its 10 bins are represented by a pseudo-punctuation symbol.,21,22
10447,104744,"However, when two or more types of the prosodic pseudo-punctuation symbols are used at once there is a larger number of different pseudo-punctuation symbols and a greater number of words appearing with a following pseudo-punctuation symbol.",42,43
10448,104744,"For example, when P, R and S prosodic annotations are used together there are 89 distinct types of prosodic pseudo-punctuation symbols in our corpus, and 54% of words are followed by a prosodic pseudo-punctuation symbol.",42,43
10449,104744,"For example, we also experimented with a ""raised"" representation in which the prosodic pseudo-punctuation symbol also serves as the preterminal label.",19,20
10450,104744,"Raising"" should permit the generative model to condition not just on the presence of a prosodic pseudo-punctuation symbol but also on its actual identity.",20,21
10451,104744,Consider the effect of inserting a prosodic pseudo-punctuation symbol on such a model.,10,11
10452,104744,"The prosodic pseudo-punctuation symbol would replace the true preceding sibling's category in the model, thus possibly resulting in poorer overall performance (note however that the parser also includes a higher-order backoff distribution in which the next category is predicted using the preceding two sibling's categories, so the true sibling's category would still have some predictive value).",5,6
10453,19263469,"We use a special symbol denoting the type of slot (e.g. <food>) as the first decoder input, which is also mapped to a trainable embedding E type .",4,5
10454,12689722,"Deterministic FSA (DFA) is efficient because a unique ""next state"" is determined, when given an input symbol and the current state.",21,22
10455,7785983,Below is a derivation that would generate the sample sentence and its MR simultaneously: (Note that RULE is the start symbol for CLANG) Here the MR string is said to be a translation of the NL string.,22,23
10456,10756783,This recursion terminates at the start symbol TOP.,6,7
10457,5628888,"Thus every correctly recognized pin-yin symbol has a chance of being transformed with some error, resulting in higher character error rate than the pin-yin error rate.",7,8
10458,5628888,"Note that while significantly lower error rates have been reported for converting pin-yin to characters in generic Chinese text, ours is a highly specialized subset of transliterated foreign names, where the choice between several characters sharing the same pin-yin symbol is somewhat arbitrary.",45,46
10459,9042532,"Next, the sequent is built by combining the sequence of the categories for the words with the ⊢ symbol and the sentence category (e.g. S ).",19,20
10460,8497268,"Equation 2 requires that token w can be fully decomposed into a sequence 2 For algorithmic reasons, we use the start position 0 to represent a fictitious start symbol before the first character of the word.",29,30
10461,763701,"lev(i − 1, j − 1) + d(xi, yj ) lev(i − 1, j) + 1 lev(i, j − 1) + 1 In the definition above, d(x i , y j ) is a cost of substituting one symbol in x by a symbol from y. The insertion and deletion costs are equal to 1.",46,47
10462,763701,"lev(i − 1, j − 1) + d(xi, yj ) lev(i − 1, j) + 1 lev(i, j − 1) + 1 In the definition above, d(x i , y j ) is a cost of substituting one symbol in x by a symbol from y. The insertion and deletion costs are equal to 1.",51,52
10463,10396032,"In this model of learning it is assumed that we have a sample space U* of vectors over an alphabet U, where each position in a vector denotes the presence (1) or absence (0) of a symbol a ~_--U in the sample vector.",42,43
10464,1954850,"Let G = (N , T , S, R, θ) be a Probabilistic Context Free Grammar with nonterminal symbols N , terminal symbols T , start symbol S ∈ N , set of productions R of the form N → β, N ∈ N , β ∈ (N ∪ T ) * .",30,31
10465,1954850,"1  In the split-head bilexical CFG framework, each nonterminal in the grammar is annotated with a terminal symbol.",21,22
10466,17719925,"We hereafter name such HPSG trees after the TFS-to-symbol mapping, 'simplified HPSG trees.'",12,13
10467,17719925,Each nonterminal symbol on the yields of the synchronous production is linked to a non-terminal symbol on the other rule's yield.,2,3
10468,17719925,Each nonterminal symbol on the yields of the synchronous production is linked to a non-terminal symbol on the other rule's yield.,17,18
10469,1915538,We add a special begin symbol to the sequences and replace the initial probabilities with P (s|begin).,5,6
10470,1915538,"Based on the terminology used in (4), the outcomes belong to two categories: ω = (s, s ′ ) where state s ′ follows state s, and ω = (s, a) where symbol a is emitted from state s. Define the feature function f ω (x, y, t) to be 1 if the outcome ω happens at time step t, and 0 otherwise.",42,43
10471,8658809,"Background A PCFG is a tuple (V, M, µ 0 , R, q : R → [0, 1]), where V is a set of terminal symbols; M = {µ i } is a set of nonterminal symbols; µ 0 is a start or root symbol; R is a set of productions of the form µ i → ρ, where ρ is a sequence of terminals and nonterminals; and q is a family of probability distributions over rules conditioned on each rule's left-hand side.",56,57
10472,15053972,"The non-local constraints, on the other hand, allow us to state that the region of one symbol in an elementary tree must have a certain spatial relation to the region of another symbol in the same elementary tree.",20,21
10473,15053972,"The non-local constraints, on the other hand, allow us to state that the region of one symbol in an elementary tree must have a certain spatial relation to the region of another symbol in the same elementary tree.",36,37
10474,15053972,"As in SGIS, the terminals and non-terminals at the time of writing an SCTAG are not pairs of (symbol, region), but simply behaviors and intentions.",22,23
10475,15053972,"In our case, we additionally store for each symbol in each chart entry the set of regions in which it may occur, i.e. when inserting a new chart entry we resolve the spatial constraints by a simple look-up in the spatial relation table.",9,10
10476,15053972,The scan operation reads the next symbol from the input and matches it with the chart entries.,6,7
10477,15053972,"Although we write our SCTAG rules on intentions and behaviors, we get pairs of (symbol, region) during parsing.",16,17
10478,15053972,"We first execute scan using symbol, as in the original parser, and then use the region information to throw away those regions in our chart entries that are not consistent with the region information.",5,6
10479,15053972,As soon as a symbol in a chart entry has an empty set of possible regions we throw away the chart entry.,4,5
10480,11379360,We call the set of features that are based on the fact that the ciphertext contains a specific symbol HAS.,18,19
10481,11379360,"This set contains binary features firing when the cipher con-tains a digit, a letter (A-Z), the ""#"" symbol, the letter ""j"", the digit ""0"".",27,28
10482,2752707,"A relational head acceptor writes (or accepts) a pair of symbol sequences, a left sequence and a right sequence.",12,13
10483,2752707,The possible actions taken by a relational head acceptor m. in state qi are: • Left transition: write a symbol r onto the right end of the left sequence and eater state qi+l. •,21,22
10484,2752707,Right transition: write a symbol r onto the left end of the right sequence and enter state qi+l. •,5,6
10485,2752707,"From a state qi these actions are as follows: • Left transition: write a symbol rl onto the right end of L1, write symbol r2 to position a in the target sequences, and enter state qi+l. *",16,17
10486,2752707,"From a state qi these actions are as follows: • Left transition: write a symbol rl onto the right end of L1, write symbol r2 to position a in the target sequences, and enter state qi+l. *",26,27
10487,2752707,"Right transition: write a symbol rl onto the left end of R1, write a symbol r~ to position a in the target sequences, and enter state qi+t. .",5,6
10488,2752707,"Right transition: write a symbol rl onto the left end of R1, write a symbol r~ to position a in the target sequences, and enter state qi+t. .",16,17
10489,51884176,"It may also help with reducing the number of necessary stimuli required for symbol selection (Speier et al.,",13,14
10490,51884176,"In such systems, a language model provides the prior distribution that serves as a bias to the EEG evidence when computing the posterior distribution of a symbol as shown in Equation 1 p(sym|EEG) = p(sym)p(EEG|sym) p(EEG) (1) Traditionally, such BCI systems commit to a single decision (given their posterior results), which introduces a problem once the decision is not aligned with the user.",27,28
10491,51884176,"F topn = (F wh F cw ) • F wLM (2) F char topn = proj char (F topn • F spellout ) (3) F sym = F char topn ∪ F charLM (4) F OCLM = (F tp σ) • F sym (5) We next take the lattice of ""current words"", compose that with a word-to-character spellout machine (F spellout ), project the result into character space (Equation 3 ), and then take the union of the result with a character language model to produce a ""symbol language model"" lattice (F sym in Equation 4 ).",112,113
10492,51884176,We are now ready to compute probabilities for the likely next symbol the user intends to select.,11,12
10493,51884176,Infrequent words that were seen less than five times but at least once during training are mapped to <unk> word-symbol.,23,24
10494,202789894,"Thus, to identify morpheme boundaries, morpheme processing of the proposed model generates a special symbol of morpheme ending for every explicit morpheme boundary instead of adopting the IOB tagging scheme.",16,17
10495,51874610,"The decoder RNN predicts the output sequence y = [y 1 , • • • , y T T ], through the following dynamics and prediction model: s t = f (y t−1 , s t−1 , c t ); (2) p(y t |y <t , x) = g(y t−1 , s t , c t ), (3) where s t is the RNN state and c t is the context vector at time t. y t is the predicted symbol from the target vocabulary at time t via prediction function g(•).",92,93
10496,12313253,"Let W = W 0 W 1 • • • W n , where W 0 is always the special symbol ROOT.",20,21
10497,12106333,"There is a symbol for ka, another for ko, etc.",3,4
10498,12106333,"Even our Japanese decoder would fail on an alternative syllabic script for Japanese which employed a single symbol for the sound KAO, instead of separate kana symbols for KA and O. One ambitious line of research would be to examine writing systems in an effort to invent a single, generic ""mother of all writing systems,"" whose specializations include a large fraction of actual ones.",17,18
10499,7106007,"This kind of noise and ill-formedness is also seen in written text, for example, misspelled words, unbalanced parentheses, incorrect punctuation, symbol characters, and special formatting characters which fail to be filtered by text preprocessing modules.",27,28
10500,7106007,"It is difficult to have a preprocessing module which perfectly filters out symbol characters and special formatting characters, especially in the early stage of development.",12,13
10501,7106007,"Skipping a word such as adverb or symbol character is almost harmless for the whole translation, but skipping an important noun or verb would be disastrous.",7,8
10502,7106007,"Those symbol characters included a dot or a small circle at the beginning of a sentence, or a pair of parentheses enclosing an entire sentence.",1,2
10503,7106007,"Even though more than half of the correct skipped words were symbol characters, parentheses, and chunker markers, this does not mean that skip parsing mainly works as a compensation of bad preprocessing effects.",11,12
10504,7106007,"Newspaper articles make extensive use of commas for lexical, phrasal, and sentential conjunctions, and small dots and other symbol characters often appear in compound nouns or phrasal itemizations.",21,22
10505,7106007,Human readers benefit from comma and other symbol characters when they read long texts.,7,8
10506,7106007,"Therefore, skip parsing not only works as a compensation of wrong but indispensable preprocessing (as in chunker marking), but also works really well for the gaps that the grammar rules could not fill (as in commas and symbol characters, as well as adverbs and particles).",42,43
10507,7245369,These templates apply only when the stack contains at least two nodes; p represents a separator punctuation symbol.,18,19
10508,1625811,These templates apply only when the stack contains at least two nodes; p represents a separator punctuation symbol.,18,19
10509,248496004,"w |V| }, the special end-of-sentence symbol as w 1 = </s>, the source sentence as x = x 1 , . . . ,",11,12
10510,9687923,"For each category in the grammar that is distinct in terms of features, invent an atomic non-terminal symbol.",20,21
10511,9687923,"With these atomic symbols, create a SCFG by mapping each category in a DCG rule to an atomic symbol, yielding a context free (backbone) grammar, and with this grammar, specify a SCFG, /Vii.",19,20
10512,9687923,Let P(A -~ (~ ] A) be the probability of expanding (backbone) nonterminal symbol A with the (backbone) rule A --+ o when deriving some sentence si.,17,18
10513,9687923,Each test sentence was trivially rooted with an S symbol (necessary for the evaluation software).,9,10
10514,9367805,"It relies on a set of elementary trees (defined in the lexicon) which have at least one terminal symbol on its frontier, called the anchor.",20,21
10515,9367805,"Table 5 displays rather lim-Figure 3 : elementary trees and attached semantic features for the sentence ""give me more information about the company"" In figures 3 and 4, the heads of the trees are standard syntactic categories, the star symbol on the 4It should be borne in mind that the term substitution when speaking of Tree Grammars has nothing to do with the term substitution that refers to a recognition error right or left of the head indicates an auxiliary tree that will combine with a compatible tree ; a X* head symbol indicates a tree which combines with a node of category X on its right, a *X node combines with a node X on its left.",45,46
10516,9367805,"Table 5 displays rather lim-Figure 3 : elementary trees and attached semantic features for the sentence ""give me more information about the company"" In figures 3 and 4, the heads of the trees are standard syntactic categories, the star symbol on the 4It should be borne in mind that the term substitution when speaking of Tree Grammars has nothing to do with the term substitution that refers to a recognition error right or left of the head indicates an auxiliary tree that will combine with a compatible tree ; a X* head symbol indicates a tree which combines with a node of category X on its right, a *X node combines with a node X on its left.",99,100
10517,9367805,"Nodes X0, X1, or more generally Xn, are substitution sites, they are awaiting a tree whose head symbol is X. Substitution sites bear syntactic and semantic constraints on their possible substitutors.",21,22
10518,9367805,"Here, the semantic constraints are made visible in the node symbol (e.g. N0-PERSON means the substitutor of this node must be of category N -noun-and must possess a semantic feature :PERSON).",11,12
10519,18083801,"The total number of resulting distinct features was about 14K. Although some symbols are treated as distinct tags in the PTB tag definitions, we aggregated these symbols into a symbol tag (SYM) since it is easy to restore original symbol tags from the SYM tag.",30,31
10520,18083801,"The total number of resulting distinct features was about 14K. Although some symbols are treated as distinct tags in the PTB tag definitions, we aggregated these symbols into a symbol tag (SYM) since it is easy to restore original symbol tags from the SYM tag.",42,43
10521,33414645,"Each constant of Σ, i.e., each symbol of rank 0, is a tree.",8,9
10522,33414645,"t k ] = t. Definition 1 A context-free tree (CF T ) grammar is a tuple G = (F, Ω, S, P ) where F and Ω are ranked alphabets of non-terminals and terminals, respectively, S ∈ F (0) is the start symbol and P is a finite set of productions of the form F (y 1 , . . . ,",56,57
10523,33414645,AT T s with rules R σ at an input symbol σ in which each outside attribute occurs exactly once are called simple attributed tree transducers.,10,11
10524,33414645,"The derived N -sorted alphabet Σ ′ is defined as follows: For each n ≥ 0, Well-Nested Tree Languages and Attributed Tree Transducers Σ ′ ε,n = {f ′ | f ∈ Σ n } is a new set of symbols of type ε, n ; for each n ≥ 1 and each i, 1 ≤ i ≤ n, π n i is a new symbol, the ith projection symbol of type ε, n ; for each n, k ≥ 0 the new symbol c (n,k) is the (n, k)th composition symbol of type nk 1 • • • k n , k with k 1 = . . .",75,76
10525,33414645,"The derived N -sorted alphabet Σ ′ is defined as follows: For each n ≥ 0, Well-Nested Tree Languages and Attributed Tree Transducers Σ ′ ε,n = {f ′ | f ∈ Σ n } is a new set of symbols of type ε, n ; for each n ≥ 1 and each i, 1 ≤ i ≤ n, π n i is a new symbol, the ith projection symbol of type ε, n ; for each n, k ≥ 0 the new symbol c (n,k) is the (n, k)th composition symbol of type nk 1 • • • k n , k with k 1 = . . .",80,81
10526,33414645,"The derived N -sorted alphabet Σ ′ is defined as follows: For each n ≥ 0, Well-Nested Tree Languages and Attributed Tree Transducers Σ ′ ε,n = {f ′ | f ∈ Σ n } is a new set of symbols of type ε, n ; for each n ≥ 1 and each i, 1 ≤ i ≤ n, π n i is a new symbol, the ith projection symbol of type ε, n ; for each n, k ≥ 0 the new symbol c (n,k) is the (n, k)th composition symbol of type nk 1 • • • k n , k with k 1 = . . .",96,97
10527,33414645,"The derived N -sorted alphabet Σ ′ is defined as follows: For each n ≥ 0, Well-Nested Tree Languages and Attributed Tree Transducers Σ ′ ε,n = {f ′ | f ∈ Σ n } is a new set of symbols of type ε, n ; for each n ≥ 1 and each i, 1 ≤ i ≤ n, π n i is a new symbol, the ith projection symbol of type ε, n ; for each n, k ≥ 0 the new symbol c (n,k) is the (n, k)th composition symbol of type nk 1 • • • k n , k with k 1 = . . .",110,111
10528,33414645,Syn = {α} with α = α m at the root node • Inh = {β j |1 ≤ j ≤ m} with m the maximal sort of a lifted non-terminal F ′ • Every symbol in the derivation trees is assigned one synthesized attribute. •,41,42
10529,33414645,Every projection symbol is assigned one inherited attribute • Every lifted terminal symbol f ′ of sort n is assigned n inherited attributes. •,2,3
10530,33414645,Every projection symbol is assigned one inherited attribute • Every lifted terminal symbol f ′ of sort n is assigned n inherited attributes. •,12,13
10531,33414645,Each R c contains the rules απ → απ1 β i π1 → απi + 1 β i πj → β i π provided that the inherited attributes belong to the outside attributes of the composition symbol c. The correctness of the construction follows from an easy rule induction.,36,37
10532,33414645,"Proof Let A = (Syn, Inh, Σ, Ω, α m , R) be an attributed tree transducer with one synthesized attribute only such that each outside attribute at an input symbol σ occurs exactly once in R σ .",36,37
10533,12964363,Z Z P P) As a symbol of our love.,7,8
10534,219299821,lt is a reference to an item wa.iting on a foot symbol.,11,12
10535,219299821,"Then an evolution stage applies a predict or reduce primitive on every newly introduced item, the type of whkh is chosen from the symbol in the rule \\•hich b right after the ma.rk.",24,25
10536,219299821,If there is no such symbol.,5,6
10537,7898033,"To generate patterns from a phrase, we replace the given word pairs with variables, X and Y , and we replace the remaining words with a wild card symbol (an asterisk) or leave them as they are.",30,31
10538,10002479,"Hence, we replaced all typographical quotation marks ( "", "" ) with the standard double quote symbol ("").",18,19
10539,219302427,"The arguments of function symbol (1 l) p(sign(v, Agr, sign(_, Agr, _))) ~-{not_3s(Agr)} p( sign (n , agr( sing, 3rd) , _)) *-- not_3s( agr( sing, Per)) (--{ l st_or_2nd( Per ) } not_3s( agr(plural, _)) ~- lst_or_2nd( lst) ~-- 1 st_or_2nd(2nd) ~-- Literal p(X) means that variable X is a candidate for the disjunctive feature structure (DFS) specified by predicate p. The constraint element lst_or_2nd(Per) in (11) constrains variable Per to be either 1st or 2nd.",4,5
10540,219302427,"In the case of a phrase structure rule and a lexical item, let a newly created symbol be the predicate and all the variables in the third element be the arguments.",17,18
10541,219302427,The symbol verb is replaced by the LCGR constant verb.,1,2
10542,219308589,"A dependency grammar is a six-tuple <W, C, S, D, I, H>, where W is a finite set of symbols (words of a natural language); C is a set of syntactic categories (among which the special category E); S is a non-empty set of root categories (C ~ S); D is the set of dependency relations, e.g. SUB J, OBJ, XCOMP, P-OBJ, PRIED (among which the special relation VISITORI); I is a finite set of symbols (among which the special symbol 0), called u-indices; H is a set of dependency rules of the form x:X (<r 1YlUl'l:l> ... <ri-1Yi-1Ui-l'l;i-I > # <ri+lYi+lUi+l'~i+l> ... <rmYmum~m>) 1) xe W, is the head of the rule; 2) Xe C, is its syntactic category; 3) an element <r i Yi ui xi> is a d-quadruple YeC. Finally, it holds that: I) For each u~ I that appears in a u-triple <u, r, Y>eUj, there exists exactly one d-quad <riYiui~i> in the same rule such that u=ui, i ~j.",112,113
10543,219308589,A triple consisting of a word w (e W) or the trace symbol e(EW) and two integers g and v is a word object of the grammar.,14,15
10544,219308589,The dot can advance across a d-quadruple <riYiui'q> or across the special symbol #.,16,17
10545,219308589,"When an item P contains a dotted rule with the dot at its end and a T-stack with the empty set O as the top symbol, the parser looks for the items that can advance the dot, given the completion of the dotted dependency rule in P. Here is the algorithm.",27,28
10546,219308589,"Scanner: When the dot precedes the symbol #, the parser can scan the current input word wi (if y, the head of the item P, is equal to it), or pseudoscan a trace item, respectively.",7,8
10547,219308589,"In this example we neglect the problem of subject-verb agreement: it can bc coded by inserting the AGR features in the category label (in a similar way to the +EX feature in the grammar G1); the comments on the right help to follow the events; the separator symbol I helps to keep trace of the sets in the stack; finally, we have left in plain text the d-quad sequence of the dotted rules; the other components of the items appear in boldface.",55,56
10548,219310339,"While the portability of Java was tempting, we eventually decided on Common LISP ((Steele, 1990) ) with its more powerful symbol and list manipulation facilities.",25,26
10549,29830292,"A symbol in rw, s is called an Operator of type (w,s), arityw, sorts and ranki(w), where t(w) denotes the length of w, In the case of a single-sorted signature we write rs"",s as l:n. The set of n -ary trees over such a single-sorted signature .L is built up from a finite set Xn = {x1, ... ,Xn} of variables using the operators in the expected way: If er E Ln and t1, ... , tn are n-ary trees, then er(t1, ... , tn) is an n-ary tree.",1,2
10550,29830292,Note that apart from the start symbol the only other nonterminal is of arity one.,6,7
10551,219309884,"Since tile number of sorts of bunsetsu is enormous, considering it as a symbol to be predicted would surely invoke the datasparseness problem.",14,15
10552,219309884,"The terminal symbol is the attribute of a bunsetsu, represented by the product of the head of the content part and that of the function part.",2,3
10553,219309884,The simplest way is to select each bunsetsu as a terminM symbol.,11,12
10554,219309884,The attribute sequence of a sentence is generated through applications of these rewriting rules to tile start symbol S. Each rewriting rule has a probability and the probability of the attribute sequence is the product of those of tile rewriting rules used for its generation.,17,18
10555,219309884,"Solution Search Algorithm The stochastic context-free grammar used for sy~ltactic analysis consists of rewriting rules (see formula (3)) in Chomsky normal form (Hopcroft and Ullma~l, 1979) except for the derivation from the start symbol (formula (2)).",46,47
10556,219309884,It should be noted that it is necessary to multiply the probability of the derivation from the start symbol at the end of the process.,18,19
10557,7048904,"X/Y Y > X This derivation of the symbol X is known as the normal-form derivation (Steedman, 2000) , since it uses function application whenever possible.",10,11
10558,7048904,"We then enumerate all non-normalform derivations that result in the same top-level symbol, packing all derivations (normal-form and non-normal-form) into a parse chart (see Figure 4 ).",16,17
10559,7048904,"6 Type-raisings are almost always possible, and will always be closer to the top-level symbol.",19,20
10560,7048904,"2010) uses a simple categorial grammar with only a single atomic symbol -i.e.,",12,13
10561,7048904,"E.g., the phrase pair den Weg für eine ⇔ the way for an will query the CCG chart-derived reordering model with the same symbol as the phrase pair den Weg für eine baldige ⇔ the way for an early twenty-seven year old abdullah britain blasts in hatcheries planning accused of is .",26,27
10562,41602520,"The presence or absence of these rules defines a family of unimodal logics of conjunction and implication, some of whose members (with characteristic arrows) are: logic structural rules arrows NL none (A/B) • B => A, B => (A/B)\B L RAssoc, LAssoc LP RAssoc,LAssoc,Perm A/B => (A/C)/(B/C), A\(B/C) => (A\B)/C A/B => B\A, (A/B)/C => (A/B)/C) When a particular formuia is provable in a particular logical system, we indicate this using Frege's symbol f-.",120,121
10563,2586427,"It consists of the following five types of rules: j i j i p e c e c AA AA A / | / | / | | ] [ ε ε > < ⎯→ ⎯ (1) Where A is the non-terminal symbol, [] and <> represent the two operations which generate outputs in straight and inverted orientation respectively.",47,48
10564,219308613,"We consider two dynamic grammar mechanisms: activation and deactivation of grammar rules, and the substitution of a new regular language for a terminal symbol when recognizing the next utterance.",25,26
10565,219308613,"a is a monoid morphism verifying: Vx e r, c ncg(a-× R) where Rcg(A* x R) denotes the set of weighted regular languages over the alphabet A. Thus a simply substitutes for each symbol a E E a weighted regular expression a(a).",38,39
10566,219308613,"For example, Figure 3 shows the weighted automaton for grammar G2 consisting of the last three rules of G1 with start symbol X. However, the standard methods for left-and right-linear grammars cannot be used for grammars such as G1 that generate regular sets but have rules that are neither left-nor right-linear.",22,23
10567,219308613,"The automaton M(X) that represents the language generated by nonterminal symbol X can be defined using K(S), where S is the strongly connected component containing X, X E S. For instance, when the subgrammar of S is right-linear, M(X) is the automaton that has the same states, transitions, and final states as K(S) and has the state corresponding to X as initial state.",11,12
10568,219308613,"The dynamic substitution of a terminal symbol a by a weighted automaton 9 aa is done by replacin9 the symbol a by the automaton a~, using the replacement operation discussed earlier. ]'",6,7
10569,219308613,"The dynamic substitution of a terminal symbol a by a weighted automaton 9 aa is done by replacin9 the symbol a by the automaton a~, using the replacement operation discussed earlier. ]'",19,20
10570,219309586,"A subtree can be identified |)y a pair (t, N), where t is an elementary tree and N is a node in that tree; the pair indicates the subtree of t rooted at N. The set of all trees needed by our algorithm is given by: T--IUAtO{(t,N) I tEIUA,NEAf(t)} From here on, we will use the symbol t exclusively to range over I U A, and r to range over 5/' in general.",72,73
10571,219309586,"We will use the symbol A to range over stacks and substrings of stacks, and the symbol X to range over elements from 3.4.",4,5
10572,219309586,"We will use the symbol A to range over stacks and substrings of stacks, and the symbol X to range over elements from 3.4.",17,18
10573,219306979,"If a sentence c~ is the topic of another sentence/3, this is formalised as c~ 1~ /3.1 This symbol also occurs in the graph, indicating a further SDRS condition.",19,20
10574,219302104,"A TAG is a tuple G --= (N, Z, I, A, S), where N, Z are the finite sets of nonterminal and terminal symbols, respectively, I, A are the finite sets of initial and auxiliary trees, respectively, and S E N is the initial symbol.",57,58
10575,11487448,"These probabilities are pre-computed for every non-terminal symbol and for every possible number of preceding and succeeding words, leading to large look-up tables.",11,12
10576,1827517,"Ain E N is a distinguished symbol, called the start symbol, Co E Co is the initial configuration, and R is a set of production rules of the following two forms: A -+ if 7r then ( 1 (B, .f)(2 A ---+ if 7r then w where A, BEN, 7r E BE(P)) and (i. (2 E (N U I;)•, f E F, w EI:*.",6,7
10577,1827517,"Ain E N is a distinguished symbol, called the start symbol, Co E Co is the initial configuration, and R is a set of production rules of the following two forms: A -+ if 7r then ( 1 (B, .f)(2 A ---+ if 7r then w where A, BEN, 7r E BE(P)) and (i. (2 E (N U I;)•, f E F, w EI:*.",11,12
10578,1827517,For two function symbols fi and f 2 the meaning of the composed function symbol fi&h is defined as m(f 1 &h) = m(h) o m(f1).,14,15
10579,1827517,"Definition 6 (linear controlled grammar) A linear distinguished grammar (LDG) is a quadruple G = (N, I:, R, Ain), where N and I: are finite sets ofnon-terminal and terminal symbols, respectively, A;n E N is the start symbol and where R is a finite set of production rules of the form: A -t ß 1 X!ß2 with A E N, X E NU I:, called the distinguished symbol, ß 1 ,ß 2 E ( N U I:) •, and !",53,54
10580,1827517,"Definition 6 (linear controlled grammar) A linear distinguished grammar (LDG) is a quadruple G = (N, I:, R, Ain), where N and I: are finite sets ofnon-terminal and terminal symbols, respectively, A;n E N is the start symbol and where R is a finite set of production rules of the form: A -t ß 1 X!ß2 with A E N, X E NU I:, called the distinguished symbol, ß 1 ,ß 2 E ( N U I:) •, and !",87,88
10581,1827517,a special symbol not in ( N U I;).,2,3
10582,1827517,"by replacing every symbol Y E N U ~ by (Y, t).",3,4
10583,1827517,Reading from the second nested stack is possible if the top of the first one is a bottom of stack symbol of an embedded stack.,20,21
10584,14342868,"Each constant of Σ, i.e., each symbol of rank 0, is a tree.",8,9
10585,14342868,"A context-free tree (CF T ) grammar is a tuple G = (F, Ω, S, P ) where F and Ω are ranked alphabets of non-terminals and terminals, respectively, S ∈ F (0) is the start symbol and P is a finite set of productions of the form F (y 1 , . . . ,",49,50
10586,14342868,AT T s with rules R σ at an input symbol σ in which each outside attribute occurs exactly once are called simple attributed tree transducers.,10,11
10587,14342868,Inspecting the rules of the example grammar it turns out that they exhibit a particular normal form with a terminal symbol as head and a possibly empty string of non-terminals.,20,21
10588,14342868,Context-free tree grammars in which the root of the right-hand side of each rule is labelled with a terminal symbol are in Greibach normal form.,23,24
10589,14342868,"Syn = {0, 1} with α = α m at the root node • Inh = {y} • Every symbol in the derivation trees is assigned one synthesized attribute. •",24,25
10590,14342868,We assume again for simplicity that the terminals of the given grammar G are of arity at most one and that the copy rules introduce a new output symbol id. The set of copy nodes consists of the synthetic and the inherited attribute.,28,29
10591,14342868,"The edge formulas mirror the information transport of rules of the form ( * ) (γ, ρ) → a(γ , ρ ) where (γ, ρ) is an inside, (γ , ρ ) an outside attribute and a an output symbol or the new symbol id. Such an edge formula χ 1,γ,γ (x, y) is the disjunction of all formulas ∃z(lab σ (z) ∧ edge j (z, x) ∨ edge j (z, y)) for all input symbols σ with j, j equal to 0 or 1 and (γ, πj) depending on (γ , πj ) in R σ .",47,48
10592,14342868,"The edge formulas mirror the information transport of rules of the form ( * ) (γ, ρ) → a(γ , ρ ) where (γ, ρ) is an inside, (γ , ρ ) an outside attribute and a an output symbol or the new symbol id. Such an edge formula χ 1,γ,γ (x, y) is the disjunction of all formulas ∃z(lab σ (z) ∧ edge j (z, x) ∨ edge j (z, y)) for all input symbols σ with j, j equal to 0 or 1 and (γ, πj) depending on (γ , πj ) in R σ .",51,52
10593,14342868,"To illustrate the simplest case for a unary output symbol a and a ""synthesized"" copy name γ : If the formula ψ a,γ (x) is true at a node with extended label σ and the only true edge formula is χ 1,γ,γ (x, y) then R σ contains just (γ, ρ) → a(γ, ρ1).",9,10
10594,11244643,"Consider a context-free grammar consisting of a set C of ""categories"" or non-terminal sym~ bols, including one distinguished symbol s (for ""sentence""); a set of terminal symbols 7' (""lexical slots""), none of which are also in C, a lexicon L which is basically a set of words and an indication of the kind of lexical slot each word can fill, and a set of rewrite rules R of form c -+ vl ...vn where c is a symbol in C, and the string on the right hand side vl'""vn consists of one or more symbols in T or C. A sentence is derived by writing s, then rewriting s by the string ul'""Um on the right hand side of any rule in R of form s -+ ul""""Um, then rewriting any ui which is non-terminal by some rule of form ui -+ wl...wp and so on.",25,26
10595,11244643,"Consider a context-free grammar consisting of a set C of ""categories"" or non-terminal sym~ bols, including one distinguished symbol s (for ""sentence""); a set of terminal symbols 7' (""lexical slots""), none of which are also in C, a lexicon L which is basically a set of words and an indication of the kind of lexical slot each word can fill, and a set of rewrite rules R of form c -+ vl ...vn where c is a symbol in C, and the string on the right hand side vl'""vn consists of one or more symbols in T or C. A sentence is derived by writing s, then rewriting s by the string ul'""Um on the right hand side of any rule in R of form s -+ ul""""Um, then rewriting any ui which is non-terminal by some rule of form ui -+ wl...wp and so on.",98,99
10596,11244643,"We will make use of phrase structure tree representation for sentences and their constitue,~t.~. Each symbol appearing in the derivation is represented by a node of the tree; it dominates the constituent of which it is the highest node, and may be used to represent that constituent. (",15,16
10597,11244643,"Furthermore we assume grammatical congruence: there is a one-to-one connection between the rulesRA of :language A and R~ of language B --if RA contains a rule c -+ vl'""vn, then Rs must contain a rule c -~ ul""""un, where each symbol in v]'""vn has its counterpart in ul""""un, and vice-versa, though the order of the terms in one string will not in general correspond to the order of the terms in the other.",48,49
10598,11244643,"Not only must each monolingual sentence be derivable in exactly one way, but each rule may contain any one symbol only once on its right hand side.",20,21
10599,35356939,"As we can see from the Hindi example in the last section, the number of possible symbols in Brahmic scripts is large, due to the combinations of consonants and vowels via diacritics, and the multi-symbol ligatures.",39,40
10600,35356939,The starting point for all such methods is a simple per-symbol alignment of both the input string and the output string.,12,13
10601,35356939,"Thus, for example, the word ""phlegm"" is pronounced F L EH M (again using the ARPAbet representation), and one natural alignment between the grapheme and phoneme sequences is: p:ϵ h:F l:L e:EH g:ϵ m:M For transliteration, we can align a Devanagari script Hindi word (सं स्कृ त) with its romanization (sanskrit or sanskrt), where we make use of the Unicode symbol sequence (स◌ं स◌् क◌ृ त): s:स a:ϵ n:◌ं s:स ϵ:◌् k:क r:◌ृ i:ϵ t:त Note that symbols on either the input or the output may not directly correspond to a symbol on the other side (such as 'a', 'i' and ◌् in the above example), which we represent with an ϵ on the other side of the transduction.",82,83
10602,35356939,"Thus, for example, the word ""phlegm"" is pronounced F L EH M (again using the ARPAbet representation), and one natural alignment between the grapheme and phoneme sequences is: p:ϵ h:F l:L e:EH g:ϵ m:M For transliteration, we can align a Devanagari script Hindi word (सं स्कृ त) with its romanization (sanskrit or sanskrt), where we make use of the Unicode symbol sequence (स◌ं स◌् क◌ृ त): s:स a:ϵ n:◌ं s:स ϵ:◌् k:क r:◌ृ i:ϵ t:त Note that symbols on either the input or the output may not directly correspond to a symbol on the other side (such as 'a', 'i' and ◌् in the above example), which we represent with an ϵ on the other side of the transduction.",135,136
10603,35356939,"It is a bigram model, so the incoming arcs into any state are labeled with the same pair symbol, e.g., state 5 has two incoming arcs, both labeled with k:क.",19,20
10604,35356939,"If these were single symbol pairs, then converting to a transducer would be trivial: just change from a pair (encoded) symbol (k:क) to an input symbol (k) and an output symbol (क).",4,5
10605,35356939,"If these were single symbol pairs, then converting to a transducer would be trivial: just change from a pair (encoded) symbol (k:क) to an input symbol (k) and an output symbol (क).",24,25
10606,35356939,"If these were single symbol pairs, then converting to a transducer would be trivial: just change from a pair (encoded) symbol (k:क) to an input symbol (k) and an output symbol (क).",33,34
10607,35356939,"If these were single symbol pairs, then converting to a transducer would be trivial: just change from a pair (encoded) symbol (k:क) to an input symbol (k) and an output symbol (क).",40,41
10608,35356939,Two arcs leaving state 1 have symbols consisting of multiple input Unicode symbols ('sa' and 'sha'); and one of the arcs leaving state 1 has a symbol consisting of multiple output Unicode symbols (स् which is two symbols स and ◌् ).,33,34
10609,35356939,"In their algorithm, they added an auxiliary phone symbol to the end of each pronunciation that indicated the word identity.",9,10
10610,35356939,"In our case, as seen in Figure 5a , we create a transducer with each input symbol paired with ϵ, followed by an extra arc that pairs an input ϵ with the full pair symbol.",17,18
10611,35356939,"In our case, as seen in Figure 5a , we create a transducer with each input symbol paired with ϵ, followed by an extra arc that pairs an input ϵ with the full pair symbol.",36,37
10612,35356939,"We then encode the transducer (treating the input:output as a a single symbol), determinize it, then decode it again to a transducer, resulting in the transducer in Figure 5b .",15,16
10613,35356939,"Since the words can be inferred from the input symbols, we do not need to store the word labels explicitly; we project L•G to its input labels (with an appropriate word boundary symbol to determine where such boundaries occur), and the decoder outputs concatenations of character label strings.",35,36
10614,35356939,arcs with word labels rather than ϵ -with a linear path corresponding to the word's symbols followed by the word end symbol (here </w>).,22,23
10615,35356939,"These require LM probabilities, which we can extract from L • G by traversing arcs recursively until an arc with the word end symbol is en-countered.",24,25
10616,21719302,"When the FSA F is probabilistic, the result of the Forward algorithm can be interpreted as the marginal probability of all paths through F while consuming x (hence the symbol ""p"").",31,32
10617,21719302,"2010) , we replace low frequency words with a special wildcard symbol.",12,13
10618,235358854,The symbol [•; ] represents a concatenation operation.,1,2
10619,219309905,"IIere Q is an additional symbol e.g. ""initial"" for identifying a specific state of this transducer.",5,6
10620,44135781,"Each l t ∈ l is chosen from L = {roles ∪ nonrole}, where roles are language-specific semantic roles (mostly consistent with PropBank) and nonrole is a symbol to present tokens that are not arguments.",34,35
10621,203902309,"Given a label y, let u(y) be the uncased symbol.",11,12
10622,9057197,"f for the initial hypothesis are set to −1, corresponding to the start of sentence symbol, making it easy to compute the correct orientation for spans at the beginning of the input (with index 0).",16,17
10623,51872436,S ∈ N is the start symbol.,6,7
10624,51872436,S ∈ N is the start symbol.,6,7
10625,51872436,"P is a finite set of productions of the form A → R, R , ∼ , where A ∈ N , R is a hypergraph fragment with edge labels over N ∪ T , and R is a symbol sequence over N ∪ T .",40,41
10626,51872436,"The grammar extraction process repeatedly replaces a subgraph with a nonterminal hyperedge, defining the nonterminal symbol as LHS and the subgraph as RHS.",16,17
10627,6908205,The position of x in y is denoted by the hole symbol '@'.,11,12
10628,10062143,"Hashtags are words proceeded by a pound (""#"") character, mentions by an ""@"" symbol.",20,21
10629,5905489,"So the Addressee is present only in the deep (tectogrammatical) sentence structure (in Figure 1 , it is captured as a small square with the symbol of Addressee ADDR).",28,29
10630,44167998,"We use the symbol ""@"" to denote the number of missing arguments in a predicate.",3,4
10631,44167998,"We use the symbol ""#"" to denote the number of omitted tokens.",3,4
10632,16265930,"The vast majority of previous work on transliteration has considered only lexical features, for example spelling similarity and transliteration symbol mapping, however we build on the inspiration of Li et al. (",20,21
10633,236477950,"Next, we verify the effect of the semantic representation of the type label by treating every label of event type or subtype as a special symbol, not using their word embedding learned in the pretrained language model.",26,27
10634,236477950,The symbol -indicates that the method does not use type label embeddings.,1,2
10635,31952864,The rule(s) used in every step are indicated above the derivation symbol (⇒) combined with the right-hand side index of the used rule (starting at 1).,12,13
10636,7083365,"1   Here, a token is an instantiation of a linguistic entity (e.g., a letter, symbol, sound, word, phrase, or other related entity), and a property is an ostension of language (García-Carpintero, 2004; Saka, 2006) , such as spelling, pronunciation, meaning (for a variety of interpretations of meaning), structure, connotation, or quotative source.",19,20
10637,7083365,"Mention-significant words were a set of 8,735 words and collocations with potential metalinguistic significance (e.g., word, symbol, call), extracted from the WordNet lexical ontology (Fellbaum, 1998) .",21,22
10638,18254763,"Non-terminals are generalized form of phrases, i.e., all possible phrases allowed in the framework of Chiang (2005) are represented by the symbol X. There is another symbol S to start the parse tree.",27,28
10639,18254763,"Non-terminals are generalized form of phrases, i.e., all possible phrases allowed in the framework of Chiang (2005) are represented by the symbol X. There is another symbol S to start the parse tree.",32,33
10640,18254763,"Here are the left side rules:- • X → few has X 1 • X → arrived Also, there is a ""glue rule"" to combine two trees or just derive a non terminal from the start symbol S. • S → S 1 X | X The glue rule is used to start the parsing process.",39,40
10641,233189585,"Zero-shot performances of the other five types, num, sym (symbol), adp, sconj and intj (in-9 https://catalog.ldc.upenn.edu/ LDC2016T10 terjection), are comparable or underwhelmed to those of annotation projection.",14,15
10642,231719393,"In this grammar, an x-word acts as a non-terminal symbol in a phrase structure grammar and can appear as a head or dependent in a dependency tree.",14,15
10643,2133607,"Note that the CTC formulation uses L-length letter sequence C = {c l ∈ U|l = 1, • • • , L} with a set of distinct letters U. Similarly to Section 2.1, by introducing framewise letter sequence with an additional ""blank"" ( < b >) symbol Z = {z t ∈ U ∪ < b >|t = 1, • • • , T }, and by using the probabilistic chain rule and conditional independence assumption, the posterior distribution p(C|X) is factorized as follows: p(C|X) ≈ Z t p(z t |z t−1 , C)p(z t |X) pctc(C|X) p(C) p(Z) (2) As a result, CTC has three distribution components similar to the DNN/HMM case, i.e., framewise posterior distribution p(z t |X), transition probability p(z t |z t−1 , C) 1 , and prior distributions of letter and hidden-state sequences, p(C) and p(Z), respectively.",55,56
10644,2133607,"Softmax(•) is a sofmax activation function, and Lin(•) is a linear layer to convert hidden vector h t to a (|U| + 1) dimensional vector (+1 means a blank symbol introduced in CTC).",35,36
10645,2133607,"Let Ω l be a set of partial hypotheses of the length l. At the beginning of the beam search, Ω 0 contains only one hypothesis with the starting symbol <sos> and the hypothesis score α(<sos>, X) is set to 0.",30,31
10646,2133607,"The score of each new hypothesis is computed in the log domain as α(h, X) = α(g, X) + log p(c|g, X), (9) where g is a partial hypothesis in Ω l−1 , c is a letter appended to g, and h is the new hypothesis such that h = g • c. If c is a special symbol that represents the end of a sequence, <eos>, h is added to Ω but not Ω l , where Ω denotes a set of complete hypotheses.",68,69
10647,2133607,Let γ where we assume that γ (b) 0 (<sos>) = 1 and <b> is a blank symbol.,25,26
10648,198119822,"Mordatch and Abbeel (2018) find that syntactic structure emerges in the stream of symbol uttered by agents, where symbols and syntax can be mapped to specific meanings or instructions.",15,16
10649,21673427,"CTC formulates the conditional probability by introducing a framewise label sequence Z consisting of a label set U and an additional blank symbol defined as Z = {z l ∈ U ∪ {'blank'}|l = 1, • • • , L}: p ctc (Y |O) = Z L l=1 p(z l |z l−1 , Y )p(z l |O), (7) where p(z l |z l−1 , Y ) represents monotonic alignment constraints in CTC and p(z l |O) is the framelevel label probability computed by p(z l |O) = Softmax(Linear(h l )), (8) where h l is the hidden representation generated by an encoder network, here taken to be the encoder of the attention-based encoder-decoder network defined in Eq. (",22,23
10650,233189566,"from statistics of reconstruction errors occurred in our computer experiments , we observed that many reconstruction error events ( i.e. , @xmath97 ) occur due to only one symbol mismatch .",28,29
10651,231802248,"2005) which helps generate contextual representation h i of each token as follows: h c i = [ ← − h i ; − → h i ] ∈ R 2H , where ← − h i and − → h i are hidden states in forward and backward pass of the BiLSTM, symbol ; means concatenation and H is hidden size.",56,57
10652,52167541,"Additionally, p k represents a non-terminal symbol (constituent, phrase) in the target sentence constituency tree. [",9,10
10653,52167541,Each node on the tree represents either a terminal symbol (a word) or a non-terminal symbol (a clause or phrase type).,9,10
10654,52167541,Each node on the tree represents either a terminal symbol (a word) or a non-terminal symbol (a clause or phrase type).,19,20
10655,52167541,SynC creates connections in the tree-structured decoder that pays attention to the structural context of generation of each terminal or non-terminal symbol in the phrase structure tree.,25,26
10656,52167541,"When a nonterminal symbol is expanded into a sub-tree, it's first child will not have a previous sibling to provide fraternal information (S(v) = Null, as in Equ 3).",3,4
10657,235313949,"Part-of-Speech: The part of speech tags with the highest negative association with p gen (i.e. those most likely to be copied) are $ (currency symbols), UH (interjection), # (pound symbol), followed by NNP (singular proper nouns).",43,44
10658,17133526,"The C com ponent is a vector of costs, one per symbol in a. The r component is called the least cost predecessor.",12,13
10659,219302391,It is straightforward to see that this grammar partition definition is more general than the one defined with respect to nonterminal symbol set [10] .,21,22
10660,219302391,"That is, instead of moving from i to i + 1,: the new algorithm will look for the next adjacent symbol to the current node in the lattice.",23,24
10661,219302391,"If we add rule set { S i --+ OJ }j =l to G i , use Si as its start symbol for partition element G i , and label the output node in the lattice with corresponding O} during parsing, the correctness of derivation is still guaranteed.",23,24
10662,59840575,"The modular approach to grammar development leads us away from the traditional view of parsing a string of input symbols as the recognition of some start symbol, and towards a richer and more flexible view where inputs and outputs share the same structural properties.",26,27
10663,235352668,We use a special symbol to delineate article boundaries.,4,5
10664,235352668,"Next, for each cluster, we concatenate all elements of c i similar to B2, and mask out the articles that do not entail y i by the padding symbol; effectively, we use {x (j) i } j∈e i as input to the transformer.",31,32
10665,53083091,"We use this heuristic to augment the score estimation (SCORE): FMH(φ ) = log ν(f ) ν(e) f ∈ V f , e ∈ V e (3) ν(f ) is the percentage relative frequency of the ciphertext symbol f , while ν(e) is the percentage relative frequency of the plaintext token e in the plaintext language model.",42,43
10666,621320,"Our Setup Given a sentence with L words, to which we prepend a root symbol $, let A := { h, m | h ∈ {0, . . . ,",15,16
10667,219304588,0 } Each tuple T/ is a member of the cartesian product a 1 xa 2 x ... xam and represents a possible value combination for the fe atures of the symbol xi.,31,32
10668,219304588,Ambiguities at the feature level happen when a symbol is accompanied by more than one fe ature tuples.,8,9
10669,6859686,"Scan [R; Γ, r = ρ • s α, Φ; Γ] ρ ∈ ρ • s [R; Γ, r = ρ • α, Φ; Γ] When the next symbol to read is a terminal, its range restriction is concatenated with the range for what the row has found so far.",40,41
10670,6859686,"Complete [R; Γ, r = ρ • , {Φ; r = α; Ψ}; Γ] ρ ≤ k ≤ |w| [R; {Γ; r = ρ}, r = (k, k) • α, {Φ; Ψ}; Γ] Whenever a linearization row r is fully traversed, we predict an item for every remaining linearization row r and every remaining input position k. Scan [R; Γ, r = ρ • s α, Φ; Γ] ρ ∈ ρ • s [R; Γ, r = ρ • α, Φ; Γ] If the next symbol in the linearization row is a terminal, its range restriction is concatenated to the range for the partially recognized row.",122,123
10671,6859686,there is an passive item that has found the first symbol in α.,10,11
10672,219300705,"Finally, a child symbol can have multiple shifts and multiple reductions to its right.",4,5
10673,11516540,"these being the name of structural units and denoted by enclosure within angle-brackets, (iii) The disjunction of two or more components, the 'or' being represented by the 'l' symbol, and (iv) The conjunction of two or more components with the 'and' being represented by juxtaposition.",38,39
10674,11865576,"The initial configuration is (ROOT, w) where ROOT is a distinguished non-terminal symbol.",17,18
10675,567820,$ ∈ Σ is a distinguished root symbol; let Σ = Σ ∪ {$}.,7,8
10676,567820,"The decoder was applied iteratively, with each iteration transforming the best (or n-best) output from the previous one until only the root symbol remained.",27,28
10677,195064464,"2011) to aid in the decipherment of the Copiale cipher, where it was able to identify meaningful groups such as word boundary markers as well as signs which correspond to the same plaintext symbol.",35,36
10678,208059118,"The ↓ symbol on a leaf node represents a substitu-tion node which can be expanded by a tree rooted in the same label, e.g. t3 rooted in NP substitutes into the NP↓ node in t46.",2,3
10679,208059118,"The * symbol on the leaf node of a tree t represents an adjunction node (also called a footnode) and signifies that t can be inserted into an internal node of another tree with the same label, e.g. t103 adjoins into the AP node in t46.",2,3
10680,542758,"This assumption corresponds to the independence assumption in PCFG; that is, only a nonterminal symbol of a mother is considered in further processing by ignoring the structure of its daughters.",16,17
10681,542758,The parser can efficiently calculate the Viterbi parse by taking the maximum of the probabilities of the same nonterminal symbol in each cell.,19,20
10682,2310201,"In the first iteration, the chunker identifies two base phrases, (NP Estimated volume) and (QP 2.4 million), and replaces each phrase with its nonterminal symbol and head.",31,32
10683,2310201,The Right-Hand-Side (RHS) of the CFG rule The left-adjacent nonterminal symbol.,18,19
10684,2310201,The right-adjacent nonterminal symbol.,5,6
10685,2310201,"¥ ""£ ¡ ¦ ¡ £ ¡ ¦ ¡ ¢ £ ¡ ¦ ¡ ¢¡ ¦ ¦ where ¡ is a binary output indicating whether the candidate is a phrase of the target type or not, ¥ is the RHS of the CFG rule, ¨is the symbol on the left, and is the symbol on the right.",50,51
10686,2310201,"¥ ""£ ¡ ¦ ¡ £ ¡ ¦ ¡ ¢ £ ¡ ¦ ¡ ¢¡ ¦ ¦ where ¡ is a binary output indicating whether the candidate is a phrase of the target type or not, ¥ is the RHS of the CFG rule, ¨is the symbol on the left, and is the symbol on the right.",58,59
10687,2310201,"Note that the information about the result of the rule application, i.e., the LHS symbol, is considered in this filtering scheme because different naive Bayes classifiers are used for different LHS symbols (phrase types).",16,17
10688,2310201,We use the features on RHSs combined with symbol features.,8,9
10689,53636068,"At each position k, we determine whether to insert a NULL symbol or not by calculating an n-gram score using Eqns (1) and (2).",12,13
10690,53636068,"In the insertion case, for example, if the original word hello is disguised as helo, a NULL symbol is inserted inside the disguised word helo to decipher.",20,21
10691,53636068,The ideal position to insert the NULL symbol is he<NULL>lo but it is not known during training.,7,8
10692,231786398,w 0 ) (5) where w 0 is the beginning-of-word symbol.,16,17
10693,231786398,"We then feed these vectors into a k-layer LSTM h t = LSTM(z t−1 , h t−1 ) (13) where h ∈ R d , h 0 is a vector with all zeros and w 0 is the beginning-of-word symbol.",47,48
10694,231786398,"Our implementation starts similar to the LSTM one, getting embedding vectors z t for each segment in the string w ∈ Σ * , except that we replace segment w t with a MASK symbol.",35,36
10695,231786398,"To get positionspecific surprisal values, we again use a transformer architecture, but instead of replacing a single segment with a MASK symbol, we replace all of them.",23,24
10696,231786398,"We then filtered all non-word tokens-by removing the ones with any symbol not in the language's scripts-and kept only the 10,000 most frequent types in each language.",15,16
10697,231786398,"The end-of-word (EOW) symbol is a special ""segment"" which symbolises the end of a string.",9,10
10698,231786398,"Furthermore, since all realisable strings must eventually end, it will be 16 King and Wedel (2020) indeed present a similar correlation in their Figure 2 present in all words, making it a very frequent symbol-in fact, Tab.",39,40
10699,231786398,"Through the same logic, other segments should also be analysed separately from EOW-or else, lower word final surprisals may be due to this symbol alone.",27,28
10700,231786398,"As such, we analyse the surprisal of LSTM ""language models"" without the EOW symbol here.",16,17
10701,231786398,3 shows the difference between word initial and final positions is considerably reduced when we remove the EOW symbol from the forward surprisal analysis.,18,19
10702,17769145,"Non-terminal symbols are in capitals, the terminal symbol α corresponds to the vocabulary of the training set and g( f .v) is a function which generates integers given the field value f .v.",10,11
10703,17769145,Rule (1) defines the expansion from the start symbol S to the first record R of type start.,10,11
10704,17769145,"R(r i .t) is the source record, R(r j .t) is the target record and FS(r j .start) is a place-holder symbol for the set of fields of record token r j .",27,28
10705,17769145,"Rule (a) defines the expansion from the start symbol D to a sequence of sentences, each represented by the non-terminal SENT .",10,11
10706,17769145,"The weight of rule (a) is simply the joint probability of all the record types present, ordered and segmented appropriately into sentences in the document, given the start symbol.",32,33
10707,17769145,"Once record types have been selected (on a per sentence basis) we move on to rule (b) which describes how each non-terminal SENT expands to an ordered sequence of records R, as they are observed within a sentence (see the terminal symbol '.'",49,50
10708,17769145,"Again, we define a sub-grammar G RST which replaces rules (1)-(2) from Figure 2 : Definition 3 (G RST grammar) G RST = {Σ R , N RST , P RST , D} where Σ R is the alphabet of leaf nodes as defined in Section 5.1, N RST is a set of non-terminals corresponding to rhetorical relations augmented with nucleus-satellite information (e.g., Elaboration[N][S] stands for the elaboration relation between the nucleus EDU left-adjoining with the satellite EDU), P RST is the set of production rules of the form P RST ⊆ N RST × {N RST ∪ Σ R } × {N RST ∪ Σ R } associated with a weight for each rule, and D ∈ N RST is the root symbol.",147,148
10709,235313916,We use the symbol |A| to denote the size of a set.,3,4
10710,3832329,All the numbers were replaced with a NUM symbol.,8,9
10711,3212077,"We represent a lexicalized constituent as a nonterminal symbol followed by a bracket which contains the lexicalizing anchors, e.g. P (H) where H is the anchors lexicalizing P . •",8,9
10712,3212077,We propagate the symbol if it corresponds to an anchor.,3,4
10713,3212077,"To accommodate the lexicalization, we first assign a unique nonterminal symbol for each, i.e. X for monotone reordering and Y for inverse reordering.",11,12
10714,3212077,The shift operation advances the input stream by one symbol and push the symbol into the stack; while the reduce operation applies some rule to the top-most elements of the stack.,9,10
10715,3212077,The shift operation advances the input stream by one symbol and push the symbol into the stack; while the reduce operation applies some rule to the top-most elements of the stack.,13,14
10716,3212077,"In our case, the input stream is the target string of the rule and the symbol is the corresponding source index of the elements of the target string.",16,17
10717,3212077,"Unless the symbol corresponds to an anchor, it tries to apply the reduce operation.",2,3
10718,3212077,"If the symbol being read is a nonterminal, then we push the entire stack that corresponds to that nonterminal.",2,3
10719,227230595,"Additionally, we normalize the usage of punctuation, as there are multiple similar ASCII characters for the same punctuation symbol and there is the possibility of one symbol being equivalent to multiple ones (e.g. "". . . """,20,21
10720,227230595,"Additionally, we normalize the usage of punctuation, as there are multiple similar ASCII characters for the same punctuation symbol and there is the possibility of one symbol being equivalent to multiple ones (e.g. "". . . """,28,29
10721,227230595,"as a single ASCII symbol and ""..."" as three period symbols).",4,5
10722,243840300,"As previously, they must belong to the same part of rhyming scheme, A or B. We make sure to remove any obvious datapoints, such as lines that start a quotation, or corrupted limericks that now end with a non-line ending symbol, such as comma, as a result of the swap.",46,47
10723,247940139,This is possible because ROT-k is a 1:1 cipher where each ciphertext symbol corresponds to a unique plaintext symbol; this means it will preserve distributional features from the plaintext.,14,15
10724,247940139,This is possible because ROT-k is a 1:1 cipher where each ciphertext symbol corresponds to a unique plaintext symbol; this means it will preserve distributional features from the plaintext.,20,21
10725,247940139,"Source: sie ist das symbol all dessen, was wir sind und wozu wir als erstaunlich wissbegierige spezies fähig sind.",5,6
10726,236477357,"We handle separately sentences that have several occurrences of phenomena resulting in discontinuities (+ symbol), we exclude combinations with fewer than 5 occurrences.",15,16
10727,29235606,"The RTG contains one rule for each elementary tree α in the TAG grammar, expanding a nonterminal symbol of the form X S if α is an initial tree with root symbol X, or of the form X A if α is an auxiliary tree (cf.",18,19
10728,29235606,"The RTG contains one rule for each elementary tree α in the TAG grammar, expanding a nonterminal symbol of the form X S if α is an initial tree with root symbol X, or of the form X A if α is an auxiliary tree (cf.",32,33
10729,29235606,"In general, an algebra over the signature Σ of operation symbols is a structure A = (A, I) consisting of a set A (the domain of A) and an interpretation function I. This function assigns to each operation symbol f ∈ Σ of arity k a function I(f ) that takes k arguments from A and returns a value in A. Thus, a term τ over the signature Σ can be evaluated to a value τ recursively by letting f (τ 1 , . . . ,",44,45
10730,29235606,"For any path π ∈ N * , we have a function symbol embi π for initial-tree embedding.",12,13
10731,29235606,"For any path π ∈ N * , we have a function symbol emba π for auxiliary-tree embedding.",12,13
10732,29235606,"Given a feature structure F ∈ F, we let I(emba π ) = EA π with EA π (F ) =   π TOP F .RT.TOP BOT F .FT.BOT   • The binary function symbol unify represents unification of two feature structures.",38,39
10733,29235606,"tree, we let R(α) = RT 1 1 If α is an auxiliary tree with its foot node at address π, we let R(α) =      RT 1 FT 2 1 π 2      In addition to rules that encode elementary trees, the IRTG also contains rules X A → nop X for every nonterminal symbol X. For these symbols, we let h F (nop N ) = c nop , with a constant c nop which evaluates to the feature structure I(c nop ) = F nop =    RT TOP 1 FT BOT 1    Thus, whenever we choose not to adjoin an auxiliary tree at a certain node, the IRTG encodes this with a nop operation.",68,69
10734,29235606,"We can then define rules for FG which simply interpret the function symbols of F: [I(c)] → c for constants c [EI π (F )] → embi π ([F ]) [EA π (F )] → emba π ([F ]) [F G] → unify([F ], [G]) if defined We add a start symbol S FG and rules S FG → [F ] for all feature structures F .",73,74
10735,21687876,"In graph theory, there is isomorphism between the nodes V S of graph S and the nodes V T of graph T if there exists a bijection f (V S ) → V T such that ∀s i , s j ∈ V S , it holds that: s i s j ⇔ f (s i ) f (s j ), where the symbol stands for adjacency between nodes.",69,70
10736,219299738,The A word matches in this rule if the word ends with the symbol 'b' followed by any three symbols followed by the string 'dad'.,13,14
10737,219299738,"1 (PST (Prediction Suffix Tree)) A PST T over an alphabet Ei s a tree of degree I EI-Each edge in the tree is labeled by a single symbol in E such that from every internal node there (0.8, 0.2) (0.3, 0.7) is exactly I :EI edges each one labeled with a different symbol.",34,35
10738,219299738,"1 (PST (Prediction Suffix Tree)) A PST T over an alphabet Ei s a tree of degree I EI-Each edge in the tree is labeled by a single symbol in E such that from every internal node there (0.8, 0.2) (0.3, 0.7) is exactly I :EI edges each one labeled with a different symbol.",66,67
10739,219299738,"The nodes of the tree are labeled by pairs (s, 18 ) where s is the string associated with the walk starting from that node and ending in the root of the tree, and 18 : :E ➔ [O, 1) is the next symbol probability function related with s. It is necessary that fo r every string s labeling a node in the tree, I:u e� 18 (u) = 1.",50,51
10740,219299738,4 (Observation probabilities) The probability of a state s and a symbol a in the learning sample is calculated as the relative frequency of the state s ( number of times the state s appears in the sample divided by the length of the sample) and the relative frequency of the symbol u in the context of the state s (number of times the symbol u appears in the sample aft er the state s divided by the number of times the state s appears in the sample).,13,14
10741,219299738,4 (Observation probabilities) The probability of a state s and a symbol a in the learning sample is calculated as the relative frequency of the state s ( number of times the state s appears in the sample divided by the length of the sample) and the relative frequency of the symbol u in the context of the state s (number of times the symbol u appears in the sample aft er the state s divided by the number of times the state s appears in the sample).,54,55
10742,219299738,4 (Observation probabilities) The probability of a state s and a symbol a in the learning sample is calculated as the relative frequency of the state s ( number of times the state s appears in the sample divided by the length of the sample) and the relative frequency of the symbol u in the context of the state s (number of times the symbol u appears in the sample aft er the state s divided by the number of times the state s appears in the sample).,68,69
10743,219299738,"Therefore, the term ti in equation 5 is a vector of a symbols 2 (a symbol for each attribute).",17,18
10744,30222920,"For convenience, we represent the particular states in the FSA encoding as dotted rules, e.g., S 1 → NP 2 • VP 3 corresponds to the state reached by following the symbol NP 2 on the path (NP 2 , VP 3 , S 1 ).",34,35
10745,30222920,4 The symbol • stands for function composition.,2,3
10746,2968061,"The root declaration has the form root(S , L) and states the start symbol S of the grammar and any linear precedence constraints L constraining the root domain.",14,15
10747,2968061,"Precedence constraints are notated as follows: • Weak precedence: A < B. • Immediate precedence: A B. A pair of elements is considered appropriate when one element in a domain matches the symbol A, another matches B, and neither element dominates the other (it would otherwise be impossible to express an order constraint on a recursive rule).",35,36
10748,2968061,4  Our parsing algorithm begins by seeding the chart with passive edges corresponding to each word in the input and then predicting a compacted instance of the start symbol covering the entire input; each final completion of this edge will correspond to a successful parse.,29,30
10749,2968061,"Thus in the prediction step, the parser considers each rule in the grammar that provides the symbol being predicted, and for each rule, it generates bitmasks for the new edge, taking both rule-level and domain-level order constraints into account.",17,18
10750,29117692,"The start symbol is [(1, n + 1), ∅].",2,3
10751,51868869,Notable approaches are listed as follows and indicated with a symbol for reference in the Experiments and Discussion section (Section 5). #,10,11
10752,13747066,"In addition, we use Y to denote the set of all possible sequences with one and only one eos symbol at the end, and W to denote the set of all possible symbols in a position.",20,21
10753,13747066,"Terminal condition: Secondly, let's consider the value of Q R when emitting an eos symbol to immediately terminate the generation.",17,18
10754,13747066,"2016) to sum over the entire symbol set W, instead of using the single sample estimation often seen in RL.",7,8
10755,35867886,"The only caveat preventing a full equivalence is that a single-rooted DAG automaton simulating a multicounter automaton will contain some extraneous strings, which are, however, clearly labeled by an additional alphabet symbol.",36,37
10756,35867886,"When the symbol ( is read, counter one is increased by one.",2,3
10757,35867886,"Conversely, when the symbol ) or ] is read, the respective counter is decreased by one.",4,5
10758,35867886,The automaton to the right in Figure 1 reads an increase or decrease symbol each time it reads a parenthesis symbol. (,13,14
10759,35867886,The automaton to the right in Figure 1 reads an increase or decrease symbol each time it reads a parenthesis symbol. (,20,21
10760,35867886,"For the remainder of this section, fix an alphabet Σ, and let Σ = Σ ∪ { } where is a special symbol not in Σ. A DAG G = (V, E, in, out, lab) over Σ is special if it has a unique full path with all node labels in Σ and all nodes not on this path are labeled .",24,25
10761,18420081,"Each squared symbol is a distinct Chinese character, and there are no separators between characters calls for Chinese Word Segmentation (CWS) techniques to group adjacent characters into words.",2,3
10762,14043792,The two syntactic paths are then joined using the + symbol to form a single feature.,10,11
10763,6708547,"Chomsky's claim is unfair, because it supposes that analogy applies only on the symbol level.",15,16
10764,6708547,"An example (q with a and u) in Protosemitic is: yaqtilu : yuqtiIu = qatal : qutaI. 4.4 Language-independence/Codedependence Because the present algorithm performs compuration only on a symbol level, it may be applied to any language.",35,36
10765,6708547,"The algorithm does not solve the first two analogies (solutions: ~-~ $ #, hatarokimasu) because it does not solve the elementary analogies, -9:~ = < : ~ and tsu:chi=ku:ki, which are beyond the symbol level r. More generally speaking, the interaction of analogy with coding seems the basis of a frequent reasoning principle: f(A) : f(B) = f(C) : x ~ A : B==_ C : f-t (x) Only the first analogy holds on the symbol level and, as is, is solved by our algorithm, f is an encoding function for which an inverse exists.",45,46
10766,6708547,"The algorithm does not solve the first two analogies (solutions: ~-~ $ #, hatarokimasu) because it does not solve the elementary analogies, -9:~ = < : ~ and tsu:chi=ku:ki, which are beyond the symbol level r. More generally speaking, the interaction of analogy with coding seems the basis of a frequent reasoning principle: f(A) : f(B) = f(C) : x ~ A : B==_ C : f-t (x) Only the first analogy holds on the symbol level and, as is, is solved by our algorithm, f is an encoding function for which an inverse exists.",97,98
10767,2617867,The symbol w denotes the lexical head of an item; the symbol c denotes the constituent label of an item; the symbol t is the POS of a lexical head; u denotes unary child; s i ll denotes the left child of s i 's left child.,1,2
10768,2617867,The symbol w denotes the lexical head of an item; the symbol c denotes the constituent label of an item; the symbol t is the POS of a lexical head; u denotes unary child; s i ll denotes the left child of s i 's left child.,12,13
10769,2617867,The symbol w denotes the lexical head of an item; the symbol c denotes the constituent label of an item; the symbol t is the POS of a lexical head; u denotes unary child; s i ll denotes the left child of s i 's left child.,23,24
10770,2617867,"The symbol g s and g e denote the next level constituent in the s-type hierarchy and e-type hierarchy, respectively.",1,2
10771,14131495,The symbol # denotes incorrect attributes.,1,2
10772,14131495,The symbol * is attached to the attributes that are not aggregated into their synonyms.,1,2
10773,227905199,"In our annotation scheme, tokens with multiple PoS tags are joined with the ""+"" symbol: for example, VERB+INF (an infinitive marker).",17,18
10774,46988470,"c n ], ∅), meaning that the stack and the partial graph are initially empty, and the cache is filled with m occurrences of the special symbol $.",30,31
10775,46988470,The symbol Σ a is the set of all actions.,1,2
10776,18253183,"We study the following decision problems, where the symbol P is used to indicate what the parameter is: • P-LCFRS(j)-MEMBERSHIP, where j ∈ N is the membership problem for LCFRS(j), parameterized by the rank. •",9,10
10777,18253183,"These intermediary configurations are in H serialized into strings of nonterminals, with a ""nonterminal marker"" symbol in each position where a nonterminal is referred to (i.e., H generates a symbol stating ""the ith string generated by instance j of the nonterminal A goes here"").",18,19
10778,18253183,"These intermediary configurations are in H serialized into strings of nonterminals, with a ""nonterminal marker"" symbol in each position where a nonterminal is referred to (i.e., H generates a symbol stating ""the ith string generated by instance j of the nonterminal A goes here"").",34,35
10779,227905528,A complete parser of the Sanskritam specification based on a computational formal grammar starting from its start symbol will be discussed in future.,17,18
10780,5135260,"Formally, an RNNG is a tuple N, Σ, R, S, Θ , where N is the set of nonterminals, Σ is the set of terminals, R is a set of top-down transition-based rules, S is the start symbol and Θ is the set of model parameters.",49,50
10781,21665298,"Specifically, the layer consists of a forward GRU and a backward GRU for the preceding and following contexts of a stock symbol, s, respectively.",22,23
10782,21665298,"The stock symbol is regarded as the last unit in both the preceding and the following contexts where the hidden values, − → h l , ← − h l , are averaged to acquire the message embedding m. Gathering all message embeddings for the tth trading day, we have a mes-sage embedding matrix M t ∈ R dm×K .",2,3
10783,51879089,"Introduction Greibach normal form (GNF; Greibach, 1965) is an important construction in formal language theory which allows every context-free grammar (CFG) to be rewritten so that the first character of each rule is a terminal symbol.",43,44
10784,51879089,"Background SCFG An SCFG is a tuple G = (N, Σ, P, S) where N is a finite nonterminal alphabet, Σ is a finite terminal alphabet, S ∈ N is a distinguished nonterminal called the start symbol, and P is a finite set of synchronous rules of the form (1) must be linked to exactly one nonterminal in α 2 , and vice versa.",43,44
10785,51879089,"STAG An STAG (Shieber, 1994) is a tuple G = (N, Σ, T, S) where N is a finite nonterminal alphabet, Σ is a finite terminal alphabet, S ∈ N is a distinguished nonterminal called the start symbol, and T is a finite set of synchronous tree pairs of the form (4) t 1 , t 2 where t 1 and t 2 are elementary trees as defined in Joshi et al. (",47,48
10786,51879089,2013) has shown that it is useful for the target side of a synchronous grammar to start with a terminal symbol.,21,22
10787,51879089,"For this reason, we define a synchronous grammar to be prefix lexicalized when the leftmost character of the target side 2 of every synchronous production in that grammar is a terminal symbol.",32,33
10788,51879089,"In i, k + m = n + p because the final derived strings must contain an equal number of bs, and n ≥ 1 because G is prefix lexicalized; in ii the constraints on q, r and s follow likewise from L. iii follows from the fact that, in order to pump on the cycle in ii, this cycle must be reachable from the start symbol.",72,73
10789,51879089,"Prefix lexicalizing the grammar would force it to generate some terminal symbol in the target string at each step of the derivation, making it unable to generate the original language where a source string may be unboundedly longer than its corresponding target.",11,12
10790,51879089,"A TTLD is a derivation of the form in Figure 2 , where the leftmost nonterminal in the target string is expanded until it produces a terminal symbol as the first character.",27,28
10791,51879089,The terminal and nonterminal alphabets and start symbol are unchanged.,7,8
10792,227915051,The symbol ⊗ represents the concatenation of tensors.,1,2
10793,1235981,"Pinyin is originally designed as the phonetic symbol of a Chinese character, using Latin letters as its syllable notation.",7,8
10794,249204459,We denote row-wise lack of significant differences with the same color or symbol.,14,15
10795,43982267,"Because of that, they do not suffer from the symbol grounding problem (Harnad, 1999) , and can generalize better than classical symbolic approaches.",10,11
10796,51869205,All the remaining words are replaced by a special token <UNK> symbol.,13,14
10797,3033303,"To normalize the data, words with more than two repeated letters, symbol strings of more than one repeated punctuation symbols or emojis are shortened, for example, '!!!!'",13,14
10798,44083182,"We introduce Latent Vector Grammars (LVeGs), a new framework that extends latent variable grammars such that each nonterminal symbol is associated with a continuous vector space representing the set of (infinitely many) subtypes of the nonterminal.",21,22
10799,44083182,"In addition, what a symbol represents may subtly depend on its context, and a continuous vector representation has the potential of representing each instance of the symbol in a more precise manner.",5,6
10800,44083182,"In addition, what a symbol represents may subtly depend on its context, and a continuous vector representation has the potential of representing each instance of the symbol in a more precise manner.",28,29
10801,44083182,"Model Definition A latent vector grammar is defined as a 5-tuple G = (N, S, Σ, R, W ), where N is a finite set of nonterminal symbols, S ∈ N is the start symbol, Σ is a finite set of terminal symbols such that N ∩Σ = ∅, R is a set production rules of the form X γ where X ∈ N and γ ∈ (N ∪ Σ) * , W is a set of rule weight functions indexed by production rules in R (to be defined below).",43,44
10802,44083182,"Then the expected count (or posterior) of A BC, i, k, j is calculated as: q(A BC, i, k, j) = s(A BC, i, k, j) s I (S, 1, n) , (9) where s I (S, 1, n) is the inside score for the start symbol S spanning the whole sentence w 1:n .",70,71
10803,1423969,"We use σ to denote an arbitrary symbol in Σ. The set of all finite strings over Σ is denoted by Σ , where we write λ for the empty string.",7,8
10804,1423969,"Notationally, for outsides the semi-colon between two strings, i.e. x; z, will simbolize a cut where we can insert an inside string y. Finally, we note that Probabilistic Context Free Grammars (PCFG) are a special case of WCFG where: w (i) is the probability to start a derivation with non-terminal i; w R (i → j k) is the conditional probability of rewriting nonterminal i into j and k; w T (i → σ) is the probability of rewriting i into symbol σ; i w (i) = 1; and for each i ∈ V , j,k w R (i → j k) + σ w T (i → σ) = 1.",101,102
10805,25436150,"Whether p ends with the special [w 1 ] symbol cosine(bLS([w 2 ], p, [w 1 ]) 2 , V p pi ) • pp i where V p pi is the biLSTM encoding of the predicted paraphrase computed in Equation 1 and pp i 4 https://www.cs.york.ac.uk/semeval-2013/task4 is its confidence score.",10,11
10806,59863706,The lexicalization imposes that each elementary tree contains at least one lexical terminal symbol called the anchor.,13,14
10807,59863706,"Connection driven parsing technics Lexicalized Tree Grammars A Thee Adjoining Grammar is specified as a quintuplet G = (E, NT , I , A , S), where E is a set of terminal symbols, NT the disjoint set of nonterminals including the start symbol S, and where I and A are two finite sets of trees called respectively initial trees and auxiliary trees.",48,49
10808,249204499,"The backtick (`) and dash symbol (-) show up as the best unigram features indicating HT, but are not present after the punctuation is normalized.",7,8
10809,16528760,We will use the notation ai to indicate the ith terminal symbol in the input string .,11,12
10810,16528760,"Note that in Rule 6, the top-down left-corner check on the mother of the proposed incomplete edge and the bottom-up left-corner check on the symbol immediately to the right of the dot in the proposed incomplete edge are independent of each other, and therefore could be performed in either order.",33,34
10811,16528760,"Replace the set of rules A ➔ a /3 1 , ... , A ➔ af3 n with A ➔ aA', A' ➔ /3 1 , ... , A' ➔ f3 n , where A' is a new nonterminal symbol.",44,45
10812,16528760,"Replace the set of rules A ➔ a /3 1 , ... , A ➔ af3n with A ➔ aA', A' ➔ (3 1 , ... , A' ➔ f3 n , where A' is a new nonterminal symbol.",44,45
10813,16528760,"There is another grammar transformation that seems better suited to LC parsing, introduced by Griffiths and Petrick (1965), but apparently neglected since: For each grammar symbol X, let a be the longest nonempty sequence such that there is more than one grammar rule of the form A ➔ X af3.",30,31
10814,16528760,"Replace the set of rules A 1 ➔ X a/31 , ... , A n ➔ X af3n with A' ➔ X a, A1 ➔ A' /31 , ... , A n ➔ A' f3 n , where A' is a new nonterminal symbol.",48,49
10815,16528760,Conclusions Probably the two most significant results of this investigation are the discoveries that: • LC chart parsing incorporating both a top-down left-corner check on the mother of a proposed incomplete edge and a bottom-up left-corner check on the symbol immediately to the right of the dot in the proposed incomplete edge is substantially fas ter if the bottom-up check is performed before the top-down check . •,48,49
10816,241583554,"Some phones from different languages shared the same IPA symbol (for example, Turkish and French [i]).",9,10
10817,219308772,The variables a and fl can be calculated according to the following recursion equations (assuming a start and an end symbol at rank t := 0 and t = T+I):,21,22
10818,14910889,"When an expression matches, the hardware can output a constant to the CPU, output the span matched, push a symbol onto the stack, pop from the stack, or halt.",22,23
10819,13165418,The words between the # symbol refers to a relevant topic and the symbol @ means a mention or reply.,5,6
10820,13165418,The words between the # symbol refers to a relevant topic and the symbol @ means a mention or reply.,13,14
10821,218718982,"Additionally, we add a special token to indicate whether a symbol is a word character or a feature.",11,12
10822,219304993,"Parser actions arise as before, but are executed by relativising them with respect to the incomplete item participating in the action, and passing this relativised parser action as the next input symbol for the automaton referenced by that item.",33,34
10823,219304993,The Recognition Phase This section illustrates a simple bottom-up parsing algorithm that makes use of minimized automata produced from sets of trees that anchor the same input symbol.,29,30
10824,219304993,"The indices l, l', #, r are positions between input symbols (position 0 is before the first input symbols and position n is after the final input symbol) and we use Wp,p, to denote that substring of the input w between positions p and p'.",32,33
10825,219304993,"Roughly speaking, an item (T, q, [I, r, l', r]) is ineluded in I when for every 7 E T, anchored by some ak (where I < k _< r and if l' #then k < l' or r' _< k); q is a state in Qk, such that some elementary subcomputation reaching q from the initial state, qk, of Mk is an initial substring of the elementary computation for 7 that reaches the elementary address 7/i, the subtree rooted at 7/i spans Wt,r, and ifT/i dominates a foot node then that foot node spans We,r,, otherwise l' = r' = -The input is accepted if an item (T, qf,[O,n, , ] ) is added to I where T contains some initial tree rooted in the start symbol S and ql E Fk for some k. When adding items to I we use the procedure add(T, q, [l, r, l', r']) which is defined such that if there is already an entry (T', q, [l, r, l', r']) E I for some T' then replace this with the entry (T U T', q, [l, r,l', rq)6; otherwise add the new entry (T, q, [/, r, l', r']) to I. I is initialized as follows.",172,173
10826,202577673,"Generally, inference and training proceed in an auto-regressive manner, namely, the next decoded symbol is predicted by a locally normalized conditional distribution (the ""softmax"").",18,19
10827,202577673,"x i 1 , C) is a normalized conditional probability over the next symbol of the sequence, computed by a neural network (NN) with parameters ⌘.",14,15
10828,5354393,"Introduction Chinese sentences contain sequences of characters that are not delimited by white spaces or any other symbol used for word identification, so Chinese word segmentation (CWS) is one of the fundamental issues in Chinese natural language processing studies.",17,18
10829,219302964,The @ symbol represents k application of the right subtree to the left.,2,3
10830,233189633,"A first illustration of this is traditional shiftreduce dependency parsing, defined by the transitions in Table 1 , here without labels, or alternatively, one may consider there to be only one such 2 There is a close connection to bilexical context-free grammars (Eisner and Satta, 1999) , on the basis of which one may alternatively choose to refer to such a label as a 'delexicalized stack symbol', in a kind of lexicalized pushdown automaton.",75,76
10831,233189633,"With the shift-before-reduce policy, if the candidate transition is reduce, then the first symbol of the remaining input becomes n p (p for 'policy').",19,20
10832,234797920,"Conceptually, we first compute the global sentiment vector v a by averaging the word embeddings of A s , and then use it to weight each word embedding v s i in s as follows: v a = 1 m v a i ∈As v a i , (5) o i = v s i • U • v a , (6) where the symbol • stands for element-wise dot product, while U ∈ R d×d is the transformation matrix (i.e., to be learned during training) between the global sentiment vector v a and the input sentence s. Next, we apply a softmax layer to yield a non-negative weight for each word in s as follows: α s i = exp(o i ) n j=1 exp(o j ) , (7) where the value of α s i can be read as the probability of each word in the sentence s being a sentiment word.",70,71
10833,219299924,"7)£ -an interpretation Z is a function that assigns to each concept symbol (the set A) a subset of the domain A, 27 : A -4 2 A, to each role symbol (the set P) a binary relation of A, Z : P ~ 2 AxA, and to each individual symbol (the set I) an element of A, I : I -9 A. Concept terms and role terms are defined inductively.",12,13
10834,219299924,"7)£ -an interpretation Z is a function that assigns to each concept symbol (the set A) a subset of the domain A, 27 : A -4 2 A, to each role symbol (the set P) a binary relation of A, Z : P ~ 2 AxA, and to each individual symbol (the set I) an element of A, I : I -9 A. Concept terms and role terms are defined inductively.",35,36
10835,219299924,"7)£ -an interpretation Z is a function that assigns to each concept symbol (the set A) a subset of the domain A, 27 : A -4 2 A, to each role symbol (the set P) a binary relation of A, Z : P ~ 2 AxA, and to each individual symbol (the set I) an element of A, I : I -9 A. Concept terms and role terms are defined inductively.",58,59
10836,219299924,"Its semantics is given by a special interpretation function Zh for each hypothesis h, which is applied to each concept and role symbol in the canonical way: Zh : A --+ 2A; Zh : P -+ 2 AxA. Notice that the instances a, b are interpreted by the interpretation function 27, because there exists only one domain A. Only tile interpretation of the concept symbol C and the role symbol R may be different in each hypothesis h. Assume that we want to represent two of tile four concept hyt)otheses that call be derived from (p3), viz.",23,24
10837,219299924,"Its semantics is given by a special interpretation function Zh for each hypothesis h, which is applied to each concept and role symbol in the canonical way: Zh : A --+ 2A; Zh : P -+ 2 AxA. Notice that the instances a, b are interpreted by the interpretation function 27, because there exists only one domain A. Only tile interpretation of the concept symbol C and the role symbol R may be different in each hypothesis h. Assume that we want to represent two of tile four concept hyt)otheses that call be derived from (p3), viz.",69,70
10838,219299924,"Its semantics is given by a special interpretation function Zh for each hypothesis h, which is applied to each concept and role symbol in the canonical way: Zh : A --+ 2A; Zh : P -+ 2 AxA. Notice that the instances a, b are interpreted by the interpretation function 27, because there exists only one domain A. Only tile interpretation of the concept symbol C and the role symbol R may be different in each hypothesis h. Assume that we want to represent two of tile four concept hyt)otheses that call be derived from (p3), viz.",74,75
10839,6863735,"PCFG Induction A PCFG consists of production rules (of the form Ax), each indicating that a nonterminal symbol A (a parent symbol) is replaced with a sequence x of symbols (child symbols) with a predefined probability.",20,21
10840,6863735,"PCFG Induction A PCFG consists of production rules (of the form Ax), each indicating that a nonterminal symbol A (a parent symbol) is replaced with a sequence x of symbols (child symbols) with a predefined probability.",25,26
10841,6863735,"The probability of each rule is determined by maximumlikelihood estimation (MLE), which divides the total number of the occurrences of the rule in training parse trees by the total number of the occurrences of its parent symbol in training parse trees.",39,40
10842,6863735,"We build two sets of non-lexical rules, one generating positive sequences and another generating negative sequences, together with the following two non-lexical rules, where ""S"" stands for the start symbol and ""Positive"" and ""Negative"" symbols are the ones to be expanded into positive and negative sequences, respectively. (",38,39
10843,6863735,"2007) , by adding production rules, to be called lexical item production rules, that replace lexical symbols with a sequence of terminal symbols, such as tokens and dependencies (e.g., ""To-kensToken Token*""), and by labeling lexical symbols as an adaptor symbol, whose expansion to terminal symbols is collected during learning.",52,53
10844,6863735,"For each lexical symbol, we remove the least probable lexical items whose occurrences form a predefined percentage 3 of the occurrences of the lexical symbol.",3,4
10845,6863735,"For each lexical symbol, we remove the least probable lexical items whose occurrences form a predefined percentage 3 of the occurrences of the lexical symbol.",25,26
10846,233189518,"In Figure 1 , this is the 5-star symbol on the central-left part; it represents the reviewrating attribute.",10,11
10847,233189518,"The symbol † marks a statistical significant difference with a paired t-test (α = 0.05, for both Attribute and Product categories the p-value is ∼ 0.009).",1,2
10848,219302714,"There is a word or phrase (wof) of category s stretching from point A to point Din the string if, for some points B and C between A and D, there is a phrase of category np from A to B, of category aux from B to C, and of category vp from C to D. A terminal symbol, say dog, is recognized as belonging to category n by virtue of the clause: wo f (dog , [dog I X] , X) .",64,65
10849,219302714,A simple way to construct a weakend version of a grammar is to construct a partition that symbols and to map each class in the partition onto a single symbol.,29,30
10850,219302714,"Ano ther wo uld be to eliminate the distinctions made by X-bar theory, collapsing, for example, N, N-bar, and NP onto the same symbol.",32,33
10851,219302714,Note also that the start symbol S[$] is a lexicalized nonterminal.,5,6
10852,219302714,"Linear indexed grammars associate a stack of indices with each non-terminal symbol, with the restriction that the indices stack of the head non-terminal of each pro duction (the fa ther) can be inherited by at most one body non-terminal (the dependent child) while the other stacks must have a bounded stack size .",13,14
10853,219302714,"Linear Indexed Grammars A linear indexed grammar is a tuple (Vr, V N , Vi, P, S), where Vr is a finite set of terminals, V N a finite set of non-terminals, Vi is a finite set of indices, SE V N is the start symbol and Pisa finite set of productions.",56,57
10854,219302714,A CYK-like algorithm generalized for linear indexed grammar with productions manipulating more than one symbol at the top of the indices stacks is described in [17] .,16,17
10855,219302714,"Vijay-Shanker and Weir [18] try to solve this problem by defining a non-deterministic finite state automaton that determines if a given LIGed forest symbol (A, i, j)[a:] derives a string of terminals.",29,30
10856,219302714,"In fact, a v' i symbol is equivalent to a dotted production with the dot just before the non-terminal Ai + I or with the dot at the end of the right-hand side in the case of v' m • It is interesting to remark that the set of non-terminals is a subset of the set of items for CYK like and bottom-up Earley-like algorithms, and Earley-like algorithms without the VPP.",7,8
10857,219302714,"Positive Range Concatenation Grammars Ranges & Bindings If we consider a derivation in a CFG, headed at the start symbol and leading to some sentence, we know that each nonterminal occurrence is responsible for the generation of the substring laying between two indexes say i and j. For a given input string w = a 1 ••• a n , such a couple (i, j) is called a range.",20,21
10858,219302714,"In the sequel terminal symbol in T are denoted by early occurring lower case letters such as a, b, c, .",4,5
10859,219302714,"1 Multiple context-free grammars are non-concatenative in the sense that a non-terminal symbol in this grammar can domi nate a sequence of strings of terminal symbols, rather than just one string, as in the case of ordinary context-free grammars.",18,19
10860,219302714,"Each of the strings dominated by a non-terminal symbol in a multiple context free grammar will be a substring of a sentence whose der ivation includes this non-terminal, but in the sentence these strings are not necessarily adjacent.",10,11
10861,219302714,"We only use the first letter of each label, resulting in 15 nonterminal labels (including a new start symbol).",20,21
10862,219302714,"In many cases, a portion of the input sentence may be reduced to a non-terminal symbol in many different ways, when considering different subsets of the input that may be skipped.",18,19
10863,219302714,The lexicalization imposes that each elementary tree contains at least one lexical terminal symbol called the anchor.,13,14
10864,219302714,"Connection driven parsing technics Lexicalized Tree Grammars A Thee Adjoining Grammar is specified as a quintuplet G = (E, NT , I , A , S), where E is a set of terminal symbols, NT the disjoint set of nonterminals including the start symbol S, and where I and A are two finite sets of trees called respectively initial trees and auxiliary trees.",48,49
10865,219302714,We will use the notation ai to indicate the ith terminal symbol in the input string .,11,12
10866,219302714,"Note that in Rule 6, the top-down left-corner check on the mother of the proposed incomplete edge and the bottom-up left-corner check on the symbol immediately to the right of the dot in the proposed incomplete edge are independent of each other, and therefore could be performed in either order.",33,34
10867,219302714,"Replace the set of rules A ➔ a /3 1 , ... , A ➔ af3 n with A ➔ aA', A' ➔ /3 1 , ... , A' ➔ f3 n , where A' is a new nonterminal symbol.",44,45
10868,219302714,"There is another grammar transformation that seems better suited to LC parsing, introduced by Griffiths and Petrick (1965), but apparently neglected since: For each grammar symbol X, let a be the longest nonempty sequence such that there is more than one grammar rule of the form A ➔ X af3.",30,31
10869,219302714,"Replace the set of rules A 1 ➔ X a/31 , ... , A n ➔ X af3n with A' ➔ X a, A1 ➔ A' /31 , ... , A n ➔ A' f3 n , where A' is a new nonterminal symbol.",48,49
10870,219302714,Conclusions Probably the two most significant results of this investigation are the discoveries that: • LC chart parsing incorporating both a top-down left-corner check on the mother of a proposed incomplete edge and a bottom-up left-corner check on the symbol immediately to the right of the dot in the proposed incomplete edge is substantially fas ter if the bottom-up check is performed before the top-down check . •,48,49
10871,219302714,"Definition 4 (Discontinuous Phrase Structure Grammar) A Discontinuous Phrase Structure Grammar (DPSG) is a quadruple (VN, Vr ,S, R), where • VN is a fi.nite set of nonterminal symbols; • Vr is a fi.nite set of terminal symbols; Jet V denote V N U Vr ; • S E V N is the distinguished start symbol of the grammar; • Ri sa fi.nite set of rules X ➔ y l ,c i y 2 , c 2 ••• y n,cn ' where X E VN , Y 1 ' ... ' y n E V, and Ci E {O, 1} (1 � i � n) indicates whether y i is a context daughter in the rule.",67,68
10872,219302714,"To this end, each edge is additionally asso ciated with two bit-strings (fields covers and ctxt_covers ) Later on, it also builds the edge [O, 4, S ➔ P • Q f, 110100, 000000] , which is called active since it still needs to be combined with consd.tuents for Q and f. Notice that the input symbol c is not yet dominated by a direct or a context daughter of the edge .",67,68
10873,219302714,The left-hand side category of ie (ie.ihs) must match the symbol to the right of the dot on ae's right-hand side (ae.next_cat) .,14,15
10874,219302714,"Furthermore , in order to ensure that the constituents corresponding to the right-hand side symbols of ae form an adjacency sequence, the next edge to be combined with ae has to start at the position of the first terminal symbol within ae's span that is neither dominated by a direct nor by a context daughter of ae.",42,43
10875,219302714,The algorithm maintains a set of accumulators c> n (Y) for each symbol YE V and each node n in the parse tree .,15,16
10876,219302714,"After the values for all & b, b(X i ) have been computed, the probability of the MPP is that of the accumulator for parse trees headed by the start symbol of the input grammar (X 1 ) that dominate all terminal symbols in the input (b = 1 T , b = o T ).",32,33
10877,219302714,"Finally, when the agenda is empty, the edge that is headed by the start symbol of the grammar and that spans the entire input represents the Most Probable Parse for the given input sentence.",16,17
10878,219302714,goto's are traditionally made on a symbol basis.,7,8
10879,219302714,"To refer to the symbol (label) of a node n, we use symb(n).",4,5
10880,219302714,"f is used to label anchors to represent the absence of a terminal symbol (the ""empty"" label).",13,14
10881,219302714,"To define a parsing table for a grammar G with goal symbol S, we first extend G by adding one new tree called start, with two nodes: the root, labeled with a fresh symbol ST, marked NA; and ST 's single child, a substitution node labeled S. Then, let I be the set of all equivalence classes of items of G under �-Let N be the set of symbols, T � N be the set of symbols that appear in some anchor, and N the set of non-negative integers.",11,12
10882,219302714,"To define a parsing table for a grammar G with goal symbol S, we first extend G by adding one new tree called start, with two nodes: the root, labeled with a fresh symbol ST, marked NA; and ST 's single child, a substitution node labeled S. Then, let I be the set of all equivalence classes of items of G under �-Let N be the set of symbols, T � N be the set of symbols that appear in some anchor, and N the set of non-negative integers.",37,38
10883,219302714,"Formally a CFG is a quadruple g = (V N, Vr , R, S), where VN is the set of all nonterminals, Vr the set of terminals, R is the set of rules and S denominates the starting symbol that has to cover the analysed sentence.",45,46
10884,219302714,"An algebra A is a pair (A, F) where A is a nonvoid set (the carrier of A) and Fis a fam ily of finitary operations f: A n ➔ A. An n-ary nondeterministic operation is a set-valued function f: A n ➔ P(A) , where P(A) denotes the powerset of A. We use the notation f(a1 (N, ""E, Q, P, S, h, o) where (N, ""E, P, S, h ) is a headed CFG, Q is a finite stack alphabet and o is a function that assigns each production p E P a stack operation push q or pop q (where q E Q) if the head of p is a nonterminal symbol, and nop otherwise.",144,145
10885,219302714,4 (Observation probabilities) The probability of a state s and a symbol a in the learning sample is calculated as the relative frequency of the state s ( number of times the state s appears in the sample divided by the length of the sample) and the relative frequency of the symbol u in the context of the state s (number of times the symbol u appears in the sample aft er the state s divided by the number of times the state s appears in the sample).,13,14
10886,219302714,4 (Observation probabilities) The probability of a state s and a symbol a in the learning sample is calculated as the relative frequency of the state s ( number of times the state s appears in the sample divided by the length of the sample) and the relative frequency of the symbol u in the context of the state s (number of times the symbol u appears in the sample aft er the state s divided by the number of times the state s appears in the sample).,54,55
10887,219302714,4 (Observation probabilities) The probability of a state s and a symbol a in the learning sample is calculated as the relative frequency of the state s ( number of times the state s appears in the sample divided by the length of the sample) and the relative frequency of the symbol u in the context of the state s (number of times the symbol u appears in the sample aft er the state s divided by the number of times the state s appears in the sample).,68,69
10888,219302714,"Therefore, the term ti in equation 5 is a vector of a symbols 2 (a symbol for each attribute).",17,18
10889,219302714,It is straightforward to see that this grammar partition definition is more general than the one defined with respect to nonterminal symbol set [10] .,21,22
10890,219302714,"That is, instead of moving from i to i + 1,: the new algorithm will look for the next adjacent symbol to the current node in the lattice.",23,24
10891,219302714,"If we add rule set { S i --+ OJ }j =l to G i , use Si as its start symbol for partition element G i , and label the output node in the lattice with corresponding O} during parsing, the correctness of derivation is still guaranteed.",23,24
10892,219302714,"In the case of generative approaches, this amounts to finding a derivation from a distinguished symbol to this input.",16,17
10893,219302714,"The C com ponent is a vector of costs, one per symbol in a. The r component is called the least cost predecessor.",12,13
10894,219302714,Ambiguities at the feature level happen when a symbol is accompanied by more than one fe ature tuples.,8,9
10895,219302714,"non-terminal A, and there is a production A ➔ X 1 X 2 X3 •.. X n in the grammar, then the automaton must have a path labeled X 1 X 2 X 3 ... X n starting at q 0 • This is usually represented by saying that each state in the path contains a ""dotted item"" for the proq.uction, starting with A ➔ • X 1 X 2 X 3 ... X n at q0 , with the dot moving one symbol ahead at each state in the path.",91,92
10896,219302714,The dot being in front of a symbol Xi represents the fact that we expect to see the expansion of Xi in the string.,7,8
10897,219302714,"Although reduce and bpack actions do not actually depend on a terminal symbol for the current algorithm, in practice the table could be so constrained, either by having the algorithm extended to an LR ( l) version or by using empirical evidence to reduce/resolve conflicts.",12,13
10898,219302714,"More precisely, an element of the stack is a pair (X, q), where q is a state of the parsing table, and X is either a grammar symbol or another stack.",33,34
10899,219302714,"Two operations are defined over it : look, which returns the leftmost symbol of input or $ if input is the null sequence; and advance, which removes the leftmost symbol from input.",13,14
10900,219302714,"Two operations are defined over it : look, which returns the leftmost symbol of input or $ if input is the null sequence; and advance, which removes the leftmost symbol from input.",32,33
10901,219302714,"I, D ) is correct then a string w is in the language of Giff an item representing a G_ -derivation of w from the start symbol of G is valid.",28,29
10902,219302714,"By definition, w E L( G) iff there is a derivation of w from some start symbol of G. An element in A that represents a derivation of a string w E :E* from a start symbol is called a complete ( augmented} derivation tree.",19,20
10903,219302714,"By definition, w E L( G) iff there is a derivation of w from some start symbol of G. An element in A that represents a derivation of a string w E :E* from a start symbol is called a complete ( augmented} derivation tree.",41,42
10904,219302714,"We borrow a practical notation for trees from [7] : (A """"4 /3) denotes an arbitrary tree with root symbol A and yield (i.e., sequence of labels on the leaves, from left to right) f3 (possibly of height 0, in which case f3 = A). (",25,26
10905,219302714,A ➔ /3) denotes the unique tree of height 1 with root symbol A and yield /3.,13,14
10906,219302714,"A headed context-free grammar G is a tuple (N, �, P, S, h) such that (N, :E, P, S) is a CFG without € productions, where N, �,P are finite sets of nonterminal symbols, terminal symbols and productions, respectively, S is a start symbol and h : P ➔ N is a function that assigns each production p = A ➔ f3 a position 1 � h(p) � 1 /3 1-The h(p)-th symbol in f3 is called the head of p (for simplicity it is assumed that the same production cannot occur twice with different heads).",64,65
10907,219302714,"A headed context-free grammar G is a tuple (N, �, P, S, h) such that (N, :E, P, S) is a CFG without € productions, where N, �,P are finite sets of nonterminal symbols, terminal symbols and productions, respectively, S is a start symbol and h : P ➔ N is a function that assigns each production p = A ➔ f3 a position 1 � h(p) � 1 /3 1-The h(p)-th symbol in f3 is called the head of p (for simplicity it is assumed that the same production cannot occur twice with different heads).",96,97
10908,219302714,"First, observe that if two admissible buHC-LIG trees are congruent then they must have the same symbol Q on top of the stacks at their root nodes, because of the buHCA operation.",19,20
10909,219302714,A word matches in this rule if the word ends with the symbol 'b' followed by any three symbols followed by the string 'dad'.,12,13
10910,219302714,"SHIFT(</>) moves 0 by one token to the right, i.e. SHIFT(a 0 /3) = bi, = a.NEXT(/3) 0 (/3 \ NEXT(/3))} where NEXT(</>) returns a set of all alternatives for the next symbol (Gorn address) .",45,46
10911,219302714,"Finally, a child symbol can have multiple shifts and multiple reductions to its right.",4,5
10912,17779249,Note also that the start symbol S[$] is a lexicalized nonterminal.,5,6
10913,59681682,"A string a belongs to category C if either, a consists of the single symbol C, or there is a rule C ➔ c 1 ••. e n and a is the concatenation of strings that are phrases of categories c 1 ... e n , in that order.",15,16
10914,59681682,"The root is named for the grammar's distinguished symbol and the daughters of a node labeled C are labeled, from left to right, c 1 ... e n , given that C ➔ c 1 ... e n is a grammar rule.",9,10
10915,59681682,"There is a word or phrase (wof) of category s stretching from point A to point Din the string if, for some points B and C between A and D, there is a phrase of category np from A to B, of category aux from B to C, and of category vp from C to D. A terminal symbol, say dog, is recognized as belonging to category n by virtue of the clause: wo f (dog , [dog I X] , X) .",64,65
10916,59681682,"In the present formulation, edges contain no information about the mem bers of a phrase, so that polynomial complexity is achieved automatically without having to conflate edges with the same category symbol and string coverage, but different internal structures.",33,34
10917,59681682,A simple way to construct a weakend version of a grammar is to construct a partition that symbols and to map each class in the partition onto a single symbol.,29,30
10918,59681682,"Ano ther wo uld be to eliminate the distinctions made by X-bar theory, collapsing, for example, N, N-bar, and NP onto the same symbol.",32,33
10919,120038201,"Ranges & Bindings If we consider a derivation in a CFG, headed at the start symbol and leading to some sentence, we know that each nonterminal occurrence is responsible for the generation of the substring laying between two indexes say i and j. For a given input string w = a 1 ••• a n , such a couple (i, j) is called a range.",16,17
10920,222272332,"We marginalize the distribution as late as possible, when the controller queries the stack for the current top stack symbol.",20,21
10921,222272332,⊥ ∈ Γ is the initial stack symbol.,7,8
10922,222272332,"1999) , our definition makes the PDA operations conditional on the input symbol a. The difference is not very important, because the RNN controller will eventually assume responsibility for reading and writing symbols, but our definition makes the shift to an RNN controller below slightly simpler. (",13,14
10923,222272332,"Each state of the stack WFA is of the form (i, q, x), where i is a position in the input string, q is a PDA state, and x is the top stack symbol.",40,41
10924,222272332,"Similarly at time step j = 4, and the existence of a state with top stack symbol ⊥ indicates that the string is of the form ww R .",17,18
10925,222272332,"At each position, it must either push the input symbol to the stack, or else guess that the middle point has been reached and start popping symbols from the stack.",10,11
10926,222272332,"Evaluation Since, in these languages, the next symbol cannot always be predicted deterministically from previous symbols, we do not use prediction accuracy as in previous work.",9,10
10927,222272332,"Instead, we compute per-symbol cross-entropy on a set of strings S .",6,7
10928,222272332,We manually choose a small number of PDA states and stack symbol types for the NS-RNN for each task.,11,12
10929,222272332,"For marked reversal, unmarked reversal, and Dyck, we use 2 states and 2 stack symbol types.",17,18
10930,222272332,"For padded reversal, we use 3 states and 2 stack symbol types.",11,12
10931,222272332,"For the hardest CFL, we use 3 states and 3 stack symbol types.",12,13
10932,235125628,"We decode v into frames and then feed them into a (frozen) video encoder Encoder video (•) and a trainable MLP layer to obtain video tokens: x v = MLP(Encoder video (f v )), (1) where we use a bolded symbol to indicate a sequence and f v is a sequence of continuous frames from a video.",50,51
10933,236477930,"At each decoding step, the decoder either generates a symbol or propagates the decoder state to the next module. (",10,11
10934,207908028,"Thus, for each sentence in the Z monolingual corpus, we have its translation in both X and Y, so we can create a pseudo-parallel corpus X'↔Y' (where the prime symbol indicates machine-translated text).",36,37
10935,15503257,"1 Multiple context-free grammars are non-concatenative in the sense that a non-terminal symbol in this grammar can domi nate a sequence of strings of terminal symbols, rather than just one string, as in the case of ordinary context-free grammars.",18,19
10936,15503257,"Each of the strings dominated by a non-terminal symbol in a multiple context free grammar will be a substring of a sentence whose der ivation includes this non-terminal, but in the sentence these strings are not necessarily adjacent.",10,11
10937,8464480,"Also, we used the # symbol for the pauses and silence, and ˆsymbol for other non-speech events.",6,7
10938,6306013,"In many cases, a portion of the input sentence may be reduced to a non-terminal symbol in many different ways, when considering different subsets of the input that may be skipped.",18,19
10939,35252957,"If (I, D) is correct then a string w is in the language of Giff an item representing a G_ -derivation of w from the start symbol of G is valid.",30,31
10940,35252957,"By definition, w E L( G) iff there is a derivation of w from some start symbol of G. An element in A that represents a derivation of a string w E :E* from a start symbol is called a complete ( augmented} derivation tree.",19,20
10941,35252957,"By definition, w E L( G) iff there is a derivation of w from some start symbol of G. An element in A that represents a derivation of a string w E :E* from a start symbol is called a complete ( augmented} derivation tree.",41,42
10942,35252957,"We borrow a practical notation for trees from [7] : (A """"4 /3) denotes an arbitrary tree with root symbol A and yield (i.e., sequence of labels on the leaves, from left to right) f3 (possibly of height 0, in which case f3 = A). (",25,26
10943,35252957,A ➔ /3) denotes the unique tree of height 1 with root symbol A and yield /3.,13,14
10944,35252957,"A headed context-free grammar G is a tuple (N, �, P, S, h) such that (N, :E, P, S) is a CFG without € productions, where N, �,P are finite sets of nonterminal symbols, terminal symbols and productions, respectively, S is a start symbol and h : P ➔ N is a function that assigns each production p = A ➔ f3 a position 1 � h(p) � 1 /3 1-The h(p)-th symbol in f3 is called the head of p (for simplicity it is assumed that the same production cannot occur twice with different heads).",64,65
10945,35252957,"A headed context-free grammar G is a tuple (N, �, P, S, h) such that (N, :E, P, S) is a CFG without € productions, where N, �,P are finite sets of nonterminal symbols, terminal symbols and productions, respectively, S is a start symbol and h : P ➔ N is a function that assigns each production p = A ➔ f3 a position 1 � h(p) � 1 /3 1-The h(p)-th symbol in f3 is called the head of p (for simplicity it is assumed that the same production cannot occur twice with different heads).",96,97
10946,35252957,"The pair ( p, h(p)) is called a r, k, l ) is an admissible buHC tree then ( r, k, l) is computed by buHGaA-+f3g_""f or we find admissible buHC trees (r i ,ki, l i ) for 1 � i � j such that (r, k, l) is computed from (ri ,ki , l i ) by some j-ary buHC tree oper ation and A(r i , ki , li) <r A(r, k, l) for all i. Then completeness follows by induction on the values of A. (N, ""E, Q, P, S, h, o) where (N, ""E, P, S, h ) is a headed CFG, Q is a finite stack alphabet and o is a function that assigns each production p E P a stack operation push q or pop q (where q E Q) if the head of p is a nonterminal symbol, and nop otherwise.",187,188
10947,35252957,"First, observe that if two admissible buHC-LIG trees are congruent then they must have the same symbol Q on top of the stacks at their root nodes, because of the buHCA operation.",19,20
10948,211258800,"The filler has exactly the same lengths as the summary, but each summary token is replaced by a period symbol (""."").",20,21
10949,8000929,The value h = 0 is used for dependencies where the head is a special root-symbol of the sentence.,17,18
10950,18658079,"This means that we enumerate all pairs of an input symbol and label and represent them as xi , ỹi using index i (1 ≤ i ≤ m).",10,11
10951,219179684,We combined these arcs into one and concatenate the labels of these arcs with a symbol '+' representing the combination of two arcs.,15,16
10952,219179684,"In the postprocessing, we split arcs with the '+' symbol in the corresponding labels into multiple arcs.",12,13
10953,384994,"Our extraction method is basically the same as that of Block (2000) , except we allow more than one nonterminal symbol in a rule, and use a more sophisticated probability model.",22,23
10954,384994,"Thus the hierarchical phrase pairs from our above example could be formalized in a synchronous CFG as: X → yu X 1 you X 2 , have X 2 with X 1 (10) X → X 1 de X 2 , the X 2 that X 1 (11) X → X 1 zhiyi, one of X 1 (12) where we have used boxed indices to indicate which occurrences of X are linked by ∼. Note that we have used only a single nonterminal symbol X instead of assigning syntactic categories to phrases.",91,92
10955,7406488,"The nonterminal S is the start symbol, with φ(S) = 1.",6,7
10956,7406488,"Each of the output strings is the concatenation of a sequence of elements, each element being an input string or a terminal symbol.",23,24
10957,7406488,"The symbol π is the label of the rule, and each rule is uniquely identified by its label.",1,2
10958,7406488,"ζ r ) is a derivation for A, with an uninterpreted function symbol g. When r = 0, we write the derivation as g().",13,14
10959,7406488,"We say that G is proper if for each A: π:A→g(A 1 ,A 2 ,...,A ρ(g) ) p(π) = 1 The probability of a derivation ζ in G, denoted L G (ζ), is obtained by multiplying the probability of the rule corresponding to each occurrence of a function symbol in ζ.",61,62
10960,7406488,"In this paper, we will consider a type of finitestate transducer (FST) that has a single final state and that consumes exactly one input symbol in each transition.",27,28
10961,7406488,"The start symbol S is naturally S( q s , q f ), as the composition ultimately needs to match strings that are generated by G against those input strings that take the automaton from the start state to the final state.",2,3
10962,7406488,"Second, restricting attention to the generating nonterminals, a top-down phase identifies the possibly smaller set of nonterminals that are also reachable from the start symbol.",28,29
10963,7406488,"v m if and only if |w 1 • • • w k | < |v 1 • • • v m | or |w 1 • • • w k | = |v 1 • • • v m | and m < k. In words, a sequence is smaller than another if it contains fewer occurrences of symbols, and when the two sequences contain the same number of occurrences of symbols, then the first is smaller if the symbol occurrences are distributed over more strings.",83,84
10964,7406488,"All nonterminals of this LCFRS, except the start symbol, have fan-out 2, and all generated strings are of the form w$v, where w functions as input string, v functions are output string, and $ is a separator between the two, which does not occur in the input and output alphabets of the synchronous contextfree grammar.",9,10
10965,7406488,"The first form is a single rule with the start symbol S † in the left-hand side: π † : S † → g † (S), where g † ( x 1,1 , x 1,2 ) = x 1,1 $x 1,2 The other rules all have a form that ensures that variables associated with the input string are never combined with variables associated with the output string: π : A → g(A 1 , . . . ,",10,11
10966,14078334,"An ET is a single ""symbol"" in a transducer's language.",6,7
10967,14078334,"As shown in Figure 2 , each circle stands for an ET and thick arrows denote the transduction of each ET as a single symbol.",24,25
10968,2923012,"Given a binarized probabilistic contextfree grammar (PCFG) defined as the tuple (V, T, S † , P, ρ) where V is the set of nonterminals, T is the set of terminals, S † is a special start symbol, P is the set of grammar productions, and ρ is a mapping of grammar productions to probabilities, we subdivide P into binary rules, P b , and unary rules, P u .",46,47
10969,369260,"The lexicalized translation rules of the grammar may contain a single nonterminal symbol, denoted X. We will use a, b, c and d to denote terminal symbols, and u, v, and w to denote (possibly empty) sequences of these terminals.",12,13
10970,369260,"Baseline Approach In the pattern-matching literature, words spanned by the nonterminal symbols of Chiang's grammar are called don't cares and a nonterminal symbol in a query pattern that matches a sequence of don't cares is called a variable length gap.",27,28
10971,369260,This includes all patterns containing exactly one more terminal symbol than the current pattern.,9,10
10972,1102745,"For example, (L1) in Figure 1 (b) is a lexical rule providing a score (of −0.23) for mapping the word ""I"" to the symbol ""PRP.""",32,33
10973,1102745,"To keep track of the scores of these structures, a chart indexed by the start and end positions and the symbol under consideration is used: scores [start][end][symbol] (see also Figure 2 ).",21,22
10974,1102745,"However, if we map a symbol to a thread, then it not only fails to provide enough parallelism to fully utilize the massive number of threads in  GPUs, 3 but it can also suffer from load imbalance due to the fact that each symbol has a varying number of rules associated with it.",6,7
10975,1102745,"However, if we map a symbol to a thread, then it not only fails to provide enough parallelism to fully utilize the massive number of threads in  GPUs, 3 but it can also suffer from load imbalance due to the fact that each symbol has a varying number of rules associated with it.",47,48
10976,1102745,"Since each rule is mapped to a different thread, threads for rules with the same parent symbol need to be synchronized in order to avoid write conflicts.",17,18
10977,1102745,"We can map each symbol to a thread block, and the rules associated with each symbol to threads in the respective thread block.",4,5
10978,1102745,"We can map each symbol to a thread block, and the rules associated with each symbol to threads in the respective thread block.",16,17
10979,1102745,"For example, when the first SM completes the computation of a thread block (because the associated symbol has few rules), it can proceed to the next available thread block (which corresponds to another symbol).",18,19
10980,1102745,"For example, when the first SM completes the computation of a thread block (because the associated symbol has few rules), it can proceed to the next available thread block (which corresponds to another symbol).",38,39
10981,1102745,"In block-based mapping, only one thread needs to check and determine if a symbol is a preterminal and can be skipped.",16,17
10982,1102745,"A challenging aspect of the block-based mapping comes from the fact that the number of rules per symbol can exceed the maximum number of threads per thread block (1,024 or 512 depending on the GPU architecture).",19,20
10983,1102745,"However, in this mapping, all threads in a thread block have the same parent symbol, and therefore only one shared variable per thread block is needed for the parent symbol (as shown in line 15 of Figure 7 ).",16,17
10984,1102745,"However, in this mapping, all threads in a thread block have the same parent symbol, and therefore only one shared variable per thread block is needed for the parent symbol (as shown in line 15 of Figure 7 ).",32,33
10985,1102745,This synchronization method is in practice only applicable to blockbased mapping and cannot be applied to threadbased mapping since it assumes that all threads in a thread block perform reductions for the same parent symbol.,35,36
10986,1102745,"There are three global memory accesses for each binary rule: the left child symbol ID, right child symbol ID, and the rule score itself.",14,15
10987,1102745,"There are three global memory accesses for each binary rule: the left child symbol ID, right child symbol ID, and the rule score itself.",19,20
10988,1102745,"If we look into the access pattern (ignoring the symbol dimension of the scores array), we can see that the accesses actually occur only in restricted locations that can be easily enumerated.",10,11
10989,1102745,"For each unique left/right child symbol, we need to load the score from scores to sh scores L and sh scores R once through a global memory access.",7,8
10990,6079971,"Definition (1) For each , find a labeling , where e f f j ∈ { ∪ ∈ e j j a e } 0 a stands for the ""empty symbol"", which means could be aligned to nothing.",33,34
10991,11364094,"Note, we only apply this rule if the head phrase consists of a content word followed by zero or more function words, followed by an optional punctuation symbol.",29,30
10992,11364094,"FFS: If a phrase consists of zero or more content words, function words, or punctuation symbols followed by a sequence of two function words and a punctuation symbol, then set the head of the first function word in the sequence to the second function word and the head of the second function word to the punctuation symbol.",30,31
10993,11364094,"FFS: If a phrase consists of zero or more content words, function words, or punctuation symbols followed by a sequence of two function words and a punctuation symbol, then set the head of the first function word in the sequence to the second function word and the head of the second function word to the punctuation symbol.",60,61
10994,6619269,"We write s, t ((s, t) = ( , )) to denote an edit operation in which the symbol s is replaced by t. If s = and t = , s, t is an insertion.",24,25
10995,6619269,"target) symbol at position i. 1  To model the edit operations, we introduce a hidden RV, Z, that takes values in (A ∪ × B ∪ ) \ {( , )}.",2,3
10996,6619269,"If Z 1 is an insertion (i.e. Z (s) 1 = : the source symbol in the first frame is not output), then a 2 retains the same value as a 1 ; otherwise a 2 is incremented by 1 to point to the next symbol in the source string.",17,18
10997,6619269,"If Z 1 is an insertion (i.e. Z (s) 1 = : the source symbol in the first frame is not output), then a 2 retains the same value as a 1 ; otherwise a 2 is incremented by 1 to point to the next symbol in the source string.",50,51
10998,6619269,"When end = 1, Z takes, with probability 1, a fixed value outside the range of edit operations but consistent with s and t. This ensures 1) no ""null"" state ( , ) is required to fill in the value of Z until the end of the graph is reached; our likelihoods and model parameters therefore do not become dependent on the amount of ""null"" padding; and 2) no probability mass is taken from the other states of Z as is the case with the special termination symbol # in the original RY model.",98,99
10999,6619269,This ensures the last source symbol is indeed consumed.,5,6
11000,6619269,"snext i is an RV whose value is set to the symbol at the a i +1 position of the string, i.e., snext i =s a i +1 .",11,12
11001,6619269,The key feature of this model is that we are required to consume a target symbol per frame.,15,16
11002,6619269,"The biggest improvement (27.65% ER) however comes from conditioning on Z (t) i−1 , the target symbol that is hypothesized in the previous step.",21,22
11003,236486084,"Their system takes a queue of symbols as input, and process it in an incremental fashion, consuming one symbol at a time.",20,21
11004,236486084,"Word tagging tasks (POS tagging and morphological tagging) are realized through a single action: TAG L (t), which simply writes symbol t on tape L at the word index position.",26,27
11005,236486084,"Each action can write a symbol on an output tape, move the character head or the word head either to the left or to the right and push or pop the stack (see Table 2 ).",5,6
11006,259144,"When X = Σ + for some symbol alphabet Σ, certain kinds of neighborhoods have natural, compact representations.",7,8
11007,259144,Consider first the neighborhood consisting of all sequences generated by deleting a single symbol from the m-length sequence x m 1 : DEL1WORD(x m 1 ) = x −1 1 x m +1 | 1 ≤ ≤ m ∪ {x m 1 } This set consists of m + 1 strings and can be compactly represented as a lattice (see Fig.,13,14
11008,259144,"When the vocabulary Σ is the set of words in a natural language, it is never fully known; approximations for defining LENGTH = Σ m include using observed Σ from the training set (as we do) or adding a special OOV symbol.",45,46
11009,6961896,"x n , where x 0 is a special root symbol, which we will denote as * .",10,11
11010,6961896,"For each h, m, l, s ∈ S the tuple h, m, l is an element of D; there is one member of S for each member of D. The index s is the index of the word that was adjoined to the spine for h immediately before m (or the NULL symbol if no previous adjunction has taken place). •",59,60
11011,7498,The @ symbol represents A application of the right subtree to the left.,2,3
11012,227014650,"We will not make this precise here, but we hope that Figure 1 , in which homeomorphic spaces are connected by the symbol "" ∼ ="", gives a clear intuition.",23,24
11013,18046808,"Create Non-terminal from a Terminal (CreateNT): Given a terminal symbol t in the grammar, this operator adds a new production T → t to it and replaces all the occurrences of the terminal t in all the other productions by the new non-terminal T .",13,14
11014,1462544,"Most work on ITG has focused on the 2-normal form, which consists of unary production rules that are responsible for generating word pairs: X → e/f and binary production rules in two forms that are responsible for generating syntactic subtree pairs: X → [Y Z] and X → Y Z The rules with square brackets enclosing the right hand side expand the left hand side symbol into the two symbols on the right hand side in the same order in the two languages, whereas the rules with pointed brackets expand the left hand side symbol into the two right hand side symbols in reverse order in the two languages.",74,75
11015,1462544,"Most work on ITG has focused on the 2-normal form, which consists of unary production rules that are responsible for generating word pairs: X → e/f and binary production rules in two forms that are responsible for generating syntactic subtree pairs: X → [Y Z] and X → Y Z The rules with square brackets enclosing the right hand side expand the left hand side symbol into the two symbols on the right hand side in the same order in the two languages, whereas the rules with pointed brackets expand the left hand side symbol into the two right hand side symbols in reverse order in the two languages.",104,105
11016,1462544,"The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar: it has a start symbol S, a single preterminal C, and two intermediate nonterminals A and B used to ensure that only one parse can generate any given word-level alignment, as discussed by Wu (1997) and Zens and Ney (2003) .",21,22
11017,1462544,"Starting from the start symbol S, we first choose the head word pair for S, which is see/vois in the example.",4,5
11018,6139243,Normalize the FST such that each arc has exactly one input symbol.,11,12
11019,59860790,"Each node is labelled with a ter minal symbol, a nonterminal symbol or the empty string.",8,9
11020,59860790,"Each node is labelled with a ter minal symbol, a nonterminal symbol or the empty string.",12,13
11021,59860790,When context permits we will abuse this notation and not distinguish between a symbol and the multiset that contains only that symbol.,13,14
11022,59860790,When context permits we will abuse this notation and not distinguish between a symbol and the multiset that contains only that symbol.,21,22
11023,59860790,"Second , not only is the current ENA encoded by the nonterminal symbol but we also store it in the multiset.",12,13
11024,59860790,"/t [•• T/t ••] -t 11b [•• 11b ••] E P. For each 11 that is labelled with the same symbol as 1Jt and is the root of some elementary d-tree in D let 11t [•• 1Jt ••] -t 11[ .. TJ, 1]b ••] E For each TJ that is labelled with the same symbol as T/t , is the root of some elementary component but is not the root of a d-tree let TJt [•• T/t ••] ---+ TJ[•• T/b ••] E P. The first production corresponds to the case where a component is not inserted within this d-edge.",26,27
11025,59860790,"/t [•• T/t ••] -t 11b [•• 11b ••] E P. For each 11 that is labelled with the same symbol as 1Jt and is the root of some elementary d-tree in D let 11t [•• 1Jt ••] -t 11[ .. TJ, 1]b ••] E For each TJ that is labelled with the same symbol as T/t , is the root of some elementary component but is not the root of a d-tree let TJt [•• T/t ••] ---+ TJ[•• T/b ••] E P. The first production corresponds to the case where a component is not inserted within this d-edge.",68,69
11026,59860790,"Case 6: Suppose that T/ is a substitution node in some cl-tree in D. For each TJ r that is labelled with the same symbol as 77 and is the root of an elementary cl-tree in D let 77[ .. For each T/ r that is labelled with the same symbol as T/, is the root of an elementary component but not the root of ad-tree in D let 77[•• 77 ••] -t 77 r [•] E P. Any component (whether the topmost of an elementary d-tree or not) can be substituted at a substitution node provided their labels match.",27,28
11027,59860790,"Case 6: Suppose that T/ is a substitution node in some cl-tree in D. For each TJ r that is labelled with the same symbol as 77 and is the root of an elementary cl-tree in D let 77[ .. For each T/ r that is labelled with the same symbol as T/, is the root of an elementary component but not the root of ad-tree in D let 77[•• 77 ••] -t 77 r [•] E P. Any component (whether the topmost of an elementary d-tree or not) can be substituted at a substitution node provided their labels match.",56,57
11028,59860790,"Because no distinction is made between different occurrences of the same multiset symbol in a multiset, there may be several ways of associating occurrences of a multiset symbols at different nodes in the derivation tree.",12,13
11029,2864310,"Concepts are unary predicates, roles are binary predicates over a domain A, with individuals being the elements of A. We assume a common set-theoretical semantics for C7)£ -an interpretation Z is a function that assigns to each concept symbol (the set A) a subset of the domain A, Z : A -+ 2 n, to each role symbol (the set P) a binary relation of A, Z : P --+ 2 ~×n, and to each individual symbol (the set I) an element of A, Z : I --+ A. Concept terms and role terms are defined inductively.",42,43
11030,2864310,"Concepts are unary predicates, roles are binary predicates over a domain A, with individuals being the elements of A. We assume a common set-theoretical semantics for C7)£ -an interpretation Z is a function that assigns to each concept symbol (the set A) a subset of the domain A, Z : A -+ 2 n, to each role symbol (the set P) a binary relation of A, Z : P --+ 2 ~×n, and to each individual symbol (the set I) an element of A, Z : I --+ A. Concept terms and role terms are defined inductively.",65,66
11031,2864310,"Concepts are unary predicates, roles are binary predicates over a domain A, with individuals being the elements of A. We assume a common set-theoretical semantics for C7)£ -an interpretation Z is a function that assigns to each concept symbol (the set A) a subset of the domain A, Z : A -+ 2 n, to each role symbol (the set P) a binary relation of A, Z : P --+ 2 ~×n, and to each individual symbol (the set I) an element of A, Z : I --+ A. Concept terms and role terms are defined inductively.",88,89
11032,2864310,"Its semantics is given by a special interpretation function Zh for each hypothesis h, which is applied to each concept and role symbol in the canonical way: Zh : A --+ 2zx; Zh : P --+ 2 AxA. Notice that the instances a, b are interpreted by the interpretation function Z, because there exists only one domain £x. Only the interpretation of the concept symbol C and the role symbol R may be different in each hypothesis h. Assume that we want to represent two of the four concept hypotheses that can be derived from (P3), viz.",23,24
11033,2864310,"Its semantics is given by a special interpretation function Zh for each hypothesis h, which is applied to each concept and role symbol in the canonical way: Zh : A --+ 2zx; Zh : P --+ 2 AxA. Notice that the instances a, b are interpreted by the interpretation function Z, because there exists only one domain £x. Only the interpretation of the concept symbol C and the role symbol R may be different in each hypothesis h. Assume that we want to represent two of the four concept hypotheses that can be derived from (P3), viz.",70,71
11034,2864310,"Its semantics is given by a special interpretation function Zh for each hypothesis h, which is applied to each concept and role symbol in the canonical way: Zh : A --+ 2zx; Zh : P --+ 2 AxA. Notice that the instances a, b are interpreted by the interpretation function Z, because there exists only one domain £x. Only the interpretation of the concept symbol C and the role symbol R may be different in each hypothesis h. Assume that we want to represent two of the four concept hypotheses that can be derived from (P3), viz.",75,76
11035,219306295,"Notation A context-free grammar is a four-tuple G = (V N , Vr , P, S), where S is the start symbol, V N is the bunch of nonterminals, Vr is the bunch of terminals.",29,30
11036,219306295,Equation (9) states that La not only maps grammar-symbol strings into languages: it also maps an operation on its input objects ( concate nation) to an operation on its output objects (lan guage multiplication).,12,13
11037,219306295,"In other words, the recognition function [a] for regular expression a can be obtained by replacing every grammar symbol X that occurs in it by its function [X] and interpreting all con structors in the regular expression (alternation, concatenation, iteration, optionality) as combi nators of recognition functions.",21,22
11038,219306295,"If a non-empty prefix can be found, this corresponds to a part of the input sentence, which is a string rewritable to the first symbol of 8 (i.e. X) followed by the remainder of the prefix ( which are terminals).",28,29
11039,59904527,"In the figures ahead, which graphically display the GSS of the parser in various stages of the pars ing process, we use the following notation: • An active ( top level) state node is repre sented by the symbol ""©"" , with the state number indicated above it .",42,43
11040,59904527,"An inactive state node is represented by the symbol ""*"" .",8,9
11041,59904527,"Grammar symbol nodes are represented by the symbol ""#"" , with the grammar symbol itself displayed above it.",1,2
11042,59904527,"Grammar symbol nodes are represented by the symbol ""#"" , with the grammar symbol itself displayed above it.",7,8
11043,59904527,"Grammar symbol nodes are represented by the symbol ""#"" , with the grammar symbol itself displayed above it.",15,16
11044,59904527,"The symbol node labeled ""det"" has been shifted and connected to the initial state node and to the new active state node of state 3.",1,2
11045,59904527,"A local ambiguity is a part of the input sentence that corresponds to a phrase (thus, reducible to some non-terminal symbol of the grammar), and is parsable in more than one way.",24,25
11046,59904527,A second symbol node is created when the determiner is skipped and a reduction by the second rule takes place.,2,3
11047,59904527,Locally ambiguous symbol nodes are detected as nodes that are surrounded by common state nodes in the GSS.,2,3
11048,59904527,The original GLR parser de tects such local ambiguities and packs them into a single symbol node.,15,16
11049,59904527,Locally ambigu ous symbol nodes are compared in terms of the words skipped within them.,3,4
11050,59904527,"When parsing spontaneous spoken input that was recognized by a speech recognition system, the parser must deal with three major types of extra-grammaticality: Since the skipping of words is the result of per forming shift operations from inactive state nodes of the GSS, our heuristic limits the number of in active state nodes from which a input symbol is shifted.",62,63
11051,19033508,"We choose carefully a symbol for a diacritic, on the basis of formal similarity between them, on the condition that the symbol should not be used for existing meta-languages such as tags (<, >) or parentheses ( (, ), [, ], {, }).",4,5
11052,19033508,"We choose carefully a symbol for a diacritic, on the basis of formal similarity between them, on the condition that the symbol should not be used for existing meta-languages such as tags (<, >) or parentheses ( (, ), [, ], {, }).",23,24
11053,59658449,"The rule .talso must have a ref erent, which it gets from the corresponding part of the case in the schema, subsituting its role 'per son' for the symbol 'patient' given in the schema, again as indicated by t;h� mapping field. .• • ,",32,33
11054,233365122,"Lastly, each emoji symbol within a text was replaced with its Arabic name.",4,5
11055,60454657,we assume in the rest of the paper that there is only one production rule with S as its LBS symbol.,20,21
11056,219310061,"Probabilistic CFG A probabilistic context-free grammar (PCFG) (Suppes 1970 , We therall 1980 , Wright and Wrigley 1989) G, is a 4-tuple (N, T, R, S) where N is a set of non-terminal symbols including S the start symbol, T a set of terminal symbols, and R a set of probabilistic productions of the form <A--+ a, p > where A E N, a E (N U T) * , an d   p the production probability.",54,55
11057,219310061,"The probability p is the conditional probability P(alA) , which is the probability that the non-terminal A which appears during a derivation process is rewritten by the se quence a. Clearly if there are k A-productions with probabilities P i, ... ,Pk, then I::= l Pi = 1, since the symbol A must be rewritten by the right hand side of some A-production.",59,60
11058,219310061,1 (2) s-+ s pp 1 (3) NP -+ n � (4) NP -+ det n f (5) NP -+ NP PP 10 (6) PP -+ prep NP 1 (7) VP -+ V NP 1 pends only upon the presence of a given nonter minal symbol ( the premis) in a derivation and not upon how the premis was generated.,57,58
11059,219310061,Local Ambiguity Packing Local ambiguity packing occurs when two or more branches of the stack are reduced to the same non terminal symbol.,22,23
11060,219310061,"Let V be a partial derivation before seeing the input symbol v. At this point, the possible derivations which ,vill lead to item Ji are: 1 'D� VP --.v-N P�NP-+•n • v � VP ..:..... v-NP .Jb_ NP -+ -NP VP � NP -+ •n .L ..1.. i 'D � VP --.",10,11
11061,219310061,"ft action say from State i to State z + 1 on seemg the in�ut symbol x, the corresponding stochas tic factor for this action would be Sr, the sum of the stochastic values of all the leaf items in State i which are expecting the symbol x. For reduce-action, the stochastic factor is simply the stochastic value S i of the item representing the re duction, namely [Ai � Oi • , Si ] if the red � ction is via production Ai � Oi .",17,18
11062,219310061,"ft action say from State i to State z + 1 on seemg the in�ut symbol x, the corresponding stochas tic factor for this action would be Sr, the sum of the stochastic values of all the leaf items in State i which are expecting the symbol x. For reduce-action, the stochastic factor is simply the stochastic value S i of the item representing the re duction, namely [Ai � Oi • , Si ] if the red � ction is via production Ai � Oi .",50,51
11063,219310061,"For accept-action, the stochastic factor is the stochastic value S n of the item [S' � S•, S n ], since acceptance can be trea � ed as a final reduction of the augmented production S' � S, where S' is the system-introduced start symbol for the grammar.",56,57
11064,219310061,Let &B be the set of items in C that are expecting B as the next symbol on the stack.,17,18
11065,219310061,"1 ... I n }, X), where the first argument { Ii ... I n } is a set of n probabilistic items and the second argument X a grammar symbol in ( N U T) .",33,34
11066,219310061,"I n } are such that those with symbol X after When k = 0, GOTO( {Ii }, X) is undefined.",8,9
11067,219310061,"Given a grammar G = (N, T, R, S ) , we de fine a corresponding grammar G' with a system generated start symbol S': (N U {S'}, T, R U { < S' --+ S, 1 > }, S').",28,29
11068,219310061,"If [A --+ a• a,B, q a ] is in 'Wi, a ET, and GOTO(W'i,a) = '11; , set ACTION[i,a] to ( ""shift j"" , P a) where P a is the sum of q a 's -that is the stochastic values of items in 'Wi with symbol a after the dot.",66,67
11069,14848278,"A context-free grammar G = (T, N, P, S) consists of two finite disjoint sets N and T of nonterminals and terminals, respectively, a start symbol S E N, and a finite set of rules P. Every rule has the form A� a, where the left-hand side (lhs) A is an element from N and the right-hand side (rhs) a is an element from V*, where V denotes (NUT).",34,35
11070,14848278,"If S is nonfalse in G, then add the rules Example 2.1 Let the grammar G2 be <le st -+ s and st -+ € to G o and make fined by the rules st the new start symbol of G o . (",40,41
11071,14848278,This yields the set of rules A -+ B[C]D A -+ B[CD] A -+ [BC]D Step 4 adds the rules At -+ A and At -+ f. The new start symbol is At.,32,33
11072,14848278,"We have now obtained E-elim(G 2 ), which is defined by At -+ A At -+ f A -+ B[C]D A -+ B[CD] A -+ [BC]D B -+ b D -+ d □ Note that in the case that E-elim introduces a new start symbol st, there is no need to aug-.",51,52
11073,14848278,inent the• grammar (i.e. add the rule S' -+ st and make S' the new start symbol) for the pur pose of constructing the LR automaton.,19,20
11074,14848278,Aug mentation is in this case superfluous because the start symbol st is not recursive.,10,11
11075,219307952,"Label' is an atomic symbol which Jabels the arc, �d 'value' is a pointer to a node.",5,6
11076,60770482,An AG is based on a CF G. Every grammar symbol carries a series of attributes of a certain type.,10,11
11077,60770482,"Synthesized at tributes of a symbol depend on other attributes in the production below that symbol, whereas inher ited attributes depend on attributes in the pro duction above it.",5,6
11078,60770482,"Synthesized at tributes of a symbol depend on other attributes in the production below that symbol, whereas inher ited attributes depend on attributes in the pro duction above it.",15,16
11079,60770482,in which all attributes have the same type; • in which every symbol has exactly one inherited and exactly one synthesized at tribute. •,13,14
11080,60770482,"First of all, we must capture the fact that the lexical analyzer yields, instead of a string of al phabet symbols, a string of pairs, each pair con sisting of a symbol and a synthesized attribute value associated with the symbol.",35,36
11081,60770482,"First of all, we must capture the fact that the lexical analyzer yields, instead of a string of al phabet symbols, a string of pairs, each pair con sisting of a symbol and a synthesized attribute value associated with the symbol.",44,45
11082,60770482,"Notice that in most states, being those that have at least one symbol (say Y) fol lowing and one symbol (say X) preceding the dot, carry a value of the inherited attribute occurrence of Y as well as the value of the synthesized one of X. -If a = c:, it suffices to copy the synthe sized attribute value from the remain ing input. •",13,14
11083,60770482,"Notice that in most states, being those that have at least one symbol (say Y) fol lowing and one symbol (say X) preceding the dot, carry a value of the inherited attribute occurrence of Y as well as the value of the synthesized one of X. -If a = c:, it suffices to copy the synthe sized attribute value from the remain ing input. •",22,23
11084,60770482,"This value is calculated by using the values of the synthesized attribute occurrences of X1 , ... , X n and the inher ited attribute of A. These can be found at a distance from the stack's top that equals the position number of their corresponding symbol in the string X n ... X1 A. Also, a reduction step is only made if the semantic condition associated with the production evaluates to TRUE.",47,48
11085,60770482,"Therefore, at the moment of extracting the value, the only information available is the symbol im mediately following the dot in the intended dotted rule (being A), and not the entire dotted rule.",16,17
11086,60770482,"In general, the same symbol may immediately follow the dot in more than one dotted rule in one state. •",5,6
11087,60770482,"For instance, a certain high light may forbid a shift of some symbol a, even when the underlying static state allows such a shift step.",13,14
11088,60770482,"Then, if all parsers remove the same symbol from the input at the same time, they all will always have the same remaining input, which therefore can be fac tored out of the graph-structured instantaneous description.",8,9
11089,60770482,"Because all shift steps shift the same symbol, the maximum number of wait nodes resulting is the maximum number of states that can occur af ter a shift of a particular symbol.",7,8
11090,60770482,"Because all shift steps shift the same symbol, the maximum number of wait nodes resulting is the maximum number of states that can occur af ter a shift of a particular symbol.",32,33
11091,60770482,"Every block corresponds to a gram mar symbol X and contains those states that can become current state after a shift of X. Now, we take a number of processes such that, to each of them, ( at most) one state in every block is as signed.",7,8
11092,60770482,"In this algorithm, the input is di vided over processors such that every processor processes one input symbol.",18,19
11093,60770482,"A marked symbol consists of a grammar sym bol (terminal or nonterminal) and two numbers, which indicate the left and right border of a sub string of the input string, that is derivable from that grammar symbol.",2,3
11094,60770482,"A marked symbol consists of a grammar sym bol (terminal or nonterminal) and two numbers, which indicate the left and right border of a sub string of the input string, that is derivable from that grammar symbol.",40,41
11095,60770482,Actual recognition of a new marked symbol with left border i is done by the processor associated with the i th input sym bol.,6,7
11096,60662416,An LCFG is lexicalized be cause every initial and auxiliary tree is required to contain a terminal symbol on its frontier. (,17,18
11097,60662416,A derivation is complete when every frontier node is labeled with a terminal symbol.,13,14
11098,60662416,Label the new root of T' with a unique new symbol S'.,11,12
11099,60662416,is�introduced as an additional initial tree and the start symbol of G 77 is S' .,11,12
11100,29541408,A cycle can be removed by introduction of a new symbol.,10,11
11101,29541408,This symbol rewrites to any member of the cycle.,1,2
11102,29541408,"In such a grammar, not every 3-SAT variable is encoded in a different symbol in the grammar.",16,17
11103,219301673,"For example in figure 3, the symbol ""o"" at the intersection of row 2 (p1) and column 3 (vs4k) indicates that the morpho logical category vs4k can immediately follow the morphological category p 1. .",7,8
11104,5118729,"An expression is either an atomic symbol or a pair of expressions combined with one of two types of binary operators: / and \. A sentence is in the language defined by the categorial grammar if, after choosing one expres sion associated with each word, there is a deriva tion which transforms the chosen sequence of ex pressions into S, a single expression consisting of a special atomic symbol.",6,7
11105,5118729,"An expression is either an atomic symbol or a pair of expressions combined with one of two types of binary operators: / and \. A sentence is in the language defined by the categorial grammar if, after choosing one expres sion associated with each word, there is a deriva tion which transforms the chosen sequence of ex pressions into S, a single expression consisting of a special atomic symbol.",72,73
11106,5118729,"If a word w has the set { d1 , d2 , .•. , dk } of categorial expressions, then we'll give that word the following link grammar expression: E(d1 )orE(d2 )or • • • orE(dk) The function E( •) is defined recursively as fol lows: E(J /e) = f /e-or f /e+ or (e+ & E(J)) E(e\f) = e\f-or e\f+ or (e-& E(J)) E(A) = Aor A+ Here A stands for any atomic symbol from the categorial grammar, and A, f /e and e\f are con ..: nector names in the link grammar formulas.",97,98
11107,6869876,These models define scores for each mention to link to its antecedent or to an artifical root symbol $ (in which case it is not anaphoric).,17,18
11108,18401679,"If an �rdi nary parser detects a syntax error on s. ome sym bol, the substring parser can be started on the next symbol to discover additional syntax errors.",25,26
11109,18401679,"Using this method, it is not necessary to let the parser make any assumption about how to correct the error, or to let it skip input until a trusted symbol is found.",31,32
11110,18401679,"9, p. 73-76] Useless symbols and unreachable rules do not influence substring parsing as these are ignored by the parse table generator.. This is due to the f a ct that LR parse tables are generated top-down, starting with the start symbol of the grammar, and that useless symbols and unreachable rules are, by definition, unreachable from the start sym bol.",49,50
11111,18401679,obtains an action from the parse table with the state on top of its stack and with input symbol Sk .,18,19
11112,18401679,"ains -at 1 least -one symbol of •s; i.e., we do not gener ,a;te subtrees whose frontier lies -entirel y. within u 1 •or 0-2 .",5,6
11113,18401679,The dots indi cate the amount of time needed and they are at tributed with the first symbol of the substring.,17,18
11114,195064060,"We use the symbol ""^"" to represent such combinations.",3,4
11115,219310120,The main idea behind this is to allow a traditional LR parser to choose the next symbol to parse from a two-dimensional space.,16,17
11116,219310120,"This column contains the position of the next symbol to be shifted, for each state.",8,9
11117,219310120,"If PG is a grammar with starting symbol S, then PG', the aug mented positional grammar for PG, is PG with a new starting symbol S' and production S' := SP S. AMBIGUITY CONSIDERATIONS In this Section we will show that conflicts in positions can lead to conflicts in the ""action"" part of the parsing table even if it has no multiple entries.",7,8
11118,219310120,"If PG is a grammar with starting symbol S, then PG', the aug mented positional grammar for PG, is PG with a new starting symbol S' and production S' := SP S. AMBIGUITY CONSIDERATIONS In this Section we will show that conflicts in positions can lead to conflicts in the ""action"" part of the parsing table even if it has no multiple entries.",28,29
11119,219310120,We will show now that this grammar is not pLR( 1) from the fact that it has conflicts regar�ing the position of the next symbol.,28,29
11120,219310120,A first approach regards the extension of the concept of symbol to an N-Attaching Point Entity as defined in [14] . •,10,11
11121,219310120,"In this way a symbol can be connected to non-adjacent symbols, too.",4,5
11122,15257398,Since the next symbol in the input sequence is the end of the sequence and the parser detects another error.,3,4
11123,219301290,"R, the representation relation of IP, a re lation on C x S. R( c, s) indicates that the color c 'represents' the symbol s. A color which represents a symbol is a termi nal color. •",30,31
11124,219301290,"R, the representation relation of IP, a re lation on C x S. R( c, s) indicates that the color c 'represents' the symbol s. A color which represents a symbol is a termi nal color. •",37,38
11125,219301290,"If s is a symbol and the color c represents s, then Single(c) is an admissible tree, it is an analysis tree for s. {d, n,p} { NP, PP } NP 2.",4,5
11126,219301290,"n) (F2) F(NP --+ NP, pp --+ p WP, NP --+ NP pp ) (F3) F(PP --+ p, NP --+ d n, PP --+ p NP ) (F4) F(PP --+ p, NP--+ NP PP, PP --+ p NP ) empty (Ll) L(NP --+ d n, NP --+ NP ) (L2) L(NP--+ NP PP, NP --+ NP ) (L3) L( d--+ d, NP --+ d ) (L4) L( p --+ p, PP --+ p ) The set of root colors of the FB-system con tains the colors that are production rules for the start symbol.",122,123
11127,219301290,"The representation relation R contains three pa!rs, one for every symbol.",11,12
11128,219301290,"The F rules state how to extend partial recognition, the L rules tell how a terminal or a completely recognized non terminal can be the leftmost symbol in a partially recognized other non-terminal.",27,28
11129,219301290,"h) and the next symbol ak+I proceeds as follows: It starts with the set K = { (k + 1, c) IR (c, ak+ l )} Of this set, the completion is constructed.",5,6
11130,219301290,"The representation relation R contains three pairs, one for every symbol.",11,12
11131,219301290,"Note finally that the absence of Lift here and the importance of Lift in the case of contextfree grammars reflect the fact that every node in a dependency tree represents a symbol, whereas in parse trees the internal nodes are representatives of constituents.",31,32
11132,219182094,"S E V \:Eis a start symbol and Pisa set of production rules of the form a--+ {3, with a, (3 E V*, where a contains at least one nonterminal symbol.",6,7
11133,219182094,"S E V \:Eis a start symbol and Pisa set of production rules of the form a--+ {3, with a, (3 E V*, where a contains at least one nonterminal symbol.",36,37
11134,219182094,For every unlabeled symbol in the ACSG there is a vertex in the graph.,3,4
11135,219182094,The weight of this symbol is i. Remove the vertex.,4,5
11136,16425409,"Henceforth, we assume that our language L 0 is generated by a context-free grammar G = (V, l:, P, S) consisting of an alphabet V, a terminal alphabet l: ( l: � V), an initial symbol S ( S e V), and a finite set P of rules ( P � N x V* where N = V -l:).",49,50
11137,16425409,"Note that for each symbol a. in V, the value of P( a.) is a finite language over the alphabet V that contains a.. The containment of a. in this value allows us to interpret P as a nested finite substitution; a concept intro duced in (10] and to be recalled in §2.",4,5
11138,16425409,"A fa mily of fuzzy languages is a s_ et of pairs (L , :E L ) where L is a fuzzy subset of :E{ and :E L is a finite sub set of :E w We assume that the alphabet :E L is minimal with respect to L, i.e., a symbol a is a member of I:L if and only if a occurs in a word x with µ( x; L) > 0.",61,62
11139,16425409,3) we modify each grammar G a ( a.eV) in such a way that P aCJ3) = { J3} holds for each terminal symbol J3 in V. (b) For each nested fu zzy K -substitution P a over V m we define a corresponding nested fu zzy K -substitution Q a by Q aC J3) = P aC J3) iff J3 E V a -V Q a( �) = { �' S 13} iff � e V Q a C J3) = { J3} iff J3 E V 1 -V a with V 1 =U{Y V O of V 1 .,27,28
11140,543881,In general if w = σ 1 • • • σ n ∈ T * then C T (w) = Σ * σ 1 (Σ − T ) * σ 2 (Σ − T ) * • • • (Σ − T ) * σ n Σ * In the case where w begins (ends) with a word boundary symbol then the first (last) Σ * in the previous GRE must be replaced with (Σ − T ) * .,66,67
11141,543881,"In the cases where w begins (ends) with a word boundary symbol, the first (last) ∅ in the GRE above should be replaced with ∅T ∅. Since every C T (w) can be expressed as a GRE without the Kleene-star, every TSL language is star-free.",13,14
11142,219301122,increment�lly: We e�pect to derive benefit m our ap_p�1cabon dom8:ms fr�m processing each symbol m the order m which it is created by a user.,24,25
11143,219306732,  The symbol '©' is to be considered as an anony mous variable; no two occurrences are taken as identical.,2,3
11144,219182113,This check is based upon the composition of simple relations and does not require any computation of symbol stacks.,17,18
11145,219182113,"Each time a push or pop occurs at a node, there is a twin node where the opposite action, acting on the same symbol at the same stack level, should take place.",25,26
11146,219182113,S is an element of V N called the start symbol. •,10,11
11147,219182113,"On V* we define IPI disjoint binary relations named right derive by B � /3 £( G) = { x I s � x "" x E v; } G In an S / x-derivation, to accurately define the contribution of any symbol X (its X-sentence) to the sentence x, we will define the notion of split of x by X. A triple (x1 , x2, x3) is called 3-split (or more simply split) of x when x = x1x2x3.",49,50
11148,219182113,S E V N is the start symbol.,7,8
11149,219182113,.i) is O(n) and the number of occurrences of such a non-terminal symbol X in the RHSs is also 0( n ).,16,17
11150,219182113,"The idea of our algorithm is based upon the remark that each time a symbol ; is pushed on a stack at a given place, this very symbol should be popped at some other place.",14,15
11151,219182113,"The idea of our algorithm is based upon the remark that each time a symbol ; is pushed on a stack at a given place, this very symbol should be popped at some other place.",28,29
11152,219182113, Our algorithm will simply compose the previous relations in order to relate an object where a symbol is pushed to the object(s) where this very symbol is popped in order to finally answer the question: is there at least one valid spine between o 1 and o 2 where o 1 is initial and o 2 is final?,17,18
11153,219182113, Our algorithm will simply compose the previous relations in order to relate an object where a symbol is pushed to the object(s) where this very symbol is popped in order to finally answer the question: is there at least one valid spine between o 1 and o 2 where o 1 is initial and o 2 is final?,27,28
11154,219182113,"line (23) The classical algorithm for the elimination of useless symbol is performed in time linear with the size of the grammar, so in our case it will take 0( n 3 ). •",12,13
11155,219182113,"LIG conditions checking relies upon a very simple principle which can be expressed by binary relations; • the recognition test is simply performed by composition of the previous relations; • therefore, no symbol stack computation is needed; • it can be applied to unrestricted fair LIGs (though the 0( n 6 ) limit can then be exceeded).",35,36
11156,219182113,Our argument is that it wastes time to compute symbol stacks in 0( n 6 ) along paths which can be discovered as syntactically illegal in 0( n 3 ).,9,10
11157,219182113,"An Example In this section, we illustrate our algorithm with aLIG L = ( {S, T}, { a, b, c}, {, a , 'Yb , , c }, PL , S) where PL contains the following productions: We can remark that in L the key part is played by the middle c, introduced by the last production T() -c, and that this grammar is non ambiguous, while in G the symbol c, introduced by the last production T -c, is only a separator between w and w' and that this grammar is ambiguous (any occurrence of c may be this separator).",89,90
11158,219182113,"Its production set Pf; is: (S, X1.. 4) -(S, x The corresponding LIGed forest whose start symbol is s x = (S, x 1 . .",23,24
11159,2304543,"A a(f ) ∈ N are called argument categories, f ∈ F is the function symbol and w > 0 is a weight.",16,17
11160,13495463,The variables a and/3 can be calculated according to the following recursion equations (assuming a start and an end symbol at rank t = 0 and t = T+I):,20,21
11161,219303380,"u R "" E R z } The joint graph fo r the symbol X in N U T is: Jgraph(X) = u Jgraph(Xi) i:name(Xi )=X Example 3.2.",13,14
11162,219303380,"symbol ""c"", a substring parser based on G will parse ""cd"" and a substring parser based • on G' will parse ""cd'"".",0,1
11163,219303380,"The input is given by a grammar G and its reverse grammar G', the Jgraph for each symbol of the grammars, the two parsing tables and an input sentence ao ... a n with an index O ::s; i ::s; n from where the parsing process is supposed to start.",19,20
11164,219303380,"The LR parser with an arbitrary starting point Input: An LR grammar G = (N, T, S, P) and its reverse G', the Jgraph for every symbol in NUT, a sentence w = aoa 1 When all the parsers, both forward and backward, are in the wait state, apply the rendezvous operation wherever p�ssible and go to step 3.",34,35
11165,219303380,"Example 4.1 COSTAGLIOLA On the lookahead symbol • $, the state R 4 of the backward parser built on G', requires the reduc tion ""(1) S := CC"".",6,7
11166,219303380,In the case of 2-D symbolic parsing this infor mation is given by spatial operators that take in input the position of the last visited symbol and return the position of the next symbols to parse.,27,28
11167,219303380,"Example 5.1 The following positional grammar generates a simple subset of the arithmetic expressions: N = {E, S, T, F} T = {+, 1:, (, ), id, num} E is the starting symbol POS = { >, _} p = { E := E > + > T I T T := s >TI F s := L -id F := id _ id I num I ( > E > ) } Definition 3 (positional grammar) A context-where the characters '>' and '-' stand for ""horfree positional grammar PG is a six-tuple (N, T, izontal concatenation"" and ""under concatena-S, P, POS, PE) where: tion"" , respectively.",47,48
11168,219303380,"A positional sentence is: N is a finite non-empty set of non-terminal symbols T is a finite non-empty set of terminal symbols Nn T=</J S E N is the starting symbol P is a finite set of productions POS is a finite set of spatial relation identifiers PE is a positional evaluator Each production in P has the fa llowing fo rm: ""5 > + > L -i > ( > x -i > + > y -i > ) "" From its evaluation the particular positional eval uator PE for this grammar produces the following picture: 5 + 1:/xi + Yi)-A more detailed definition of PE for this type of grammars can be found in ( Costagliola et al.,",37,38
11169,219303380,This can be done by marking each visited symbol and looking for unmarked ones.,8,9
11170,219303380,"Looking at Figure p.l, if state Is is reached by shifting a terminal 'a' whose position is (i, j), then the next symbol to process is the terminal Down(i, j) in positioQ (i, j-1).",29,30
11171,219303380,"In this type of pictorial parsing, the spatial relations are defined such that the position of the next symbol only depends on the last symbol processed.",19,20
11172,219303380,"In this type of pictorial parsing, the spatial relations are defined such that the position of the next symbol only depends on the last symbol processed.",25,26
11173,219303380,COSTAGLIOLA More complex forms include the possibility to calculate the next symbol based on the .positions of the elements of the last handle or of the whole input so far visited. ',11,12
11174,235899306,"2019) and pre-train our character-level sequence models using a masked language modeling objective: during training, we randomly replace 15% of the characters with a special mask symbol and the models are trained to predict the identity of those characters in the original texts.",34,35
11175,219305622,"6) GERMAN : A term x(Level, Category) represents a phrasal projection, where Level is the X level taking the values {0,1,2}, and Category takes as its value an atomic category symbol from the set {N,V,A,P,Adv,Det ,Infl,Comp,Conj}.",37,38
11176,7037231,"In this representation, p(m|w) is the probability of a symbol m being the meaning of a word w. In the absence of any prior knowledge, all symbols are equally likely to be the meaning of a word.",11,12
11177,7037231,We estimate the alignment probabilities of words and meaning symbols based on a localized version of the principle of contrast: that a meaning symbol in a scene is likely to be highly associated with only one of the words in the corresponding utterance.,24,25
11178,7037231,"1 For a symbol m ∈ S (t) and a word w ∈ U (t) , the higher the probability of m being the meaning of w (according to p(m|w)), the more likely it is that m is aligned with w in the current input.",3,4
11179,7037231,"To accommodate for such cases, a dummy word is added to each utterance before the alignment probabilities are calculated, in order to let a meaning symbol not be (strongly) aligned with any of the words in the current utterance.",27,28
11180,7037231,The association score of a word and a symbol is basically a weighted sum of their cooccurrence counts.,8,9
11181,7037231,"The model then uses these association scores to update the meaning of the words in the current input: p (t) (m|w ) = assoc (t) (m, w ) + λ m j ∈M assoc (t) (m j , w ) + β × λ (3) where M is the set of all symbols encountered prior to or at time t, β is the expected number of symbol types, and λ is a small smoothing factor.",80,81
11182,7037231,"This formulation results in a uniform probability of 1/β over all m ∈ M for a novel word w, and a probability smaller than λ for a meaning symbol m that has not been previously seen with a familiar word w. Our model updates the meaning of a word every time it is heard in an utterance.",31,32
11183,7037231,We get these from an input-generation lexicon that contains a symbol associated with each word as its semantic referent.,12,13
11184,7037231,"Therefore, the default probability of a symbol for a novel word will be 1/8500.",7,8
11185,7037231,"Therefore, the probability of a previously unseen symbol for it (which, based on Eq. (",8,9
11186,133611643,S ∈ N is the start symbol with dim(S) = 1; 4.,6,7
11187,218681433,"Consider a context-free grammar consisting of a set C of ""categories"" or non-terminal symbols, including one distinguished symbol s (for ""sentence""); a set of terminal symbols T (""lexical slots""), none of which are also in C, a lexicon L which is basically a set of words and an indication of the kind of lexical slot each word can fill, and a set of rewrite rules R of form c --~ vl -""vn where c is a symbol in C', and the string on the right hand side vl...vn consists of one or more symbols in T or C. A sentence is derived by writing s, then rewriting s by the string Ul'""um on the right hand side of any rule in R of form s -~ ul"".um, then rewriting any ui which is non-terminal by some rule of form ui --~ wl...wp and so on.",24,25
11188,218681433,"Consider a context-free grammar consisting of a set C of ""categories"" or non-terminal symbols, including one distinguished symbol s (for ""sentence""); a set of terminal symbols T (""lexical slots""), none of which are also in C, a lexicon L which is basically a set of words and an indication of the kind of lexical slot each word can fill, and a set of rewrite rules R of form c --~ vl -""vn where c is a symbol in C', and the string on the right hand side vl...vn consists of one or more symbols in T or C. A sentence is derived by writing s, then rewriting s by the string Ul'""um on the right hand side of any rule in R of form s -~ ul"".um, then rewriting any ui which is non-terminal by some rule of form ui --~ wl...wp and so on.",95,96
11189,218681433,"Each symbol appearing in the derivation is represented by a node of the tree; it dominates the constituent of which it is the highest node, and may be used to represent that constituent. (",1,2
11190,218681433,"Furthermore we assume grammatical congruence: there is a one-to-one connection between the rulesRA of language A and RB of language B --if RA contains a rule c ~ Vl""""Vn, then RB must contain a rule c --+ Ul...~Zn, where each symbol in Vl""""Vn has its counterpart in Ul'""un, and vice-versa, though the order of the terms in one string will not in general correspond to the order of the terms in the other.",49,50
11191,218681433,"Not only must each monolingual sentence be derivable in exactly one way, but each rule may contain any one symbol only once on its right hand side.",20,21
11192,1211840,"An edge contains a English non-terminal (NT) symbol (NP, VP, etc), border words for LM combination, pointers to child edges, and a score.",11,12
11193,10104995,"Let the grammar of the S-parser be denoted as a 5-tuple G = (V, Σ, P, S, D) where V is the variable (non-terminal) set,Σ is the terminal symbol set,P is the production rule set, S is the sentence symbol, and D is a function defined on P for rule probability (Jurafsky and Martin, 2009) .",44,45
11194,10104995,"Let the grammar of the S-parser be denoted as a 5-tuple G = (V, Σ, P, S, D) where V is the variable (non-terminal) set,Σ is the terminal symbol set,P is the production rule set, S is the sentence symbol, and D is a function defined on P for rule probability (Jurafsky and Martin, 2009) .",58,59
11195,361281,"We use the symbol W to refer to the random variable related to the word, G for the associated gold standard tag, and T for the tag produced by one of our algorithms.",3,4
11196,14001621,"There is also a failure transition leaving the state, labeled with some reserved symbol φ, which can only be traversed if the next symbol in the input does not match any transition leaving the state.",14,15
11197,14001621,"There is also a failure transition leaving the state, labeled with some reserved symbol φ, which can only be traversed if the next symbol in the input does not match any transition leaving the state.",25,26
11198,1443318,"All our models were built with the same 30367word vocabulary, which includes the sentence-end symbol and a special symbol for out-of-vocabulary words (UNK).",17,18
11199,1443318,"All our models were built with the same 30367word vocabulary, which includes the sentence-end symbol and a special symbol for out-of-vocabulary words (UNK).",21,22
11200,18419162,"This index is represented as a weighted finite-state transducer (SearchFST) as shown in Figure 3 where w d is the input symbol, d is the output symbol and c (w d ,d) is the weight of that arc.",25,26
11201,18419162,"This index is represented as a weighted finite-state transducer (SearchFST) as shown in Figure 3 where w d is the input symbol, d is the output symbol and c (w d ,d) is the weight of that arc.",31,32
11202,2321969,"The start state is (<s>, <s>, 0, 0, 0, 0) where <s> is the start symbol in the language model.",28,29
11203,2321969,"This transition takes the form (w 1 , w 2 , N, l, m, r) (N,N +1,</s>) − −−−−−−−−−− → END For this transition, we define the score as score = h(</s>|w 1 , w 2 ); thus this transition incorporates the end symbol </s> in the language model.",55,56
11204,3161786,"An STSG is a 5-tuple V n , V t , S, T, w , where V n is the set of non-terminal symbols; V t is the set of terminal symbols; S ∈ V n is the start symbol; T is a set of elementary trees, such that for every t ∈ T the unique root node r(t) ∈ V n , the set of internal nodes i(t) ⊂ V n and the set of leaf nodes l(t) ⊂ V n ∪ V t ; finally, w : T → [0, 1] is a probability (weight) distribution over the elementary trees, such that for any t ∈ T , t ∈R(t) w(t ) = 1, where R(t) is the set of elementary trees with the same root label as t. It will prove useful to also define the set of all possible trees θ over the defined alphabets (with the same conditions on root, internal and leaf nodes as for T ), and the set of all possible complete parse trees Θ (with r(t) = S and all leaf nodes l(t) ⊂ V t ).",47,48
11205,18837411,"The start symbol is N $ , where '$' is a distinguished symbol used to indicate the beginning and end of sentences.",2,3
11206,18837411,"The start symbol is N $ , where '$' is a distinguished symbol used to indicate the beginning and end of sentences.",14,15
11207,14287962,"3, where θ correspond to K = |N | multinomial distributions, where each distribution attaches probabilities to rules with a specific left hand symbol.",25,26
11208,5972337,Each symbol in the alphabet is a character.,1,2
11209,5972337,"Mathematically, word strings are nothing but symbol strings, with each symbol representing a word in the dictionary.",7,8
11210,5972337,"Mathematically, word strings are nothing but symbol strings, with each symbol representing a word in the dictionary.",12,13
11211,7672793,The left-hand side of the rule is constituted by the original root symbol R of the fragment raised to a metanonterminal R .,14,15
11212,10366378,"Parser control The input to the parser is a string w n 0 , a grammar G, a mapping Φ from derivations to feature vectors, and a parameter vector ᾱ. The grammar G = (V, T, S † , S, C, B) consists of a set of non-terminal symbols V , a set of terminal symbols T , a start symbol S † ∈ V , an end-ofconstituent symbol S ∈ V , a set of ""allowable chains"" C, and a set of ""allowable triples"" B. S is a special empty non-terminal that marks the end of a constituent.",70,71
11213,10366378,"Parser control The input to the parser is a string w n 0 , a grammar G, a mapping Φ from derivations to feature vectors, and a parameter vector ᾱ. The grammar G = (V, T, S † , S, C, B) consists of a set of non-terminal symbols V , a set of terminal symbols T , a start symbol S † ∈ V , an end-ofconstituent symbol S ∈ V , a set of ""allowable chains"" C, and a set of ""allowable triples"" B. S is a special empty non-terminal that marks the end of a constituent.",80,81
11214,10366378,"Each chain is a sequence of non-terminals followed by a terminal symbol, for example S † → S → NP → NN → Trash .",13,14
11215,10366378,"The triples specify which nonterminals Z are allowed to follow a non-terminal Y under a parent X. For example, the triple S,NP,VP specifies that a VP can follow an NP under an S. The triple NP,NN, S would specify that the S symbol can follow an NN under an NP -i.e.,",52,53
11216,10366378,"that the symbol NN is allowed to be the final child of a rule with parent NP The initial state of the parser is the input string alone, w n 0 .",2,3
11217,14888137,"Each symbol has an observation likelihood b i (o t ), which gives the probability of observing o in state i at time t. The transition and emission probabilities are learnt during training using the Baum-Welch algorithm.",1,2
11218,9337654,Here a word w q i (e.g old) is the input symbol for a set of arcs whose output symbol is the index of the QA pairs where old appears in the question.,13,14
11219,9337654,Here a word w q i (e.g old) is the input symbol for a set of arcs whose output symbol is the index of the QA pairs where old appears in the question.,21,22
11220,232104884,"Unlike PCFGs, which assign each grammar rule r a non-negative scalar π r such that r:A γ π r = 1 for each given left-hand-side symbol A, C-PCFGs relax the strong context-free assumption of PCFGs by assuming that rule probabilities follow a compound distribution: π r = g r (z; θ), z ∼ p(z) , 2 Start rules generate a nonterminal symbol from the start symbol S (e.g., S A), preterminal rules generate a word from a nonterminal symbol (e.g., A w), and nonterminal rules are binary rules of the form A BC, which involve only nonterminal symbols.",34,35
11221,232104884,"Unlike PCFGs, which assign each grammar rule r a non-negative scalar π r such that r:A γ π r = 1 for each given left-hand-side symbol A, C-PCFGs relax the strong context-free assumption of PCFGs by assuming that rule probabilities follow a compound distribution: π r = g r (z; θ), z ∼ p(z) , 2 Start rules generate a nonterminal symbol from the start symbol S (e.g., S A), preterminal rules generate a word from a nonterminal symbol (e.g., A w), and nonterminal rules are binary rules of the form A BC, which involve only nonterminal symbols.",81,82
11222,232104884,"Unlike PCFGs, which assign each grammar rule r a non-negative scalar π r such that r:A γ π r = 1 for each given left-hand-side symbol A, C-PCFGs relax the strong context-free assumption of PCFGs by assuming that rule probabilities follow a compound distribution: π r = g r (z; θ), z ∼ p(z) , 2 Start rules generate a nonterminal symbol from the start symbol S (e.g., S A), preterminal rules generate a word from a nonterminal symbol (e.g., A w), and nonterminal rules are binary rules of the form A BC, which involve only nonterminal symbols.",85,86
11223,232104884,"Unlike PCFGs, which assign each grammar rule r a non-negative scalar π r such that r:A γ π r = 1 for each given left-hand-side symbol A, C-PCFGs relax the strong context-free assumption of PCFGs by assuming that rule probabilities follow a compound distribution: π r = g r (z; θ), z ∼ p(z) , 2 Start rules generate a nonterminal symbol from the start symbol S (e.g., S A), preterminal rules generate a word from a nonterminal symbol (e.g., A w), and nonterminal rules are binary rules of the form A BC, which involve only nonterminal symbols.",102,103
11224,225067186,"From each token, a chain of nodes is generated until the stop symbol ∅ is predicted.",13,14
11225,225067186,"In that case, h node 1 will be a weighted sum of LSTM representations of other nodes and input tokens, where the weights are defined by O. Similarly, the termination symbol ∅ for the token 'opinion' is predicted from its hidden representation; we refer to this representation as h tail 1 , where 1 is the position of 'opine' in the sentence.",33,34
11226,9847590,"Grammar and Parsing Preliminaries Given a probabilistic context-free grammar (PCFG) defined as the tuple (V, T, S † , P, ρ) where V is the set of non-terminals, T is the set of terminals, S † is a special start symbol, P is the set of grammar productions, and ρ is a mapping of grammar productions to probabilities, we divide the set of non-terminals V into two disjoint subsets V POS and V PHR such that V POS contains all pre-terminal part-of-speech tags and V PHR contains all phrase-level non-terminals.",53,54
11227,9847590,"There is some flex- ibility in this process, but most pre-processing efforts include (1) affixing a ROOT unary production to the root symbol of the original tree, (2) removal of empty nodes, and (3) striping functional tags and cross-referencing annotations.",27,28
11228,10031285,"We shall assume an order ≺ or on Σ which we shall extend to Σ * in the normal way by saying that u ≺ v if |u| < |v| or |u| = |v| and u is lexicographically before v. A grammar is a quadruple G = V, Σ, P, S where Σ is a finite alphabet of terminal symbols, V is a finite alphabet of variables or non-terminals, P is a finite set of production rules, and S ∈ V is a start symbol.",92,93
11229,10031285,"Distributional learning The key to the Harris approach for learning a language L, is to look at pairs of strings u and v and to see whether they occur in the same contexts; that is to say, to look for pairs of strings of the form lur and lvr that are both in L. This can be taken as evidence that there is a non-terminal symbol that generates both strings.",70,71
11230,10031285,"For all strings a of length one (i.e. letters of Σ), we add productions of the form [a] → a. • The start symbol is the congruence class which contains all the strings of the language.",28,29
11231,993603,"It can be easily seen that we must assign probability 1 to all transitions except τ c and τ d , since this is the only pair of distinct transitions that can be applied for one and the same top-of-stack symbol, viz.",44,45
11232,993603,"A CFG G is a tuple (Σ, N, S, R), with Σ and N the sets of terminals and nonterminals, respectively, S the start symbol and R the set of rules.",32,33
11233,993603,"Note that in our notation, stacks grow from left to right, i.e., the top-most stack symbol will be found at the right end.",20,21
11234,993603,"Without loss of generality, we assume that combinations of different types of transitions are not allowed for a given stack symbol.",21,22
11235,993603,"More precisely, for each stack symbol X = X f in , the PDA can only take transitions of a single type (push, pop or swap).",6,7
11236,993603,"For each string w ∈ Σ *  and each complete computation c on w, f (out(c)) = d is a (leftmost) derivation of w. Furthermore, each symbol from R occurs as often in out(c) as it occurs in d. • Conversely, for each string w ∈ Σ *  and each derivation d of w, there is precisely one complete computation c on w such that f (out(c)) = d. If c is a complete computation, we will write f (c) to denote f (out(c)).",34,35
11237,993603,"Informally, this means that when a subcomputation starts with some stack α and some push transition τ , then solely on the basis of τ we can uniquely determine what stack symbol Z 1 = Z 2 will be on top of the stack in the firstly reached configuration with stack height equal to |α|.",32,33
11238,993603,"We now map (A, p A ) to a language-equivalent PCFG (G , p G ), G = (Σ  , Q, X in , R ), where R contains the following rules with the specified associated probabilities: • X → YZ with p G (X → YZ ) = p A (X → XY ), for each X → XY ∈ ∆ with Z the unique stack symbol such that there is at least one transition XY → Z with Y ; Y ; • X → xY with p G (X → xY ) = p A (X x → Y ), for each transition X x → Y ∈ ∆; • Y → ε with p G (X → ε) = 1, for each stack symbol Y such that there is at least one transi- tion XY → Z ∈ ∆ or such that Y = X f in .",81,82
11239,993603,"We now map (A, p A ) to a language-equivalent PCFG (G , p G ), G = (Σ  , Q, X in , R ), where R contains the following rules with the specified associated probabilities: • X → YZ with p G (X → YZ ) = p A (X → XY ), for each X → XY ∈ ∆ with Z the unique stack symbol such that there is at least one transition XY → Z with Y ; Y ; • X → xY with p G (X → xY ) = p A (X x → Y ), for each transition X x → Y ∈ ∆; • Y → ε with p G (X → ε) = 1, for each stack symbol Y such that there is at least one transi- tion XY → Z ∈ ∆ or such that Y = X f in .",148,149
11240,1657857,"This assumption may be motivated by pragmatic considerations, as such a proper model is easy to train by relative frequency estimation: count the number of times a transition is applied with respect to a treebank, and divide it by the number of times the relevant stack symbol (or pair of stack symbols) occurs at the top of the stack.",49,50
11241,1657857,"An LR parser is constructed on the basis of a CFG that is augmented with an additional rule S † → S, where S is the former start symbol, and the new nonterminal S † becomes the start symbol of the augmented grammar.",29,30
11242,1657857,"An LR parser is constructed on the basis of a CFG that is augmented with an additional rule S † → S, where S is the former start symbol, and the new nonterminal S † becomes the start symbol of the augmented grammar.",40,41
11243,1657857,The initial stack symbol is p init = {S † → • S}.,3,4
11244,1657857,"We define the operation goto on a set p of dotted rules and a grammar symbol X ∈ Σ ∪ N as: goto(p, X) = {A → αX • β | (A → α • Xβ) ∈ closure(p)} The set of LR states is the smallest set such that: 1.",15,16
11245,1657857,"For the PDT that implements the LR strategy, the stack symbols are the LR states, plus symbols of the form [p; X], where p is an LR state and X is a grammar symbol, and symbols of the form (p, A, m), where p is an LR state, A is the left-hand side of some rule, and m is the length of some prefix of the right-hand side of that rule.",39,40
11246,1657857,"The final stack symbol is p final = (p init , S † , 0).",3,4
11247,1657857,"For properness we have to assume that for each stack symbol P , we either have one or more transitions of the form P It has been shown that imposing properness is without loss of generality in the case of PDTs constructed by a wide range of parsing strategies, among which are top-down parsing and left-corner parsing.",10,11
11248,1657857,"Now we have to assume that for each stack symbol R, we either have one or more transitions of the form P → P R is 1 for each P .",9,10
11249,1657857,"Start symbol is p final = (p init , S † , 0).",1,2
11250,237452545,"For our models training, verdicts, pros, and cons were joined to one sequence with a separator symbol indicating boundaries.",19,20
11251,703412,"Note that the symbol $ is a kind of NULL suffix, which shows that the parent node is itself a suffix and thus corresponds to the end of an actual word.",3,4
11252,221971028,"A CFG is defined as a 5-tuple G = (S, N , P, Σ, R) where S is the start symbol, N is a finite set of nonterminals, P is a finite set of preterminals, Σ is a finite set of terminals, 2 and R is a set of production rules in the Chomsky normal form: S A, A ∈ N , A BC, A ∈ N , B, C ∈ N ∪ P, T w, T ∈ P, w ∈ Σ .",27,28
11253,221971028,"Depending on the rule type, g r (•; θ) takes one of these forms: π S A = exp(u T A f s ([w S ; z])) A ∈N exp(u T A f s ([w S ; z])) , π A BC = exp(u T BC [w A ; z]) B ,C ∈N ∪P exp(u T B C [w A ; z]) , π T w = exp(u T w f t ([w T ; z])) w ∈Σ exp(u T w f t ([w T ; z])) , where u is a parameter vector, w N is a symbol embedding and N ∈ {S} ∪ N ∪ P. [•; •] indicates vector concatenation, and f s (•) and f t (•) encode the input into a vector (parameters are dropped for simplicity).",131,132
11254,12386976,Here a word w q i (e.g old) is the input symbol for a set of arcs whose output symbol is the index of the QA pairs where old appears in the question.,13,14
11255,12386976,Here a word w q i (e.g old) is the input symbol for a set of arcs whose output symbol is the index of the QA pairs where old appears in the question.,21,22
11256,6624035,"The symbol + represents a positive instance, namely, a mention pair that refers to the same entity.",1,2
11257,1140108,"This LF is a representation of the semantic content that comes from the sentence, and would be input to a context-dependent understanding component in a full dialog system, for example to find the date that the symbol F RI refers to.",40,41
11258,12069059,"The fuction c(•) maps a character into one of six character types: symbol, alphabet, arabic, number hiragana, katakana, and kanji.",14,15
11259,12069059,"F6: concept tag of u ∧ concept tag of v, Method Precision F7: concept tag of u ∧ concept tag of v ∧ whether u and v are in the same sentence, F8: concept tag of u ∧ concept tag of v ∧ whether Ac exists between u and v ∧ whether a function word exists between u and v, F9: concept tag of u ∧ concept tag of v ∧ function words between u and v. Here the symbol ∧ indicates the combination of individual features.",86,87
11260,544132,Grammar Development Table 4 lists the symbol refinements used in our grammar.,6,7
11261,544132,"A PTSG is a 5-tuple V, Σ, R, ♦, θ where c ∈ V are non-terminals; t ∈ Σ are terminals; e ∈ R are elementary trees; 5 ♦ ∈ V is a unique start symbol; and θ c,e ∈ θ are parameters for each tree fragment.",46,47
11262,544132,A Appendix A.1 Notes on the Rising Factorial The rising factorial-also known as the ascending factorial or Pochhammer symbol-arises in the context of samples from a Dirichlet process (see Prop.,20,21
11263,5265778,"Each arc in the FST has an or adult-language word as input symbol, and a possibly errorful childlanguage word or as output symbol.",14,15
11264,5265778,"Each arc in the FST has an or adult-language word as input symbol, and a possibly errorful childlanguage word or as output symbol.",25,26
11265,2399250,"Here Q is an additional symbol e.g. ""initial"" for identifying a specific state of this transducer.",5,6
11266,919901,"Similarly, when the parser predicts a node(n) action, we compose the embedding of the nonterminal symbol that is added (n) with the representation of the element at the top of the stack (s), that might represent a spine or a single terminal symbol.",18,19
11267,919901,"Similarly, when the parser predicts a node(n) action, we compose the embedding of the nonterminal symbol that is added (n) with the representation of the element at the top of the stack (s), that might represent a spine or a single terminal symbol.",50,51
11268,919901,"1 ) where W is a learned parameter matrix, s represents the token in the stack (and its partial spine, if non-terminals have been added to it) and n represents the non-terminal symbol that we are adding to s; b is a bias term.",40,41
11269,1537286,"Thus if we denote the final decoded path (under some decoding scheme, which will become apparent next) in the j th sub lattice by π * j and the concatenation symbol by '•', then W * = W [π * 1 ] • W [π * 2 ] • . . . •",33,34
11270,43254881,"By evaluating this term over a string algebra A which interprets the symbol * as string concatenation, we obtain ""John loves Mary"" ∈ L(G).",12,13
11271,43254881,"The symbol before the curly brack- ets can be one of N 0 or N 1 , to allow the grammar to make finer distinctions beyond the source information.",1,2
11272,6508977,"The nodes labeled with the symbol λ, e.g., λ and λx , correspond to operators.",5,6
11273,309404,"Sum of probabilities of all derivations Assume a probabilistic context-free grammar G, represented by a 5-tuple (Σ, N, S, R, p), where Σ and N are two finite disjoint sets of terminals and nonterminals, respectively, S ∈ N is the start symbol, R is a finite set of rules, each of the form A → α, where A ∈ N and α ∈ (Σ ∪N ) * , and p is a function from rules in R to real numbers in the interval [0, 1].",55,56
11274,309404,The probability p(w) of a string w is the sum of all complete derivations deriving that string from the start symbol: p(w) = d: S d ⇒w p(d).,21,22
11275,309404,"For A ∈ N and α ∈ (N ∪ Σ) * , we write f (A, α) to denote the number of occurrences of symbol A within string α.",29,30
11276,309404,"In particular, the systems of equations for C 1 and C 3 change fundamentally if the infix is extended by one more symbol, which seems to at least make incremental computation very difficult, if not impossible.",23,24
11277,5195900,"-./+& % (b) Figure 1 : An example of an initial q-automaton (a), and the refined q-automaton (b) Each state corresponds to a context (only state 6 has a non-empty context) and each edge represents the emission of a symbol.",55,56
11278,199661439,"Each Ui j denotes the set of state vertices created while parsing the j-th null construct after the i-th input symbol a, is shifted and before the shifting of next actual input symbol ai+1 takes place.",24,25
11279,199661439,"Each Ui j denotes the set of state vertices created while parsing the j-th null construct after the i-th input symbol a, is shifted and before the shifting of next actual input symbol ai+1 takes place.",37,38
11280,199661439,".A are the viable prefixes when the parser scans the first input symbol ""x"".",12,13
11281,199661439,"For the same reason, we are not combining identical symbol vertices which are adjacent to the same state vertex, (a measure of optimization suggested in [8] ), in the illustrated examples or in the algorithm that to follow.",10,11
11282,658259,"The symbol ""@"" indicates the best result across systems.",1,2
11283,56824959,"Each stage Ui corresponds to the zth symbol x, from the input string.",7,8
11284,56824959,"All processes are synchronized, scanning the same symbol at the same time.",8,9
11285,56824959,"It then follows the general procedure outlined above for each symbol in the input string, continuing until there are either no leaves added to Ux (i.e., no more active processes), which denotes rejection, or a process executes the accept action on scanning the n + 1st input symbol 'H,' which denotes acceptance.",10,11
11286,56824959,"It then follows the general procedure outlined above for each symbol in the input string, continuing until there are either no leaves added to Ux (i.e., no more active processes), which denotes rejection, or a process executes the accept action on scanning the n + 1st input symbol 'H,' which denotes acceptance.",53,54
11287,56824959,"Each vertex of the graph-structured stack is a triple (i,s,l), where i is an integer corresponding to the ith input symbol scanned (at which point the vertex was created as a leaf), 5 is a parse state (corresponding to a row of the parse table), and / is a set of parent vertices.",29,30
11288,56824959,"In REC, [1] adds the end-of-9entence symbol H ' to the end of the input string; [2] initializes the root of the graph-structured stack; [3] iterates through the symbols of the input string.",11,12
11289,56824959,"On each symbol X,-, [4] processes the vertices (denoting the active processes) of successive C/,-_i's, adding each vertex to P to signify that it has been processed.",2,3
11290,56824959,"Interestingly enough, Earley states that almost all LR(k) grammars are bounded state, as well, which suggests that T om ita's algorithm, given fc-symbol look ahead, should perform with little loss of efficiency as compared to a standard LR(fc) algorithm when the grammar is ""close"" to LR(fc).",30,31
11291,219182005,"Non-projective context-free dependency grammar (NCFDG) is a quadruple (N, T, S, P) , where N, T are sets of nonterminals and terminals, SEN is a starting symbol and P is a set of rewriting rules of the fo rm A ➔L BC or A ➔R BC, A EN , B, C E V where V = NUT.",39,40
11292,219182005,"The relation of immediate derivation ⇒ is defined as: rAst ⇒ rBsCt, if (A ➔L BC) E P rsAt ⇒ rBsCt, if (A ➔R BC) E P, where A E N, B,C E V, r, s, t E V* Th e relation of derivation is the transitive and reflexive closure of the relation ⇒. NCFD grammar G defines language L( G) as a .set of all words t E T* that can be derived fro m the starting symbol S. We say that L(G) is recognized (g enerated) by G. Remark 2.",98,99
11293,219182005,"A Tree of a word a1 a2 ... a n E T* dominated by the symbol X E V, created by NCFD G G is a binary branching tree Tr fu llfilling the fo llowing conditions a) a node of Tr is a triad [A, i, j], where A E V and i, j E 1 .. ; = n. Th e number i is the horizontal index of the node and j is the vertical index.",16,17
11294,219182005,"b) a node [A, i, m] of Tr has daughters [B,j,p], [C, k, r] only if 1} j < k, m = p + 1, j = i Tree instead of the term Tree dominated by the symbol S {where S is a starting symbol).",56,57
11295,219182005,"b) a node [A, i, m] of Tr has daughters [B,j,p], [C, k, r] only if 1} j < k, m = p + 1, j = i Tree instead of the term Tree dominated by the symbol S {where S is a starting symbol).",64,65
11296,219182005,"Those items are represented by six tuples [symbol, position, coverage, ls, rs, rule] containing enough information necessary for both the reconstruction of the parsing process and for the creation of a Trees representing the structures of the parsed sentence.",8,9
11297,219182005,"This algorithm works with a list D of items D i = [symbol , position, coverage, ls, rs, rule] where every item corresponds to some derivations of a subsequence of the word a 1 a2 .. The basic idea of the algorithm is to add new items to the list as long as possible.",13,14
11298,219182005,"symbol, -, coverage, -,-,-,-] .",0,1
11299,219182005,"In contrast to other formalisms, the form of the description of meta-rules does not only declare constraints on the applicability of a particular rule (symbol A in case of A ➔ x BC rule) , but also explicitly defines the order of applying the constraints together with forming the resulting data structure (symbol A) from B or from C (with respect to whether the symbol x is equal to L or to R).",28,29
11300,219182005,"In contrast to other formalisms, the form of the description of meta-rules does not only declare constraints on the applicability of a particular rule (symbol A in case of A ➔ x BC rule) , but also explicitly defines the order of applying the constraints together with forming the resulting data structure (symbol A) from B or from C (with respect to whether the symbol x is equal to L or to R).",58,59
11301,219182005,"In contrast to other formalisms, the form of the description of meta-rules does not only declare constraints on the applicability of a particular rule (symbol A in case of A ➔ x BC rule) , but also explicitly defines the order of applying the constraints together with forming the resulting data structure (symbol A) from B or from C (with respect to whether the symbol x is equal to L or to R).",72,73
11302,59638266,"0 < i , j < m , ^ j S --.4,- Ai -■ BjAi At --Bj Bj -a The parser shifts over the first input symbol a to the state shown in (4) (3) [Bj -♦ a-] 0 < j < m (4) This is a non-deterministic state, since all of the m reductions Bj -a are possible parsing actions from this state.",27,28
11303,59638266,"5) Ai --Bj.-, • .4, Ai -Bkr Ai -BjAi Ai -Bj Bj -a After shifting over the next input symbol the parser again reaches the same ambiguous state as before, namely the state shown in (4).",22,23
11304,59659044,"In our system, we assume that an acoustic processing device provides a symbol sequence for a given speech input In this paper, we assume that a phoneme-level sequence is provided to the system1.",13,14
11305,59659044,"Activation: For each input symbol, a corresponding node is activated and an A-Marker is created.",5,6
11306,59659044,A confusion matrix defines the output probability of a phoneme when an input symbol is given.,13,14
11307,59659044,"It is a measure of the distance between symbols and phonemes as well as a measure of the cost of hypotheses that interpret the symbol i, as the phoneme pj.",24,25
11308,59659044,"In the context-dependent model, the confusion matrix will defined as a,y* which gives a probability of a symbol /, to be interpreted as a phoneme pj at a transition ft.",23,24
11309,59659044,"A transition matrix defines the transition probability which is a probability of a symbol /,♦ i to follow a symbol I,-.",13,14
11310,59659044,"A transition matrix defines the transition probability which is a probability of a symbol /,♦ i to follow a symbol I,-.",21,22
11311,59659044,For an input symbol A-Markers are passed up to all phoneme nodes that have a probability^) greater than the threshold (Th).,3,4
11312,59659044,"When the next input symbol il+i generates an A-Marker that hits the P-Marker on the i+l-th element, the P-Marker is moved using the dual prediction method The probability density measure computed on the P-Marker is as follows: ppm(i) = ppm{i -1) x a*.,,*.,",4,5
11313,2670385,"Notes can be modified by ""accidentals"" -a sharp or a flat symbol that can change the note by half a tone.",13,14
11314,57228482,"It is not difficult to see that, when we start a left-to-right top-down construction of a parse tree with respect to G at the leftmost symbol of a string w and a bottom-up right-to-left construction of a parse tree with respect to G* at the rightmost symbol of w, thenassuming the grammar is unambiguous -the resulting partial parse trees can be tied together and a parse tree of w with respect to G is obtained.",32,33
11315,57228482,"It is not difficult to see that, when we start a left-to-right top-down construction of a parse tree with respect to G at the leftmost symbol of a string w and a bottom-up right-to-left construction of a parse tree with respect to G* at the rightmost symbol of w, thenassuming the grammar is unambiguous -the resulting partial parse trees can be tied together and a parse tree of w with respect to G is obtained.",60,61
11316,57228482,"However, at least in theory, Fischer's method allows any symbol in the input string as the starting point of each SPM.",12,13
11317,57228482,"Suppose we want to start an SPM immediately to the left of some symbol a. In the LR-parse table M we can find which states have a non-empty entry for symbol a. For each of these states the SPM which will be started, possesses a stack containing this state only.",13,14
11318,57228482,"Suppose we want to start an SPM immediately to the left of some symbol a. In the LR-parse table M we can find which states have a non-empty entry for symbol a. For each of these states the SPM which will be started, possesses a stack containing this state only.",34,35
11319,57228482,"Hence, the SPM is started with just those states that can validly scan the next symbol in the string. (",16,17
11320,57228482,2) Scan the next symbol.,5,6
11321,57228482,Let a be the symbol to be scanned.,4,5
11322,57228482,"Q (a) if R(Q) = ( An SPM which has been stopped can be restarted-If an SPM is about to scan a symbol already scanned by an SPM to its immediate right, then a merge of the two SPM's will be attempted.",27,28
11323,57228482,"We cannot, as was done in the initialization, simply take those states which allow a scan of the next input symbol.",23,24
11324,57228482,The entry may allow reduction of a production rule but at the same time it may allow shifting of the next input symbol onto the stack.,22,23
11325,57228482,Each occurrence of a (pre-)terminal or a nonterminal symbol in the grammar rules corresponds with an agent with modest pro cessing power and internal memory.,9,10
11326,57228482,A next step is to eliminate all double agents and give their tasks to the agents which correspond with the rightmost symbol of a grammar rule.,21,22
11327,57228482,Another configuration with a reduced number of computing agents is obtained if we have an agent for each nonterminal symbol of the grammar.,19,20
11328,57228482,For each non terminal symbol each entry in the table which is not on the diagonal will represent a configuration of nodes.,4,5
11329,57228482,"In the traditional algorithm a nonterminal symbol X is added to the set of nonterminal symbols associ ated with the entry if there are symbols Y e r, * and Z e such that X -»YZ is in P. In the connec tionist adaptation of the algorithm we already have a node for each nonterminal symbol in entry r,j. Therefore, rather than adding a symbol, here node X at position t^j is made active if node Y at posi tion tik and node Z at position t^j are active.",6,7
11330,57228482,"In the traditional algorithm a nonterminal symbol X is added to the set of nonterminal symbols associ ated with the entry if there are symbols Y e r, * and Z e such that X -»YZ is in P. In the connec tionist adaptation of the algorithm we already have a node for each nonterminal symbol in entry r,j. Therefore, rather than adding a symbol, here node X at position t^j is made active if node Y at posi tion tik and node Z at position t^j are active.",56,57
11331,57228482,"In the traditional algorithm a nonterminal symbol X is added to the set of nonterminal symbols associ ated with the entry if there are symbols Y e r, * and Z e such that X -»YZ is in P. In the connec tionist adaptation of the algorithm we already have a node for each nonterminal symbol in entry r,j. Therefore, rather than adding a symbol, here node X at position t^j is made active if node Y at posi tion tik and node Z at position t^j are active.",68,69
11332,57228482,We assume that there is a node for each terminal symbol in each position at the diagonal of the matrix.,10,11
11333,57228482,The input is accepted as soon as the node for the start symbol in the topmost entry of the column of the last input symbol becomes active.,12,13
11334,57228482,The input is accepted as soon as the node for the start symbol in the topmost entry of the column of the last input symbol becomes active.,24,25
11335,57228482,"In order to activate the top-down unit of the node for the start symbol in the upper right comer of the table we assume that it receives input from its bottom-up counterpart and from the node at position t where n is the length of the input, which is used to represent endmarker $ of the input and which is made active when parsing starts.",15,16
11336,219182318,2) Part-of-speech sequences directly derived from the same non-terminal symbol have similar environments.,16,17
11337,219182318,"Based on these hypotheses, the system finds a set of constituent-like part-of-speech sequences and replaces them with a new symbol.",26,27
11338,219182318,"Our method models natural language on context-free language, which is described by a grammar G = (N, T, P, S), where l\ r is the set of non-terminal symbols, T is the set of terminal symbols, P is the set of rewriting rules and S is the start symbol.",61,62
11339,219182318,"To explain this hypothesis concretely, let us consider the following two different POS sequences: pos a which appears on the right-hand side of a rewriting rule and pos x = pos x 1 I ) _ P(pos • posi) _ f(pos • posi) p pos pos , pos -P(pos) f (pos) Therefore, n-gram statistics, the frequencies of all the symbol sequences appearing in the corpus, are applicable to compute the entropies.",72,73
11340,219182318,"Notice that n is more than one and does not exceed the length of the longest sentence in the corpus, because n-grams containing a symbol for final punctuation mark never appear in rewriting rules, except for ones with Son the left-hand side.",27,28
11341,219182318,POS sequences directly derived from the same non-terminal symbol have similar environ ments.,10,11
11342,219182318,"Selecting Rewriting Rules After clustering, a new set of rewritill'g rules can be obtained from any cluster by putting each of the POS sequences on the right-hand si�e of a rule and introducing a new single non terminal symbol on the left-hand sides.",43,44
11343,219182318,One may expect that a large value for area means that the rewriting rules of the cluster are basic symbol sequences which appear regardless of their global environment.,19,20
11344,219182318,"There are two cases, depending on the number of POS sequences in the cluster which consist of a single non-terminal symbol.",23,24
11345,219182318,"If there is exactly one, that non-terminal symbol is put on the left-hand side to produce rewriting rules, and the rewriting rule which would have that non-terminal symbol on both sides is erased.",10,11
11346,219182318,"If there is exactly one, that non-terminal symbol is put on the left-hand side to produce rewriting rules, and the rewriting rule which would have that non-terminal symbol on both sides is erased.",35,36
11347,219182318,"In the other case, i.e. the number of POS sequences composed of a single non-terminal symbol is zero or more than one, the system introduces a new non-terminal symbol and puts it on the left-hand side to produce rewriting rules.",18,19
11348,219182318,"In the other case, i.e. the number of POS sequences composed of a single non-terminal symbol is zero or more than one, the system introduces a new non-terminal symbol and puts it on the left-hand side to produce rewriting rules.",34,35
11349,219182318,"For example, suppose that the rewriting rule is syn 1 ---+-tag 1 • tag 1 and the following symbol sequence exists in the corpus: ... tag 1 • tag 1 • tag1 ... The rule is applicable to both the first and the last tag 1 • tag1 at the same time.",19,20
11350,219182318,"For example, suppose that there are two rewriting rules such as and the following symbol sequence exists in the corpus: ... tag 1 • tag2 • tag3 ... ( 3) ( 4) In this case both of the rules are applicable.",15,16
11351,219182318,POS sequences directly derived from the same non-terminal symbol have similar environ ments.,10,11
11352,199661633,"Four different chart parsers: TD, LC, CKY, BI Let G be a context-free grammar with S as start symbol.",24,25
11353,499440,"dominance) between the nodes of a tree T will be represented with the symbol _~T and T. Whenever they are unambiguous, the notations -< and _ will be used.",14,15
11354,499440,"In the rewriting operation, we introduce a multiset of new nonterminals and exactly one terminal symbol (the head).",16,17
11355,499440,"In addition, we have some lexical mappings (they are obvious from the example), and the start symbol is Yfinite: +.",20,21
11356,499440,"We start our derivation with the start symbol Vclause and rewrite it using dependency rules d2 and d3, and the lifting rule ll which introduces an objective NP argument.",7,8
11357,199661549,"In Earley's version of the algorithm, there is a function, Hk which when applied to a category C returns a set of k-symbol strings of terminals which could begin a phrase of category C. When applied to unification grammars, however, the problem of having an infinite number of categories again appears.",27,28
11358,15441214,symbol expresses negation of the predicate attached with the symbol.,0,1
11359,15441214,symbol expresses negation of the predicate attached with the symbol.,9,10
11360,57008378,"The edit distance metric between two strings measures the minimum number of unit editing operations of insertion, deletion, replacement of a symbol, and transposition of adjacent symbols [3] , that are necessary to convert one string into another.",23,24
11361,57008378,"/2, ... , Y n denote strings of length mandnrespectivelyof m symbols from an alphabet A. X[j] (Y[j]) denotes the initial substring of X (Y) up to and including the / h symbol.",39,40
11362,57008378,"They can be abstracted as finite state transducers over an alphabet of lexical and surface symbol pairs 1: s, where either 1 or s (but not .",15,16
11363,57008378,"both) may be the null symbol o. It is possible to apply error-tolerant recognition to languages whose word formations employ productive compounding and/or agglutination, and in fact to any language whose morphology is described completely as one (very large) finite state transducer. _",6,7
11364,57008378,"After a successful match with a surface symbol, the corresponding lexical symbol is appended to the output gloss string.",7,8
11365,57008378,"After a successful match with a surface symbol, the corresponding lexical symbol is appended to the output gloss string.",12,13
11366,57008378,"Oflazer and Giizey [15] have used a two level morphology approach to spelling correction in agglutinative languages, which has used a coarser morpheme-based morphotactic description instead of the finer lexical/surface symbol approach presented here.",37,38
11367,57008378,"For morphological analysis, the concurrent generation of the lexical gloss string requires that occasional transitions with an empty surface symbol be taken, to generate the gloss properly.",20,21
11368,11896512,The value h = 0 is a special root-symbol that may only appear as the head of a dependency.,10,11
11369,11896512,"In a single-root tree y, the root-symbol has exactly one child, while in a multi-root tree, the root-symbol has one or more children.",11,12
11370,11896512,"In a single-root tree y, the root-symbol has exactly one child, while in a multi-root tree, the root-symbol has one or more children.",28,29
11371,11896512,"For a sentence x with n words, define a complete directed graph G on n nodes, where each node corresponds to a word in x, and each edge corresponds to a dependency between two words in x. Note that G does not include the root-symbol h = 0, nor does it account for any dependencies (0, m) headed by the root-symbol.",49,50
11372,11896512,"For a sentence x with n words, define a complete directed graph G on n nodes, where each node corresponds to a word in x, and each edge corresponds to a dependency between two words in x. Note that G does not include the root-symbol h = 0, nor does it account for any dependencies (0, m) headed by the root-symbol.",71,72
11373,11896512,"n: A h,m (θ) = 0, if h = m exp {θ h,m } , otherwise To account for the dependencies (0, m) headed by the root-symbol, we define a vector of root-selection scores r(θ) ∈ R n , for m = 1 . . .",40,41
11374,11896512,"n: r m (θ) = exp {θ 0,m } Let the weight of a dependency structure y ∈ T s np (x) be defined as: ψ(y; θ) = r root(y) (θ) (h,m)∈y : h =0 A h,m (θ) Here, root(y) = m : (0, m) ∈ y is the child of the root-symbol; there is exactly one such child, since y ∈ T s np (x).",79,80
11375,11896512,"This distinction can be important as one often expects a dependency structure to have exactly one child attached to the root-symbol, as is the case in a single-root structure.",22,23
11376,199661667,"Each layer is an acyclic graph th at has only one source, and it corresponds to either a nonterm inal symbol or an integer.",21,22
11377,199661667,An layer corresponding to a nonterm inal symbol has only one sink.,7,8
11378,199661667,L{A) T he layer corresponding to a nonterm inal symbol .4.,10,11
11379,199661667,Note th a t the layer corresponding to a nonterm inal symbol has only one sink.,11,12
11380,8959569,"This is likely due to several factors, including the role of the begin string symbol <s>, which helps to capture word preferences for occurring first in a modifier sequence; also the behavior of modifiers when they occur in NPs may differ from how they behave in other contexts.",15,16
11381,5665620,𝐻 = − Pr 𝑡 log 2 Pr 𝑡 𝑑𝑡 ∞ 𝑡=0 𝐻 = 1 log 𝑒 2 + log 2 1 𝜆 The ceiling operator ⌈ ⌉ is introduced because Huffman codes use an integer number of bits to encode each symbol.,42,43
11382,5665620,"Now, let's consider an infinite alphabet where Pr 𝑎 = 1 2 , Pr 𝑏 = 1 4 and the probability of the t+1 st symbol is Pr 𝑡 = (1 − 𝛽)𝛽 𝑡 where 𝛽 = 1 2 .",27,28
11383,5665620,"Bigrams are hashed the same way, except that the vocabulary is padded with an extra symbol for NA (not applicable).",16,17
11384,14124213,Hashtags One emergent phenomenon in the Twitter ecosystem is the use of hashtags: words or phrases prefixed with a hash symbol (#).,21,22
11385,1000782,"To transform D into G we to define the set P of productions, the set N of non-terminals, and the start symbol S as follows: • For each v in W , transform the automaton L v into a right-linear grammar G Lv whose start symbol is L 1 v ; by construction, G Lv consists of rules such as L p v → u L q v or L p v → , where terminal symbols such as u belong to W and nonterminals such as L p v correspond to the states of the L v automaton; include all -productions in P , and, if a rule such as L p v → u L q v is in G Lv , include the rule L p v → 2 l u L q v in P . •",25,26
11386,1000782,"To transform D into G we to define the set P of productions, the set N of non-terminals, and the start symbol S as follows: • For each v in W , transform the automaton L v into a right-linear grammar G Lv whose start symbol is L 1 v ; by construction, G Lv consists of rules such as L p v → u L q v or L p v → , where terminal symbols such as u belong to W and nonterminals such as L p v correspond to the states of the L v automaton; include all -productions in P , and, if a rule such as L p v → u L q v is in G Lv , include the rule L p v → 2 l u L q v in P . •",52,53
11387,1000782,"For each v in V , transform the automaton R v into a left-linear grammar G Rv whose start symbol is R 1 v ; by construction, G Rv consists • V is a set of terminal symbols which include a distin- guished element root; • L is a function that, for any v ∈ W (= V − { root}), returns a finite automaton that recognises the well-formed sequences in W * of left dependents of v; • R is a function that, for each v ∈ V , returns a finite automaton that recognises the well-formed sequences of right dependents in W * for v. of rules such as R p v → R q v u or R p v → , where terminal symbols such as u belongs to W and non-terminals such as R p v correspond to the states of the R v automaton; include allproductions in P , and, if a rule such as R p v → R q v u is in G Rv , include the rule R p v → R q v 2 r u in P . •",21,22
11388,1000782,"For each symbol 2 l u occurring in P , include the productions 2 l u → L 1 u 1 l u , 1 l u → 0 u R 1 u , and 0 u → u in P ; for each symbol 2 r u in P , include the productions 2 r u → 1 r u R 1 u , 1 r u → L 1 u 0 u , and 0 u → u in P . •",2,3
11389,1000782,"For each symbol 2 l u occurring in P , include the productions 2 l u → L 1 u 1 l u , 1 l u → 0 u R 1 u , and 0 u → u in P ; for each symbol 2 r u in P , include the productions 2 r u → 1 r u R 1 u , 1 r u → L 1 u 0 u , and 0 u → u in P . •",44,45
11390,1000782,Set the start symbol S to R 1 root .,3,4
11391,1000782,"By construction, a 2 r v symbol is the right successor of a non-terminal R p u .",7,8
11392,1000782,"5 Any second-order grammar resulting from transforming the derivations of right and left dependents 4 Symmetrically, the derivation α L p u β α 2 l v L q u β α L 1 v 1 l v L q u β involving the 2 l v symbol is transformed into α L p u β α L 1 v L p u \L 1 v β α L 1 v 1 l v L q u β.",50,51
11393,59858612,"Non-kernel transitions correspond to taking in account the static predictions while shifting a symbol -terminal for scans, non-terminal for reductions.",15,16
11394,59858612,"Each non-kernel transition nk-T deals with the same action than the kernel one k-T, but manages the realization of the prediction : nk-T shifts a symbol X in a non-kernel rule, the corresponding move in the tree is (1) getting down from the kernel, by performing a PUSH, and (2) shifting X, by SWAPing the pushed position by the shifted one.",34,35
11395,59858612,Each time a rule appears with its first symbol recognized (i.e. in non-kernel transitions and £-reductions) its probability must be communicated to the parser to compute the recognition probability.,8,9
11396,59913665,"2 can be further reduced to the following equation if only one left and one right context symbol are considered where ""0"" is the null symbol.",17,18
11397,59913665,"2 can be further reduced to the following equation if only one left and one right context symbol are considered where ""0"" is the null symbol.",27,28
11398,59637023,Each elementary tree is constrained to have at least one terminal symbol which acts as its 'head'.,11,12
11399,59637023,"It should be emphasized that in our approach the category of a word is not a non-terminal symbol but a multi-level structure corresponding to minimal linguistic structures: sentences (for predicative verbs, nouns and adjectives) or phrases (NP for nouns, AP for adjectives, PP for prepositions yielding adverbial phrases).",19,20
11400,2510315,A tree (without NTs) is accepted by the grammar if it can be derived by a sequence of rule applications from a given start symbol.,26,27
11401,2510315,"2 ; its start symbol is {1; 7}, and it describes exactly the five trees in 1e ) is derived by expanding the start symbol with the first rule in Fig.",4,5
11402,2510315,"2 ; its start symbol is {1; 7}, and it describes exactly the five trees in 1e ) is derived by expanding the start symbol with the first rule in Fig.",28,29
11403,6348449,"For each vertex in a given cut, we create a new cut by replacing the start vertex of some edge with the end vertex of that edge, observing the following rules: "" the vertex that is the start of several edges labeled using the special symbol § is replaced by a sequence of all the end vertices of these edges (for example, ¦¥ # ¢ is a cut derived from ¢¡ (Figure 2 (b) )); a mirror rule handles the special symbol ; "" the vertex that is the start of an edge labeled using vocabulary items or is replaced by the end vertex of that edge (for example, $% ¢ , ¤¥ # , ¤¥ & £ , ¤¥ & ¤' are cuts derived from ¦¥ & ¢ , ¤¥ # ¢ , v1 v0 ve vs finally ε ε ε ε ε ε ε ε ε ε ε ε ε released were captives prisoners the the v2 1 1 1 1 1 1 1 1 v20 v19 v18 v17 v16 v15 v14 v13 v12 v11 v10 v9 v8 v7 v6 v5 v4 v3 Figure 1 : The IDL-graph corresponding to the IDLexpression £ 65 87 @9 A HA 6C 1 ¤ ¥ 3 FE HG §I ¢ !",48,49
11404,6348449,"For each vertex in a given cut, we create a new cut by replacing the start vertex of some edge with the end vertex of that edge, observing the following rules: "" the vertex that is the start of several edges labeled using the special symbol § is replaced by a sequence of all the end vertices of these edges (for example, ¦¥ # ¢ is a cut derived from ¢¡ (Figure 2 (b) )); a mirror rule handles the special symbol ; "" the vertex that is the start of an edge labeled using vocabulary items or is replaced by the end vertex of that edge (for example, $% ¢ , ¤¥ # , ¤¥ & £ , ¤¥ & ¤' are cuts derived from ¦¥ & ¢ , ¤¥ # ¢ , v1 v0 ve vs finally ε ε ε ε ε ε ε ε ε ε ε ε ε released were captives prisoners the the v2 1 1 1 1 1 1 1 1 v20 v19 v18 v17 v16 v15 v14 v13 v12 v11 v10 v9 v8 v7 v6 v5 v4 v3 Figure 1 : The IDL-graph corresponding to the IDLexpression £ 65 87 @9 A HA 6C 1 ¤ ¥ 3 FE HG §I ¢ !",93,94
11405,60156140,"This adaptation also allows the LR approach to be used with uncertain input [7] , and this approach enables a grammar model to interface with the speech recognition front end as naturally as does a Markov model Probabilistic Context-Free Grammars A ""probabilistic context-free grammar (PCFG)"" [8-10] is a 4-tuple <N,T,R,S> where N is a nonterminal vocabulary including the start symbol S, T is a terminal vocabulary, and R is a set of production-rules each of which is a pair of form <A a , p>, with AeN, a€(NuT)*, and p a probability.",83,84
11406,60156140,"New kernel sets Starting from an initial state I0 consisting of the closure of {<S' -> -S, 1>> where S' is an auxiliary start symbol, this process continues until no further sets are created.",32,33
11407,10576017,"Y 1 Y 2 : : : Y n for X; Y i 2 (N Σ), and S is a distinguished start symbol in N. The grammar defines a set of possible strings, and possible string/tree pairs, in a language.",26,27
11408,10576017,The main condition is that the parameters define conditional distributions over the alternative ways of rewriting each non-terminal symbol in the grammar.,20,21
11409,244077658,"Since the average length of a word in English is about 5 letters, and since allowance must be made for a segmentation symbol (space), an average of 15 letters, or 7 • 15 = 105 bits, must be provided in each entry for the target-language material.",23,24
11410,794463,"Parser actions arise as before, but are executed by relativising them with respect to the incomplete item participating in the action, and passing this relativised parser action as the next input symbol for the automaton referenced by that item.",33,34
11411,794463,The Recognition Phase This section illustrates a simple bottom-up parsing algorithm that makes use of minimized automata produced from sets of trees that anchor the same input symbol.,29,30
11412,794463,"Theindices l, l', #, r are positions between input symbols (position 0 is before the first input symbols and position n is after the final input symbol) and we use wp,p, to denote that substring of the input w between positions p and p~. I can be viewed as a four dimensional array, each entry of which contains a set of pairs comprising of a set of nonterminals and an automata state.",31,32
11413,794463,"The input is accepted if an item (T, qs,[O,n,-,-] ) is added to I where T contains some initial tree rooted in the start symbol S and qf • Fk for some k. When adding items to I we use the procedure add(T, q, [/, r, l', r']) which is defined such that if there is already an entry (T ~, q, [/, r, l ~, rq/ • I for some T ~ then replace this with the entry (T U T', q, [/, r, l', #])6; otherwise add the new entry {T, q, [l, r, l', r']) to I. I is initialized as follows.",29,30
11414,10946295,"An PTSG is a 5-tuple V n , V t , S, T, w , where V n is the set of non-terminal symbols; V t is the set of terminal symbols; S ∈ V n is the start symbol; T is a set of elementary trees, such that for every τ ∈ T the unique root node r(τ ) ∈ V n , the (possibly empty) set of internal nodes i(τ ) ⊂ V n and the set of leaf nodes l(τ ) ⊂ V n ∪ V t ; finally, w : T → [0, 1] is a probability (weight) distribution over the elementary trees, such that for any τ ∈ T , τ ∈R(τ ) w(τ ) = 1, where R(τ ) is the set of elementary trees with the same root label as τ .",47,48
11415,10946295,"1) For simplicity, but without loss of generality, we assume there are no recursions on the start symbol.",20,21
11416,290977,"They respectively assume the existence of a distinguished symbol in each rule, the head, and certain distinguished words in the sentence to be parsed, the islands, playing a central role on the respective parsing approach.",8,9
11417,290977,The Local Model The local approach is based on regarding the probability of an edge to be extended (and the same applies to the prediction) as the probability of the next symbol to be expanded having the terminal(s) symbol(s) in the corresponding position of the sentence as either left or right corner.,33,34
11418,290977,"Being G a SCFG, T the set of terminal symbols of G, N the set of nonterminal symbols of G, R i the i-th production of G and P(R i ) its attached probability, [A, i, j] 1 is an island labelled A spanning positions i to j, and {left|right}_corner are functions from N x T to [0,1], being {left|right}_corner(A, a) the probability that a derivation tree rooted A could have symbol a as a {left|right} corner: ) / ( ) , ( _ : , G a A P a A corner right T a N A >> = ∈ ∈ ∀ 2 Similarly, {left|right}_corner* are functions from N x T* to [0,1], so that, for any list of symbols la: ∑ ∈ = la a a A corner right la A corner right ) , ( _ ) , ( * _ Left-corner probabilities are symmetrically defined.",89,90
11419,2829536,"For each expansion we distinguish one of the right-hand side labels as the ""middle"" or ""head"" symbol M (c).",22,23
11420,2829536,"To the left of M is a sequence of one or more left labels L i (c) including the special termination symbol △, which indicates that there are no more symbols to the left.",23,24
11421,57276380,"By using TFSP links, new edge creation is necessary only when there is no edge with a certain starting vertex, ending vertex, label and remainder symbol sequence.",28,29
11422,57276380,"A genda Control M echanism and Plausibility Score In order to select the most plausible analysis candidate in the early stages, the TFSP parser selects the pending edge with the best edge score among the pending edge list during parsing, and selects the TFS with the best TFS score among sets of TFSs in complete edges, each of which has as its label the start symbol, as its remainder symbol sequence an empty sequence, as its starting vertex the leftmost vertex of the chart, and as its ending vertex the rightmost vertex of the chart just after parsing finishes.",68,69
11423,57276380,"A genda Control M echanism and Plausibility Score In order to select the most plausible analysis candidate in the early stages, the TFSP parser selects the pending edge with the best edge score among the pending edge list during parsing, and selects the TFS with the best TFS score among sets of TFSs in complete edges, each of which has as its label the start symbol, as its remainder symbol sequence an empty sequence, as its starting vertex the leftmost vertex of the chart, and as its ending vertex the rightmost vertex of the chart just after parsing finishes.",73,74
11424,219299773,"This paper will attempt to show that many of these types of concerns can be addressed with syntactic methods (symbol pushing), and need not require explicit semantic interpretation.",20,21
11425,8008954,"Each non-terminal node in the complete data is labeled with a complete symbol of the form D E9 F GB , where D is the non-terminal symbol of the corresponding node in the observed tree and F is a latent annotation symbol, which is an element of a fixed set H .",14,15
11426,8008954,"Each non-terminal node in the complete data is labeled with a complete symbol of the form D E9 F GB , where D is the non-terminal symbol of the corresponding node in the observed tree and F is a latent annotation symbol, which is an element of a fixed set H .",30,31
11427,8008954,"Each non-terminal node in the complete data is labeled with a complete symbol of the form D E9 F GB , where D is the non-terminal symbol of the corresponding node in the observed tree and F is a latent annotation symbol, which is an element of a fixed set H .",45,46
11428,8008954,p rq Q gx e9 F f uB ut ih j ¢k gl ml h Tn Y gS where d 4Q gf 9 F ¥ B Y denotes the probability of an occurrence of the symbol f 69 F ¥ B at a root node and q Q j Y denotes the probability of a CFG rule j .,35,36
11429,8008954,"Model definition We define a PCFG-LA | as a tuple | I } v 1t S v S H S t ES d S iq , where v { a set of observable non-terminal symbols v a set of terminal symbols H a set of latent annotation symbols a set of observable CFG rules d 4Q D @9 F B Y the probability of the occurrence of a complete symbol D @9 F B at a root node q Q j Y the probability of a rule j 9 H B W We use D S t eS XW XW XW for non-terminal symbols in v 1t ; ¥ XS U S XW XW XW for terminal symbols in v @ ; and F S i S XW XW XW for latent annotation symbols in H .",74,75
11430,8008954,"A complete tree is denoted by 8 @9 A CB , where A I Q F ¥ TS XW XW XW S F ¡ Y dH ¡ is a vector of latent annotation symbols and F m¢ is the latent annotation symbol attached to the k -th non-terminal node.",41,42
11431,8008954,"Therefore, an annotation symbol, say, F , generally does not express any commonalities among the complete non-terminals annotated by F , such as D E9 F GB S t 9 F GB S T 0 .",4,5
11432,8008954,"µ If node k is a pre-terminal node above a termi- nal symbol ' ¶ , then ³ ¢ ¬ Q F Y I q Q v ¢ 9 F GB •t © ¶ Y .",14,15
11433,8008954,"r If E D is a root node, let ã be the non-terminal symbol of E .",16,17
11434,7828046,"Figure 4 shows examples: root is for the root node, in which the phrase symbol is S and the surface form, part-of-speech, and lexical entry of the lexical head are ""saw"", VBD, and a transitive verb, respectively.",16,17
11435,17668015,"For any relation cluster RC k that does not feature in the subtree, let RC k be paired with the abstract symbol w / 0 in st.",22,23
11436,17668015,This symbol represents uncertainty about a potential RC k -neighbor.,1,2
11437,5707769,"General Transition Systems Assume an input alphabet Σ with a special symbol $ ∈ Σ , which we use as the root of our parse structures.",11,12
11438,5707769,"Elements of σ and β are nodes from V w and, in the case of the stack, a special symbol ¢ that we will use as initial stack symbol.",21,22
11439,5707769,"Elements of σ and β are nodes from V w and, in the case of the stack, a special symbol ¢ that we will use as initial stack symbol.",30,31
11440,5707769,"Transition sh b removes the first node from the buffer, in case this node represents symbol b ∈ Σ , and pushes it into the stack.",16,17
11441,5707769,"This means that we can consider the first element of the buffer as an additional stack element, always sitting on the top of the top-most stack symbol.",29,30
11442,5707769,"More precisely, each transition is conditioned on the lexical form of the three symbols at the top of the stack σ, indicated as b 3 , b 2 , b 1 ∈ Σ below, with b 1 referring to the topmost symbol.",44,45
11443,5707769,"If t i = sh a i , the content of σ is irrelevant at 1239 this step, since in our model sh a i is conditioned only on the topmost stack symbol of σ i−1 , and we have |σ i−1 | ≥ |σ| + 2.",33,34
11444,199661655,We write A[--] (or A [--7 ] ) to denote the nonterminal A associated with an arbitrary stack (or an arbitrary stack whose top symbol is 7 ).,28,29
11445,199661655,"D e fin itio n 2.1 A LIG, G, is denoted by (V>/, Vj, V>, 5, P) where V'v is a finite set of nonterminals, V j is a finite set of terminals, Vj is a finite set of indices (stack symbols), S 6 Vn is the start symbol, and P is a finite set of productions, having one of the following forms.",62,63
11446,16830421,"The attributes of each symbol can be classified in physic, syntactic, and semantic attributes.",4,5
11447,16830421,"More formally, an Extended Positional Grammar is the pair (G, PE), where PE is a positional evaluator, and G is a particular type of context-free string attributed grammar (N, T∪POS, S, P) where: • N is a finite non-empty set of non-terminal symbols; • T is a finite non-empty set of terminal symbols, with N∩T=∅; • POS is a finite set of binary relation identifiers, with POS∩N=∅ and POS∩T=∅; • S∈N denotes the starting symbol ; • P is a finite non-empty set of productions of the following format: A → x 1 R 1 x 2 R 2 . . .",99,100
11448,16830421,"x m−1 R m−1 x m , ∆, Γ where A is a non-terminal symbol, x 1 R 1 x 2 R 2 . . .",17,18
11449,16830421,"x m−1 R m−1 x m is a linear representation with respect to POS where each x i is a symbol in N∪T and each R j is partitioned in two sub-sequences ( REL h1 j1 , . . . ,",20,21
11450,16830421,"Driver relations are used during syntax analysis to determine the next symbol to be scanned, whereas tester relations are used to check whether the last scanned symbol (terminal or non-terminal) is properly related to previously scanned symbols.",11,12
11451,16830421,"Driver relations are used during syntax analysis to determine the next symbol to be scanned, whereas tester relations are used to check whether the last scanned symbol (terminal or non-terminal) is properly related to previously scanned symbols.",27,28
11452,16830421,"In particular, -N j is a terminal symbol to be inserted in the input visual sentence; -Cond j is a pre-condition to be verified in order to insert N j ; -∆ j is the rule used to compute the values of the syntactic attributes of N j from those of x 1 ,. . .,",8,9
11453,16830421,"We write α ⇐ β and say that β reduces to α in one step, if there exist δ, γ, A, η such that A → η, ∆, Γ is a production in P, β = δηγ, α = δA πγ, where A is a symbol whose attributes are set according to the rule ∆ and π results from the application of the rule Γ. We also write α i ⇐ β to indicate that the reduction has been achieved by applying production i. Moreover, we write α * ⇐ β and say that β reduces to α, if there exist α 0 , α 1 , . . .,",54,55
11454,16830421,"Note that L(G) is equivalent to L(rev (G)) for each reversible XPG grammar G. In following, the notation V sym i denotes the attaching point i of the symbol V sym.",33,34
11455,16830421,"The set of relations is given by POS = {LINK h,k }, where the relation identifier LINK i,j is defined as: ""a symbol x is in relation with a symbol y iff attaching point i of x is connected to attaching point j of y"", and will be denoted as i j to simplify the notation.",30,31
11456,16830421,"The set of relations is given by POS = {LINK h,k }, where the relation identifier LINK i,j is defined as: ""a symbol x is in relation with a symbol y iff attaching point i of x is connected to attaching point j of y"", and will be denoted as i j to simplify the notation.",37,38
11457,16830421,In production 2) a compound statement C is defined as a begin symbol connected to a non-terminal R. Productions 3) and 4) define a non-terminal R as a sequence of statements S ending with the end symbol.,13,14
11458,16830421,In production 2) a compound statement C is defined as a begin symbol connected to a non-terminal R. Productions 3) and 4) define a non-terminal R as a sequence of statements S ending with the end symbol.,43,44
11459,16830421,"The set of non-terminals is given by N = {StateTD, Graph, Node} where each symbol has one attaching region as syntactic attribute, and StateTD is the starting symbol, i.e. S = StateTD.",20,21
11460,16830421,"The set of non-terminals is given by N = {StateTD, Graph, Node} where each symbol has one attaching region as syntactic attribute, and StateTD is the starting symbol, i.e. S = StateTD.",34,35
11461,16830421,The terminal symbol EDGE has two attaching points as syntactic attributes corresponding to the start and end points of the edge.,2,3
11462,16830421,"Finally, PLACEHOLD is a fictitious terminal symbol to be dynamically inserted in the input sentence during the parsing process.",7,8
11463,16830421,"The action and goto sections are similar to the ones used in the LR parsing tables for string languages [1] , while the next section is used by the parser to select the next symbol to be processed.",36,37
11464,16830421,"An entry next[k] for a state s k contains the pair (R driver , x), which drives the parser in selecting the next symbol (derivable from x) by using the sequence of driver relations R driver .",27,28
11465,16830421,"The special entries (start, S) and (end, EOI) are used to retrieve the first symbol to be parsed and to check whether the whole input sentence has been parsed, respectively.",20,21
11466,16830421,As a matter of fact if a parser inserts a new terminal symbol into the input the other parsers cannot scan it.,12,13
11467,16830421,"We say then that the symbol reintroduction is ""local"" to that parser.",5,6
11468,16830421,It is worth noting that in the LR parsing of visual languages it is difficult to establish from which symbol of a sentence the parsing process has to start.,19,20
11469,16830421,"The idea is to use two parsers that proceed in parallel, scanning the input sentence in opposite directions from an arbitrary starting symbol.",23,24
11470,16830421,"reachable after the occurrence of the starting symbol the algorithm starts an incremental IG-XpLR parser, named forward (backward, resp.).",7,8
11471,16830421,State 2 is the starting symbol since we assume that the first symbol inserted into the editor is the starting symbol.,5,6
11472,16830421,State 2 is the starting symbol since we assume that the first symbol inserted into the editor is the starting symbol.,12,13
11473,16830421,State 2 is the starting symbol since we assume that the first symbol inserted into the editor is the starting symbol.,20,21
11474,16830421,"In the same way, the user can change the starting symbol.",11,12
11475,10017599,"We represent these as context-free trees which can be defined formally as 4-tuples {S, T, N, H}, where S is a start symbol, typically the root node of the tree; T = {t 0 , t 1 , t 2 . . .",32,33
11476,10017599,"In this way, each non-terminal symbol has a semantic representation and an associated parse category.",8,9
11477,14079757,"We only use the first letter of each label, resulting in 15 nonterminal labels (including a new start symbol).",20,21
11478,14701636,"s i-1 indicates the number of the state nodes at the top of the stack, and l i is the lookahead symbol (POS) read by the parser.,",22,23
11479,11708630,"The system chooses ""small white box"" instead of ""ADSL filter"" and ""monitor symbol"" instead of ""network icon"", because it learnt that the user is a novice based on their clarification requests.",17,18
11480,11708630,network icon / flashing computer symbol 6.,5,6
11481,6376593,"A context-free grammar (CFG) G = (V, T, P, S † ), consists of a set of non-terminal symbols V , a set of terminal symbols T , a start symbol S † ∈ V , and a set of rule productions P of the form: A → γ, where A ∈ V and γ ∈ (V ∪ T ) * .",41,42
11482,8059508,"The # symbol acts as a wildcard allowing a classifier to aggregate states; for example, the state string 1#1 matches the states 111 and 101.",2,3
11483,937602,The values of random variables correspond to surface variants of a semantic symbol.,12,13
11484,17254158,"For example, the specification of a stochastic push-down automaton (PDA) requires a specification of the conditional probability distribution over push(X) and pop actions given the current state, input symbol, and top stack symbol.",35,36
11485,17254158,"For example, the specification of a stochastic push-down automaton (PDA) requires a specification of the conditional probability distribution over push(X) and pop actions given the current state, input symbol, and top stack symbol.",40,41
11486,17254158,"For example, the configuration of a finite state machine is a single state symbol, and the configuration of a push-down automaton is a single state symbol plus an arbitrarily deep stack of stack symbols.",14,15
11487,17254158,"For example, the configuration of a finite state machine is a single state symbol, and the configuration of a push-down automaton is a single state symbol plus an arbitrarily deep stack of stack symbols.",29,30
11488,17254158,"In the example in figure 1 , the BN in (b) includes four copies The actions are ""s-1"" for popping one symbol from the stack and ""s-1+2"" for popping one and pushing two symbols.",25,26
11489,219299683,"Senten tial forms and elements of derivations are for mally defined as sets of relations, each of which in turn is a set of ordered tuples, in the symbol (or object type) vocabulary set.",30,31
11490,219299683,It is natural to think of a grammar rule as providing a defini tion of a composite (nonterminal) object as a set (usually nonunary) whose type is the symbol on the left-hand-side of the rule and whose parts are the union of the parts of the objects whose types are the symbols on the right-hand-side of the rule.,32,33
11491,219299683,Derivations are defined as a sequence of replace ments that are headed by a type that is a root symbol of the grammar and terminate in a set of objects whose types are taken from the vocabu lary of terminal symbols.,19,20
11492,219299683,The root symbol for this grammar is Flowchart.,2,3
11493,219299683,"A scan ning action is valid, given some positional index, only if the terminal symbol at that position is a valid left-corner of a possible derivation subtree predicted to start at that index.",16,17
11494,219299683,"For a parser to adquately predict top-down expansions of the grammar's root symbol from any position in the input and only that position, it follows that ev ery element of any parsable input sets must be a possible left-corner of a valid derivation tree headed by the root symbol.",15,16
11495,219299683,"For a parser to adquately predict top-down expansions of the grammar's root symbol from any position in the input and only that position, it follows that ev ery element of any parsable input sets must be a possible left-corner of a valid derivation tree headed by the root symbol.",54,55
11496,62584777,"In the traditional context-free grammar, a non-terminal symbol represents a phrase, which is a substring of the original input string.",12,13
11497,62584777,"A region can be represented with a non-terminal symbol and 4 positional parameters: x, y, X and Y, which determine the upper-left position and the lower-right position of the rectangle (assuming that the coordinate origin is the upper-left corner of the input text).",10,11
11498,18153744,"a bottom-up phase to identify the grammar symbols that generate substrings, which may include the start symbol if the generated language is non-empty; and 2.",19,20
11499,18153744,a top-down phase to identify the grammar symbols that are reachable from the start symbol.,16,17
11500,18153744,"Furthermore, we assume a weighted finite automaton A, with an input alphabet equal to the set of terminal labels of G. The transitions of A are of the form: q a → q w , where q and q are states, a is a terminal symbol, and w is a weight.",49,50
11501,679599,"An ET is a single ""symbol"" in a transducer's language.",6,7
11502,679599,"As shown in Figure 2 , each circle stands for an ET and thick arrows denote the transduction of each ET as a single symbol.",24,25
11503,1797761,We write Σ k to denote the set of all k-ary symbols in Σ. We use a special symbol e ∈ Σ 0 to syntactically represent the empty string ε.,20,21
11504,1797761,We denote the symbol of t at position w by t(w) and its rank by rk t (w).,3,4
11505,1797761,"The size of a transition in a WTA, viewed as an instance (σ, q 0 , q 1 • • • q k ) of some mapping µ k , is defined as |σq 0 • • • q k |, that is, the rank of the input symbol occurring in the transition plus two.",53,54
11506,1797761,Let Σ be a ranked alphabet and assume a fresh symbol @ / ∈ Σ (corresponding to the basic list concatenation operator).,10,11
11507,1797761,"Definition 2 Let M = (Q, Σ, S, µ, F ) be a WTA such that the maximum rank of a symbol in Σ is 2, and let N = (P, Σ 0 \ {e}, S, I, ν, G) be a WSA over the same semiring.",26,27
11508,1797761,"ii) For every symbol γ ∈ Σ 1 , states p 0 , p 1 ∈ P , and states q 0 , q 1 ∈ Q let µ 1 (γ) (p 0 ,q 0 ,p 1 ),(p 0 ,q 1 ,p 1 ) = µ 1 (γ) q 0 ,q 1 . (",4,5
11509,1797761,"iii) For every symbol α ∈ Σ 0 , states p 0 , p 1 ∈ P , and q ∈ Q let µ 0 (α) (p 0 ,q,p 1 ),ε = µ 0 (α) q,ε • s where s = ν(p 0 , α, p 1 ) if α = e 1 if α = e and p 0 = p 1 . (",4,5
11510,1797761,"As an example, prefix probabilities can be used to compute the probability distribution on the terminal symbol that follows a given prefix (under the given model).",17,18
11511,13421101,"2005) have proposed probabilistic CFG learning with latent annotation (hereafter PCFG-LA), as a way to automate symbol splitting in unlexicalized probabilistic parsing (cf.",22,23
11512,13421101,adding latent annotations to a symbol is comparable to splitting this symbol). (,5,6
11513,13421101,adding latent annotations to a symbol is comparable to splitting this symbol). (,11,12
11514,43407292,"In this notation, the sign symbol -and + indicate the leftward or rightward disjuncts, and # is the delimiter.",6,7
11515,43407292,"In +PP#P#NP#1, PP is the parent node in the sub-tree, and NP is to the right of P. Similarly, in -PP#P#NP#1, symbol PP is the parent node in that sub-tree, and P is a constituent to the left of constituent NP.",28,29
11516,43407292,The last symbol 2 (1) in the strings means that the head is on the right (left) side of the link.,2,3
11517,53814149,It applies when the dot is to the left and above a terminal symbol.,13,14
11518,53814149,Step 1 applies when the dot is to the left and above a non-terminal symbol.,16,17
11519,53814149,Step 2 also applies when the dot is to the left and above a non-terminal symbol.,17,18
11520,216086600,In this approach nodes are generated recursively in a top-down manner starting from the special symbol EOS (end of sentence).,17,18
11521,9165619,"The symbol @ denotes the reduced vowel [ ] (schwa), and ˜designates negated values.",1,2
11522,5459895,The conflict can exist because there is already a token at Parse Position as described above or it can exist simply because the Queue Element does not fit into the terminal or non terminal symbol at the Parse Position.,34,35
11523,5459895,"A conventional LL( 1) grammar can be described as a tuple [PLRS76] G = (S, T, N, P) where S is the start symbol of G, S E N. T is a finite set of terminal symbols.",32,33
11524,5459895,"For the purpose of constructing the select sets of normal non terminals (non-terminals that are not unstructured non terminals) each occurrence of an unstructured non terminal is treated as a unique, distinguished terminal symbol 11, 11 <t T. Thus a non-terminal's select set will contain an entry for each terminal symbol in its first set and an entry for any unstructured element that it can be derived from it.",38,39
11525,5459895,"For the purpose of constructing the select sets of normal non terminals (non-terminals that are not unstructured non terminals) each occurrence of an unstructured non terminal is treated as a unique, distinguished terminal symbol 11, 11 <t T. Thus a non-terminal's select set will contain an entry for each terminal symbol in its first set and an entry for any unstructured element that it can be derived from it.",60,61
11526,6469543,The root of a sentence is a designated special symbol which all words in the sentence directly or indirectly modify.,9,10
11527,5217667,"In some earlier research work on LR(k) extensions for NLP [4] and generalization of compiler generator [5] , an idea naturally occurred that one can make a modification to the LR parsing so that after each reduction, the resulting symbol may not be necessarily pushed onto the analysis stack, but instead put back into the input, waiting for decisions in the following steps.",45,46
11528,5217667,"Shift-reduce parsing with an intermediate buffer To enable more control of the reduction nodes within the parse, we add a symbol buffer to the traditional shift-reduce parser, as shown in the following figure, where # stands for the end of input string, and pi, pbh/pbt and ps for pointers to input, buffer and parsing stack respectively.",23,24
11529,5217667,"The basic flow of input tokens is that instead of being directly moved onto the symbol stack, they are first shifted into the buffer queue, where they may be reduced to other symbols and may also result other sequential reductions in the buffer and/or on the parsing stack.",15,16
11530,5217667,"Symbols contained in the buffer are not limited to a fixed number, and will be finally shifted onto the symbol stack, where they are to be reduced to other symbols.",20,21
11531,5217667,"Here we will only discuss triple configurations [s, A, a], where the parser can make shift-reduce decisions in the buffer using the tail symbol at a given stack state.",30,31
11532,5217667,"Since we consider only one tail buffer symbol, we can further simply the construction of parsing tables by noticing that for this simplified parsing, the lookahead symbol may be nonterminals besides terminals.",7,8
11533,5217667,"Since we consider only one tail buffer symbol, we can further simply the construction of parsing tables by noticing that for this simplified parsing, the lookahead symbol may be nonterminals besides terminals.",28,29
11534,5217667,"Follow set: Given any grammar symbol N T V V X ∪ ∈ , ) ( X Follow is defined as . : } ?{# , ) ( * *",6,7
11535,5217667,"Algorithm of SBLR(0) parsing table construction Input: an augmented grammar G'; Output: a parsing table of G' Action[]; Summary and future work Buffered shift-reduce parsing, by postponing the decisions of shift/reduce actions using a symbol buffer, provides more control and flexibility of ambiguity handling, and is applicable to parsing a class of ambiguous grammars that may introduce typical conflicts to LR parsing.",48,49
11536,219308174,"These symbols include the current token or lookahead, denoted curtok, the token immediately preceding the current token, de noted prevtok, and an input buffer 1 denoted b• uff er, containing a predetermined number of the input tokens following curtok .. A number of attributes are associated with each input symbol such as its class, its location within the input source, its char acter string representation, • etc ... An input sym bol together with all its attributes is referred to as a token element.",54,55
11537,219308174,"Each state q in the state stack is also associated with certain attributes includ ing the grammar symbol that caused the transition into-q ( called the in_symbol of q ), and the location of the first input token on which an action was ex ecuted on q. An LR parsing configuration may be repre sented by a string of the form: The sequence to the left of the vertical bar is the content of the state stack, with q m at the top; q1 ... q m is a valid sequence of states in the LR parsing machine.",17,18
11538,219308174,Each ele ment t i represents the class of a corresponding in put symbol .,13,14
11539,219308174,"The symbol t 1 represents the class of the current token, t 2 represents the class of the suc cessor of cttrtok, etc.",1,2
11540,219308174,The symbol to which is not shown above represents the class of prevtok.,1,2
11541,219308174,"i.e., the insertion of a single symbol into the inp�t stream, the dele tion of an input token , the su:bsti�ution of a grammat symbol for an input token �or _ the merging of two adjacent tokens to form a smgle one.",7,8
11542,219308174,"i.e., the insertion of a single symbol into the inp�t stream, the dele tion of an input token , the su:bsti�ution of a grammat symbol for an input token �or _ the merging of two adjacent tokens to form a smgle one.",32,33
11543,219308174,Deletion of as small a se quence of tokens as-possible in tf-1.e vicinity of the error token or replacement or such a se� quence with a nontennfoal symbol.,30,31
11544,219308174,"To achieve this goal, an ad ditional state stack is required for each deferred symbol.",15,16
11545,219308174,"If a representation of the parsing tables with default ac tion is used, then the parser will never consult the lookahead symbol when it is in one of these states.",22,23
11546,219308174,"A read reduce action is referred to as a shift-reduce when # let #x denote the number of elements in a # sequence x. rhs and lhs are maps that yield the # size of the right-hand side and left-hand side # symbol of a given rule, respectively.",49,50
11547,219308174,"In such a case, all actions induced by the lookahead symbol must be invali dated and the original configuration of the parser (prior to the initial reduction) must be restored.",11,12
11548,219308174,"Asso ciated with each of these stacks are three integer variables: ppos, pos and npos which are used to mark the position of the top element in the corre sponding stack that is still valid after the actions induced by the relevant lookahead symbol are ap plied.",45,46
11549,219308174,"Here, qi , A and t; are the recovery state; reduction goal and recovery symbol, respectively.",17,18
11550,219308174,"For this reason, misplacement trials -are performed sep arately from the other secondary trials and given, higher priority, since such a rep-air does not delete any symbol from the forward context and tehds to remove whole structures from the left context that have been previously analysed.",31,32
11551,219308174,+ A. In this method a more restricted concept of an important symbol is used.,12,13
11552,219308174,"To understand the importance of T and F, assume that the rules from which the items of Figure 5 are derived are all the productions of a grammar and consider the following erroneous input strings: () ) ( *i d+ id () ) ( j id + id If Ei s the only important symbol considered, then the best secondary repair that is achievable is the replacement of ""( ) ) ( * id"" by E in the first sentence and "" ( ) ) ( j id"" by E in the second sentence.",62,63
11553,219308174,"One further notices that using F as a reduc tion goal in the first sentence would have worked just as well, since after a transition on F, with the symbol ""*"" as lookahead, a reduction by the rule ""T --+ F"" would be applied.",31,32
11554,219308174,"If a nonterminal B can be substituted for an error phrase, then the recovery symbol tin question must be a valid lookahead symbol for any rule derivable from B. In particular, if Bi ⇒ f m B j and B j is substituted for an error phase where Bi is known to be a valid reduction goal, the recovery symbol will cause B; to be reduced to Bi .",15,16
11555,219308174,"If a nonterminal B can be substituted for an error phrase, then the recovery symbol tin question must be a valid lookahead symbol for any rule derivable from B. In particular, if Bi ⇒ f m B j and B j is substituted for an error phase where Bi is known to be a valid reduction goal, the recovery symbol will cause B; to be reduced to Bi .",23,24
11556,219308174,"If a nonterminal B can be substituted for an error phrase, then the recovery symbol tin question must be a valid lookahead symbol for any rule derivable from B. In particular, if Bi ⇒ f m B j and B j is substituted for an error phase where Bi is known to be a valid reduction goal, the recovery symbol will cause B; to be reduced to Bi .",62,63
11557,219308174,"State qi contains nonterminal transitions on the symbols E, T, F and P. The only unimpor tant symbol in that set is P. After P is removed, the irrelevant symbols E and T are removed from the subset {E, T, F} leaving Fas the only relevant reduction goal in qi .",19,20
11558,219308174,"The notion of an important symbol can also be extended to terminal candidates in the Lsymbols ,sets.",5,6
11559,219308174,"This state contains a single terminal action on the symbol id, but, since id appears only in the item P --+ -id, it is not an important candidate in qi.",9,10
11560,219308174,"For example, if a user specifies an expression that is missing a sin gle right parenthesis, primary recovery can success-• fully insert that symbol.",25,26
11561,219308174,The repair that is nec essary for this kind of error is the insertion of a sequence of symbols; called multiple symbol insertion.,22,23
11562,219308174,"To put it in the form A -o:B/3, let B be the symbol ""stJ.ist"" .",15,16
11563,219308174,"Let 1/; be the string ""elsifJ. ist opt...else"" and let X be the symbol ""END'' .",18,19
11564,219308174,"For each scope (1ri , (ji , ai, Ai , Qi), a three-step test is performed: step 1: The lookahead_action fu nction is invoked with ai as the current token to check if ai is a valid lookahead symbol for the viable pre fix.",47,48
11565,219308174,"This error is • detected on the symbol "": ="" .",7,8
11566,6649098,"In this paper, we argue that this imperfect relationship between written symbol and spoken sound can be automatically inferred from textual patterns.",12,13
11567,6649098,"Thus, automatic and generic methods for determining sound-symbol relationships are needed.",10,11
11568,6649098,"As an imperfect proxy for this idea, we made the following observation: for most Latin graphemes, the most common phonemic value across languages is the identical IPA symbol of that grapheme (e.g. the most common phoneme for g is /g/, the most common phoneme for t is /t/, etc).",30,31
11569,60520470,"After receiving• , the terminal symbol c, the state with the (closed) item set shown in Table 1 is entered, with the first coluinn of.",5,6
11570,60520470,"After receiving the terminal symbol c again , a state with the same item set is entered but with the second column of probabilities for each item , and ' these •are different from the first unless q1 = r 1 .",4,5
11571,60520470,"One way to prevent thi s series is to associate with each item in the lists (wh ich form the states) an array of states for each nonterminal symbol , recording the state ( s) in wh ich that• symbol occurred as .",30,31
11572,60520470,"One way to prevent thi s series is to associate with each item in the lists (wh ich form the states) an array of states for each nonterminal symbol , recording the state ( s) in wh ich that• symbol occurred as .",42,43
11573,60520470,"Pairs of nonterminals satisfying the conditions above are then easily detected within the ""goto"" function, so that an appropriate multiple shift or goto entry can be automatically created for the last symbol of a. Only the items leading to the loop ing behaviour need to be separated by this means , and the numb er of additional states generated is small.",34,35
11574,60520470,sentence is derivab le from a nonterminal symbol in more than one way then.,7,8
11575,60520470,"A value P (�, (D} 1 ••• J I A) is attached to each node in a parse tree , where � denotes a particular derivation of the string w 1 ••• w J from the symbol A, and (D} 1 ••• J represents the corresponding acous tical data .",41,42
11576,219306118," The sequence of a first right-hand side symbol dominating a DAG with an edge ""cat(egory)"" having ""prep(osition)"" as its value, a second symbol with ""cat"" ""det(erminer)"", and a third symbol with ""noun"" as ""cat"" makes an ""adv(erbial)"".",10,11
11577,219306118," The sequence of a first right-hand side symbol dominating a DAG with an edge ""cat(egory)"" having ""prep(osition)"" as its value, a second symbol with ""cat"" ""det(erminer)"", and a third symbol with ""noun"" as ""cat"" makes an ""adv(erbial)"".",32,33
11578,219306118," The sequence of a first right-hand side symbol dominating a DAG with an edge ""cat(egory)"" having ""prep(osition)"" as its value, a second symbol with ""cat"" ""det(erminer)"", and a third symbol with ""noun"" as ""cat"" makes an ""adv(erbial)"".",45,46
11579,219306118,"Additionally the second and the third symbol must share the same value for the feature ""agreement"".",6,7
11580,219306118,"Each class in the network corresponds to an occurrence of a symbol, whether terminal or not, appearing in the grammar.",11,12
11581,219306118,"For instance, for each left-hand side symbol in the grammar, a class is created inheriting from the inactive agent class.",9,10
11582,219306118,"If it finds such a constituent, it then creates an instance of the class corresponding to the following symbol in the right-hand side part of the rule.",19,20
11583,219306118,The pivot of the rule is the symbol starting the whole analysis.,7,8
11584,302179,"In this case Bayesian analysis yields an augmented marginalised likelihood (Buntine and Hutter, 2012), after integrating out µ, of p z, t τ, µ 0 , PDP = (b|a) T (b) N k S n k t k ,a (µ 0,k ) t k (3) where T = k t k , (x|y) N = N −1 n=0 (x + ny) de- notes the Pochhammer symbol, (x) N = (x|1) N , and S N M,a is a generalized Stirling number that is readily tabulated (Buntine and Hutter, 2012) .",84,85
11585,2784765,If adjunction is not mandatory at N γ then nil ∈ adj(N γ ) where nil / ∈ I ∪ A is a dummy symbol.,24,25
11586,16394809,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank n, a total function f A : A n → A, the operation associated with f .",6,7
11587,16394809,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank n, a total function f A : A n → A, the operation associated with f .",34,35
11588,16394809,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",54,55
11589,16394809,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",76,77
11590,16394809,"Formally, h is specified by pairs (f, h(f )), where f ∈ Σ is a symbol with some rank n, and h(f ) ∈ T ∆∪{x 1 ,...,xn} is a term with variables.",20,21
11591,16394809,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",6,7
11592,16394809,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",34,35
11593,16394809,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",54,55
11594,16394809,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",76,77
11595,16394809,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",6,7
11596,16394809,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",34,35
11597,16394809,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",54,55
11598,16394809,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",76,77
11599,16394809,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",6,7
11600,16394809,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",34,35
11601,16394809,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",54,55
11602,16394809,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",76,77
11603,16394809,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",6,7
11604,16394809,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",34,35
11605,16394809,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",54,55
11606,16394809,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",76,77
11607,16394809,"We write t(π) for the symbo path π in the tree t. A Σ-algebra A consists of a non-empty set called the domain and, for each symbol f ∈ Σ w rank m, a total function f A : A m → A, called operation associated with f .",32,33
11608,16394809,"Formally, such a grammar is a struct G = (N, Σ, P, S), where N is a signature of non minal symbols, all of which are taken to have rank Σ is a signature of terminal symbols, S ∈ N i distinguished start symbol, and P is a finite set productions of the form B → t, where B is a n terminal symbol, and t ∈ T N ∪Σ .",52,53
11609,16394809,"Formally, such a grammar is a struct G = (N, Σ, P, S), where N is a signature of non minal symbols, all of which are taken to have rank Σ is a signature of terminal symbols, S ∈ N i distinguished start symbol, and P is a finite set productions of the form B → t, where B is a n terminal symbol, and t ∈ T N ∪Σ .",74,75
11610,16394809,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",6,7
11611,16394809,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",34,35
11612,16394809,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",54,55
11613,16394809,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",76,77
11614,16394809,"We write t(π) for the symbo path π in the tree t. A Σ-algebra A consists of a non-empty set called the domain and, for each symbol f ∈ Σ w rank m, a total function f A : A m → A, called operation associated with f .",32,33
11615,16394809,"Formally, such a grammar is a struct G = (N, Σ, P, S), where N is a signature of non minal symbols, all of which are taken to have rank Σ is a signature of terminal symbols, S ∈ N i distinguished start symbol, and P is a finite set productions of the form B → t, where B is a n terminal symbol, and t ∈ T N ∪Σ .",52,53
11616,16394809,"Formally, such a grammar is a struct G = (N, Σ, P, S), where N is a signature of non minal symbols, all of which are taken to have rank Σ is a signature of terminal symbols, S ∈ N i distinguished start symbol, and P is a finite set productions of the form B → t, where B is a n terminal symbol, and t ∈ T N ∪Σ .",74,75
11617,16394809,"Note that by doing so, we view p as a symbol of rank |nt(α)|.",11,12
11618,16394809,"The nonterminals and the start symbol of G are as for G. We now interpret the trees generated by G over the string algebra over T , which we denote by T * .",5,6
11619,16394809,The tree algebra T ∆ over some signature ∆ consists of all trees over ∆; every symbol f ∈ ∆ of rank m is interpreted as an m-place operation that returns the tree with root symbol f and its arguments as subtrees.,17,18
11620,16394809,The tree algebra T ∆ over some signature ∆ consists of all trees over ∆; every symbol f ∈ ∆ of rank m is interpreted as an m-place operation that returns the tree with root symbol f and its arguments as subtrees.,38,39
11621,16394809,"This algorithm computes an RTG H for h −1 (L(H)), where H is an RTG in a normal form in which every rule contains a single terminal symbol; bringing a grammar into this form only leads to a linear size increase (Gécseg and Steinby, 1997) .",31,32
11622,16394809,"The algorithm derives items of the form [f, π, A, σ], stating that H can generate the subtree of h(f )σ at node π if it uses A as the start symbol; the substitution σ is responsible for replacing the variables in h(f ) by nonterminal symbols.",38,39
11623,16791026,"For practical reasons, we pad u with a special start symbol when the context overlaps a phrase boundary.",11,12
11624,16839291,"The softmax classifier computes the probability of the role (including special 'NULL' role): p(r|t i , t p , l) / exp(W l,r (t i t p )), (5) where t i and t p are representations produced by the graph convolutional encoder, l is the lemma of predicate p, and the symbol / signifies proportionality.",66,67
11625,7138313,S ∈ N is a special nonterminal called the start symbol.,10,11
11626,7138313,Our special start symbol is separately denoted as X0.,3,4
11627,7138313,"Starting from our initial hypergraph with one edge labeled with the start symbol ""X0"", we select one edge with nontermi- [X0-1] The [X1-1, 1] [X3-100, 2] the [X2-10, 3] him | [X1-1] girl | [X1-1] boy | wants | [X3-100] believe | [X3-100] [X1-1, 1] to [X3-100, 2] | [X2-10] [X2-10,3] [X3-100,2] [X1-1,1] 1 want-01 ARG0 ARG1 2 [X3-100, 2] [X1-1, 1] 1 boy 2 ARG0 believe-01 ARG1 1 girl Figure 3 : A series of symbol-refined SHRG rules used to derive the AMR graph for the sentence ""The boy wants the girl to believe him"".",12,13
11628,7138313,"Starting from our initial hypergraph with one edge labeled with the start symbol ""X0"", we select one edge with nontermi- [X0-1] The [X1-1, 1] [X3-100, 2] the [X2-10, 3] him | [X1-1] girl | [X1-1] boy | wants | [X3-100] believe | [X3-100] [X1-1, 1] to [X3-100, 2] | [X2-10] [X2-10,3] [X3-100,2] [X1-1,1] 1 want-01 ARG0 ARG1 2 [X3-100, 2] [X1-1, 1] 1 boy 2 ARG0 believe-01 ARG1 1 girl Figure 3 : A series of symbol-refined SHRG rules used to derive the AMR graph for the sentence ""The boy wants the girl to believe him"".",150,151
11629,7138313,The first rule rewrites the start symbol with a subgraph shown on the r.h.s.. We continue the rewriting steps until there are no more nonterminal-labeled edges.,6,7
11630,7138313,The fragment decomposition forest is a variation of the phrase decomposition forest 1 X0-1 is different as X0 is the start symbol of type one and should always have a concept edge at the root defined by Chung et al. (,23,24
11631,7138313,The symbol ∅ represents that the word is not aligned to any concept in the AMR graph and this word is called an unaligned word.,1,2
11632,15743685,"The symbol ""NULL"" is used to denote features that fail to instantiate.",1,2
11633,7146903,"1) The equivalence class symbol ""url"" (resp. """,5,6
11634,7146903,"all words that start with the @ symbol, e.g., @thomasss).",7,8
11635,16651000,"Therefore, we approach the static, visual aspects of the symbol grounding problem with an eye towards ensuring that our grounded representations of attributes can be composed in the same way that their semantic analogues can.",11,12
11636,16651000,"Future Work Work on this system is ongoing, with extensions planned for improving performance, generating more complete symbol grounding, and allowing more flexibility in both environment and language.",19,20
11637,10905562,"The terminal symbol strings are first pre-processed by stripping punctuation and other non-vocalized terminal symbols, which could not be expected from the output of a speech recognizer.",2,3
11638,1667249,"and binary production rules in two forms that are responsible for generating syntactic subtree pairs: X → [Y Z] and X → Y Z The rules with square brackets enclosing the right hand side expand the left hand side symbol into the two symbols on the right hand side in the same order in the two languages, whereas the rules with pointed brackets expand the left hand side symbol into the two right hand side symbols in reverse order in the two languages.",42,43
11639,1667249,"and binary production rules in two forms that are responsible for generating syntactic subtree pairs: X → [Y Z] and X → Y Z The rules with square brackets enclosing the right hand side expand the left hand side symbol into the two symbols on the right hand side in the same order in the two languages, whereas the rules with pointed brackets expand the left hand side symbol into the two right hand side symbols in reverse order in the two languages.",72,73
11640,1667249,"The many-to-one constraint has the same dynamic programming structure as IBM Model 1, where each target word is supposed to be translated from any of the source words or the NULL symbol.",36,37
11641,1667249,"When there is the null symbol in the bag of candidate words, the decoder can choose to translate a word into null to decrease the output length.",5,6
11642,1667249,"The goal item is S[0, N, s , /s ], where s stands for the beginning-of-sentence symbol and /s stands for the end-of-sentence symbol.",23,24
11643,1667249,"The goal item is S[0, N, s , /s ], where s stands for the beginning-of-sentence symbol and /s stands for the end-of-sentence symbol.",34,35
11644,1448113,"In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol.",30,31
11645,222225253,"The reviews are truncated, and delimited with the symbol '||'.",9,10
11646,815001,"Starting with a conjunction, verb, preposition and symbol is not allowed.",9,10
11647,815001,"Ending with a conjunction, verb, preposition, symbol, determiner, adjective or certain pronouns is not allowed. (",9,10
11648,241583340,"All corpora were lower-cased, and white space was inserted between the last word in every sentence and the punctuation symbol.",22,23
11649,176227592,The finite set of terminals is denoted Σ. There is a single start symbol S ∈ N .,13,14
11650,176227592,"The structures that are achievable are captured by the grammar in Table 4 , with P for prefix and S for suffix (also for ''start symbol'').",27,28
11651,52132833,"Length bias in NMT In NMT, unlike the word-by-word translation example in the previous section, each output symbol is conditioned on the entire input sequence.",23,24
11652,52132833,"In other words, the most likely translation is to immediately generate the stop symbol.",14,15
11653,1998416,2015) reads through all the source words until the end-of-sentence symbol <eos> is reached.,15,16
11654,247476436,The symbol @@ stands for sub-word tokenization of the dataset.,1,2
11655,247476436,The symbol * denotes the synonym token of the target language.,1,2
11656,52154258,"Here, we will use the symbol Θ to denote the set of shared parameters in our model.",6,7
11657,235293810,"Our grammar has a single distinguished start symbol S. It describes verb phrases (VP), containing transitive and intransitive verbs, as well as verbs that take a sentential complement (complementizers are denoted Comp).",7,8
11658,214713480,"N −1} where 2 ≤ i ≤ N −1, l ∈ L (2) Each word w j has one head (parent) w i with dependency label l from the label set L, where the parent can also be the ROOT symbol w 2 (see Section 3.1.1).",47,48
11659,214713480,"For dependency parsing, it also adds the ROOT symbol to the front of the sentence to represent the root of the dependency tree.",9,10
11660,248780263,"2020) so that it can map an utterance prefix (terminated with the MASK symbol, just as in BART's pre-training recipe) to the full utterance (freely hallucinating content words).",15,16
11661,247749071,"Specifically, we train a recognizer to output a sequence of symbols consisting of either fingerspelled letters or a special non-fingerspelling symbol <x>.",23,24
11662,247749071,as well as the blank symbol for CTC.,5,6
11663,11345360,In addition u j (j ≤ 0) are special symbols introduced to simplify the notation and u m+1 is a special symbol indicating a sentence boundary.,23,24
11664,248366328,"Another issue with distributional semantic models, as discussed by Emerson (2020c) , is the symbol grounding problem -if meanings of words are defined in terms of other words, the definitions are circular.",17,18
11665,14554263,"We also introduce a special ""any edge"" symbol, and say that all other edge types are simi-lar to this edge type.",9,10
11666,222272207,"y t−2 , y t−1 ], and y s denotes the start symbol of a sentence.",13,14
11667,222272198,"formly at random from a vocabulary Σ. We insert n instances of the long-distance rule αΣ k ω, with scaffold Σ k of length k, open symbol α, and close symbol ω, with α, ω ∈ Σ (with α as ""either"" and ω as ""or"").",30,31
11668,222272198,"formly at random from a vocabulary Σ. We insert n instances of the long-distance rule αΣ k ω, with scaffold Σ k of length k, open symbol α, and close symbol ω, with α, ω ∈ Σ (with α as ""either"" and ω as ""or"").",35,36
11669,222272198,"In Figure 7a , the model learns to predict the close symbol faster if the scaf- Vanishing Gradients: A familiar intervening span is predictably a less effective scaffold, because the familiarity will limit longer distance information due to vanishing gradients.",11,12
11670,222272198,"In other words, an unpredictable scaffold associated with a high error will dominate the gradient's sum over recurrences, delaying the acquisition of the symbol-matching rule.",26,27
11671,222272198,"To illustrate that the rule is learned regardless of training scaffolds, we use CD to isolate the contribu- tions of the open symbol in the out-domain test setting (Figure 8 ).",23,24
11672,222272198,Figure 9 illustrates how the unfamiliar-scaffold model predicts the close symbol ω with high probability based only on the contributions of the open symbol α.,12,13
11673,222272198,Figure 9 illustrates how the unfamiliar-scaffold model predicts the close symbol ω with high probability based only on the contributions of the open symbol α.,25,26
11674,222272198,"Meanwhile, the familiar-scaffold model probability increases substantially with each symbol consumed until the end of the scaffold, indicating that the model is relying on interactions between the open symbol and the scaffold rather than registering only the effect of the open symbol.",12,13
11675,222272198,"Meanwhile, the familiar-scaffold model probability increases substantially with each symbol consumed until the end of the scaffold, indicating that the model is relying on interactions between the open symbol and the scaffold rather than registering only the effect of the open symbol.",32,33
11676,222272198,"Meanwhile, the familiar-scaffold model probability increases substantially with each symbol consumed until the end of the scaffold, indicating that the model is relying on interactions between the open symbol and the scaffold rather than registering only the effect of the open symbol.",45,46
11677,222272198,"In particular, as seen in Figure 10 , the DI between open symbol and scaffold is substantially higher for the familiar-setting model and increases throughout training.",13,14
11678,222272198,"We consider the probability assigned to the close symbol according to the contributions of the open symbol, excluding interaction from any other token in the sequence.",8,9
11679,222272198,"We consider the probability assigned to the close symbol according to the contributions of the open symbol, excluding interaction from any other token in the sequence.",16,17
11680,222272198,"For contrast, we also show the extremely low probability assigned to the close symbol according to the contributions of the scaffold taken as an entire phrase.",14,15
11681,222272198,"In particular, note the pattern when the rule is extremely rare: The probability of the close symbol β as determined by the open symbol α is low but steady, while the probability as determined by the scaffold declines with scaffold length due to the accumulated low probabilities from each element in the sequence.",18,19
11682,222272198,"In particular, note the pattern when the rule is extremely rare: The probability of the close symbol β as determined by the open symbol α is low but steady, while the probability as determined by the scaffold declines with scaffold length due to the accumulated low probabilities from each element in the sequence.",25,26
11683,221448158,"q i−1 , a i−1 ) Find answer a i by posing q i to submodel t i : a i = A t i (q i , p) where q i is the i th generated sub-question and a i is the answer produced by a sub-model t i based on a given context paragraph p. This simple iterative process ends when q i+1 equals a special end-ofsequence symbol (denoted throughout as [EOQ]) with the final output answer a i .",77,78
11684,235097257,"1 ) except for the variable y * m takes values from the union of the K symbol sets that represent the K output sequences and the length of this sequence M * = K k=1 M k , is the sum of the lengths of the K output sequences.",17,18
11685,232223322,The Equation Template for each example is obtained by converting the corresponding equation into prefix form and masking out all numbers with a meta symbol.,24,25
11686,52111066,"n src + 1} ntrg , where we define (n src + 1) to be none, a distinguished symbol indicating unalignment.",22,23
11687,236460073,"The statement-slot technique is also used to capture and solve a small part of the data by symbol calculation, which helps to increase the performance and stability of our system.",19,20
11688,236460073,"For the simplicity of presentation, we define the following usage of symbols: a ""+"" symbol added behind the name of the model means the model is trained on train set and stopped when reaching the maximum accuracy on dev set; a ""-"" symbol added behind the name of the model means the model is trained on train set and stopped when reaching the minimum loss on dev set; no symbol added behind the name of the model means the model is trained on both train and dev set and stopped at fixed epochs.",18,19
11689,236460073,"For the simplicity of presentation, we define the following usage of symbols: a ""+"" symbol added behind the name of the model means the model is trained on train set and stopped when reaching the maximum accuracy on dev set; a ""-"" symbol added behind the name of the model means the model is trained on train set and stopped when reaching the minimum loss on dev set; no symbol added behind the name of the model means the model is trained on both train and dev set and stopped at fixed epochs.",49,50
11690,236460073,"For the simplicity of presentation, we define the following usage of symbols: a ""+"" symbol added behind the name of the model means the model is trained on train set and stopped when reaching the maximum accuracy on dev set; a ""-"" symbol added behind the name of the model means the model is trained on train set and stopped when reaching the minimum loss on dev set; no symbol added behind the name of the model means the model is trained on both train and dev set and stopped at fixed epochs.",77,78
11691,241583152,"We further discuss this topic in Appendix A. At the end of a sentence, the word generator either outputs a special sentence separator symbol, prompting the sentence generator to generate the next sentence representation, or an end-ofsummary symbol, stopping generation.",24,25
11692,241583152,"We further discuss this topic in Appendix A. At the end of a sentence, the word generator either outputs a special sentence separator symbol, prompting the sentence generator to generate the next sentence representation, or an end-ofsummary symbol, stopping generation.",42,43
11693,207852571,"Besides, there is the ROOT symbol, for the root of the dependency tree, which is always on the bottom of the stack.",6,7
11694,207852571,"The sequence of input tokens starts with the CLS symbol, then includes the tokens on the stack from bottom to top.",9,10
11695,207852571,"Then it has a SEP symbol, followed by the tokens on the buffer from front to back so that they are in the same order in which they appeared in the sentence.",5,6
11696,207852571,It also includes the ROOT symbol for the root of the dependency tree.,5,6
11697,7632722,"a k , is associated with a head index i (1 ≤ i ≤ k) that states that the head h(τ ) of any node labeled τ in a constituency tree that is built using this rule is the same as the head of the right-hand side symbol a i .",51,52
11698,7632722,"The set of actions contains one Shift (S), one Ghost Reduction (GR) a set of |Σ| unary reductions (RU-X), one for each symbol, a set of |Σ| binary left reductions (RL-X) and a set of |Σ| binary right reductions (RR-X) (see also Sagae and Lavie (2006) and Figure 3 for details).",32,33
11699,201666088,"The brackets ""["" and ""]"" indicate that the tagged word is the starting or ending of a mention respectively, the symbol ""+"" indicates that one or more mention brackets are open, and ""-"" indicates that none mention bracket is open.",25,26
11700,1511192,The symbol denotes a element-wise product of the two vectors.,1,2
11701,1511192,"The features used in the baseline model include the current word, its POS tag, its prefixes and suffixes between one to four characters, its position, its stylistics (e.g., case, digit, symbol, alphanumeric), and its context (i.e., the same features for the two preceding and the two following words).",38,39
11702,233004708,"When end conversation is selected, all future predictions are ignored, much like an <EOS> symbol signifies stopping.",18,19
11703,225041060,"T i = [[CLS], w 1 , w 2 , ..., w n ] (1) H i = Bert(T i ) (2) X S i = H i [0] ∈ R d S (3) where T i is the raw contents of text fragment i with the added tag [CLS]; Bert is a symbol for BERTlike pretrained models; H i is the last hidden states of the pretrained model; and X S i is our extracted semantic feature for the fragment i. The dimension of X S i equals to the hidden states of pretrained model, and we denote it as d S .",69,70
11704,14581926,"We use the usual definition of a context-free grammar (CFG) as a 4-tuple G = (E, V, R, S), where E is the set of terminals, V is the set of nonterminals, R is the set of productions, and S C V is the start symbol.",60,61
11705,14581926,"Finally, we complete the construction with productions for the start symbol S: (S-rules) S > WCp,qW, l <_ p,q < n 2.",11,12
11706,1345,"R,~+l(r,~+l) is defined as STOP -the STOP symbol is added to the vocabulary of nonterminals, and the model stops generating right modifiers when it is generated.",8,9
11707,1345,"Most importantly, the probability of generating the STOP symbol will be 0 when the subcat frame is non-empty, and the probability of generating a complement will be 0 when it is not in the subcat frame; thus all and only the required complements will be generated.",9,10
11708,9781787,"ii) Features on (i) (see below) (iii) values for the CL(itics) label are: prod (dialogue pronouns, for me , re, noua, vous); protob (third person object pronouns : le, la, /es); prota (third person dative pronouns : lui, leur) ; se, en and y, (for se, en and y pronouns respectively); n is a barrier symbol (see below). (",84,85
11709,226283454,"Each of these agents predicts a transition distributions, from which actions can be sampled from: Skipping amounts to ignoring the next word in the sequence, i.e., not updating the LSTM, whereas jumping ignores all information up to some point, which can either be the next clausal separator symbol (, or ;), or the next sentence segmentator (., !",53,54
11710,925851,The corresponding contexts of both strings are generalized to '=' symbol which means remains unchanged.,12,13
11711,925851,"And then, a rule name is given to the mop by creating a symbol representing the transformation form.",14,15
11712,9435670,"In this model, dependents on one side of the head are generated by repeatedly sampling from a categorical distribution until a special stop-symbol is generated.",25,26
11713,9435670,"× w∈W l pL(w) w∈Wr pR(w), (3) where PARTS(W) is the set of all partitions of multiset W into two multisets W l and W r , p L is the probability mass function for a dependent to the left of the head, p R is the function for a dependent to the right, and stop is a special symbol in the support of p L and p R which indicates that generation of dependents should halt.",67,68
11714,61621900,"A dotted rule is a grammar rule with a dot inserted somewhere on the right-hand side, e.g. S -+ -NP VP S -+ NP • VP S --~ NP VP • However, since these dotted rules are to be used as terminal symbols of a regular language, it is convenient to use a more compact notation: they can be replaced by a triple made out of the nonterminal symbol on the left-hand side, an integer to determine one of the productions for that nonterminal, and an integer to denote the position of the dot on the righthand side by counting the number of symbols to the left of the dot.",74,75
11715,61621900,"It will turn out to be convenient to use a slightly more complicated notation: when the dot is located after the last symbol on the right-hand side we use z as the third element of the triple instead of the corresponding integer, so the last triple is (S, 4, z) instead of (S, 4,2). (",23,24
11716,61621900,"Note that z is an additional symbol, not a variable.)",6,7
11717,61621900,"Moreover, for epsilon-rules, where there are no symbols on the right-hand side, we treat the e as it were a real symbol and consider there to be two corresponding dotted rules, e.g. (MOD, 1, O) and (MOD, 1, z) corresponding to 'MOD --~ • e' and 'MOD --~ e -' for the rule 'MOD -+ e'.",28,29
11718,61621900,"Using these dotted rules as auxiliary symbols we can work with regular languages over the alphabet E= TU{ (X,m,n) ]X E V Am= I,...,mxA n = O,...,max{nx,m -1,O},z} where T is the set of terminal symbols, V is the set of nonterminals, mx is the number of productions for nonterminal X, and nx,m is the number of symbols on the right-hand side of the ruth production for X. It will be convenient to use the symbol * as a 'wildcard', so (s,*, O) means { (X,m,n} E E IX = s,n=O} and (*,*,z) means {(X,m,n) E Eln= z }. (",97,98
11719,61621900,"For each non-epsilon-rule with dotted rules (X,m,n), n = O,...,nx,m -1,z, for each n = 0,...,nx,m-1: E*(X,m,n)next(X,m,n + 1)E* (3) where next(X, m, n) = a(X,m,n) (rhs(X, m, n) = a, aCT, n<nx,m) a(X,m,z) (rhs(X, m, n) = a, aeT, n=nx,m) (A, *, 0) (rhs(X, m, n) = A, A e V) where rhs(X, m, n) is the nth symbol on the righthand side of the ruth production for X. Formula 3 states that the dotted rule (X, m, n) must be followed by a(X, m, n + 1) (or a(X, m, z) when n+ 1 = nx,m) when the next item to be parsed is the terminal a, or by C A, *, 0) (starting to parse an A) when the next item is the nonterminal A. For each non-epsilon-rule with dotted rules (X,m,n), n = O,...,nx,,~ -1,z, for each n = 1,..., nx,m -1, z: E*prev(X, m, n)(X, m, n)E* (4) where prev(X, m, n) = iX, re, n-1)a (rhs(X, m, n) = a, a C T, n ~ z) (X, m, nx,m -1)a (rhs(X, m, n) = a, a • T, n = z) (A, *, z) (rhs(X, m, n) = A, A • V) Formula 4 similarly states that the dotted rule (X, m, n) must be preceded by i X, m, n -1)a (or (X,m, nx,m -1) when n = z) when the previous item was the terminal a, or by (A,*,z) when the previous item was the nonterminal A. For each epsilon-rule corresponding to dotted rules (X,m,O) and (X,m,z): and (5) E*(X,m,O)(X,m,z)E*, (x, m, 0)(x, m, (6) Formulae 5 and 6 state that the dotted rule (X, ra,0) must be followed by (X,m,z), and (X, m, z) must be preceded by iX, m, 0).",139,140
11720,61621900,"The terms need not be ground, so the Prolog variable symbol _ is used instead of the 'wildcard' symbol • in the description of the algorithm. •",11,12
11721,61621900,"The terms need not be ground, so the Prolog variable symbol _ is used instead of the 'wildcard' symbol • in the description of the algorithm. •",21,22
11722,61621900,"In a regular expression: -#X refers to the contents of register X; -$ represents E, any single terminal symbol; -s represents a string of terminals with length equal to the number of arguments; so s with no arguments represents the empty string e, s(a) represents the single terminal a, and s(s/_/0) represents the dotted rules (s, *, 0); -Kleene star is * (redefined as a postfix operator), and concatenation and union are ^ and +, respectively; other operators provided include ~ (intersection) and -(difference); there is no operator for complementation; instead subtraction from E* may be used, e.g. ($ *)-(#1) instead of L; -rein(RE,L) denotes the result of removing from the language RE all terminals that match one of the expressions in the list L. The context-free language recognised by the original context-free grammar is { anb n [ n > 0 }.",21,22
11723,61621900,"It records, in effect, one of three possibilities for each terminal symbol: whether it has not yet appeared, has appeared and must appear again, or has appeared and need not appear again.)",13,14
11724,61621900,To 'remove' a symbol means to substitute it by e: a regular operation.),5,6
11725,61621900,"Finally, combine the results from each subgrammar by starting with the approximation for the start symbol S and substituting the approximations from the other subgrammars in an order consistent with the partial ordering that is induced by 7~ on the subgrammars.",16,17
11726,61621900,"Each has the property that the symbols d, a and n occur only in the combination d a* n. This fact has been used to simplify the state diagrams by treating this combination as a single terminal symbol dan; hence the approximations are drawn with 10 and 9 states, respectively.",39,40
11727,61621900,"When n = 1 this reduces to next(t), the set of terminals that may follow the terminal t. The effect of filtering with Rimon and Herz's next(t) is similar to applying conditions 1-6 from section 3, but the use of auxiliary symbols causes two differences which can both be illustrated with the following grammar: S~aXa[bXb X--+e On the one hand, Rimon and Herz's 'next' does not distinguish between different instances of the same terminal symbol, so any a, and not just the first one, may be followed by another a. On the other hand, Rimon and Herz's 'next' looks beyond the empty constituent in a way that conditions 1-6 do not, so ab is disallowed.",88,89
11728,16478155,Let the values for the expected cell counts that are estimated by the model be represented by the symbol 7hljk .... The interaction terms in the loglinear nmdels represent constraints on the estimated expected marginal totals.,18,19
11729,220047816,The symbol n w z is the term frequency of the word w in the cluster z. The current vocabulary size of the model is represented by V .,1,2
11730,226283504,"This is shown in Figure 4 , where the right-hand-side (RHS) fragment can be represented as its left-to-right linearization, with reentrant nodes flagged by a dedicated $ symbol.",38,39
11731,226283504,"An RDG is a tuple P, N, Σ, S, V where P is a set of productions of the form α → β; N is the set of non-terminal symbols {L, T 0 , • • • , T n } up to a maximum number of n; Σ is the set of terminal symbols; S is the start symbol; V is an unbounded set of variable references {$1, $2,...}, whose role is described below.",70,71
11732,226283504,"The non-terminal L can only be rewritten as a terminal symbol l ∈ Σ. If a node is reentrant, we mark it with a superscript * over v. Variable references are percolated down the derivation and are replaced once a reentrant variable v * is found on the RHS.",12,13
11733,226283504,"Our grammar derives strings by first rewriting the start symbol S, a non-terminal function T 0 .",9,10
11734,226283504,For G 1 the first fragment predicted initializes the graph (this corresponds to substituting the start symbol S).,17,18
11735,248779968,An empty IPA symbol can be specified for graphemes that are to be ignored.,3,4
11736,248779968,"The input text transcription is parsed, matching first the longest grapheme, to yield an IPA symbol sequence.",17,18
11737,248779968,"To easily map between BPE units in language modeling and IPA symbols in acoustic modeling, we use an intermediate code (that we call ""nxsampa"") which unambiguously represents any IPA symbol with a single character symbol.",34,35
11738,248779968,"To easily map between BPE units in language modeling and IPA symbols in acoustic modeling, we use an intermediate code (that we call ""nxsampa"") which unambiguously represents any IPA symbol with a single character symbol.",39,40
11739,980618,"We also show that when an unbounded number of these symbol classes are allowed within a transformation, then the associated learning problem becomes NP-hard.",10,11
11740,980618,Throughout the paper we assume that the rightmost symbol of w is an end-marker not found at any other position in the string.,8,9
11741,980618,"We also say that implicit node q immediately dominates node p if q splits the arc between parent(p) and p. Of main interest here are the following properties of suffix trees: • if node p has children Pl .... , Pd, then d _> 2 and strings label(p, pi) differ one from the other at the leftmost symbol; .",63,64
11742,980618,"function Slow_scan(p, u): Starting at p, scan u symbol by symbol.",11,12
11743,980618,"function Slow_scan(p, u): Starting at p, scan u symbol by symbol.",13,14
11744,980618,Return the {implicit) node corresponding to the last matching symbol.,11,12
11745,980618,"function Fast_scan(p, u): Starting at p, scan u by iteratively (i) finding the edge between the current node and one of its children, that has the same first symbol as the suffix of u yet to be scanned, and (ii) skipping a prefix of u equal to the length of the selected edge label.",34,35
11746,980618,Stop as soon as a symbol is not matched.,5,6
11747,980618,"For each u j, 1 < j _< d-1, we charge a constant amount of time to the symbol in w ""corresponding"" to the last symbol of uj.",21,22
11748,980618,"For each u j, 1 < j _< d-1, we charge a constant amount of time to the symbol in w ""corresponding"" to the last symbol of uj.",30,31
11749,980618,"The visit to Ud, on the other hand, is charged to the ith symbol of w. (Note that charging the visit to ud to the symbol in w ""corresponding"" to the last symbol of Ud does not work, since in the case of sbi ---"" bi the same symbol would be charged again at the next iteration of the for-cycle.)",15,16
11750,980618,"The visit to Ud, on the other hand, is charged to the ith symbol of w. (Note that charging the visit to ud to the symbol in w ""corresponding"" to the last symbol of Ud does not work, since in the case of sbi ---"" bi the same symbol would be charged again at the next iteration of the for-cycle.)",28,29
11751,980618,"The visit to Ud, on the other hand, is charged to the ith symbol of w. (Note that charging the visit to ud to the symbol in w ""corresponding"" to the last symbol of Ud does not work, since in the case of sbi ---"" bi the same symbol would be charged again at the next iteration of the for-cycle.)",37,38
11752,980618,"The visit to Ud, on the other hand, is charged to the ith symbol of w. (Note that charging the visit to ud to the symbol in w ""corresponding"" to the last symbol of Ud does not work, since in the case of sbi ---"" bi the same symbol would be charged again at the next iteration of the for-cycle.)",55,56
11753,980618,"It is not difficult to see that, in this way, each symbol of w is charged at most once.",13,14
11754,980618,"The second result holds even if we restrict ourselves to IEI = 2 and Irl = 1, that is if we use a don~t care symbol.",28,29
11755,980618,denotes the don't care symbol): aJl-l?aJ~-ji-1 ? ...?,5,6
11756,2883471,"Even the presence of word correspondences, a tradition cue for detecting and correcting speech repairs, sometimes marks boundary tones as well, as illustrated by the following example where the intonational phrase boundary is marked with the ToBI symbol %.",40,41
11757,1870332,"OTP with automata We may encode each timeline as a string over an enormous alphabet E. If [Tiersl = k, then each symbol in E is a k-tuple, whose components describe what is happening on the various tiers at a given moment.",24,25
11758,1870332,Such a predicate allows any symbol from A on the tiers it does not mention.,5,6
11759,1870332,"This means that the timeline ( [, -)(+, -)*(+, [)(% +)*('], +)(-, +)*(-, ]) cannot avoid violating a clash constraint simply by instantiating the (+, +)* part as e. Furthermore, the ] convention means that a zero-width input constituent (more precisely, a sequence of zero-width constituents, represented as a single 1 symbol) will often act as if it has an interior.",86,87
11760,1870332,"Clash FSAs are therefore just degenerate versions of implication FSAs, where the arcs looking for/3j do not exist because they would accept no symbol. (",24,25
11761,2397927,"The symbol ,-r denotes 'feasible tuples', similar to 'feasible pairs' in traditional two-level morphology.",1,2
11762,2397927,"Accepting rs Let 7-be the set of all rs in a regular grammar, p be an auxiliary boundary symbol (not in the grammar's alphabets) and p' = Ida(p).",21,22
11763,2397927,"However, since in §4.1 above, the symbol p appears freely, we need to introduce it in the above expression.",9,10
11764,2397927,"devoid of p~. The first condition is accomplished by simply placing p' to the left and right of r. As for the second condition, we use an auxiliary symbol, w, as a place-holder representing r, introduce p freely, then substitute r in place of w. Formally, let w be an auxiliary symbol (not in the grammar's alphabet), and let w ~ = Ida(w) be a place-holder representing r. The above expression becomes For all rs, we subtract this expression from the automaton under construction, yielding CR = Centers -U Restrict( ') (S) T CR now accepts only the sequences of tuples which appear in contexts in the grammar (but including the partitioning symbols p~); however, it does not force surface coercion constraints.",30,31
11765,2397927,"devoid of p~. The first condition is accomplished by simply placing p' to the left and right of r. As for the second condition, we use an auxiliary symbol, w, as a place-holder representing r, introduce p freely, then substitute r in place of w. Formally, let w be an auxiliary symbol (not in the grammar's alphabet), and let w ~ = Ida(w) be a place-holder representing r. The above expression becomes For all rs, we subtract this expression from the automaton under construction, yielding CR = Centers -U Restrict( ') (S) T CR now accepts only the sequences of tuples which appear in contexts in the grammar (but including the partitioning symbols p~); however, it does not force surface coercion constraints.",60,61
11766,2397927,"a!blcldlOlelf!O!g!h!i!OlS""""Saee The lexical expression contains the lexical forms {abcd}, {ef} and {ghi}, separated by a boundary symbol, b, which designates the end of a lexical entry.",24,25
11767,2397927,"Firstly, we suffix each lexical entry in the lexicon with the boundary symbol, ~, and it's feature structure. (",13,14
11768,2397927,"A lexical entry # (e.g., morpheme) which is associated with a feature structure ¢ is simply expressed by/~¢, where k is a (morpheme) boundary symbol which is not in the alphabet of the lexicon.",30,31
11769,2397927,"For convenience in later expressions, we incorporate features with ~ as follows ~¢ -,T U • (16) The overall lexicon can be expressed by, 14 Lexicon = LI × L~ × ... ( 17 ) 14To make the lexicon describe equal-length relations, a special symbol, say 0, is inserted throughout.",51,52
11770,2397927,"This lexicon can be substantially reduced by intersecting it with Proj ect~'l (~0).. If a two-level grammar is compiled into an automaton, denoted by Gram, and a lexicon is compiled into an automaton, denoted by Lez, the automaton which enforces lexical constraints on the language is expressed by L = (Proj,ctl(~)* × Lex) A Gram (18) The first component above is a relation which accepts any surface symbol on its first tape and the lexicon on the remaining tapes.",84,85
11771,2005706,"The precedence constraints are formulated as a binary relation ""~"" over dependency labels, including the special symbol ""self"" denoting the head.",19,20
11772,16571013,"Linguistic Background Languages based on the Arabic script typically employ an abjad writing system, where each symbol represents a consonant while vowels and other phonetic units, commonly known as diacritics, are usually omitted in writing.",17,18
11773,247778738,"For a product quantizer, the dimension of codeword is h = D/M , and the decompressed vector is x = c 1 • c 2 • • • • c M where symbol • denotes vector concatenation.",35,36
11774,247084324,"Hahn shows that for languages where acceptance depends on a single input symbol, a transformer's classification decisions become less and less confident (that is, with crossentropy approaching 1 bit per string) as input strings get longer and longer.",12,13
11775,247084324,"Hahn (2020) tries to explain some of these by showing (his Lemma 5) that changing a single input symbol only changes the output of a transformer encoder by 𝑂 (1/𝑛), where 𝑛 is the input string length.",22,23
11776,247084324,"Thus, for a language where acceptance depends on a single input symbol, a transformer might accept or reject strings with perfect accuracy, but for large 𝑛, it must do so with low confidence, giving accepted strings a probability just above ½ and rejected strings a probability just below ½. More precisely, as 𝑛 increases, the cross-entropy approaches its worst possible value of 1 bit per string.",12,13
11777,247084324,"It only requires attention on the first symbol, but the lemma still applies because a change in this symbol changes the correct answer.",7,8
11778,247084324,"It only requires attention on the first symbol, but the lemma still applies because a change in this symbol changes the correct answer.",19,20
11779,247084324,"Let 𝑛 = |𝑤| + 1, let 𝑤 0 = CLS, and let 𝑤 𝑖 be the 𝑖-th symbol of 𝑤. The input layer has a word embedding and positional encodings, WE : Σ → R 𝑑 PE : N → R 𝑑 which are used to compute input vectors for 𝑖 = 0, . . .",20,21
11780,247084324,"However, we did find that the limited influence of a single input symbol, implied by Hahn's lemma, has a serious practical implication for learnability (c).",13,14
11781,8620787,"Thus, the symbol MR.",3,4
11782,8620787,"The symbol ]5 -~ SAm +5 SAID :'.= 2 7 ,I stands for a feature which asks ""Does the"" word SAID appear in the previous five sentences but not in the next five sentences?""",1,2
11783,1085832,"In the table, the symbol O is a placeholder for a possible French word and the symbol [] is a placeholder for a possible English word.",5,6
11784,1085832,"In the table, the symbol O is a placeholder for a possible French word and the symbol [] is a placeholder for a possible English word.",17,18
11785,1085832,"Figure 7 illustrates an unsafe segmentation, in which a segment boundary (denoted by the II symbol) lies between a and mangd, where there is no rift.",17,18
11786,6658277,"Hash tags are annotated as if the hash symbol was not there, e.g. #truestory is labelled lang1.",8,9
11787,989144,We first use the standard technique of converting regular mrelations into same-length regular relations by padding them with a space symbol 0.,22,23
11788,989144,"We proceed subtraetively, starting as an initial approximation with an art)itrary concatenation of the possible l)artitions, i.e. the (:entres of Cl/, rules: co(Dee)* (4) From this we wish to subtract tim set of strings containing a t)artition that is not allowed by any CR rule: We introduce a new placeholder symbol T, r ~ 7c O {co}, to represent the centre of a rule, so the set of possihle contexts for a given centre G D is given by: [.J z%, .",60,61
11789,989144,"This peculiarity, in the Koskenniemi formalism, is due to the dual interpretation of the 0 symbol in the parallel formalisin: it is a genuine symbol in the alphabet, yet it acts as the empty string e in two-level ext)ressions.",17,18
11790,989144,"This peculiarity, in the Koskenniemi formalism, is due to the dual interpretation of the 0 symbol in the parallel formalisin: it is a genuine symbol in the alphabet, yet it acts as the empty string e in two-level ext)ressions.",27,28
11791,7786603,"In order to perform quick comparisons while keeping symbol names readily available, a symbol in C-PATR is designated to be the location in memory of its print name, maintained on a letter tree, where each unique symbol-name has only one entry.",8,9
11792,7786603,"In order to perform quick comparisons while keeping symbol names readily available, a symbol in C-PATR is designated to be the location in memory of its print name, maintained on a letter tree, where each unique symbol-name has only one entry.",14,15
11793,7786603,"In order to perform quick comparisons while keeping symbol names readily available, a symbol in C-PATR is designated to be the location in memory of its print name, maintained on a letter tree, where each unique symbol-name has only one entry.",41,42
11794,10282937,"Whenever a hypothesis ends with the sentence end symbol </s> and its score is the highest, the decoder reports it as the search result.",8,9
11795,2753602,"In Table 4 , the symbol ""=str"" denotes string equality, N the natural numbers.",5,6
11796,2144,"With probability 6,(y[zn), a Markov model in the state z '~ will emit the symbol y and transition to the state z'~y.",17,18
11797,2144,"Therefore, the probability Prn(ZtlX t-1 , ¢) assigned by an order n basic Markov model ¢ to a symbol z' in the history z t-1 depends only on the last n symbols of the history.",20,21
11798,2144,"The idea of the proof is that our nonemitting model will encode the first symbol Zl of the string z T in its state distribution, for an unbounded distance.",14,15
11799,2144,This will allow it to predict the last symbol ZT using its knowledge of the first symbol zl.,8,9
11800,2144,This will allow it to predict the last symbol ZT using its knowledge of the first symbol zl.,16,17
11801,2144,"The basic model will only be able predict the last symbol ZT using the preceding n symbols, and therefore when T is greater than n, we can arrange for p,(zTl¢,T) to differ from any p,~(zT[¢',T), simply by our choice of zl.",10,11
11802,2144,The first symbol zl will determine whether the non-emitting model goes to the order 2 state or stays in the order 0 state.,2,3
11803,2144,"No matter what probability the basic model assigns to the final symbol ZT, the non-emitting model can assign a different probability by the appropriate choice of Zl, 6O(ZT), and Consider the second order non-emitting model over a binary alphabet with )~(0) = 1, A(1) = 0, and A(ll) = 1 on strings in AI'A. When zl = 0, then x2 will be predicted using the 1st order model 61(x21xl), and all subsequent zt will be predicted by the second order model 62(ztlxtt_-~).",11,12
11804,2144,"MAXIMIZATION-STEP(~b,~+ ,)~-); 7.Initialize ~ using B; Here ,~+ (zi) accumulates the expectations of emitting a, symbol from state z i while )~-(zi) accumulates the expectations of transitioning to the state z~ without emitting a symbol.",26,27
11805,2144,"MAXIMIZATION-STEP(~b,~+ ,)~-); 7.Initialize ~ using B; Here ,~+ (zi) accumulates the expectations of emitting a, symbol from state z i while )~-(zi) accumulates the expectations of transitioning to the state z~ without emitting a symbol.",47,48
11806,2144,were mapped to a unique OOV symbol.,6,7
11807,2144,"In contrast, the non-emitting model will immediately transition to the empty context in order to predict the first symbol Yl, and then it need never again transition past any suffix of x n-].",21,22
11808,10777279,"1995; Cover and Thomas, 1991) to represent how many times and in what context each symbol appeared in a sequence of symbols.",18,19
11809,10777279,"The left node of T(4) represents that both 'a' and ""b' appeared only once after symbol 'a', while the right node of T(4) represents only 'a' occurred once after 'b '.",20,21
11810,198921556,"Moreover, as the end of sequence symbol <S> is part of the vocabulary, the loss is also implicitly favouring shorter sentences.",7,8
11811,198921556,"Active symbols The counterpart of the average message length is the active symbols metric, which expresses how many symbol types from its vocabulary are used by the Sender.",19,20
11812,198921556,Perplexity per symbol.,2,3
11813,198921556,"As in Havrylov and Titov (2017) , we used the perplexity per symbol metric to measure how often a symbol was used in a message to describe the same object: P pl = exp(− [s (t) • log(s (t) )]) where s (t) are the vocabulary scores (given by an affine transformation of the Sender's hidden state at timestep t) for all symbols in the vocabulary.",14,15
11814,198921556,"As in Havrylov and Titov (2017) , we used the perplexity per symbol metric to measure how often a symbol was used in a message to describe the same object: P pl = exp(− [s (t) • log(s (t) )]) where s (t) are the vocabulary scores (given by an affine transformation of the Sender's hidden state at timestep t) for all symbols in the vocabulary.",21,22
11815,198921556,It is given by the formula S = − w∈V [c w • log(c w )] where c w is the count of the occurrences in the produced messages of each symbol w for all symbols in the vocabulary V .,33,34
11816,198921556,"Moreover, there is 26% more symbol reuse within the sequences in the penalty case, as shown by the lower number of unique symbols per message.",7,8
11817,198921556,These results show that the use of the vocabulary loss gives rise to languages with symbol reuse.,15,16
11818,198921556,"Perplexity per symbol The colour constancy game achieved the lowest perplexity per symbol, both with and without the vocabulary penalty.",2,3
11819,198921556,"Perplexity per symbol The colour constancy game achieved the lowest perplexity per symbol, both with and without the vocabulary penalty.",12,13
11820,198921556,"Here, similarly to the perplexity per symbol metric, the colour constancy condition triggered the highest scores both for the Sender and the Receiver when the penalty is on.",7,8
11821,49742817,"First, we assume that the language under consideration contains a fixed set of phonemes Σ, plus an edge symbol # marking the beginning and end of words.",20,21
11822,49742817,"The encoder RNN reads each symbol in the input string one at a time, first assigning it a unique embedding, then processing that embedding to produce a representation of the phoneme given the rest of the phonemes in the string.",5,6
11823,49742817,Decoding ends when a halt symbol is output.,5,6
11824,49742817,"MacWhinney and Leinbach (1991), Plunkett and Marchman (1991, 1993) , and Plunkett and Juola (1999) map phonological strings to phonological strings using feed-forward networks, but rather than turning to Wickelphones to imprecisely represent strings of any length, they use fixed-size input and output templates, with units representing each possible symbol at each input and output position.",64,65
11825,9732862,"The void symbol ∅ indicates a correct null subject pronoun in Spanish, and an incorrect object pronoun dropped by the MT system.",2,3
11826,67875226,"Pair #1 uses sequential commas (i.e., in ""the egg, larva, pupa, and adult"") or a special symbol sequence (i.e., in ""egg −> larva −> pupa −> adult"") to form a set or sequence; pair #2 has ""A (or B)"" to express the equivalence of A and B. This challenge is expected to be handled by DNNs with specific training signals.",25,26
11827,41234072,The lack of a symbol indicates an exact match and will be replaced by E thereafter.,4,5
11828,238259136,"B Subword Tokenizers B.1 Byte-Pair Encoding GPT-2 uses a variation on Byte-Pair Encoding (BPE) to iteratively choose the most frequently occurring bigram of symbols in the training corpus, merge them into a single symbol, and add the merged symbol to its subword vocabulary until it reaches its maximum vocabulary size of 50,256.",40,41
11829,238259136,"B Subword Tokenizers B.1 Byte-Pair Encoding GPT-2 uses a variation on Byte-Pair Encoding (BPE) to iteratively choose the most frequently occurring bigram of symbols in the training corpus, merge them into a single symbol, and add the merged symbol to its subword vocabulary until it reaches its maximum vocabulary size of 50,256.",46,47
11830,210156767,"The majority of existing computational models in phonology, however, model learning as symbol manipulation and operate with discrete units-either with completely abstract made-up units or with discrete units that feature some phonetic properties that can be approximated as phonemes.",14,15
11831,247218432,"z k • S 0 the start symbol, a distinguished element of N satisfying d(S 0 ) = 1 The choice of MCFGs as our formal backbone comes due to their many advantages.",7,8
11832,248779969,Recognizes all possible prefixes composed of user input followed by any character up to the next morph boundary symbol.,18,19
11833,248779969,"If C s (x : y) denotes the counts at state s for symbol-pairs x : y, then the transition weight w is defined as: w = C s (x : y) (f s + z:u C s (z : u)) In the evaluation we refer to this weighting scheme as tWFST.",15,16
11834,235898904,"Then, we uniformly select a keyword content/specificity pair for each span from the set of keyword candidates (including the * symbol).",24,25
11835,52185846,"In such cases, the decoder outputs an <unk> symbol for any word that is not in the vocabulary, which will harm the translation quality.",11,12
11836,52185846,"The word baseline model cannot decode this; therefore, an <unk> symbol is produced.",15,16
11837,248779978,"Therefore, we leverage the bigrams and symbol information to enrich features by applying feature templates.",7,8
11838,248779978,"Moreover, we customize the feature by adding the additional symbol feature.",10,11
11839,248779978,"Through symbol projection, each character is project into a onedimensional array such as [0, 0, 1], each position represents [date, digit, letter].",1,2
11840,248779978,"Followed by an activate function ReLU, symbol embedding is generated with the vector size of 3 and denoted as S n .",7,8
11841,248779978,The character embedding generated from BERT is a 768-dimensional vector (denoted as c n ) and is resized as (768 + 3) through concatenating with symbol embedding.,30,31
11842,248779978,Two adjacent character embeddings with their symbol embeddings are concatenated as bigram features.,6,7
11843,248779978,"Compared with the baseline which is the best result of Bakeoff2005 Ablation Study As detailed in Figure 1 and Figure 5 , the structure of the SpIn contains four main components: the ""C-NC"" tagging schema, the bigram features, the symbol features, and the inference layer.",46,47
11844,248779978,"Our investigation is mainly carried out through: • substituting ""C-NC"" with traditional tagging schemas; • substituting bigram with unigram features; • removing symbol features; Substitution of ""C-NC"" Contrast experiments of tagging schemas are illustrated in Table 2 .",29,30
11845,248779978,"Each character feature is substituted with the bigram feature, representing the concatenation of the current and the previous character feature with their corresponding symbol feature.",24,25
11846,248779978,4 illustrates the effect of the symbol features.,6,7
11847,248779978,"After employing the symbol features, the result is further pushed up to +2.6% F1 score on the CTB6 dataset.",3,4
11848,248779978,"Hence, the symbol features are leveraged in the following experiments by default.",3,4
11849,248779978,"Similarly, the decline in F score has been observed after removing the symbol feature.",13,14
11850,248779978,The bigram feature is generated by concatenating the current and the previous character feature with their corresponding symbol feature.,17,18
11851,248779978,Table 8 illustrates the effect of the symbol features for the deep neural model.,7,8
11852,248779978,"In contrast with the results in Table 4 , the symbol features are insignificant in result improvements.",10,11
11853,226283639,"Define the collection of TT-cores {G (k) ∈ R 1×I k ×J k ×1 } N k=1 using the equations G (k) [:, (i, j), :] = δ ij , (4) with δ ij denoting the Kronecker delta symbol.",52,53
11854,202788853,"We next embed every symbol of x, again using a learned embedding lookup, and apply one-dimensional convolutions of increasing widths over this list of vectors, similar to Kim (2014) ; Shrestha et al. (",4,5
11855,202788853,"For both Twitter and Reddit, we estimate the sub-word vocabulary on training data using an inventory of 65,536 word pieces, including a distinguished end-of-sequence symbol.",32,33
11856,202788853,"3 We restrict to the 2048 most popular subreddits, mapping all others to a distinguished unk symbol.",17,18
11857,243865314,"At training time, we fix p ∈ {0.05, 0.15, 0.30, 0.45, 0.60} and replace each subword of each Amazon review with the distinguished <mask> symbol with probability p in each training epoch.",33,34
11858,233219935,"In this work, we take Σ to be the set of 41 phonetic symbols in ASJP plus the end-of-string symbol.",24,25
11859,44096230,"For all experiments we apply a consistent set of pre-processing steps designed to reduce sparseness in the data: we lowercase the text, remove punctuation, replace each digit with a common symbol, expand contractions, and lemmatize (using NLTK for the last two).",35,36
11860,235097551,"Given a sequence of tokens w = [w 1 , ..., w N ], a corrupted version ŵ is constructed by randomly setting a portion of tokens in w to a special [MASK] symbol.",38,39
11861,248780515,"F (j) = 0, j = 0 1, j = 0 (5) F (j) converts all positive numbers to 1 and j is a variable symbol.",33,34
11862,224804104,"For an example i, we calculate e i P = | Ê− i ∩E i + | | Ê− i | , e i R = | Ê+ i ∩E i − | |E i − | where symbol + denotes the temporal transitive closure (Allen, 1983) of the edge set.",39,40
11863,8509316,"if a is a singleton sort and f is a featnre symbol, then fz maps a z into NONE ""/ When interpreting a feature term with variables and named disjunctions, we have to make sure that the same value is assigned to each occurrence of a variable and that the same branch is chosen for each occurrence of a named disjunction.",11,12
11864,234679287,We remove the # symbol from hashtags in tweets and tokenize the hashtags.,4,5
11865,222271899,"We compare the performance of two variants of SA, one with a starting symbol (SA + ) and one without (SA − ).",14,15
11866,222271899,"We empirically show that with the addition of a starting symbol to the vocabulary, a two-layer multi-headed SA network (i.e., the encoder of a Transformer) is able to learn D n languages, and generalize to longer sequences, although not perfectly.",10,11
11867,222271899,"For example, the symbol ""]"" in the string ""([])"", will first pop ""["" from the stack, then it attends to ""("", the last unmatched symbol, which will determine the next valid closing bracket.",4,5
11868,222271899,"For example, the symbol ""]"" in the string ""([])"", will first pop ""["" from the stack, then it attends to ""("", the last unmatched symbol, which will determine the next valid closing bracket.",39,40
11869,222271899,"The starting symbol (T) enables the model to learn the occurrence of the end of a clause or the end of the sequence, which can be regarded as a mechanism to represent an empty stack.",2,3
11870,222271899,"We present detailed comparison between an SA which incorporates a starting symbol (SA + ), and one that does not (SA − ), and demonstrate significant differences in their generalization across the length of sequences and the depth of dependencies.",11,12
11871,222271899,Our analysis sheds light on the ability of SA to learn hierarchical structures by elegantly attending to the correct preceding symbol.,20,21
11872,222271899,"We train two multi-headed self-attention networks (i.e., only the encoder part of a Transformer), one of which incorporates an additional starting symbol in the vocabulary (SA + ), and the other does not (SA − ).",29,30
11873,222271899,We use learnable embeddings to convert each input symbol to a 256-dimensional vector.,8,9
11874,222271899,"We train two unidirectional LSTMs, one with the starting symbol (LSTM + ) and the other without it (LSTM − ).",10,11
11875,222271899,"  In our sequence prediction task, the input vocabulary (V i n ) for a D n language consists of 2n+1 symbols: n pairs of brackets (or parentheses), and an additional starting symbol T whereas the output vocabulary (V o n ) does not include the starting symbol T. Since there might exist multiple possibilities for the next bracket in a sequence, we adopt a multi-label classification approach wherein the outputs are encoded as a k-hot vector and the network is optimized using the binary cross-entropy loss function given by L = |V o n | i=1 ŷi log(y i )+(1− ŷi ) log(1−y i ) , (2) where |V o n | is the output vocabulary size (2 for D 1 , 4 for D 2 , 6 for D 3 , 8 for D 4 ), ŷi ∈ {0, 1} and y i are the target and prediction for label i, respectively.",38,39
11876,222271899,"  In our sequence prediction task, the input vocabulary (V i n ) for a D n language consists of 2n+1 symbols: n pairs of brackets (or parentheses), and an additional starting symbol T whereas the output vocabulary (V o n ) does not include the starting symbol T. Since there might exist multiple possibilities for the next bracket in a sequence, we adopt a multi-label classification approach wherein the outputs are encoded as a k-hot vector and the network is optimized using the binary cross-entropy loss function given by L = |V o n | i=1 ŷi log(y i )+(1− ŷi ) log(1−y i ) , (2) where |V o n | is the output vocabulary size (2 for D 1 , 4 for D 2 , 6 for D 3 , 8 for D 4 ), ŷi ∈ {0, 1} and y i are the target and prediction for label i, respectively.",54,55
11877,222271899,"Unlike SA, the performance of LSTM degrades after the addition of the starting symbol, with the biggest drop (4.3%) on D 4 for sequence length of 102-106.",14,15
11878,222271899,"The starting symbol has enabled SA to attend to the correct preceding token, but it has been ineffective for LSTM.",2,3
11879,222271899,"Error Analysis We define failure position (f p ) as the position of the first symbol in the sequence where the model failed to correctly predict the next set of possible parentheses, For each symbol in a D n sequence: (i) depth (d p ) is the number of unmatched parenthesis up to and including that symbol, and (ii) distance to head (d h ) is the number of symbols between the mis-classified closing bracket and its opening counterpart.",16,17
11880,222271899,"Error Analysis We define failure position (f p ) as the position of the first symbol in the sequence where the model failed to correctly predict the next set of possible parentheses, For each symbol in a D n sequence: (i) depth (d p ) is the number of unmatched parenthesis up to and including that symbol, and (ii) distance to head (d h ) is the number of symbols between the mis-classified closing bracket and its opening counterpart.",36,37
11881,222271899,"Error Analysis We define failure position (f p ) as the position of the first symbol in the sequence where the model failed to correctly predict the next set of possible parentheses, For each symbol in a D n sequence: (i) depth (d p ) is the number of unmatched parenthesis up to and including that symbol, and (ii) distance to head (d h ) is the number of symbols between the mis-classified closing bracket and its opening counterpart.",62,63
11882,222271899,"For example, the symbol ""]"" in the string ""([])"", will first pop ""["" from the stack, then it attends to ""("", the last unmatched symbol, which will determine the next valid closing bracket.",4,5
11883,222271899,"For example, the symbol ""]"" in the string ""([])"", will first pop ""["" from the stack, then it attends to ""("", the last unmatched symbol, which will determine the next valid closing bracket.",39,40
11884,222271899,"If for every closing symbol in the sequence, the highest attention score of at least one of the heads points to the correct bracket, then we consider the SA compatible.",4,5
11885,222271899,"Furthermore, for a fair comparison between SA + and SA − , we do not push the starting symbol to the stack and only consider closing brackets which are not at the end of a clause.",19,20
11886,222271899,An important feature of the third head is that the last symbol attends to the starting symbol T. The starting symbol has enabled the model to learn the occurrence of the end of a clause and the end of the whole sequence.,11,12
11887,222271899,An important feature of the third head is that the last symbol attends to the starting symbol T. The starting symbol has enabled the model to learn the occurrence of the end of a clause and the end of the whole sequence.,16,17
11888,222271899,An important feature of the third head is that the last symbol attends to the starting symbol T. The starting symbol has enabled the model to learn the occurrence of the end of a clause and the end of the whole sequence.,20,21
11889,222271899,"We compare the performance of two SA networks, SA + and SA − , which differ only in the inclusion of a starting symbol in their vocabulary.",24,25
11890,222271899,We demonstrate that a simple addition of the starting symbol helps SA + generalize to sequences that are longer and have higher depths.,9,10
11891,5893757,The symbol • denotes the Hadamard products (element wise product) of two matrices.,1,2
11892,233219842,"We follow the convention popularized by BERT and assume the first token is a special symbol (i.e., [CLS]), so that E 1 (x) and F 1 (y) represent single-vector summaries of x and y. We have the following design choices: • Direction: If x → y, define the query Q = E(x) and key K = F (y).",15,16
11893,233219842,Architectures We represent x and y as length-128 wordpiece sequences where the leftmost token is the special symbol [CLS]; we mark the boundaries of a mention span in x with special symbols.,17,18
11894,233219842,"The reranker defines s θ (x, y) = w E 1 (x, y) + b where E(x, y) ∈ R H×256 is BERT (either base H = 768 or large H = 1024) embeddings of the concatenation of x and y separated by the special symbol [SEP], and w, b are parameters of a linear layer.",55,56
11895,222066642,"The first is to replace some tokens with the ""[MASK]"" symbol, BERT has to recover this masked token from outputs of the last layer.",14,15
11896,16020183,Schiller & Steffens (1989) use still another diacritic symbol for this task.,10,11
11897,247158700,The •X symbol is the dotted symbol or the next symbol for which the system has to generate the subtree.,2,3
11898,247158700,The •X symbol is the dotted symbol or the next symbol for which the system has to generate the subtree.,6,7
11899,247158700,The •X symbol is the dotted symbol or the next symbol for which the system has to generate the subtree.,10,11
11900,247158700,The Python ASDL grammar includes rules with star ( * ) qualifiers allowing zero or more occurrences of the starred symbol.,20,21
11901,247158700,Grammatical symbols can also be annotated with qualifiers ( * ) that allow for zero or more iterations of the symbol.,20,21
11902,247158700,"Then the probability of generating the symbol v is given by the marginal probability: p(y t = GENERATE[v]|x, e <t ) = p(gen|x, e <t )p(v|gen, x, e <t ) +p(copy|x, e <t )p(v|copy, x, e <t ) (6) The probabilities p(gen|.)",6,7
11903,247158700,"When the generator is in a state where the dot of the item on the top of the stack points on a nonterminal symbol, the PREDRULE is used.",23,24
11904,247158700,"This task either outputs a PREDICT(C) action or the CLOSE action: p(y t = PREDRULE[c]|x, e <t ) = softmax(e r • W p • a t ) (7) When the generator is in a state where the dot of the item on the top of the stack points on a terminal symbol, the generate task is used.",58,59
11905,241583659,"First, it relies heavily on a number of models (metaphor classification, FrameNet frame tagging, and COMET symbol extraction), each of which introduces error.",20,21
11906,247597105,"In this study, the token vocabulary V simply consists of integers (or integers with a special symbol) and is not intended to correspond to a vocabulary of any natural language.",18,19
11907,226284009,"A WFA with k states is defined as a tuple: A = α 0 , α ∞ , {A σ } σ∈Σ (1) where: α 0 , α ∞ ∈ R k are the initial and final weight vectors; and A σ ∈ k×k are the transition matrices associated to each symbol σ ∈ Σ. The function f A : Σ → R realized by a WA A is defined as: f (x) = α 0 A x 1 • • • A xn α ∞ . (",57,58
11908,18860664,iii) Levenshtein distance joins the canonical segments with a special symbol # into a single string and computes the Levenshtein distance between predicted and gold strings.,11,12
11909,728087,"We will use x i:j = x i x i+1 • • • x j to denote a sequence of symbols x t with i ≤ t ≤ j. A SHAG generates sentences s 0:N , where symbols s t ∈ X with 1 ≤ t ≤ N are regular words and s 0 = ∈ X is a special root symbol.",65,66
11910,728087,"As special cases, the LEFT sequence of the root symbol is always empty, while the RIGHT one consists of a single word corresponding to the head of the sentence.",10,11
11911,728087,"Consider an HMM with n hidden states and initial-state probabilities π ∈ R n , transition probabilities T ∈ R n×n , and observation probabilities O a ∈ R n×n for each a ∈ X , with the following meaning: • π(i) is the probability of starting at state i, • T (i, j) is the probability of transitioning from state j to state i, • O a is a diagonal matrix, such that O a (i, i) is the probability of generating symbol a from state i. Given an HMM, an equivalent operator model can be defined by setting α 1 = π, A a = T O a and α ∞ = 1.",96,97
11912,728087,"At each step in the chain of products (3), α t+1 = A xt α t updates the state distribution from positions t to t + 1 by applying the appropriate operator, i.e. by emitting symbol x t and transitioning to the new state distribution.",39,40
11913,728087,"Hence, A a (i, j) is the probability of generating symbol a and moving to state i given that we are at state j. HMM are only one example of distributions that can be parameterized by operator models.",14,15
11914,728087,"In general, operator models can parameterize any PNFA, where the parameters of the model correspond to probabilities of emitting a symbol from a state and moving to the next state.",22,23
11915,728087,"Finally, we define vectors p 1 ∈ R l and p ∞ ∈ R l as follows: p 1 (a) = s∈X * P(as), the probability that a string begins with a particular symbol; and p ∞ (a) = p∈X * P(pa), the probability that a string ends with a particular symbol.",39,40
11916,728087,"Finally, we define vectors p 1 ∈ R l and p ∞ ∈ R l as follows: p 1 (a) = s∈X * P(as), the probability that a string begins with a particular symbol; and p ∞ (a) = p∈X * P(pa), the probability that a string ends with a particular symbol.",62,63
11917,728087,We will denote as t a and w a the PoS tag and word of a symbol a ∈ X .,16,17
11918,239016040,"A dropout rate of 6.25% is applied both to the embeddings before being passed to the encoder, and to the hidden-state and start-symbol encodings input to the decoder (see Downey et al.,",28,29
11919,239016040,"A dropout rate of 6.25% is applied to the input embeddings, plus h and the start-symbol for the decoder.",19,20
11920,3545443,"The probability P(G D ) is itself an integral over the product of a deterministic transform φ from an unbounded grammar to a bounded grammar P(G D | G) = G D = φ(G) and a prior over unbounded grammars P(G): P(G D ) = P(G D | G) • P(G) • dG. ( 12 ) Distributions P(G) for each nonterminal symbol (rows) within this unbounded grammar can then be sampled from a Dirichlet distribution with a symmetric parameter β: G ∼ Dirichlet(β), (13) which then yields a corresponding transformed sample in P(G D ) for corresponding nonterminals.",67,68
11921,3545443,"2013) , this can be done by iteratively defining a side-and depthspecific containment likelihood h (i)  s,d for left-or rightside siblings s ∈ {L, R} at depth d ∈ {1..D} at each it-eration i ∈ {1..I}, 10 as a vector with one row for each nonterminal or terminal symbol (or null symbol ⊥) in G, containing the probability of each symbol generating a complete yield within depth d as an s-side sibling: h (0) s,d = 0 (14a) h (i) L,d =        G (1 ⊗ δ ⊥ + h (i−1) L,d ⊗ h (i−1) R,d ) if d ≤ D + 1 0 if d > D + 1 (14b) h (i) R,d =              δ T if d = 0 G (1 ⊗ δ ⊥ + h (i−1) L,d+1 ⊗ h (i−1) R,d ) if 0 < d ≤ D 0 if d > D. ( 14c ) where 'T' is a top-level category label at depth zero.",72,73
11922,3545443,"2013) , this can be done by iteratively defining a side-and depthspecific containment likelihood h (i)  s,d for left-or rightside siblings s ∈ {L, R} at depth d ∈ {1..D} at each it-eration i ∈ {1..I}, 10 as a vector with one row for each nonterminal or terminal symbol (or null symbol ⊥) in G, containing the probability of each symbol generating a complete yield within depth d as an s-side sibling: h (0) s,d = 0 (14a) h (i) L,d =        G (1 ⊗ δ ⊥ + h (i−1) L,d ⊗ h (i−1) R,d ) if d ≤ D + 1 0 if d > D + 1 (14b) h (i) R,d =              δ T if d = 0 G (1 ⊗ δ ⊥ + h (i−1) L,d+1 ⊗ h (i−1) R,d ) if 0 < d ≤ D 0 if d > D. ( 14c ) where 'T' is a top-level category label at depth zero.",76,77
11923,3545443,"2013) , this can be done by iteratively defining a side-and depthspecific containment likelihood h (i)  s,d for left-or rightside siblings s ∈ {L, R} at depth d ∈ {1..D} at each it-eration i ∈ {1..I}, 10 as a vector with one row for each nonterminal or terminal symbol (or null symbol ⊥) in G, containing the probability of each symbol generating a complete yield within depth d as an s-side sibling: h (0) s,d = 0 (14a) h (i) L,d =        G (1 ⊗ δ ⊥ + h (i−1) L,d ⊗ h (i−1) R,d ) if d ≤ D + 1 0 if d > D + 1 (14b) h (i) R,d =              δ T if d = 0 G (1 ⊗ δ ⊥ + h (i−1) L,d+1 ⊗ h (i−1) R,d ) if 0 < d ≤ D 0 if d > D. ( 14c ) where 'T' is a top-level category label at depth zero.",87,88
11924,3545443,"21) Finally, a lexical observation model L is defined as a matrix of unary rule probabilities with one row for each combination of store state and preterminal symbol and one column for each observation symbol: L = 1 ⊗ G (diag(1) ⊗ δ ⊥ ). (",29,30
11925,3545443,"21) Finally, a lexical observation model L is defined as a matrix of unary rule probabilities with one row for each combination of store state and preterminal symbol and one column for each observation symbol: L = 1 ⊗ G (diag(1) ⊗ δ ⊥ ). (",36,37
11926,46971301,"Then, we used the pyenchant 2 spell-correction library to correct each token if it was composed of letters, not digits, and not a currency symbol.",29,30
11927,5638176,"Different types of ambiguity have also been identified: subjective interpretations of symbols, as well as different renderings of what should be the same symbol (Miller et al.,",25,26
11928,248779880,Notations We use w ∈ R d to denote word vectors with dimension d. We overload the symbol w to represent a word in some contexts.,17,18
11929,15053998,"An additional label, KIND, was introduced for usages of the concept as a kind, where quantification does not apply (e.g. beaver symbol-of-Canada).",25,26
11930,7421176,"Since the continuous representation of a symbol (e.g., h j and s t ) encodes multiple meanings of a word, NMT models need to spend a substantial amount of their capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al.,",6,7
11931,235458643,"Therefore, we regard the probability distribution at the second [MASK] symbol as the prompt-only distribution.",13,14
11932,248780547,"Dimension 4096 in W F C2 and 1024 in W K , W Q , W V , W O , W F C1 are the symbol for the bias term.",26,27
11933,52933530,"Instead of working directly at the discrete symbol level as SMT, it projects and manipulates the source sequence of discrete symbols in a continuous vector space.",7,8
11934,239009583,We insert a special symbol as the output vector of [X_SEP] after paragraph p i .,4,5
11935,239009583,"As the decoder produces summaries for all sections in one pass, we add a special symbol [Y_SEP] between summaries from neighboring sections to indicate their boundaries.",16,17
11936,239009583,"After generating the first heading ending symbol [Y_SEP], the decoder changes the segmentation-aware attention to the third paragraph for generating the summary for the second section.",6,7
11937,17695712,"In general an HMM generates an observation sequence (output) by starting in one of the available states based on the initial probabilities, going from state to state based on the transition probabilities while emitting an output symbol in each state based on the emission probability of that output symbol in that state.",39,40
11938,17695712,"In general an HMM generates an observation sequence (output) by starting in one of the available states based on the initial probabilities, going from state to state based on the transition probabilities while emitting an output symbol in each state based on the emission probability of that output symbol in that state.",51,52
11939,17695712,The only difference between the PairHMM and the HMM is that it outputs a pair of symbols instead of only one symbol.,21,22
11940,17695712,"In the substitution state two symbols are emitted, while in the other two states a gap and a symbol are emitted, corresponding with a deletion and an insertion, respectively.",19,20
11941,17695712,Also the probability of substituting a phonetic symbol with itself (i.e. identity substitution) should be higher than the probability of a substitution with any other phonetic symbol.,7,8
11942,17695712,Also the probability of substituting a phonetic symbol with itself (i.e. identity substitution) should be higher than the probability of a substitution with any other phonetic symbol.,28,29
11943,17695712,In these maps phonetically close dialectal varieties are marked with the same symbol.,12,13
11944,17205799,"In Table I , 'A' stands for a given key, 'B' stands for a sequence of kanji characters (we only treat kanjicompound nouns in this paper), and 'D' stands for an ""extended"" delimiter: D is identical to a space, a symbol, a katakana or a hiragana except ""©"" (no; o3').",54,55
11945,744802,Each lexical terminal is denoted by an atomic formula with lexical category as its predicate symbol. (,15,16
11946,744802,Each phrasal non-terminal is represented by an atomic formula with phrasal category as its predicate symbol. (,17,18
11947,744802,"A botmding non-terminal is a phrasal non-terminal with bounding node as its predicate symbol. (,",17,18
11948,7968497,This is done by introducing an auxiliary symbol such that: i(π) = i(α).,7,8
11949,7968497,"In this figure, ""eps"" stands for epsilon or null symbol.",12,13
11950,7968497,"The set of transitions of D is defined as E = E 0 {(f, #, x, 0, w)} where f is the unique final state of D, 0 is the unique initial state of D, x is any symbol and # is a symbol representing the end of a word.",48,49
11951,7968497,"The set of transitions of D is defined as E = E 0 {(f, #, x, 0, w)} where f is the unique final state of D, 0 is the unique initial state of D, x is any symbol and # is a symbol representing the end of a word.",53,54
11952,9468905,"NOTATIONAL PRELIMINARIES We assume an alphabet L, with a reserved symbol 0 ~ £ for insertions and deletions.",11,12
11953,9468905,"3.1) Moreover, the underlying symbol of the replacement (namely, +) matches the argument of the ~ule's replacement function.",6,7
11954,9468905,"But the rule is not satisfied, because (3.2) the surface symbol of the replacement (namely, 0) does not match the value of the rule's replacement function (namely, e): thus, (+,0) ~[ {(+,e)}.",13,14
11955,9468905,"Given an input formula, w, we construct a DFSM consisting of one state over an alphabet consisting of 0, 1, ~, one symbol, u~, for each variable in w, and one symbol, ej, for each conjunct in w. Let m be the number of variables in w, and n, the number of conjuncts.",27,28
11956,9468905,"Given an input formula, w, we construct a DFSM consisting of one state over an alphabet consisting of 0, 1, ~, one symbol, u~, for each variable in w, and one symbol, ej, for each conjunct in w. Let m be the number of variables in w, and n, the number of conjuncts.",39,40
11957,9468905,"The loops enforce a value of 1 on the symbol uj~ if lj~ is a positive literal, or 0, if it is negative.",9,10
11958,9468905,"The DFSM first copies each surface input symbol to the corresponding position in the underlying form, and then adds the pair $:0, completing the task in a state So.",7,8
11959,9468905,"Each rewriting will be separated by a $ symbol, and, as the string length changes, it will be padded by !",8,9
11960,9468905,"Finally, we add one arc to the DFSM from So to the only final .state which checks that the final copy of the string contains only the distinguished symbol, S: $ -* 0 / ( h£ ...~-I h£) S:£ $:£__ Thus, the DFSM recognises the surface form if and only if there is a series of rewritings from the input string to S using the rules of the grammar, and the translation is linear in the size of the input string times the number of rules.",29,30
11961,3561638,"For example, the grammar X → Y h Y ∪ YY h could mark with an h either the first or second symbol of YY.",23,24
11962,3561638,"But this creates a third and final problem: in the grammar X → (YY h ∪ Y h )(YY ∪ Y), it is not defined which symbol of YYY should be marked, that is, which union operator takes priority over the other.",32,33
11963,3561638,"1993) , in which a nonterminal symbol is a category together with zero or more functional tags, we adopt the following scheme: the atomic pattern a matches any label with category a or functional tag a; moreover, we define Boolean operators ∧, ∨, and ¬. Thus NP ∧ ¬ADV matches NP-SBJ but not NP-ADV.",7,8
11964,8979459,It is exactly equivalent to the IPA symbol /j/more commonly used for the same feature specification.,7,8
11965,8979459,"In this case, as the lowest level word boundaries involved only monomorphemic definite and indefinite articles, it was derided to treat these as an additional word-internal level and reserve the word boundary symbol to designate phrase boundaries and higher level constituent boundaries.",36,37
11966,8979459,"Thus, most word internal affixes were designated type 1, inflectional affixes were designated type 2, definite and indefinite articles were designated type 3 and phrase boundaries were identified by the word boundary symbol.",35,36
11967,248834170,"A bi-text word alignment is a set of bisymbols A, where each bi-symbol (x i , y j ) couples a word x i in the input sequence at position i to a word y j in the target sequence at position j. If a word x i from the input sequence does not need an alignment to a word in the target, we introduce an ε in y at position i. This bi-symbol (x i , ε i ) amounts to a deletion, i.e. mapping from input to target involves deleting a word from the input.",17,18
11968,248834170,"A bi-text word alignment is a set of bisymbols A, where each bi-symbol (x i , y j ) couples a word x i in the input sequence at position i to a word y j in the target sequence at position j. If a word x i from the input sequence does not need an alignment to a word in the target, we introduce an ε in y at position i. This bi-symbol (x i , ε i ) amounts to a deletion, i.e. mapping from input to target involves deleting a word from the input.",82,83
11969,248834170,"Conversely, if a word y j from the target does not require an alignment to a word in the input, we introduce an ε in x at position j. This bi-symbol (ε j , y j ) amounts to an insertion, i.e. mapping from input to target involves inserting an extra word in the target.",34,35
11970,3241321,"Example of translation based HMM In such framework, we can asssume that a lexical rule corresponds to a hidden state and a substring in the input sentence as an observed symbol procduced form the state and the problem of translation is equivalent to find a lexical rule for each variable.",31,32
11971,3241321,"After obtaining a sequence of lexical rules, the sequence of observed symbols is generated because each observed symbol is a left hand side of a lexical rule.",18,19
11972,37336,"To allow titles being generated from the 'distilled information source' instead of the original document, we can expand the probability P(T|D) as the sum of the probabilities P(T| 'information source' S) over all the possible 'information sources' S, where probability P(T|S) stands for the probability of using the word sequence T as the title for the 'information source' S. Formally, this idea can be expressed as: ∑ = S D S P S T P D T P ) | ( ) | ( ) | ( (4) where symbol S stands for a possible 'information source' S for the document D. In Equation ( 4 ), term P(T|S)P(S|D) represents the idea of two noisy channels, with term P(S|D) corresponding to the first channel that samples 'information source' S out of the original document D and term P(T|S) corresponding to the second noisy channel that creates title T from the 'distilled information source' S. Since the first noisy channel, i.e. P(S|D), is new to the old framework for title generation, we will focus on the discussion of the noisy channel P(S|D).",106,107
11973,1548313,The Figure 2 explains the equivalent of our grammar symbol with English grammar symbol.,9,10
11974,1548313,The Figure 2 explains the equivalent of our grammar symbol with English grammar symbol.,13,14
11975,1548313,Each English word entry includes several meanings in Vietnamese and each meaning was associated with a symbol meaning.,16,17
11976,1548313,"The set of symbol meanings in each word entry is defined by using WordNet database.(C. Fellbaum, 1998) The dictionary also contained several phrases, expressions in Figure 1 : An example of syntax tree of ""Like FaceLift, much of ATM's screen performance depends on the underlying application"" English and its equivalent to Vietnamese.",3,4
11977,1548313,3) Syntax to combine sequence meanings into one symbol meaning (this process called a inherit process from the node N to its children).,9,10
11978,1548313,"Thus, semantic parsing using syntax control problem can be described mathematically as follows: Given a sequence of children nodes n 1 , n 2 , ..., n k of a node N , each node n i consist of a list of meaning, in which each meaning was associated with a symbol meaning.",55,56
11979,1548313,Ensure: a syntax tree with rich semantic information {SemanticParsingTree} 1: if N is leaf then 2: Update all symbol-meaning in word entry 3: else 4: FindRules(N); 5: end if{main procedure} 6: SemanticParsingNode(root); Generation reduced sentences The input of this process is a syntax tree which associated with rich information after applying the semantic parsing process.,23,24
11980,14834950,"These operators are formulated as REDUCE (k, x ) , in which k is an integer and X is a grammar symbol. •",23,24
11981,14834950,They were written as DROP x with X is a grammar symbol. •,11,12
11982,14834950,The processes repeat until the Input list is empty and there is only one sub tree in CSTACK with its root node is the one of terminal symbols (the symbol to recognize it as a root symbol) or RSTACK is empty.,30,31
11983,14834950,The processes repeat until the Input list is empty and there is only one sub tree in CSTACK with its root node is the one of terminal symbols (the symbol to recognize it as a root symbol) or RSTACK is empty.,37,38
11984,2245732,"REDUCE(lk, X) action pops the lk syntactic trees located at the top of CSTACK and combines them in a new tree, where lk is an integer and X is a grammar symbol. •",34,35
11985,6084883,"2) Use of symbol/numeral correspondences: In the present implementation, the correspondences of symbols and numerals are not used in calculating the correlation because the bilingual dictionary does not contain them.",4,5
11986,2845357,"Lists of segments can be combined by the append operation, represented by the symbol "" ⊕"".",14,15
11987,16777765,"In preprocessing, all the names of the same type are translated into one symbol (a special word).",14,15
11988,763228,"Collapsing the representation of syllables into a single symbol a for convenience, table (11) gives the assignment of stress to a number of partially specified representations.",8,9
11989,8156597,"The grammar is restricted to binary rules, which can have the symbols in the right hand side appear in the same order in both languages, represented with square brackets: X → [Y Z] or the symbols may appear in reverse order in the two languages, indicated by angle brackets: X → Y Z Individual lexical translations between English words e and French words f take place at the leaves of the tree, generated by grammar rules with a single right hand side symbol in each language: X → e/f Given a bilingual sentence pair, a synchronous parse can be built using a two-dimensional extension of chart parsing, where chart items are indexed by their nonterminal Y and beginning and ending positions l, m in the source language string, and beginning and ending positions i, j in the target language string.",90,91
11990,8156597,"In our experiments we use a grammar with a start symbol S, a single preterminal C, and two nonterminals A and B used to ensure that only one parse can generate any given word-level alignment (ignoring insertions and deletions) (Wu, 1997; Zens and Ney, 2003) .",10,11
11991,8156597,"The French word is predicted conditioned only on the English word, and each English word can generate at most one French word, or can generate a NULL symbol, representing deletion.",29,30
11992,232021999,"Atomic number 82, symbol Pb (from Latin plumbum)."")",4,5
11993,15028284,"We use the symbol ""-"" to annotate such cases.",3,4
11994,14299491,"Because the program is a tool to validate Thai word segmentation, the input files must be word-separated by pipe symbol ""|"", as shown in Figure 2 .",22,23
11995,14299491,"Firstly, the symbol <QUESTION>...</QUES-TION> is used to mark any ambiguous words or texts which have various meanings or are still in discussion.",3,4
11996,2629900,"A finite constructor tree t is a finite tree in which each node is labelled with a symbol of Σ, and the number of children of the node is exactly the arity of this symbol.",17,18
11997,2629900,"A finite constructor tree t is a finite tree in which each node is labelled with a symbol of Σ, and the number of children of the node is exactly the arity of this symbol.",35,36
11998,2629900,"We write T (Σ) for the finite constructor trees over Σ. A regular tree grammar (RTG) is a 4-tuple G = (S, N, Σ, R) consisting of a nonterminal alphabet N, a terminal alphabet Σ, a start symbol S ∈ N, and a finite set of production rules R of the form A → β , where A ∈ N and β ∈ T (Σ ∪ N); the nonterminals count as zero-place constructors.",50,51
11999,2629900,"Two finite constructor trees t,t ∈ T (Σ ∪ N) stand in the derivation relation, t → G t , if t can be built from t by replacing an occurrence of some nonterminal A by the tree on the right-hand side of some production for A. The language generated by G, L(G), is the set {t ∈ T (Σ) | S → * G t}, i.e. all terms of terminal symbols that can be derived from the start symbol by a sequence of rule applications.",94,95
12000,2629900,"1 can be represented as the tree language accepted by the following grammar with start symbol S. S → a x (A 1 , A 2 ) | a z (B 1 , A 3 ) | every y (B 3 , A 4 ) A 1 → a z (B 1 , B 2 ) A 2 → every y (B 3 , B 4 ) A 3 → a x (B 2 , A 2 ) | every y (B 3 , A 5 ) A 4 → a x (A 1 , B 4 ) | a z (B 1 , A 5 ) A 5 → a x (B 2 , B 4 ) B 1 → comp z B 2 → repr-of x,z B 3 → sample y B 4 → see x,y More generally, every finite set of trees can be written as the tree language accepted by a nonrecursive regular tree grammar such as this.",15,16
12001,2629900,"1, 2, 3, 4, 5, 6, 7} S → 1({2, 4, 5} S , {3, 6, 7} S ) {1, 2, 3, 4, 5, 6, 7} S → 2({4} S , {1, 3, 5, 6, 7} Q 1 ) {1, 2, 3, 4, 5, 6, 7} S → 3({6} S , {1, 2, 4, 5, 7} S ) {1, 3, 5, 6, 7} Q 1 → 3({6} S , {1, 5, 7} S ) {1, 2, 4, 5, 7} S → 1({2, 4, 5} S , {7} S ) {1, 2, 4, 5, 7} S → 2({4} S , {1, 5, 7} Q 1 ) {2, 4, 5} S → 2({4} S , {5} Q 1 ) {4} S → 4 {3, 6, 7} S → 3({6} S , {7} S ) {5} S → 5 {1, 5, 7} S → 1({5} S , {7} S ) {5} Q 1 → 5 {6} S → 6 {7} S → 7 Significantly, the grammar contains no productions for {1, 3, 5, 6, 7} Q 1 with terminal symbol 1, and no production for {1, 5, 7} Q 1 .",291,292
12002,6010484,"An atom is a predicate symbol applied to a list of arguments, which may be variables or constants (e.g. ).",5,6
12003,8869777,"The root of the hypergraph is a special node S ′ [0, |w|, s , /s ] which means the entire input sentence has been translated to a string starting with the beginning-of-sentence symbol and ending at the end-of-sentence symbol.",40,41
12004,8869777,"The root of the hypergraph is a special node S ′ [0, |w|, s , /s ] which means the entire input sentence has been translated to a string starting with the beginning-of-sentence symbol and ending at the end-of-sentence symbol.",50,51
12005,8869777,"2007) also take a two-pass decoding approach, with the first pass leaving the language model boundary words out of the dynamic programming state, such that only one hypothesis is retained for each span and grammar symbol.",40,41
12006,11720812,"In general, the set T Is composed a priori of all possible transitions between states in S producing a symbol in A. The determination of probabilities p associated with these transitions Is equivalent to the restriction of T to elements with non null probability which induces the structure of the associated automaton.",20,21
12007,8085311,"Now we introduce the symbol { } M K, which refers to the final context cluster configuration, where K refers to the number of distinct sense, and M represents the many-to-one mapping (from contexts to a sense) such that ( ) K]. [",4,5
12008,2101297,We handle out-of-vocabulary events by replacing words or context-values that occur only once during training with a special unknown symbol.,25,26
12009,989544,"For notational convenience, we include in the vocabulary a special ""begin-of-document"" symbol d which appears only at the beginning of each document.",18,19
12010,6644729,"A basic (bidirectional) categorial grammar B is defined by a set of categories C := C0U{zl ~ = x/Y or z =x\y; x, yC C} (C0 a finite set of basic categories, the category y is referred to as the argument category, the category x is called value category, complex categories are named functor categories), a goal category g (the start symbol) which is a basic category, a lexicon L which is a function from a finite set of lexemes onto a set of finite sets of categories, and the two combination rules ""leftward application"" (app\) and ""rightward application"" (app/) which state how argument positions are filled: (app\) y, xky ~ x An object U --* x where U is a sequence of categories is called a sequent.",77,78
12011,3083998,"These tables are represented as matrices (see section 2.1.2): each row corresponds to a lexical item of the corresponding class; each column lists all features that may be valid or not for the different members of the class; at the intersection of a row and a column, the symbol + (resp. -)",53,54
12012,15119277,"It is marked with the symbol ""*"".",5,6
12013,15119277,"The correct preposition is described after the symbol ""→"" as in ""→ Prep at"".",7,8
12014,14096356,"We can achieve this by adding a special symbol such as φ to the beginning of each NP whose head noun is a common noun and that has no determiner in it as in ""I like φ orange juice.""",8,9
12015,10557542,"N -gram models describe probability distributions over all strings on the basis of the Markov assumption (Markov, 1913) : that the probability of the next symbol only depends on the previous contiguous sequence of length n − 1.",28,29
12016,10557542,"In contrast with the Markov assumption, our assumption is that the probability of the next symbol is conditioned on the previous set of discontiguous subsequences of length k − 1 in the string.",16,17
12017,10557542,"As a result, SP distributions are efficiently computable even though they condition the probability of the next symbol on the occurrences of earlier (possibly very distant) discontiguous subsequences.",18,19
12018,2489047,"As we can see, the schema for Lombardo and Lesmo's parser resembles the Earley-style parser in Sikkel (1997) , with some changes to adapt it to dependency grammar (for example, the Scanner always moves the dot over the head symbol * ).",47,48
12019,12427622,"The most obvious alternative to parsing with a context-free backbone is using graph unification, rather than atomic symbol identity, to drive the parsing process.",20,21
12020,18089228,"For example, let ha k 1 1 1 a 2 a 1 ia 0 represent a tree consisting of the root labeled with a 0 and k child nodes labeled with a k ; 1 1 1 ; a 2 , a n d a 1 , so the right-most node at the bottom of the ACT in Figure 3 has a probability distribution of the symbol x under the condition that the history matches the partial parse trees hhz?iaihbi, where \?""",69,70
12021,18089228,matches with an arbitrary symbol.,4,5
12022,10409467,"For the Latin script, we simply treat every letter as an IPA symbol (International Phonetic Association, 1999) .",13,14
12023,34312692,"The terms of each sentence are matched with the existing wordnet MWEs lexicon and if an MWE is found it is rewritten to a single token, with spaces replaced by an underbar ""_"" symbol.",36,37
12024,232021844,"We decided to use only the elements corresponding to levels 1, 2 and 5, i.e., sections, divisions, and subclasses; • element (""element"") -this is the basic element grouping the descriptions of respective sections, divisions, and subclasses; • nazwa (""name""), symbol (""symbol"")these two sub-elements of the element tag uniquely identify the members of the PKD 2007 hierarchy and can be also used for tracking the relationships between the different levels of the hierachy; • opisObejmujeNieobejmuje (""description includes/excludes"") -this element is the most crucial from the perspective of the keywords database construction.",57,58
12025,232021844,"On level 1, we can find a section with symbol A and name Rolnictwo, leśnictwo, łowiectwo i rybactwo (""Agriculture, forestry and fishing"").",10,11
12026,232021844,"On level 2, we can find a division with symbol 01 and name Uprawy rolne, chów i hodowla zwierz ąt, łowiectwo, wł ączaj ąc działalność usługow ą (""Crop and animal production, hunting and related service activities"").",10,11
12027,232021844,"Finally, on level 5, we can find a subclass with symbol 01.41.Z and name Chów i hodowla bydła mlecznego (""Raising of dairy cattle"").",12,13
12028,232021844,"Since the keywords database aims to support the assignment of companies to sections only, we used the name and symbol elements to combine the descriptions of divisions and subclasses with the description of the section.",20,21
12029,14598745,"A synchronous CFG consists of rewrite rules such as the following: X → γ, α (1) where X is a non-terminal symbol, γ (α) is a string of terminal and non-terminal symbols in the source (target) language, and there is a one-to-one correspondence between the non-terminals in γ and α indicated by co-indexation.",27,28
12030,14598745,"When the rule is extracted from the parallel corpus, it has these alignments between the words of its Chinese and English portion: {c 1 -e 3 ,c 2 -e 4 ,c 3 -e 1 ,c 3 -e 2 ,c 3 -e 5 }, which means that c 1 is aligned to e 3 , c 2 is aligned to Input: rule R considered during decoding with its own associated costR Lc = list of symbols in Chinese portion of R WSDcost = 0 i = 1 while i ≤ len(Lc): ci = ith symbol in Lc if ci is a Chinese word (i.e., not a non-terminal symbol): seenChunk = ∅ // seenChunk is a global variable and is passed by reference to matchWSD if (ci is not the last symbol in Lc) and (ci+1 is a terminal symbol): then ci+1=(i+1)th symbol in Lc, else ci+1 = NULL if (ci+1!=NULL) and (ci, ci+1) as a single unit has WSD translations: W SDc = set of WSD translations for (ci, ci+1) as a single unit with context-dependent probabilities WSDcost = WSDcost + matchWSD(ci, W SDc, seenChunk) WSDcost = WSDcost + matchWSD(ci+1, W SDc, seenChunk) i = i + 1 else: W SDc = set of WSD translations for ci with context-dependent probabilities WSDcost = WSDcost + matchWSD(ci, W SDc, seenChunk)  e 4 , and c 3 is aligned to e 1 , e 2 , and e 5 .",103,104
12031,14598745,"When the rule is extracted from the parallel corpus, it has these alignments between the words of its Chinese and English portion: {c 1 -e 3 ,c 2 -e 4 ,c 3 -e 1 ,c 3 -e 2 ,c 3 -e 5 }, which means that c 1 is aligned to e 3 , c 2 is aligned to Input: rule R considered during decoding with its own associated costR Lc = list of symbols in Chinese portion of R WSDcost = 0 i = 1 while i ≤ len(Lc): ci = ith symbol in Lc if ci is a Chinese word (i.e., not a non-terminal symbol): seenChunk = ∅ // seenChunk is a global variable and is passed by reference to matchWSD if (ci is not the last symbol in Lc) and (ci+1 is a terminal symbol): then ci+1=(i+1)th symbol in Lc, else ci+1 = NULL if (ci+1!=NULL) and (ci, ci+1) as a single unit has WSD translations: W SDc = set of WSD translations for (ci, ci+1) as a single unit with context-dependent probabilities WSDcost = WSDcost + matchWSD(ci, W SDc, seenChunk) WSDcost = WSDcost + matchWSD(ci+1, W SDc, seenChunk) i = i + 1 else: W SDc = set of WSD translations for ci with context-dependent probabilities WSDcost = WSDcost + matchWSD(ci, W SDc, seenChunk)  e 4 , and c 3 is aligned to e 1 , e 2 , and e 5 .",120,121
12032,14598745,"When the rule is extracted from the parallel corpus, it has these alignments between the words of its Chinese and English portion: {c 1 -e 3 ,c 2 -e 4 ,c 3 -e 1 ,c 3 -e 2 ,c 3 -e 5 }, which means that c 1 is aligned to e 3 , c 2 is aligned to Input: rule R considered during decoding with its own associated costR Lc = list of symbols in Chinese portion of R WSDcost = 0 i = 1 while i ≤ len(Lc): ci = ith symbol in Lc if ci is a Chinese word (i.e., not a non-terminal symbol): seenChunk = ∅ // seenChunk is a global variable and is passed by reference to matchWSD if (ci is not the last symbol in Lc) and (ci+1 is a terminal symbol): then ci+1=(i+1)th symbol in Lc, else ci+1 = NULL if (ci+1!=NULL) and (ci, ci+1) as a single unit has WSD translations: W SDc = set of WSD translations for (ci, ci+1) as a single unit with context-dependent probabilities WSDcost = WSDcost + matchWSD(ci, W SDc, seenChunk) WSDcost = WSDcost + matchWSD(ci+1, W SDc, seenChunk) i = i + 1 else: W SDc = set of WSD translations for ci with context-dependent probabilities WSDcost = WSDcost + matchWSD(ci, W SDc, seenChunk)  e 4 , and c 3 is aligned to e 1 , e 2 , and e 5 .",145,146
12033,14598745,"When the rule is extracted from the parallel corpus, it has these alignments between the words of its Chinese and English portion: {c 1 -e 3 ,c 2 -e 4 ,c 3 -e 1 ,c 3 -e 2 ,c 3 -e 5 }, which means that c 1 is aligned to e 3 , c 2 is aligned to Input: rule R considered during decoding with its own associated costR Lc = list of symbols in Chinese portion of R WSDcost = 0 i = 1 while i ≤ len(Lc): ci = ith symbol in Lc if ci is a Chinese word (i.e., not a non-terminal symbol): seenChunk = ∅ // seenChunk is a global variable and is passed by reference to matchWSD if (ci is not the last symbol in Lc) and (ci+1 is a terminal symbol): then ci+1=(i+1)th symbol in Lc, else ci+1 = NULL if (ci+1!=NULL) and (ci, ci+1) as a single unit has WSD translations: W SDc = set of WSD translations for (ci, ci+1) as a single unit with context-dependent probabilities WSDcost = WSDcost + matchWSD(ci, W SDc, seenChunk) WSDcost = WSDcost + matchWSD(ci+1, W SDc, seenChunk) i = i + 1 else: W SDc = set of WSD translations for ci with context-dependent probabilities WSDcost = WSDcost + matchWSD(ci, W SDc, seenChunk)  e 4 , and c 3 is aligned to e 1 , e 2 , and e 5 .",155,156
12034,14598745,"When the rule is extracted from the parallel corpus, it has these alignments between the words of its Chinese and English portion: {c 1 -e 3 ,c 2 -e 4 ,c 3 -e 1 ,c 3 -e 2 ,c 3 -e 5 }, which means that c 1 is aligned to e 3 , c 2 is aligned to Input: rule R considered during decoding with its own associated costR Lc = list of symbols in Chinese portion of R WSDcost = 0 i = 1 while i ≤ len(Lc): ci = ith symbol in Lc if ci is a Chinese word (i.e., not a non-terminal symbol): seenChunk = ∅ // seenChunk is a global variable and is passed by reference to matchWSD if (ci is not the last symbol in Lc) and (ci+1 is a terminal symbol): then ci+1=(i+1)th symbol in Lc, else ci+1 = NULL if (ci+1!=NULL) and (ci, ci+1) as a single unit has WSD translations: W SDc = set of WSD translations for (ci, ci+1) as a single unit with context-dependent probabilities WSDcost = WSDcost + matchWSD(ci, W SDc, seenChunk) WSDcost = WSDcost + matchWSD(ci+1, W SDc, seenChunk) i = i + 1 else: W SDc = set of WSD translations for ci with context-dependent probabilities WSDcost = WSDcost + matchWSD(ci, W SDc, seenChunk)  e 4 , and c 3 is aligned to e 1 , e 2 , and e 5 .",159,160
12035,9390274,"Unfortunately, IBAL's algorithm appears not to terminate if the PCFG contains any kind of recursion reachable from the start symbol.",21,22
12036,10372442,"In LWFGs, each nonterminal symbol is a lefthand side in at least one ordered non-recursive rule and the empty string cannot be derived from any nonterminal symbol.",5,6
12037,10372442,"In LWFGs, each nonterminal symbol is a lefthand side in at least one ordered non-recursive rule and the empty string cannot be derived from any nonterminal symbol.",30,31
12038,10372442,"92 #¤ §¦ is the start nonterminal symbol, and @ A2 B¤ C¦ U £ 79 (we use the same notation for the reflexive, transitive closure of ¨).",9,10
12039,10372442,"3 The language of a grammar is the set of all syntagmas generated from the start symbol , i.e., w e f £ x d y d £ e U ¡ f U 2 ¡ 4 U £ D ¦ E d .",16,17
12040,3071447,"s i in Figure 3 is the i th symbol in the source language string S while t j is the j th symbol in T. Figure 3 : Pair Hidden Markov Model [Adapted from Mackay and Kondrak, 2005] Pair HMM Emission parameters are stored in matrix form in three tables associated with the edit operations; transition parameters are also stored in matrix form in a table.",9,10
12041,3071447,"s i in Figure 3 is the i th symbol in the source language string S while t j is the j th symbol in T. Figure 3 : Pair Hidden Markov Model [Adapted from Mackay and Kondrak, 2005] Pair HMM Emission parameters are stored in matrix form in three tables associated with the edit operations; transition parameters are also stored in matrix form in a table.",23,24
12042,3071447,"The main difference is in the structure; for the pair-HMM, the state transition parameter is also incorporated into the weight that measures the level of relationship between the input and output symbol when transformed to a WFST arc.",35,36
12043,3071447,"In Figure 4 , e is an empty symbol while s i and s j are as defined for the pair HMM in Figure 3 .",8,9
12044,745280,"Table 4 shows the simple predicates and the corresponding yield of the local winners in English, German, and The symbol marks the insertion site for the other SR.",21,22
12045,6795973,"Normally, extra affixes are composed of one or more copies of a unique special symbol, such as space, that does not belong to the string alphabet.",15,16
12046,6795973,We define an alphabet of special symbols that contains a unique symbol for each of the symbols in the original string alphabet.,11,12
12047,6795973,The extra affixes are assumed to contain copies of special symbols that correspond to the initial symbol of the string.,16,17
12048,2382426,The internal code table represents mappings from each phonetic symbol to a single character within ASCII code table.,9,10
12049,586636,"Finally, we assume that words are explicitly separated in the text, either by white space or a special symbol.",20,21
12050,586636,"The set of edit operations includes character substitutions, insertions, and deletions, as well as a special end symbol: {(u, h), (ϵ, h), (u, ϵ), EN D} (where u and h range over characters in the lost and known languages, respectively).",20,21
12051,10639238,"3, we only sum the posterior probabilities of base-phrase constituents, and not the ROOT symbol or POS tags.",18,19
12052,202539082,"In the same figure, the mappings of the synsets are also provided: machine 1 n and machine 1 v are connected to Machine c = and Making c +, where the symbol '=' means that machine 1 n is semantically equivalent to the Machine c , while '+' means that the semantics of Making c is more general than the semantics of machine 1 v .",34,35
12053,6610908,"For example, consider the chart in Figure 1 for the five symbol string ""abcde"".",12,13
12054,6610908,"Each cell in the chart is labeled with the substring that the cell spans, along with the begin and end indices of the substring, e.g., (3, 5) spans the third symbol to the fifth symbol: cde.",36,37
12055,6610908,"Each cell in the chart is labeled with the substring that the cell spans, along with the begin and end indices of the substring, e.g., (3, 5) spans the third symbol to the fifth symbol: cde.",40,41
12056,204921777,"We represent them with the notation α|∅ → α ′ |∅, were the symbol ∅ represents the absence of a phoneme.",14,15
12057,204921777,"To be more explicit, we treated rules of the type α|β → γ as if they could be read as α|* → γ or *|β → γ, where the wildcard symbol * stands for an arbitrary sequence of zero or more phonemes; similarly, we treated rules of the type α|β → α ′ |β ′ as if they could be read as α|* → α ′ |* or *|β → *|β ′ ; and we treated rules of the form α|∅ → α ′ |∅ as α|* → α ′ |*.",34,35
12058,10670320,"In reading text ean be selected according to the wishes or needs of the reader (to allow ""dynamic reading""; a display screen can be regarded as a dynamic blackboard): l) for the novice and the expert, different parts may be left out or included 2) to get a global overview, we can generate a table of contents at arbitrary levels of abstraction 5) information can be reordered such that all oecurences of a certain concept are selected (which occur in other representations at arbitrary places) Similar possibilities exist representation of programs: for the i) certain modules of the program can be listed selectively (eg all the data accessing functions, all declarative information, all procedures which achieve a specific subtask); procedures can be listed in different orders (eg alphabetically or according to U~ calling structure) 2) the calling structure which Shows the connectivity structure between different procedures can be displayed at arbitrary levels of detail; the user should be allowed to define a ""view specification"" 3) symbol tables give a receding of information according to a different criterion to have imagined the final result before the first step had been taken (for a general discussion of these issues see FISCHER, BROWN and BUR[ON, 1978) .",191,192
12059,17035653,1997) Language models Mapping all filled pauses to a unique symbol FP we compare three LM approaches: 1.,11,12
12060,10313983,The bilanguage string consists of source-target symbol pair sequences as shown in Equation 3 .,8,9
12061,52202785,"For example, in this sentence: ""Flags were hoisted as symbol of lament"", the concept of ""flag"" will have role of ""patient"" for the concept of ""to hoist"".",12,13
12062,737345,An SFA begins operation with those start states active that are compatible with the first symbol in the input string.,15,16
12063,737345,"Then Ix, T, y I }-""A IX'~ T', y'I iff there is a The first condition in the definition concerns or, the tape symbol being scanned.",29,30
12064,737345,This symbol is the first in the string y and the last in the string x'.,1,2
12065,737345,"In order to signify that an SFA accepts the empty string A (i.e., if e is true), we shall include a special state, labeled with a distinguished symbol 0, which is marked both initial and final.",32,33
12066,737345,"As an illustration, we shall consider the automaton that prohibits two adjacent occurrences of any given symbol in a string, a constraint known in autosegmental phonology as the Obligatory Contour Principle.",17,18
12067,737345,"Because states carry labels, there is no need to use the contentless bullet symbol to mark a state.",14,15
12068,737345,"The SFA that does not admit adjacent occurrences of any symbol in the alphabet is: Notice that the number of labels required by the SFA is considerably smaller (5 labels) than the number required in the FSA (12 labels), while the number of transitions is the same for both.",10,11
12069,737345,"In general, when we employ a symbol like b it will be interpreted as a simple segment.",7,8
12070,737345,"Braces around a frame component behave in the same way, but only if they are scanning the final symbol on a tape. (",19,20
12071,737345,"Finally, the symbol G is used instead of C for geminate consonants.",3,4
12072,737345,t b k V G C V C C [] k The first CV tape symbol is a C and so the current consonant k is written onto the surface tape and the read heads are advanced.,16,17
12073,737345,"k b II C Va~ a G C V C V {a} k a In ( 31 ), the CV tape symbol is a V, and so the a is copied to the surface tape.",24,25
12074,737345,"However, the G symbol requires that the head stays put.",4,5
12075,737345,"Two conservative extensions to the model that encompass this infixation are (i) to insert infixes into the CV tape directly (so form VIII is CtVCVC) or (ii) to introduce another CV tape symbol A (for affix), which directs the transducer to read from a fifth tape.",38,39
12076,5737214,"An SCFG rule has the following form: , ,γ α → X where X is a non-terminal symbol shared by all the rules; each rule has at most two nonterminals.",21,22
12077,14469542,This can be guaranteed in practice by annotating each terminal symbol with its position in the input.,10,11
12078,1795418,"Let the PCFG have nonterminal set N, start symbol S ∈ N, terminal alphabet Σ, and rules of the form A → B C and A → x. (We assume Chomsky normal form for clarity; the generalization is straightforward.)",9,10
12079,1795418,"We replaced the first occurrence of every tag and of every word in the training data with an OOV symbol, giving a fixed tag vocabulary of 46 and a fixed word vocabulary of 9,014.",19,20
12080,15619421,The grammar's start symbol is S φn .,4,5
12081,8481093,"This means, the start symbol is the root node, all inner nodes are nonterminals and all leaves are terminals (e.g., in Figure 1 tree ~).",5,6
12082,1878772,"Background A linear context-free rewriting system (LCFRS) is defined as a tuple G = (V N , V T , P, S), where V T is a set of terminal symbols, V N is a set of nonterminal symbols, P is a set of productions, and S ∈ V N is a distinguished start symbol.",65,66
12083,1878772,The fan-out of the symbol corresponding to a word is the number of continuous intervals in the sentence formed by the word and its descendants in the tree.,6,7
12084,12919540,"customer, boot ) (customer, shut off ) (tp, shut off ) The symbol means the noun modifies the verb, and means the verb modifies the noun.",17,18
12085,5418052,"In trees containing repairs, the symbol ET represents any number of editing terms and the sub-structure within them.",6,7
12086,1391765,"A context-free grammar (CFG) is a 4-tuple G = (Σ, N, S, R) where Σ and N are finite disjoint sets of terminals and nonterminals, respectively, S ∈ N is the start symbol and R is a finite set of rules.",45,46
12087,7455516,The tick symbol represents a semantic object that dominates a semantic subtree containing no underspecifications.,2,3
12088,7455516,"The arrow symbol describes a semantic object that does not take part in an underspecification, but which dominates a subtree that contains at least one.",2,3
12089,7455516,"The exclamation mark symbol denotes a semantic type that is underspecified, and for which at least two semantic objects are in competition.",3,4
12090,7455516,"Semantic objects in competition are denoted by the interrogation mark symbol , and are ordered according to the highest score of the candidate representation to which they belong.",10,11
12091,16568485,A word may end with a single quote with no preceding matching quote symbol to signify relational parameter.,13,14
12092,16568485,The module also identifies words with an apostrophe sign and substitutes it with a dummy symbol.,15,16
12093,12589105,"Ranked Alphabets, Trees and Substitution A ranked alphabet is a finite set of symbols in which each symbol is associated with a natural number, called the rank of a symbol.",18,19
12094,12589105,"Ranked Alphabets, Trees and Substitution A ranked alphabet is a finite set of symbols in which each symbol is associated with a natural number, called the rank of a symbol.",31,32
12095,12589105,Let ε be the special symbol that may be contained in Σ 0 .,5,6
12096,12589105,"S, the initial nonterminal, is a distinguished symbol in N 0 .",9,10
12097,12589105,"x n ) → α in P , the symbol ε doesn't occur in α.",9,10
12098,12589105,"Epsilon-Freeness on Linear, Monadic CFTGs According to our definition of CFTGs, they are allowed to generate trees with the special symbol ε, which is treated as the empty string while taking the yields of trees.",24,25
12099,12589105,"The set of terminal is Σ = Σ ∪ {c}, where c is a new symbol of rank 1.",18,19
12100,1458372,"To simplify the theoretical discussion, we assume that each symbol that we use in trees has a fixed rank, which determines the number of children of each node with that label.",10,11
12101,1458372,"The size |t| of the tree t ∈ T Σ is the number of occurrences of symbols from Σ∪V that appear in t. A context c is a tree of T Σ∪{ } (V ), in which the nullary symbol occurs exactly once.",42,43
12102,1458372,"The tree c[t] is obtained from c by replacing the symbol by t. A weighted synchronous tree substitution grammar (STSG) is a system (N, Σ, ∆, I, P ) where • N is an alphabet of nonterminals, • Σ and ∆ are ranked alphabets of input and output symbols, respectively, • I : N → R assigns initial weights, and ) and u = δ(n 1 , . . . ,",11,12
12103,1458372,"To keep the presentation simple, we assume that those links are re- S NP 1 @ V NP 2 ↔ S V @ NP 1 NP 2 S NP x 1 @ V x 2 NP x 3 → S S x 2 @ x 1 x 3 Figure 1: STSG production (top) and corresponding MBOT rule (bottom) where @ is an arbitrary symbol that is introduced during binarization.",69,70
12104,1458372,"We chose the symbol ""→"" because rules have a distinguished left-and right-hand side.",3,4
12105,1458372,Binarization of tree language devices typically consists of two steps: (i) binarization of the involved trees (using the auxiliary symbol @) and (ii) adjustment (binarization) of the processing device to work on (and fully utilize) the binarized trees.,23,24
12106,1458372,"S NP 1 V NP 2 ↔ S V NP 1 NP 2 S @ NP 1 V NP 2 ↔ S @ V NP 1 NP 2 An MBOT is in one-symbol normal form if there is at most one input and at most one output symbol, but at least one symbol in each rule (see Figure 2 ).",34,35
12107,1458372,"S NP 1 V NP 2 ↔ S V NP 1 NP 2 S @ NP 1 V NP 2 ↔ S @ V NP 1 NP 2 An MBOT is in one-symbol normal form if there is at most one input and at most one output symbol, but at least one symbol in each rule (see Figure 2 ).",49,50
12108,1458372,"S NP 1 V NP 2 ↔ S V NP 1 NP 2 S @ NP 1 V NP 2 ↔ S @ V NP 1 NP 2 An MBOT is in one-symbol normal form if there is at most one input and at most one output symbol, but at least one symbol in each rule (see Figure 2 ).",55,56
12109,1458372,2009) prove that every MBOT can be transformed into one-symbol normal form.,12,13
12110,1458372,"Consequently, we can transform each STSG to an equivalent MBOT in one-symbol normal form in linear time.",14,15
12111,1458372,"Finally, we note that a MBOT in one-symbol normal form has binarized derivation trees, which proves that we fully binarized the STSG.",10,11
12112,1458372,"2009) is illustrated in Figure 2 , which shows the rules of an MBOT in one-symbol normal form.",18,19
12113,1458372,"Let M be an STSG and A a weighted string automaton with states S. In the BAR-HILLEL construction for M and A, the maximal rank rk(M ) of a symbol in the derivation forest of M enters as an exponent into the complexity O(|M | • |S| 2 rk(M )+5 ).",32,33
12114,1458372,"The first type of rule (second item) does not involve an input symbol, and thus the nonterminal of G is just forwarded to the new state.",14,15
12115,1458372,The second type of rule (third item) uses a rule of R with the input symbol σ and a production of P that also contains σ.,17,18
12116,1458372,"Thus, the STSG should be transformed into an equivalent MBOT in one-symbol normal form, which can be achieved in linear time, and the BAR-HILLEL construction should be performed on this MBOT.",14,15
12117,1458372,The small gain is due to the one-symbol normal form and binarization.,9,10
12118,1458372,"Let M = (S , Σ, ∆, F , R ) and M = (S , ∆, Γ, F , R ) be MBOTs in one-symbol normal form.",33,34
12119,1458372,"Finally, in the third type, first a rule of R that produces an output symbol of ∆ is used and then this symbol is processed by a single rule of R .",16,17
12120,1458372,"Finally, in the third type, first a rule of R that produces an output symbol of ∆ is used and then this symbol is processed by a single rule of R .",24,25
12121,1458372,Note that every rule of R can produce at most one output symbol and the rules of R either process none or one input symbol due to the assumption that M and M are in one-symbol normal form.,12,13
12122,1458372,Note that every rule of R can produce at most one output symbol and the rules of R either process none or one input symbol due to the assumption that M and M are in one-symbol normal form.,24,25
12123,1458372,Note that every rule of R can produce at most one output symbol and the rules of R either process none or one input symbol due to the assumption that M and M are in one-symbol normal form.,37,38
12124,1458372,"Indeed, STSGs in one-symbol normal form, which can be defined as for MBOTs, can be composed as well, which shows that the one-symbol normal form is the key for composition.",6,7
12125,1458372,"Indeed, STSGs in one-symbol normal form, which can be defined as for MBOTs, can be composed as well, which shows that the one-symbol normal form is the key for composition.",30,31
12126,1458372,"Moreover, the complexity could be slightly improved by the observation that our construction only relies on (i) M having at most one output symbol per rule and (ii) M having at most one input symbol per rule.",26,27
12127,1458372,"Moreover, the complexity could be slightly improved by the observation that our construction only relies on (i) M having at most one output symbol per rule and (ii) M having at most one input symbol per rule.",39,40
12128,7700492,"Ranked Alphabets, Trees and Substitution A ranked alphabet is a finite set of symbols in which each symbol is associated with a natural number, called the rank of a symbol.",18,19
12129,7700492,"Ranked Alphabets, Trees and Substitution A ranked alphabet is a finite set of symbols in which each symbol is associated with a natural number, called the rank of a symbol.",31,32
12130,7700492,It is defined that ε is the special symbol that may be contained in Σ 0 .,8,9
12131,7700492,"S, the initial nonterminal, is a distinguished symbol in N 0 .",9,10
12132,7700492,"Definition 2.2 A head-pointing ranked alphabet is a ranked alphabet in which each symbol is additionally associated with a natural number, called the head of a symbol, and the head of a symbol is satisfying the following conditions: • If the rank of the symbol is 0, then the head of the symbol is also 0. •",15,16
12133,7700492,"Definition 2.2 A head-pointing ranked alphabet is a ranked alphabet in which each symbol is additionally associated with a natural number, called the head of a symbol, and the head of a symbol is satisfying the following conditions: • If the rank of the symbol is 0, then the head of the symbol is also 0. •",29,30
12134,7700492,"Definition 2.2 A head-pointing ranked alphabet is a ranked alphabet in which each symbol is additionally associated with a natural number, called the head of a symbol, and the head of a symbol is satisfying the following conditions: • If the rank of the symbol is 0, then the head of the symbol is also 0. •",36,37
12135,7700492,"Definition 2.2 A head-pointing ranked alphabet is a ranked alphabet in which each symbol is additionally associated with a natural number, called the head of a symbol, and the head of a symbol is satisfying the following conditions: • If the rank of the symbol is 0, then the head of the symbol is also 0. •",49,50
12136,7700492,"Definition 2.2 A head-pointing ranked alphabet is a ranked alphabet in which each symbol is additionally associated with a natural number, called the head of a symbol, and the head of a symbol is satisfying the following conditions: • If the rank of the symbol is 0, then the head of the symbol is also 0. •",58,59
12137,7700492,"If the rank of the symbol is greater than 0, then the head of the symbol is greater than 0 and less or equal to the rank of the symbol.",5,6
12138,7700492,"If the rank of the symbol is greater than 0, then the head of the symbol is greater than 0 and less or equal to the rank of the symbol.",16,17
12139,7700492,"If the rank of the symbol is greater than 0, then the head of the symbol is greater than 0 and less or equal to the rank of the symbol.",30,31
12140,7700492,"The Construction of Epsilon-Free Spine Grammars According to our definition of spine grammars, they are allowed to generate trees with leaves labeled by the special symbol ε, which is treated as the empty string while taking the yields of trees.",28,29
12141,7700492,"x n ) → α in P , α has no node labeled by the symbol ε.",15,16
12142,7700492,"The set of terminal is Σ = Σ ∪ {c}, where c is a new symbol of rank 1.",18,19
12143,6050768,Match states form the core of the model; each match state is represented by a set of emission probabilities for each symbol in the output alphabet.,22,23
12144,6050768,"They are represented in the same manner as match states, with each output symbol having an associated probability.",14,15
12145,6050768,The simple heuristic that we adopt is to label those columns match states for which half or more of the sequences have a symbol present (rather than a gap).,23,24
12146,6050768,"Then the probability a kl of state k transitioning to state l can be estimated by counting the number of times A kl that the transition is used in the alignment: a kl = A kl l A kl Similarly, the probability e k (a) of state k emitting symbol a is estimated by counting the number of times E k (a) that the emission is used in the alignment: e k (a) = E k (a) a E k (a ) There is the danger that some probabilities may be set to zero, so it is essential to add pseudocounts.",53,54
12147,6050768,"In the following equations, e j (a) is the probability of state j emitting character a. c ja represents the observed counts of state j emitting symbol a. A is the weight given to the pseudocounts.",29,30
12148,39211191,"PPM generates a prediction for each input symbol based on its previous context (i.e., a few, say k, forecoming symbols in the text).",7,8
12149,39211191,Each character is represented by its respective type symbol.,8,9
12150,39211191,"We then compute the predictions for each symbol as described in the previous section, and the results are shown in Table 2 .",7,8
12151,39211191,"Given an order of k, the algorithm computes the likelihood of each possible next symbol (i.e., the next character in the text or a space) by considering a context of size k at a time and then proceed to the next symbol in the text.",15,16
12152,39211191,"Given an order of k, the algorithm computes the likelihood of each possible next symbol (i.e., the next character in the text or a space) by considering a context of size k at a time and then proceed to the next symbol in the text.",45,46
12153,39211191,"In order to predict the next symbol, the algorithm follows the concept of PPM by attempting to find first the context of length k (k = 2 in this example) for this symbol in the context table (i.e., e*->f).",6,7
12154,39211191,"In order to predict the next symbol, the algorithm follows the concept of PPM by attempting to find first the context of length k (k = 2 in this example) for this symbol in the context table (i.e., e*->f).",35,36
12155,39211191,"To handle zero frequency, we use method D (PPMD) (Witten and Bell, 1991) where the escape character gets a probability of (d/2n), and the symbol gets a probability of (2c-1)/2n where n is the total number of symbols seen previously, d is the total number of distinct contexts, and c is the total number of contexts that appear in the string.",33,34
12156,8985117,The special symbol $ represents a starting phoneme or ending phoneme.,2,3
12157,16206042,"Based on the syntactic characteristics of Japanese reviews and the results of related work (Kobayashi, 2003; Taku, 2002) , we designed the following templates to extract opinion phrases: < noun + auxiliary word + adj / verb / noun > < adj + noun / undefined / verb > < noun + verb > < noun + symbol + adj / verb / noun > Except the above < adj > Opinion Mining Opinion mining methods can usually be divided into two types: supervised and unsupervised approaches.",63,64
12158,7926469,"HHMM probabilities Converting PCFGs to HHMMs requires the calculation of expected frequencies of generating symbols A η•µ in the left-progeny of a nonterminal symbol A η (in other words, of A η•µ being a left child of A η , or a left child of a left child of A η , etc.).",25,26
12159,14708750,"Our solution is to simply treat every letter as an IPA symbol (International Phonetic Association, 1999).",11,12
12160,14708750,"2 For example, the IPA symbol [m] denotes ""voiced bilabial nasal,"" which is the phoneme represented by the letter m in most languages that utilize Latin script.",6,7
12161,14708750,"s c i a n c h i | | | | | S a N k i 2 ALINE can also be applied to non-Latin scripts by replacing every grapheme with the IPA symbol that is phonetically closest to it (Jiampojamarn et al.,",36,37
12162,441443,Grammar Transformation The CKY algorithm requires a grammar in Chomsky normal form where the right-hand side of each rule either consists of two non-terminals or a single terminal symbol.,32,33
12163,441443,It uses the transformed grammar with grammar rules P and non-terminal symbol set N. The chart is conceptually a three-dimensional bit array containing one bit for each possible constituent.,13,14
12164,441443,The value of prob(r) is 1 if the lefthand side of r is an auxiliary symbol inserted during the grammar transformation and otherwise the probability of the corresponding PCFG rule.,16,17
12165,7573383,"An adaptor grammar is a 7-tuple (N, W, R, S, θ, A, C) where (N, W, R, S, θ) is a PCFG with nonterminals N , terminals W , rules R, start symbol S ∈ N and rule probabilities θ, where θ r is the probability of rule r ∈ R, A ⊆ N is the set of adapted nonterminals and C is a vector of adaptors indexed by elements of A, so C X is the adaptor for adapted nonterminal X ∈ A. Informally, an adaptor C X nondeterministically maps a stream of trees from a base distribution H X whose support is T X (the set of subtrees whose root node is X ∈ N generated by the grammar's rules) into another stream of trees whose support is also T X .",49,50
12166,559079,Future perfect is translated into future using the perfective aspect if there is no indicator of subjunctive meaning which is expressed in Russian by the preterite form an and insertion of BepoflTnO 'probably'(see the symbol PRT+VEROJ^TNO in fig.l).,35,36
12167,11256667,"This does not mean that every symbol has a corresponding one in the other language, but instead that word transformation comes from three basic operations: substitution, insertion and deletion.",6,7
12168,11256667,"Second, we assume that each symbol is aligned to at most one symbol in the other word.",6,7
12169,11256667,"Second, we assume that each symbol is aligned to at most one symbol in the other word.",13,14
12170,11256667,"If there is a many-to-one correspondence that is consistent between languages, it would be beneficial to change the word representation so that the many symbols are considered as a single symbol instead.",35,36
12171,11256667,"M"", the match state, emits an aligned pair of symbols (not necessarily identical) with one symbol on the top and the other on the bottom output stream. """,20,21
12172,11256667,"X"" and ""Y"", the gap states, output a symbol on only one stream against a gap on the other.",13,14
12173,11256667,These individual scores are combined to produce an overall score for the pair of sequences in the same way as individual symbol probabilities are combined in other algorithms.,21,22
12174,11256667,"These algorithms continually multiply probabilities together every time they process a symbol (or a symbol pair), which means that the overall probability of an alignment strongly depends on word lengths.",11,12
12175,11256667,"These algorithms continually multiply probabilities together every time they process a symbol (or a symbol pair), which means that the overall probability of an alignment strongly depends on word lengths.",15,16
12176,2542653,The presence of the symbol √ indicates significance of the Grammatical Function effect as well as the direction of the bias.,4,5
12177,2694479,"The metarules are generally context-free rules which take the form: METANOTION :: hypcrnotion-1; hypcrnotlon-2; ... ; hypernotion-n. where METANOTION is tile left-hand side ""nonterminal"" symbol of the production and hypernotion-1, hypernotlon-2, ... hypcrnotion-n are the n alternatives of the production right-hand side.",36,37
12178,991005,"w n , where w i denotes the word at the ith position, and w 0 = $ is a wall symbol.",22,23
12179,1681736,Categorial grammars that assign at most k types to each symbol in the alphabet are called k-valued grammars; 1-valued grammars are also called rigid grammars.,10,11
12180,52817936,"Cycles are not allowed, and w 0 is taken to be the dummy ""wall"" symbol, $, whose only child is the root word of the sentence (normally the main verb).",17,18
12181,52817936,"Let the correspondence x : {1, ..., n} → {0, ..., m} be a mapping from indices of words in t to indices of words in s. (We require each target word to map to at most one source word, though multiple target words can map to the same source word, i.e., x(i) = x(j) while i = j.) When x(i) = 0, the ith target word maps to the wall symbol, equivalently a ""null"" word.",88,89
12182,2862221,A symbol α ∈ U is adjacent to a word x relative to a set of links L over U if for every word z between x and α there is a path of links in L from x to z but there is no link from z to α.,1,2
12183,2862221,"Therefore, the lexical entry of x should collect statistics about each of the adjacency positions of x. As seen above, adjacency positions may move, so the learner waits until the parser completes parsing the utterance and then updates each adjacency point A x i with the symbol α at the ith adjacency position of x (relative to the parse generated by the parser).",49,50
12184,2862221,"Let • A α i =        true if l ∈ L(W ) : A α i (l) > A α i (Stop) f alse otherwise The update of A x i by α begins by incrementing the count: #(A x i ) += 1 If α is a boundary symbol (∅ l or ∅ r ) or if x and α are words separated by stopping punctuation (full stop, question mark, exclamation mark, semicolon, comma or dash): A x i (Stop) += 1 Otherwise, for every l ∈ L(W ): A x i (l −1 ) += 1 if l = [α] Āα Sign(−i) (l) otherwise (In practice, only l = [α] and the 10 strongest labels in A α Sign(−i) are updated.",64,65
12185,16091111,"HMMs, for example, can be understood as a random walk through a probabilistic finite-state network, with an output symbol sampled at each state.",23,24
12186,16091111,"Each ""step"" of the walk and each symbol emission corresponds to one derivation step.",9,10
12187,16091111,"PCFGs generate phrase-structure trees by recursively rewriting nonterminal symbols as sequences of ""child"" symbols (each itself either a nonterminal symbol or a terminal symbol analogous to the emissions of an HMM).",24,25
12188,16091111,"PCFGs generate phrase-structure trees by recursively rewriting nonterminal symbols as sequences of ""child"" symbols (each itself either a nonterminal symbol or a terminal symbol analogous to the emissions of an HMM).",28,29
12189,16091111,"x 0 is a special ""wall"" symbol, $, on the left of every sentence.",8,9
12190,10659860,An atomic FS is represented by an atomic symbol and a complex FS is represented by a set of feature-value pairs.,8,9
12191,10659860,"The type symbol lattice contains the greatest type symbol Top, which subsumes every type symbol, and the least type symbol Bottom, which is subsumed by every I.ype symbol.",2,3
12192,10659860,"The type symbol lattice contains the greatest type symbol Top, which subsumes every type symbol, and the least type symbol Bottom, which is subsumed by every I.ype symbol.",8,9
12193,10659860,"The type symbol lattice contains the greatest type symbol Top, which subsumes every type symbol, and the least type symbol Bottom, which is subsumed by every I.ype symbol.",15,16
12194,10659860,"The type symbol lattice contains the greatest type symbol Top, which subsumes every type symbol, and the least type symbol Bottom, which is subsumed by every I.ype symbol.",21,22
12195,10659860,"The type symbol lattice contains the greatest type symbol Top, which subsumes every type symbol, and the least type symbol Bottom, which is subsumed by every I.ype symbol.",30,31
12196,10659860,An example of a type symbol lattice is shown in Fig.,5,6
12197,10659860,An extended complex FS is represented by a type symbol and a set of feature-value pairs.,9,10
12198,10659860,"Once complex IeSs are extended as above, an atomic FS can be seen as an extended complex FS whose type symbol has only Top as its greater type symbol and only Bottom as its lesser type symbol and which has an empty set of feature value pairs.",21,22
12199,10659860,"Once complex IeSs are extended as above, an atomic FS can be seen as an extended complex FS whose type symbol has only Top as its greater type symbol and only Bottom as its lesser type symbol and which has an empty set of feature value pairs.",29,30
12200,10659860,"Once complex IeSs are extended as above, an atomic FS can be seen as an extended complex FS whose type symbol has only Top as its greater type symbol and only Bottom as its lesser type symbol and which has an empty set of feature value pairs.",37,38
12201,10659860,"Among such structures, unification c'm be defined IAP,-Kaci 861 by using the following order; ATFS tl is less than or equal to a TFS t2 if and only if: • the type symbol of tl is less than or equal to the type syn'bol of/2; and • each of the features of t2 exists in t1 and.",35,36
12202,10659860,"The NODE structure has the slots TYPESYMBOL to represent a type symbol, ARCS to represent a set of feature-value pairs, GENERATION to specify the unification process in which the structure has been created, FORWARD, and COPY.",11,12
12203,10659860,"Next, the procedure calculates the meet of their type symbol.",10,11
12204,10659860,"As in TFS unification, failure tendency information is recorded in terms of a triplet consisting of the greatest lower bound type symbol of the input TFSs' type symbols, a feature and success/failure flag.",22,23
12205,10659860,This is because the type symbol of a 'rFS represents salient information on the whole TFS.,5,6
12206,10659860,This is achieved by the sorting procedure of common label arc pairs attached to the meet type symbol.,17,18
12207,10659860,"Moreover, it is possible for each type symbol to select whether to apply feature unification order sorting or not.",8,9
12208,365927,"The symbol 'x' indicates that the verse is derived from Mark, and '-' indicates that it is not.",1,2
12209,13108739,"For each of the seed NP pairs (E i :E j ), we search in a large corpus for the strings that match the regular expression ""E i * * * E j "" or ""E j * * * E i "", where * is a wildcard for any word or symbol.",58,59
12210,13108739,"If the string is followed by a symbol, the symbol will be also included in the pattern.",7,8
12211,13108739,"If the string is followed by a symbol, the symbol will be also included in the pattern.",10,11
12212,5545122,The problem is the non-uniform appearance of an explicit empty string symbol that allows for insertions.,13,14
12213,5545122,Table 1 summarizes the following aspects of the performance of the induced decision tree classifiers on the test data relative to the size of context used for classification: classification accuracy per symbol; micro-averaged precision (P) and recall (R) per symbol; size of the tree in number of nodes; and size of the saved tree data in kilobytes.,32,33
12214,5545122,Table 1 summarizes the following aspects of the performance of the induced decision tree classifiers on the test data relative to the size of context used for classification: classification accuracy per symbol; micro-averaged precision (P) and recall (R) per symbol; size of the tree in number of nodes; and size of the saved tree data in kilobytes.,47,48
12215,5545122,"As before, classification accuracy is given on a per-symbol basis; average accuracy per word is around 85%.",11,12
12216,2869969,We include the dummy root symbol x 0 = root so that each word can be a modifier.,5,6
12217,2869969,We use the symbol ∧ to represent feature conjunctions.,3,4
12218,553426,"To make the exposition clearer by avoiding overuse of the symbol →, we introduce notation for rooted subtrees not only for nodes, but also for edges: Subtree i = {v ∈ V | i → * v}, Subtree i↔ j = {v ∈ V | Parent i↔ j → * v} (note that the subtree of an edge is defined relative to its parent node).",10,11
12219,19024647,"The nonterminal S is the start symbol, with f (S) = 1.",6,7
12220,19024647,"Production representation We introduce here a specialized representation for p. Let $ be a fresh symbol that does not occur in p. We define the characteristic string of p as the string σ N (p) = α 1 $α 2 $ • • • $α f (A) , where each α j is obtained from α j by removing all the occurrences of symbols in V T .",15,16
12221,19024647,"Consider now some occurrence A i of a nonterminal symbol in the right-hand side of p. We define the position set of A i , written X A i , as the set of all non-negative integers j ∈ [|σ N (p)|] such that the j-th symbol in σ N (p) is a variable of the form x i,h for some h. Example 2 Let p : A → g(A 1 , A 2 , A 3 ), where g( x 1,1 , x 1,2 , x 2,1 , x 3,1 , x 3,2 ) = α with α = x 1,1 ax 2,1 x 1,2 , x 3,1 bx 3,2 .",9,10
12222,19024647,"Consider now some occurrence A i of a nonterminal symbol in the right-hand side of p. We define the position set of A i , written X A i , as the set of all non-negative integers j ∈ [|σ N (p)|] such that the j-th symbol in σ N (p) is a variable of the form x i,h for some h. Example 2 Let p : A → g(A 1 , A 2 , A 3 ), where g( x 1,1 , x 1,2 , x 2,1 , x 3,1 , x 3,2 ) = α with α = x 1,1 ax 2,1 x 1,2 , x 3,1 bx 3,2 .",55,56
12223,19024647,"We replace p by means of a new production p obtained from p by substituting A i and A j with a fresh nonterminal symbol B, so that r(p ) = r(p) − 1.",24,25
12224,478500,"The nonterminals of the naive encoding CFG consist of the start symbol S and symbols X u for each terminal u ∈ Σ, and the productions of the CFG are the instances of the following schemata: S → X u where 0 u X u → u X u → X v X u where v u X u → X u X v where u v The dependency annotations associated with each production specify how to interpret a local tree generated by that production, and permit us to map a CFG parse to the corresponding dependency parse.",11,12
12225,16125561,"The GIZA++ toolkit performs one-to-many alignments, which means that a single symbol in the source language may be aligned to at least one symbol in the target language.",16,17
12226,16125561,"The GIZA++ toolkit performs one-to-many alignments, which means that a single symbol in the source language may be aligned to at least one symbol in the target language.",28,29
12227,16125561,"Then, each phoneme symbol is transliterated into corresponding Korean letter based on the transliteration mapping table.",4,5
12228,1300761,"The relation among these informational elements is represented by the right-arrow ""--,"" symbol.",17,18
12229,7307655,"The word graph is initialized with a begin-of-sentence (BOS) and an end-of-sentence (EOS) symbol, with an edge of weight 1 from BOS to EOS.",25,26
12230,380199,"For the purpose of uniform encoding of phonetic symbols, we adopted the ALINE scheme (Kondrak, 2002) , in which every phonetic symbol is represented by a single lowercase letter followed by zero or more uppercase letters.",25,26
12231,380199,The initial lowercase letter is the base letter most similar to the sound represented by the phonetic symbol.,17,18
12232,14983672,"In practice, any IPA symbol can be encoded as a vector of universal phonetic features.",5,6
12233,18045031,"Japanese Katakana In the Japanese Katakana generation task, we replace each Katakana symbol with one or two letters using standard romanization tables.",13,14
12234,18045031,"Then, for each grapheme, we select a letter (or a null symbol) that has the highest conditional probability.",14,15
12235,18045031,"First of all, we observe that most of the titles that contain a separation symbol "" • "" on the target side are transliterations.",15,16
12236,6595785,"Again, each layout-unit has an id attribute, which carries an identifying symbol; in addition, however, the stand-off annotation is achieved via an attribute xref which points to the base units which belong to that layout unit.",16,17
12237,9964459,The following simple heuristic works well for a number of languages: treat every letter as if it were a symbol in the International Phonetic Alphabet (IPA).,20,21
12238,9964459,"For example, the IPA symbol [Ñ] denotes a voiced bilabial nasal consonant, which is the phoneme represented by the letter m in most languages that utilize Latin script.",5,6
12239,237433860,"To give an example of the difference between S and M, the SENT → PP rule which is typical of the telegraphic style and corresponds to expressions such as ""en service"" (frequency 263 in S) does not appear at all in M-on the other hand, the NP → DET NOUN rule that corresponds to the fundamental property of French nouns of being preceded by a determinant (a property that is often relaxed in telegraphic style) is the second most frequent rule in M but only the eleventh in S. Frequency-Based Subgrammars Let T, N be fixed sets of terminals and nonterminals, and S an initial symbol.",119,120
12240,18364781,"Definition 1 A synchronous context-free grammar (SCFG) is a tuple G = (V N , V T , P, S), where V N , V T are finite, disjoint sets of nonterminal and terminal symbols, respectively, S ∈ V N is the start symbol and P is a finite set of synchronous productions, each of the form [A 1 → α 1 , A 2 → α 2 ], with A 1 , A 2 ∈ V N and α 1 , α 2 ∈ V * I synchronous strings.",54,55
12241,18364781,"A probabilistic regular grammar (PRG) is a PCFG with underlying productions of the form A → aB or A → ε, with A, B nonterminal symbols and a a terminal symbol.",34,35
12242,18364781,"Given the input PRG G p , we construct a target PSCFG G p that translates string $ into $, with $ a special symbol.",25,26
12243,5390840,"One system failed when the entity was a name (vs. a symbol), it contained a number, and the number was in the right-most (vs. a medial) position in a word.",12,13
12244,5390840,Morphosyntactic features The most salient morphosyntactic feature is whether an entity is a name or a symbol.,16,17
12245,5390840,The fault model motivating this feature suggests that a system might perform differently depending on whether an input is a name or a symbol.,23,24
12246,9658331,"We change the patterns to lower-case, convert sequences of digits to the # symbol, and run the Porter stemmer 4 (Porter, 1980).",16,17
12247,9658331,"In this data, tokens appearing less than 200 times have been mapped to the UNK symbol.",16,17
12248,16050554,"In case the text was garbled, we then filtered the first 3 lines from every file and any line with an '@' symbol (which might be part of an affiliation).",25,26
12249,2020038,"The method requires a manual enumeration of the possible transliterations for each katakana symbol, which is unfeasible for many language pairs.",13,14
12250,12223441,The '$' symbol is the sentence boundary marker.,4,5
12251,13472899,"Unfortunately, tokens appearing less than 200 times have been mapped to the UNK symbol, and only N-grams appearmore than 40 times are included.",14,15
12252,17143000,"Linear context-free rewriting systems A linear context-free rewriting system (LCFRS) is a construct G = (V N , V T , P, S), where: V N is an alphabet of nonterminal symbols in which each symbol A ∈ V N is associated with a value ϕ(A), called its fan-out; V T is an alphabet of terminal symbols; S ∈ N is a distinguished start symbol with ϕ(S) = 1; and P is a set of productions of the form p : A → g(B 1 , B 2 , . . . ,",46,47
12253,17143000,"Linear context-free rewriting systems A linear context-free rewriting system (LCFRS) is a construct G = (V N , V T , P, S), where: V N is an alphabet of nonterminal symbols in which each symbol A ∈ V N is associated with a value ϕ(A), called its fan-out; V T is an alphabet of terminal symbols; S ∈ N is a distinguished start symbol with ϕ(S) = 1; and P is a set of productions of the form p : A → g(B 1 , B 2 , . . . ,",81,82
12254,17143000,"Let $ be a fresh symbol that does not occur in G. We define the characteristic string of the production p as σ(p) = α g,1 $ • • • $α g,ϕ(g) , and the variable string of p as the string σ N (p) obtained from σ(p) by removing all the occurrences of symbols in V T .",5,6
12255,17143000,"B r−2 , X) and p 2 : X → g 2 (B r−1 , B r ) , where X is a fresh nonterminal symbol, the characteristic string σ(p 1 ) is the string obtained from σ(p) by replacing each substring γ i by the variable x r−1,i , and the characteristic string σ(p 2 ) is the string γ 1 $ • • • $γ n .",27,28
12256,18102149,"The "" * "" symbol corresponds to all features, i.e. no restriction is applied to any POS tag or NE class.",4,5
12257,18102149,"The ""-"" symbol means that the source field is fully neglected from the SV regression.",4,5
12258,1536622,We remove each h from C and integrate a new tree symbol x. If x = w i it means that we have successfully added the new word to the tree and this hypothesis goes into the queue for the next word N .,11,12
12259,1536622,"On the other hand, if x is the correct symbol, q should go up, so the two should offset and h is still competitive.",10,11
12260,1788449,"We represent an HPSG sign as a tuple ¡ £¢ ¥¤ §¦ © § , where ¦ is a lexical sign of the head word, is a part-of-speech, and is a symbol representing the structure of the sign (mostly corresponding to nonterminal symbols of the Penn Treebank).",40,41
12261,1441729,"We take a (finite constructor) tree t as a finite tree in which each node is labelled with a symbol of Σ, and the number of children of the node is exactly the arity of this symbol.",21,22
12262,1441729,"We take a (finite constructor) tree t as a finite tree in which each node is labelled with a symbol of Σ, and the number of children of the node is exactly the arity of this symbol.",39,40
12263,1441729,"An RTG is a 4-tuple G = (S, N, Σ, P), where N and Σ are nonterminal and terminal alphabets, S ∈ N is the start symbol, and P is a finite set of production rules.",35,36
12264,1441729,"But while ordinary transducers read the input tree symbol by symbol, a context tree transducer can read multiple symbols at once.",8,9
12265,1441729,"But while ordinary transducers read the input tree symbol by symbol, a context tree transducer can read multiple symbols at once.",10,11
12266,1441729,"M R can nondeterministically choose to either copy the current symbol to the output tree unchanged, or to apply a rewrite rule from R. The rules are built in such a way that in each run, exactly one rewrite rule must be applied.",10,11
12267,1441729,"The nonterminals of G are subgraphs of D, marked either with a set of states of M R or the symbol F, indicating that G R had no production rule for a given left-hand side.",21,22
12268,1441729,The start symbol of G is marked with F because G should only generate trees that G R cannot generate.,2,3
12269,1441729,"That is, we distinguish possible different occurrences of the same symbol in D by marking each occur-rence with the name of the node.",11,12
12270,1441729,"stand for classes of labels in Σ, and a rule schema [a] (C/i, C /k) is to be read as shorthand for a set of rewrite rules which rearrange a tree where the i-th child of a symbol from C is a symbol from C into a tree where the symbol from C becomes the k-th child of the symbol from C .",47,48
12271,1441729,"stand for classes of labels in Σ, and a rule schema [a] (C/i, C /k) is to be read as shorthand for a set of rewrite rules which rearrange a tree where the i-th child of a symbol from C is a symbol from C into a tree where the symbol from C becomes the k-th child of the symbol from C .",52,53
12272,1441729,"stand for classes of labels in Σ, and a rule schema [a] (C/i, C /k) is to be read as shorthand for a set of rewrite rules which rearrange a tree where the i-th child of a symbol from C is a symbol from C into a tree where the symbol from C becomes the k-th child of the symbol from C .",60,61
12273,1441729,"stand for classes of labels in Σ, and a rule schema [a] (C/i, C /k) is to be read as shorthand for a set of rewrite rules which rearrange a tree where the i-th child of a symbol from C is a symbol from C into a tree where the symbol from C becomes the k-th child of the symbol from C .",71,72
12274,3032286,W W W ( f ) W W W ( q e 1 N 2 1 1 N 2 1 N L L = (16) q(w 1 w 2 …w N-1 ) in ( 16 ) denotes the number of different symbol w N that have directly followed the word sequence w 1 w 2 …w N-1 .,44,45
12275,10539015,"Consider for instance the following set of context-free rules with a single non-terminal symbol: A , A → A 1 A 2 , A 1 A 2 A , A → d A 1 idées A 2 , A 1 A 2 ideas A , A → incolores , colorless A , A → vertes , green A , A → dorment A , sleep A A , A → f urieusement , f uriously It is one of many rule sets that would suffice to generate the English translation 1b for the French sentence 1a.",17,18
12276,10539015,"Thus, a rule chart for a rule with one nonterminal can be denoted as as x i 1 i+1 Ax j j 1 +1 , µ , where we have introduced the symbol µ to represent the set of messages associated with a given item in the chart.",33,34
12277,10539015,"In the simplest case, illustrated in figure 2 , the non-local feature depends on the position within the span of the rule's non-terminal symbol, so that its model estimate can be computed when its rule chart is combined with the span chart for its non-terminal symbol.",29,30
12278,10539015,"In the simplest case, illustrated in figure 2 , the non-local feature depends on the position within the span of the rule's non-terminal symbol, so that its model estimate can be computed when its rule chart is combined with the span chart for its non-terminal symbol.",54,55
12279,1363212,The lexical head of a terminal symbol is the symbol itself and the lexical head of a nonterminal symbol is the lexical head of its (unique) head child.,6,7
12280,1363212,The lexical head of a terminal symbol is the symbol itself and the lexical head of a nonterminal symbol is the lexical head of its (unique) head child.,9,10
12281,1363212,The lexical head of a terminal symbol is the symbol itself and the lexical head of a nonterminal symbol is the lexical head of its (unique) head child.,18,19
12282,1363212,"In a head-lexicalized PCFG (HL-PCFG) (Carroll and Rooth, 1998; Charniak, 1997) , one symbol on the right-hand side of each rule is marked as the head.",24,25
12283,1363212,Additional rules introduce the start symbol TOP and generate special word forms like hyphenated (Thomas-Mann-Straße) or truncated words (Vor-).,5,6
12284,1363212,The quote symbol marks the head of a rule.,2,3
12285,1363212,"The first class are the rules which expand the start symbol TOP to an adjective or adverb, leading to a preference of these word classes over other word classes, in particular verbs.",10,11
12286,14692997,"A Context-Free Grammar (CFG) is a quadruple (N, W, R, S) where N and W are disjoint finite sets of nonterminal and terminal symbols respectively, R is a finite set of productions or rules of the form A → β where A ∈ N and β ∈ (N ∪W ) , and S ∈ N is the start symbol.",69,70
12287,14692997,"The set of trees generated by the CFG is T S , where S is the start symbol, and the set of strings generated by the CFG is the set of yields (i.e., terminal strings) of the trees in T S .",17,18
12288,14692997,"The PCFG generates the distribution over trees G S , where S is the start symbol.",15,16
12289,14692997,"The nonterminals consist of the start symbol Sentence, Doc j and Doc j for each j ∈ 1, . . . ,",6,7
12290,14692997,"Just as with the PCFG, an adaptor grammar generates the distribution over trees G S , where S ∈ N is the start symbol.",24,25
12291,2794522,"Table 5: ITG Rules in Normal Form In these rules, 𝑆 is the Start symbol; 𝐴 is the category for concatenating combination whereas 𝐵 for inverted combination.",16,17
12292,12211892,"Consider for instance the following set of context-free rules with a single non-terminal symbol: A , A → A 1 A 2 , A 1 A 2 A , A → d A 1 idées A 2 , A 1 A 2 ideas A , A → incolores , colorless A , A → vertes , green A , A → dorment A , sleep A A , A → f urieusement , f uriously It is one of many rule sets that would suffice to generate the English translation 1b for the French sentence 1a.",17,18
12293,6570141,"The length of a string w is noted |w|, and the number of occurrence of a symbol a in w is noted |w| a .",17,18
12294,6570141,"An ndimensional multiset-valued linear indexed grammar (MLIG) is a tuple G = N, Σ, P, (S, x 0 ) where N is a finite set of nonterminal symbols, Σ a finite alphabet disjoint from N , V = (N ×N n ) Σ the vocabulary, P a finite set of productions in (N × N n ) × V * , and (S, x 0 ) ∈ N × N n the start symbol.",88,89
12295,6570141,"To illustrate this definition, and its relevance for free word order languages, consider the 3-dimensional MLIG with productions (S, 0) → ε | (S, 1), (S, e 1 ) → a (S, 0), (S, e 2 ) → b (S, 0), (S, e 3 ) → c (S, 0) L(G) = {w ∈ Σ * | (S, x 0 ) ⇒ * and start symbol (S, 0).",96,97
12296,6570141,"Hopcroft and Pansiot (1979, Lemma 2.8 ) exhibit an example of a VASS with a non semilinear reachability set, which we translate as a 2-dimensional right linear MLIG with productions 3 and (S, e 2 ) as start symbol, that generates the non semilinear language L nsm = {a n b m | 0 ≤ n, 0 < m ≤ 2 n } .",45,46
12297,6570141,"An unordered vector grammars with dominance links (UVG-dl) is a tuple G = N, Σ, W, S where N and Σ are disjoint finite sets of nonterminals and terminals, V = N ∪ Σ is the vocabulary, W is a set of vectors of productions with dominance links, i.e. each element of W is a pair (P, D) where each P is a multiset of productions in N × V * and D is a relation from nonterminals in the right parts of productions in P to nonterminals in their left parts, and S in N is the start symbol.",113,114
12298,6570141,"Set S as start symbol and add a production S → aA to the sole vector of the grammar G constructed by Koller and Rambow (2007) from a normal dominance graph, with dominance links to all the other productions.",4,5
12299,11336246,"Given a string w 2 ˙ , we choose a lexicon entry for each occurrence of a symbol in w, line up the respective lexical categories from left to right, and apply admissible rules to adjacent pairs of categories.",17,18
12300,11336246,"As the start symbol, we use s, the final category of G. The set of productions of G T is constructed as follows.",3,4
12301,16406435,"Finally, we define S start and S end to be a start and an end symbol.",16,17
12302,19455894,"We will always assume that x 1 = <s>, the start-of-sentence symbol, and x n = </s>, the end-of-sentence symbol.",18,19
12303,19455894,"We will always assume that x 1 = <s>, the start-of-sentence symbol, and x n = </s>, the end-of-sentence symbol.",34,35
12304,13327514,"Before completely clarifying the mechanism of human learning, we tend to believe that machines can understand symbol sequences with simple logic.",17,18
12305,13327514,"The symbol • denotes the attachment operator; s denotes a subsequence of the character sequence S; c denotes a character in the alphabet Σ. In fact, x is the largest proper subsequence of s. 3.",1,2
12306,13327514,"The CV of a pair of adjacent subsequences is formulated as CV(S left • S right ) = IV(S left ) × IV(S right ) × LRV(S left • S right ) ( 9 ) where the LRV is formulated as LRV(S left • S right ) = ( H(SP1R(S left )) × H(SP1L(S right )) HRM L1 × HLM L2 ) x (10) The superscript x is the exponent; The symbol • denotes that S left and S right are two adjacent sequences; L1 and L2 denote the lengths of character sequences in SP1R(S left ) and SP1L(S right ), respectively.",76,77
12307,13327514,"Therefore, the hierarchical format will be changed to a format that looks like A B C, where the symbol denotes a space character.",20,21
12308,13327514,"2000) (TH), which is based on the partial matching (PPM) symbol-wise compression scheme.",16,17
12309,2456677,"Target: Users of Twitter use the ""@"" symbol to refer to other users on the microblog.",10,11
12310,28704517,3 First approach: Hard attention model with copy mechanism (HACM) Our first approach augments the hard monotonic attention model of Aharoni and Goldberg (2017) (eq 3) (eq 5) (eq 4) with a copy mechanism which adds a soft switch between generating an output symbol from a fixed vocabulary Σ train and copying the currently attended input symbol x i .,54,55
12311,28704517,3 First approach: Hard attention model with copy mechanism (HACM) Our first approach augments the hard monotonic attention model of Aharoni and Goldberg (2017) (eq 3) (eq 5) (eq 4) with a copy mechanism which adds a soft switch between generating an output symbol from a fixed vocabulary Σ train and copying the currently attended input symbol x i .,67,68
12312,28704517,"At each step, the model either generates an output symbol or starts to attend to the next encoded input character.",10,11
12313,28704517,"At test time, we allow the copying of out-ofvocabulary (OOV) symbols by adding the following modification to the mixture distribution in Equation 4: P t (a) =1 {a=x i } 1 {x i ∈Σ\Σ train } + w gen t P gen t (a) + (1 − w gen t )1 {a=x i } 1 {x i ∈Σ train } (6) Therefore, if the currently attended symbol x i is OOV, we copy it with probability one according to the distribution 1 {a=x i } ; otherwise, we use the mixture of generation P gen t and copy 1 {a=x i } distributions.",90,91
12314,28704517,"After copying the OOV symbol, we advance the attention pointer and use STEP as the previous predicted action.",4,5
12315,2468773,"In order to address this, we substitute such characters by a special NEW symbol and train the model on it by including it in the additional training samples we create, cf. §",14,15
12316,2468773,"The format of the input of the encoder is the same as in (Kann and Schütze, 2016a) , but with a small modification to be able to handle unlabeled data: Given the set of morphological subtags M that each target tag is composed of (e.g., the tag 1SgPresInd contains the subtags 1, Sg, Pres and Ind), and the alphabet Σ of the language of application, our input is of the form (A | M * ) Σ * , i.e., it consists of either a sequence of subtags or the symbol A signaling that the input is not annotated and should be autoencoded, and (in both cases) the character sequence of the input word.",103,104
12317,10632612,"In the CTB 7.0 processed by the procedure in Section 2.1, the set consists of 32 EC tags plus a special NULL symbol, obtained by modulating the list of ECs in Table 1 with their positions (e.g., * pro * @1 in Figure 2 ).",23,24
12318,1809816,"The numbers of distinct features for each type are denoted in parentheses, with the second number, after the addition symbol, indicating the number of histogram features (explained below) for that type.",21,22
12319,9407568,"In the case at hand, one could define a function f that replaces every voiced obstruent by the designated symbol ♦ so that the grammar G can be reduced to the single bigram ♦ .",20,21
12320,9407568,"One could posit that morphology, just like phonology, treats every phonological segment as a symbol.",16,17
12321,9407568,"Alternatively, one may treat each morpheme, including stems, as an atomic symbol.",14,15
12322,9407568,"To better understand why different representations could in principle affect subregular complexity, note first that whether a stem is represented as a single, atomic symbol or as a sequence of phonological segments seems to determine if prefixes and suffixes might be separated by an unbounded amount of symbols.",26,27
12323,9407568,"A concrete example is the nominalization circumfix ke--an in Indonesian (Mahdi, 2012; Sneddon, 1996) : (1) a. tingii high b. ke-NMN-tinggi high -an -NMN 'altitude' If a stem is a single symbol x , then x and uxv are well-formed whereas ux and xv are not due to u--v being a circumfix whose subparts cannot occur in isolation.",46,47
12324,9407568,"Suffix Substitution Closure Language L is SL iff there exists a k ∈ N such that for all strings u 1 , v 1 , u 2 , v 2 and any string x of length k − 1, if u 1 xv 1 , u 2 xv 2 ∈ L, then u 1 xv 2 ∈ L. If there is no upper bound on the length of stems, then we can infer from x k ∈ L and ux k v ∈ L that both x k v ∈ L and ux k ∈ L. It seems, then, that circumfixes are strictly local only if each stem is an atomic symbol.",116,117
12325,9407568,"2) a. maha big siswa pupil 'student' b. ke- NMN- maha big siswa pupil -an -NMN 'student affairs' Compounding is an unbounded process, so even if each stem is mapped to a single symbol x , one ends up with the same patterns as with the segmental mapping approach: x + and ux + v are well-formed, while ux + and x + v are ill-formed.",39,40
12326,9407568,"The details of the segmental mapping are as follows: within a stem, all segments are replaced by some distinguished symbol.",21,22
12327,9407568,"Symbols are chosen to maximize clarity of exposition, so that we sometimes assign each morpheme a unique symbol and sometimes map irrelevant morphemes to a randomly chosen filler symbol.",18,19
12328,9407568,"Symbols are chosen to maximize clarity of exposition, so that we sometimes assign each morpheme a unique symbol and sometimes map irrelevant morphemes to a randomly chosen filler symbol.",29,30
12329,9407568,"The resulting pattern stem-(stem + -o) is tierbased strictly 2-local under the assumption that a designated symbol occurs between stems, say .",19,20
12330,9407568,"Russian compounding, on the other hand, follows the pattern (stem-o) * -stem, which means that the addition of a new stem to the compound requires the appearance of the compounding marker -o-between the stems: Assuming once again the presence of the special symbol # -which marked the edges of stems in the previous section -we can show this pattern to also be tier-based strictly 2-local.",52,53
12331,9407568,"Let h be a map that replaces all stems by s, all compound markers by o, and all other material by some other symbol.",25,26
12332,9407568,"As before we let h be a homomorphism that replaces all stems by s, the two parts of the circumfix by o, and all other material by some distinct symbol.",31,32
12333,6858235,"PCFGs and tightness Let G = (T, N, S, R) be a Context-Free Grammar in Chomsky normal form with no useless productions, where T is a finite set of terminal symbols, N is a finite set of nonterminal symbols (disjoint from T ), S ∈ N is a distinguished nonterminal called the start symbol, and R is a finite set of productions of the form A → B C or A → w, where A, B, C ∈ N and w ∈ T .",64,65
12334,6858235,Let G A be the grammar G with the start symbol being replaced with A. We can then intersect the grammar G A with the regular language T * c A T * c A T * (for each nonterminal A ∈ N ).,10,11
12335,2439226,"Worse still, even for the same predicate, it is legitimate to use arbitrarily different predicate symbols, e.g., other symbols like hired by or even predicate1 can also be used for the employer predicate, reminiscent of the symbol grounding problem (Harnad, 1990) .",41,42
12336,2439226,"This indeed brings an additional cost, but we believe it is reasonable and even necessary for three reasons: (1) Only domain administrators know the predicate semantics the best, so it has to be them to reveal that by grounding the predicates to natural language (the symbol grounding problem (Harnad, 1990) ). (",51,52
12337,2439226,"Most Freebase relations are described using a carefully chosen symbol (surface form), e.g., place of birth, which provides strong cues for their semantics.",9,10
12338,250390579,"We achieved further performance gains on the devset across all the models by highlighting the filler in each data instance with a special ""[]"" symbol, and this forms the basis for our results in Table 3 .",27,28
12339,16158467,"Sometimes two or more morphosyntactic categories share an element of form; e.g., the futuretense prefix t-can indicate the 2nd person in the MASC gender or, in the FEM gender, either the 2nd or 3rd person: temwr 'you (M.SG) will keep' temwr 'she will keep' temwrw 'you (F.PL) will keep' Verb inflections are thus mapped to composite categories, e.g., future%(2%M)|(2|3%F), where the symbol | means 'or'.",82,83
12340,12935213,"When 'w' follows 'a', 'e', 'o' and isn't followed by 'h', treat 'w' and the preceding vowel as a new vowel symbol; Step 2 and 3 form the basic vowel set.",37,38
12341,12935213,"A consecutive vowels sequence which is formed by the basic vowel set is treated as a new vowel symbol, excepting 'iu ', 'eo', 'io', 'oi', 'ia', 'ui', 'ua', 'uo'; Step 2, 3 and 4 form the new vowel set.",18,19
12342,12935213,"Consecutive consonants are separated; a vowel symbol(in the new vowel set) followed by a consonant sequence is separated from the sequence; if a vowel followed by a consonat sequence and the first consonat is { 'h', 'l', 'm', 'n', 'r' }, the first consonat symbol is concatenated with the vowel into a syllable.",62,63
12343,10436313,Substitution Each symbol in the s-expressions is substituted for a lambda expression encoding its semantics.,2,3
12344,22248025,"Mentions are token spans that (1) were identified as ""persons"" by spaCy's named entity recognizer, and (2) have a (firstname, lastname) pair as analyzed by the HAPNIS rulebased name parser, 6 which extracts, for example, Table 3 : Training and testing settings for mention sentences x, mention labels z, and entity labels y. (John, Doe) from the string Mr. John A. Doe Jr.. 7 To prepare sentence text for modeling, our preprocessor collapses the candidate mention span to a special TARGET symbol.",102,103
12345,22248025,"To prevent overfitting, other person names are mapped to a different PER-SON symbol; e.g. ""TARGET was killed in an encounter with police officer PERSON.""",15,16
12346,980544,"For example, because Hindi has no /w/ sound, the transliteration of Gershwin instead uses a symbol that represents the phoneme /V/, similar to the /v/ phoneme in English.",17,18
12347,980544,Measuring similarity The approaches presented in the previous section crucially depend on a method for computing the similarity between various symbol sequences that represent the same word.,20,21
12348,18292719,"In order to facilitate generalization, we perform a lossless pre-processing step that replaces all accented vowels with their unaccented equivalent followed by a special stress symbol (e.g. canto').",28,29
12349,1817490,"From a collection of related sentences a word graph is constructed as follows: Initially, every sentence is augmented by a preceding start token S and a terminal end symbol E so that beginning and end of the sentences are preserved in the final graph.",30,31
12350,1817490,The second word of the first sentence also becomes a vertex v and the two vertices are connected with a directed edge v S → v. The procedure continues with the third word and so on until the end symbol E is reached.,39,40
12351,15251605,"Table 1 gives an example for the alignment of German Tochter [tɔxtər] 'daughter' and English daughter [dɔːtər]: Here, all corresponding segments are inserted in the same columns, while the velar fricative [x] of the German sequence which does not have a corresponding segment in the English word is represented by a gap symbol.",63,64
12352,15747255,"note intermediate states; the terminal symbol α corresponds to all words seen in the training set, and g( f .v) is a function for generating integer numbers given the value of a field f .",6,7
12353,15747255,"All non-terminals, save the start symbol S, have one or more features (shown in parentheses) that act as constraints, similar to number and gender agreement constraints in augmented syntactic rules.",8,9
12354,15747255,"Rule (1) denotes the expansion from the start symbol S to record R, which has the special 'start' record type (hence the notation R(start)).",10,11
12355,15747255,"Given a context-free grammar G = N, T, P, S (where N is the set of variables, T the set of terminals, P the set of production rules, and S ∈ N the start symbol) and an input string w, we can map the standard weighted CYK algorithm to a hypergraph as follows.",43,44
12356,15747255,"Each +LM item is of the form (n a b ), where a and b are boundary words of the generation string, and is a place-holder symbol for an elided part of that string, indicating a sub-generation part ranging from a to b. An example +LM deduction of a single hyperarc of the hypergraph in Figure 2 using bigrams is: (2) FS 1,2 (temp 1 , start) low : (w 1 , g 1 ), R 2,2 (temp 1 .t) around degrees : (w 2 , g 2 ) R 1,1 (skyCover 1 .t) low degrees : (w, g 1 g 2 ) w = w 1 + w 2 + e w + P lm (around | low) (3) where w 1 , w 2 are node weights, g 1 , g 2 are the corresponding sub-generations, e w is the weight of the hyperarc and w the weight of the resulting +LM item.",32,33
12357,53097966,"3.2), then it passes the embedded symbol to a unidirectional LSTM.",8,9
12358,53097966,"previously decoded symbol Start-of-sequence at the first timestep, then the last symbol produced by greedy decoding.",2,3
12359,53097966,"previously decoded symbol Start-of-sequence at the first timestep, then the last symbol produced by greedy decoding.",16,17
12360,245838374,"x), yard(?x) The symbol ?",6,7
12361,9403493,We can extend the reduction in the proof of Theorem 1 from 4k-weighted dominating set to extractive summarisation with respect to ROUGEn with budget 2 • (4k) by introducing a dummy symbol d into our documents and summaries for padding sentences.,35,36
12362,9403493,"The order-n sentence s n v is just s 1 v (first order sentence) with each vertex padded to the right by the string d (n−1) , and prefixed with d (n−1) to the resulting string, where d is a dummy symbol not in V .",50,51
12363,9403493,For V n we pad the right of each symbol in V 1 with the string d (n−1) and attach the same string as a prefix.,9,10
12364,233025340,"To enable the BERT module to capture the location of a quantity, we insert the special symbol ""$"" at the beginning and end of the Quantity span.",17,18
12365,233025340,"Subtask 3 and 5 (MeasuredEntity and HasQuantity Extraction) As done in the previous subtask to capture the location, we insert the special symbol ""$"" at the beginning and end of the quantity span.",25,26
12366,233025340,"The span of the MeasuredEntity related to Quantity enclosed in the ""$"" symbol is transformed into BIO / IOB format and used as the true-label for training the model.",14,15
12367,233025340,"We enclosed the Quantity span in ""$"" symbol and the MeasuredEntity span in ""#"" symbol.",9,10
12368,233025340,"We enclosed the Quantity span in ""$"" symbol and the MeasuredEntity span in ""#"" symbol.",18,19
12369,947703,"Note that our method is different from previous practice that simply replaces rare words with a special ""unknown"" symbol (Headden III et al.,",20,21
12370,947703,"Using POS tags instead of the ""unknown"" symbol to represent rare words can be helpful in the neural approach introduced below in that the learned word vectors are more informative.",9,10
12371,31382372,"If we assume the TAG is lexicalized (i.e., each elementary structure is anchored by at least one terminal symbol), then we can label the nodes of the derivation tree with the tree names and also the anchors of the elementary trees, and we obtain what is formally a dependency tree.",20,21
12372,9993758,"Decoding is done by choosing the single most probable symbol at each letter position, according to the final softmax layer.",9,10
12373,3137768,"A synchronous context-free grammar (SCFG) is a tuple G = (N, Σ, P, S), where N and Σ are finite, disjoint sets of nonterminal and terminal symbols, respectively, S ∈ N is the start symbol and P is a finite set of synchronous rules.",47,48
12374,3137768,"The symbol s is the label of the rule, and each rule is uniquely identified by its label.",1,2
12375,3137768,We assume without loss of generality that the start symbol S does not occur in the right-hand side of either component of any rule.,9,10
12376,3137768,The start symbol of G prefix is S ↓ .,2,3
12377,1322255,"Linear Context-Free Rewriting Systems For the purposes of this paper, a linear context-free rewriting system, henceforth LCFRS, is a construct G = (N, T, P, S), where N is an alphabet of nonterminal symbols in which each symbol A is associated with a positive integer ϕ(A) called its fan-out, T is an alphabet of terminal symbols, S ∈ N is a distinguished start symbol with ϕ(S) = 1; and P is a finite set of productions of the form p = A → f (A 1 , . . . ,",50,51
12378,1322255,"Linear Context-Free Rewriting Systems For the purposes of this paper, a linear context-free rewriting system, henceforth LCFRS, is a construct G = (N, T, P, S), where N is an alphabet of nonterminal symbols in which each symbol A is associated with a positive integer ϕ(A) called its fan-out, T is an alphabet of terminal symbols, S ∈ N is a distinguished start symbol with ϕ(S) = 1; and P is a finite set of productions of the form p = A → f (A 1 , . . . ,",81,82
12379,1322255,"We call this the characteristic string of the tuple, and an occurrence of the symbol $ a gap marker.",15,16
12380,5799193,"It is labeled with the special symbol Φ, which does not contribute to the word string accepted along a path.",6,7
12381,6683636,"1 github.com/batra-mlp-lab/lang-emerge 4 The Road to Compositionality This section details our key observation -that while the agents always successfully invent a language to solve the game with near-perfect accuracies, the invented languages are decidedly not compositional, interpretable, or 'natural' (e.g. A-BOT ignoring Q-BOT's utterances and simply encoding every object with a unique symbol if the vocabulary is sufficiently large).",65,66
12382,6683636,"We find that when |V A | is greater than the number of instances (64), the learned policy simply has A-BOT ignore what Q-BOT asks and instead convey the instance using a single symbol, e.g. token 1≡(red, square, filled).",40,41
12383,10188070,"The operations used are: (i) the deletion of a single symbol, (ii) the insertion of a single symbol, and (iii) the substitution of one symbol for another (Kruskal, 1999) .",13,14
12384,10188070,"The operations used are: (i) the deletion of a single symbol, (ii) the insertion of a single symbol, and (iii) the substitution of one symbol for another (Kruskal, 1999) .",23,24
12385,10188070,"The operations used are: (i) the deletion of a single symbol, (ii) the insertion of a single symbol, and (iii) the substitution of one symbol for another (Kruskal, 1999) .",33,34
12386,17142956,"Experimentally, we tried multiple ways of sequence generation, such as simply breaking a word into all it's characters, making symbols that are longest possible contiguous consonants but each vowel is a separate symbol, making symbols that are longest possible contiguous vowels but each consonant is a separate symbol, and the one given in the step 1 of Procedure 1.",36,37
12387,17142956,"Experimentally, we tried multiple ways of sequence generation, such as simply breaking a word into all it's characters, making symbols that are longest possible contiguous consonants but each vowel is a separate symbol, making symbols that are longest possible contiguous vowels but each consonant is a separate symbol, and the one given in the step 1 of Procedure 1.",52,53
12388,17142956,"An intuitive reasoning is that if each symbol of a sequence contains the longest possible contiguous 3 multiple words may have same vector representation, e.g. anagrams 4 we include character y in the vowel set vowels or consonants but not both, then it retains phonetic information of a word.",7,8
12389,8897045,"We replace numbers with the symbol ""CD"".",5,6
12390,14124677,Each type of symbol has its own embedding look-up table.,3,4
12391,14124677,"In our architecture (Figure 1 ), the input to the bi-LSTM encoder at step i is the concatenation of the embeddings of each typed symbol composing token i. The output for the same token is the concatenation of the forward and backward LSTM states at step i. We use a two-layer bi-LSTM encoder, the input to the second layer being the output of the first one.",28,29
12392,14124677,"These preconditions are described in Table 3 of Appendix A. a Following Kiperwasser and Goldberg (2016) , we stochastically replace a word by an unknown symbol with probability p(w) = α #{w}+α , where #{w} is the raw frequency of w in the training corpus.",27,28
12393,14124677,"If X is a temporary symbol and if B is empty, s 2 must not be a temporary symbol.",5,6
12394,14124677,"If X is a temporary symbol and if B is empty, s 2 must not be a temporary symbol.",19,20
12395,14124677,R-X s 1 is not a temporary symbol.,9,10
12396,14124677,L-X s 0 is not a temporary symbol.,9,10
12397,5912180,"For each a ∈ N , we have a parameter π a , which is the probability of a being the root symbol of a derivation.",22,23
12398,5912180,"The probability of a tree τ deriving a sentence in the language, written p(τ ), is calculated as the product of the probabilities of all rule occurrences in τ , times the parameter π a where a is the symbol at the root of τ .",41,42
12399,834762,"However, it is not obvious how best to represent strings as vectors-they have unbounded length, and furthermore the absolute position of a symbol is not usually significant in evaluating its contribution to the score.",26,27
12400,834762,"For example, suppose G is a single-state machine with self-loops accepting each symbol in the alphabet (i.e. a unigram machine).",17,18
12401,834762,"We would lower the weight of λ a to encourage H k to output more of the symbol a. However, if H k has a cyclic topology, then it could happen that a negative value of λ a could create a negative-weight cycle, in which the lowest-cost path through H k is infinitely long.",17,18
12402,834762,"As a WFSA, this would simply have one state represent each position, with arcs for each symbol going from position i to i + 1.",18,19
12403,834762,"To encode n-gram features in a WFSA, each state represents the (n−1)-gram history, and all arcs leaving the state represent the final symbol in the ngram, weighted by the score of that n-gram.",27,28
12404,834762,"For example, if the trigram abc has weight λ abc , then the trigram machine will encode this as an arc with the symbol c leaving the state representing ab, and this arc will have weight λ abc .",24,25
12405,834762,"For each arc labeled with a symbol a ∈ Σ, add competing ""substitution"" arcs labeled with the other symbols in Σ, and a competing ""deletion"" arc labeled with ; these have the same source and target as the original arc.",6,7
12406,834762,"Also, at each state, add a self-loop labeled with each symbol in Σ; these are ""insertion"" arcs.",14,15
12407,834762,"For each position in x * , we uniformly sample once among the three types of edits (substitution, insertion, deletion), and in the case of the first two, we uniformly sample from the vocabulary (excluding the current symbol for substitution).",44,45
12408,11400736,"For instance, on the source side, the Arabic token appears to the right of the nonterminal symbol in R 1 , R 2 and R 3 , while it is to the left of the nonterminal in R 4 and R 5 .",18,19
12409,11400736,"On the target side, differences are due to both nonterminal symbol position and the existence of determiners.",11,12
12410,8938702,The tail consonant can be empty; we assume a special symbol ∅ ∈ J t to denote an empty letter.,11,12
12411,9574259,"Specifically, we augment the character alphabet Σ with a distinguished symbol that indicates the language: id .",11,12
12412,9574259,"We then pre-and postpend this symbol to the character stream for every word before feeding the characters into the bidirectional LSTM Thus, we arrive at the new language-specific word embeddings, v i = LSTM id , c i 1 , . . . ,",7,8
12413,13276568,"The input word symbol S t of sentence S at each step t indexes into the embeddings matrix and the vector x t forms input to both GRU networks: x t = W e [:, S t ] (5) This input is mapped into two parallel hidden states, h V t along the visual pathway, and h T t along the textual pathway: h V t = GRU V (h V t−1 , x t ) (6) h T t = GRU T (h T t−1 , x t ) (7) The final hidden state along the visual pathway h V τ is then mapped to the predicted target image representation î by the fully connected layer with parameters V and the clipped rectifier activation: î = σ(Vh V τ ) (8) Each hidden state along the textual pathway h T t is used to predict the next symbol in the sentence S via a softmax layer with parameters L: p(S t+1 |S 1:t ) = softmax(Lh T t ) (9) The loss function whose gradient is backpropagated through time to the GRUs and the embeddings is a composite objective with terms penalizing error on the visual and the textual targets simultaneously: L(θ) = αL T (θ) + (1 − α)L V (θ) (10) where θ is the set of all IMAGINET parameters.",3,4
12414,13276568,"The input word symbol S t of sentence S at each step t indexes into the embeddings matrix and the vector x t forms input to both GRU networks: x t = W e [:, S t ] (5) This input is mapped into two parallel hidden states, h V t along the visual pathway, and h T t along the textual pathway: h V t = GRU V (h V t−1 , x t ) (6) h T t = GRU T (h T t−1 , x t ) (7) The final hidden state along the visual pathway h V τ is then mapped to the predicted target image representation î by the fully connected layer with parameters V and the clipped rectifier activation: î = σ(Vh V τ ) (8) Each hidden state along the textual pathway h T t is used to predict the next symbol in the sentence S via a softmax layer with parameters L: p(S t+1 |S 1:t ) = softmax(Lh T t ) (9) The loss function whose gradient is backpropagated through time to the GRUs and the embeddings is a composite objective with terms penalizing error on the visual and the textual targets simultaneously: L(θ) = αL T (θ) + (1 − α)L V (θ) (10) where θ is the set of all IMAGINET parameters.",163,164
12415,910689,"Table 6 shows the results of sign test, where the symbol > indicates that the method in the row significantly (with p value < 0.05) improves the performance of the method in the column, and the symbol indicates that the performance improvement is extremely significant (with p value < 0.01).",11,12
12416,910689,"Table 6 shows the results of sign test, where the symbol > indicates that the method in the row significantly (with p value < 0.05) improves the performance of the method in the column, and the symbol indicates that the performance improvement is extremely significant (with p value < 0.01).",40,41
12417,1282002,Out-of-vocabulary words were replaced with a special unknown symbol.,12,13
12418,1520275,"The decoder is an RNN that uses a conditional GRU (cGRU, more details in §2.2) with an attention mechanism to generate a word y t at each time-step t. The cGRU uses it's previous hidden state s t−1 , the whole sequence of source annotations C and the previously decoded symbol y t−1 in order to update it's hidden state s t : s t = cGRU (s t−1 , y t−1 , C) (2) In the process, the cGRU also computes a timedependent context vector c t .",57,58
12419,1520275,Both s t and c t are further used to decode the next symbol.,13,14
12420,8571670,"The special gap symbol is denoted by '−' and does not belong to Σ. Let A = (a ij ) be a K × N f matrix, where a ij ∈ Σ ∪ {−}, and the i th row has exactly (N f − N i ) gaps and is identical to S i if we ignore the gaps.",3,4
12421,8571670,Every column of A must have at least one non-gap symbol.,12,13
12422,8571670,"For the caption alignment task, we treat each individual word as a symbol in our alphabet Σ. The substitution cost for two words is estimated based on the edit distance between two words.",13,14
12423,8571670,We do that via majority voting at each position of the alignment matrix containing a nongap symbol.,16,17
12424,6163451,We use formulaperspective pairs from the dataset to create a sequence-to-sequence task: the input is composed using the formula's multiplier and descriptions of its tuples connected with the symbol '*'; the output is the perspective (Figure 7 ).,34,35
12425,1380191,"Head-driven strategies begin with one rhs symbol, and add one nonterminal at a time.",8,9
12426,1380191,"The nonterminal S is the start symbol, with f (S) = 1.",6,7
12427,885002,"Figure 20 shows examples: f root is for the root node, in which the phrase symbol is S and the surface form, part-of-speech, and lexical entry of the lexical head are saw, VBD, and a transitive verb, respectively.",17,18
12428,7075290,"Thus the score of the word is: S(w i ) = − 1 k + 1 i+k j=i log P(w j | w j−1 w j−2 ) In our experiments, k = 2 since we have a trigram model, though in cases where the target word is the last word in the string, k = 1, because there only the end-of-string symbol must be predicted in addition to the expansion.",72,73
12429,1483983,"An adaptor grammar is a tuple A = G, M, a, b, α , which contains: (i) a context-free grammar G = W, N, R, S where W is the set of terminals, N is the set of nonterminals, R is a set of production rules, and S ∈ N is the start symbol-we denote by R A the subset of R with left-hand side A; (ii) a set of adapted nonterminals, M ⊆ N; and (iii) parameters a, b and α, which are described below.",68,69
12430,1483983,"The symbol z refers to the collection of {z A | A ∈ M}, and z 1:n refers to the derivations of the data x. Note that the distribution in 2(c) is defined with the GEM distribution, as mentioned earlier.",1,2
12431,1483983,"The grammaton G(A, s) is the contextfree grammar with the start symbol A and the rules R A ∪ B∈U R B ∪ A→B 1 ...Bn∈R A i∈{i|B i ∈M} {B i → t | t ⊆ s}.",13,14
12432,12851711,"In this example, the Japanese word ""緑茶"" is aligned with the English words ""green"" and ""tea"", and the English word sequence ""a cup of"" is aligned with a special symbol ""null"", which is not explicitly translated into any Japanese words.",39,40
12433,12851711,"We added another special symbol ""eos"" for both languages and inserted it at the end of all the sentences.",4,5
12434,3721101,"It is defined by specifying, for each symbol r ∈ Σ of arity k, a term h(r) ∈ T ∆ (X k ) in which each variable occurs exactly once.",8,9
12435,3721101,"This symbol-wise mapping is lifted to entire trees by letting h(r(t 1 , . . . ,",1,2
12436,3721101,"A ∆-algebra A consists of a nonempty set A, called the domain, and for each symbol f ∈ ∆ with arity k, a function f A : A k → A, the operation associated with f .",17,18
12437,3721101,"X k ), where the terminal symbol r ∈ Σ is of arity k and X, X 1 , . . . ,",7,8
12438,3721101,"We assume that no two rules of M use the same terminal symbol; this is generally not required in tree automata, but every IRTG can be brought into this convenient form.",12,13
12439,3721101,"The agenda is initialized with all state pairs T X, for which M L has a rule T → r and M R has a rule X → r for some nullary symbol r ∈ Σ. Then, while there are state pairs left on the agenda, Alg.",33,34
12440,3721101,"The query can then iterate over the set chi(S r , 3−i), to check for each state X in that set whether M R actually contains a rule with terminal symbol r and children X and X (in the right order).",32,33
12441,3721101,It contains a single operation symbol * (plus variables); the homomorphism only replaces one symbol with another.,5,6
12442,3721101,It contains a single operation symbol * (plus variables); the homomorphism only replaces one symbol with another.,17,18
12443,3721101,These ask for all rules with parent state X and terminal symbol r. Such queries completely avoid the problem of finding siblings in M R .,11,12
12444,3721101,"For instance, if we query I from Section 2 top-down for rules with the parent [1, 5] and symbol r 1 , it will enumerate the rules [4, 5] ), without ever considering any other combination of child states.",24,25
12445,3721101,Line 6 then does a bottom-up lookup of M L rules with the terminal symbol r and with child states that are partners of X 1 and X 2 .,16,17
12446,3721101,"This can be extremely wasteful when M R is the invhom of a decomposition automaton, because it may contain a great number of rules that have the same states and only differ in the terminal symbol r. For instance, when we encode a context-free grammar as an IRTG, for every rule r of the form A → B C we have h(r) = * (x 1 , x 2 ).",36,37
12447,3721101,"The homorphism in the corresponding IRTG assigns every terminal symbol a constant or the term * (x 1 , x 2 ), as in Fig.",9,10
12448,18046005,"p(T ) = i p(w hi , w ni , r i |η), where η is a condition of the probability, which is usually the nonterminal symbol of the mother node.",29,30
12449,17354729,"PPM is based on conditional probabilities of the upcoming symbol given several previous symbols (Cleary and Witten, 1984) .",9,10
12450,9365010,"We also use features that indicate the nonexistence of a word, which we found effective in preliminary experiments; feature f l, w(x, y) is 1 if l ∈ y and w is not included in the document x. Words are stemmed and number expressions are normalized to a unique symbol.",54,55
12451,511275,"This phrase pair is turned into an SCFG rule by assigning a lefthand side nonterminal symbol, corresponding to the syntactic constituent that dominates the English phrase.",15,16
12452,14074691,"The probability of a question given a label is: P (w|L; θ) = |w| j=1 f P (f w j ; θ)#(f, j, L, |w|) The final term #(f, j, L, |w|) is the fraction of trees with root L and |w| terminals where the jth terminal symbol is generated by nonterminal f .",62,63
12453,14074691,This approach ensures that each nonterminal symbol has a proper conditional probability distribution over rules.,6,7
12454,7045397,"In The Σ wildcard matches any symbol in Σ x ; the ""?""",6,7
12455,7045397,wildcard matches the empty string ε or any symbol in Σ x .,8,9
12456,7045397,"By contrast, we use |Σ x | + 1 states, where each state records the most recent output character (initially, a special ""beginning-of-string"" symbol $).",33,34
12457,2260410,The segmentation boundaries are marked as a distinguished symbol in the target string.,8,9
12458,2591303,"The ALG(L) system represents a version of the ALG system in which the IPA symbols are respelled using combinations of the 26 letters of the Roman alphabet, with the morpheme boundary symbol removed.",33,34
12459,8174613,"The decoder RNN is to unfold the context vector c into the target sequence, through the following dynamics and prediction model: s t = f (y t−1 , s t−1 , c) p(y t |y <t , X) = g(y t−1 , s t , c) (2) where s t is the RNN state at time t, y t is the predicted target symbol at t (through function g(•)) with y <t denoting the history {y 1 , ..., y t−1 }.",73,74
12460,8174613,"State Update COPYNET updates each decoding state s t with the previous state s t−1 , the previous symbol y t−1 and the context vector c t following Eq. (",18,19
12461,243838026,Each token from the complex side of the alignment (A) is converted to a symbol formed by its respective POS tag and an index equivalent to its order of appearance in the sentence.,16,17
12462,243838026,"To use BERT's MLM, we replace such items one at a time with the <mask> symbol while executing the predictions to fill the gaps.",19,20
12463,243838026,"More specifically, given an output sequence of simplified sentences S, for each item i n not reconverted from the complex input sentence C, we mask i n on S using the <mask> symbol and feed S into MLM.",37,38
12464,13972671,"While NMT offers many advantages over traditional phrase-based approaches, such as small memory footprint and simple decoder implementation, nearly all previous work in NMT has used quite restricted vocabularies, crudely treating all other words the same with an <unk> symbol.",46,47
12465,13972671,"Other words not inside these lists are represented by a universal symbol <unk>, one per language.",11,12
12466,13972671,"For example, in Figure 1 , we run our deep character-based LSTM over 'c', 'u', 't', 'e', and '_' (the boundary symbol).",39,40
12467,14692785,"The symbols sigh and tanh denote hard sigmoid and hard tan, respectively, and the symbol denotes a element-wise product of two vectors.",16,17
12468,7271623,"This is done using the probability of an orientation given the phrase pair pp = [src, tgt] extending the hypothesis: 2 P (o|pp) ≈ cnt(o, pp) o cnt(o, pp) (1) where o ∈ {M, S, D}, cnt uses simple heuristics on word-alignments to count phrase pairs and their orientations, and the ≈ symbol allows for smoothing.",72,73
12469,8110758,"Sample connection descriptions from Mechanical Turk workers are as follows: ""Cook connects caring and flame because it is related to flame as flames are used in cooking and cooking can be a symbol of caring for someone with good food."" """,34,35
12470,15162600,"Every symbol in x, from beginning to end, represents the next line segment in that path; an occurrence of a represents a line segment from the previous point (i, j) to the next point (i + 1, j), a represents a line segment from (i, j) to (i − 1, j), b represents a line segment from (i, j) to (i, j + 1), and b represents a line segment from (i, j) to (i, j − 1).",1,2
12471,14831420,"The symbol "" * * "" indicates a significance level of p < 0.0001, and "" * "" a significance level of p < 0.05.",1,2
12472,6491057,"Formally, a 2-path is a 3tuple x, Z, y where x and y are a symbol in Σ and Z is a subset of Σ. The 2-paths of a string w = σ 0 σ 1 . . .",20,21
12473,6491057,"Intuitively, it gives the set of symbols one must 'travel over' in order to get from one symbol to another in a string.",20,21
12474,6491057,"Here, σ i is guaranteed to not be taken off of the tier hypothesis, because condition (a) for removing a symbol from the tier requires that there exists some path σ j , X, σ i and σ i , X , σ j where X, X ⊆ H i .",24,25
12475,9026691,Given source string s the predicted target string t 1 The input string is extended with an empty symbol to account for the cases where an insertion is needed at the end of the string.,18,19
12476,7925943,"2010; 2013) : p(z, w, δ|α, β, a, b, y) = D ∏ i=1 ( Beta K (α + ∑ AS i t ig ) Beta K (α) ∏ AS i ( (b|a) t ig• (b) n ig• K ∏ k=1 S ( n igk , t igk , a ) ( n igk t igk ) −1 ) ) K ∏ k=1 ( Beta W e (β e + M e k ) Beta W e (β e ) Beta W f (β f + M f k ) Beta W f (β f ) ) , where Beta K (•) and Beta W l (•) are K-and |W l |dimensional beta functions, respectively, (b|a) n is the Pochhammer symbol 3 , and (b) n is given by (b|1) n .",150,151
12477,12687036,Each symbol (word or label) in the s-expression is assigned a lambda expression.,1,2
12478,13401571,"Thus, conditioned on a set of segments s given by ρ, the joint posterior distribution of w, z and δ is computed as p(z, w, δ | ρ, Φ, a, b, γ) = d Beta K α + s t d,s Beta K (α) k Beta W (γ + M k ) Beta W (γ) d s∈s (b|a) T d,s (b) N d,s k S n d,s,k t d,s,k ,a n d,s,k t d,s,k −1 , (3) where Beta K (•) is a K-dimension Beta function, (x|y) n the Pochhammer symbol 2 , and S n t,a the generalised Stirling number of the second kind (Hsu and Shiue, 1998) 3 precomputed in a table so cost-2 The Pochhammer symbol (x|y)n denotes the rising factorial with a specified increment, i.e., y. It is defined as (x|y)n = x(x + y)...(x + (n − 1)y).",141,142
12479,13401571,"Thus, conditioned on a set of segments s given by ρ, the joint posterior distribution of w, z and δ is computed as p(z, w, δ | ρ, Φ, a, b, γ) = d Beta K α + s t d,s Beta K (α) k Beta W (γ + M k ) Beta W (γ) d s∈s (b|a) T d,s (b) N d,s k S n d,s,k t d,s,k ,a n d,s,k t d,s,k −1 , (3) where Beta K (•) is a K-dimension Beta function, (x|y) n the Pochhammer symbol 2 , and S n t,a the generalised Stirling number of the second kind (Hsu and Shiue, 1998) 3 precomputed in a table so cost-2 The Pochhammer symbol (x|y)n denotes the rising factorial with a specified increment, i.e., y. It is defined as (x|y)n = x(x + y)...(x + (n − 1)y).",174,175
12480,13939903,"In the first tree, a temporary symbol rewrites as two tempo- A[h] C:[c] A:[h] A[h] C[h] A:[a] Figure 3 : Examples of ill-formed binary trees rary symbols.",7,8
12481,13939903,"In the second one, the head of a temporary symbol is not the head of its direct parent.",10,11
12482,13939903,The first layer h (0) is a lookup layer which concatenates the embeddings of each typed symbol extracted from a configuration.,18,19
12483,13939903,"Thus, θ includes the weights and biases for each layer (W (h) , W (o) , b (h) , b (o) ), and the embedding lookup table for each symbol type.",40,41
12484,13939903,The full set of templates is specified in Table 6 of Annex A. The sequence of symbols that forms the input of the network is the instanciation of each position described in this table with a discrete symbol.,37,38
12485,13939903,"The third symbol on the stack being temporary symbol D:, the reduction to a temporary symbol will jeopardize the reachability of (C, m, j) because reduc-tions are not possible when the two symbols at the top of the stack are temporary symbols.",2,3
12486,13939903,"The third symbol on the stack being temporary symbol D:, the reduction to a temporary symbol will jeopardize the reachability of (C, m, j) because reduc-tions are not possible when the two symbols at the top of the stack are temporary symbols.",8,9
12487,13939903,"The third symbol on the stack being temporary symbol D:, the reduction to a temporary symbol will jeopardize the reachability of (C, m, j) because reduc-tions are not possible when the two symbols at the top of the stack are temporary symbols.",17,18
12488,13939903,"The best course of action is then a reduction to any non-temporary symbol, so as to keep (C, m, j) reachable.",14,15
12489,13939903,"Practically, if such constituent exists and if the third symbol on the stack is a temporary symbol, then do not predict a temporary symbol. •",10,11
12490,13939903,"Practically, if such constituent exists and if the third symbol on the stack is a temporary symbol, then do not predict a temporary symbol. •",17,18
12491,13939903,"Practically, if such constituent exists and if the third symbol on the stack is a temporary symbol, then do not predict a temporary symbol. •",25,26
12492,13939903,"This should not harm training and improve precision for the unbinarized tree, as any non temporary Configuration stack Gold tree   symbol in the binarized tree corresponds to a constituent in the n-ary tree.",22,23
12493,13939903,The input of the neural network is the instanciation of each address by a discrete typed symbol.,16,17
12494,13939903,Each v i (Section 2) is the embedding of the i th instantiated symbol of this list.,15,16
12495,6298465,"In order to reduce the ambiguity, we devised a mapping shown in mapping reduces sets of Arabic letters that have the same corresponding English letter to a single higherfrequency symbol.",30,31
12496,80895,"Additionally, we treat the period symbol as a special discourse marker, denoted by ""o"".",6,7
12497,6756958,"A consecutive vowels sequence, formed by the basic vowel set, is treated as a new vowel symbol; Step 1 and 2 form the new vowel set; 3.",18,19
12498,6756958,"When 'w' follows 'a','e','o' and isn't followed by 'h', treat 'w' and the preceding vowel as a new vowel symbol; Step 2 and 3 form the basic vowel set; 4.",29,30
12499,6756958,"A consecutive vowels sequence which is formed by the basic vowel set is treated as a new vowel symbol, excepting 'iu ', 'eo', 'io', 'oi', 'ia', 'ui', 'ua', 'uo' ; Step 2, 3 and 4 form the new vowel set; 5.",18,19
12500,17859685,The second rule is that two words are connected by coordinating conjunctions but there is a negation symbol before one of them.,17,18
12501,16228781,"In order to provide information regarding the position of the features, we mark the beginning and the end of the word with a $ symbol.",25,26
12502,6583563,"For the caption alignment task, we treat each individual word as a symbol in our alphabet Σ. The special gap symbol '−' represents a missing word and does not belong to Σ. Let A = (a ij ) be a K × N f matrix, where a ij ∈ Σ ∪ {−}, and the i th row has exactly (N f − N i ) gaps and is identical to S i if we ignore Algorithm 1 MSA-A * Algorithm Require: K input sequences S = {S1, . . . ,",13,14
12503,6583563,"For the caption alignment task, we treat each individual word as a symbol in our alphabet Σ. The special gap symbol '−' represents a missing word and does not belong to Σ. Let A = (a ij ) be a K × N f matrix, where a ij ∈ Σ ∪ {−}, and the i th row has exactly (N f − N i ) gaps and is identical to S i if we ignore Algorithm 1 MSA-A * Algorithm Require: K input sequences S = {S1, . . . ,",21,22
12504,6583563,Every column of A must have at least one non-gap symbol.,12,13
12505,1499080,"This effectively helps us sidestep the difficulty of searching for the next output symbol under a large vocabulary, with lowfrequency words and named entities whose rep-resentations can be challenging to learn.",13,14
12506,16374706,"STOCHASTIC CONTEXT-FREE GRAMMARS A stochastic context-free grammar G is specified by the quintuple < VN, VT, R, S, P > where VN is a finite set of non-terminal symbols, VT is a finite set of terminal symbols, R is a set of rewrite rules, S is a start symbol in VN, and P is a parameter vector.",61,62
12507,15770107,DTs and PSTs as Representation Types Assume we have two disjoint symbol sets: a set of terminal symbols which contains the words of the language we are describing; and a set of nonterminal symbols.,11,12
12508,203117077,"In particular, we split tag sequences into component tags, and append them at both the beginning and end of the lemma, treating each of them as an atomic symbol.",31,32
12509,5994263,"Terminal productions of the form A → e/f produce a token in each stream, or a token in one stream with the null symbol ∅ in the other.",26,27
12510,5994263,"Also, we can easily incorporate links to ∅ by including the symbol among our terminals.",12,13
12511,214079,"For a sentence with N words, this approach will lead to 2N additional START/STOP symbols (with one START and one STOP symbol for each word).",25,26
12512,10505260,"In our implementation, when the standard deviation of a subsequence time-series is less than a pre-set threshold (a very small number), all of its segments will be assigned the same symbol.",38,39
12513,10505260,"One intuitive strategy is inactivity detection (i.e., flat period can be represented with a single symbol).",17,18
12514,16766006,"Therefore, they are unable to translate rare words, and out-of-vocabulary (OOV) words are replaced with UNK symbol.",24,25
12515,16766006,"By replacing all the OOV words with the same UNK symbol, useful information is discarded, resulting in systems that are not able to correct misspelled words or even keep some of the error-free original words, as in the following examples (OOV words are underlined): Original sentence ... I am goign to make a plan ... System hypothesis ... I am UNK to make a plan ... Gold standard ... I am going to make a plan ... Original sentence I suggest you visit first the cathedral of "" Le Seu d'Mrgell "" because it is the most emblematic building in the area .",10,11
12516,429415,"Background Problem Definition Given a sequence of characters c = (c 1 , ..., c #c ), the task of word segmentation and POS tagging is to predict a sequence of word and POS tag pairs y = ( w 1 , p 1 , w #y , p #y ), where w i is a word, p i is its POS tag, and a ""#"" symbol denotes the number of elements in each variable.",77,78
12517,429415,"In this table, the symbol ""C"" means sub-word content features while the symbol ""T"" means IOB-style POS tag features.",5,6
12518,429415,"In this table, the symbol ""C"" means sub-word content features while the symbol ""T"" means IOB-style POS tag features.",17,18
12519,15126078,"During training, words that occur less than 6 times are substituted with the symbol UNKNOWN.",14,15
12520,15126078,"In testing, unknown words are generated after the generation of symbol UNKNOWN, and we define their probability by a first-order Markov model.",11,12
12521,15126078,"That is, given a flat word w = c 1 c 2 • • • c n not seen in training, we define its probability conditioned with the part-of-speech p as Pr(w|p) = n+1 i=1 Pr(c i |c i−1 , p) (7) where c 0 is taken to be a START symbol indicating the left boundary of a word and c n+1 is the STOP symbol to indicate the right boundary.",61,62
12522,15126078,"That is, given a flat word w = c 1 c 2 • • • c n not seen in training, we define its probability conditioned with the part-of-speech p as Pr(w|p) = n+1 i=1 Pr(c i |c i−1 , p) (7) where c 0 is taken to be a START symbol indicating the left boundary of a word and c n+1 is the STOP symbol to indicate the right boundary.",75,76
12523,244054458,The DBA does not allow a beam to generate the end of sentence symbol unless the constraints are met.,13,14
12524,1498096,Our contributions are that we introduce the novel idea of rare-event clustering into language modeling and that we show that the modified model performs better than a strong word-trigram symbol denotation [[w]] w (sum over all unigrams w) c(w i j ) count of w i j n 1+ (•w i j ) # of distinct w occurring before w i j Table 1 : Notation used for Kneser-Ney.,33,34
12525,1498096,"Note that our notation deviates from C&G in that they use the single symbol D 1 for the three different values d ′ (1), d ′′ (1), and d ′′′ (1) etc.",13,14
12526,6592010,"We achieve this in practice by first changing all symbols in X to an arbitrary symbol (a in this case), removing at least one symbol from the end, and using this intermediate result to remove from X all strings shorter than the maximum.",15,16
12527,6592010,"We achieve this in practice by first changing all symbols in X to an arbitrary symbol (a in this case), removing at least one symbol from the end, and using this intermediate result to remove from X all strings shorter than the maximum.",27,28
12528,12964671,"9) where (a) n denotes the Pochhammer symbol, which is defined as (a) n = a(a + 1) . . . (",10,11
12529,9651031,Table 2 shows our results: significantly different scores are marked with the same symbol where relevant (per column).,14,15
12530,14707273,"The main idea is to exploit a property of formal analogies (Lepage and Shin-ichi, 1996) : [x : y :: z : u] ⇒ |x| c + |u| c = |y| c + |z| c ∀c ∈ A (1) where A is the (input) alphabet, and |x| c stands for the number of occurrences of symbol c in x. The strategy consists in first selecting a form x in the input space.",68,69
12531,14707273,"This strategy relies on the fact that one can efficiently identify the pairs y, z that satisfy a set of constraints on symbol counts.",23,24
12532,14707273,"1 In particular, spaces in forms were considered as any ordinary symbol.",12,13
12533,12341050,"For example, an otherwise purely alphabetic Wikipedia scheme employs the symbol @ for the vowel schwa.",11,12
12534,12341050,"In fact, Fraser (1997) identifies the schwa symbol as the cause of many pronunciation errors.",10,11
12535,12341050,"The alignment is restricted to matching each letter symbol to at most one phoneme, and is derived with the ALINE phonetic aligner (Kondrak, 2000) , which has been shown to outperform other 1-1 alignment methods (Jiampojamarn and Kondrak, 2010) .",8,9
12536,19718228,"All tokens were lowercased, and all numbers were converted to a placeholder symbol <NUM>.",13,14
12537,53602910,"After the FST reads the symbol under the read head, three things occur: • The internal state of the FST changes. •",5,6
12538,53602910,"The meaning of the configuration (wqx, u) is that the input to T is wx and the machine is currently in state q with the read head on the first symbol of x (or has fallen off the right edge of the input tape if x = λ) and that u is currently written on the output tape.",33,34
12539,53602910,"The symbol Σ stands for any segment in the alphabet except for { , }.",1,2
12540,53602910,"The boundary symbol ∼ is a symbol in the output alphabet Γ, and is not necessary.",2,3
12541,53602910,"The boundary symbol ∼ is a symbol in the output alphabet Γ, and is not necessary.",6,7
12542,53602910,"In the input string, we underline the input symbol which FST will read next.",9,10
12543,53602910,The symbol λ marks the empty string.,1,2
12544,53602910,"But for other tuples, the form of the arc is: input state input symbol:output string − −−−−−−−−−−−−− → direction output state Partial reduplication Partial reduplication processes are also very common.",15,16
12545,53602910,"The symbol V M stands for monophthongs, V D for diphthongs, and C for consonants.",1,2
12546,53602910,"a. q 0 start q 1 q 2 a:λ b:ab b. q 0 start q 1 q 2 a:a b:b Given a string-to-string function, the origin semantics of a function is the origin information of each symbol o n in the output string.",49,50
12547,53135982,"Feature values are assigned based on Hayes (2009) and Moisik and Esling (2011) , and indicate either the presence, absence, or nonapplicability of an articulatory-acoustic feature for each IPA symbol.",37,38
12548,11703771,The system generalizes concrete inflection tables by associating the common symbol subsequences shared by the words (the LCS) with vari- ables.,10,11
12549,6716178,"In greedy decoding, we follow the conditional dependency path and pick the symbol with the highest conditional probability so far at each node.",13,14
12550,6716178,This is equivalent to picking the best symbol one at a time from left to right in conditional language modelling.,7,8
12551,6716178,"When all the hypotheses terminate (outputting the end-of-thesentence symbol), it returns the hypothesis with the highest log-probability.",13,14
12552,6716178,"We defined the influence as the KL divergence between the conditional distributions without the trainable greedy decoding and with the trainable greedy decoding, assuming the fixed previous hidden state and target symbol.",32,33
12553,6716178,Manual inspection of these examples as well as others has revealed that the trainable greedy decoder focuses on fixing prepositions and removing any unnecessary symbol generation.,24,25
12554,53319542,"Katakana is more accurately described as a way to write the set of Japanese morae rather than the set of Japanese syllables, as each symbol represents not a syllable but a unit of sound of Japanese speech.",25,26
12555,16714964,"Using ':' to denote a temporary symbol in Figure 1 (right), we observe for instance that the root of a tree cannot be temporary and two siblings cannot be temporaries either.",8,9
12556,16714964,"In short S denotes the shift action, RU (X) denotes an unary reduction by terminal X, RL(X) denotes a binary reduction by terminal X with left symbol marked as head, and RR(X) denotes a binary reduction by terminal X with right symbol marked as head.",31,32
12557,16714964,"In short S denotes the shift action, RU (X) denotes an unary reduction by terminal X, RL(X) denotes a binary reduction by terminal X with left symbol marked as head, and RR(X) denotes a binary reduction by terminal X with right symbol marked as head.",48,49
12558,16714964,"s i .c t , s i .c l , s i .c r denote the root left child and right child categories of tree and s i .w t , s i .w l , s i .w r denote the root, the left child and right child terminals of this tree such that a node s i .c • [s i .w • ] denotes a non terminal 2-LCFG symbol at node s i in the stack.",75,76
12559,16714964,This kind of structure licences the following new patterns of 2-CFG rules: A → B t and A → t B where t denotes a terminal symbol (in this case a tag).,29,30
12560,16714964,"The general grammar G (base) m encodes a matrix grammar that does not specifically handle compound words and for which equivalence classes are [a] the axiom symbol, [n] non terminal symbols and [w] terminal symbols.",30,31
12561,18623726,The symbol matching does not relate acoustic similarity with recognition.,1,2
12562,18623726,"W P : Phone sequence corresponding to word sequence W; LEV(,): Levenshtein distance between two symbol strings;| ⋅ |: Number of symbols in a string.)",19,20
12563,16103660,"Given such contexts, patterns for every word pair are generated by replacing the two target words w i and w j with placeholder characters X and Y , and replacing none, some or all of the other words by their associated part-of-speech tag or a wildcard symbol.",52,53
12564,15076761,"Linguistic words are made up of one or more syllables and are also separated by the same symbol, ""tsheg"".",17,18
12565,17028354,"Another special symbol EOL (End Of Label) is appended to short paths, so that all hierarchical paths have the same length.",2,3
12566,5060178,"Formally, let us express each sentence of D as a sequence of word tokens, s n := t n, Ln =0 , where t n,0 ≡ $ is a dummy symbol.",35,36
12567,5060178,"By convention, the dummy symbol is included if and only if the remaining compression is non-empty.",5,6
12568,5060178,"Compression Model For the compression score function, we follow Martins and Smith (2009) and decompose it as a sum of local score functions ρ n, defined on dependency arcs: h n (z n ) := Ln =1 ρ n, (z n, , z n,π( ) ), (8) where π( ) denotes the index of the word which is the parent of the th word in the dependency tree (by convention, the root of the tree is the dummy symbol).",97,98
12569,16383573,"The subsequence formulation also allows us to map each phonetic symbol (for example, from International Phonetic Alphabet [IPA]) to an intermediary phonetic alphabet also.",10,11
12570,16383573,We formulated an approach to integrate phonetic features of a phonetic symbol into the feature vector and showed that it beats the system of HK at cog-nate identification at cross-validation and concepts subsets experiments.,11,12
12571,52299393,"An additional copy feature generalizes the identity function from source to target, which is useful if there is an overlap between the input and output symbol sets.",26,27
12572,2865125,"We convert accented characters to their unaccented counterparts followed by a special symbol (e.g. cantó → canto'), with no loss of information.",12,13
12573,8199270,Our system treats each subtag as an indivisible atomic symbol.,9,10
12574,8199270,"After splitting the tags, we perform an additional operation of prepending the part-of-speech symbol to each subtag, in order to distinguish between identically named subtags that correspond to different parts of speech (e.g., V:SG vs. N:SG).",18,19
12575,8199270,"In this approach, se is a single-symbol affix of the lemma which is substituted by me and transposed in the output sequence.",9,10
12576,16892138,"x k } for each k ∈ N. A ranked set ∆ is a set of symbols, associated with a rank function assigning a number rk ∆ (δ) ∈ N to each symbol δ ∈ ∆. A ranked alphabet is a ranked set with a finite number of symbols.",35,36
12577,16892138,"1987) , we define a linear context-free rewriting system (LCFRS) as a tuple G = (N, S, Σ, P ), where N is a ranked alphabet of nonterminals, S ∈ N (1) is the start symbol, Σ = Σ (0) is a ranked alphabet of terminals (Σ ∩ N = ∅), and P is a finite set of rules, each of the form: A 0 (s 1,k 0 ) → A 1 (x 1,m 1 ), A 2 (x m 1 +1,m 2 ), . . . ,",48,49
12578,16892138,"The start symbol S has only one argument, which is synthesized, i.e. rk N (S) = s-rk(S) = 1 and i-rk(S) = 0.",2,3
12579,16870294,"Starting from an empty root node, a partial key is extended by one character in each iteration, so that each level of the search tree corresponds to a unique ciphertext symbol.",32,33
12580,18504651,The n-grams are extracted from each sentence by regarding the whole sentence as a large word / string and replacing the delimited symbol (i.e. white space) with a special uppercase character 'S'.,24,25
12581,15689456,"Other than the symbol at the root of the tree, the only constituents with span length greater than L in the binarized tree will be labeled with these special binarization non-  hedge-parsing accuracy, as it has access to rich contextual information (as compared to grammars trained on transformed trees).",3,4
12582,15689456,"Other than the symbol at the root of the tree, the only constituents with span length greater than L in the binarized tree will be labeled with these special binarization non-terminals.",3,4
12583,624106,We forced experts to make similar mistakes by making them select an adjacent alphabet symbol in case of an error.,14,15
12584,624106,"For example, when a mistake was made on a symbol b, the expert prediction was forced to be either a or c. The second artificial data set, ADS2, modeled the case of rather poor experts.",10,11
12585,11625068,The second task is to map each symbol of the ciphertext to the corresponding letter in the identified language.,7,8
12586,11625068,"Tiltman (1968) observes that each symbol behaves as if it had its own place in an ""order of precedence"" within words.",7,8
12587,11625068,"More formally, let P T be a discrete probability distribution where P T (i) is the probability of a randomly selected symbol in a text T being the i th most frequent symbol.",24,25
12588,11625068,"More formally, let P T be a discrete probability distribution where P T (i) is the probability of a randomly selected symbol in a text T being the i th most frequent symbol.",35,36
12589,11625068,"The algorithm repeatedly attempts to improve the current key k by considering the ""best"" swaps of ciphertext symbol pairs within the key (if the key is viewed as a permutation of the alphabet, such a swap is a transposition).",19,20
12590,11625068,The best swaps are defined as those that involve a symbol occurring among the 10 least common bigrams in the decipherment induced by the current key.,10,11
12591,12769731,"6 Additional Details of the Algorithm 6.1 Recovery of the π and q Parameters The recovery of the π and q parameters relies on the following additional (but benign) assumptions on the functions τ and ρ: 1) For any inside tree t such that t is a unary rule of the form a → x, the function τ is defined as τ (t) = t. 3 2) The set of outside tree features G contains a special symbol 2, and g(o) = 2 if and only if the outside tree o is derived from a non-terminal node at the root of a skeletal tree.",85,86
12592,16217436,"Pinyin is originally designed as the phonetic symbol of a Chinese character (based on the standard modern Chinese, mandarin) , using Latin letters as its syllable notation.",7,8
12593,11368202,Context features bind an output symbol with input n-grams in a focus window centred around the input-output alignment; the input n-grams represent the context in which the output character is generated.,5,6
12594,220274104,"Since one word's [STOP] symbol indicates the next word's [START] symbol, we combine the two symbols into a single [SEP] symbol, which can be interpreted as a word boundary marker.",7,8
12595,220274104,"Since one word's [STOP] symbol indicates the next word's [START] symbol, we combine the two symbols into a single [SEP] symbol, which can be interpreted as a word boundary marker.",16,17
12596,220274104,"Since one word's [STOP] symbol indicates the next word's [START] symbol, we combine the two symbols into a single [SEP] symbol, which can be interpreted as a word boundary marker.",29,30
12597,15548746,"Rather, each relation node R j attempts to model the relation of one single constituent U j , by taking U j 's left and right subtrees U j,L and U j,R as its first-layer nodes; if U j is a single EDU, then the first-layer node of R j is simply U j , and R j is a special relation symbol LEAF 3 .",73,74
12598,15412473,"Conventional sequence encoders simply replace rare words with a special unknown word symbol (Luong et al.,",12,13
12599,2657832,"In addition, the rank rk(ρ, p) of the replaced nonterminal should match the rank rk( t, u ) of the pre-translation and the nonterminals in the right-hand side that are aligned to p should be replaced by the translation that the alignment requests, provided that the nonterminal matches with the root symbol of the requested translation.",61,62
12600,2657832,"Since the leaf language of every regular tree language is context-free and regular tree languages are closed under intersection (needed to single out the translations that have the symbol Y at the root), this also proves that τ (R) c (T Σ ) is not regular.",31,32
12601,11502054,"XTOP allow rules with several (non-state and non-variable) symbols in the left-hand side (as in the rule of Figure 3 ), whereas a TOP rule contains exactly one symbol in the left-hand side.",38,39
12602,11502054,"The sum (1) can be infinite, which we avoid by simply requiring that all our XTOP R are producing, which means that r / ∈ Q[X] for every rule → r ∈ R. 2 In a producing XTOP R each rule application produces at least one output symbol, which limits the number n of rule applications to the size of the output tree u. A detailed exposition to XTOP R is presented by Arnold and Dauchet (1982) and Graehl et al. (",52,53
12603,11502054,"Suppose that n is a copying bound for the input XTOP R M , which means that no more than n rules are applied to each input symbol.",27,28
12604,11502054,The first XTOP R is actually a nondeterministic linear and nondeleting XTOP that annotates each input tree symbol with exactly n rules of M that are consistent with the state behavior of M .,17,18
12605,11502054,"Since we know all the rules that will potentially be applied for a certain symbol, we can make the assignment such that no annotated rule is used twice in the same derivation.",14,15
12606,11502054,"Only the weight and look-ahead of rules that are actually executed are applied (e.g., although we annotate n rules at the root symbol, we only execute the first rule and thus only apply its weight and lookahead).",26,27
12607,627938,The alphabet Σ and start symbol S retain their usual interpretations.,5,6
12608,2157456,"The symbol ""C"" as subcategory value in (8) indicates that the respective values of a potential filler and the head of the ]Jst must match whatever these values may be irl tile concrete case,. }",1,2
12609,234163935,"The role of the encoder in an encoder-decoder network is to read each input symbol one at a time, updating its hidden state at each timepoint.",16,17
12610,14678007,"This is in sharp contrast with standard bottom-up parsers (including the left corner parser), for which reduction to the start symbol cannot occur before the end of the sentence is reached.",25,26
12611,33887706,"A categorial grammar lacks rules; instead there is a can cellation ride mq)lieit ill the formalism: if X and Y are categories, then (X/Y) Y -' X. The lauguage of a categorial grammar is the set of ter minal strings with corresponding category symbol strings reducible by cancellation to the sentence symbol S. In [1] Ades and Steedman offer a form of categorial grammar in which some of the notations and concepts of the usual categorial grammar are modified.",51,52
12612,33887706,"A categorial grammar lacks rules; instead there is a can cellation ride mq)lieit ill the formalism: if X and Y are categories, then (X/Y) Y -' X. The lauguage of a categorial grammar is the set of ter minal strings with corresponding category symbol strings reducible by cancellation to the sentence symbol S. In [1] Ades and Steedman offer a form of categorial grammar in which some of the notations and concepts of the usual categorial grammar are modified.",59,60
12613,33887706,"The categorial component Cmlsists as before of a set VA of atomic categories including a distinguished symbol S, and a lexical function F mapping words to finite sets of categories.",16,17
12614,33887706,"Thus the symbol (A/(B/C )) of traditiolml categorial grammar is excluded, since A/B/C abbreviates ((A/B )/C ).",2,3
12615,33887706,"We might put in the CF rule U/V -> U/A A/V for each derivable category U/V and for each atomic category A, but in case there is a category like A/B/A, then any category symbol headed by A followed by B's and ended by A is a derivable category.",49,50
12616,33887706,"Notice, however, when the second category symbol ha~ exactly two atomic sym bols, i.e., is in form A/B, the FP rule does not violate the convention.",8,9
12617,33887706," Proof First, it is ea~sy to see that from the lexical categories, we cannot get any complex category headed by either A or C, and we can get atomic category symbol A or C only directly from the lexicon.",37,38
12618,33887706,"Sketch of Proof Sketch of Proof Second, each morpheme b would introduce one A and one C within a complex category symbol which must be cancelh~l out sooner or later in order to reduce the whole string to S. In gen eral, there are two ways for such A and C being cancelled: (1) with an A headed or C headed complex category by the FP rule, which is impossible in this example; (2) with a single atomic category A or C by either the F or P, s rule.",22,23
12619,33887706,"For even in the case of reduction by the unrestricted l,'P rule, the category symbol obtained by reduction is shorter than the corn biqed length of the two inputs t,o 1he rule.",15,16
12620,33887706,"The contradiction arises because the stack of their processor must be able to contain any derived eal~egory symbol of DA, and thus the size of the stack symbols is unlimited.",19,20
12621,250390639,"BPEmb is a variable-length encoding that views the text as a sequence of symbols, iteratively merging the pair with the highest frequency into a new symbol.",28,29
12622,1796,The first symbol is the input and the second is the output.,2,3
12623,1796,"We will use the following notation when pictorially describing a finite-state transducer: final states are depicted with two concentric circles; e represents the empty string; on a transition from state i to state j, a/b indicates a transition on input symbol a and output symbol(s) b; a a question mark (?)",48,49
12624,1796,"/b) originating at state i stands for any input symbol that does not appear as input symbol on any other outgoing arc from i. In this document, each depicted finitestate transducer will be assumed to have a single initial state, namely the leftmost state (usually labeled 0).",10,11
12625,1796,"/b) originating at state i stands for any input symbol that does not appear as input symbol on any other outgoing arc from i. In this document, each depicted finitestate transducer will be assumed to have a single initial state, namely the leftmost state (usually labeled 0).",17,18
12626,1796,"For example, from state 0 on input symbol vbd, two possible emissions are possible: vbn (from 0 to 2) and vbd (from 0 to 3).",8,9
12627,1796,"This nondeterminism is due to the rule vbd vbn NEXTTAG by, since this rule has to read the second symbol before it can know which symbol must be emitted.",20,21
12628,1796,"This nondeterminism is due to the rule vbd vbn NEXTTAG by, since this rule has to read the second symbol before it can know which symbol must be emitted.",26,27
12629,1796,"emits the empty symbol ¢, and postpones the emission of the output symbol.",3,4
12630,1796,"emits the empty symbol ¢, and postpones the emission of the output symbol.",13,14
12631,1796,"/?.13 On the other hand, if the input symbol, say a, can be processed at the initial state of T7, one doesn't know yet whether a will be the beginning of a word that can be transformed (e.g. ab) or whether it will be followed by a sequence that makes it impossible to apply the transformation (e.g. ac).",9,10
12632,1796,a is not an input symbol on any outgoing arc from this state.,5,6
12633,1796,"The next state to be considered is 2 and it is built like state 0, except that the symbol b should block the current output.",19,20
12634,1796,"the emission function is defined by: S,a= A A u.6(q,a,q') (q,u)~S q' Ed(q,a) This means that, for a given symbol, the set of possible emissions is obtained by concatenating the postponed emissions with the emission at the current state.",36,37
12635,1796,"At line 5, one takes all the possible input symbols w; here only a is possible, w' of line 6 is the output symbol, w'= e. ( A a(0,a,~')), ~'E{1,2} thus w' = a(0,a, 1) A 6(0,a,2) = b A c = e. Line 8 is then computed as follows: s'= U U ~ff{0} ~'E{1,2} thus S' = { (1, a (0, a, 1 )) } U { (2, 6 (0, a, 2) } = { (1, b), (2, c) }.",27,28
12636,1796,"The first symbol, h, at line 6, is such that w' is w' = A b. 6(1,h,~')) = bh.",2,3
12637,1796,The transition for the input symbol e is computed the same way.,5,6
12638,5363607,A headed string is a string of symbols containing one distinguished symbol referred to as the head of tile string.,11,12
12639,5363607,"Since the split point is not a symbol but a position between strings, separate operations corresponding to LL2 and LR2 are not needed.",7,8
12640,216848135,"4We use the symbol ~-to denote one principle ""taking precedence over"" another.",3,4
12641,8562498,"SE~IANTIC COMPONENTS The information to-be-communicated is represented as a set of four semantic units each of them being marked with the type-symbol (o -""object"", p -""parameter"", f -""function"", c -""constant"").",29,30
12642,8562498,The elementary component corresponds to a wordform and is traditionally represented by a lexeme symbol marked with syntactic and morphological features.,14,15
12643,2591750,"A special time interval is represented by the symbol now which stands for the speaking time, i.e., the interval during which the sentence is being uttered.",8,9
12644,17077652,We use the notation I)S(S) to state that the agent of discourse is S. The symbol --> means the context of left hand side is changed to the context of the right hand side as the result of context switch. (,16,17
12645,52287106,We try to limit the number of output actions that our network has to learn by grouping certain characters into common groups based on graphical features like accents or symbol modifiers.,29,30
12646,52287106,An input string is traversed left-to-right via an index pointer that indicates which symbol is currently being regarded.,17,18
12647,52287106,"The following actions are available: • EMIT s (for any symbol s): Appends s to the output string, irrespective of pointer symbol • COPY: Append the pointer symbol to the output string • PATCH x: Apply the graphical patch matrix x (cf.",12,13
12648,52287106,"The following actions are available: • EMIT s (for any symbol s): Appends s to the output string, irrespective of pointer symbol • COPY: Append the pointer symbol to the output string • PATCH x: Apply the graphical patch matrix x (cf.",25,26
12649,52287106,"The following actions are available: • EMIT s (for any symbol s): Appends s to the output string, irrespective of pointer symbol • COPY: Append the pointer symbol to the output string • PATCH x: Apply the graphical patch matrix x (cf.",32,33
12650,52287106,"Section 3) to the pointer symbol and append the result to the output string • MOVE: Increment the pointer to continue traversing the input word • EOW (end of word): Stop traversing the string and consider the current output string as the final inflection result Alignment We chose to implement our own mechanism to align input lemma and output strings, to accommodate for our patch concept.",6,7
12651,52287106,"A similar principle might apply to other symbols in languages unknown to the authors, so the proposed architecture is capable of extending to more symbol exceptions if desired.",25,26
12652,52287106,"Regarding Point 2, we did not find a single font that covered all alphabets in use for this Shared Task, so we had to take some drawbacks and accept rendering of ""unknown symbol"" placeholders for some languages.",35,36
12653,52287106,The symbol rendering is handled through the pygame 3 library.,1,2
12654,52287106,2014) (2014) and character embeddings to obtain a dense numerical representation from each input symbol.,17,18
12655,18926166,is the following (with the items automatically inferred by the parser preceded by the symbol @): EXERCISE: ex 1 text: 'I (live) in this house for ten years.,15,16
12656,6106484,These parsers employ k-symbol fully reduced right context in making parsing decisions.,5,6
12657,6106484,"The symbols inside the boxes are on the stack, and those inside the circles are already attached to a higher level symbol on the stack.",22,23
12658,6106484,"The grammar G, is not LR(k) for any fixed k. Any a can be reduced to an A via production A~a or can be considered as a first symbol in production A~aS (i.e., a reduce/shift conflict in LR parser).",32,33
12659,6106484,"An example is the grammar G4 : S ~aA S-~bB A..~d A~cA B~d B.~cB A Marcus-style parser after attaching the first symbol in an input sentence will activate different packets to parse A or B depending on whetller the first symbol was a or b. However, a BCP-parser cannot reduce the only phrase, i.e., d in the sentences ac...cd and bc...cd.",29,30
12660,6106484,"An example is the grammar G4 : S ~aA S-~bB A..~d A~cA B~d B.~cB A Marcus-style parser after attaching the first symbol in an input sentence will activate different packets to parse A or B depending on whetller the first symbol was a or b. However, a BCP-parser cannot reduce the only phrase, i.e., d in the sentences ac...cd and bc...cd.",48,49
12661,6106484,A basic LRRL(k) parser employs k-symbol fully reduced right contexts or lookaheads.,8,9
12662,6106484,"However the 2-symbol fully reduced context for reduction is SB, and for the shift operation is SS, which indicates a possible resolution of conflict if we can parse the lookaheads.",4,5
12663,3101294,The onset b-p symbol represents that the onset phoneme was 75% consistent with a /b/ and 25% consistent with a /p/. Transparency reflects relative word activation.,5,6
12664,35233704,"It will be more symbol-oriented, wanting word processing and access to on-line (or CD-ROM) data.",4,5
12665,13225829,"CAT = CONJUNCT (4) TRUEJ NOT-P1 = TRUE 1 NOT-P2 "" TRUEJ t ] The FD that should be the outcome of the derivation process is one which has truth-values explicitly marked in for some of the literals, in such a way that consistent assignments are given to a propositional symbol and its negation, and each conjunct contains [at least] one literal feature with TRUE as its value.",59,60
12666,15134057,The antecedent list of K] is L(K1) and L(K2) = U2 + L(K1) (we denote the union of sets by the symbol +).,26,27
12667,196471028,"The symbol ""≤"" between two boxes means that the candidates in the box on the left are predicted to have a probability at most as large as the candidates in the box on the right.",1,2
12668,5524852,"The $ symbol is equivalent to ""financial investments"" and the symbol to ""psychological investments"".",2,3
12669,5524852,"The $ symbol is equivalent to ""financial investments"" and the symbol to ""psychological investments"".",12,13
12670,17817934,"The objects used to represent linguistic information are typed feature terms, i.e., feature terms that have a type symbol associated with each node in the directed graph representing an ordinary feature term.",20,21
12671,59336265,"After the tokenization, we included these in training set i.e. in the FB/Tweet that contains 1 token (not a punctuation symbol), special token 2 .",24,25
12672,6235443,"To simulate the fact that children are able to construct the classes of common nouns, proper nouns and non-auxiliary verbs, it suffices to substitute every occurrence of common or proper nouns in the Manchester corpus by the symbol 'noun' and every occurrence of nonauxiliary verbs by the symbol 'verb'.",41,42
12673,6235443,"To simulate the fact that children are able to construct the classes of common nouns, proper nouns and non-auxiliary verbs, it suffices to substitute every occurrence of common or proper nouns in the Manchester corpus by the symbol 'noun' and every occurrence of nonauxiliary verbs by the symbol 'verb'.",53,54
12674,6235443,"In list L2, in utterances that include a word from the categories Noun or Verb, this word is substituted by the symbol 'noun' or 'verb'.",23,24
12675,11396803,"On the other hand, the load associated with the modifier attachment is 2xt~ + XTR PLUs since 1) both the verb put and the preposition on have unsatisfied 9I will prefix sentences that are difficult to parse because of memory limitations with the symbol ""#"".",45,46
12676,16156055,"When at very noisy portion is detected, the patrser skips that portion using a .fake nonterminal symbol.",17,18
12677,16156055,"I.e., the table can tell which part of the input should be replaced by a specific symbol or ignored. '['",17,18
12678,16156055,The parsing ends when tim same-name real nonterminal symbol is created out of the reutterance.,10,11
12679,16156055,"Finding an unknown word by a specific nonterminal symbol enables the interactive grammar augmentation as the following, for instance.",8,9
12680,17727105,"1) p --, p-/s__ else p --, ph/__V (where V is any vowel symbol) else p --~ p' Often, of course, grammars made with rules of this type may be (contingently) quite restricted.",19,20
12681,17727105,"Each symbol in the input text string is translated into a column-vector of distinctive phonetic features (nasal, vowel, tongue-back, etc.)",1,2
12682,15461093,"A FSAP runs as follows: in a single step, it tests which of the edges leaving the current states are labeled with a predicate satisfied by the current input symbol, and non-deterministically follows these edges).",31,32
12683,15461093,"In our case, pattern matching can be realized by a FSAP over the infinite alphabet of segments: one segment becomes one input symbol, and segment specifications become predicates.",24,25
12684,15461093,"Some authors consider jumping or skipping for automata in different meanings, e.g. simply moving from a state to another, or compressing the input to the list of gap lengths between consecutive occurrences of a given symbol (Wang, 2012) , which is somehow related but far less general.",37,38
12685,15461093,"The intuitive meaning of d(q) is the depth of the next symbol to be read when q is the current state, which leads to some kind of skipping.",12,13
12686,18653668,2 Architecture 2.1 The representational device: typed feature terras The objects used to represent partial linguistic information are typed feature terms: i.e. feature terms where each node in the directed graph usually repre-smiting an ordinary feature term can be associated with a type symbol.,47,48
12687,14252998,The symbol I indicates that the elements it separates are alternatives.,1,2
12688,14252998,If an element is enclosed in < > it indicates that the symbol is non-terminal and requires further expansion.,12,13
12689,198961613,"From the start state q 0 , if the first symbol read is a low tone, then the FST transitions to q 1 , and writes out the rest of the input faithfully.",10,11
12690,198961613,"If the first symbol read is a high tone, then the machine transitions to q 2 , where it must decide what to do with following low tones.",3,4
12691,198961613,"From the start state q 0 , if the first symbol read is a low tone, then the FST must decide what to do.",10,11
12692,5201887,A hand which holds a pencil is used as the symbol for this type of gesture (see figure 1 /symbol A).,10,11
12693,5201887,"The default strategy is as follows: if the pointing action refers to a field, the pencil is in the middle of the fie/d, if it refers to an entry, the pencil is below the entf~, so that the symbol doesn't cover it.",45,46
12694,5201887,"Each time the speaker-hearer roles are reversed, the current pointing symbol changes to a neutral symbol (i.e, the standard mouse cursor).",13,14
12695,5201887,"Each time the speaker-hearer roles are reversed, the current pointing symbol changes to a neutral symbol (i.e, the standard mouse cursor).",18,19
12696,5201887,"If the system generates a new pointing gesture, it first changes the neutral symbol into the choosen pointing symbol.",14,15
12697,5201887,"If the system generates a new pointing gesture, it first changes the neutral symbol into the choosen pointing symbol.",19,20
12698,5201887,Then it moves the symbol to the new pointing location.,4,5
12699,5201887,The pointing symbol changes only when the user takes initiative in the dialog again.,2,3
12700,5201887,"In a first release of POPEL, the gesture is performed with another symbol (hand with stretched-out forefinger, see figure 1 /symbol B).",13,14
12701,245130929,"Each y i is a symbol in a vocabulary V. The probability of a decoded sequence is given by p(y | x; θ) = n t=1 p(y t | y <t , x; θ).",5,6
12702,201734855,Both lemmas and words are appended with a special end-of-sequence symbol (indicated with '#' in Table 1 ).,14,15
12703,201734855,The decoding stops when any of the actions predict the end-of-sequence symbol.,15,16
12704,250391090,"The basic cleaning included deleting the ""#"" symbol before hashtags and emojis transformation.",9,10
12705,250391090,"This technique represents a fragment of text (symbol, word, collocation, sentence, or even paragraph) as a vector (or a set of them).",8,9
12706,199523733,"Next, data cleaning was done to remove unnecessary characters such as re-tweet symbol (RT), username, URL, and punctuation.",15,16
12707,248941960,"This paper focuses on symbol grounding (Harnad, 1999) in the context of ITL (Matuszek, 2018) : the learner must use the teacher's embodied natural language utterance and its context to learn a mapping from natural language expressions to their denotations, given the visual percepts.",4,5
12708,248941960,There are two challenges in learning symbol grounding models (grounders) in ITL.,6,7
12709,248941960,"Our focus instead is on exploiting the logical consequences of those logical forms during symbol grounding-i.e., our focus is to utilise the interpretation of logical forms, and in particular the truth functional meanings of close-class words like quantifiers and negation, to inform the learning of mappings from (open-class) symbols like red to their denotations, given the visual percepts.",14,15
12710,248941960,"This speech act provides two types of information: certain information C n in the form of denotation-symbol-semantic value triples (e n , p z , y n z ), corresponding to symbols and entities designated by the RE; and noisy information N n , corresponding to denotation-symbolsemantic value estimate triples (e n , p z , ỹn z ), which are acquired from the symbols that are part of the RE and its referent inferred via (uncertain) reasoning.",19,20
12711,248941960,The learner must achieve decent performance on its task via only a few teacher utterances: human teachers won't tolerate repeating the same REs many times and so the learner lacks the luxury of learning (and testing) symbol grounding on large data sets.,40,41
12712,248941960,"The data statistics for the training set is given in Table 2 for the general categories of symbols, where certain (C n ) means that the designation is denoted by the symbol in the RE, and noisy (N n ) means that the symbol is a part of the RE but is not designated by it.",33,34
12713,248941960,"The data statistics for the training set is given in Table 2 for the general categories of symbols, where certain (C n ) means that the designation is denoted by the symbol in the RE, and noisy (N n ) means that the symbol is a part of the RE but is not designated by it.",47,48
12714,248941960,"Category C n candidates N n candidates C1 18.67 ± 5.39 19.83 ± 5.04 S1 14.67 ± 3.98 16.50 ± 5.32 R2 0 37.75 ± 6.75 Table 2 : Average symbol counts per word for colours (C1), shapes (S1), and spatial relationships (R2).",30,31
12715,248941960,"To aid the learning process for R2, whenever a new symbol R ∈ R2 is observed, domainlevel axioms are added to ∆ for it, making it irrreflexive: ∀x.¬R(x, x) (an entity cannot be in a spatial relationship to itself) and asymmetric: ∀x, y.R(x, y) → ¬R(y, x) (reflecting the fact that entities in spatial relations take different roles (Miller and Johnson-Laird, 1976) ).",11,12
12716,248941960,"That HEAD uses only symbol-designation pairs that are acquired when the symbol denotes the referent (in our case, that's the head noun in the RE and its pre-head modifier, if it exists).",4,5
12717,248941960,"That HEAD uses only symbol-designation pairs that are acquired when the symbol denotes the referent (in our case, that's the head noun in the RE and its pre-head modifier, if it exists).",13,14
12718,248941960,"For example, the RE ""both squares"" implies there exist exactly two squares; if the symbol grounding model has an uncertain belief that there are more (or fewer) squares than this, it will select the two most probably candidates (and infer that all other entities are  non-squares).",18,19
12719,248941960,"Finally, the purpose of IGRE is to aid ITL: i.e., the (incremental) updates to beliefs about symbol grounding should enhance learning to solve domain-level planning problems.",21,22
12720,199557481,"The second is a binary softmax, which takes as input the joint encoding of 2 sentences, which we join together using a special symbol that indicates the boundary between 2 sentences.",25,26
12721,250390778,"Furthermore, our research shows that any kind of preprocessing technique is mostly useless because any character, capital letter, overextended word or symbol, could be the determining factor in recognizing ironic speech.",24,25
12722,9106419,"1 , abstract regions are denoted by the symbol 'AR' (as e.g. AR48, the father of R15 and R16).",8,9
12723,10059432,A symbol < = = = > designates that both sides are strictly corresponding.,1,2
12724,6624691,"Having completed his job Observer, who is also a part of this world, may name h~mself Observer or the Observer, and happily sit down under a tree on the grass Let us call the whole collection of objects he has just described as the Observer level and use the symbol L 0 for it Suppose then we ask Observer to tell us as much as he can about L 0 Soon he finds out that his naming has its limits As he discovers new facts about his world it becomes more and more cumbersome for him to communicate in terms of every man.",54,55
12725,6624691,"and their instances placed at LN_I T This process may remain mostly implicit until we mm(e an utterance relating (N x) to other objects at LN_i T. In general we shall say that the level L_Ni T is lower than the level L 0, and write L~_lf<Lo Often we shall drop the superscripts N and T over the level symbol assuming some lower level L_~ whenever it does riot lead to ambiguity Observe that with the above account the level structure of objects has a dynamic, ever-changing character.",64,65
12726,250390513,"We use the symbol <s> to separate different parts of the news, the symbol </s> as the separator for two news articles, and add <s> and </s> at the beginning of each news pair.",3,4
12727,250390513,"We use the symbol <s> to separate different parts of the news, the symbol </s> as the separator for two news articles, and add <s> and </s> at the beginning of each news pair.",16,17
12728,1519652,"In the horizontal progress of the algorithm (which corresponds to the answer l~nes -a new symbol is added) the output ormation concerns a single word-end, while in the vertical progress (corresponding to the answer n oo-different symbols than the one(s) in question are added) it usually concerns more than one word-end.",18,19
12729,1519652,"--r~ --pr~ --/pr# --B I A A A The three occurrences of A in Figure I can be indicated, for the sake of clarity, as AI, A 2 and A3: A I (corresponding to the horizontal string r~) accounting for those Czech adjectives (In the given foI~n) ~vhose penultimate symbol is different from r (such as velk# (big)), A 2 (correspondTng to the horizontal string pr#) accountiru~ for those Czech adjectives---[in the given form) whose second symbol from the right is r and whose third symbol from the right is ~ifferent from ~ (such as dobr@ (good)), and A 3 (c~rresponding'--~the horizontal word-end /~org) accounting for those Czech adjectives (in the given form) whose third and second symbols from the right are ~r, respectively, and whose fourth symbol from the right is different from /, i.e. which are longer than three s~nbols (in Czech, there is only one such ~djective, namely k_~ (loose, plump)).",60,61
12730,1519652,"--r~ --pr~ --/pr# --B I A A A The three occurrences of A in Figure I can be indicated, for the sake of clarity, as AI, A 2 and A3: A I (corresponding to the horizontal string r~) accounting for those Czech adjectives (In the given foI~n) ~vhose penultimate symbol is different from r (such as velk# (big)), A 2 (correspondTng to the horizontal string pr#) accountiru~ for those Czech adjectives---[in the given form) whose second symbol from the right is r and whose third symbol from the right is ~ifferent from ~ (such as dobr@ (good)), and A 3 (c~rresponding'--~the horizontal word-end /~org) accounting for those Czech adjectives (in the given form) whose third and second symbols from the right are ~r, respectively, and whose fourth symbol from the right is different from /, i.e. which are longer than three s~nbols (in Czech, there is only one such ~djective, namely k_~ (loose, plump)).",97,98
12731,1519652,"--r~ --pr~ --/pr# --B I A A A The three occurrences of A in Figure I can be indicated, for the sake of clarity, as AI, A 2 and A3: A I (corresponding to the horizontal string r~) accounting for those Czech adjectives (In the given foI~n) ~vhose penultimate symbol is different from r (such as velk# (big)), A 2 (correspondTng to the horizontal string pr#) accountiru~ for those Czech adjectives---[in the given form) whose second symbol from the right is r and whose third symbol from the right is ~ifferent from ~ (such as dobr@ (good)), and A 3 (c~rresponding'--~the horizontal word-end /~org) accounting for those Czech adjectives (in the given form) whose third and second symbols from the right are ~r, respectively, and whose fourth symbol from the right is different from /, i.e. which are longer than three s~nbols (in Czech, there is only one such ~djective, namely k_~ (loose, plump)).",106,107
12732,1519652,"--r~ --pr~ --/pr# --B I A A A The three occurrences of A in Figure I can be indicated, for the sake of clarity, as AI, A 2 and A3: A I (corresponding to the horizontal string r~) accounting for those Czech adjectives (In the given foI~n) ~vhose penultimate symbol is different from r (such as velk# (big)), A 2 (correspondTng to the horizontal string pr#) accountiru~ for those Czech adjectives---[in the given form) whose second symbol from the right is r and whose third symbol from the right is ~ifferent from ~ (such as dobr@ (good)), and A 3 (c~rresponding'--~the horizontal word-end /~org) accounting for those Czech adjectives (in the given form) whose third and second symbols from the right are ~r, respectively, and whose fourth symbol from the right is different from /, i.e. which are longer than three s~nbols (in Czech, there is only one such ~djective, namely k_~ (loose, plump)).",163,164
12733,16954606,"In her conception, negation is an abstract, operator-like functor of FOr without a label on its edge and without pertinence to the TFA of a sentence; the symbol NEG, generated as a label on the node of the functor of negation, must be changed by surface rules into such forms as not, do not, etc.",32,33
12734,11379263,"The only significant filtering was applied to numbers: all digit-based numbers (e.g. 5, 2.1) were converted to the symbol '#' and counted as such.",24,25
12735,17596252," where ""("" and "")"": all terminals, SEGRULE : a r~lle for long English segmentation, LHS: an augmented regular expression which is compo6ed of a regular expression and test(s), RHS: parsing action(s), test :: a LISP function which implements the designated test, ving: a gerund, such as going, doing, ved: a verb with endinq ""ed"", where it indicates its past or past participle form, num: a number, english : an English word, or a symbol of punctuation, closure and plus: the functions * + corresponding to R and R where R is a regular expression, (The function are done by matching the shortest pattern, covered by the functions, in the input sentence.)",103,104
12736,17596252,"opt: an optional item, Each symbol of the right-hand side of CAT: part-of-speech or category.",7,8
12737,248571865,"A WCFG is a five-tuple Σ, N, S, R, ρ , where Σ is an alphabet 5 of terminal symbols, N is a finite set of non-terminal symbols, S ∈ N is the unique start symbol, R is a set of production rules where a rule is of the form X → α where X ∈ N and α ∈ (N ∪ Σ) * , and ρ : R → R ≥0 is a scoring function that maps every production rule to a non-negative real number.",45,46
12738,248571865,"The grammar has three non-terminals N def = {S, X σ , X σ } where S is the distinguished start symbol.",25,26
12739,248571865,"A A Weighted Context-Free Grammar A.1 The Grammar In our WCFG Σ, N, S, R, ρ , Σ is the set of all tokens in the vocabulary, N = {S, X σ , X σ }, where S is the start symbol, X σ is the span of interest, and X σ is the spans that are not of interest.",51,52
12740,227231729,"Data Preprocessing I applied the following preprocessing steps on the provided datasets: (1) removing hashtag symbols, (2) removing mention tag symbol, (3) removing punctuations, (4) tagging numbers, (5) lowercasing letters and (6) word tokenization.",26,27
12741,5432501,"This phenomenon is call fusion and fused values of syntagmatic lexical functions are flagged with the ""//"" symbol in lexicographic descriptions; for instance: (2) Magn( rain V ) = hard, heavily, //pour down Years of lexical studies on a wide spectrum of natural languages have allowed for the identification of a now stable set of approximately 65 simple lexical functions; 8 additionally, these functions can be combined to form complex lexical functions (Kahane and Polguère, 2001) .",19,20
12742,16103144,"11 The ""∼"" symbol is used throughout a lexicographic article to refer to this article's headword.",5,6
12743,14440663,"Section 5, the segmentation task, shows the subjects 19 sentence stimuli and asks them to insert a word boundary symbol (""/"") at each word boundary they perceive; the subjects are required to insert a ""/"" behind each punctuation and the last character of a sentence; the subjects are also informed that they need not to care about right or wrong, but just follow their intuition.",21,22
12744,13367117,"+' is used as a mnrphenle lamm 'lary, 0 is used as the null symbol, '#' is used as an eadm~ker, and '' is used to specify a don't care context.",17,18
12745,250390490,"For each element, symbol 1 means that the corresponding PCL category is expressed in the paragraph.",4,5
12746,7844613,c-structure) The new c-structure is the expansion which results from applying r to a terminal node (122 in the example) which is labelled with the lefthand symbol of the rule.,33,34
12747,12732672,"word by word: only/rely on/the messes/only/can/do/well/job) in which the symbol ""/"" indicates word boundaries.",25,26
12748,59201,"To complete the parse, the period symbol, or corresponding vector, can be used to trigger an operation which refreshes only those inactive edges which contain the final <S> pattern.",7,8
12749,16205078,"For adjective phrases this might be given as (adjinfo (head entity ""Subject"") (adj adjective ""Adjective"") (prep prep ""Preposition"") (obj entity ""Object"")) where ""adjinfo"" is an arbitrary symbol used internally to reference adjective phrase case frames.",46,47
12750,227231053,"Then, for each target word, we add a special symbol to all its appearances in both corpora, the symbol denoting the corresponding corpus or period.",11,12
12751,227231053,"Then, for each target word, we add a special symbol to all its appearances in both corpora, the symbol denoting the corresponding corpus or period.",21,22
12752,227231564,"Semantic change is defined as the changes in lexical meaning rather than grammatical usage of a language symbol (Bloomfield, 1933) .",17,18
12753,15482228,"Phrase structure grammar rules, especially those in Greibach normal form, can naturally describe this kind of prediction: we can consider the terminal symbol (or the lexical category) on the righthand side of a rule as the current word and the nonterminal symbols that follow the terminal symbol as new predictions [3] .",25,26
12754,15482228,"Phrase structure grammar rules, especially those in Greibach normal form, can naturally describe this kind of prediction: we can consider the terminal symbol (or the lexical category) on the righthand side of a rule as the current word and the nonterminal symbols that follow the terminal symbol as new predictions [3] .",51,52
12755,5379533,"Dyer (12) has implemented a connectionist model of ""symbol recirculation"" in which ""words with similar semantics (as defined by word usage) end up forming similar distributed representations in the lexicon.""",11,12
12756,471963,"I, br the purposes of the present paper we will introduce a DATR notation that slightly differs fi'om the standard notation given in Evans & Gazdar [1989] in the following respects: • the usual DATR abbreviation conventions are spelled out * the global environment of a DATR descriptor is explicitly represented (even if it is uninstantiated) • each node-path pair N:P is associated with the set of extensional suffixes of N:P that are defined within the DATR theory In standard DATR notation, what one might call a non-terminal symbol, is a node-path pair (or an abbreviation for a node-path pair).",101,102
12757,471963,"In our notation a DATR nonterminal symbol is an ordered set [N, P, (7, N', P'].",6,7
12758,471963,"C is the set of path suffixes of N:P. A DATR terminal symbol of a theory 0 is an atom that has at least one occurence in a sentence in 0 where it is not an attribute, i.e. where it does not occur in a path.",14,15
12759,471963,"A term.thai symbol t, 'matches another tc.r'minal sy'mbolt 2 ifl' t, -t2.",2,3
12760,471963,"A non-empty sequence of (terminal and nontcrmilml) symbols s'~ ... s',~ (1 < n) matches another sequen(:e of (terminal and non-terminal) symbols s j ... s,, with suttix E mM constraint C if (a) for ca.all symbol sl (1 < i < n): s{ m~l;cho, s s,.",50,51
12761,471963,"In anal-ogy to a eontext-flee phrase structure rule, a DATR sentence has a left hand side that consists of exactly one non-terminal symbol (i.e. a node-path pair) and a right hand side that consists of an arbitrary number of non-terminal and terminal symbols (i.e. DATR atoms).",29,30
12762,471963,"Additionally, DATR difli?rs from CF-PSG in that there is not a unique start symbol but a possibly infinite set of them (i.e. the set of node-path pairs that, taken as the.",16,17
12763,471963,"Let X1 IN, 15, Ct, N', P'] be a nonterminal symbol including an evaluable path PI.",17,18
12764,471963,"Thus an active item has the structure: (START,END,CAT0, CATj ... CAT,, SUFFIX) Consider the following examples: the inactive item (0, 1, [House,<orth sing>,{<gen>},House,P']) represents the intbrmation that the substring of the input string consisting of the first symbol is the vahm of the query House:<orth sing> (with arty extensional path suffix, but not gcn) in the global environment that consists of the node House and some still uninstantiated path P'.",57,58
12765,471963,"The active item ((),l,[Noun, <orth>,0,House,P'], [Itouse,<affix>,O,House,P'],e) represents the information that there is a t)artial analysis for a substring of the input string that starts with the first symbol and ends somewhere to the right.",44,45
12766,471963,heled with a start symbol (i.e. a DATR nonterminal with identical local and global environment) for the whole string which a derivable from the given grammar.,4,5
12767,236460032,"For example, the Mods 'IsMean' and 'IsMeanHasTolerance' are very similar with the slight difference that keywords corresponding to the Mod 'IsMeanHasTolerance' contain the additional symbol, '±'.",31,32
12768,6414984,The left corner of a rewriting rule is the leftmost symbol on its rlght-hand side -the name stems from depicting the rule as a local tree it generates.,10,11
12769,222141709,6 The last symbol of each line.,3,4
12770,222141709,"Since the number of lines we want to generate is defined by the acrostic word, besides feeding an embedding which represents the number of lines to our model, we further enforce the right poem length by substituting each end-of-sentence symbol by end-of-line, if the number of lines is still too small.",45,46
12771,222141709,"Whenever this would lead to the last symbol of a poem being "","" or "";"", we substitute it by ""."".",7,8
12772,227230323,"Second, we merge all continuously and repeated ""@user"" to a single ""@user"" symbol to reduce the redundancy, since the training set has too many consecutive and repeated ""@user"".",17,18
12773,2438164,"It is possible to introduce a symbol that corresponds to the category NP, but it c,'mnot be used in deriving sentences from the stm't symbol.",6,7
12774,2438164,"It is possible to introduce a symbol that corresponds to the category NP, but it c,'mnot be used in deriving sentences from the stm't symbol.",25,26
12775,250390950,Each node in the hypertree represents a constituent in the parse tree and is labeled with the nonterminal symbol of the constituent.,18,19
12776,15105710,"When a terminal symbol must be applied, the following (meration is done.",3,4
12777,15105710,"If the terminal symbol expected by the grammar rule matches with ~e current word of the sentence, we have the following sit,ration : If current > righttru'zgt, then, we do : ri@gmos: := currem"" Clt/'ren[ ~:: clgrretg ""+"" i else, we do: current := current + 1 I f the terminal symbol 1"" expected by the grmmnar rule doesn't match with the current word of the gr~mm'Lar, the situation is : If current < righztm-)st then we do nothing, else, we record T as ~m expected word instead of tire cmTent word in the sentence.",3,4
12778,15105710,"If the terminal symbol expected by the grammar rule matches with ~e current word of the sentence, we have the following sit,ration : If current > righttru'zgt, then, we do : ri@gmos: := currem"" Clt/'ren[ ~:: clgrretg ""+"" i else, we do: current := current + 1 I f the terminal symbol 1"" expected by the grmmnar rule doesn't match with the current word of the gr~mm'Lar, the situation is : If current < righztm-)st then we do nothing, else, we record T as ~m expected word instead of tire cmTent word in the sentence.",67,68
12779,15105710,The partial synthesis imposes to place may condition in a grammar rule before the concerned symbol in order to evaluate the condition before the rewriting of the symbol.,15,16
12780,15105710,The partial synthesis imposes to place may condition in a grammar rule before the concerned symbol in order to evaluate the condition before the rewriting of the symbol.,27,28
12781,15105710,This method is not efficient when the rewriting of the concerned symbol leads to a part of the sentence yet accepted.,11,12
12782,15105710,"So, for each call to a condition that may occur in a giammar rule, we place it before and after the concerned symbol.",24,25
12783,184482773,"The ""#"" symbol is removed and the word itself is retained.",4,5
12784,16914752,$' in the action table is the end-of-input symbol.,13,14
12785,16914752,"i ) S --> VP NP (2) NP --> n (3) NP --> PP NP (4) VP --> NP v (5) PP --> NP p Figure 3-1: Reversed Grammar Action Table Goto n v p $ NP VP Directions Here we describe the algorithm for parsing the lattice starting from an anchor symbol and exp~mding in both left ,and right directions.",68,69
12786,16914752,Choose the anchor symbol A from the lattice.,3,4
12787,16914752,"Because A is a terminal symbol, the initial state(s) are determined from the action table.",5,6
12788,16914752,"3] At the reached skate s i, check if there any nontenninals already exist which s i is expecting according to the goto table [along the row of state s i under the column labeled with the nonterminal symbol]. (",41,42
12789,16914752,A single phoneme is too primitive to be an anchor symbol.,10,11
12790,17096521,"If the processing is suecessfull, erase the text of the clau~ from the input and replace it by a special non terminal symbol.",23,24
12791,17096521,Success in the above algorithm means that the input has been reduced to the PP symbol or to a string of such symbols and conjunctions.,15,16
12792,17096521,"Another interesting feature of the bottom-up algorithm is that the special symbol representing a processed subordinate clause will be naturally included, in later fragmentation steps, in the clause qualified by this subordinate, thus permitting to process correctly inter clause dependencies.",13,14
12793,17096521,"The term PINF will then be ejected by the mechanism of the last section, giving the following state: (12) PP PINF This is a dead end state, since the sentence is not reduced to a PP symbol, and yet no further clause to process can be found.",41,42
12794,17096521,"If the word to the right of the conjunction is a right delimiter, or if next word in the current direction is the special symbol PP, the conjunction is taken as delimiter (excluded).",25,26
12795,17096521,The same is true if the conjunction is adjacent to the PP symbol.,12,13
12796,216848664,"aWe uge standard TAG notation, marking foot nodes in auxiliary trees with '*' and nodes where substitution is m occur with '1/. The nonterminal names in the logical form grammar are mnemonic for Formula, Relation (or function) symbol, Term, and Quantifier.",44,45
12797,5557253,"A syntactic entry for a word consists of one or more complex categories, each specified by a principal category symbol augmented by a set of constraints on the values of syntactic features.",20,21
12798,17982145,"For example, the symbol COMP can be used to label an AS-DAG edge ibrming a constituent of a relative address, and at the same time provide direct access to a fixed k)cation, that is, label an AS-DAG node.",4,5
12799,227231304,"The data pre-processing stage includes abbreviation reduction, spelling correction, word stem extraction, upper and lower case letter conversion, special symbol processing and other operations, at the same time, the data with the same training sample but inconsistent labels after the conversion of upper and lower case letters are screened out.",25,26
12800,14110109,"A type that encodes agreement features can be written: AGR = [num: NUM,gender: GEN] and types NtJM and GEN being themselves defined as NUM = SING V PLUR (where the symbol ""y"" denotes the logical OR) and GEN : MASC V FEM V NEU.",38,39
12801,14110109,"Using the TFS syntax, where the symbol ':-' after an FS introduces a condition, a definition for append can be as follows: Note that the tagging syntax allows to specification of identity between structures and a partial instance of the structure.",7,8
12802,198183907,The initial symbol to the network is a special 'end of sequence symbol' and otherwise predictions of the previous time step are fed to the recurrent unit as input.,2,3
12803,198183907,The initial symbol to the network is a special 'end of sequence symbol' and otherwise predictions of the previous time step are fed to the recurrent unit as input.,13,14
12804,198183907,"During the lemma prediction, we predict a lemma whenever probability of end-of-sequence symbol reaches to a defined threshold.",17,18
12805,237563200,"2019) utilize the embedding layer to map an input sequence of symbol representations X to a sequence of continuous representations E = [e 1 , . . . ,",12,13
12806,221761220,We denote the final representation of the special symbol [CLS] as C ∈ R d .,8,9
12807,221761220,"In [CLS] aggregation, we directly use the representation C of the special symbol [CLS] as the aggregate feature r (Devlin et al.,",15,16
12808,227231496,"Usually, these hashtags are indicated by the symbol ""#"".",8,9
12809,227231496,"However, we know that the symbol ""#"" has no meaning.",6,7
12810,227231496,"Therefore, the ""#"" symbol is removed and the word itself is reserved for hashtags.",6,7
12811,236459785,"CLS] (classification symbol) always be added before sentence, and use classifier to compute [CLS] representation to get the result.",4,5
12812,236459785,"So, there is a problem, that is, the sentence representation from Bi-LSTM will not integrate on symbol [CLS] .",21,22
12813,236459785,"So the output of Bi-LSTM is sent into a new defined transformer layer, encode the sentence representation into the symbol [CLS] .",22,23
12814,250390792,This process is recurrently done until the model predicts a stop symbol <eop> 1 .,11,12
12815,220286796,"Somewhat randomly, the IPA provides an atomic symbol for the voiceless alveolar plosive (/t/), but only a composed symbol for the voiceless dental plosive (/t̪ /).",8,9
12816,220286796,"Somewhat randomly, the IPA provides an atomic symbol for the voiceless alveolar plosive (/t/), but only a composed symbol for the voiceless dental plosive (/t̪ /).",22,23
12817,220286796,"Korean Korean uses an alphabet that provides a symbol for each consonant and for each vowel, yet it groups symbols into square syllable blocks, which makes it look somewhat close to Chinese and Japanese writing, although it is much simpler.",8,9
12818,220286336,We use # here as a symbol to denote either beginning-of-word or end-of-word.,6,7
12819,6456136,"A conspicuous success has been part-of-speech taggers, systems that assign one and only one part-of-speech symbol to a word in a running text and do so onthe basis (usually) of statistical generalisations across very large bodies of text.",24,25
12820,220282312,"Viewed from another perspective, we need to be able to ignore the portion of the output that would correspond to a word-end symbol.",25,26
12821,220282312,Note that we use a large subscripted conjunction symbol to collapse a series of formulae that are identical aside from using different tiers.,8,9
12822,6993797,4) All user-defined special characters and non-BIG5 code were replaced with a special symbol ' í '.,18,19
12823,10727846,"The logical symbols of the complex constraint language consist of variables, constants, the logical connectives & (conjunction), I (disjunction), "" (negation),-> (material implication), <-> (logical equivalence), the binary predicatc symbol ""="" and non-logical function and predicate symbols.",47,48
12824,235417352,"Each non-terminal symbol in α is aligned to the same non-terminal symbol in β, and vice versa.",4,5
12825,235417352,"Each non-terminal symbol in α is aligned to the same non-terminal symbol in β, and vice versa.",15,16
12826,235417352,"LR(1) parser peeks ahead one lookahead input symbol, and the state transition table describes the acceptable inputs and the next states.",8,9
12827,235417352,"Only the acceptable words in the current state can be generated, and the end-of-sentence symbol can only be generated when reaching the final state.",19,20
12828,36362301,"2) For a nonterminal c, the following terminal condition The symbol s in 4-4) signifies the start symbol, and the clause 4-4) is used to make a prediction at the left most part of an utterance.",12,13
12829,36362301,"2) For a nonterminal c, the following terminal condition The symbol s in 4-4) signifies the start symbol, and the clause 4-4) is used to make a prediction at the left most part of an utterance.",22,23
12830,221319863,"signs or ""end of line"" symbol on both sides) and to complete a sentence or a subsentence (same as complete a sentence but with ',' sign included).",7,8
12831,219310365,"To each word in the vocabulary we associate a terminal symbol to (the word itseff) and a non-terminal symbol A~. The grammar consists of the following two kinds of rules: Awe --~ A~= to2 A~=A~ , (la) A=, --~ ¢, (lb) where ~ represents the null production.",10,11
12832,219310365,"To each word in the vocabulary we associate a terminal symbol to (the word itseff) and a non-terminal symbol A~. The grammar consists of the following two kinds of rules: Awe --~ A~= to2 A~=A~ , (la) A=, --~ ¢, (lb) where ~ represents the null production.",22,23
12833,219310365,"In addition, we introduce a sentence start symbol A0 with the production Ao --* A, toA. . (",8,9
12834,219310365,"iV2 For the start symbol, the probabilities are p(0--* to) and satisfy Ep(0--, to) ----1 .",4,5
12835,219310365,UJ There is no null production for the start symbol.,9,10
12836,219310365,The start symbol first generates some word in the sentence.,2,3
12837,219310365,"As expected, the trains strongly to generate the null symbol ~b.",10,11
12838,12566013,"Left Corner Left-corner parsing is a bottom-up technique where the right-hand-side symbols of the rules are matched from left to right, s Once the left-corner symbol has been found, the grammar rule can be used to predict what may come next.",36,37
12839,12566013,"Strategy 3 g (LC) Whenever an inactive edge is added to the chart, if its category is T, then for every rule in G with T as left-corner symbol add an empty active edge.",34,35
12840,12566013,"For example, Karttunen's D-PATR system SThe left corner of a rule is the leftmost symbol of its righthand side.",18,19
12841,12566013,"Strategy 4 (LCK) Whenever an inactive edge is added to the chart, if its category is T, then for every rule in G with T as left-corner symbol add an edge that subsumes the T edge.",33,34
12842,12566013,"--Whenever an inactive edge is added to the chart, if its category is T, then for every rule C in G with T as left-corner symbol add an empty active C edge if for some i r(A,) = C or r(A,)~C. Analogously, adding top-down filtering to Kilbury's strategy LCK results in the following.",29,30
12843,12566013,"--Whenever an inactive edge is added to the chart, if its category is T, then for every rule C in G with T as left-corner symbol add a C edge subsuming the T edge if for some i r(A,) = C or r(A~)~C. One of the advantages with chart parsing is direction independence: the words of a sentence do not have to be parsed strictly from left to right but can be parsed in any order.",29,30
12844,12566013,"--Whenever an inactive edge is added to the chart, if its category is T, then for every rule C in G with T as left-corner symbol add a C edge subsuming the T edge if for some ] r(C) = py or r(C)~py.",29,30
12845,12566013,"--Whenever an inactive edge is added to the chart, if its category is T, then for every rule C in G with T as left-corner symbol add a C edge subsuming the T edge if for some i r(A,) = C or r(A,)~C and for some i r(C) = py or r(C)]~pj.",29,30
12846,17559022,"For example, in Figure Further, for the word A 1,i , we define that it depends on symbol ' s '.",19,20
12847,17559022,We also use the symbol ' /s ' to represent the end of generation.,4,5
12848,17559022,"For each original sentence s ∈ D, we initiate an empty list X and set the beginning symbol ' s ' as the initial context c 6 .",18,19
12849,9072253,"To do this, we model the corruption using the edit operations used for the familiar Levenshtein edit distance (Mohri, 2003) over the alphabet Σ: S a , substitution of a word with a symbol a ∈ Σ; D, deletion of a word; I a , insertion of the symbol a ∈ Σ; or K, keeping the word.",38,39
12850,9072253,"To do this, we model the corruption using the edit operations used for the familiar Levenshtein edit distance (Mohri, 2003) over the alphabet Σ: S a , substitution of a word with a symbol a ∈ Σ; D, deletion of a word; I a , insertion of the symbol a ∈ Σ; or K, keeping the word.",56,57
12851,9072253,"In addition, there are transitions from q i to q i+1 with cost C that read any symbol in Σ (for substitution) and ones that read the empty string (for deletion).",18,19
12852,9072253,"Finally, there is a loop with cost C from each q i to itself and for any symbol in Σ, implementing insertion.",18,19
12853,9072253,"Let N b,A be a nonterminal symbol in G and let q h , q i , q j , q k be states of F (r s ) with h ≤ i ≤ j ≤ k. We then add a rule N b,A, q h ,q k → N b,A, q i ,q j to G .",8,9
12854,9072253,"If the start symbol of G is S b,A , then the start symbol of G is S b,A, q 0 ,q |rs| .",3,4
12855,9072253,"If the start symbol of G is S b,A , then the start symbol of G is S b,A, q 0 ,q |rs| .",15,16
12856,9072253,"were used to derive w. We can see that ""yellow"" was created by an insertion because the two states of F (r s ) in the preterminal symbol just above it are the same.",30,31
12857,7094749,"For set fills, the ROI defines each symbol from the set and specified reporting conditions.",8,9
12858,7177714,"It needs to be noted that the symbol ""*"" in Table 6 means that the adverb in that's + adverb + true is able to move freely, not restricted to the middle position, such as ""probably that's true"", or ""that's true also"".",7,8
12859,7177714,"That > Ø > it > this It means that the ones on the left side take priority over those on the right: that more likely occurs than this, and the symbol Ø signals no pronoun occurs.",33,34
12860,236486160,"For this, we use the CMU Pronouncing Dictionary (Weide, 1998), which is based on the ARPAbet symbol set.",21,22
12861,236486245,"Essentially, the rank of a symbol denotes the finite number of daughters that it can take.",6,7
12862,236486245,"The symbol a (0) means that a is a terminal node without daughters, while c (2) is a non-terminal node with two daughters.",1,2
12863,236486245,"A multi bottom-up tree transducer (MBOT) is a tuple M = (Q,Σ,∆,root,q f ,R), where Q, Σ ∪ ∆, {root}, {q f } are pairwise disjoint, such that: • Q is a ranked alphabet with Q (0) =∅, called the set of states • Σ and ∆ are ranked input and output alphabets, respectively • root is a unary symbol, called the root symbol • q f is a unary symbol called the final state R is a finite set of rules of two forms: • σ(q 1 (x 1,1 ,...,x 1,n 1 ),...,q k (x k,1 ,...,x k,n k )) →q 0 (t 1 ,...,t n 0 ) where k ≥ 0, σ ∈ Σ (k) , for every i ∈ [k] ∪ {0}, q i ∈ Q (n i ) for some n i ≥ 1, for every j ∈[n 0 ],t j ∈T ∆ ({x i,j |i∈[k],j ∈[n i ]}). •",84,85
12864,236486245,"A multi bottom-up tree transducer (MBOT) is a tuple M = (Q,Σ,∆,root,q f ,R), where Q, Σ ∪ ∆, {root}, {q f } are pairwise disjoint, such that: • Q is a ranked alphabet with Q (0) =∅, called the set of states • Σ and ∆ are ranked input and output alphabets, respectively • root is a unary symbol, called the root symbol • q f is a unary symbol called the final state R is a finite set of rules of two forms: • σ(q 1 (x 1,1 ,...,x 1,n 1 ),...,q k (x k,1 ,...,x k,n k )) →q 0 (t 1 ,...,t n 0 ) where k ≥ 0, σ ∈ Σ (k) , for every i ∈ [k] ∪ {0}, q i ∈ Q (n i ) for some n i ≥ 1, for every j ∈[n 0 ],t j ∈T ∆ ({x i,j |i∈[k],j ∈[n i ]}). •",89,90
12865,236486245,"A multi bottom-up tree transducer (MBOT) is a tuple M = (Q,Σ,∆,root,q f ,R), where Q, Σ ∪ ∆, {root}, {q f } are pairwise disjoint, such that: • Q is a ranked alphabet with Q (0) =∅, called the set of states • Σ and ∆ are ranked input and output alphabets, respectively • root is a unary symbol, called the root symbol • q f is a unary symbol called the final state R is a finite set of rules of two forms: • σ(q 1 (x 1,1 ,...,x 1,n 1 ),...,q k (x k,1 ,...,x k,n k )) →q 0 (t 1 ,...,t n 0 ) where k ≥ 0, σ ∈ Σ (k) , for every i ∈ [k] ∪ {0}, q i ∈ Q (n i ) for some n i ≥ 1, for every j ∈[n 0 ],t j ∈T ∆ ({x i,j |i∈[k],j ∈[n i ]}). •",96,97
12866,236486245,Rule 1 rewrites a terminal symbol σ as itself.,5,6
12867,236486245,"obtained by marking internal nodes as non-cyclic, introducing the alphabet symbol σ n : σ n (q * (x),q c (w,y)→q c (σ c (x,y,w),y) Essentially, rule 7 tells us that when a coordination node is marked as σ n , M pros just propagates the level of prosodic strength that it currently has registered (in y), without increments (see Figure 6 ).",13,14
12868,236486194,"This follows from two assumption (or constraints): first, that the set of symbols used to indicate tones is distinct from those used to indicate the vowels and consonants; and second, that one and only one such tone symbol appears per string domain (here, the syllable).",42,43
12869,236486194,"If these two constraints hold, the complexity of the syllable template should in general have a greater impact on the entropy of the string set than the position of the tone symbol, although the number of unique tone symbols relative to the number of segmental symbols may also have an effect.",32,33
12870,236486253,"Parse trees are generated top-down by expanding non-terminals N (including the start symbol S ∈ N ) to nonterminals N (excluding S) and terminals W , using the set of allowed expansion rules R with expansion probability θ r for each rule r ∈ R. PCFGs have very strong independence assumptions; the adaptor component relaxes these assumptions by allowing certain nonterminals to adapt to a particular corpus, meaning they can cache and re-use subtrees with probabilities conditioned on that corpus.",17,18
12871,967387,"Experiments for CCG analysis were performed using automatically assigned POS-tags generated by a symbol-refined HMM tagger (Huang et al.,",15,16
12872,236486252,"For instance, the IPA /j/ was transcribed as /y/ twice, the voiced alveolar approximant /ɹ/ was mistranscribed as the trill /r/ over 200 times, and we found a handful of issues where a phone was transcribed with a Unicode symbol not used in the IPA at all.",42,43
12873,236486252,Most of these were cases where the rare variant was at least two orders of magnitude less frequent than the common variant of the symbol.,24,25
12874,236486137,"In this paper, we use regular expressions of symbol-pairs (that is, regular relations) in the well-known Xerox formalism (Beesley and Karttunen, 2003) to denote rules: for example, ?",9,10
12875,236486137,"Character-level alignment results in pairs: INPUT: d o g 0 OUTPUT: d o g s INPUT: h o t d o g OUTPUT: 0 0 0 d o g Each symbol pair in the alignment represents one of the following types: (1) an identity pair x:x, (2) an insertion 0:x, (3) a deletion x:0, or (4) a substitution x:y. In order to convert a pair of aligned strings into a transformation rule, we simply replace all contiguous sequences of identity pairs with ?",37,38
12876,210155369,"The symbol • denotes string concatenation, lifted to sets: A • B := {ab | a 2 A, b 2 B}.",1,2
12877,86839130,"IO-TSL enhances this with a local tier projection mechanism: whether a symbol is projected on a tier depends on the symbol itself, its local context in the string (i.e. up to m symbols before and/or after it), and up to n previous symbols on the tier.",14,15
12878,86839130,"IO-TSL enhances this with a local tier projection mechanism: whether a symbol is projected on a tier depends on the symbol itself, its local context in the string (i.e. up to m symbols before and/or after it), and up to n previous symbols on the tier.",23,24
12879,86839130,"One can capture this by projecting L iff it occurs immediately to the right of H, and projecting H iff the previous symbol on the tier is L. The strings above would get the tiers L, empty, and LH, respectively.",23,24
12880,86839130,The tier projection mechanism of IO-TSL is defined in terms of contexts that specify when a given symbol should be added to the tier.,19,20
12881,86839130,"Let ⌃ := {a, b, c} and consider the tier projection that always projects the first and last symbol of the string, always projects a, never projects c, and projects b only if the previous symbol on the tier is a. This projection is IOSL-(2,2).",23,24
12882,86839130,"Let ⌃ := {a, b, c} and consider the tier projection that always projects the first and last symbol of the string, always projects a, never projects c, and projects b only if the previous symbol on the tier is a. This projection is IOSL-(2,2).",43,44
12883,86839130,Suppose the first symbol to be projected is an NPI.,3,4
12884,86839130,Again nothing is projected after this second projection step as Principle A is already satisfied or violated depending on whether the second symbol is T or D[ ].,22,23
12885,86839130,"If the first symbol on the tier is not subject to any constraints, then nothing else is projected.",3,4
12886,86839130,"Example grammars for dependencies Let us now apply this idea to the c-string templates from §3, repeated here for the readers convenience: NPI • • • Lic • • • R[ ] T ⇤ D[ ] • • • R[ ] • • • T • • • D[ ] • • • ↵T (↵ starts with pronoun, has more R-expressions than ↵ has pronouns) Now consider the corresponding projection contexts for any arbitrary MG G. We first include h , "", n, ""i for all 2 G so that the first symbol is always projected.",110,111
12887,19037207,"A UService 'l~ansaction System"" was simulated that could assist users with tasks that were either (1) verbal-temporal (e.g., conference registration or cax rental exchanges, in which proper names and scheduling information predominated), or (2) computational-numeric (e.g., personal banking or scientific calculations, in which digits and symbol/sign information predominated).",65,66
12888,236486159,"Input character dropout: To prevent the model from memorizing the training set and to force it to learn about syllable contexts, we randomly replace an input character with the UNK symbol according to a linearly decaying schedule.",32,33
12889,234904920,Each automaton checks if the symbol on the edge from state q 0 to q 1 is recognized.,5,6
12890,234904920,"The SP phonotactic model evaluates the co-emission probability Coemit(σ i ), which is the probability that all of the factored PDFAs emit a symbol σ i at the same time (Shibata & Heinz 2019).",26,27
12891,234904920,"The feature-based representation is also implemented by replacing the alphabet with a set of feature values means ""enter state q 1 from state q 0 , emit symbol k and output probability 1/2"".",30,31
12892,7752112,setf (symbol-functio n fn-name) (memo (symbol-function fn-name)))).,2,3
12893,7752112,setf (symbol-functio n fn-name) (memo (symbol-function fn-name)))).,13,14
12894,7752112,"When passed a symbol that names a function, memoize changes the global definition of the function to a memo-function.",3,4
12895,7752112,"The core of the parser is only 15 lines of code: (defun parse (tokens start-symbol) ""Parse a list of tokens, return parse trees and remainders."" (",19,20
12896,7752112,"if (eq (first tokens) start-symbol) (list (make-parse :tree (first tokens) :rem (rest tokens))) (mapcan #' (lambda (rule) (extend-parse (lhs rule) nil tokens (rhs rule))) (rules-for start-symbol) ) ) ) (defun extend-parse (lhs rhs rem needed) ""Parse the remaining needed symbols. "" (",9,10
12897,7752112,"if (eq (first tokens) start-symbol) (list (make-parse :tree (first tokens) :rem (rest tokens))) (mapcan #' (lambda (rule) (extend-parse (lhs rule) nil tokens (rhs rule))) (rules-for start-symbol) ) ) ) (defun extend-parse (lhs rhs rem needed) ""Parse the remaining needed symbols. "" (",62,63
12898,7752112,"rhs) (defun lhs (tree) (first tree)) (defun rhs (tree) (rest tree)) (defun rules-for (symbol) ""Return a list of the rules with symbol on the left hand side."" (",30,31
12899,7752112,"rhs) (defun lhs (tree) (first tree)) (defun rhs (tree) (rest tree)) (defun rules-for (symbol) ""Return a list of the rules with symbol on the left hand side."" (",40,41
12900,7752112,remove symbol *grammar* :key #'lhs :test-not #'eql)).,1,2
12901,7752112,"memoize 'rules-for) (memoize 'parse :test #'equal :key #'identity) (defun parser (tokens start-symbol) ""Return all complete parses of a list of tokens."" (",28,29
12902,7752112,"clear-memoize 'parse) (mapcar #'parse-tree (remove-if-not @'null (parse tokens start-symbol) :key #'parse-rem))) As an example, consider the following grammar, taken from page 321 of Aho and Ullman (1972) . (",25,26
12903,7752112,The problem is that hashing is done on the complete argument list to parse: the start symbol and the list of remaining tokens.,17,18
12904,7752112,This is because Scheme lacks symbol-value and set-symbol-value!,5,6
12905,7752112,This is because Scheme lacks symbol-value and set-symbol-value!,11,12
12906,202763794,An example fair symbol example set theoretic definition x ≡ y couch ≡ sofa x = y x y crow bird x ⊂ y x y bird crow x ⊃ y x ∧ y human ∧ nonhuman x ∩ y = ∅ ∧ x ∪ y = U x | y cat | dog x ∩ y = ∅ ∧ x ∪ y = U x y animal nonhuman x ∩ y = ∅ ∧ x ∪ y = U x # y hungry # hippo (all other cases) Table 2: P every/some ( ) = dog animal P every/some some every Figure 2 : Natural logic inference cast as composition on aligned semantic parse trees.,3,4
12907,221399281,"Suppose that we want to simulate a stochastic process by generating each symbol based on some memory representation M of the past, and that we want to find a memory representation M that simulates the process as well as possible while having minimal information content, measured in bits.",12,13
12908,221399281,"A PDFA is characterized by a set of internal states Q, an alphabet Σ, an emission distribution O of symbols ∈ Σ conditional on a state ∈ Q, a transition function T : Q × Σ → Q defining which state the machine transitions into after emitting a symbol, and distinguished initial and final states.",51,52
12909,221399281,"Our indexing convention is: at time t, the PDFA is in state q t ; it generates symbol x t before transitioning into the next state q t+1 .",19,20
12910,221399281,"q 0 q 1 b : 1/4 c : 1/4 # : 1/4 a : 1/3 a : 1/4 c : 1/3 # : 1/3 Figure 2: SL 2 PDFA of ¬ab, Σ = {a, b, c} We use the following construction to generate a stationary ergodic stochastic process from a PDFA: whenever the PDFA emits an end-of-word symbol #, it always transitions back into the initial state.",69,70
12911,221399281,"For a general PFA, the entries of this matrix are given by marginalizing over the emission distribution O: p(q t+1 |q t ) = xt∈Σ p O (x t |q t )p T (q t+1 |x t , q t ), where p T is the probability of transitioning into state q t+1 after generating symbol x t from state q t .",61,62
12912,221399281,"In the case of SL 2 languages, we compute E by constructing a symbol transition matrix, a stochastic matrix whose entries represent p(x t+1 |x t ), marginalizing over q t and q t+1 .",14,15
12913,221399281,"We also need the stationary distribution over symbols, derived from the symbol transition matrix by the same procedure as above.",12,13
12914,221399281,"Crypticity C = S − E. In general, crypticity is bounded above by the uncertainty about the emitting state given a symbol: C ≤ H[Q t |X t ], with equality iff X is an SL 2 language.",22,23
12915,1115129,"NARRATIVE Information about story structure takes the form of a grammar, whose starting symbol is complex stories.",14,15
12916,18615417,"Europarl 01-19-00 For Europe to become the symbol of peace and fraternity , we need a bold and generous policy to come to the aid of the most disadvantaged .",11,12
12917,1320875,"For example, if we have L o S o T o M, we could either apply S to L, apply T to the result and intersect the result with M, or compose S with T, reverse apply the result to M and intersect the result with L. We are thus justified in our use of the same symbol for composition, application and intersection, and we will in the rest of the paper use the term ""(generalized) composition"" for all of these operations.",62,63
12918,5511636,"reD(t) Using the Treebank training corpus, P(~a) is estimated by counting the number of times the rule a ~ fl appears in the training, divided by the number of times the nonterminal symbol Ot appears.",36,37
12919,7764845,"In their simplest version, they then generate a sequence of output symbols, one symbol at a time.",15,16
12920,7764845,"From this vector, the parsers predict the most likely output symbol.",11,12
12921,7764845,"The next state, frmo which the next output symbol is predicted, is obtained by applying the transition parameters of the decoder to the current state.",9,10
12922,201304248,"In order to represent individual sentences, we insert external [CLS] tokens at the start of each sentence, and each [CLS] symbol collects features for the sentence preceding it.",26,27
12923,201304248,"With BERTSUM, vector t i which is the vector of the i-th [CLS] symbol from the top layer can be used as the representation for sent i .",18,19
12924,53079817,"The vocabulary size is set to be 8, 000 for computational efficiency and words out of vocabulary are replaced by the symbol ""unk"".",22,23
12925,128000127,"From the perspective of a generalized substitution principle, the alignment problem in machine translation is the same as the class induction problem in language modeling, but with sequences featuring large numbers of gappy fragments and a boundary symbol .",39,40
12926,220047370,We also use a dropout of 0.5 on the input word embeddings and replace singleton words in the training set with an out-of-vocabulary symbol with a proba- bility of 0.5 to improve robustness to unseen words.,27,28
12927,5509775,The notation A[xa] is used to denote the nonterminal A associated with any stack whose top symbol is ~r.,17,18
12928,5509775,"The following production may be glossed: ""if in state q and scanning the symbol X, then change state to q~, write the symbol Y and move left"" 1 A[q(W(x), X, y)] --* A[q'(x, W, r(y))] One solution to this problem is to prevent a single daughter sharing more than one of its subtrees with the mother.",15,16
12929,5509775,"The following production may be glossed: ""if in state q and scanning the symbol X, then change state to q~, write the symbol Y and move left"" 1 A[q(W(x), X, y)] --* A[q'(x, W, r(y))] One solution to this problem is to prevent a single daughter sharing more than one of its subtrees with the mother.",26,27
12930,5509775,This can be repeated an 1There will be a set of such productions for each tape symbol W.  unbounded number of times so that all full binary trees are produced.,16,17
12931,5509775,"potentially unbounded stack in a particular array entry, it suffices to store just In this derivation tree, the node labelled B[aa] is a distinguished descendant of the root s and is the first point below A[c~rcr ~] at which the top symbol (or) of the (unbounded) stack aa is exposed.",47,48
12932,6644980,"Furthermore, object.attribute denotes the value of the property attribute at object and the symbol self refers to the current lexical item.",14,15
12933,60334,"The values of n required are those for which, for some spelling rule, there are k characters in the target lexical string and n -k from the beginning of the right context up to (but not including) a boundary symbol.",43,44
12934,17206825,"In the model described in this paper, the symbols written are dependency relation symbols, or the empty symbol e. The use of relation symbols here is a result of the historical development of the system from an earlier transfer model.",19,20
12935,17206825,"Translator A translator based on head transducers consists of the following components: * A bilingual lexicon in which entries are 5tuples (w,v, M,q,c), associating a pair of source-target words with a head transducer M, an initial state q, and a cost c. Left transition: write a symbol rl onto the right end of L1, write symbol r2 to position a in the target sequences, and enter state qi+l.",62,63
12936,17206825,"Translator A translator based on head transducers consists of the following components: * A bilingual lexicon in which entries are 5tuples (w,v, M,q,c), associating a pair of source-target words with a head transducer M, an initial state q, and a cost c. Left transition: write a symbol rl onto the right end of L1, write symbol r2 to position a in the target sequences, and enter state qi+l.",72,73
12937,17206825,"Right transition: write a symbol rl onto the left end of R1, write a symbol r~ to position a in the target sequences, and enter state qi+l. •",5,6
12938,17206825,"Right transition: write a symbol rl onto the left end of R1, write a symbol r~ to position a in the target sequences, and enter state qi+l. •",16,17
12939,6300554,"Through the disambiguation process, discriminants and the analyses they apply to can be undecided, correct (""good"", shown in normal video), or incorrect (""bad"", normal video but preceded a negation symbol ..... ).",41,42
12940,52114454,"ber of ""self-loop"" transitions, each consuming an input without changing the path score (since it's weighted by 1); it then consumes an input symbol ↵ and takes a transition weighted by µ(↵), and reaches the final state q 1 (with ⇢(q 1 ) = 1); it may further consume more input by taking selfloops at q 1 , updating the path score by multiplying it by (↵) for each symbol ↵.",32,33
12941,52114454,"ber of ""self-loop"" transitions, each consuming an input without changing the path score (since it's weighted by 1); it then consumes an input symbol ↵ and takes a transition weighted by µ(↵), and reaches the final state q 1 (with ⇢(q 1 ) = 1); it may further consume more input by taking selfloops at q 1 , updating the path score by multiplying it by (↵) for each symbol ↵.",89,90
12942,52114454,"2010) , in the sense that it consumes one input symbol to reach the final state from the initial state.",11,12
12943,12921197,"With 28 tags, 2 hypertags and a start symbol the upper bound on the number of input nodes is 313 + 312.",9,10
12944,13205733,"The method ""replace by alternative"" consists in replacing two rules with the same left-hand symbol and the same beginning of their right sides by one rule -e.g.",18,19
12945,13205733,This improves effectiveness because of a single (rather than double) attempt to expand the same non-terminal symbol to the given string of terminals.,20,21
12946,13205733,"The method ""from longest to shortest"" says that if a symbol A occurs as a non-last right-hand symbol of a rule -e.g.",12,13
12947,13205733,"The method ""from longest to shortest"" says that if a symbol A occurs as a non-last right-hand symbol of a rule -e.g.",23,24
12948,13205733,in the rule D--cAE -and the grammar includes more rules than one to replace the symbol A -e.g.,17,18
12949,13205733,"rules: A--~BC, A--~B, then it is more effective to make the algorithm check the ""longer"" rule A---~BC before checking the ""shorter"" rule A---~B. This makes it possible to block backtracking in the rule D-+A!E. The method ""replace symbol by parameter"" may be used when right-hand sides of productions for different symbols start with the same (sequence of) symbol(s).",45,46
12950,53047545,"For each word w t , it generates a label that encodes: (1) the number of ancestors in the tree that the words w t and w t+1 have in common, and (2) the nonterminal symbol at the lowest common ancestor.",41,42
12951,53047545,They propose an intermediate representation where dependency labels from a head to its dependents encode the nonterminal symbol and an attachment order that is used to arrange nodes into constituents.,17,18
12952,53047545,"2 The label generated for each word encodes the number of common ancestors in the constituent tree between that word and the next, and the nonterminal symbol associated with the lowest common ancestor.",27,28
12953,53047545,"We will assign it a 2-tuple label l i = (n i , c i ), where: n i is an integer that encodes the number of common ancestors between w i and w i+1 , and c i is the nonterminal symbol at the lowest common ancestor.",47,48
12954,53047545,"The nonterminal symbol of the second ancestor of w 2 (X) cannot be decoded, as no pair of words have X as their lowest common ancestor.",2,3
12955,53047545,"For clarity, we use the name intermediate unary chains to refer to unary chains that end up into a nonterminal symbol (e.g. X → Y in Figure 3 ) and leaf unary chains to name those that yield a PoS tag (e.g. Z → T 5 ).",21,22
12956,53047545,"Intermediate unary chains are collapsed into a chained single symbol, which can be encoded by Φ |w| as any other nonterminal symbol.",9,10
12957,53047545,"Intermediate unary chains are collapsed into a chained single symbol, which can be encoded by Φ |w| as any other nonterminal symbol.",22,23
12958,53047545,"We briefly comment on the accuracy (percentage of correctly predicted labels, no symbol excluded here) of our baselines.",14,15
12959,11849972,"Let f_p_l be a distinct relation symbol then we can equivalently define the first-daughter constraint by: • x=[fpl]y~x=f_p_l:yA x = 3f: y A f_p_l(x):p*: f(x) The translation states that y (which is the f_p_lvalue of x) precedes or is equal to every f-value of x and y is a f-value of x. For this to work, we require that the feature symbol f_p_l appears only in the translation of the constraint x = [f p 1]y.",6,7
12960,11849972,"Let f_p_l be a distinct relation symbol then we can equivalently define the first-daughter constraint by: • x=[fpl]y~x=f_p_l:yA x = 3f: y A f_p_l(x):p*: f(x) The translation states that y (which is the f_p_lvalue of x) precedes or is equal to every f-value of x and y is a f-value of x. For this to work, we require that the feature symbol f_p_l appears only in the translation of the constraint x = [f p 1]y.",81,82
12961,11849972,"If a constraint solving rule transforms Cs to Crs then: z,a ~ C, iffz, a ~ C', Proof Sketch: The soundness claim can be verified by checking that every rule indeed preserves the interpretation of every variable and every relation symbol.",48,49
12962,5334746,"1987) : e:0 <=> =:C2 < +:0 V:= > or < C:C V:V> <+:0 e:e> or {g:g c:c} <+:0 {e:e i:i} > or 1:0 +:0 or c:c <+:0 a:0 t:t> Rules are phrased in terms of symbol-pairs (written with an infix colon), where the first in the pair is a lexical symbol and the second is a surface symbol.",74,75
12963,5334746,"1987) : e:0 <=> =:C2 < +:0 V:= > or < C:C V:V> <+:0 e:e> or {g:g c:c} <+:0 {e:e i:i} > or 1:0 +:0 or c:c <+:0 a:0 t:t> Rules are phrased in terms of symbol-pairs (written with an infix colon), where the first in the pair is a lexical symbol and the second is a surface symbol.",94,95
12964,5334746,"1987) : e:0 <=> =:C2 < +:0 V:= > or < C:C V:V> <+:0 e:e> or {g:g c:c} <+:0 {e:e i:i} > or 1:0 +:0 or c:c <+:0 a:0 t:t> Rules are phrased in terms of symbol-pairs (written with an infix colon), where the first in the pair is a lexical symbol and the second is a surface symbol.",101,102
12965,5334746,"Each context has a left part and a right part, each of these being essentially a regular expression over symbol-pairs, where angle brackets indicate sequences of pairs and braces indicate alternatives (disjunction).",20,21
12966,5334746,"Certain versions of the notation may also allow the ""Kleene star"" symbol ""*"" to indicate zero or more repetitions, and the insertion of optional elements.",13,14
12967,5334746,"In this example, ""C"", ""V"", ""C2"", and ""="" represent subsets of the relevant symbol alphabets and ""+"" is an abstract symbol occurring in certain lexical forms.",25,26
12968,5334746,"In this example, ""C"", ""V"", ""C2"", and ""="" represent subsets of the relevant symbol alphabets and ""+"" is an abstract symbol occurring in certain lexical forms.",34,35
12969,5334746,"It is only there that the special null symbol ""0"" takes on special significance (see later section).",8,9
12970,5334746,"Hence most of the definitions, and the subsequent diseussion of generative power, are concerned with sequences of symbol-pairs, which is equivalent to considering only pairs of strings of equal length.",19,20
12971,5334746,"BASIC DEFINITIONS Given any two finite symbolic alphabets, A and A', a symbol-pair from A and A' is a pair <a, a'> where a ~ A and a' ~ A'.",15,16
12972,5334746,"Such symbol-pairs will normally be written as ""a:a'"".",1,2
12973,5334746,"A symbol-pair sequence from A and A' is simply a sequence (possibly empty) of symbol-pairs from A and A', and a symbol-pair language over A and A' is a set of symbol-pair sequences (i.e. a subset of (A x A')*).",1,2
12974,5334746,"A symbol-pair sequence from A and A' is simply a sequence (possibly empty) of symbol-pairs from A and A', and a symbol-pair language over A and A' is a set of symbol-pair sequences (i.e. a subset of (A x A')*).",19,20
12975,5334746,"A symbol-pair sequence from A and A' is simply a sequence (possibly empty) of symbol-pairs from A and A', and a symbol-pair language over A and A' is a set of symbol-pair sequences (i.e. a subset of (A x A')*).",30,31
12976,5334746,"A symbol-pair sequence from A and A' is simply a sequence (possibly empty) of symbol-pairs from A and A', and a symbol-pair language over A and A' is a set of symbol-pair sequences (i.e. a subset of (A x A')*).",43,44
12977,5334746,"Given two alphabets A and A', and a symbol-pair sequence S from A and A', a sequence <P1,..P~> of symbol-pair sequences from A and A' is said to be a partition of S iff S = P1P2....Pn (i.e. the concatenation of the P3 CONTEXTS AND '.RULES Given two symbol sets A and A', a context-expression from A and A' is a regular expression over A x A'.",10,11
12978,5334746,"Given two alphabets A and A', and a symbol-pair sequence S from A and A', a sequence <P1,..P~> of symbol-pair sequences from A and A' is said to be a partition of S iff S = P1P2....Pn (i.e. the concatenation of the P3 CONTEXTS AND '.RULES Given two symbol sets A and A', a context-expression from A and A' is a regular expression over A x A'.",29,30
12979,5334746,"Given two alphabets A and A', and a symbol-pair sequence S from A and A', a sequence <P1,..P~> of symbol-pair sequences from A and A' is said to be a partition of S iff S = P1P2....Pn (i.e. the concatenation of the P3 CONTEXTS AND '.RULES Given two symbol sets A and A', a context-expression from A and A' is a regular expression over A x A'.",65,66
12980,5334746,"That is, a contextexpression characterises a regular set of sequences of symbol-pairs.",12,13
12981,5334746,"For example, the expression Given two alphabets A and A', a twolevel morphological rule over A and A' consists of a pair <P, C> where P is a symbol-pair from A and A', and C is a non-empty set of pairs <LC,RC> where LC and RC are contextexpressions from A and A'.",35,36
12982,5334746,"A context-expression ce is said to match at the right-end a symbol-pair sequence S iff there is a partition "",~v 1, P2> of S such that Pz is an element of the set characterised by ce.",15,16
12983,5334746,"A context-expression ce is said to match at the left-end a symbol-pair sequence S iff there is a partition <P1, P2 > of S such that P1 is an element of the set characterised by ce.",15,16
12984,5334746,"The first basic form of rule is the context restriction rule written with the operator ""=>"" separating the symbol-pair from the specification of the contexts.",21,22
12985,5334746,"On the other hand, a surface coercion rule, written using the operator ""<--"" indicates that wherever the contexts (i.e. the right side of the rule) occur, and the lexical symbol is as given in the pair on the left side of the rule, then the surface symbol must be as given on the left side of the rule.",37,38
12986,5334746,"On the other hand, a surface coercion rule, written using the operator ""<--"" indicates that wherever the contexts (i.e. the right side of the rule) occur, and the lexical symbol is as given in the pair on the left side of the rule, then the surface symbol must be as given on the left side of the rule.",55,56
12987,5334746,"For example: 15 <= b:b e:e would mean that ""whenever there is a lexical b and surface b on the left, a lexical e and surface e on the right, and a lexical 1, then the surface symbol must be i"".",47,48
12988,5334746,"A set R of two-level morphological rules contextually allows a symbol-pair sequence S iff, for every partition <P1, a:a', P2> of S, either there is no rule of the form <a:a', C> in R, or there is at least one rule <a:a', C> in R such that C contains a context pair <LC, RC> such that LC matches P1 at the right end and RC matches P2 at the left end.",12,13
12989,5334746,"A two-level morphological rule R = <<a,a'>, C> coercively allows a symbol-pair sequence S iff for every possible partition <Px, b:b', P2> of S and every element <LC, RC> in C such that LC matches P1 at the right end, and RC matches/'2 at the left end, if b = a, then b' = a'.",21,22
12990,5334746,"An alternative but equivalent variation on the last definition would be that a two-level morphological rule R = <<a,a'>, C> coercively disallows a symbol-pair sequence S iff there is a possible partition <P1, b:b', P2 > of S and an element <LC, RC> in C such that LC matches P1 at the right end, RC matches P2 at the left end, b = aand b' # a'.",33,34
12991,5334746,"When set-mnemonics and variables are used within rules, these are deemed to cover not all possible symbol-pairs, but only those which are ""feasible"".",19,20
12992,5334746,"Even when not using these abbreviatory devices, it is necessary to have some notion of feasible symbol-pair, since such pairs are allowed to occur freely even if licensed by no rule (providing no rule forbids them).",17,18
12993,5334746,"If we assume that the notion of a symbol-pair occurring in a regular expression is clear enough, occurrence within a rule set is straightforward--a symbol-pair a:a' is said to occur in a rule <b:b', C> iff either a:a' = b:b' or for at least one element <LC, RC> of C, a:a' occurs in at least one of LC and RC.",8,9
12994,5334746,"If we assume that the notion of a symbol-pair occurring in a regular expression is clear enough, occurrence within a rule set is straightforward--a symbol-pair a:a' is said to occur in a rule <b:b', C> iff either a:a' = b:b' or for at least one element <LC, RC> of C, a:a' occurs in at least one of LC and RC.",29,30
12995,5334746,"Given a two-level morphological grammar G = <CR, SC>, the set of feasible pairs in G is the set of symbol-pairs {a:a' I a:a' occurs in some element of CR u SC} (In an implemented system, the user may be allowed to declare certain pairs as feasible, but at this level of abstraction we do not need to include this in our definition of a two-level morphological grammar, since such an effect could be represented by including rather vacuous context-restriction rules of the form Given a two-level morphological grammar G = <CR, SC>, a symbol-pair sequence S is generated by G iff all the following hold: (i) all the symbol-pairs in S are feasible pairs in G; (ii) each rule in SC coercively allows S; (iii) the set CR of rules contextually allows S. Notice that the two classes of rules are treated slightly differently -surface coercion rules are conjoined, forming a set of constraints all of which must be met, and context restriction rules are disjoined, giving a set of possible licensing contexts.",26,27
12996,5334746,"Given a two-level morphological grammar G = <CR, SC>, the set of feasible pairs in G is the set of symbol-pairs {a:a' I a:a' occurs in some element of CR u SC} (In an implemented system, the user may be allowed to declare certain pairs as feasible, but at this level of abstraction we do not need to include this in our definition of a two-level morphological grammar, since such an effect could be represented by including rather vacuous context-restriction rules of the form Given a two-level morphological grammar G = <CR, SC>, a symbol-pair sequence S is generated by G iff all the following hold: (i) all the symbol-pairs in S are feasible pairs in G; (ii) each rule in SC coercively allows S; (iii) the set CR of rules contextually allows S. Notice that the two classes of rules are treated slightly differently -surface coercion rules are conjoined, forming a set of constraints all of which must be met, and context restriction rules are disjoined, giving a set of possible licensing contexts.",124,125
12997,5334746,"Given a two-level morphological grammar G = <CR, SC>, the set of feasible pairs in G is the set of symbol-pairs {a:a' I a:a' occurs in some element of CR u SC} (In an implemented system, the user may be allowed to declare certain pairs as feasible, but at this level of abstraction we do not need to include this in our definition of a two-level morphological grammar, since such an effect could be represented by including rather vacuous context-restriction rules of the form Given a two-level morphological grammar G = <CR, SC>, a symbol-pair sequence S is generated by G iff all the following hold: (i) all the symbol-pairs in S are feasible pairs in G; (ii) each rule in SC coercively allows S; (iii) the set CR of rules contextually allows S. Notice that the two classes of rules are treated slightly differently -surface coercion rules are conjoined, forming a set of constraints all of which must be met, and context restriction rules are disjoined, giving a set of possible licensing contexts.",144,145
12998,5334746,"If no rules apply to a particular symbol-pair, it is acceptable if and only if it is feasible.",7,8
12999,5334746,"It is in the course of integrating the string-matching with the segmentation that the special null symbol will be needed, so we must first define the notion of two strings being the same after the removal of nulls.",18,19
13000,5334746,The other minor formal definition we need is to allow us to move from equal-length sequences of symbol-pairs to pairs of equallength symbol-sequences in the obvious way.,19,20
13001,5334746,The other minor formal definition we need is to allow us to move from equal-length sequences of symbol-pairs to pairs of equallength symbol-sequences in the obvious way.,26,27
13002,5334746,"Then the symbol-pair sequence associated with $1 and $2 is the sequence al :bl....an:b,, We can then define a two-level morphological grammar as licensing a pair of strings of equal length, iff their associated symbol-pair sequence is generated by the grammar.",2,3
13003,5334746,"Then the symbol-pair sequence associated with $1 and $2 is the sequence al :bl....an:b,, We can then define a two-level morphological grammar as licensing a pair of strings of equal length, iff their associated symbol-pair sequence is generated by the grammar.",48,49
13004,5334746,"A lexical segmentation system consists of a tuple (AL, AS, 0, L, (3) where AL is a finite set (the lexical alphabet ), AS is a finite set (the surface alphabet ), 0 is a symbol which is not an element of AL u AS, L is a set (the set of lexical forms ) of non-null elements of AL*, and G is a two-level morphographemic grammar based on AL u {0} and AS u {0}.",46,47
13005,5334746,"Given a lexical segmentation system (AL, AS, 0, L, G), a siring S e AS* can be segmented as <ll,...l~> where li ~ L for all i, if there are strings S 1 ~ AL*, $2 ~ AS* such that the following all hold: delete(0, $1) = lll2....In delete(0, $2) = S G licenses <$1, $2> Notice that there is no distinguished symbol indicating a morpheme boundary or word boundary.",93,94
13006,5334746,"LANGUAGES GENERATED With the above definitions, it is now possible to ask what sorts of symbol-pair languages can be characterised using a two-level morphological grammar.",16,17
13007,5334746,"Here we shall ignore the issue of the interface to the lexicon, and simply consider the capacity of two-level morphological grammars to characterise sets of sequences of symbol-pairs.",30,31
13008,5334746,"Let EI and E2 be symbol-pair sequences such that R contextually allows El, and R contextually allows E2.",5,6
13009,5334746,"Proof : If there is no symbol-pair a:a' in ErE 2 such that there is some rule <<a:a'>, C> in R, then R contextually allows ErE 2 for trivial reasons.",6,7
13010,5334746,"Let a:a' be a symbol-pair occurring in ErE2 such that there is at least one rule <<a:a',C> in R. Let <Pt, a:a',P2> be a partition of ERE2.",7,8
13011,5334746,"Let El, E2, E3 be symbol-pair sequences such that E1E2E3 is coercively allowed by R. Then E2 is coercively allowed by R. Proof : If E 2 were not coercively allowed by R, it would mean that there is a partition <$1, a:b, $2> of E2 such that for some <LC, RC> in C, LC matches S~ at the right end, RC matches $2 at the left end, and b ~ a'.",7,8
13012,5334746,"Corollary : Let C be a set of two-level morphological rules, all of which coercively allow a symbol-pair sequence E. Then all of the rules in C coercively allow any subsequence of E. Lemma 3 : Let G be a two-level morphological grammar <CR, SC>, and let L(G) be the set of symbol-pair sequences generated by G. Suppose that there are sequences E 1, E2, E B, E 4 such that E2 ~ L(G), E3 ~ L(G), and E1E2E3E4 L(G).",20,21
13013,5334746,"Corollary : Let C be a set of two-level morphological rules, all of which coercively allow a symbol-pair sequence E. Then all of the rules in C coercively allow any subsequence of E. Lemma 3 : Let G be a two-level morphological grammar <CR, SC>, and let L(G) be the set of symbol-pair sequences generated by G. Suppose that there are sequences E 1, E2, E B, E 4 such that E2 ~ L(G), E3 ~ L(G), and E1E2E3E4 L(G).",64,65
13014,5334746,"Proof: (i) Since E1E2E3E4 ~ L(G), all the symbol-pairs in it are feasible with respect to G, hence all the symbol-pairs in E2E3 are feasible. (",13,14
13015,5334746,"Proof: (i) Since E1E2E3E4 ~ L(G), all the symbol-pairs in it are feasible with respect to G, hence all the symbol-pairs in E2E3 are feasible. (",28,29
13016,5334746,"In the latter case, each transducer deals with some linguistic phenomenon, and a sequence of symbol-pairs is generated by the grammar if every transducer in the grammar accepts it.",17,18
13017,5334746,"That is, the symbol-pair sequence must be in the intersection of the languages accepted by the transducers (viewed as acceptors); in procedural terms, this is often referred to as ""having the transducers executed in parallel"".",4,5
13018,5334746,"Kaplan(1988) discussed the notion of a regular relation, which is, roughly speaking, a symbol-pair language which can be characterised by a regular expression of symbol-pairs.",17,18
13019,5334746,"Kaplan(1988) discussed the notion of a regular relation, which is, roughly speaking, a symbol-pair language which can be characterised by a regular expression of symbol-pairs.",30,31
13020,5334746,"Not surprisingly, a set of symbol-pair sequences is regular if and only if it can be accepted by a finite-state transducer in the obvious way.",6,7
13021,5334746,It follows that the symbol-pair languages accepted by the two-level transducer model are exactly the regular relations.,4,5
13022,5334746,"The obvious question is whether there is a difference in power; in fact, there is: Theorem: There are regular relations (i.e. symbol-pair languages characterised by regular expressions of symbol-pairs) which cannot be generated by any two-level morphological grammar.",26,27
13023,5334746,"The obvious question is whether there is a difference in power; in fact, there is: Theorem: There are regular relations (i.e. symbol-pair languages characterised by regular expressions of symbol-pairs) which cannot be generated by any two-level morphological grammar.",35,36
13024,5334746,"According to the definitions given here, the empty sequence of symbol-pairs is in every language generated by a two-level morphological grammar, since it conforms to the definition regardless of the content of the rules.",11,12
13025,5334746,"The generative power of two-level morphological grammars, viewed as ways of characterising sets of sequences of symbol-pairs, is less than that of arbitrary transducers, despite the fact that the transducer formulation is sometimes discussed as if it were the essential definition of the two-level model.",19,20
13026,5602513,"P(al,..., an), where P is an n-ary predicate symbol and the ai are discourse referents.",15,16
13027,220046511,"For example, previous works have included the 'symbol' annotations provided, as an additional stream.",9,10
13028,220046511,These annotations are image regions depicting symbol objects.,6,7
13029,220046511,A symbol object signifies an abstract concept.,1,2
13030,220046511,2018) by adding a symbol stream using the 'symbol' annotations provided by the dataset.,5,6
13031,220046511,2018) by adding a symbol stream using the 'symbol' annotations provided by the dataset.,10,11
13032,220046511,"i) The main branch, which uses attention mechanism to represent an image as a weighted combination of object regions, (ii) The knowledge branch, which provides 'symbol' distribution for the image by making use of densecaps (Johnson et al.,",32,33
13033,220046511,2016) to map image to the 'symbol' labels.,8,9
13034,220046511,"2018) proposes a weakly supervised learning algorithm that uses a multi-hop coattention mechanism to iteratively refine the attention map that associates image proposals with symbol labels, thereby aggregating information from both modalities.",27,28
13035,7589418,"In addition, entities not appearing in a sentence are marked by a special symbol (-).",14,15
13036,52054914,"The premise-oblivious text-classifier that 7 The symbol → represents entailment relationship achieves 67.0 F1 on SNLI, and 53.9 on Multi-NLI achieves 61.9 on MedNLI.",10,11
13037,6494933,"Observed word error rates were 1.1 percent for the 3050-word, 52-symbol ATIS task, and 4.1 percent for the 25,000-word, 86-symbol WSJ task, using a version of BYBLOS with virtually no fine tuning for cursive handwriting.",15,16
13038,6494933,"Observed word error rates were 1.1 percent for the 3050-word, 52-symbol ATIS task, and 4.1 percent for the 25,000-word, 86-symbol WSJ task, using a version of BYBLOS with virtually no fine tuning for cursive handwriting.",30,31
13039,35435461,"In addition to transliterating words into phoneme sequences, we also represent word break characters as a specific symbol.",18,19
13040,35435461,"That is, we compute the most probable hypothesis word W given a phoneme sequence ρ: arg max i P ( W i | ρ ) (1) We can consider the phonetic encoding of plaintext to be a homophonic cipher; that is, a cipher in which each symbol can correspond to one or more possible decodings.",52,53
13041,2423360,"FunQL is a variable-free query language, where each predicate is treated as a function symbol that modifies an argument list.",17,18
13042,2423360,"However, since the key idea of our model is to capture salient meaning for the task at hand rather than strictly obey syntax, we would not expect the As a reminder, the task in SPADES is to predict the entity masked by a blank symbol ( ).",47,48
13043,2423360,"For subordinate clauses, SCANNER tends to take shortcuts identifying as predicates words closest to the blank symbol.",17,18
13044,39021228,"Finally, this work establishes a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby: the parameters of SDR models can be obtained from those of Skip-Gram models simply by adding information on symbol frequencies.",47,48
13045,43225062,"State Update In the generic decoding process, each RNN hidden state s t is updated with the previous state s t−1 , the word embedding of previous predicted symbol y t−1 , and an optional context vector c t (with attention mechanism).",29,30
13046,52156976,The ⊥ symbol is reserved to denote documents which are not associated with any cluster yet.,2,3
13047,53082684,"As shown in Figure 1 , the FOFE code could be efficiently computed via time-delayed recursive structure, where the symbol z −1 in the figure represents a unit time delay (or equivalently a memory unit) from z t to z t−1 .",22,23
13048,229923207,"Given the linguistic symbol sequence U , we generate a speech waveform S according to the U2S module P (S | U ).",3,4
13049,229923207,We can decrease the average length of the symbol sequence U by employing a lossy form of run-length encoding (RLE) (see Figure 2 ) which retains the sequence of symbol identities but discards duration information.,8,9
13050,229923207,We can decrease the average length of the symbol sequence U by employing a lossy form of run-length encoding (RLE) (see Figure 2 ) which retains the sequence of symbol identities but discards duration information.,34,35
13051,12718048,"2016) is closely related to our model in using the underlying grammar as prior knowledge to constrain the generation process of derivation trees, while our method is based on a unified grammar model which jointly captures production rule application and terminal symbol generation, and scales to general purpose code generation tasks.",43,44
13052,14235614,"2 In fact in normal form ITGs, we can simply check if there • The LHS nonterminal symbol (possibly suffixed by the empty string ǫ) can be derived from the start symbol. •",18,19
13053,14235614,"2 In fact in normal form ITGs, we can simply check if there • The LHS nonterminal symbol (possibly suffixed by the empty string ǫ) can be derived from the start symbol. •",34,35
13054,14235614,"Some brief motivation for the properties singled out: is a production rule with the start symbol in the LHS that introduces all the words in the right order, since all production rules with nonterminal symbols in the RHS are branching and contain no terminal symbols.",16,17
13055,14235614,Note that there can be 2 n many distinct feature structures for each nonterminal symbol in a chart.,14,15
13056,14235614,Note also that the proof can be modified for the case where both sides are islandfree: Just add a dummy symbol to the clause side and allow (or force) all propositional variables to be aligned to this dummy symbol.,21,22
13057,14235614,Note also that the proof can be modified for the case where both sides are islandfree: Just add a dummy symbol to the clause side and allow (or force) all propositional variables to be aligned to this dummy symbol.,41,42
13058,236486290,"The buffer interacts with the input in restricted ways: 1) the buffer is queue-like; 2) the buffer needs to work on the same alphabet as the input, unlike the stack in a pushdown automata (PDA), for example; 3) once one symbol is removed from the buffer, everything else must also be wiped off before the buffer is available for other symbol addition.",52,53
13059,236486290,"The buffer interacts with the input in restricted ways: 1) the buffer is queue-like; 2) the buffer needs to work on the same alphabet as the input, unlike the stack in a pushdown automata (PDA), for example; 3) once one symbol is removed from the buffer, everything else must also be wiped off before the buffer is available for other symbol addition.",73,74
13060,236486290,"A Finite-State Buffered Machine (FSBM) is a 7-tuple Σ, Q, I, F, G, H, δ where • Q: a finite set of states • I ⊆ Q: initial states • F ⊆ Q: final states • G ⊆ Q: states where the machine must enter buffering (B) mode • H ⊆ Q: states visited while the machine is emptying the buffer • G ∩ H = ∅ • δ: Q × (Σ ∪ { }) × Q: the state transitions according to a specific symbol Specifying G and H states allows an FSBM to control what portions of a string are copied.",107,108
13061,236486290,The informal intuition goes as follows: relabel the odd arcs to mapped strings and add states to split the arcs so that there is only one symbol or on each arc in M h .,27,28
13062,236486290,The fact that complete-path FSBMs guarantee the closure under homomoprhism allows theorists to perform analyses at certain levels of abstraction of certain symbol representations.,24,25
13063,236486290,"Finite state machines cannot handle the incurred crossing dependencies while the augmented copying mechanism only contributes to recognizing identical copies, but not general cases of symbol correspondence.",27,28
13064,236486290,"After reading the first w in input and buffering this chunk of string in the memory, the transducer can output for each matched symbol when transiting among H states.",24,25
13065,236486255,"The key difference between deterministic 1-way FSTs and deterministic 2-way FSTs are the addition of the 'direction' parameters {+1, 0, −1} on the transitions which tell the FST to advance to the next symbol on the input tape (+1), stay on the same symbol (0), or return to the previous symbol (-1).",43,44
13066,236486255,"The key difference between deterministic 1-way FSTs and deterministic 2-way FSTs are the addition of the 'direction' parameters {+1, 0, −1} on the transitions which tell the FST to advance to the next symbol on the input tape (+1), stay on the same symbol (0), or return to the previous symbol (-1).",56,57
13067,236486255,"The key difference between deterministic 1-way FSTs and deterministic 2-way FSTs are the addition of the 'direction' parameters {+1, 0, −1} on the transitions which tell the FST to advance to the next symbol on the input tape (+1), stay on the same symbol (0), or return to the previous symbol (-1).",66,67
13068,236486255,"After outputting the first copy (state q 1 ) and rewinding (state q 2 ), the machine changes to state q 3 when it scans the left boundary symbol and outputs ∼ to indicate that another copy will be created.",31,32
13069,236486255,"However, when the machine reaches a vowel, it outputs a vowel with a tone that is being read at the same time on the T-tape (in our exam-ple, (L, V) → V) and transitions to state q 2 (if the symbol on the T-tape is H) or q 3 (for L, as in our example).",53,54
13070,236486255,"In states q 2 and q 3 , consonants are simply output as in state q 1 , but for vowels, one of three conditions may occur: the read head on the Tonal tape may be H or L, in which case the automaton transitions (if not already there) to q 2 (for H) or q 3 (for L), and outputs the appropriate orthographic symbol.",74,75
13071,236486255,This is when the OCP determines the interpretation: all vowels get the tone of the last symbol on the Tone tier (which we remember as states q 2 and q 3 ).,17,18
13072,236486255,"The meaning of the configuration ( #» w, q, #» x , u) is that the input to T is #» w #» x and the machine is currently in state q with the n read heads on the first symbol of each x i (or has fallen off the right edge of the i-th input tape if x i = λ) and that #» u is currently written on the m output tapes.",47,48
13073,236486255,The ∼ symbol significantly indicates the morpheme boundary and facilitates further output linearization.,2,3
13074,52812226,"For this extension, an additional symbol is added to the input vocabulary.",6,7
13075,18461192,"So, apart from the linguistic information mentioned in section 2, every entry of the dictionary contains also a. a list of rules that permit the use of a particular entry (rules That have the entry as Their Terminal symbol).",41,42
13076,5571461,"Preliminaries A CFG is a 4-tuple G = N, T, P, S where N, T are finite and disjoint sets of nonterminal and terminal symbols, P a finite set of production rules of the form A → φ where A ∈ N and φ ∈ (N ∪ T ) * , and S ∈ N is the start symbol.",66,67
13077,52229163,"In particular, we opt for a full-vocabulary setup where no word encountered at training time is treated as an unknown symbol, in order to a) ensure a fair comparison across languages with different word frequency rates and b) avoid setting an arbitrary threshold on vocabulary size (Cotterell et al.,",23,24
13078,52229163,"A majority of language models rely on the fixed-vocabulary assumption: they use a special symbol <UNK> that represents all words not present in the fixed vocabulary V , which are termed out-of-vocabulary (OOV).",17,18
13079,5038627,"When all the hypotheses are complete (they end in an end-of-thesentence symbol or reach a predetermined length limit), it returns the hypothesis with the highest likelihood.",16,17
13080,202770165,"This however ignores the cost of expressing each symbol, which increases with the vocabulary size.",8,9
13081,202770165,"In general in NMT, prefixfreeness is attained by separating these tokens with a special symbol (whitespace).",15,16
13082,1507164,"Such variables include the value of the top-category for parsing (the start symbol); the default parser; whether or not the system should check if an object is created whether such an object already exists (this feature is used to recognize spurious ambiguities), etc.",15,16
13083,202540407,"In the field of NMT, BPE sub-words and endof-sentence symbol should be carefully handled as they do not appear in the conventional dependency tree.",14,15
13084,202540407,"In this work, we assign the BPE sub-words share the absolute structural position of the original word and set the the first larger integer than the max absolute structural position in dependency tree as the absolute structural position of end-ofsentence symbol.",45,46
13085,250390877,"A deterministic top-down tree transducer is top-down sequential iff it holds for every state q and every leaf symbol σ that the transducer has a transition rule q(σ()) → t, where t is some tree not containing any states.",22,23
13086,202776401,"According to the setting of the polyglot toolkit, there are seventeen categories of POS tags: noun (NOUN), verb (VERB), adjective (ADJ), 2071 adposition (ADP), adverb (ADV), auxiliary verb (AUX), coordinating conjunction (CONJ), determiner (DET), interjection (INTJ), numeral (NUM), particle (PART), pronoun (PRON), proper noun (PROPN), punctuation (PUNCT), subordinating conjunction (SCONJ), symbol (SYM) and other unknown types (X).",97,98
13087,250390677,The symbol V: represents long vowels only.,1,2
13088,250390601,"is the morpheme separation symbol in the given data, ""S"" means substitution, ""C"" means copy, and ""D"" means deletion.",4,5
13089,250390909,We replace all aligned substrings that are present in the lemma and all its forms by a placeholder symbol.,18,19
13090,236486076,"Model specification 2.1 Probabilistic Finite-state Automata A probabilistic finite-state automaton (PFA) for generating sequences consists of a finite set of states Q, an inventory of symbols Σ, an emission distribution with probability mass function p(x|q) which gives the probability of generating a symbol x ∈ Σ given state q ∈ Q, and a transition distribution with probability mass function p(q |q, x) which gives the probability of transitioning into new state q from state q after emission of symbol x. We parameterize a PFA using a family of rightstochastic matrices.",51,52
13091,236486076,"Model specification 2.1 Probabilistic Finite-state Automata A probabilistic finite-state automaton (PFA) for generating sequences consists of a finite set of states Q, an inventory of symbols Σ, an emission distribution with probability mass function p(x|q) which gives the probability of generating a symbol x ∈ Σ given state q ∈ Q, and a transition distribution with probability mass function p(q |q, x) which gives the probability of transitioning into new state q from state q after emission of symbol x. We parameterize a PFA using a family of rightstochastic matrices.",90,91
13092,236486076,"The emission matrix E, of shape |Q| × |Σ|, gives the probability of emitting a symbol x given a state.",17,18
13093,236486076,"Each row in the matrix represents a state, and each column represents an output symbol.",15,16
13094,236486076,"Given a distribution on states represented as a stochastic vector q, the probability mass function over symbols is: p(•|q) = q E. (1) Each symbol x ∈ Σ is associated with a rightstochastic transition matrix T x of shape |Q|×|Q|, so that the probability distribution on following states given that the symbol x was emitted from the distribution on states q is p(•|q, x) = q T x . (",29,30
13095,236486076,"Given a distribution on states represented as a stochastic vector q, the probability mass function over symbols is: p(•|q) = q E. (1) Each symbol x ∈ Σ is associated with a rightstochastic transition matrix T x of shape |Q|×|Q|, so that the probability distribution on following states given that the symbol x was emitted from the distribution on states q is p(•|q, x) = q T x . (",57,58
13096,236486076,"2) Generation of a particular sequence x ∈ Σ * works by starting in a distinguished initial state q 0 , generating a symbol x, transitioning into the next state q , and so on recursively until reaching a distinguished final state q f .",24,25
13097,236486076,"Applied to induce a PFA with states Q and symbol inventory Σ, our formulation yields a total of |Q| × (|Q| × |Σ| − 1) meaningful trainable parameters.",9,10
13098,236486076,"In order to account for this, we include in the symbol inventory Σ a special word boundary delimiter #, which occurs as the final symbol of each word, and which only occurs in that position.",11,12
13099,236486076,"In order to account for this, we include in the symbol inventory Σ a special word boundary delimiter #, which occurs as the final symbol of each word, and which only occurs in that position.",26,27
13100,236486076,"Furthermore, we constrain all matrices T to transition deterministically back into the initial state following the symbol #, effectively reusing the initial state q 0 as the final state q f .",17,18
13101,236486076,"If an automaton of this form is allowed to keep generating past the symbol #, it will generate successive concatenated independent and identically distributed samples from its distribution over words, with boundary symbols # delineating them.",13,14
13102,236486076,"We implement Strictly Local and Strictly Piecewise automata by hard-coding the transition matrices T. For these automata, we only do optimization over the emission matrices E. Strictly Local In a Strictly k-Local (k-SL) language, each symbol is conditioned only on immediately preceding k − 1 symbol(s) (Heinz, 2018; Rogers and Pullum, 2011) .",45,46
13103,236486076,"We implement a 2-SL automaton by associating each state q ∈ Q with a unique element x in the symbol inventory Σ. Upon emitting symbol x, the automaton deterministically transitions into the corresponding state, denoted q x .",21,22
13104,236486076,"We implement a 2-SL automaton by associating each state q ∈ Q with a unique element x in the symbol inventory Σ. Upon emitting symbol x, the automaton deterministically transitions into the corresponding state, denoted q x .",26,27
13105,236486076,"Strictly Piecewise A Strictly k-Piecewise k-SP) language, each symbol depends on the presence of any preceding k − 1 symbols at arbitrary distance (Heinz, 2007 (Heinz, , 2018;; Shibata and Heinz, 2019) .",14,15
13106,236486076,"For example, in a 2-SP language, in a string abc, c would be conditional on the presence of a and the presence of b, without regard to distance nor the relative order of a and b. The implementation of an SP automaton is slightly more complex than the SL automaton, as the number of states required in a naïve implementation is exponential in the symbol inventory size, resulting in intractably large matrices.",71,72
13107,236486076,"We associate each symbol x ∈ Σ with a sub-automaton A x which has two states q x 0 and q x 1 , with state q x 0 indicating that the symbol x has not been seen, and q x 1 indicating that it has been seen.",3,4
13108,236486076,"We associate each symbol x ∈ Σ with a sub-automaton A x which has two states q x 0 and q x 1 , with state q x 0 indicating that the symbol x has not been seen, and q x 1 indicating that it has been seen.",34,35
13109,236486076,"Then the probability of the t'th symbol in a sequence x t given a context of previous symbols x t−1 i=1 is the geometric mixture of the probability of x t under each sub-automaton, also called the co-emission probability p(x t |x t−1 i=1 ) ∝ |Σ| y=1 p Ay (x t |x t−1 i=1 ).",6,7
13110,236486076,"For calculating the probability of a sequence, we assume an initial state of having seen the boundary symbol #; that is, the sub-automaton A # starts in state q # 1 .",18,19
13111,236486076,"Languages All languages are defined over the symbol inventory {a, b, c} plus the boundary symbol #.",7,8
13112,236486076,"Languages All languages are defined over the symbol inventory {a, b, c} plus the boundary symbol #.",19,20
13113,12225963,"One member of the pair of symbols on a transition can be the designated null symbol, which we will write ~. When this appears, the corresponding tape is not examined, and it does not advance as the machine moves to the next state.",15,16
13114,12225963,"The effect of this is to insist that they not only accept the same pairs of tapes, but that they agree on the particular sequence of symbol pairs that must be rehearsed in the course of accepting each of thetn.",27,28
13115,12225963,"Kaplan has been able to put a more formal construction on this in the following way l,et the empty symbols appearing in the pairs labeling any transition in the transducers be replaced by some ordinary symbol not otherwise part of the alphabet.",37,38
13116,12225963,"Suppose, now, that this configuration of parallel transducers is put in series with two other standard transducers, one which carries the real empty symbol onto its surrogate, and everything else onto itself, and another transducer that carries the surrogate onto the real empty symbol, then the resulting configuration accepts just the desired set of languages, all of which are also acceptable by single transducers that can be algorithmicalLy derived form the originals.",26,27
13117,12225963,"Suppose, now, that this configuration of parallel transducers is put in series with two other standard transducers, one which carries the real empty symbol onto its surrogate, and everything else onto itself, and another transducer that carries the surrogate onto the real empty symbol, then the resulting configuration accepts just the desired set of languages, all of which are also acceptable by single transducers that can be algorithmicalLy derived form the originals.",48,49
13118,12225963,"When the automaton moves from one state to another, each of the four tapes will advance over the symbol corresponding to it on the transition that sanctions the move.",19,20
13119,12225963,"The extensions have to do with separating the process of reading or writing a symbol on a tape, from advancing the tape to the next position.",14,15
13120,12225963,"The quadruples that label the transitions in the transducers we shall be constructing will be elements each consisting of two parts, a symbol, and an instruction concerning the movement of the tape.",23,24
13121,12225963,"A unadorned symbol will be read in the traditional way, namely, as requiring the tape on which that symbol appears to move to the next position as soon as it has been read or written.",2,3
13122,12225963,"A unadorned symbol will be read in the traditional way, namely, as requiring the tape on which that symbol appears to move to the next position as soon as it has been read or written.",20,21
13123,12225963,"If the symbol is shown in brackets, on the other hand, the tape will not advance, and the quadruple specifying the next following transition will therefore clearly have to be one that specifies the same symbol for that tape, since the symbol will still be under the read-write head when that transition is taken.",2,3
13124,12225963,"If the symbol is shown in brackets, on the other hand, the tape will not advance, and the quadruple specifying the next following transition will therefore clearly have to be one that specifies the same symbol for that tape, since the symbol will still be under the read-write head when that transition is taken.",38,39
13125,12225963,"If the symbol is shown in brackets, on the other hand, the tape will not advance, and the quadruple specifying the next following transition will therefore clearly have to be one that specifies the same symbol for that tape, since the symbol will still be under the read-write head when that transition is taken.",45,46
13126,12225963,"With this convention, it is natural to dispense with the e symbol in favor of the notation ""[l"", that is, an unspecified symbol over which the corresponding tape does not advance.",12,13
13127,12225963,"With this convention, it is natural to dispense with the e symbol in favor of the notation ""[l"", that is, an unspecified symbol over which the corresponding tape does not advance.",28,29
13128,12225963,"A symbol can also be written in braces, in which case the corresponding tape will move if the symbol under the read-write head is the last one on the tape.",1,2
13129,12225963,"A symbol can also be written in braces, in which case the corresponding tape will move if the symbol under the read-write head is the last one on the tape.",19,20
13130,12225963,"In other words, the root tape does not move, and this possibility is allowed on the specific grounds that it is the last symbol on the tape.",25,26
13131,12225963,"First, the prosodic template contains the hitherto unused symbol ""G"".",9,10
13132,211893,The omitted object is represented by the symbol ø.,7,8
13133,211893,"Speaker Korean dialogue In table 1, the dialogue""s omitted object is represented by the symbol ø.",15,16
13134,52157882,The fact that we don't see a peak in activation at the right bracket symbol (which should be a cue for a following space) suggests that the former explanation is more plausible.,15,16
13135,52123353,SET MARKER: Insert a marker symbol into the target sentence at the position of the write head. •,6,7
13136,52123353,The initial state of the target sentence is a single marker symbol X 1 .,11,12
13137,52123353,Generative operations like SET MARKER or INSERT(t) insert a single symbol left of the current marker (highlighted).,11,12
13138,52123353,"The start symbol of G is [(S), (X 1 ), (P 1 )] T (1) which initializes the source sentence stream with a single nonterminal S, the target sentence with the initial marker X 1 and the position of the write head with 1 (P 1 ).",2,3
13139,52123353,1 ) as they represent the fact that target token t is aligned to the last terminal symbol in the first stream.,17,18
13140,52123353,"We formalize removing markers/nonterminals at the end by introducing a special nonterminal T which is eventually mapped to the end-of-sentence symbol EOS: [(S), (), ()] T → [(T ), (), ()] T (7) [(T ), (), ()] T → [(EOS), (), ()] T (8) ∀i ∈ N : [(T ), (X i ), ()] T → [(T ), ( ), ()] T (9) ∀i ∈ N : [(T ), (), (P i )] T → [(T ), (), ( )] T (10) Tab.",26,27
13141,5236680,"In this example, the omitted subject is represented by the symbol ø. (",11,12
13142,18385371,"This means we require that the start symbol of the grammar, and the set lexical categories must be product-free.",7,8
13143,5041395,"SDN repeats a twostage process of generating a state trajectory segment, until a trajectory termination symbol is generated: first it uses an initial segment hidden state to start a new segment, or a trajectory termination symbol to terminate the trajectory, given all previous states; if the trajectory is not terminated, then keep generating the next state in this trajectory segment given previous states until a segment termination symbol is generated.",16,17
13144,5041395,"SDN repeats a twostage process of generating a state trajectory segment, until a trajectory termination symbol is generated: first it uses an initial segment hidden state to start a new segment, or a trajectory termination symbol to terminate the trajectory, given all previous states; if the trajectory is not terminated, then keep generating the next state in this trajectory segment given previous states until a segment termination symbol is generated.",38,39
13145,5041395,"SDN repeats a twostage process of generating a state trajectory segment, until a trajectory termination symbol is generated: first it uses an initial segment hidden state to start a new segment, or a trajectory termination symbol to terminate the trajectory, given all previous states; if the trajectory is not terminated, then keep generating the next state in this trajectory segment given previous states until a segment termination symbol is generated.",73,74
13146,5041395,"During the training, at each time step, RNN1 predicts the next state with the current state as input, until it reaches the option termination symbol #.",27,28
13147,5041395,"The agent starts from the initial state s 0 , keeps sampling the output from the distribution related to the top-level RNN (RNN1) until a termination symbol # is generated, which indicates the agent reaches a subgoal.",30,31
13148,5041395,"As illustrated in Section 3.3, SDN starts a new RNN1 instance and issues a subgoal-completion query when the probability of outputting the termination symbol # is above a certain threshold p (as in Algorithm 2).",26,27
13149,209411989,by the time it reads the input symbol (or within an a priori bounded distance after) associated with 's decision problem.,7,8
13150,209411989,"Finally, ⇢'s output for the symbol associated with the decision problem must then depend on the information about y that has injected into x ® .",7,8
13151,202621527,"To date, such work either requires thousands of training observations (Gildea and Jurafsky, 1996) or has used abstracted and greatly simplified symbol inventories and training data (Chandlee et al.,",25,26
13152,9225286,after the symbol.,2,3
13153,9225286,"Because transcriptions are linear strings of symbols, one way to calculate agreement between 2 transcribers is: Matches Agreement = I00( ) Matches + Insertions + Substitutions where: Matches = number of symbols in the string where the transcribers agree concerning location and the symbol itself 1, Insertions = number of symbols marked by one transcriber only (an omission by either transcriber is equivalent to an insertion by the other), and Substitutions = number of locations where each transcriber used a different symbol.",47,48
13154,9225286,"Because transcriptions are linear strings of symbols, one way to calculate agreement between 2 transcribers is: Matches Agreement = I00( ) Matches + Insertions + Substitutions where: Matches = number of symbols in the string where the transcribers agree concerning location and the symbol itself 1, Insertions = number of symbols marked by one transcriber only (an omission by either transcriber is equivalent to an insertion by the other), and Substitutions = number of locations where each transcriber used a different symbol.",89,90
13155,3130414,Each list consists of a type symbol followed by zero or more keyword-value pairs.,6,7
13156,3130414,"The first symbol gives the part of speech; a word with several parts of speech will have several dictionary entries, one for each part of speech.",2,3
13157,52179278,"2018) , this can be done by iteratively defining a side-and depth-specific containment likelihood h (i) s,d for left-or right-side siblings s ∈ {1, 2} at depth d ∈ {1..D} at each iteration i ∈ {1..I}, as a vector with one row for each nonterminal or terminal symbol (or null symbol ⊥) in G, containing the probability of each symbol generating a complete yield within depth d as an s-side sibling: h (0) s,d = 0 (9a) h (i) 1,d =        G (1 ⊗ δ ⊥ + h (i−1) 1,d ⊗ h (i−1) 2,d ) if d ≤ D + 1 0 if d > D + 1 (9b) h (i) 2,d =              δ T if d = 0 G (1 ⊗ δ ⊥ + h (i−1) 1,d+1 ⊗ h (i−1) 2,d ) if 0 < d ≤ D 0 if d > D (9c) where 'T' is a top-level category label at depth zero.",72,73
13158,52179278,"2018) , this can be done by iteratively defining a side-and depth-specific containment likelihood h (i) s,d for left-or right-side siblings s ∈ {1, 2} at depth d ∈ {1..D} at each iteration i ∈ {1..I}, as a vector with one row for each nonterminal or terminal symbol (or null symbol ⊥) in G, containing the probability of each symbol generating a complete yield within depth d as an s-side sibling: h (0) s,d = 0 (9a) h (i) 1,d =        G (1 ⊗ δ ⊥ + h (i−1) 1,d ⊗ h (i−1) 2,d ) if d ≤ D + 1 0 if d > D + 1 (9b) h (i) 2,d =              δ T if d = 0 G (1 ⊗ δ ⊥ + h (i−1) 1,d+1 ⊗ h (i−1) 2,d ) if 0 < d ≤ D 0 if d > D (9c) where 'T' is a top-level category label at depth zero.",76,77
13159,52179278,"2018) , this can be done by iteratively defining a side-and depth-specific containment likelihood h (i) s,d for left-or right-side siblings s ∈ {1, 2} at depth d ∈ {1..D} at each iteration i ∈ {1..I}, as a vector with one row for each nonterminal or terminal symbol (or null symbol ⊥) in G, containing the probability of each symbol generating a complete yield within depth d as an s-side sibling: h (0) s,d = 0 (9a) h (i) 1,d =        G (1 ⊗ δ ⊥ + h (i−1) 1,d ⊗ h (i−1) 2,d ) if d ≤ D + 1 0 if d > D + 1 (9b) h (i) 2,d =              δ T if d = 0 G (1 ⊗ δ ⊥ + h (i−1) 1,d+1 ⊗ h (i−1) 2,d ) if 0 < d ≤ D 0 if d > D (9c) where 'T' is a top-level category label at depth zero.",87,88
13160,202781916,"Within 𝑥, we sampled a new start index and length ∈ [1, 10] to select the target sequence 𝑦; its characters' positions were replaced with the special symbol '?',",33,34
13161,53082616,"By contrast, our system has no notion of lexeme and we simply work from the symbol strings which are collections of inflected forms of a lexeme given in the test data which may in principle be completely disjoint from training data lexemes.",16,17
13162,936966,"The symbol Q,te refers to miscellaneous questions about the current state of the parser, such as the number of nodes in the sentence and the number of children of a particular node.",1,2
13163,2078255,"For Indonesian, we exploit the fact that the hyphen symbol '-' typically separates the first and second occurrence of a reduplicated morpheme, as in the examples of Section 2.",10,11
13164,9862311,"For FORM and LEMMA, all hapaxes are replaced with a single arbitrary symbol.",13,14
13165,9862311,"7 https://github.com/wlin12/wang2vec/ with options {-type 3 -hs 0 -min-count 2 -window 7 -sample 0.1 -negative 7 -iter 20}; Though in fact [-min-count 2] had no effect, as we had all hapaxes replaced by an obscure symbol.",45,46
13166,250390576,Introduce a wildcard (character subsequence) 1 and replace the matching part by the wildcard symbol.,16,17
13167,250390457,The inputs to each model are the individual characters of the lemma followed by their morphosyntactic tags separated by # symbol.,20,21
13168,519236,"In addition, we preprend a symbol λ ∈ L to the input string (e.g., λ = Es, also represented by an embedding), so the RNN can handle multiple languages simultaneously and generalize over them.",6,7
13169,5316292,"Probabilistic interpretation of PCFGs We review the standard probabilistic interpretation of PCFGs 1 A PCFG is a four-tuple < W,N, N1,R > , where W is a Set of terminal symbols {wl,..., w~}, N is a set of non-terminal symbols {N1,...,N~}, N1 is the starting symbol and R is a set of rules of the form N ~ ~ (J, where (J is a string of terminals and non-terminals.",63,64
13170,5316292,The differences between these results and the earlier ones are: • The hierarchy uses bot rather than s as its start symbol.,22,23
13171,2670687,The symbol '#' stands for a word boundary.) (,1,2
13172,15086701,"A distinguishing symbol, $EXP, indicates that only the occurrence of something expected by preceding words (i.e. for which an impulse was set up) will allow the transition.",2,3
13173,14989027,"The symbol ""-"" indicates that the phoneme has not been observed in the training corpus.",1,2
13174,14989027,"Moreover, the symbol ""Q SR T# UR VR W8 "" indicates a probability of less than 0.001 (which means a occurrence frequency of less than 0.1%).",3,4
13175,2946526,"The main component of the model, a token, was defined as any character string: a word, number, symbol, punctuation or any combination.",22,23
13176,5487010,"3 can be derived using this grammar, starting with the start symbol S, and is assigned a weight (= probability) of 0.24.",12,13
13177,5487010,"Finally, this term is evaluated in the underlying algebra; in this case, a simple string algebra, which interprets the symbol * as string concatenation.",23,24
13178,8388406,"Tbe image generator consists of a message source, which generates a symbol string containing the informarion to be cornrnunicated, and an imager, which formats or encodes the message into an ideal bitmap.",12,13
13179,184483020,"text1 is composed of Fsubject, F-category and the body of question separated by the special symbol (∼), while text2 is the text of corresponding reply.",18,19
13180,184483020,"The input of the model is the concatenation of the subject and the body of each question with the symbol [SEP], which is represented as follows: [CLS] + text1 + [SEP ] + text2 + [SEP ] text1 and text2 are the subject and body of a question separately.",19,20
13181,6229041,Another important feature is the importing of libraries of regular expressions much like symbol libraries in drawing programs and the like.,13,14
13182,18932500,"Each transition has an origin and a terminus and is labeled with a symbol of the alphabet; i.e. a transition is a 3-tuple (o, a, t) where o, t ∈ Q and a ∈ Σ. Empirically, it has been observed that most phonological phenomena are regular (Johnson, 1972; Kaplan and Kay, 1981; Kaplan and Kay, 1994; Ellison, 1994; Eisner, 1997; Karttunen, 1998) .",13,14
13183,18932500,"In 29 of the canonical acceptors, each state can be uniquely identified by its incoming symbol set, its outgoing symbol set, and whether it is final or non-final.",16,17
13184,18932500,"In 29 of the canonical acceptors, each state can be uniquely identified by its incoming symbol set, its outgoing symbol set, and whether it is final or non-final.",21,22
13185,18932500,"With each word, each symbol a is considered in order.",5,6
13186,18932500,A new arc is therefore created on every symbol in the first word.,8,9
13187,18932500,"For example, suppose states p and q in the prefix tree are both final or both nonfinal, and they share the same incoming symbol set and outgoing symbol set.",25,26
13188,18932500,"For example, suppose states p and q in the prefix tree are both final or both nonfinal, and they share the same incoming symbol set and outgoing symbol set.",29,30
13189,18932500,This is because the neighborhood learner cannot distinguish between sequences of the same symbol with length greater than two.,14,15
13190,202780722,"We write: p(x | w) = n t=1 p(x t | x <t , w) (1) where t is a time step, x 0 is a distinguished beginning-of-sentence symbol, w are the parameters, and every sequence x ends with a distinguished end-of-sentence symbol x n .",39,40
13191,202780722,"We write: p(x | w) = n t=1 p(x t | x <t , w) (1) where t is a time step, x 0 is a distinguished beginning-of-sentence symbol, w are the parameters, and every sequence x ends with a distinguished end-of-sentence symbol x n .",59,60
13192,202782965,"Specifically, it requires ""Any user, whether they're the OP or not, should reply to a comment that changed their view with a delta symbol and an explanation of the change.""",28,29
13193,2785958,"The 'x' symbol is used to denote deleted vowels, which violate MAX, which assigns a violation if 'x' appears in the pronunciation.",4,5
13194,6293901,The # character is a boundary symbol marking the end of the root.,6,7
13195,198184456,"Following standard practice in formal semantics, we use the term atom to refer to a predicate symbol and a list of terms, such as [grandfatherOf, X, Y ], where the predicate grandfatherOf denotes the relation between the two variables, X and Y .",17,18
13196,16796502,The Xerox tool makes use of a so-called 'other' symbol which stands for characters not mentioned in the rule.,13,14
13197,16796502,FOMA implements the 'other' symbol and is expected to improve the processing of larger alphabets.,6,7
13198,202785816,"Finally, tokens whose occurrence is lower than a certain threshold are replaced with the special symbol <unk>.",16,17
13199,220046222,"2019) , replacing the mentions with a certain symbol user, cleaning up samples in which the regular words include 'sarcasm' related words (e.g. sarcasm, sarcastic, irony, ironic) and co-occur words (e.g. jokes, humor, exgag), and removing the stop words and URLs.",9,10
13200,17250803,"As regards adverbials, the structure given is only one of several possible: NOMINAL = nil; nominal( ART, ADJEKTIVAL, SUBKERN PREPP, CS ) ADVERBIAL : nil; adverbial( CONN, DEGREEF, SITUATF, ADVKERN, PREPP, CS ) The CS is a symbol representing subordinate sentences, which have the form: CS = nil; cs( S, SYNT ) where S is the field structure, and SYNT the corresponding syntactical structure of the subordinate sentence represented by the token of the symbol type CS.",51,52
13201,17250803,"As regards adverbials, the structure given is only one of several possible: NOMINAL = nil; nominal( ART, ADJEKTIVAL, SUBKERN PREPP, CS ) ADVERBIAL : nil; adverbial( CONN, DEGREEF, SITUATF, ADVKERN, PREPP, CS ) The CS is a symbol representing subordinate sentences, which have the form: CS = nil; cs( S, SYNT ) where S is the field structure, and SYNT the corresponding syntactical structure of the subordinate sentence represented by the token of the symbol type CS.",94,95
13202,17250803,"subordinate clauses were introduced to noun phrases and adverbial phrases, this was a very simple operation in the grammar (it required the addition of a single symbol) but it had severe consequenses for execution time: roughly a 25% increase in analysis time for the sentence 'den meget gode dreng vil gerne f~ givet moderen den gode gave' ('The very good boy will behappy-to manage-to give the-mother the-",28,29
13203,235694169,"Another representation of policy is extending target sequence y to length |x| + |y| with blank symbol φ as ŷ ∈ (v ∪ {φ}) |x|+|y| , where v is the vocabulary of the target language.",16,17
13204,243865395,"If the READ action is replaced with a blank symbol ∅, the policy can also be represented by the expanded target sequence ŷ ∈ (V ∪ {∅}) |x|+|y| , where V is the vocabulary of the target language.",9,10
13205,243865395,"Recurrent Neural Network Transducer RNN-T (Graves, 2012) draws condition probability Pr(y|x) by marginalizing all possible alignment paths as : Pr (y|x) = ŷ∈H(x,y) Pr(ŷ|x) = ŷ∈H(x,y) |x|+|y| k=1 Pr(ŷ k |i k , j k ) (1) where i k and j k denote the source and target position of the k-th element in ŷ, respectively, and ŷ = (ŷ 1 , ŷ2 , ..., ŷ|x|+|y| ) ∈ H(x, y) ⊂ {V ∪ ∅} |x|+|y| corresponds to a possible expansion path which yields y after removing the blank symbol ∅. As shown in Figure 1 (b), to calculate P (ŷ k |h i k , y <j k ), RNN-T divides decoder into predictor and joiner, where the predictor, denoted f pred , produces target history representation (Eq. (",115,116
13206,243865395,"The maximum decision step is I = |x| d , and the output logits at decision step i, target position j should be s(i, j) = g h <i * d , h pred j (15) s(i,j) is a vector of |V |+1 dimension corresponding to V and blank symbol ∅. s(k, i, j) denotes the k-th dimension of s(i, j).",58,59
13207,237490428,"2019 ) such as #WPWW (white pride world wide), #National-Socialism (Nazism), and (((they))) (an anti-Semitic symbol) are contained in 15% of instances, and our models fail to grasp their semantics.",34,35
13208,218580945,"For every non-root word and hence syntactic dependency in D (since every word is a dependent of some other word or an added ROOT symbol), we calculate the k-dimensional head-dependent vector between the head and the dependent after projection by B. Specifically, for all head-dependent pairs (w head , w dep ), we compute v diff = B(h head − h dep ).",27,28
13209,14863802,"The first of these is very simple: Word → Morph + Morph → Char + (1) The underline notation indicates an adapted nonterminal, and + abbreviates a set of recursive rules, e.g., Word → Morph + is short for Word → Morphs Morphs → Morph Morphs Morphs → Morph Grammar 1 (MorphSeq) is just a unigram model over morphs: the Morph symbol is adapted, so the probability of each Morph will be roughly proportional to its (inferred) frequency in the corpus.",70,71
13210,211029226,"This in turn means that for a human or a machine to learn a language, they must solve what Harnad (1990) calls the symbol grounding problem.",26,27
13211,243865354,"Using the algorithm in Appendix C, we randomly add edges to the DFA to satisfy the following constraints: (1) every state is connected to approximately 4 other states, and (2) each symbol appears on approximately 4 edges.",38,39
13212,243865354,"We generate surprising test examples by again sampling a random walk, then appending a symbol that cannot be produced along any out-edge from that random walk's final state.",15,16
13213,243865354,"In regular languages, the local context model thus hypothesizes that lexical information governs out-of-distribution prediction, predicting that LM outputs are determined by the set of states attached to an edge labeled with the surprising symbol.",40,41
13214,243865354,"Conversely, the global context model hypothesizes that structural information governs out-of-distribution prediction: LM outputs are determined by the set of states reachable from the last state visited before the surprising symbol.",36,37
13215,1873417,Points marked with the same symbol should be compared to each other.,5,6
13216,227904591,We feed EOS symbol as input to decoder from both forward and backward dircetion at time t 0 .,3,4
13217,5418779,"We use uni-grams, bigrams, tri-grams and tetra-grams, but we also include skip-grams such as the character sequence ""a b"" where the underscore is a wild-card placeholder symbol.",41,42
13218,227904565,An UNK test set is created by randomly replacing one word in one sentence with an UNK symbol if the sentence has no unknown words present.,17,18
13219,227904565,All out-ofvocabulary words are replaced by the UNK symbol.,10,11
13220,243865611,"In- stead of a bracketed representation, predicates, logic operators and discourse relations, along with their arguments, are represented as a sequence of tokens separated by a special symbol ""|||"".",31,32
13221,243865611,"Clauses in DRS are represented as sequences without changing the order, where a special symbol ""|||"" is used to start a new clause and variables in clauses are represented as relative indices.",15,16
13222,222141862,"If a comment changes the original poster (OP)'s view, the OP acknowledges it by replying to the comment with a ∆ symbol.",25,26
13223,222141862,Attacked vs. Unattacked: Some comments use direct quotes with the > symbol to address specific sentences of the post (Figure 1 ).,12,13
13224,222141862,"A matched text span should contain at least one word and four characters, and cover at least 80% of the quote to exclude cases where the > symbol is used to quote external content.",29,30
13225,222378364,"Preliminaries and definitions Formal languages A formal language L is a set of strings L ⊆ Σ * ω over a fixed vocabulary, Σ (with the end denoted by special symbol ω).",32,33
13226,222378364,The new cell candidate (c t ) is used to attempt to write the input symbol i to all m stack slots.,16,17
13227,222378364,"So that it's easy to tell what symbol is at the top of the stack, we only want the top slot of the stack passing through the output gate.",8,9
13228,222378364,"In our stack constructions, we assumed each stack slot is a kdimensional one-hot vector e i to encode symbol i .",21,22
13229,222378364,"This does not work because some p (i) are   strict supersets of other p (j) , so the true symbol cannot decoded through any V h t .",24,25
13230,222378364,"To solve this, we use a constant factor more space, to ensure each symbol is decodable by V .",15,16
13231,222378364,"Using ψ i for the row V i , we have the following expression for the dot product in determining the probability distribution: V j ψ i = log k =1 p (i) p (j) + log k =1 (1 − p (i) )(1 − p (j) ) = log k i = j ≤ log k − 1 i = j Thus, we can always detect which symbol i is encoded by setting b i = log k − 0.5.",83,84
13232,222378364,"Strings over the vocabulary are w 1:T ∈ Σ * ω, where ω ∈ Σ is a special symbol representing the end of the sequence.",21,22
13233,222378364,"Empty stack state The state [] can transition either to the accept state or to another stack state: δ([], ω) = [ω] (B.1) δ([], i ) = [ i ] (B.2) while any other symbol transitions to the reject state.",51,52
13234,222378364,"ω] No transitions from the accept state need be defined, since only δ([], ω) = [ω], and ω must be the last symbol of any string in the universe, and can only occur once.",32,33
13235,222378364,"Finally, we define b := −β • 1 R 2km (D.8) Proof of Correctness To prove correctness of the construction, the first step is to show that the transition dynamics of the Simple RNN correctly simulates the dynamics of the stack when consuming one symbol.",50,51
13236,222378364,"Assume that the hidden state h t encodes a stack q: Q(h t ) = q (D.9) Let w t+1 be the new input symbol, and assume that w 1:t+1 is a valid prefix of a word in Dyck-(k,m).",27,28
13237,222378364,"s m are the zero vector 0 because only the first m slots encode a symbol i , and ψ(0) = ∅. On the ψ function These one-hot encodings (e i ) of symbols ( i ) allow for clear exposition, but are not fundamental to the construction; in ( § G), we replace these k-dimensional one-hot encodings given by ψ with O(log k)-dimensional encodings.",15,16
13238,222378364,"k, n =1 ψ −1 ( i ) = 1 (E.4) that is, the sum of all dimensions of the representation is equal to 1, and ∀ i∈[k] ψ −1 ( i ) ∈ {0, 1} n (E.5) that is, the representation takes on values only in 0 and 1, and ψ(0) = ∅, (E.6) that is, the empty stack slot is encoded by the zero vector, and ∀ i,j∈[k] , i = j =⇒ ψ −1 ( i ) = ψ −1 ( j ) (E.7) ∀ i∈[k] , ψ −1 ( i ) = 0 (E.8) that is, encodings of symbols i are unique, and none are equal to to the encoding of the empty symbol ∅. The encoding we've so far provided, ψ −1 (e i ) = i , obeys these properties: the sum of a single onehot vector is 1, no two such vectors are the same, one-hot vectors only take on values in {0, 1}, and we let ψ(0) = ∅. E.2 Description of stack state dynamics Before we describe the memory dynamics, we introduce a useful property of the stack slots.",148,149
13239,222378364,"Now, the symbol w t+1 can be one of 2k + 1 symbols: any of the k open brackets i , the k close brackets i , or ω.",3,4
13240,222378364,"This ensures that the softmax matrix correctly distinguishes between the symbol, i , that is encoded in the top of the stack, from any other symbol.",10,11
13241,222378364,"This ensures that the softmax matrix correctly distinguishes between the symbol, i , that is encoded in the top of the stack, from any other symbol.",27,28
13242,222378364,The logits for each symbol are as follows.,4,5
13243,222378364,"In this case, if at timestep t, then symbol w t = ω, since the only transition to state [ω] is δ([], ω) = [ω].",10,11
13244,222378364,"Thus, the probability assigned to the lowest-probability allowed symbol converges to 1 k+1 as ζ grows large, while the probability assigned to the highestprobability disallowed symbol converges to 0.",11,12
13245,222378364,"Thus, the probability assigned to the lowest-probability allowed symbol converges to 1 k+1 as ζ grows large, while the probability assigned to the highestprobability disallowed symbol converges to 0.",29,30
13246,222378364,"Under this, the smallest probability assigned to any allowed symbol is lowerbounded by e 0.5ζγ (k+1)e 0.5ζγ +ke −0.5ζγ > 1 (k+1)+0.1k > , and the largest probability assigned to any disallowed symbol is upper-bounded by e −0.5ζγ ke +0.5ζγ ≤ 0.1 k = 1 10k < .",10,11
13247,222378364,"Under this, the smallest probability assigned to any allowed symbol is lowerbounded by e 0.5ζγ (k+1)e 0.5ζγ +ke −0.5ζγ > 1 (k+1)+0.1k > , and the largest probability assigned to any disallowed symbol is upper-bounded by e −0.5ζγ ke +0.5ζγ ≤ 0.1 k = 1 10k < .",36,37
13248,222378364,"However, we do know that there must be some t such that the first time q t = r is for t = t , that is, the first timestep in which a disallowed symbol is seen and D m,k transitions to the reject state r (after which it self-loops in r by definition.)",36,37
13249,222378364,"Then the encoding of symbol i is: ψ −1 ( i ) * = ζ[p (i) ; (1 − p (i) ); 1] ∈ {0, 1} 3 log k −1 (G.1) Where the semicolon (; ) denotes concatenation.",4,5
13250,222378364,"Intuitively, this dot product simply counts up the number of bits that agree between the row j and the symbol i encoded; if they're the same, all bits agree; if not, at least 1 must disagree.",20,21
13251,221995757,"To understand this difference, we run a series of experiments to measure the impact of changing (a) the threshold for replacing rare words with a special symbol; (b) the source of data for initialisation; (c) the amount of training data for the language model; and (d) the hyperparameters for both the baseline and our proposed approach.",29,30
13252,221995757,"Std is the standard version used in language modeling, with words of frequency less than five converted to UNK, all words lowercase, numbers replaced with a special symbol, and punctuation removed.",30,31
13253,221995757,"In all cases, output embeddings are not frozen, so we leave out the symbol.",15,16
13254,221995757,We use only one symbol for pretraining/random because both embeddings are the same in most cases.,4,5
13255,221995757,"To probe the impact of rare words, we explore replacing them with UNK (using the same UNK symbol as used in embedding pretraining).",19,20
13256,202614150,"These symbols may be letters, linguistic tags or archiphonemes⁵ Analysing or generating a form involves traversing the graph from left to right, while reading a symbol and outputting its corresponding symbol.",27,28
13257,202614150,"These symbols may be letters, linguistic tags or archiphonemes⁵ Analysing or generating a form involves traversing the graph from left to right, while reading a symbol and outputting its corresponding symbol.",32,33
13258,202614150,⁵An archiphoneme is a symbol represents an underspecified phoneme which is determined by context; that is a phoneme which can have more than one surface realisation depending on context.,4,5
13259,85548630,We then use an attentional LSTM decoder for generating output analyses one symbol at a time.,12,13
13260,248780576,"This means that each input symbol is interpreted as a unitary transformation, or equivalently as a unitary matrix.",5,6
13261,248780576,So we cannot make Q(x) a simple lookup table from symbol to orthogonal matrix without additional restrictions.,12,13
13262,248780576,"Each input symbol x i indexes an embedding layer, yielding a skew-symmetric matrix S(x i ).",2,3
13263,248780576,"2011 ) describe what they call a ""tensor recurrent neural network"" in which the transition matrix is determined by each input symbol.",23,24
13264,236459825,"In the following, we omit the symbol x and θ in equations for presentation simplicity.",7,8
13265,222310469,"15 Therefore, for each model, the context vector is given by the start sentence symbol.",16,17
13266,212409690,and each symbol in the vocabulary is mapped to a continuous vector in R d .,2,3
13267,212409690,"Discrete representation learning gives us a way to mitigate this issue by representing each symbol v in the vocabulary as a discrete vector z v = [z (1) v , . . .",14,15
13268,222133328,"Grammar nature Lastly, to get a more qualitative perspective in the nature of the induced grammar, we consider a few statistics expressing the number of non-terminals and pre-terminals in the grammar, as well as the number of recursive production rules, defined as a production rule where the symbol from the lefthand side also appears on the right-hand side.",55,56
13269,222133328,"The symbol distribution in these languages are thus identical to the symbol distribution in the languages they are created from, but the symbol order is entirely random.",1,2
13270,222133328,"The symbol distribution in these languages are thus identical to the symbol distribution in the languages they are created from, but the symbol order is entirely random.",11,12
13271,222133328,"The symbol distribution in these languages are thus identical to the symbol distribution in the languages they are created from, but the symbol order is entirely random.",23,24
13272,222133328,"A first observation is that in all grammars each symbol is unambiguously associated with only one pre-terminal symbol, indicating that there is no ambiguity with respect to the word class it belongs to.",9,10
13273,222133328,"A first observation is that in all grammars each symbol is unambiguously associated with only one pre-terminal symbol, indicating that there is no ambiguity with respect to the word class it belongs to.",19,20
13274,222133328,"The number of terminals per pre-terminal suggests that our grammar induction algorithms also do not find many word classes: with some notable exceptions, every pre-terminal symbols expand only to a single terminal symbol.",38,39
13275,222133328,"Examples of more sophisticated game scenarios are bidirectional conversations where multi-symbol messages are challenging to analyse (Kottur et al.,",12,13
13276,222133328,"B.1 Example grammars In the following examples, TOP denotes the start symbol, NP the pre-terminal group, and the numbers the terminals that represent the symbols in the generated messages.",12,13
13277,221891676,"Architecture and Training Our methodology consists of training LSTM language models on k different first languages (L1s) which include natural languages, artificial languages, and non-linguistic symbol systems, and testing the performance of these models on a common second (L2) language.",31,32
13278,5408609,"Certain tokens were escaped from the data, such as the 'mentioned' character (@ symbol), subject tags 'hashtags' which are preceded by a # symbol, hyperlinks and the sequence rt which stands for 're-tweet'.",17,18
13279,5408609,"Certain tokens were escaped from the data, such as the 'mentioned' character (@ symbol), subject tags 'hashtags' which are preceded by a # symbol, hyperlinks and the sequence rt which stands for 're-tweet'.",31,32
13280,5408609,"In order to build a character language model we replaced spaces with the underscore symbol ' ', and then placed a space character between each character.",14,15
13281,222125311,x i represents the lowest non-terminal symbol shared between w i and w i+1 at level abs(n i ). •,8,9
13282,222125311,"Then, we extend the tree encoding function Φ to Φ : T |w| → L |w| where l i ∈ L is enriched with a fourth component p i such that l i = (n i , x i , u i , p i ), where p i is a discrete symbol such that the sequence of p i 's encodes the permutation τ (typically each p i will be an encoding of τ (i), i.e. the position of w i in the continuous arrangement, although this need not be true in all encodings, as will be seen below).",55,56
13283,222125311,"and Vilares, 2018)), and could potentially happen with any label component, e.g. predicting the non-terminal symbol.",22,23
13284,222125311,"However, it is very unlikely that a non-terminal symbol has not been observed in the training set.",11,12
13285,243865234,w * from w * fr.city to w * w is the wildcard symbol for words and it matches any word; symbol * is the Kleene star operation which means the preceding symbol or subexpression can appear for zero or more times.,13,14
13286,243865234,w * from w * fr.city to w * w is the wildcard symbol for words and it matches any word; symbol * is the Kleene star operation which means the preceding symbol or subexpression can appear for zero or more times.,22,23
13287,243865234,w * from w * fr.city to w * w is the wildcard symbol for words and it matches any word; symbol * is the Kleene star operation which means the preceding symbol or subexpression can appear for zero or more times.,33,34
13288,44064214,"An emoji is a small picture or symbol of a standardized set to represent a feeling or another concept (Dictionary.com, 2018) , contributing to the sentiment of its sender (Barbieri et al.,",7,8
13289,7975795,"well-side meeting"", which translates most naturally into English as ""idle gossip""; high productivity and frequency In order to quantify the high productivity and frequency of NN compounds, we carried out a 1 With all Japanese NN compound examples, we segment the compound into its component nouns through the use of the ""D "" symbol.",63,64
13290,10679025,"In addition, the same symbol ""/"" is used for shorthand notations while making notes, for example, the chief complaint texts heavily contained ""o/a"" meaning ""overall pale"".",5,6
13291,10679025,"We also observed that the use of such notation depended on the nurse's own preference; while there were many chief complaints with ""o/a"" there were also many occurrences of ""o a"" without the symbol ""/"" in between the characters.",41,42
13292,209431708,"First, the DeletePrecedingVowel rule is activated by the observation of a vowel ""u"" followed by the morpheme boundary marker ""ˆ"" and the ""~"", which is an arbitrary symbol we encode in the lexicon to indicate that the TAM inflection om tends to override any final vowel in the preceding morpheme.",35,36
13293,209431708,"But we aren't done yet: the cleanup step occurs with the CleanMorphBoundaries rule, which recognizes the ""ˆ"" symbol in any context, and deletes it.",22,23
13294,243865154,"2018) , is that a dependency tree may only have one edge em-anating from the designated root symbol.",20,21
13295,227230897,We model this subtask as mapping to a special symbol (#) used to identify which input words to copy to the output in a postprocessing step.,9,10
13296,227230897,"Although for a small set of commonly used emoticons, we use a dictionary and preprocess them into the (#) symbol.",22,23
13297,227230897,"In a different experiment, we inserted a random sample of 21K words (or about 10% of training data, to avoid biasing the model too much) into random locations throughout the existing training data and paired those additions with the proper # symbol on the target side.",47,48
13298,1850410,"In their treatment they divide tree representations into two types: those for ranked trees (that is, where each symbol has a fixed number of children, with this number constituting the rank of the symbol), and those for unranked trees.",21,22
13299,1850410,"In their treatment they divide tree representations into two types: those for ranked trees (that is, where each symbol has a fixed number of children, with this number constituting the rank of the symbol), and those for unranked trees.",37,38
13300,1850410,"A regular tree grammar is a 4-tuple G = (Σ, N, P, S) such that • Σ is a finite set of terminal symbols; • N is a finite set of nonterminal symbols; • P is a finite set of productions of the form X → a(R), where X ∈ N , a ∈ Σ, and R is a regular expression over N ∪Σ; and • S is the start symbol, S ∈ N .",83,84
13301,1850410,"Derivation ⇒ (with transitive closure * ⇒) is defined in the usual way, with nonterminals symbols rewritten by means of production rules, starting from the start symbol S. is the conventional null terminal symbol.",30,31
13302,1850410,"Derivation ⇒ (with transitive closure * ⇒) is defined in the usual way, with nonterminals symbols rewritten by means of production rules, starting from the start symbol S. is the conventional null terminal symbol.",37,38
13303,1850410,"Each permutation of children at a node would require a separate production, so as shorthand the & symbol is used: a&b represents ab and ba (a, b ∈ Σ).",18,19
13304,1850410,"First, to allow regular expressions over paths in tree patterns, we make a further notational extension, either adding the symbol c next to the non-immediate dominance link in the tree pattern, to indicate that the condition holds over the nonimmediate dominance link, or the symbol * if it does not.",22,23
13305,1850410,"First, to allow regular expressions over paths in tree patterns, we make a further notational extension, either adding the symbol c next to the non-immediate dominance link in the tree pattern, to indicate that the condition holds over the nonimmediate dominance link, or the symbol * if it does not.",51,52
13306,49666383,"2015) and Kiperwasser and Goldberg (2016) , we also apply word dropout to learn an embedding for unknown words: we replace each word token w appearing #(w) times in the training set with a special ""unk"" symbol with probability p unk (w) = 0.25 0.25+#(w) .",45,46
13307,215416146,"The LATEX subset also has fine-grained extraction and labeling of mathematical formulas, which can be used to understand proof construction, or to assist in symbol co-reference resolution.",28,29
13308,227905313,"The "" \ "" symbol means ""and"".",4,5
13309,5674957,where the OCP algorithm misclassifies one symbol less (see Figure 4 ).,6,7
13310,8365362,Henceforth we will use the term sign to refer to a hieroglyph or a hieratic symbol.,15,16
13311,13619705,Any symbol in alphabet 0 The empty string (epsilon) Aˆk k-ary concatenation % Escape symbol [ and ] Grouping brackets A:B Cross product A/B A ignoring intervening B T.2 Output projection of T A -> B Rewrite A as B || C _ D Context specifier .#.,1,2
13312,13619705,Any symbol in alphabet 0 The empty string (epsilon) Aˆk k-ary concatenation % Escape symbol [ and ] Grouping brackets A:B Cross product A/B A ignoring intervening B T.2 Output projection of T A -> B Rewrite A as B || C _ D Context specifier .#.,18,19
13313,13619705,Every symbol in position k in the single-tape representation corresponds to-in the case of n tapes-position k/n on tape (k mod n).,1,2
13314,13619705,"We assume a special representation for empty symbols ( -symbols) in the single-tape model, and represent them with the symbol .",23,24
13315,13619705,"A string of length l×n in the single-tape representation would correspond to the multi-tape representation as follows, where, in parentheses, the tape number is shown first, followed by the symbol position in the multitape representation.",37,38
13316,13619705,"To convert a transducer where transitions are encoded as symbol pairs, one simply expands each symbol pair x:y to a two-symbol sequence x y in the corresponding n-tape automaton.",9,10
13317,13619705,"To convert a transducer where transitions are encoded as symbol pairs, one simply expands each symbol pair x:y to a two-symbol sequence x y in the corresponding n-tape automaton.",16,17
13318,13619705,"To convert a transducer where transitions are encoded as symbol pairs, one simply expands each symbol pair x:y to a two-symbol sequence x y in the corresponding n-tape automaton.",25,26
13319,13619705,"In the multi-tape case, however, this filtering mechanism can be encoded entirely as a regular language filter which disallows certain interleavings of epsilon-symbols, in particular those where an x: -transition (when automaton A has an epsilon on the last tape in some position) immediately follows or precedes a :ytransition (when automaton B inserts a symbol on its first pair of tapes).",66,67
13320,207853431,"Then a decoder f dec with attention network attentively accesses H for an auto-regressive generation of each y i until the end of sequence symbol (EOS) is generated: P (y i |y <i , X) = softmax(f dec (y i−1 , s t , c t ; θ dec )) (1) where c t is the attentive result for current decoder state s t given H. Actor-Critic for Reinforcement Learning Reinforcement learning (Sutton and Barto, 2018, RL) is a widely used machine learning technique following the paradigm of explore and exploit, which is apt for unsupervised policy learning in many challenging tasks (e.g., games (Mnih et al.,",26,27
13321,15860214,"2 Worsening The fundamental technique behind the finite-state matching approach to OT is a device which we call 'worsening', used to filter out strings from a transducer containing more occurrences of some designated special symbol s (e.g. a violation marker), 2 Also discussed in Jäger (2002) .",39,40
13322,15860214,"This, of course, presupposes that we are using transducers to mark constraint violations in input strings, say by the symbol *.",22,23
13323,240353789,"2) Sentence Features & Similarity (SF & Sim): using the sentence-level features of label annotations (i.e., the embedding of [CLS] symbol) instead of token-level features.",29,30
13324,11412166,"First, the input to the grammar is assumed to be a string containing (separated by the '&' symbol) the following information : • the morphological information (using EAGLES-style tags Leech and Wilson (1996) ) for all nodes (separated by '+' symbol) in the Spanish verbal chunk (haber[VAIP3S0]+negar[VMP00SM]); • the morphological information of the subject ([sub3s]), the direct object ([obj3p]) and the indirect object ([iobj3p]); • the translation of the main verb in Basque (ukatu) and information about its transitivity  ([DIO]), indicating a ditransitive construction: haber[VAIP3S0]+negar[VMP00SM] & [sub3s][obj3p][iobj3p] & ukatu[DIO] The grammatical rules are organized into three groups according to the three main steps defined for translating verbal chunks: 1.",21,22
13325,11412166,"First, the input to the grammar is assumed to be a string containing (separated by the '&' symbol) the following information : • the morphological information (using EAGLES-style tags Leech and Wilson (1996) ) for all nodes (separated by '+' symbol) in the Spanish verbal chunk (haber[VAIP3S0]+negar[VMP00SM]); • the morphological information of the subject ([sub3s]), the direct object ([obj3p]) and the indirect object ([iobj3p]); • the translation of the main verb in Basque (ukatu) and information about its transitivity  ([DIO]), indicating a ditransitive construction: haber[VAIP3S0]+negar[VMP00SM] & [sub3s][obj3p][iobj3p] & ukatu[DIO] The grammatical rules are organized into three groups according to the three main steps defined for translating verbal chunks: 1.",53,54
13326,11412166,"simpleVerbEuSchema is the type of the verbal chunk (SimpleVerb) and an automaton that contains as strings the pattern of elements (separated by the '/' symbol) that the corresponding Basque verb chunk will need to have (in this case, the main verb and the auxiliary verb): SimpleVerb (main) AspectMain / Aux TenseMood Abs Dat Erg 2.",29,30
13327,11437620,the length of the conditioning environment being 1 (1 symbol needs to be seen to the left plus zero symbols to the right).,10,11
13328,11437620,"Naturally, in this example we have two competing alternatives to the shortest generalization: we could also have chosen to condition the i-deletion rule by the t that follows the i. Both conditioning environments are exactly one symbol long.",40,41
13329,11437620,"String-to-string vs. single-symbol rules In some cases several consecutive input symbols fail to correspond to the output in the learning data, as in for example the pairing d a u t d i ∅ t corresponding to the dialect-standard pair daut/dit.",8,9
13330,11437620,"Since there is no requirement in our formalism of rewrite rules that they be restricted to single-symbol rewrites only, there are two ways to handle this: either one can create a string-to-string rewriting rule: au → i / CONTEXT or create two separate rules a → i / CONTEXT , u → ∅ / CONTEXT where CONTEXT refers to the minimal conditioning environment determined by the rest of the data.",18,19
13331,237491955,Entries starting with symbol ?,3,4
13332,15217163,"Additionally, the symbol #X# is used to mark readings that have been removed, and the symbol #0# readings that are still possible.",3,4
13333,15217163,"Additionally, the symbol #X# is used to mark readings that have been removed, and the symbol #0# readings that are still possible.",19,20
13334,15217163,These three transducers are again composed with a transducer that restricts the occurrence of the $A$symbol to those cohorts where the rule contexts are in place.,17,18
13335,2604757,"For instance, the input for the tag-independent AG model for a noun table is just a sequence of characters separated by spaces: t a b l e However, for the tag-dependent model it has to be reformatted as: N t a b l e, where N is the terminal symbol denoting the noun POS.",58,59
13336,2604757,"The tag assignments of the unsupervised POS induction model are just integer numbers and thus for instance, if the model has assigned a tag 3 to the noun table then the input has to be reformatted as: 3 t a b l e, where 3 is the terminal symbol denoting the induced tag cluster 3.",51,52
13337,233805178,Each character is treated as as separate symbol by the model.,7,8
13338,53609568,"If the token was a punctuation or an emoticon, they were marked as punctuation and symbol without using the tagger.",16,17
13339,209683601,"Tweets are different from ordinary text due to their short length, hashtags, mentions (user identifiers preceded by an @ symbol), links, and other non-standard text tokens that they contain.",22,23
13340,220046498,"The names of stocks, such as ""AAPL"" (the ticker symbol for Apple Inc.), typically appear in a financial news article text.",13,14
13341,220046498,"Our stock embedding can be considered as one technique to specialize, or ground, a symbol that has a practical reality outside of text.",16,17
13342,220046498,"For each stock, we used the word embedding of its ticker symbol, e.g., the word embedding of ""AAPL"" for Apple Inc. Word2vec-news: (text only) Σ was the cosine matrix of the word embedding vectors trained on news text corpora.",12,13
13343,220046498,"We used the full text of the R&B dataset for training, in which all mentions of a stock in the text were replaced by the stock's ticker symbol.",29,30
13344,233253,The × symbol shows the results of modifier-biased interpretation (α = 0.8) and the + symbol shows the results of head noun-biased interpretation (α = 0.2).,2,3
13345,233253,The × symbol shows the results of modifier-biased interpretation (α = 0.8) and the + symbol shows the results of head noun-biased interpretation (α = 0.2).,19,20
13346,57189528,"Following Kiperwasser and Goldberg (2016) , we also use word dropout to learn embeddings for unknown syllables/words: we replace each syllable/word token s/w appearing #(s/w) times with a ""unk"" symbol with probability p unk (s/w) = 0.25 0.25+#(s/w) .",45,46
13347,7604036,Regular expression (RE) representation of the chunk types The consecutive identical chunk types in the Chunk-typesequence feature merged into a single symbol.,25,26
13348,221856691,"In an LM setting, when the Transformer processes the input sequentially, each input symbol can only attend over itself and the previous inputs, masking is applied over the inputs following it.",15,16
13349,221856691,"Let Reset-Dyck-1 be the language defined over the alphabet Σ = {[, ], #}, where # denotes a symbol that resets the counter.",25,26
13350,221856691,"When the machine encounters the reset symbol #, it must ignore all the previous input, reset the counter to 0 and go to start state.",6,7
13351,221856691,"The key limitation for both with and without encodings is the fact that for a single layer network the scor- ing function Q(x n ), K(x # ) and the value vector corresponding to the reset symbol is independent of the preceding inputs which it is supposed to negate (reset).",37,38
13352,221856691,"The same limitation does not hold for multilayer networks where the value vector, as well as the scoring function for the reset symbol, are dependent on its preceding inputs.",23,24
13353,221856691,The counters can be incremented or decremented by any values but it will only depend on the input symbol.,18,19
13354,221856691,The embedding vector x i of each symbol will have 0s in the first 2k dimensions and the last |Σ| dimensions will have the one-hot encoding representation of the symbol.,7,8
13355,221856691,The embedding vector x i of each symbol will have 0s in the first 2k dimensions and the last |Σ| dimensions will have the one-hot encoding representation of the symbol.,31,32
13356,221856691,"That is, x 2j:2j+1 will be reserved for the jth counter where 0 ≤ j < k. For any given input symbol s, if u(s) has counter operation of +m at the jth counter, then the value will be such that v will contain +m at index 2j and −m at index 2j + 1 upto index 2k.",22,23
13357,221856691,"Let s 0 be a special start symbol with embedding f e = [+1, −1].",7,8
13358,221856691,"The embeddings of each input symbol s ∈ Σ are defined as follows, f e (s) = [+(r − 1), −(r − 1)] where r denotes the arity of the symbol.",5,6
13359,221856691,"The embeddings of each input symbol s ∈ Σ are defined as follows, f e (s) = [+(r − 1), −(r − 1)] where r denotes the arity of the symbol.",40,41
13360,221856691,"Let Reset-Dyck-1 be a language defined over alphabet Σ = {[, ], 1}, where 1 denotes a symbol that requires a reset operation.",24,25
13361,221856691,"So essentially, when the machine encounters the reset symbol 1, it has to ignore all the previous inputs, reset the counter to 0 and go to start state.",9,10
13362,221856691,s n be an input sequence w. Let s r denote the r-th symbol where the reset symbol occurs.,15,16
13363,221856691,s n be an input sequence w. Let s r denote the r-th symbol where the reset symbol occurs.,19,20
13364,221856691,"It is easy to see that the scoring function q n , K(v r ) is independent of the position as well as the inputs before the reset symbol which are relevant for the reset operation.",28,29
13365,221856691,"If the reset symbol occurs after the first half of the sequence, then the word belongs to Dyck-1 and if it occurs in the beginning then it does not belong to the language Dyck-1.",3,4
13366,221856691,"However, by construction, the output of the model z n will remain the same regardless of the position of the reset symbol and hence by contradiction, it cannot recognize such a language.",23,24
13367,221856691,The scoring function as well as value vector of the reset symbol will be dependent of the inputs that precede it.,11,12
13368,221856691,"Since it is a unary language, the input at each step will be the same symbol and hence the embedding as well as query, key and value vectors will be the same.",16,17
13369,221856691,"When the end-of-sequence symbol is allowed as one of the next set of legal characters, it implies that the underlying automaton is in the final state and the input can be accepted.",7,8
13370,221856691,"If the next character is among the model's predicted set of valid characters at each step i and the end of symbol character is allowed at the n-th step, then the word is accepted and if any character is not within the model's predicted set of valid characters, then the word is rejected.",22,23
13371,237572382,These include an author on BMR with frequent '£' symbol and ellipses ('...') and an author on Agora who only posted referral links (with an eponymous username 'Refer-ralLink').,11,12
13372,4088755,"n-tape space complexity There is a fundamental space complexity problem with multi-tape automata, which is that when the number of tapes grows, the required joint symbol alphabet grows with exponential rapidity unless special mechanisms are devised to curtail this growth.",31,32
13373,4088755,"Now, assume we want to restrict the co-occurrence of s 1 on any combination of tapes, meaning s 1 can only occur once on one tape in the same position, i.e. we would be accepting any strings containing a symbol such as s 1 :s 2 :s 2 :s 2 :s 2 or s 2 :s 2 :s 2 :s 2 :s 3 but not, s 1 :s 2 :s 3 :s 4 :s 1 .",44,45
13374,4088755,The encoding also avoids the abovementioned blow-up problems related to symbol combinations on multiple tapes.,12,13
13375,4088755,"We use the symbol Σ to specify the alphabet, and the shorthand \a to denote any symbol in the alphabet except a. Slight additional notation will be introduced in the course of elaborating the model.",3,4
13376,4088755,"We use the symbol Σ to specify the alphabet, and the shorthand \a to denote any symbol in the alphabet except a. Slight additional notation will be introduced in the course of elaborating the model.",17,18
13377,4088755,"The first symbol corresponds to the first symbol on tape 1, the second to the first on tape 2, etc.:",2,3
13378,4088755,"The first symbol corresponds to the first symbol on tape 1, the second to the first on tape 2, etc.:",7,8
13379,4088755,"For instance, the two-tape correspondence: T 1 a T 2 b c would be encoded as the string abεc, ε being a special symbol used to pad the blanks on a tape to keep all tapes synchronized.",28,29
13380,4088755,"This means that, for example, for an 8-tape representation, every 8th symbol from the beginning is a symbol representing tape 1.",16,17
13381,4088755,"This means that, for example, for an 8-tape representation, every 8th symbol from the beginning is a symbol representing tape 1.",22,23
13382,4088755,"Although this is the final encoding we wish to produce, we have added one extra temporary feature to facilitate the construction: every symbol on any 'tape' is always preceded by a symbol indicating the tape number drawn from an alphabet T 1 , . . . ,",24,25
13383,4088755,"Although this is the final encoding we wish to produce, we have added one extra temporary feature to facilitate the construction: every symbol on any 'tape' is always preceded by a symbol indicating the tape number drawn from an alphabet T 1 , . . . ,",35,36
13384,4088755,"The second step is to constrain the co-occurrence of symbols on the individual tapes, i.e. a consonant on the root tape must be matched by a consonant of the input tape as well as the symbol C on the pattern tape, etc.",38,39
13385,4088755,"Base ∩ Rules Populating the tapes We have three auxiliary functions, TapeL(X,Y), TapeM(X,Y), and TapeA(X,Y), where the argument X is the tape number, and Y the language we with to insert on tape X. 3 TapeL(X,Y) creates strings where every symbol from the language Y is preceded by the tape indicator T X and where the entire tape is left-aligned, meaning there are no initial blanks on that tape.",57,58
13386,4088755,"Hence, to illustrate this behavior, TapeL(4,C V C V C) will produce strings like: XT 4 CXT 4 VXT 4 CXT 4 VXT 4 CY where X is any sequence of symbols not containing the symbol T 4 , and Y any sequence possibly containing T 4 but where T 4 is always followed by ε, i.e. we pad all tapes at the end to allow for synchronized strings on other tapes containing more material to the right.",39,40
13387,4088755,Every other symbol is the T X tape marker symbol and every other symbol is the actual symbol on that tape (allowing for the special symbol ε also to represent blanks on a tape).,2,3
13388,4088755,Every other symbol is the T X tape marker symbol and every other symbol is the actual symbol on that tape (allowing for the special symbol ε also to represent blanks on a tape).,9,10
13389,4088755,Every other symbol is the T X tape marker symbol and every other symbol is the actual symbol on that tape (allowing for the special symbol ε also to represent blanks on a tape).,13,14
13390,4088755,Every other symbol is the T X tape marker symbol and every other symbol is the actual symbol on that tape (allowing for the special symbol ε also to represent blanks on a tape).,17,18
13391,4088755,Every other symbol is the T X tape marker symbol and every other symbol is the actual symbol on that tape (allowing for the special symbol ε also to represent blanks on a tape).,26,27
13392,4088755,To take an example: in order to constrain the template we need two rules that effectively say that every C and V symbol occurring in the template tape must be matched by 1) a consonant on the root tape and 2) a vowel on the input tape.,23,24
13393,4088755,Because of our single-tape encoding the first rule translates to the idea that every T 4 C sequence must be directly preceded by T 2 followed by some consonant followed by T 3 and any symbol at all: T 4 C ⇒ T 2 Cons T 3 Σ (1) and the second one translates to: T 4 V ⇒ T 1 Vow T 2 Σ T 3 Σ (2) assuming that Vow is the language that contains any vowel and Cons the language that contains any consonant.,37,38
13394,4088755,"In (1), for instance, which constrains a symbol on tape 4 with a consonant on tape 2, there are only 2 intervening symbols, namely that of tape 3.",11,12
13395,4088755,"To distinguish this C behavior from that of Form X which is also commonly described with two adjacent C symbols where, however, there is no such association (as in the stem staktab) we need to introduce another symbol.",41,42
13396,4088755,"This symbol C 2 occurs in Form II, which becomes CVCC 2 VC.",1,2
13397,4088755,"To give an overview of some of the subsequent constraints that are still necessary, we include here a few descriptions and examples (where the starred (***) tape snippets exemplify illegal configurations): • Every root consonant has a matching consonant on the input tape T 1 k a t a b a T 2 k t b T 1 k a t a b a T 2 *** d r s • A vowel in the input which is matched by a V in the pattern, must have a corresponding vocalization vowel T 1 k a t a b a T 4 C V C V C T 7 a a T 1 k a t a b a T 4 C V C V C T 7 *** u i • A position where there is a symbol in the input either has a symbol in the pattern tape or a symbol in the affix tape (but not both) T 1 k a t a b a T 4 C V C V C T 5 a T 1 k a t a b a T 4 C V C V C T 5 *** 7 The idea to preserve the gemination in the grammar is similar to the solutions regarding gemination and spreading of Forms II, V, and IX documented in Beesley (1998b) and Habash and Rambow (2006) .",149,150
13398,4088755,"To give an overview of some of the subsequent constraints that are still necessary, we include here a few descriptions and examples (where the starred (***) tape snippets exemplify illegal configurations): • Every root consonant has a matching consonant on the input tape T 1 k a t a b a T 2 k t b T 1 k a t a b a T 2 *** d r s • A vowel in the input which is matched by a V in the pattern, must have a corresponding vocalization vowel T 1 k a t a b a T 4 C V C V C T 7 a a T 1 k a t a b a T 4 C V C V C T 7 *** u i • A position where there is a symbol in the input either has a symbol in the pattern tape or a symbol in the affix tape (but not both) T 1 k a t a b a T 4 C V C V C T 5 a T 1 k a t a b a T 4 C V C V C T 5 *** 7 The idea to preserve the gemination in the grammar is similar to the solutions regarding gemination and spreading of Forms II, V, and IX documented in Beesley (1998b) and Habash and Rambow (2006) .",156,157
13399,4088755,"To give an overview of some of the subsequent constraints that are still necessary, we include here a few descriptions and examples (where the starred (***) tape snippets exemplify illegal configurations): • Every root consonant has a matching consonant on the input tape T 1 k a t a b a T 2 k t b T 1 k a t a b a T 2 *** d r s • A vowel in the input which is matched by a V in the pattern, must have a corresponding vocalization vowel T 1 k a t a b a T 4 C V C V C T 7 a a T 1 k a t a b a T 4 C V C V C T 7 *** u i • A position where there is a symbol in the input either has a symbol in the pattern tape or a symbol in the affix tape (but not both) T 1 k a t a b a T 4 C V C V C T 5 a T 1 k a t a b a T 4 C V C V C T 5 *** 7 The idea to preserve the gemination in the grammar is similar to the solutions regarding gemination and spreading of Forms II, V, and IX documented in Beesley (1998b) and Habash and Rambow (2006) .",163,164
13400,4088755,"For instance, in the running example we have presented, one of the parse tapes has included the symbol +3P +Masc +Sg, aligned with the affix that represents the grammatical information: . . .",19,20
13401,4088755,This alignment can be achieved by a constraint in the grammar to the effect that the first non-blank symbol on the affix tape is in the same position as the first non-blank symbol on the affix parse tape.,20,21
13402,4088755,This alignment can be achieved by a constraint in the grammar to the effect that the first non-blank symbol on the affix tape is in the same position as the first non-blank symbol on the affix parse tape.,36,37
13403,4088755,"Single-tape transduction For our implementation, we have simply modified the automaton matching algorithm in the toolkit we have used, foma 8 to, instead of matching every symbol, matching the first symbol as the 'input', then outputting the subsequent n (where n is 7 in our example) legal symbols if the subsequent input symbols match.",31,32
13404,4088755,"Single-tape transduction For our implementation, we have simply modified the automaton matching algorithm in the toolkit we have used, foma 8 to, instead of matching every symbol, matching the first symbol as the 'input', then outputting the subsequent n (where n is 7 in our example) legal symbols if the subsequent input symbols match.",36,37
13405,4088755,"Future work The transduction mechanism mentioned above works well and is particularly easy to implement when the first 'tape' is the input tape containing the word one wants to parse, since one can simply do a depth-first search until the the next symbol on the input tape (in our running example with 8 tapes, that would be 7 symbols forward) and discard the paths where the subsequent tape 1 symbols do not match, resulting in nearly linear running time.",47,48
13406,4088755,Brackets indicate grouping and Σ any symbol.,6,7
13407,4088755,"The notation \X denotes any single symbol, except X. The symbol ε here is the special 'blank' symbol used to pad the tapes and keep them synchronized.",6,7
13408,4088755,"The notation \X denotes any single symbol, except X. The symbol ε here is the special 'blank' symbol used to pad the tapes and keep them synchronized.",11,12
13409,4088755,"The notation \X denotes any single symbol, except X. The symbol ε here is the special 'blank' symbol used to pad the tapes and keep them synchronized.",20,21
13410,218628898,The variable node assigns one (S)DRS node with a specific symbol.,11,12
13411,218628898,"Encoder Given a paragraph, we concatenate all the sentences into one sequence, where each sentence is augmented with a start symbol s and an end token e at the front and end positions, respectively, obtaining a final input sequence for the paragraph D = s , w 1,1 , ..., w 1,n 1 , e , s , w 2,1 , ..., w m,nm , e .",22,23
13412,218628898,"Decoder We transform the DRTS structure into a sequence of symbols, so that the original DRTS can be restored from the symbol sequence as well.",22,23
13413,218628898,"We define two types of symbols for each skeleton, where the first is the node label conjoined by a left bracket, indicting the start of traversal of the current node, and the second symbol is a right bracket, indicting the end of traversal of the current node.",36,37
13414,218628898,"In this way, we can obtain a symbol sequence Y skt = y skt 1 , ..., y skt s which is equivalent to the skeleton tree.",8,9
13415,218628898,"Given a set of labeled data, the model is trained to minimize average cross-entropy losses over all individual symbol predictions: L(θ) = − 1 N i logp y * i (4) where θ are the set of model parameters, p y * i denotes the output probability of y * i , which is computed by softmax over o i , N is the total length of the output sequence.",21,22
13416,237635233,"Formally, given an input conversation x, this objective is to minimize the negative log likelihood of the corresponding correct label sequence y. Our model predict the corresponding label y t based on the token representation p t and its corresponding utterance representation h k : p(y t |x; θ) = p(y t |p t , h k ; θ) = softmax(W c [p t ⊕ h k ]) T δ yt (1) where W c is the softmax matrix and δ yt is Kronecker delta with a dimension for each output symbol, so softmax(W c [h t ⊕ g k ]) T δ yt is exactly the y t 'th element of the distribution defined by the softmax.",100,101
13417,7371255,"Emoticons, which may be isolated or part of an input token, are also identified, and converted into a conventional symbol (#).",22,23
13418,7371255,"In cases where an Arabizi word represents a prefix or suffix that should be joined in CODA to the next or previous word, a [+] symbol is added to mark this decision.",28,29
13419,7371255,"Similarly, for Arabizi words that should be split into multiple CODA words, the CODA words are written with added [-] symbol delimiting the word boundaries.",24,25
13420,7371255,"All emoticons, whether free-standing or attached to a word, are replaced by a single hash symbol (#).",19,20
13421,2727624,"ways headed either by a conjunction, or, if no conjunction is present, by a punctuation symbol.",18,19
13422,8461406,One of tile drawbacks of an tlMM-based approach is that laborious manual tuning of symbol and transition biases is nec: essary to achieve high accuracy.,16,17
13423,204949631,"First, each word is represented as a sequence of tokens plus a special end of word symbol.",17,18
13424,204949631,"First, a word is split into distinct characters plus the end of word symbol.",14,15
13425,236459917,"However, those with developmental disabilities (e.g., autism spectrum disorder, ASD) or lexical and semantic processing impairments that limit their ability to spell out words (e.g., adults with aphasia 1 ) must usually rely on less expressive symbol-based systems, for which those techniques offer little sup-1 a language disorder mostly often caused by a stroke.",43,44
13426,236459917,"Users of symbol-based AAC typically do not construct full, grammatically correct sentences, complete with prepositions and infections, but rather often only need a few key content words (i.e., nouns, adjectives, verbs)-appearing at any part of the sentence-to supplement other forms of communication, including preserved speech, gestures, or drawings.",2,3
13427,236459917,"Nonetheless, there is much opportunity for improving symbol-based AAC, which is often abandoned because it offers too little communication support relative to the effort required to learn and use (Moffatt et al.,",8,9
13428,236459917,"In this paper, we call for more research in the NLP community devoted to language generation for symbol-based AAC systems.",18,19
13429,235795670,"The symbol ' §' indicates gold segmentation, and the symbol ' ¶' denotes automated segmentation.",1,2
13430,235795670,"The symbol ' §' indicates gold segmentation, and the symbol ' ¶' denotes automated segmentation.",11,12
13431,11319902,"We will represent the pattern as a string made up of numbers to indicate radical position, of the symbol V to indicate the position of the vocalism, and of pattern consonants (if needed).",19,20
13432,9225732,"We use numbers (i.e. 1, 2, 3, 4, or 5) to indicate radical position 3 and the symbol V is used to indicate the position of the vocalism.",23,24
13433,9225732,"There are two special symbols: (1) % is a wild card symbol that can match anything (appropriate for that tier) and (2) @<Letter> (e.g., @X) is a variable whose type can be defined explicitly.",14,15
13434,9225732,"For our example (9), V1tV2V3 leads to this initial MTT sequence: (10) [V0%00] [1%000] [t0000] [V0%00] [2%000] [V0%00] [3%000] When the symbol V appears in the template, a 0 is inserted in the radical position (since no radical can be inserted here) and a wild card is inserted in the vocalism position.",41,42
13435,9225732,"The opposite is true for when radical symbol (C,1,2,3,4,5) appears in the template, a 0 is inserted in the vocalism tier (as no vowel from the vocalism can be inserted here) and a wild card in the radical tier.",7,8
13436,9225732,"21) a. <@X,,,0> -> @X, @X= [LETTER] b. <C,@X,,0> -> @X c. <V,,@X,0> -> @X Phonological and morphemic rules have the same format, as they write to the fourth tier, usually overwriting a symbol placed there by the default rules.",48,49
13437,222141864,The plus symbol is part of the alphabet of the language.,2,3
13438,221739114,We assume an arbitrary order and o j refers to one particular source in o. The symbol ⊕ denotes concatenation.,16,17
13439,44095805,<symbol> Prefix token <noun> 私 <particle> は <verb> <verb-plain> 走る <symbol> 。,1,2
13440,44095805,<symbol> Prefix token <noun> 私 <particle> は <verb> <verb-plain> 走る <symbol> 。,23,24
13441,44095805,Circumfix token <noun> 私 <particle> は <verb> 走 る <verb-plain> <symbol> 。,21,22
13442,543,Estimation based on Training Data Counts 3.1 Notation We will use the symbol f to denote the number of times a particular tuple is seen in training data.,12,13
13443,235097316,"All material after the # symbol is ignored by the compiler, but a preprocessing command strips all lines containing Dir/LR before compiling the generator transducer (but not the analyser transducer).",5,6
13444,235097316,The twol formalism allows for symbol mappings to be restricted based on context.,5,6
13445,235097316,symbol conjoins the patterns.,0,1
13446,236460083,"B Procedures for Out-of-vocabulary Handling • GLSTMSurp, JLSTMSurp, JLCSurp: Out-ofvocabulary (OOV) words in the test corpus were replaced with a corresponding ""UNK"" symbol prior to surprisal estimation. •",35,36
13447,226262271,"t k ) of length k, let e i be the embedding of source symbol i, let h i be the encoder hidden state at source position i and let d j be the decoder state at target position j. A single forward encoder state is obtained as follows: − → h i = LSTM( − → h i−1 , e i ).",15,16
13448,226262271,"We apply soft-dual attention (Junczys-Dowmunt and Grundkiewicz, 2017 ) to be able to attend over both encoders in the decoder (also see Figure 2 ): d j = LSTM 1 d j−1 , e t j−1 a j = ATT C tok , d j ; ATT C char , d j d j = LSTM 2 d j , a j Here, e t j−1 is the embedding of the previously decoded symbol t, C the set of encoder hidden states for either the tokens or characters, ATT the attention function (dot-product) and d j the final decoder hidden state at step j. This model can easily be extended to more than two encoders, which we will experiment with in Section 4.",81,82
13449,11474481,"Thus, the following are two distinct productions in an ITG: C ~ [AB] c (A B) Consider each nonterminal symbol to stand for a pair of matched strings, so that for example (A1, A2) denotes the string-pair generated by A. The operator [ ] performs the ""usual"" pairwise concatenation so that [AB] yields the string-pair (C1, C2) where C1 = AiB~ and C~ = A2B2.",25,26
13450,11474481,"Either or both x and y may take the special value E denoting an empty string, allowing a symbol of either language to have no counterpart in the other language by being matched to an empty string.",19,20
13451,11474481,"The S category (not to be confused with the start symbol SO) is a placeholder for miscellaneous items including punctuation and adverbs, and functions as a fallback category similar to the A nonterminal in the generic bracketing grammars.",11,12
13452,17128446,"Hence, the two kinds of modification possibilities are analogous to two different arguments (here, actually, dependents) of a predicate occupying different syntactic positions, and the same notation could be used to specify them, with the + symbol: {lex(prepnp(na,acc),sg,'myśl ', ratr({prepnp(o,loc) ;cp(że);prepncp(o,loc,że)} + {adjp(agr)}))} Such argument specifications involving lex may get even more complex due to the fact that lex may occur inside modification specifications of another lex, as in the following description of arguments such as z otwartymi ramionami 'with open arms', more accurate than the one given at the beginning of this subsection: {lex(prepnp(z,inst),pl,'ramię ', ratr({lex(adjp(agr) ,agr,agr,'otwarty',natr)}))} Note that lex(adjp(agr),agr,agr,'otwarty',natr) replaces adjp(agr) within ratr and it specifies that not just any agreeing adjective phrase may modify the nominal form rękami 'arms', but only the simple adjective phrase consisting of an agreeing form of 'open' does.",43,44
13453,17128446,"We propose abbreviating such specifications as follows, with the use of OR: {lex(np(gen),pl,'siła ', ratr({lex(adjp(agr) ,agr,agr,OR('własny','swój'),natr)}))} While we could have reintroduced the batr notation to make this even more readable, this symbol is not used in Walenty uniformly, so it would make sense to replace it with more explicit notation involving lex.",47,48
13454,58275,"We also extend PSTs with a wildcard symbol that can match against any input word, thus allowing the model to capture statistical dependencies between words separated by a fixed number of irrelevant words.",7,8
13455,58275,"A wildcard symbol, '*', is available in node labels to allow a particular word position to be ignored in prediction.",2,3
13456,245769658,Each word is first appended with a start ( w ) and an end ( /w ) symbol.,17,18
13457,226262457,"This process is repeated until a sequence stop symbol is generated by the decoder, to indicate that all entities have been generated.",8,9
13458,226262457,"Note that if the constraint set is empty, which means the last character in s is a leaf node, it will be changed to the set of all category labels joined with a sequence-ending symbol.",38,39
13459,235313435,"Then in the training stage, we can pad T − T special tokens <pad> to the tail of Y to make T = T , then Y = (y 1 , y 2 , <eos>, <pad>); 4976 (3) When T < T , say Y = (y 1 , y 2 , y 3 , y 4 , y 5 , <eos>), which means that more information should be inserted into the original sentence X. Then, we will pad the special symbol <mask> to the tail of X to indicate that these positions possibly can be translated into some new real tokens: X = (x 1 , x 2 , x 3 , <eos>, <mask>, <mask>).",100,101
13460,15618372,"These formulae represent a complete classical propositional logic: each atomic symbol is a variable over the domain {T, F}, and the only operators are truth-functional ones.",11,12
13461,15618372,"We use the quantifiers some, most, all, two, and three, and their negations no, not-all, not-most, less-than-two, and less-than-three, and also include five nouns, four intransitive verbs, and the negation symbol not.",54,55
13462,235097435,Every word in the eventual training data was appended with a task-specific input symbol.,15,16
13463,235253844,"For example: King − Man = Queen − Woman In fact, manipulating embeddings in the vector space reveals syntactic and semantic relations between the original symbol sequences and this feature is indeed useful in true applications.",27,28
13464,235253844,"n-gram Extracting Given a symbol sentence, Joshi et al. (",6,7
13465,235253844,"2020) are widely used for pre-trained contextualized language modeling, they do not focus on our concerned ULR, 1 http://www.speech.sri.com/projects/srilm/download.html which demands an arithmetic corresponding relationship between the symbol and its represented vector.",32,33
13466,235253844,"For example, the following symbol sequence equation ""London is"" + ""the capital of England"" =""London is the capital of England"" (1) indicates a vector algorithmic equation according to our ULR goal, vector(""London is"") + vector(""the capital of England"") =vector(""London is the capital of England"") Thus, if the symbol equation ( 1 ) cannot imply the respective vector equation (2), we may set a training objective to let the ULR model forcedly learn such relationship.",5,6
13467,235253844,"For example, the following symbol sequence equation ""London is"" + ""the capital of England"" =""London is the capital of England"" (1) indicates a vector algorithmic equation according to our ULR goal, vector(""London is"") + vector(""the capital of England"") =vector(""London is the capital of England"") Thus, if the symbol equation ( 1 ) cannot imply the respective vector equation (2), we may set a training objective to let the ULR model forcedly learn such relationship.",66,67
13468,11283565,"1The French LTAG comprises trees with flat structure (no VP node); in the passive tree, the auxiliary is substituted; the same symbol N is used for nominal phrases and nouns, the difference being expressed with a feature <det> (Abeill6, 91) .",26,27
13469,2892917,"The ""@"" symbol preceding the word indicates that no entry has been found for the Taiwanese word shown in the first field; hence the HR mixed script automatically served as the Mandarin candidate word.",4,5
13470,60779225,"There are two types of moves: either an input is read and pushed onto a new stack on top of the stack of stacks, or a fixed number of stacks below and above a designated stack on the stack of stacks is removed and a new symbol is pushed on the top of the designated stack, which is now the top stack (an ""unwrap"" move).",48,49
13471,16813083,"Grammar induction A context-free grammar G = (V, T, S † , P ), or CFG in short, consists of a set of non-terminal symbols V , a set of terminal symbols T , a start symbol S † ∈ V , and a set of production P of the form: A → α, where A ∈ V and α ∈ (V ∪ T ) * .",45,46
13472,16813083,"For example, the CYK parsing algorithm takes as input a Chomsky Normal Form PCFG, i.e., a grammar where all productions are of the form X → Y Z or X → a, where X, Y , and Z are non-terminals and a a terminal symbol.",51,52
13473,16813083,"Let V o be the original, unfactored non-terminal set, and let α ∈ (V o :) * be a sequence of zero or more nonterminal/colon symbol pairs.",32,33
13474,3097408,"Without any context, the symbol ""１０／２１"" can be identified either as a fractional number and read as ""ershiyifenzhishi"" (二十一分之十 'ten over twenty one') with the pos Neqa or as a time point and read as ""shiyueershiyiri"" (10 月 21 日 'Oct. 21') with the pos Ndabd whose semantic role is time.",5,6
13475,219304267,"Without context, the symbol "" １０／２１""can be a fractional number and read as "" ten over twenty one""with the POS "" Neqa""and as ""10月21日 shiyue ershiyiri""with the POS "" Nd""whose semantic role is time.",4,5
13476,219304267,"The mathematical symbol indicating a specific time usually denotes the year together so "" 2005／06／30""in ( 27 ) is tagged as Nd.",2,3
13477,1607750,A failure transition is labeled with a distinct symbol 5 .,8,9
13478,1607750,Hence the WFA with v -transitions gives a lower cost (higher probability) to all strings beginning with the symbol ( .,20,21
13479,62061290,"R. To continue our treatment, we take a function symbol abs : r • !",10,11
13480,59868529,"The resulting grammar contains at most one new nonterminal for any nonterminal symbol of the input grammar, and new rules are formed out of rules from the input grammar by means of a straightforward decomposition.",12,13
13481,59868529,"An approximate grammar ' 0 is obtained from ' ( by introducing at most one new non-terminal symbol $ for each non-terminal ¨and by introducing the rule 1¡ 2"" .",19,20
13482,751575,"Katakana writing is a syllabary rather than an alphabet--there is one symbol for ga (~""), another for gi ( 4 ~"" ), another for gu ( Y"" ), etc.",13,14
13483,751575,"For example, to romanize T y ""Y ~, we look up each symbol in Figure 1 and substitute characters.",15,16
13484,751575,"Inputs and outputs may include the empty symbol ¢. Also following Pereira and Riley (1997) , we have implemented a general composition algorithm for constructing an integrated model P(xlz) from models P(xly ) and P(y[z), treating WFSAs as WFSTs with identical inputs and outputs.",7,8
13485,751575,"2 This gives a total of 40 sounds, including 14 vowel sounds (e.g., AA, AE, UN), 25 consonant sounds (e.g., K, HH, R), plus one special symbol (PAUSE).",39,40
13486,751575,"Our Japanese sound inventory includes 39 symbols: 5 vowel sounds, 33 consonant sounds (including doubled consonants like kk), and one special symbol (pause).",26,27
13487,751575,"We then applied the estimation-maximization (EM) algorithm (Baum 1972; Dempster, Laird, and Rubin 1977) to generate symbol-mapping probabilities, shown in Figure 2 .",25,26
13488,751575,These are the symbol-mapping probabilities shown in Figure 2 .,3,4
13489,751575,Each alignment is scored with the product of the scores of the symbol mappings it contains.,12,13
13490,751575,Repeat 3--6 until the symbol-mapping probabilities converge.,6,7
13491,751575,"We then build a WFST directly from the symbol-mapping probabilities: Figure 2 English sounds (in capitals) with probabilistic mappings to Japanese sound sequences (in lower case), as learned by estimation-maximization.",8,9
13492,751575,"We then ran the EM algorithm to determine symbol-mapping (""garbling"") probabilities.",8,9
13493,14527730,"The sentential symbol S expands to a sequence of X's, where X expands to every other nonterminal symbol in the grammar.",2,3
13494,14527730,"The sentential symbol S expands to a sequence of X's, where X expands to every other nonterminal symbol in the grammar.",19,20
13495,14527730,"Initially, the set of nonterminal symbols consists of a different nonterminal symbol expanding to each terminal symbol.",12,13
13496,14527730,"Initially, the set of nonterminal symbols consists of a different nonterminal symbol expanding to each terminal symbol.",17,18
13497,14527730,"We maintain this property throughout the search process, that is, for every symbol A ~ that we add to the grammar, we also add a rule X ---+ A I. This assures that the sentential symbol can expand to every symbol; otherwise, adding a symbol will not affect the probabilities that the grammar assigns to strings.",14,15
13498,14527730,"We maintain this property throughout the search process, that is, for every symbol A ~ that we add to the grammar, we also add a rule X ---+ A I. This assures that the sentential symbol can expand to every symbol; otherwise, adding a symbol will not affect the probabilities that the grammar assigns to strings.",38,39
13499,14527730,"We maintain this property throughout the search process, that is, for every symbol A ~ that we add to the grammar, we also add a rule X ---+ A I. This assures that the sentential symbol can expand to every symbol; otherwise, adding a symbol will not affect the probabilities that the grammar assigns to strings.",43,44
13500,14527730,"We maintain this property throughout the search process, that is, for every symbol A ~ that we add to the grammar, we also add a rule X ---+ A I. This assures that the sentential symbol can expand to every symbol; otherwise, adding a symbol will not affect the probabilities that the grammar assigns to strings.",49,50
13501,14527730,"As mentioned before, with each new symbol A we also create a rule X ---* A. Evaluating the Objective Function Consider the task of calculating the objective function p(OIG)p(G ) for some grammar G. Calculating We cannot afford to parse the training data for each grammar considered; indeed, to ever be practical for data sets of millions of words, it seems likely that we can only afford to parse the data once.",7,8
13502,14527730,"For the left-hand side of a rule, we always create a new symbol.",15,16
13503,14527730,"We create n new nonterminal symbols {X1,...,X,}, and create all rules of the form: X~ ~ Xj Xk i,j, k e {1,...,n} Xi--* A iE {1,...,n}, A E No~d-{S, X} Nold denotes the set of nonterminal symbols acquired in the initial grammar induction phase, and X1 is taken to be the new sentential symbol.",78,79
13504,14527730,"For smoothing, we combine the expansion distribution of each symbol with a uniform distribution, that is, we take the smoothed parameter ps(A ---* a) to be 1 p,(A ~ a) = (1 -A)p,,(A ---* a) + An3 -F n[T[ where p~ (A --~ a) denotes the unsmoothed parameter.",10,11
13505,14527730,The value n 3 + n[TI is the number of different ways a symbol expands under the Lari and Young methodology.,13,14
13506,14527730,"For each symbol that occurs on the right-hand side of a rule but which was not one of the most frequent 20 symbols, we create a rule that expands that symbol to a unique terminal symbol.",2,3
13507,14527730,"For each symbol that occurs on the right-hand side of a rule but which was not one of the most frequent 20 symbols, we create a rule that expands that symbol to a unique terminal symbol.",33,34
13508,14527730,"For each symbol that occurs on the right-hand side of a rule but which was not one of the most frequent 20 symbols, we create a rule that expands that symbol to a unique terminal symbol.",38,39
13509,14717794,"Definition 6 [TIG] A tree insertion grammar (TIG) is a five-tuple (G, NT, L A, S), where ~. is a set of terminal symbols, NT is a set of nonterminal symbols, I is a finite set of finite initial trees, A is a finite set of finite auxiliary trees, and S is a distinguished nonterminal symbol.",71,72
13510,14717794,A derivation is complete when every frontier node in the tree(s) derived is labeled with a terminal symbol.,18,19
13511,14717794,A tree is lexicalized if at least one frontier node is labeled with a terminal symbol.,15,16
13512,14717794,Rule 4 recognizes the presence of a terminal symbol in the input string.,8,9
13513,14717794,"Second, when predicting a node ~B whose first child is a terminal symbol, it is known from the above that this child must match the next input element.",13,14
13514,14717794,1988) if every elementary structure contains a terminal symbol called the anchor.,9,10
13515,14717794,"Similarly, a TIG is lexicalized if every tree contains a terminal symbol.",12,13
13516,14717794,"Moreover, this grammar can be left anchored--one where the first element of the right hand side of each rule is a terminal symbol.",25,26
13517,14717794,The Al-initial tree is retained under the assumption that A1 is the start symbol of the grammar.,15,16
13518,1858099,"Let R be a binary relation; R + denotes the transitive closure of R and R* denotes the reflexive and transitive closure of R. A context=free grammar G = (N, T, P, S) consists of two finite disjoint sets N and T of nonterminal and terminal symbols, respectively, a start symbol S E N, and a finite set of rules P. Every rule has the form A ~ a, where the left-hand side (lhs) A is an element from N and the right-hand side (rhs) e~ is an element from V +, where V denotes (N U T). (",61,62
13519,1858099,"For technical reasons we sometimes need the augmented set of rules PI, consisting of all rules in P plus the extra rule S ~ ~ .kS, where ff is a fresh nontermihal, and 3_ is a fresh terminal acting as an imaginary zeroth input symbol.",48,49
13520,1858099,"There are two kinds of stack symbol (items), one of the form [i, A, j], which indicates that some subderivation from A is needed deriving a substring of ai+l • • • aj, the other of the form [i, k, A --* a • 7 •/3, m, j], which also indicates that some subderivation from A is needed deriving a substring of ai+l • • • aj, but specifically using the rule A --~ ot7/3, where 7 -'-~* ak+x ... a,n has already been establishe~. Formally, we have 1~ D = {[i,A,j]li<j} If ° = {[i,k,A-*a.y.fl, m,j ] IA--~aZ/3EP~ A i<k<m<_j) AIKorlthm 1 (Head-drlven top-down) a~ = (T, I~ D U IT D, 1nit(n), ~-*, Fin(n)), where Init(n) = [-1, -1, S' ---* • 3_ • S, 0, n], [i,A,j] ~ [i,A,j][i,B,j] where there is A ~ aB__fl E pt 0a [i, k, A ---, a • 7 * B/3, m, j] ~-, [i,k,A---, a • 7 • B/3, m,j][m,B,j] 0b [i,k,A---* aB •7 * /3, m,j] [i,k,A ~ aB • 7 */3, m,j][i,B,k] 1 [i,A,j]~--*[i,k-l,A---*a.a•/3, k,j] where there are A ~ aafl E pt and k such that i<k_<jandak=a 2a [i,k,A ---* a • 7 * a[3, m,j] ~-~ [i,k,A ~ a • 7a */3, m + 1,j] provided m < j and am+l = a 2b Symmetric to 2a (eft 0a and 0b) 3 [i,A,j][i',k,B~•6•,m,j']~ [i,k,a ~ a • B * fl, m,j] where there is A ~ aBBfl E P? (",6,7
13521,1858099,"The consequence is that a rhs can be seen as a binary tree, in which each node is labelled by a grammar symbol.",23,24
13522,1858099,"the smallest set which satisfies closure(Q) D_ Q U {A --~ (a)X(g) • P ] (7)A(~f) • closure(Q) v B • cl0sure(Q)} The trees or rules of which the main head is some specified symbol X can be selected from a set Q by goto(Q, x)= (t • Q It = = A -.",44,45
13523,649372,An LCFG is lexicalized in the sense that every initial and auxiliary tree is required to contain at least one terminal symbol on its frontier.,21,22
13524,649372,"More precisely, an LCFG is a five-tuple (Z, NT, I, A, ,5'), where ~ is a set of terminal symbols, NT is a set of non-terminal symbols, I and A are sets of trees labeled by terminal and nonterminal symbols, and ,5' is a distinguished nonterminal start symbol.",66,67
13525,649372,A derivation is complete when every frontier node is labeled with a terminal symbol.,13,14
13526,649372,"In contrast, the wellknown Creibach Normal Form (CNF) for CFCs is lexicalized, because every production rule is required to be of the form A --+ ac~ (where a is a terminal symbol, A a non-terminal symbol and a a possibly empty string of non-terminal symbols) and therefore locally introduces a lexical item a. It can be shown that for any CFG (.7 (that does not derive the empty string), there is a CNF grammar (.7 ~ that derives the same language.",36,37
13527,649372,"In contrast, the wellknown Creibach Normal Form (CNF) for CFCs is lexicalized, because every production rule is required to be of the form A --+ ac~ (where a is a terminal symbol, A a non-terminal symbol and a a possibly empty string of non-terminal symbols) and therefore locally introduces a lexical item a. It can be shown that for any CFG (.7 (that does not derive the empty string), there is a CNF grammar (.7 ~ that derives the same language.",43,44
13528,649372,"As usual in the above, a CFG (.7 is a fourtuple, (E, NT, P, S), where N is a set of terminal symbols, NT is a set of non-terminal symbols, P is a set of production rules that rewrite non-terminal symbols to strings of terminal and non-terminal symbols, and S is a distinguished non-terminal symbol that is the start symbol of any derivation.",74,75
13529,649372,"As usual in the above, a CFG (.7 is a fourtuple, (E, NT, P, S), where N is a set of terminal symbols, NT is a set of non-terminal symbols, P is a set of production rules that rewrite non-terminal symbols to strings of terminal and non-terminal symbols, and S is a distinguished non-terminal symbol that is the start symbol of any derivation.",79,80
13530,649372,L(TG contains a node for every symbol in E U NT and an arc for every rule in P as follows.,6,7
13531,649372,"For each terminal and non-terminal symbol X in G create a node in LCG labeled with X. For each rule X --+ Ya in G create a directed arc labeled with X ~ Ya from the node labeled with X to the node labeled Y. As an example, consider the example CFG in Figure 7 and the corresponding L(TG shown in Figure 8 .",7,8
13532,649372,"In particular, an initial tree is created corresponding to each noncyclic path in L(/G that starts at a non-terminal symbol X and ends on a terminal symbol y. (A non-cyclic path is a path that does not touch any node twice.)",22,23
13533,649372,"In particular, an initial tree is created corresponding to each noncyclic path in L(/G that starts at a non-terminal symbol X and ends on a terminal symbol y. (A non-cyclic path is a path that does not touch any node twice.)",29,30
13534,649372,"Each initial tree created is lexicalized, because each one has a non-terminal symbol as the left corner element of its frontier.",15,16
13535,649372,"In particular, an attxiliary tree is created corresponding to each minimM cyclic path in LCG that starts at a non-terminM symbol.",23,24
13536,1826215,"In the parsing of augmented grammars, even when two edges have the same nonterminal symbol, they are different in the annotated structures associated with those edges, e.g., feature structures; in such a case, we cannot use one edge in place of another.",15,16
13537,8762213,"For example, in the rewrite rule A ~ B x/y C z/e the terminal symbols z and z are symbols of the language Lx and are emitted on stream 1, while the terminal symbol y is a symbol of the language L2 and is emitted on stream 2.",39,40
13538,8762213,"For example, in the rewrite rule A ~ B x/y C z/e the terminal symbols z and z are symbols of the language Lx and are emitted on stream 1, while the terminal symbol y is a symbol of the language L2 and is emitted on stream 2.",43,44
13539,8762213,A matched terminal symbol pair such as z/y is called a couple.,3,4
13540,8762213,"As a spe,Aal case, the null symbol e in either language means that no output token is generated.",9,10
13541,8762213,"We call a symbol pair such as x/e an Ll-singleton, and ely an L2-singleton.",3,4
13542,8762213,"lfe E LI(G) and e E L2(G), then G' contains a single production of the form S' --~ e / c, where S' is the start symbol of G' and does not appear on the right-hand side of any production of G' ; 2.",33,34
13543,8762213,"Aside from the start symbol S, BTGs contain only one non-terminal symbol, A, which rewrites either recursively as a string of A's or as a single terminal-pair.",4,5
13544,8762213,"Aside from the start symbol S, BTGs contain only one non-terminal symbol, A, which rewrites either recursively as a string of A's or as a single terminal-pair.",14,15
13545,8762213,"To be legal, a rotation must preserve symbol order on both output streams.",8,9
13546,9712152,"There are four positions for a dot associated with a symbol in a dotted tree: left above, left below, right below and right above.",10,11
13547,9712152,A dotted tree has one such dotted symbol.,7,8
13548,9712152,"New states are built by three transitions: s,{*a} -a sj {a'}, a is a terminal symbol; s,{A,} #""g~' sj{A'}, fl can adjoin at node A; s,{.A} #.?oo, sj{A,}, A is a footnode.",21,22
13549,8468339,"Gh) S • N is the initial symbol, and d(S) = 1.",8,9
13550,8468339,"t~ is called the root symbol, or shortly, the root of t. Hereafter, a term in 7""~ (X) is also called a tree, and we use terminology of trees such as subtree, node and so on.",5,6
13551,8468339,"Let G -(N, T, P, S) be a context-free grammar (cfg) where N, T, P and S are a set of nonterminal symbols, a set of terminal symbols, a set of productions and the initial symbol, respectively.",47,48
13552,8468339,"Let T~(G) be the set of derivation trees in G, and 7¢s(G) C 7¢(G) be the set of derivation trees whose root is labeled with a production of which left-hand side is the initial symbol S. Clearly, T~s(G) C_ T~(¢) holds.",40,41
13553,8468339,"Sometimes, a nonterminal symbol, a terminal symbol and an annotated production are abbreviated as a nonterminal, a terminal and a production, respectively, i 4) S • N is the initial symbol, (5) Nat~ is a finite set of attributes, and (6) A~tm is a finite set of atoms.",4,5
13554,8468339,"Sometimes, a nonterminal symbol, a terminal symbol and an annotated production are abbreviated as a nonterminal, a terminal and a production, respectively, i 4) S • N is the initial symbol, (5) Nat~ is a finite set of attributes, and (6) A~tm is a finite set of atoms.",8,9
13555,8468339,"Sometimes, a nonterminal symbol, a terminal symbol and an annotated production are abbreviated as a nonterminal, a terminal and a production, respectively, i 4) S • N is the initial symbol, (5) Nat~ is a finite set of attributes, and (6) A~tm is a finite set of atoms.",36,37
13556,8468339,"We define the values of both sides of a functional schema attached to the symbol in p (on v) as follows: * the value of T atr(atr • Nat,) is Fv.atr, • the value of + in an S schema is Fv~ if the S schema is attached to the i(1 _< i _< q)th symbol in the right-hand side of p, and • the value of atom atm in a V schema is arm itself.",14,15
13557,8468339,"We define the values of both sides of a functional schema attached to the symbol in p (on v) as follows: * the value of T atr(atr • Nat,) is Fv.atr, • the value of + in an S schema is Fv~ if the S schema is attached to the i(1 _< i _< q)th symbol in the right-hand side of p, and • the value of atom atm in a V schema is arm itself.",63,64
13558,8468339,"Let T' = A td {b} where b is a newly introduced symbol and let N' = {S',RI,...,Rm, AI,...,An} where d(Ri) = d(Aj) = t for 1 < i <_ m and 1 < j <_ n. Productions and functions of G ~ will be constructed to have the following property.",15,16
13559,8468339,D The basic idea is to simulate the move of tree transducer M which is scanning a symbol Ph (resp.,17,18
13560,8468339,"Ah) of pmcfg G I. During the move of M, it may happen that no rule is defined for a current configuration and hence no output will be derived• The symbol b is introduced to represent such an undefined move explicitly.",32,33
13561,8468339,"Intuitively, the right-hand side of this production corresponds to the initial configuration, that is, M is in the initial state ql and scanning the root symbol Ph of a derivation tree, where the left-hand side of Ph is the initial symbol S. The pmcfg G I constructed above satisfies Property 6.1.",30,31
13562,8468339,"Intuitively, the right-hand side of this production corresponds to the initial configuration, that is, M is in the initial state ql and scanning the root symbol Ph of a derivation tree, where the left-hand side of Ph is the initial symbol S. The pmcfg G I constructed above satisfies Property 6.1.",48,49
13563,8468339,Each pair of a symbol (either nonterminal or terminal) X of G and a state qj of M is represented by a single nonterminal X[J] in G'.,4,5
13564,8468339,"Step 2: A move when M at state qj reads a symbol p which is the label of a production p : C --+ ..., can be simulated by a production in G ~ whose left-hand side is C[J] {T ute = p}"" Formally, the set P~ of productions of G I is constructed as follows. (",12,13
13565,540117,"Finally, the RMRS ( 6 ) is a notational variant of the MRS derived by the ERG, a wide-coverage deep grammar: (6) l 1 : a 1 : every q 1(x 1 ), RSTR(a 1 , h 2 ), BODY(a 1 , h 3 ) l 41 : a 41 : fat j 1(e ), ARG 1 (a 41 , x 2 ) l 42 : a 42 : cat n 1(x 3 ) l 5 : a 5 : chase v 1(e), ARG 1 (a 5 , x 4 ), ARG 2 (a 5 , x 5 ) l 6 : a 6 : some q 1(x 6 ), RSTR(a 6 , h 7 ), BODY(a 6 , h 8 ) l 9 : a 9 : dog n 1(x 7 ) h 2 = q l 42 , l 41 = l 42 , h 7 = q l 9 x 1 = x 2 , x 2 = x 3 , x 3 = x 4 , x 5 = x 6 , x 5 = x 7 RSTR and BODY are conventional names for the ARG 1 and ARG 2 of a quantifier predicate symbol.",216,217
13566,540117,"The fact that the former symbol should be more specific than the latter can be represented using SPEC atoms like dog n 1 dog n. Note that even a deep grammar will not fully disambiguate to semantic predicate symbols, such as WordNet senses, and so dog n 1 can still be consistent with multiple symbols like dog n 1 and dog n 2 in the semantic representation.",5,6
13567,540117,"However, unlike the output of a POS tagger, an RMRS symbol that's output by a deep grammar is consistent with symbols that all have the same arity, because a deep grammar fully determines lexical subcategorisation.",12,13
13568,540117,"Finally, a function σ from Pred to the power set of Σ ≥1 maps each RMRS predicate symbol to a set of constructors from Σ. As we'll see shortly, this function allows an RMRS to underspecify lexical ambiguities.",18,19
13569,540117,"And σ will map each RMRS predicate symbol (which represents a word) to the set of its fully resolved meanings, e.g. cat n to a set containing cat n 1 and possibly others.",7,8
13570,14963601,"fl where A ---+ aft is some rule of G..A4(G) is the determinization by the standard subset construction (Aho and Ullman, 1977) of the FSA defined as follows: • The initial state is the dotted rule S' ~ S. where S is the start symbol of G and S' is a new auxiliary start symbol. •",52,53
13571,14963601,"fl where A ---+ aft is some rule of G..A4(G) is the determinization by the standard subset construction (Aho and Ullman, 1977) of the FSA defined as follows: • The initial state is the dotted rule S' ~ S. where S is the start symbol of G and S' is a new auxiliary start symbol. •",63,64
13572,14963601," * There is a transition labeled X, where X is a terminal or nonterminal symbol, from dotted rule A ~ a.X/3 to A ~ aX ./3. •",16,17
13573,14963601,"There is an e-transition from A -+ a. B/3 to B ~ ""7, where B is a nonterminal symbol and B ~ ""7 a rule in G. .M(G) can be seen as a finite state control for a nondeterministic shift-reduce pushdown recognizer for G. A state transition labeled by a terminal symbol z from state s to state s' licenses a shift move, pushing onto the stack of the recognizer the pair (s, Ix).",22,23
13574,14963601,"There is an e-transition from A -+ a. B/3 to B ~ ""7, where B is a nonterminal symbol and B ~ ""7 a rule in G. .M(G) can be seen as a finite state control for a nondeterministic shift-reduce pushdown recognizer for G. A state transition labeled by a terminal symbol z from state s to state s' licenses a shift move, pushing onto the stack of the recognizer the pair (s, Ix).",59,60
13575,14963601,"In what follows, G is a fixed CFG with terminal vocabulary ~, nonterminal vocabulary N and start symbol S..M is the characteristic machine for G, with state set Q, start state so, final states F, and transition function 6 : S x (E U N) --~ S. As usual, transition functions such as 6 are extended from input symbols to input strings by defining 6(s, e) = s and 6(s, aft) = 6(6(s, a), fl).",19,20
13576,14963601,"The stack is a sequence of pairs (s, X) of a state and a terminal or nonterminal symbol.",20,21
13577,14963601,"The symbol "")"" appearing as the value of a feature in the right-hand side of a rule indicates that that feature must have the same value as the feature of the same name of the category in the left-hand side of the rule.",1,2
13578,14963601,declares cat as the start symbol of the grammar.,5,6
13579,14963601,"In the grammar rules, the symbol ""'"" prefixes terminal symbols, commas are used for sequencing and ""l"" for alternation.",6,7
13580,48940,"Then, for any a E ~, uav is in ~*~. Hence, uav is accepted by the automaton a, and, since ~ is deterministic, there exists a transition labeled with a leaving q. Thus, one can read any string u E E* using the automaton a. Since by definition of a, the state reached when reading a prefix u ~ of u is final iff u ~ E ~*~, by construction, the transducer r inserts the symbol # after the prefix u ~ iff u ~ ends with a pattern of ft.",86,87
13581,48940,"Instead of inserting a symbol, one can delete a symbol which would be necessarily present after each occurrence of a pattern of 8.",4,5
13582,48940,"Instead of inserting a symbol, one can delete a symbol which would be necessarily present after each occurrence of a pattern of 8.",10,11
13583,8413372,"Both of the predictive parsers employ one symbol of lookahead, incorporated into the parsing tables by the LALR technique.",7,8
13584,659645,"I° In cme mow~, depending on the state of the finite control, along with the symbols scanned by the input aml storage heads, the AFSA may do any (n' all of the following: • mow~ its ~t input heads independently c,n,:~ l)osil.iou to the right; • print a symbol on the coil scanned by the sLot'age head and (optionally) move that; head ont, l)osition to the right or loft.",58,59
13585,659645,"iff t.he machine emt move from state p to state q wlfile s(:antfin Z the n-tuplo cr from the input tapes and r from the current storage cell, and upon ente.ring state q, writes the symbol w onto the.",39,40
13586,63967455,"To present the notion of compatibility between a derivation and a bracketed string, we need first to define the span of a symbol occurrence in a context-free derivation.",23,24
13587,63967455,"Let (w,B) be a bracketed string, and c~0 ==~ al :=¢, ... =~ c~m = w be a derivation of w for (S)CFG G. The span of a symbol occurrence in (~1 is defined inductively as follows: • Ifj --m, c U = w E E*, and the span of wi in ~j is (i-1, i). •",42,43
13588,63967455,"Xk is a rule of G. Then the span of A in aj is (il,jk), where for each 1 < l < k, (iz,jt) is the span of Xl in aj+l-The spans in (~j of the symbol occurrences in/3 and 7 are the same as those of the corresponding symbols in ~j+l.",49,50
13589,63967455,"A derivation of w is then compatible with a bracketing B of w if the span of every symbol occurrence in the derivation is valid in B. GRAMMAR REESTIMATION The inside-outside algorithm (Baker, 1979 ) is a reestimation procedure for the rule probabilities of a Chomsky normal-form (CNF) SCFG.",18,19
13590,63967455,"In what follows, we will take N, ~ as fixed, n -IN[, t = [El, and assume enumerations N -{A1,... ,An} and E = {hi,... ,bt}, with A1 the grammar start symbol.",48,49
13591,63967455,"Finally, the probability of a sentence or sentential form is the sum of the probabilities of all its analyses (equivalently, the sum of the probabilities of all of its leftmost derivations from the start symbol).",37,38
13592,63967455,"More precisely, for each w E W, the inside probability I~ (i, j) estimates the likelihood that Ap derives iwj, while the outside probability O~(i, j) estimates the likelihood of deriving sentential form owi Ap j w from the start symbol A1.",48,49
13593,12928004,"A literal has a predicate symbol, and possibly some arguments (in parentheses, separated by commas), e.g. father(X,Y) false number(O) A literal is to be interpreted as denoting a relation between its arguments; e.g. ""father(X,Y)"" denotes the relation 'father' between X and Y. Arguments are terms, standing for partially specified objects.",5,6
13594,12928004,"The first symbol in s 1, the leading symbol, is restricted to be a non-terminal.",2,3
13595,12928004,"The first symbol in s 1, the leading symbol, is restricted to be a non-terminal.",9,10
13596,12928004,"Thinking procedurally, one can say that a nonterminal may be expanded by matching it to the leading symbol on the left-hand side of a rule, and the rest of the left-hand side is ""put aside"" to wait for the derivation of symbols which match each of its symbols in sequence.",18,19
13597,12928004,"Given an XG with initial symbol s, a sentence t is in the language defined by the XG if there is a se- quence of rule applications that transforms s into a string from which t can be obtained by deleting all brackets.",5,6
13598,12928004,"In a derivation graph, as in a parse tree, each node corresponds to a rule application or to a terminal symbol in the derived sentence, and the edges leaving a node correspond to the symbols in the right-hand side of that node's rule.",22,23
13599,12928004,"Of these edges, only the one corresponding to the leading symbol is used to define the left-to-right order of the symbols in the sentence whose derivation is represented by the graph.",11,12
13600,12928004,"In the rest of this paper, I often refer to a constituent being repositioned into a bracketed string (or into a fragment of derivation graph), to mean that a rule having that constituent as a non-leading symbol in the left-hand side has been applied, and the symbol matches some symbol in the string (or corresponds to some edge in the fragment).",42,43
13601,12928004,"In the rest of this paper, I often refer to a constituent being repositioned into a bracketed string (or into a fragment of derivation graph), to mean that a rule having that constituent as a non-leading symbol in the left-hand side has been applied, and the symbol matches some symbol in the string (or corresponds to some edge in the fragment).",55,56
13602,12928004,"In the rest of this paper, I often refer to a constituent being repositioned into a bracketed string (or into a fragment of derivation graph), to mean that a rule having that constituent as a non-leading symbol in the left-hand side has been applied, and the symbol matches some symbol in the string (or corresponds to some edge in the fragment).",58,59
13603,12928004,"Each element of the extraposition list represents a symbol being repositioned as a 4-tuple x(context, type, symbol, xlist) where context is either 'gap', if the symbol was preceded by '...' in the rule where it originated, or 'nogap', if the symbol was preceded by ','; type may be 'terminal' or 'nonterminal', with the obvious meaning; symbol is the symbol proper; xlist is the remainder of the extraposition list (an empty list being represented by '[ ]').",8,9
13604,12928004,"Each element of the extraposition list represents a symbol being repositioned as a 4-tuple x(context, type, symbol, xlist) where context is either 'gap', if the symbol was preceded by '...' in the rule where it originated, or 'nogap', if the symbol was preceded by ','; type may be 'terminal' or 'nonterminal', with the obvious meaning; symbol is the symbol proper; xlist is the remainder of the extraposition list (an empty list being represented by '[ ]').",20,21
13605,12928004,"Each element of the extraposition list represents a symbol being repositioned as a 4-tuple x(context, type, symbol, xlist) where context is either 'gap', if the symbol was preceded by '...' in the rule where it originated, or 'nogap', if the symbol was preceded by ','; type may be 'terminal' or 'nonterminal', with the obvious meaning; symbol is the symbol proper; xlist is the remainder of the extraposition list (an empty list being represented by '[ ]').",34,35
13606,12928004,"Each element of the extraposition list represents a symbol being repositioned as a 4-tuple x(context, type, symbol, xlist) where context is either 'gap', if the symbol was preceded by '...' in the rule where it originated, or 'nogap', if the symbol was preceded by ','; type may be 'terminal' or 'nonterminal', with the obvious meaning; symbol is the symbol proper; xlist is the remainder of the extraposition list (an empty list being represented by '[ ]').",55,56
13607,12928004,"Each element of the extraposition list represents a symbol being repositioned as a 4-tuple x(context, type, symbol, xlist) where context is either 'gap', if the symbol was preceded by '...' in the rule where it originated, or 'nogap', if the symbol was preceded by ','; type may be 'terminal' or 'nonterminal', with the obvious meaning; symbol is the symbol proper; xlist is the remainder of the extraposition list (an empty list being represented by '[ ]').",79,80
13608,12928004,"Each element of the extraposition list represents a symbol being repositioned as a 4-tuple x(context, type, symbol, xlist) where context is either 'gap', if the symbol was preceded by '...' in the rule where it originated, or 'nogap', if the symbol was preceded by ','; type may be 'terminal' or 'nonterminal', with the obvious meaning; symbol is the symbol proper; xlist is the remainder of the extraposition list (an empty list being represented by '[ ]').",82,83
13609,12928004,An XG rule is translated into a clause for the predicate corresponding to the leading symbol of the rule.,15,16
13610,12928004,"In the case where the XG rule has just a single symbol on the left-hand side, the translation is very similar to that of DCG rules.",11,12
13611,12928004,The translation of a rule with more than one symbol in the left-hand side is a bit more complicated.,9,10
13612,12928004,"Informally, each symbol after the first is made into a 4-tuple as described above, and fronted to the extraposition list.",3,4
13613,12928004,"where 'virtual(C,X0,X)', defined later, can be read as ""C is the constituent between X0 and X in the extraposition list"", and the variables Vi transfer the arguments of the symbol in the extraposition list to the predicate which translates that symbol.",39,40
13614,12928004,"where 'virtual(C,X0,X)', defined later, can be read as ""C is the constituent between X0 and X in the extraposition list"", and the variables Vi transfer the arguments of the symbol in the extraposition list to the predicate which translates that symbol.",50,51
13615,12928004,"The first clause for 'terminal' says that, provided the current extraposition list allows a gap to appear in the derivation, terminal symbol T may be taken from the position SO in the source string, where T connects SO to some new position S. The second clause for 'terminal' says that if the next symbol in the current extraposition list is a terminal T, then this symbol can be taken as if it occurred at S in the source string.",25,26
13616,12928004,"The first clause for 'terminal' says that, provided the current extraposition list allows a gap to appear in the derivation, terminal symbol T may be taken from the position SO in the source string, where T connects SO to some new position S. The second clause for 'terminal' says that if the next symbol in the current extraposition list is a terminal T, then this symbol can be taken as if it occurred at S in the source string.",60,61
13617,12928004,"The first clause for 'terminal' says that, provided the current extraposition list allows a gap to appear in the derivation, terminal symbol T may be taken from the position SO in the source string, where T connects SO to some new position S. The second clause for 'terminal' says that if the next symbol in the current extraposition list is a terminal T, then this symbol can be taken as if it occurred at S in the source string.",73,74
13618,12928004,"Symbols are placed in the extraposition list by rules with more than one symbol in the left-hand side, and removed by calls to 'virtual', on a first-in-last-out basis; that is, the extraposition list is a stack.",13,14
13619,12928004,"The symbol ' ' as a predicate or functor argument denotes an ""anonymous"" variable, i.e. each such occurrence stands for a separate variable with a single occurrence. %",1,2
13620,12928004,"Create the clause % Nt(S,S,XO,X) :-virtual(Nt,XO,X) % for extraposed symbol Nt virtual rule(Nt) :functor(Nt,F,N), functor(Y,F,N), tag(Y,S,S,Hx,Hy,P), ( clause(P,virtual( , , ), ), !;",22,23
13621,6604545,Results The system just described produces sharp and natural-sounding distinctions of intonation contour in minimal pairs of queries like the following: 14The ~ symbol separates syntactic categories from their corresponding prosodic categories and lexical items from their pitch/boundary markings.,26,27
13622,8626751,The absurdity symbol ⊥. 2.,2,3
13623,8626751,"A relation symbol A list of zero or more ordinary variable arguments of the relation (i.e., indices) This is written relation(arg 1 , . . . ,",2,3
13624,8626751,The absurdity symbol ⊥. 2.,2,3
13625,18592508,"for each qi E F and a E Q* such that aootqi is non-repeating and /f(q, S) = (qI, up, w) for some w where q is the last symbol in q0a.",38,39
13626,2616243,Shifting an input symbol from an inactive state is equivalent to skipping the words of the input that were encountered after the parser reached the inactive state and prior to the current word that is being shifted.,3,4
13627,2616243,Information about skipped words is maintained in the symbol nodes that represent parse sub-trees.,8,9
13628,2616243,Locally ambiguous symbol nodes are compared in terms of the words skipped within them.,2,3
13629,5090413,The ESL book measured as most and least difficult are indicated with a † and an asterisk ('*') symbol respectively.,22,23
13630,15988916,A phrase level with each symbol annotated with its feature structure is called an annotated phrase level (APL).,5,6
13631,2909838,"adopts a local activation processing model instead of relies upon symbol passing, as symbolic systems usually do.",10,11
13632,554709,"Analysis of Rhythmic Constraints We divide our analysis of the use of rhythm in Chinese phrases into two categories, based on two types of phrases in Chinese: (1) simple phrases, containing only words, i.e. all the child nodes are POS tag in the derivation tree; and (2) complex phrases in which at least one constituent is a phrase itself, i.e. it has at least one child node with phrase type symbol (like NP, VP) in its derivation tree.",80,81
13633,554709,"For example, in Goodman's probabilistic feature grammar (PFG), each symbol in a PCFG is replaced by a set of features, so it can describe specific constraints on the rule.",14,15
13634,3992766,"We base our experiments on a binary-valued symbol set E1 = {W, S} and on a ternary-valued symbol set E2 = {W, S, P}, where 'W' indicates weak stress, 'S' indicates strong stress, i (, Figure 2: A 5-gram model viewed as a first-order Markov chain and 'P' indicates a pause.",9,10
13635,3992766,"We base our experiments on a binary-valued symbol set E1 = {W, S} and on a ternary-valued symbol set E2 = {W, S, P}, where 'W' indicates weak stress, 'S' indicates strong stress, i (, Figure 2: A 5-gram model viewed as a first-order Markov chain and 'P' indicates a pause.",24,25
13636,3992766,"We wish to create a model that yields approximate values for probabilities of the form p(sklso, sl,..., Sk-1), where si E ~ is the stress symbol at syllable i in the text.",31,32
13637,3992766,"To gauge the regularity and compressibility of the training data we can calculate the entropy rate of the stochastic process as approximated by our model, an upper bound on the expected number of bits needed to encode each symbol in the best possible encoding.",39,40
13638,3992766,"To supplement the ~z binary vocabulary tests we ran the same experiments with ~2 = {0, 1, P}, in-troducing a pause symbol to examine how stress behaves near phrase boundaries.",28,29
13639,3992766,"Commas, dashes, semicolons, colons, ellipses, and all sentenceterminating punctuation in the text, which were removed in the E1 tests, were mapped to a single pause symbol for E~. Pauses in the text arise not only from semantic constraints but also from physiological limitations.",32,33
13640,3992766,"Expectedly, adding the symbol increases the confusion and hence the entropy, but the rates remain less than a bit.",4,5
13641,13510104,"exists (Xl, farmer (X i) &talk (X3)) Rforall (X2, senator (X2) =>talk (X3)) We therefore need to use distinct variables in place of D for the two range constraints which will have the same predicate symbol for their range categories.",52,53
13642,10386635,"Additionally, the symbol ""*"" now denotes Kleene star as applied to the alphabet of the respective tier.",3,4
13643,10386635,"Rule R1 sanctions root consonants by mapping a [c] from the first (pattern) sublexicon, a consonant [X] from the second (root) sublexicon, and no symbol from the third (vocalism) sublexicon to surface [X].",34,35
13644,10386635,The rule basically states that any symbol not fern.,6,7
13645,10386635,"The compilation process is preceded by a preprocessing stage during which all mappings of unequal lengths are made same-length mappings by inserting a special symbol, 0, when necessary. (",26,27
13646,10386635,"The grammar writer need not worry about this special symbol, but cannot use it in the grammar.)",9,10
13647,10386635,"The symbol s denotes the partition symbol ¢. The surface symbol appears to the left of "":"" and the lexical tuple to its right.",1,2
13648,10386635,"The symbol s denotes the partition symbol ¢. The surface symbol appears to the left of "":"" and the lexical tuple to its right.",6,7
13649,10386635,"The symbol s denotes the partition symbol ¢. The surface symbol appears to the left of "":"" and the lexical tuple to its right.",10,11
13650,10386635,"Further, let ¢ be a special symbol (not in the grammar's alphabet) to denote a subsequence boundary within a partition, and let or' = Tdn(Cr ).",7,8
13651,10386635,"The first condition is accomplished by simply placing er I to the left and right of c. As for the second condition, an auxiliary symbol co is used as a placeholder representing c in order to avoid inserting ¢' within the tuples of c by Insert.",25,26
13652,10386635,The last step in compiling rules is to remove all instances of the symbol ~ and the symbol 0.,13,14
13653,10386635,The last step in compiling rules is to remove all instances of the symbol ~ and the symbol 0.,17,18
13654,10386635,"For example, the following rule, which uses prosodic templates, demonstrates the intrasyllabic spreading of vowels: Intrasyllabic spreading: (a,a, C, V) CVV Extrametricality: {ax, C, e} ¢~ C The symbol a~, above is a templatic segment denoting a bimoraic syllable.",44,45
13655,10386635,"The c in the right lexical context is a concrete symbol from a pattern morpheme, while V represents the class of all vowels.",10,11
13656,10386635,"Further, let si denote the symbol on the ith tape.",6,7
13657,10386635,"Transitions are marked with quadruples of elements (for vocalism, root, pattern, and surface form, respectively), where each element is a pair: a symbol and an instruction concerning the movement of the tape's head.",30,31
13658,10386635,Kay uses the following notation: An unadorned symbol is read and the tape's head moves to the next position.,8,9
13659,10386635,"A symbol in brackets, [ ], is read and the tape's head remains stationary• A symbol in braces, { }, is read and the tape's head moves only if the symbol is the last one on the tape.",1,2
13660,10386635,"A symbol in brackets, [ ], is read and the tape's head remains stationary• A symbol in braces, { }, is read and the tape's head moves only if the symbol is the last one on the tape.",18,19
13661,10386635,"A symbol in brackets, [ ], is read and the tape's head remains stationary• A symbol in braces, { }, is read and the tape's head moves only if the symbol is the last one on the tape.",36,37
13662,10386635,"After the first transition on the quadruple {[ ], k, C, k} in Figure 9 (a): no symbol is read from the vocalism tape, [k] is read from the root tape and the tape's head is moved, [C] is read from the pattern tape and the tape's head is moved, and [k] is written on the surface tape and the tape's head is moved• At the final configuration, all the tapes have been exhausted.",24,25
13663,10386635,"Kay makes use of a special symbol, G, to handle gemination; when read, a symbol from the root tape is scanned without advancing the read head of that tape.",6,7
13664,10386635,"Kay makes use of a special symbol, G, to handle gemination; when read, a symbol from the root tape is scanned without advancing the read head of that tape.",18,19
13665,10386635,"The symbol ""~"" between the lower surface tape and the lexical tapes indicates the current symbols under the read/write heads.",1,2
13666,10386635,The symbol X in expression (15) indicates gemination in a way reminiscent of Kay's G symbol.,1,2
13667,10386635,The symbol X in expression (15) indicates gemination in a way reminiscent of Kay's G symbol.,18,19
13668,10386635,"The lexical description gives the root and pattern superficially concatenated in the form (Beesley 1996, p. c.) : [ktb&CaCaC] (17) The square brackets are special symbols that delimit the stem, and ""&"" is another special symbol that separates the root from the pattern; it is not the intersection operator.",45,46
13669,10386635,"As ""&"" in ( 17 ) and ( 18 ) is a concrete symbol, no real intersection, in the settheoretic sense, takes place, though Beesley refers to this method as well as to the bracketing mechanism described in the previous section as ""intersection"".)",15,16
13670,10386635,"After modifications to Beesley's version--such as intersecting the disjunction of roots with the disjunction of patterns once, 5 delimiting root consonants with one special symbol only, and inserting other symbols only between root consonants rather than at arbitrary positions so that a typical root now has the shape [sym*, <, k, sym*, <, t, sym*, <, b, sym*], where syms expands to (?",28,29
13671,10386635,"While the pattern database is shared between the two implementations, the meaning of ' C' within the patterns is different: for Kiraz, it is just an ordinary symbol, but for Beesley it expands into a marker for a root consonant followed by any other symbol (the root consonant itself).",31,32
13672,10386635,"While the pattern database is shared between the two implementations, the meaning of ' C' within the patterns is different: for Kiraz, it is just an ordinary symbol, but for Beesley it expands into a marker for a root consonant followed by any other symbol (the root consonant itself).",49,50
13673,5093281,"In this fashion each sentence was broken down into a set of such category-patterns, resulting in a set of different category-patterns for each punctuation symbol, which were then processed to extract the underlying rule patterns which represent all the ways that punctuation behaves in this corpus, and are good indicators of how the punctuation marks might behave in the rest of language.",29,30
13674,696805,"To present the notion of compatibility between a derivation and a bracketed string, we need first to define the span of a symbol occurrence in a context-free derivation.",23,24
13675,696805,"Let (w, B) be a bracketed string, and a0 ~ al =::> ... am = w be a derivation of w for (S)CFG G. The span of a symbol occurrence in aj is defined inductively as follows: • Ifj = m, aj = w E E*, and the span ofwi in aj is (i -1, i). •",35,36
13676,696805,The spans in aj of the symbol occurrences in/~ and 7 are the same as those of the corresponding symbols in otj+t.,6,7
13677,696805,A derivation of w is then compatible with a bracketing B of w if no span of a symbol occurrence in the derivation overlaps a span in B. THE INSIDE-OUTSIDE ALGORITHM The inside-outside algorithm [2] is a reestimation procedure for the rule probabilities of a Chomsky normal-form (CNF) SCFG.,18,19
13678,696805,"In what follows, we will take N, E as fixed, n = [NI, t = I~1, and assume enumerations N = {A1,...,An} and E = {bl,...,bt}, with A1 the grammar start symbol.",46,47
13679,696805,"Finally, the probability of a sentence or sentential form is the sum of the probabilities of all its analyses (equivalently, the sum of the probabilities of all of its leftmost derivations from the start symbol).",37,38
13680,696805,"In the original inside-outside algorithm, for each tv E W, the inside probability I~(i,j) estimates the likelihood that Ap derives iwj, while the outside probability O~(i,j) estimates the likelihood of deriving sentential form owi Apjw from the start symbol A1.",49,50
13681,8586936,"symbols, NT the set of non-terminal symbols, P a set of production rules, S the start symbol.",21,22
13682,8586936,The goto tables are accessed by a state and a nonterminal symbol.,11,12
13683,8586936,The action table is accessed by a state and a terminal symbol.,11,12
13684,8586936,"The parsing actions for state si are determined for all terminal symbols a E ~ as follows: (i) for all r e gotok(si,a), add ksh(r) to ACTION(si, a); (ii) for all r E goto, k(si,a), add nksh(r) to to ACTION(si, a); (iii) if A --* a* is in si, then add red(A--* a) to ACTION(si, a) for all terminal symbol a and for the end marker $.",90,91
13685,8586936,"If an arc from 51 to 52 is labeled by a non-sharped symbol X, then s2 is in gotot(Sl,X).",14,15
13686,8586936,"If an arc from sl to 52 is labeled by a sharped symbol X~, then 52 is in gotont(Sx, X).",12,13
13687,10037247,"The symbol ""−"" is used for referents which do not occur in the sentence.",1,2
13688,10037247,"Referents The entity grid is a two-dimensional array that captures the distribution of NP referents across sentences in the text using the aforementioned symbols for their grammatical role and the symbol ""−"" for a referent that does not occur in a sentence.",32,33
13689,8891611,"Semantic Decoding The RMT grammar is represented, according to a context free formalism, by a set of 990 sentence generation templates of the form: Sj = ~ ai2 ...a~, (1) where a generic ~ may be either a terminal symbol, hence a word belonging to the 991 word vocabulary and identified by its orthographic transcription, or a non-terminal symbol (represented by sharp parentheses in the rest of the paper).",46,47
13690,8891611,"Semantic Decoding The RMT grammar is represented, according to a context free formalism, by a set of 990 sentence generation templates of the form: Sj = ~ ai2 ...a~, (1) where a generic ~ may be either a terminal symbol, hence a word belonging to the 991 word vocabulary and identified by its orthographic transcription, or a non-terminal symbol (represented by sharp parentheses in the rest of the paper).",69,70
13691,8891611,Two examples of sentence generation templates and the corresponding production of non-terminal symbols are given in Table 1 in which the symbol e corresponds to the empty string.,23,24
13692,8891611,The category is then identified with the symbol used to represent the non-terminal on the left hand side of the production.,7,8
13693,13678634,"Instead of replacing nonterminal symbols in a derivation with strings from the righthand side of corresponding rules, we would remove the nonterminal symbol and insert the symbols from the righthand side of the rule at arbitrary places in the string.",23,24
13694,7492118,"Label' is an atomic symbol which labels the arc, and 'value' is a pointer to a node.",5,6
13695,7492118,"atomic type nodes represent atomic symbol values (such as Noun), :bottom type nodes are variables and :complex type nodes are nodes that have arcs coming out of them.",5,6
13696,10254810,"Hash symbol: set to 1 if a word contains the symbol '#', otherwise 0.",1,2
13697,10254810,"Hash symbol: set to 1 if a word contains the symbol '#', otherwise 0.",11,12
13698,10254810,Rate symbol: set to 1 if the current word contains the symbol '@'.,1,2
13699,10254810,Rate symbol: set to 1 if the current word contains the symbol '@'.,12,13
13700,9228040,"Feature descriptions are interpreted over feature trees as one would expect: • Every sort symbol A is taken as a unary predicate, where a sort constraint A(x) holds if and only if the root of the tree x is labeled with A. • Every feature symbol f is taken as a binary predicate, where a feature constraint f(x,y) holds if and only if the tree x has the direct subtree y at feature f. The theory of the corresponding first-order structure (i.e., the set of all closed formulae valid in this structure) is called FT.",15,16
13701,9228040,"Feature descriptions are interpreted over feature trees as one would expect: • Every sort symbol A is taken as a unary predicate, where a sort constraint A(x) holds if and only if the root of the tree x is labeled with A. • Every feature symbol f is taken as a binary predicate, where a feature constraint f(x,y) holds if and only if the tree x has the direct subtree y at feature f. The theory of the corresponding first-order structure (i.e., the set of all closed formulae valid in this structure) is called FT.",48,49
13702,9228040,"The symbol c denotes the empty path, which satisfies cp = p = pc for every path p. A path p is called a prefix of a path q, if there exists a path p' such that pp' = q. We also assume an infinite alphabet of variables and adopt the convention that x, y, z always denote variables, and X, Y always denote finite, possibly empty sets of variables.",1,2
13703,6323862,"A slot value could be either a variable, indicated by a symbol enclosed in <..> (e.g. <NPS>), or a constant (e.g. how).",12,13
13704,5655307,"NSFS unification, written with the symbol rlN, is associative and commutative.",6,7
13705,9949485,"The particular task we examine is one of ""pseudo name disambiguation,"" in which the texts containing matched pairs of different names are extracted, and then the two different names are replaced by a single symbol, leading to an ambiguous ""name"" across the two sets of texts.",38,39
13706,14329451,"If P is a symbol for an n-place relation, x 1 , , xn are discourse markers, then pred(P, x l , , xn) is a LUD-condition; 3.",4,5
13707,856537,DETERMIN-ISTIC finite automata without e-moves which consume exactly one input symbol at a time.,14,15
13708,856537,"The first one represents the non-final states Q \ F. Because we assume that exactly one input symbol is consumed every time an edge is taken, we are allowed to separate the input list into the first element and the rest list in order to structure-share the first element with EDGE (the consumed input symbol) and to pass the rest list one level deeper to the next state. (",19,20
13709,856537,"The first one represents the non-final states Q \ F. Because we assume that exactly one input symbol is consumed every time an edge is taken, we are allowed to separate the input list into the first element and the rest list in order to structure-share the first element with EDGE (the consumed input symbol) and to pass the rest list one level deeper to the next state. (",60,61
13710,856537,"FDL Specification of Two-Level Morphology Two-level descriptions of allomorphy can be specified in FDLs straightforwardly if we model not transducers, but rather two-level acceptors (of strings of symbol pairs), following Ritchie et al.",35,36
13711,856537,NORPH]FORM [~] ] (18) allomorphy =_ INPUT [] Rules of the sort found in ( 14 ) can be directly compiled into FA acceptors over strings of symbol pairs (Ritchie et al.,33,34
13712,2518391,An utterance uniL instead of a sentence as in Gral-J is used as a grammatical category and is taken as the start symbol.,24,25
13713,2518391,"A GRAMMAR, FOR SPONTANEOUS SPEECH '['his section describes Grass-Z 4.1 Processing Units 'Sentence' is used as the start symbol in granunars for written languages but sentence boundaries are not clear in spontaneous speech. ;",26,27
13714,2518391,Sentence' therefore can not be used as the start symbol in grammars lbr spontaneous speech.,10,11
13715,1043632,The symbol NP in Figure 2 indicates boundaries assigned by the algorithm.,1,2
13716,15580189,"Observe that the vector in (3) contains exactly one terminal symbol (the verb); grammars in which every elementary structure (vector in UVG, tree in TAG, rule in CFG) contains at least one terminal symbol we will call lexicalized.",12,13
13717,15580189,"Observe that the vector in (3) contains exactly one terminal symbol (the verb); grammars in which every elementary structure (vector in UVG, tree in TAG, rule in CFG) contains at least one terminal symbol we will call lexicalized.",42,43
13718,15580189,"In order to account for the index multiset, we let the entries in the recognition matrix be pairs consisting of a nonterminal symbol and a [Y}l-tuple of integers: (A, (nl,..., nlv, I)) The IVil-tuple of integers represents a multiset, with each integer designating the number of copies of a given index symbol that the set contains.",23,24
13719,15580189,"In order to account for the index multiset, we let the entries in the recognition matrix be pairs consisting of a nonterminal symbol and a [Y}l-tuple of integers: (A, (nl,..., nlv, I)) The IVil-tuple of integers represents a multiset, with each integer designating the number of copies of a given index symbol that the set contains.",68,69
13720,15580189,"More precisely, if the input word is al .-.an, and if ~ = {il,...,ilv, I}, then we have (A, (nl,...,nlvd)) in entry ti,j of the recognition matrix if and only if there is a derivation As ::=¢. ai+l ...aj, where multiset s contains nk copies of index symbol it,, 1 < k < I vii.",69,70
13721,15580189,"Definition 5 An Unordered Vector Grammar with Dominance Links (UVG-DL) is a 4-tuple (VN, VT, V, S), where VN and VT are sets of nonterminals and terminals, respectively, S is the start symbol, and V is a set of vectors of context-free productions equipped with dominance links.",46,47
13722,15580189,"For a given vector v E V, the dominance links form a binary relation domv over the set of occurrences of non-terminals in the productions of v such that if domv(A, B), then A (an instance of a symbol) occurs in the right-hand side of some production in v, and B is the left-hand symbol (instance) of some production in v. IfG is a UVG-DL, L(G) consists of all words w E VYt which have a derivation p of the form such that ~ meets the following two conditions: 1.",45,46
13723,15580189,"For a given vector v E V, the dominance links form a binary relation domv over the set of occurrences of non-terminals in the productions of v such that if domv(A, B), then A (an instance of a symbol) occurs in the right-hand side of some production in v, and B is the left-hand symbol (instance) of some production in v. IfG is a UVG-DL, L(G) consists of all words w E VYt which have a derivation p of the form such that ~ meets the following two conditions: 1.",67,68
13724,6534419,"2) TwO-LEVEL FORMALISM LLC - LEX RLC LSC -SURF -RSC where LLC LEX RLC LSC SURF RSC = left lexical context = lexical form = right lexical context = left surface context = surface form = right surface context The special symbol * is a wildcard matching any context, with no length restrictions.",44,45
13725,6534419,"3) ERROR FORMALISM ErrSurf =~ Surf { PLC-PRC } where PLC = partition left context (has been done) PRC = partition right context (yet to be done) 5Our implementation interprets rules directly; hence, we allow ~. If the rules were to be compiled into automata, a genuine symbol, e.g. 0, must be used.",58,59
13726,6534419,Each lexical expression is a triple; lexical expressions with one symbol assume e on the remaining positions.,11,12
13727,3263890,"The current criteria are: • The left bigram score: the probability of correctness of an edge considering only the following data about it: -its tag (corresponding to its major category symbol plus, for a few categories, some ad-ditional distinctions derived from feature values); -for a lexical edge, its word or semantic word class (words with similar distributions, such as city names, are grouped into classes to overcome data sparseness); or for a phrasal edge, the name of the final (topmost) grammar rule that was used to create it; -the tag of a neighbouring edge immediately to its left.",34,35
13728,10499055,We use the symbol Pr(•) to denote general probability distribution and p(•) to denote model-based probability distribution.,3,4
13729,1566285,"The transducer is deterministic, that is, there is only one arc leaving a given state for each input symbol.",20,21
13730,1566285,"Each time a transition is made, exactly one symbol of the input string is consumed.",9,10
13731,1566285,A unique end of string symbol is introduced.,5,6
13732,1566285,"At the end of each input string, the transducer makes an additional transition on the end of string symbol.",19,20
13733,1566285,"Perhaps worse, it will fail completely upon seeing any symbol other than 'er' or end-of-string after a 't'.",10,11
13734,1566285,The decision trees describe the behavior of the machine at a given state in terms of the next input symbol by generalizing from the arcs leaving the state.,19,20
13735,1566285,The decision trees classify the arcs leaving each state based on the arc's input symbol into groups with the same behavior.,15,16
13736,1566285,"Thus the branches of the decision tree are labeled with phonetic feature values of the arc's input symbol, and the leaves of the tree correspond to the different behaviors.",18,19
13737,1566285,"Two arcs are considered to have the same behavior if they agree each of the following: • the index i of the output symbol corresponding to the input symbol (determined from the alignment procedure) • the difference of the phonetic feature vectors of the input symbol and symbol i of the output string • the suffix of the output string beginning at position i+1 • the destination state After the process of merging states terminates, a decision tree is induced at each state to classify the outgoing arcs.",24,25
13738,1566285,"Two arcs are considered to have the same behavior if they agree each of the following: • the index i of the output symbol corresponding to the input symbol (determined from the alignment procedure) • the difference of the phonetic feature vectors of the input symbol and symbol i of the output string • the suffix of the output string beginning at position i+1 • the destination state After the process of merging states terminates, a decision tree is induced at each state to classify the outgoing arcs.",29,30
13739,1566285,"Two arcs are considered to have the same behavior if they agree each of the following: • the index i of the output symbol corresponding to the input symbol (determined from the alignment procedure) • the difference of the phonetic feature vectors of the input symbol and symbol i of the output string • the suffix of the output string beginning at position i+1 • the destination state After the process of merging states terminates, a decision tree is induced at each state to classify the outgoing arcs.",48,49
13740,1566285,"Two arcs are considered to have the same behavior if they agree each of the following: • the index i of the output symbol corresponding to the input symbol (determined from the alignment procedure) • the difference of the phonetic feature vectors of the input symbol and symbol i of the output string • the suffix of the output string beginning at position i+1 • the destination state After the process of merging states terminates, a decision tree is induced at each state to classify the outgoing arcs.",50,51
13741,1045826,"We develop several versions of conditional replacement that allow the operation to be constrained by context O. Introduction Linguistic descriptions in phonology, morphology, and syntax typically make use of an operation that replaces some symbol or sequence of symbols by another sequence or symbol.",36,37
13742,1045826,"We develop several versions of conditional replacement that allow the operation to be constrained by context O. Introduction Linguistic descriptions in phonology, morphology, and syntax typically make use of an operation that replaces some symbol or sequence of symbols by another sequence or symbol.",45,46
13743,1045826,"One is to define replacement in a very general way, explicitly allowing replacement to be constrained by input and output contexts, as in two-level rules (Koskenniemi 1983) , but without the restriction of only single-symbol replacements.",42,43
13744,1045826,"An fst pair a : b can be thought of as the crossproduct of a and b, the minimal relation consisting of a (the upper symbol) and b (the lower symbol).",27,28
13745,1045826,"An fst pair a : b can be thought of as the crossproduct of a and b, the minimal relation consisting of a (the upper symbol) and b (the lower symbol).",34,35
13746,1045826,any symbol in the known alphabet and its extensions.,1,2
13747,1045826,"Thus %& denotes a literal ampersand as opposed to &, the intersection operator; %0 is the ordinary zero symbol.",22,23
13748,1045826,"For convenience we represent identity pairs by a single symbol; for example, we write a : a as a. The symbol ?",9,10
13749,1045826,"For convenience we represent identity pairs by a single symbol; for example, we write a : a as a. The symbol ?",22,23
13750,1045826,"In our regular expression language, we have to prefix the auxiliary context markers with the escape symbol % to distinguish them from other uses of < and >. [",17,18
13751,1045826,But the two-level formalism is only defined for symbol-tosymbol replacements.,10,11
13752,16273847," Alternatively, we can assume that N is the only multivalued relation symbol while both SUBCAT and C-DTRS are single-valued and then employ the intuitively appealing subcategorisation principle given in ( 4 ). (",13,14
13753,16273847,Let f be some relation symbol.,5,6
13754,3265939,Another important point is that each decision tree considered here has the property that its predictions specify how to rewrite a symbol (in context) in an input string.,21,22
13755,3265939,"The compiled transducer corresponding to that rule will replace ¢ with ¢ with the appropriate weights in the context A p. The Tree Compilation Algorithm The key requirements on the kind of decision trees that we can compile into WFSTs are (1) the predictions at the leaf nodes specify how to rewrite a particular symbol in an input string, and ( 2 ) the decisions at each node are stateable as regular expressions over the input string.",56,57
13756,3265939,"Assuming that we have a symbol a representing a single segment, the symbol # representing a word boundary, and allowing for the possibility of intervening optional stress marks ~ which do not count as segments, these two possibilities can be represented by the regular expressions for A in (a) of Table 3 .",5,6
13757,3265939,"Assuming that we have a symbol a representing a single segment, the symbol # representing a word boundary, and allowing for the possibility of intervening optional stress marks ~ which do not count as segments, these two possibilities can be represented by the regular expressions for A in (a) of Table 3 .",13,14
13758,3265939,"Jah2.6sU ax2.s4)/#Opt(') Opt(')(alv) By a similar construction, the rule at node 6, for example, would be represented as: Each node thus represents a rule which states that a mapping occurs between the input symbol ¢ and the weighted expression ¢ in the condition described by A p. Now, in cases where ¢ finds itself in a context that is not subsumed by A p, the rule behaves exactly as a two-level surface coercion rule (Koskenniemi, 1983) : it freely allows ¢ to correspond to any ¢ as specified by the alphabet of pairs.",41,42
13759,3265939,"The WFST for a single leaf L is thus defined as follows, where CT is the input symbol for the entire tree, eL is the output expression defined at L, t95 represents the path traversed from the root node to L, p is an individual branch on 4One can thus define intersection for transducers analogously with intersection for acceptors.",18,19
13760,3265939,Note that the entire alphabet comprises 215 symbol pairs.,7,8
13761,256905,"Definition A TAG G is a 5-tuple (VN, Vv, S,I,A) where • VN is a finite set of nonterminal symbols, • VT is a finite set of terminal symbols, • S is a distinguished nonterminal symbol the start symbol, • I is a set of initial trees, • A is a set of auxiliary trees.",47,48
13762,256905,"Definition A TAG G is a 5-tuple (VN, Vv, S,I,A) where • VN is a finite set of nonterminal symbols, • VT is a finite set of terminal symbols, • S is a distinguished nonterminal symbol the start symbol, • I is a set of initial trees, • A is a set of auxiliary trees.",50,51
13763,256905,"The language L(G) generated by a TAG G is the set of strings, i.e., sequences of leaves of trees in T(G) when the leaves of these trees are only labeled with terminal nodes, and whose root is the start symbol.",44,45
13764,256905,"We can define the language accepted by such a calculus as follows: Let us take only those sequents in CL(M(Gt)) whose right part is the propositional variable S (the start symbol of the grammar), and such that propositional variables of the left part of the sequent correspond to terminal symbols of the grammar, i.e., words of the language.",34,35
13765,5448554,"Following (Vijay-Shanker and Weir, 1994) Definition 2 L = (VN,VT,VI,PL,S) denotes a LIG where VN, VT, VI and PL are respectively finite sets of non-terminals, terminals, stack symbols and productions, and S is the start symbol.",57,58
13766,5448554,"In (Boullier, 1995) • VTD = pL • S ° = [S] • Below, [F1F2] symbol [X] when FIF2 = string e when F1F2 • V~. being denotes either the non-terminal X 0 or the empty po is defined as {[A] -+ r 0 I rO = AO -~ w • PL} (1) U{[A] -+ r0[A +-~ B]I r 0 = B 0 -+ w • PL} (2) UI[A +~-C] ~ [rlr~]r0 I r 0 = A(..) ~ r,c(..)r: • PL} (3) u{[A +-~ C] --+ [A ~ C]} (4) u{[A c] [B c][rlr:lr0 I r0 = AC) rls(..)r2 • PL} (5) (6) U{[A +-~ C] -> [B ~ C][A ~ B]} U{[A ~ C] ~ [B ~-c][rlr2]r0 I + r 0 = A(..) ~ rlB(..~)r2 • PL} (7) 5Though in the referred paper, these relations are defined on constituents, the algorithm also applies to nonterminals.",23,24
13767,5448554,"This checking can result in some subtree (production) deletions, namely the ones for which there is no valid symbol stack evaluation.",21,22
13768,5448554,One way is to consider a top-down strategy: the Xproductions in a LDG are generated iff X is the start symbol or occurs in the RHS of an already generated production.,23,24
13769,5448554,"Assume that the symbol S-+ Sa S-~ Sb S-+ S c S-~ T T-}aT T -+ bT T -~ cT T -+ c defines the language £(G) = {wcw ' I w,w' 6 {a, b, c]*}.",3,4
13770,5448554,"We note that in L the key part is played by the middle c, introduced by production rs0, and that this grammar is non ambiguous, while in G the symbol c, introduced by the last production T ~ c, is only a separator between w and w' and that this grammar is ambiguous (any occurrence of c may be this separator).",32,33
13771,5448554,"Second Example The following LIG L, where A is the start symbol: We can easily checked that this grammar is reduced.",12,13
13772,5448554,"When a symbol 3' is pushed on a stack at a given index at some place, this very symbol must be popped some place else, and we know that such (recursive) pairing is the essence of context-freeness.",2,3
13773,5448554,"When a symbol 3' is pushed on a stack at a given index at some place, this very symbol must be popped some place else, and we know that such (recursive) pairing is the essence of context-freeness.",20,21
13774,5448554,For this LIGed forest the relations are: The start symbol of the LDG associated with the LIGed forest L * is [[S]o3].,10,11
13775,333410,"A context-free grammar (CFG) is a tuple G = (VN, VT, P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, and S E VN is the start symbol.",47,48
13776,333410,"A[a] ~ B[b] C[a] (1) • A[a] --+ C[a] B[b] (2) • A[a] ~ a (3) Thus every nonterminal is lexicalized at some terminal a. A constituent of nonterminal type A[a] is said to have terminal symbol a as its lexical head, ""inherited"" from the constituent's head child in the parse tree (e.g., C[a]).",50,51
13777,333410,"Notice that the start symbol is necessarily a lexicalized nonterminal, T [$] .",4,5
13778,333410,A special symbol $ E VT plays the role of start symbol.,2,3
13779,333410,A special symbol $ E VT plays the role of start symbol.,11,12
13780,333410,"A single head automaton is an acceptor for a language of string pairs (z~, Zr) E V~ x V~. Informally, if b is the leftmost symbol of Zr and q~ E 5a(q, b, -~), then Ha can move from state q to state q~, matching symbol b and removing it from the left end of Zr.",29,30
13781,333410,"A single head automaton is an acceptor for a language of string pairs (z~, Zr) E V~ x V~. Informally, if b is the leftmost symbol of Zr and q~ E 5a(q, b, -~), then Ha can move from state q to state q~, matching symbol b and removing it from the left end of Zr.",54,55
13782,333410,"Symmetrically, if b is the rightmost symbol of zl and ql E 5a(q, b, ~---) then from q Ha can move to q~, matching symbol b and removing it from the right end of zl.5 More formally, we associate with the head automaton Ha a ""derives"" relation F-a, defined as a binary relation on Qa × V~ x V~. For every q E Q, x,y E V~, b E VT, d E D, and q' E ~a(q, b, d), we specify that (q, xb, y) ~-a (q',x,Y) if d =+-; (q, x, by) ~-a (q', x, y) if d =--+.",7,8
13783,333410,"Symmetrically, if b is the rightmost symbol of zl and ql E 5a(q, b, ~---) then from q Ha can move to q~, matching symbol b and removing it from the right end of zl.5 More formally, we associate with the head automaton Ha a ""derives"" relation F-a, defined as a binary relation on Qa × V~ x V~. For every q E Q, x,y E V~, b E VT, d E D, and q' E ~a(q, b, d), we specify that (q, xb, y) ~-a (q',x,Y) if d =+-; (q, x, by) ~-a (q', x, y) if d =--+.",29,30
13784,333410,"If the HAs in H happen to be deterministic, then in each binary production given by (ii) above, symbol A is fully determined by a, b, and C. In this case p = O(t2), so the parser will operate in time O(n4t2).",22,23
13785,333410,"The special symbol F also appears as a literal in some items, and effectively means ""an unspecified final state.""",2,3
13786,34048,"Each arc has an input symbol and an output symbol, separated by a colon.",5,6
13787,34048,"Each arc has an input symbol and an output symbol, separated by a colon.",9,10
13788,34048,"A single symbol (such as t or V) is a shorthand for a symbol that is the same in the input and output (i.e., t:t or V:V).",2,3
13789,34048,"A single symbol (such as t or V) is a shorthand for a symbol that is the same in the input and output (i.e., t:t or V:V).",15,16
13790,34048,Either the input or the output symbols can be null; a null input symbol is used for an insertion of a phone; a null output symbol for a deletion.,14,15
13791,34048,Either the input or the output symbols can be null; a null input symbol is used for an insertion of a phone; a null output symbol for a deletion.,27,28
13792,34048,Table 1 shows our phone set--an ASCII symbol set based on the ARPA-sponsored ARPAbet alphabet--with the IPA equivalents.,9,10
13793,34048,Labels on arcs are of the form (input symbol):(output symbol).,10,11
13794,34048,"The transducer is deterministic, that is, there is only one arc leaving a given state for each input symbol.",20,21
13795,34048,"Each time a transition is made, exactly one symbol of the input string is consumed.",9,10
13796,34048,A unique end-of-string symbol is introduced.,7,8
13797,34048,"At the end of each input string, the transducer makes an additional transition on the end-of-string symbol.",21,22
13798,34048,The second property is merely a convention; any transducer with multiple input symbols on an arc can easily be transformed into one with single arcs with one symbol each.,28,29
13799,34048,The introduction of an end-of-string symbol serves to expand the range of functions that can be represented.,9,10
13800,34048,"The most significant difference between our subsequential transducers and twolevel models is that the two-level transducers described by Karttunen (1993) \ Flapping about to occur Figure 2 Subsequential transducer for English flapping; ""#"" is the end-of-string symbol.",47,48
13801,34048,"In addition, Karttunen's transducers may have only zero or one symbol as either the input or output of an arc, and they have no special end-of-string symbol.",12,13
13802,34048,"In addition, Karttunen's transducers may have only zero or one symbol as either the input or output of an arc, and they have no special end-of-string symbol.",33,34
13803,34048,"These representational differences between the two formalisms lead to different ways of handling certain classes of phonological rules, particularly those that depend on the context to the right of the affected symbol.",32,33
13804,34048,The subsequential transducer does not emit any output until enough of the right-hand context has been seen to determine how the input symbol is to be realized.,24,25
13805,34048,"Rather, the machine goes to state 2 and waits to see if the next input symbol is the requisite unstressed vowel; depending on this next input symbol, the machine will emit the t or a dx along with the next input symbol when it makes the transition from state 2 to state 0.",16,17
13806,34048,"Rather, the machine goes to state 2 and waits to see if the next input symbol is the requisite unstressed vowel; depending on this next input symbol, the machine will emit the t or a dx along with the next input symbol when it makes the transition from state 2 to state 0.",28,29
13807,34048,"Rather, the machine goes to state 2 and waits to see if the next input symbol is the requisite unstressed vowel; depending on this next input symbol, the machine will emit the t or a dx along with the next input symbol when it makes the transition from state 2 to state 0.",44,45
13808,34048,"If the machine takes the wrong transition, the subsequent transitions will leave the transducer in a non-accepting state, or a state will be reached with no transition on the current input symbol.",35,36
13809,34048,"The algorithm begins by constructing a tree transducer that covers all the training samples according to the following procedure: for each input pair, the algorithm walks from the initial state taking one transition on each input symbol, as if doing a transduction.",38,39
13810,34048,"When there is no move on the next input symbol from the present state, a new branch is grown on the tree.",9,10
13811,34048,"The entire output string of each transduction is initially stored as the output on the last arc of the transduction, that is, the arc corresponding to the end-of-string symbol.",34,35
13812,34048,A conflict arises whenever two states are merged that have outgoing arcs with the same input symbol.,16,17
13813,34048,"For example, the transducer of Figure 8 will fail completely upon seeing any symbol other than er or end-of-string after a t. Of course this transducer is only trained on three samples, but the same problem occurs with transducers trained on large corpora.",14,15
13814,34048,"When building the initial tree transducer, the alignment is used to ensure that no output symbol appears on an arc further up the tree than the corresponding input symbol.",16,17
13815,34048,"When building the initial tree transducer, the alignment is used to ensure that no output symbol appears on an arc further up the tree than the corresponding input symbol.",29,30
13816,34048,"For the theoretical property of language identification in the limit, we must be guaranteed that the alignments used are correct: that is, the alignment must not show an output symbol to correspond to an input symbol that comes after the input symbol that, in the target transducer, generates the output symbol.",32,33
13817,34048,"For the theoretical property of language identification in the limit, we must be guaranteed that the alignments used are correct: that is, the alignment must not show an output symbol to correspond to an input symbol that comes after the input symbol that, in the target transducer, generates the output symbol.",38,39
13818,34048,"For the theoretical property of language identification in the limit, we must be guaranteed that the alignments used are correct: that is, the alignment must not show an output symbol to correspond to an input symbol that comes after the input symbol that, in the target transducer, generates the output symbol.",44,45
13819,34048,"For the theoretical property of language identification in the limit, we must be guaranteed that the alignments used are correct: that is, the alignment must not show an output symbol to correspond to an input symbol that comes after the input symbol that, in the target transducer, generates the output symbol.",55,56
13820,34048,The resulting decision trees describe the behavior of the machine at a given state in terms of the next input symbol by generalizing from the arcs leaving the state.,20,21
13821,34048,"Thus for each state in a transducer, we gave the algorithm the set of arcs leaving the state (the samples), the phonological features of the next input symbol (the features), and the output/transition behaviors of the automaton (the decisions).",31,32
13822,34048,in the output string indicates the arc's input symbol (with no features changed).,9,10
13823,34048,indicates that the input symbol is emitted with no features changed.,4,5
13824,34048,"If the end-of-word symbol follows, the corresponding unvoiced stop will be emitted.",7,8
13825,34048,"If any other symbol follows, however, the original Transducer for word-final stop devoicing. []",3,4
13826,34048,indicates that the input symbol is emitted with no features changed.,4,5
13827,34048,"voiced stop will be emitted, along with the current input symbol.",11,12
13828,34048,"Just as the generalized arcs can now specify one of their output symbols as being the current input symbol with certain phonological features changed, they are now able to reference previous 4 The rules previously discussed in this paper avoid this problem because they apply to only one phone.",18,19
13829,34048,"When performing a transduction, variables are interpreted as referring to a certain symbol in the input string with specific phonological features changed.",13,14
13830,34048,Thus rewriting each output symbol in variable notation is done in constant time and adds nothing to the algorithm's computational complexity.,4,5
13831,34048,"When performing the state mergers of the OSTIA algorithm, two variables are considered to be the same symbol if they agree in both components: the index and list of phonological features.",18,19
13832,34048,"Once alignment information for each input/output pair has been computed, an output symbol can be rewritten in variable notation in constant time.",15,16
13833,17228429,A system should produce carta (a message or document) for Sentence (1) and letra or carácter (a symbol or handwriting) for (2).,22,23
13834,1912329,"For a CFG, the components of a parse forest are nodes labeled by couples (A, p) where A is a nonterminal symbol and p is a range, while for an RCG, the labels have the form (A, p-') where # is a vector (list) of ranges.",25,26
13835,1912329,"A call TinT*(T, X) is true if and only if the terminal symbol T occurs in X. The .MinV+-clauses spell from left-to-right the suffix part.",14,15
13836,467985,"The term glyph is used in this section to describe a symbol that represents an OCR recognition character, whether by logical or visual segmentation.",11,12
13837,3265280,The symbol ... denotes the matching part of the input which itself remains unchanged.,1,2
13838,3265280,"If the LOWER language consists of a single string, then the relation encoded by the transducer is in Berstel's terms a rational function, and the network is an unambigous transducer, even though it may contain states with outgoing transitions to two or more destinations for the same input symbol.",52,53
13839,3265280,A transducer is sequential just in case there are no states with more than one transition for the same input symbol.,20,21
13840,3265280,"On the other hand, the transducer in Figure 4 is sequentiable because there the choice between a and a:x just depends on the next input symbol.",27,28
13841,3265280,"To implement this idea, we introduce the special symbol ... on the right-hand side of the replacement expression to mark the place around which the insertions are to be made.",9,10
13842,3265280,"Thus the rule that introduces the END_0F_TOKEN symbol needs to combine the LETTER+ pattern with a list of multiword tokens which may include spaces, periods and other delimiters.",7,8
13843,3265280,"The percent sign here means that the following blank is to be taken literally, that is, parsed as a symbol.",21,22
13844,3265280,"Extensions Marking As we observed in section 3, by using the ... symbol on the lower side of the replacement expression, we can construct transducers that mark instances of a regular language without changing the text in any other way.",13,14
13845,3265280,"We recognize two types of symbols: unary symbols (a, b, c, etc) and symbol pairs (a:x, b:0, etc. ).",19,20
13846,3265280,"A symbol pair a:x may be thought of as the crossproduct of a and x, the minimal relation consisting of a (the upper symbol) and x (the lower symbol).",1,2
13847,3265280,"A symbol pair a:x may be thought of as the crossproduct of a and x, the minimal relation consisting of a (the upper symbol) and x (the lower symbol).",26,27
13848,3265280,"A symbol pair a:x may be thought of as the crossproduct of a and x, the minimal relation consisting of a (the upper symbol) and x (the lower symbol).",33,34
13849,3265280,"stands for any symbol in the known alphabet and its extensions; in replacement expressions, .#.",3,4
13850,3265280,"Thus Z[ denotes the literal square bracket as opposed to [, which has a special meaning as a grouping symbol; %0 is the ordinary zero symbol.",21,22
13851,3265280,"Thus Z[ denotes the literal square bracket as opposed to [, which has a special meaning as a grouping symbol; %0 is the ordinary zero symbol.",29,30
13852,3265280,The symbol ?,1,2
13853,2075553,"This dominance link will act as a constraint on derivations: if p is used in a derivation, then p' must be used subsequently in the subderivation that starts with the occurrence of A introduced by p. A UVG-DL is lexicalized iff at least one production in every vector contains a terminal symbol.",56,57
13854,2075553,"In Figures 6 and 7 , the synchronous productions are designated by a bold-italic left-hand side symbol.",20,21
13855,2075553,"Most important, each pair of linked nonterminals generated by Gs is represented by G using a compound symbol.",18,19
13856,2075553,"Then the two synchronous productions in v ~ and v"" are composed into a single production in v, by composing the two left-hand sides in a compound symbol and by concatenating the two right-hand sides.",31,32
13857,665441,"Let a parse tree T be defined as a set of triples (s, t, X)--where s denotes the position of the first symbol in a constituent, t denotes the position of the last symbol, and X represents a terminal or nonterminal symbol--meeting the following three requirements: • The sentence was generated by the start symbol, S. Formally, (1, n, S) E T. • Every word in the sentence is in the parse tree.",25,26
13858,665441,"Let a parse tree T be defined as a set of triples (s, t, X)--where s denotes the position of the first symbol in a constituent, t denotes the position of the last symbol, and X represents a terminal or nonterminal symbol--meeting the following three requirements: • The sentence was generated by the start symbol, S. Formally, (1, n, S) E T. • Every word in the sentence is in the parse tree.",37,38
13859,665441,"Let a parse tree T be defined as a set of triples (s, t, X)--where s denotes the position of the first symbol in a constituent, t denotes the position of the last symbol, and X represents a terminal or nonterminal symbol--meeting the following three requirements: • The sentence was generated by the start symbol, S. Formally, (1, n, S) E T. • Every word in the sentence is in the parse tree.",46,47
13860,665441,"Let a parse tree T be defined as a set of triples (s, t, X)--where s denotes the position of the first symbol in a constituent, t denotes the position of the last symbol, and X represents a terminal or nonterminal symbol--meeting the following three requirements: • The sentence was generated by the start symbol, S. Formally, (1, n, S) E T. • Every word in the sentence is in the parse tree.",62,63
13861,665441,"Ta = argmTaX~,P(Tc l w~) ITnTcl Tc This can be further expanded to (4) Ta = arg mTax E P(Tc I w~)E1 if (s,t,X) 6 Tc Tc (,,t,X)eT (5) Now, given a PCFG with start symbol S, the following equality holds: P(s .",52,53
13862,8097323,"Being inspired by lack of attention-sharing in autistic children, we assumed that observation of others' behavior by attention-sharing plays an indispensable role in symbol acquisition.",29,30
13863,8097323,"As a basis for language acquisition, we deal with acquisition of a symbol system, which articulates things and events in the world into categories and gives phonological labels to the categories.",13,14
13864,8097323,"This paper describes the role of ""attention-sharing"" (Baron-Cohen, 1995) , especially that based on gaze, in infants' symbol acquisition.",28,29
13865,8097323,Attention-Sharing and Symbol Acquisition Observation of others' verbal behavior provides infants with learning data for symbol acquisition.,18,19
13866,8097323,5 C o n c l u s i o n a n d F u t u r e R e s e a r c h We described our preliminary model of attentionsharing as a device for observing learning data (others' verbal behavior) for symbol acquisition.,49,50
13867,8097323,The model will work in the bootstrapping stage of infants' symbol acquisition; it only deals with referring to physical objects.,11,12
13868,3851014,"The formulation for the syntactic scoring function can thus be expressed as follows: Ssyn ( Treex ) P(A r (O},B,C, ($}) x P(C f {B},F,6, {$}) X''"" X P (D I {0},c1,(c2,c3,c4} ) P(A ]17,B,C, r7) x P(C ]16,F,G, r6) x ... x P(D ] ll,Cl, rl), (11)   where Tree× is the parse tree X, $ and 0 correspond to the end-of-sentence marker and the null symbol, respectively; and li and ri represent the left and right contexts to be consulted in the ith phrase level) respectively.",107,108
13869,3851014,"Instead of assigning probabilities to the production rules as a conventional stochastic context-free grammar parser does, Briscoe and Carroll distribute probability to each state so that the probabilities of the transitions from a state sum to one; the preference to a SHIFT action is based on one right context symbol (i.e., the lookahead symbol), and the preference for a RE-DUCE action depends on the lookahead symbol and the previous state reached after the REDUCE action.",53,54
13870,3851014,"Instead of assigning probabilities to the production rules as a conventional stochastic context-free grammar parser does, Briscoe and Carroll distribute probability to each state so that the probabilities of the transitions from a state sum to one; the preference to a SHIFT action is based on one right context symbol (i.e., the lookahead symbol), and the preference for a RE-DUCE action depends on the lookahead symbol and the previous state reached after the REDUCE action.",59,60
13871,3851014,"Instead of assigning probabilities to the production rules as a conventional stochastic context-free grammar parser does, Briscoe and Carroll distribute probability to each state so that the probabilities of the transitions from a state sum to one; the preference to a SHIFT action is based on one right context symbol (i.e., the lookahead symbol), and the preference for a RE-DUCE action depends on the lookahead symbol and the previous state reached after the REDUCE action.",75,76
13872,3851014,"The probabilities assigned to the states implicitly imply different preferences for left-hand side contextual environment of the reduced symbol, since a state, in general, can indicate part of the past parsing history (i.e., the left context) from which the current reduced symbol follows.",20,21
13873,3851014,"The probabilities assigned to the states implicitly imply different preferences for left-hand side contextual environment of the reduced symbol, since a state, in general, can indicate part of the past parsing history (i.e., the left context) from which the current reduced symbol follows.",49,50
13874,3851014,"Therefore, there are cases in which the same string is reduced, under different left contexts, to the same symbol at the same state and return to the same state after reduction.",21,22
13875,1289124,"Using the P(X3:X2)=P(E}D,w2,w3,w4) as an example, with D as the left context of t/~e current derivation symbol w2, we checked if P(X31X2)=P(E:D,w2) is true?",22,23
13876,9923219,The first is to construct a top-down PPDA that mimics directly the process of generating a PCFG derivation from the start symbol by repeatedly replacing the leftmost nonterminal in a sentential form by the right-hand side of one of its rules.,23,24
13877,9923219,"A weighted context-free grammar (WCFG) consists of a distinguished start symbol S E N plus a finite set of weighted productions of the form X -~ a, (alternately, u : X --~ a), where X E N, a E (Nt2E)* and the weight u is a nonnegative real number.",14,15
13878,9923219,"A weighted push-down automaton (WPDA) consists of a distinguished start state q0, a distinguished start stack symbol X0 and a finite set of transitions of the following form where p and q are states, a E E L.J {e}, X and Z1, ..., Zn are stack symbols, and w is a nonnegative real weight: x, pa~ Zl .. .",21,22
13879,9923219,"Zn, q A WPDA is a probabilistic push-down automaton (PPDA) if all weights are in the interval [0, 1] and for each pair of a stack symbol X and a state q the sum of the weights of all transitions of the form X,p ~ Z1 ...Z=, q equals 1.",34,35
13880,9923219,"The initial stack symbol is 1 and the initial state is (the state corresponding to) _L. For each production of the form X --+ a in G the PDA SIt(G) contains all shift transitions of the following form Y,Z-~ YZ, X The PDA SR(G) also contains the following termination transitions where S is the start symbol of G. Y, Z -~, X All reachable configurations are in one of the following four forms where the first is the initial configuration, the second is a template for all intermediate configurations with a E N*, and the last two are terminal configurations.",3,4
13881,9923219,"The initial stack symbol is 1 and the initial state is (the state corresponding to) _L. For each production of the form X --+ a in G the PDA SIt(G) contains all shift transitions of the following form Y,Z-~ YZ, X The PDA SR(G) also contains the following termination transitions where S is the start symbol of G. Y, Z -~, X All reachable configurations are in one of the following four forms where the first is the initial configuration, the second is a template for all intermediate configurations with a E N*, and the last two are terminal configurations.",63,64
13882,9923219,"We say that a nonterminal X is nontrivial in the grammar G if Pa(a # e I P = X) > O. We now define the grammar G' to consist of all productions of the following form where X, Y, and Z are nontrivial nonterminals of G and a is a terminal symbol appearing in G. X PG(~=YZ~p=x, ~#~) YZ X PG(~=a 12+=x, ~¢~) a We leave it to the reader to verify that G' has the property stated in theorem 4. •",56,57
13883,9923219,"A tree consisting of a single node labeled with X is a left corner G-derivation from X toX. For each pair of nonterminals X, Y in G we introduce a new nonterminal symbol X/Y. The H-derivations from X/Y will be in one to one correspondence with the left-corner Gderivations from X to Y. For each production in G of the form X ~ a we include the following in H where S is the start symbol of G: S --~ a S/X We also include in H all productions of the following form where X is any nonterminal in G: x/x If G consists only of productions of the form S -~ a these productions suffice.",35,36
13884,9923219,"A tree consisting of a single node labeled with X is a left corner G-derivation from X toX. For each pair of nonterminals X, Y in G we introduce a new nonterminal symbol X/Y. The H-derivations from X/Y will be in one to one correspondence with the left-corner Gderivations from X to Y. For each production in G of the form X ~ a we include the following in H where S is the start symbol of G: S --~ a S/X We also include in H all productions of the following form where X is any nonterminal in G: x/x If G consists only of productions of the form S -~ a these productions suffice.",86,87
13885,9923219,"We now define the final grammar G' to consist of all productions of the following form where X, Y, and Z are nontrivial nonterminals appearing in J and a is a terminal symbol appearing in J. The construction in this proof is essentially the standard left-corner transformation (Rosenkrantz and II, 1970) , as extended by Salomaa and Soittola (1978, theorem 2.3 ) to algebraic formal power series.",35,36
13886,9923219,The initial state is Ws and the initial stack symbol is ±. We have assumed that G contains a unique production of the form S -~ e. We include the following transition in M corresponding to this production.,9,10
13887,9923219,"A_,Ws~,T Then, for each rule of the form X -~ a~ in G and each symbol of the form Wx,~ we include the following in M: Z, Wx.",16,17
13888,210152110,"Since the C ⌧ relation does not attend to non-tier symbols, insertion of such a symbol at a given state must lead to a Nerode-equivalent state.",18,19
13889,210152110,"by parallel edges on each symbol in ⌧ \ {x, y}, and then add a self loop on ?",5,6
13890,210152110,This use of a distinct placeholder symbol allows constraints to be defined by automata of minimal alphabet that expand in a way that preserves their semantics.,6,7
13891,210152110,"Since TSL ⌧ stringsets are closed under insertion of symbols not in ⌧ , any transition on such a symbol from a given state must lead to a Nerode-equivalent state.",19,20
13892,7920631,"The symbol ""*' stands for 'not present').",1,2
13893,1253015,"One can also think of the cross entropy between a language model and a probabilistic source as the number of bits that will be needed on average to encode a symbol from the source when it is assumed, albeit mistakenly, that the language model is a perfect probabilistic characterization of the source.",30,31
13894,1253015,"1992) 1.25 (Shannon 1951) The cross entropy, H, of a code and a source is given by: H(source, code) = -~ ~ Pr(s, h I source) log 2 Pr(s I h, code) s h where Pr(s, h I source) is the joint probability of a symbol s following a history h given the source.",59,60
13895,69924960,"From a procedural perspective, these judgments can be made by a scanner, a mechanism that simply scans a window of a fixed size across the input, one symbol at a time, looking at each factor in sequence.",30,31
13896,855184,"Parse Forests A parse forest (see also Billot and Lang (1989) ) in labeled grammar notation is a tuple v I 8w x § Q y § Q i z § Q { § Q }| § ~P where I 8w g § Q y § Q i z § Q { § ~P is a context free grammar (consisting of non-terminals w § , terminals y § , rules i § , and a start symbol | § is also extended to map trees licensed by the parse forest grammar to trees licensed by the underlying grammar.",83,84
13897,855184,"Where }w § H y § Hi z § , let g § # ' be the set of trees licensed by I 8w § Q y § Q i z § Q { § P which have root symbol in the case of a symbol, and the set of trees which have as the rule expanding the root in the case or a rule.",40,41
13898,855184,"Where }w § H y § Hi z § , let g § # ' be the set of trees licensed by I 8w § Q y § Q i z § Q { § P which have root symbol in the case of a symbol, and the set of trees which have as the rule expanding the root in the case or a rule.",46,47
13899,855184,g # ' is the multiset of inside trees represented by parse which contain as a symbol or use as a rule.,16,17
13900,855184,is the multiset of complete trees represented by the parse forest symbol or rule .,11,12
13901,855184,"Where `is a probability function on trees licensed by the underlying grammar and is a symbol or rule in v , # ' def 8 U `# S ' (2) a# ' def $ `# S ' 8 $ X 2 `# S ' V (3) # ' is called the inside probability for and a# ' is called the flow for .",16,17
13902,855184,"5 Where 0 is a parse forest symbol on the right hand side of a parse forest rule n , we will simply state the condition ""0 is the head of n "".",7,8
13903,855184,"Let S be a tree in the parse forest grammar, let % be a symbol in S , let 0 be the maximal symbol in S of which % is a head, or % itself if % is a non-head child of its parent in S , and let 0 R1 be the parent of 0 in S .",15,16
13904,855184,"Let S be a tree in the parse forest grammar, let % be a symbol in S , let 0 be the maximal symbol in S of which % is a head, or % itself if % is a non-head child of its parent in S , and let 0 R1 be the parent of 0 in S .",24,25
13905,855184,"A parse forest tree S and symbol % in S thus determine the vector (4), where 0 and 0 1 are defined as above.",6,7
13906,855184,"Where % is parse forest symbol in v and n is a parse forest rule in v , let Â # &% (' def 8 9 « `# 8| § # S ' ¢' # S Q ¢% 9' 9 « À Ã `# 8| § # S ' ¢' (5) PF-GOVERNORS(v Q ¢¥ ) 1 § PF-INSIDE # v Q ¢¥ u' 2 § PF-FLOW # v Q ' 3 Initialize array Â o i § Ä zw x § y § $q to empty maps from governor labels to float 4 Â o { § $q « § À 8¾ startc ¾ startw 5 for n in i z § in top-down order 6 do Â o n q § 8¹ ¤º 8¹ » ® } } Xº Â o lhs # n ' q 7 for 0 in rhs # n ' 8 do if 0 is the head of n 9 then Â o 0 Rq « § Â o 0 q (°Â o n q 10 else Â o 0 q © § Â o 0 Rq (° o n q 9 « 8¾ 9 « » ® ¢ ¢ 8¾ Á m « » ® } } Å 11 return Â Â # n ' def 9 ¢ `# 8| § # S ' ¢' # S Q lhs # n ' ¢' 9 © À Ã `# 8| § # S ' ¢' V (6) Assuming that v I 8w § Q y § Q i § Q { § Q }| § P is a parse forest representing each tree analysis for a sentence exactly once, the quantity y for terminal position x (as defined in section 1) is found by summing Â # &% 9' for terminal symbols % in y § which have string position x .",5,6
13907,855184,"Consider a parse forest rule n , and a parse forest symbol 0 on its right hand side which is not the head of n .",11,12
13908,855184,"o { § $q is the inside probability of the root symbol, which is also the sum of the probabilities of all parse trees.",12,13
13909,855184,"According to Charniak (1993) , the outside probabilities in a parse forest are computed by: Ë 9o % q eÍ W Ã ¢® ¢ } Ë 9o lhs # n ' q o n q o % q The outside probability of the start symbol is 1.",48,49
13910,855184,"We prove by induction over the depth of the parse forest that the following relationship holds: o % q Ë (o % q o % q o { § ¢q It is easy to see that the assumption holds for the root symbol { § : o { § $q s Ë 9o { § $q o { § q o { § ¢q The flow in a parse forest is computed by: o % q eÍ Å ¢® } } o lhs # n ' q o n q o lhs # n ' q Now, we insert the induction hypothesis: o % q eÍ Å W ¢® } } Ë 9o lhs # n ' q o lhs # n ' q o n q o { § $q o lhs # n ' q After a few transformations, we get the equation o % q o % q o { § $q eÍ Å W Ã ¢® ¢ } Ë 9o lhs # n ' q o n q o % q which is equivalent to o % q Ë 9o % q o % q o { § mq according to the definition of Ë 9o % q .",44,45
13911,855184,"B Parse Forest Lexicalization The function LEXICALIZE below takes an unlexicalized parse forest as argument and returns a lexicalized parse forests, where each symbol is uniquely labeled with a lexical head.",24,25
13912,855184,A nonterminal is only created if no symbol with the same lexical head was linked to the original node.,7,8
13913,7954922,"After an LC parser has recognized the first symbol X of such an c~, it will as next step predict all aforementioned rules.",8,9
13914,7954922,"A context-free grammar G = (T, N, P, S) consists of two finite disjoint sets N and T of nonterminals and terminals, respectively, a start symbol S E N, and a finite set of rules P. Every rule has the form A --* c~, where the left-hand side (lhs) A is an element from N and the right-hand side (rhs) a is an element from V*, where V denotes (NUT).",34,35
13915,7954922,"The initial configuration is (Init, w), where Init E Alph is a distinguished stack symbol, and w is the input.",18,19
13916,7954922,"The input w is accepted if (Init, w) F-* (Fin, e), where Fin E Alph is a distinguished stack symbol.",27,28
13917,7954922,"The informal meaning of an item is ""The part before the dot has just been recognized, the first symbol after the dot is to be recognized next"".",20,21
13918,7954922,where S' is a fresh symbol.,6,7
13919,7954922,"In [11] this problem is solved by forcing the parser to decide at each call goto(Q, X) whether a) X is one more symbol of an item in Q of which some symbols have already been recognized, or whether b) X is the first symbol of an item which has been introduced in Q by means of the closure function.",28,29
13920,7954922,"In [11] this problem is solved by forcing the parser to decide at each call goto(Q, X) whether a) X is one more symbol of an item in Q of which some symbols have already been recognized, or whether b) X is the first symbol of an item which has been introduced in Q by means of the closure function.",51,52
13921,7954922,Formally: I ELR .~ {[A ---+ a] ] 0 C A G {A I A --* aft E pt} A (4 # E v a = {s'})} where we use the symbol A to range over sets of nonterminals.,44,45
13922,16783605,"Given a stack and an input string of symbols, the shift/reduce parser may only shift a symbol to the stack (Figure la ) or reduce n symbols on the stack by rewriting them as a single symbol (Figure lb ).",19,20
13923,16783605,"Given a stack and an input string of symbols, the shift/reduce parser may only shift a symbol to the stack (Figure la ) or reduce n symbols on the stack by rewriting them as a single symbol (Figure lb ).",40,41
13924,16783605,We further constrain the parser to reduce no more than two symbols on the stack to a single symbol.,18,19
13925,16783605,"In the following example the stack and input parts are separated by the symbol ""*/' as the idea is applied to the sentence ""The old man from Spain ate fish.""",13,14
13926,16783605,"The symbol _ stands for blank, art for article, adj for adjective, p for preposition, n for noun, and v for verb.",1,2
13927,16783605,"2 The analysis terminates with an empty input string and the single symbol ""snt"" on the stack, successfully completing the parse.",12,13
13928,16783605,"It can be observed that the last state in the analysis is the single symbol SNT--the designated root symbol, on the stack along with an empty input string, successfully completing the parse.",14,15
13929,16783605,"It can be observed that the last state in the analysis is the single symbol SNT--the designated root symbol, on the stack along with an empty input string, successfully completing the parse.",20,21
13930,16783605,"For each combination, there are 65 possible operations: 6 a shift or a reduction to another symbol.",18,19
13931,16783605,entries in the table refer literally by symbol rather than by reference to the stack.,7,8
13932,16783605,The symbol v.. refers to any verb.,1,2
13933,16783605,"It must apply a shift or any transformation to construct the new stack-string for the linguist user, and it must record the shift or transformation as the right half of a context-sensitive rule--still composed of a ten-symbol left half and an operation as the right half.",45,46
13934,5923203,"This paper will attempt to show that many of these types of concerns can be addressed with syntactic methods (symbol pushing), and need not require explicit semantic interpretation.",20,21
13935,26744929,"The automaton A is total iff for every symbol σ ∈ Σ and for every state q ∈ Q, there exists some q ′ such that σ, q, q ′ ∈ δ.",8,9
13936,26744929,"That is, given a state q ∈ Q and a symbol σ ∈ Σ, there is at most one q ′ ∈ Q such that σ, q, q ′ ∈ δ.",11,12
13937,26744929,This is because if a symbol σ can follow x in some string of L(A) then x • σ is a permitted factor and σ can follow x in any string of L(A).,5,6
13938,26744929,"If the first symbol of a forbidden factor is '⋊', then it can only occur at the left end of the word; this is an initial forbidden factor.",3,4
13939,26744929,"If the last symbol is '⋉', then it can only occur at the right end of the word; it is a final forbidden factor.",3,4
13940,26744929,"But these are all in ground form, with each syllable type represented by a distinct alphabet symbol.",17,18
13941,16007831,"As a final tree operation, OT denotes storage of episode token e T (a new episode symbol not yet used in T) at the current 5A node is past-domlnated if there is a past branch in its ancestry (where embedding finks also count as ancestry links).",18,19
13942,46125,"In the following I use the symbol --~ to also stand for the relation over pairs of derivations such that the second is derived from the first by one application of ,7.",6,7
13943,650496,"I~ can receive as input any string over V U L U R. ... i The rule table will be updated as t-he input is read so that when any position corresponding to any rule (I) of R contains the m entry (3), then immediately to the left in the input of the scanned symbol is a string analyzable as c~ 1 ... cLi.",61,62
13944,650496,"j Thus if a pointer m appears in the entry of a position immedi'ately to the left of the symbol ~ (dash), then the left-context of the corresponding -9.",19,20
13945,650496,"It is clear that the rule When started in its initial state scanning the leftmost symbol on the input tape with an empty pushdown-store, _M prints S on the store and initializes its tables as follows: for each rule (1) of 1~ m a co~responding position of the rule table receives the entry (4) and each position of the right-context table receives an entry with a pointer at its extreme right.",15,16
13946,650496,"At each successive step of its computation, M performs whichever one of the operations (5)... ( 8 ) is possible in view of the top symbol on its pushdownstore, the scanned symbol on its input tape and the contents of its tables.",29,30
13947,650496,"At each successive step of its computation, M performs whichever one of the operations (5)... ( 8 ) is possible in view of the top symbol on its pushdownstore, the scanned symbol on its input tape and the contents of its tables.",36,37
13948,650496,"If a right bracket ] A is on top of the pushdown-store and if is the scanned input symbol, then (i) advance the input tape one (ii) remove the symbol ]A from the top of the pushdown-store if every right-context table entry has a pointer at its extreme right, then nondeterministicany decide whether or not to enter the accepting state. (",20,21
13949,650496,"If a right bracket ] A is on top of the pushdown-store and if is the scanned input symbol, then (i) advance the input tape one (ii) remove the symbol ]A from the top of the pushdown-store if every right-context table entry has a pointer at its extreme right, then nondeterministicany decide whether or not to enter the accepting state. (",36,37
13950,650496,"Remark: For any phrase structunre grammar G, a pushdown-storage automaton M' accepting I~G) can be obtained from the automaton M described in the proof of Theorem 1 by altering operations ( 5 ) and ( 7 ) so that they apply regardless of what input symbol is scanned and do not move the input tape.",53,54
13951,13318099,Information abont the linguistic properties of an entry is represented by complex categories that include a principal category symbol and specifications of constraints on the values of syntactic/semantic features.,18,19
13952,219309482,"No Preposition--if there is no preposition, the noun or verb is simply entered with a special symbol NULL, conceived of as the null preposition. (",19,20
13953,2472777,"Any other word withi, a repair is notated with X. A hyphen affixed to a symbol indicates a word fragment.",16,17
13954,2710986,"Word translation probabilities are placed onto arcs emitting the word as an output symbol (in the figure, note the arcs emitting ""committee"", ""the"", etc.).",13,14
13955,3264671,"Preliminaries Tree Adjoining Grammars Formally, a TAG is a five-tuple (E, NT, I, A, S / where: E is a finite set of terminal symbols, NT is a finite set of non-terminal symbols, I is a finite set of elementary initial trees, A is a finite set of elementary auxiliary trees, S is a distinguished non-terminal, the start symbol.",76,77
13956,3264671,The projection is necessary because the automaton can distinguish between nodes labeled with the same symbol while the CFG cannot.,15,16
13957,11601325,"That is, the symbol X may be rewritten as as the string Y in the context u-..v. More generally, the right-hand side of a context-sensitive rule must contain at least as many symbols as the left-hand side.",4,5
13958,11601325,"A sequence of elements on the stack may be reduced --rewritten as a single symbol, or a new element may be shifted from the input to the stack.",14,15
13959,11601325,"Whenever a reduce occurs, a subtree of the parse is constructed, dominated by the new symbol and placed on the stack.",17,18
13960,11601325,The parse is complete when the input string is empty and the stack contains only the root symbol of the parse tree.,17,18
13961,11601325,"The final state in the parse is an empty input string and a stack containing only the root symbol, SNT.",18,19
13962,11601325,"To obtain manageable rules we limit the stack and input parts of the state to five symbols each, forming a ten symbol pattern for each state of the parse.",22,23
13963,11601325,"In the example of Figure 1 we separate the stack and input parts with the symbol ""*"", as we illustrate the basic idea on the sentence ""The late launch from Alaska delayed interception.""",15,16
13964,11601325,"The symbol b stands for blank, ax-1; for article, adj for adjective, p for preposition, n for noun, and v for verb.",1,2
13965,11601325,"The analysis terminates successfully with an empty input string and the single symbol ""snt"" on the stack.",12,13
13966,11601325,"For each combination, there are 65 possible operations3: a shift or a reduction to another symbol.",17,18
13967,11601325,In view of the large combinatoric space provided by the ten symbol parse states --it could be as large as 641° --our prediction of 25-40 thousand examples as mainly sufficient for news stories seems contra~intuitive.,11,12
13968,12555584,"Forward-chaining rules have a ""left-hand side "" (LHS) and a ""right-han d side"" (RHS), which are delimited from each other by an arrow symbol, __> .",37,38
13969,231750182,"Each clause with an arrow has a single event symbol on the left, and a disjunction of alternative events on the right of the arrow.",9,10
13970,231750182,Multiplying by the state symbol h in the term • * h has the effect of conjoining h with the atom at the end of the world.,4,5
13971,6673092,transition to one-symbol two-symbol state state NN JJ: 0.45 INTRODUCTION Many words in English have several parts of speech (POS).,4,5
13972,6673092,transition to one-symbol two-symbol state state NN JJ: 0.45 INTRODUCTION Many words in English have several parts of speech (POS).,7,8
13973,6673092,"For the sake of simplicity, a POS tag is termed a symbol and a sequence of tags is called a string.",12,13
13974,6673092,"A prediction suffix tree T over ]E, is a tree of degree I~l. The edges of the tree are labeled by symbols from ~E, such that from every internal node there is at most one outgoing edge labeled by each symbol.",45,46
13975,6673092,"We require that for every two states qX, q2 E Q and for every symbol a E ~, if r(q 1,or) = q2 and qt is labeled by a string s 1, then q2 is labeled by a string s ~ which is a suffix of s 1 • or.",15,16
13976,6673092,"Thus, in order that r be well defined on a given set of string S, not only must the set be suffix free, but it must also have the property, that for every string s in the set and every symbol a, there exists a string which is a suffix of scr.",44,45
13977,6673092,"The resulting automaton has 49 states: the null state (e), 43 first order states (one symbol long) and 5 second order states (two symbols long).",20,21
13978,6673092,"The two-symbol states were ""AT JJ"", ""AT NN"", ""AT VBN"", ""JJ CC"", and ""MD RB"" (article adjective, article noun, article past participle, adjective conjunction, modal adverb).",3,4
13979,3261520,"The symbol VF'C is read ""finite verb chunk''; similarly we work with noun chunks (Nc), prepositional chunks (Pc), and so forth.",1,2
13980,3261520,"v) s E N, with the interpretation of a start symbol.",12,13
13981,5094703,"3 where A ---, a/~ is some rule of G..A4(G) is the determinization by the standard subset construction (Aho and Ullman, 1977) of the FSA defined as follows: • The initial state is the dotted rule ff ---, -S where S is the start symbol of G and S' is a new auxiliary start symbol. •",53,54
13982,5094703,"3 where A ---, a/~ is some rule of G..A4(G) is the determinization by the standard subset construction (Aho and Ullman, 1977) of the FSA defined as follows: • The initial state is the dotted rule ff ---, -S where S is the start symbol of G and S' is a new auxiliary start symbol. •",64,65
13983,5094703,"The final state is S' --~ S.. • The other states are all the possible dotted rules of G. • There is a transition labeled X, where X is a terminal or nonterminal symbol, from dotted rule A -+ a. X~ to A --+ c~X.//. • There is an e-transition from A --~ a • B/~ to B --~ ""7, where B is a nonterminal symbol and B -+ 7 a rule in G. 2Unification-based grammars not in this class would have to be weakened first, using techniques akin to those of Sato and Tamaki (1984), Shieber (1985b) and Haas (1989) .",36,37
13984,5094703,"The final state is S' --~ S.. • The other states are all the possible dotted rules of G. • There is a transition labeled X, where X is a terminal or nonterminal symbol, from dotted rule A -+ a. X~ to A --+ c~X.//. • There is an e-transition from A --~ a • B/~ to B --~ ""7, where B is a nonterminal symbol and B -+ 7 a rule in G. 2Unification-based grammars not in this class would have to be weakened first, using techniques akin to those of Sato and Tamaki (1984), Shieber (1985b) and Haas (1989) .",75,76
13985,5094703,".A~(G) can be seen as the finite state control for a nondeterministic shift-reduce pushdown recognizer TO(G) for G. A state transition labeled by a terminal symbol z from state s to state s' licenses a shift move, pushing onto the stack of the recognizer the pair (s, z).",29,30
13986,5094703,"In what follows, G is a fixed CFG with terminal vocabulary ~, nonterminal vocabulary N, and start symbol S; V = ~ U N. Soundness Let J~4 be the characteristic machine for G, with state set Q, start state so, set of final states F, and transition function ~ : S x V --* S. As usual, transition functions such as 6 are extended from input symbols to input strings by defining 6(s, e) -s and 6is , a/~) = 5(6(s, a),/~).",20,21
13987,5094703,"The stack is a sequence of pairs / s, X) of a state and a symbol.",17,18
13988,5094703,"The closure [R] of a set R of dotted rules is the smallest set of dotted rules containing R that contains B --~ ""7 whenever it contains A --~ a • Bfl and B ---* 7 is in G. The core of the initial state so contains just the dotted rule ff ~ .S. For any other state s, there is a state 8 ~ and a symbol X such that 8 is the closure of the set core consisting of all dotted rules A ~ aX./~ where A --* a. X/~ belongs to s'.",72,73
13989,5094703,The equivalent CFG is derived by finding all full instantiations of the initial APSG rules that are actually reachable in a derivation from the grammar's start symbol.,27,28
13990,5094703,declares cat as the start symbol of the grammar.,5,6
13991,5094703,"In the grammar rules, the symbol ""'"" prefixes terminal symbols, commas are used for sequencing and ["" for alternation.",6,7
13992,5094703,"The symbol ""!""",1,2
13993,2769726,"To combat a limited set of specific circumstances in which the hypothesis fails, we use a small (4 rule, 8 symbol) distituent grammar, which indicates when two parts of speech cannol remain in the same constituent.",23,24
13994,3934380,"Introduction A core computational problem in online language comprehension is to deal with local ambiguity, the one-to-many mapping from a unit symbol w k (e.g., word) to symbol strings containing w at the k-th position W * k = • • • w k • • • and their interpretations S (e.g., sentences and their parses).",26,27
13995,3934380,"Introduction A core computational problem in online language comprehension is to deal with local ambiguity, the one-to-many mapping from a unit symbol w k (e.g., word) to symbol strings containing w at the k-th position W * k = • • • w k • • • and their interpretations S (e.g., sentences and their parses).",35,36
13996,3934380,"Rational models of sentence comprehension solve this problem by computing P (S|W k ), a conditional probability of interpretations given a partial string of symbols (henceforth, prefix) W k = w 1 • • • w k , and updating it discretely for every new symbol input (Jurafsky, 1996; Hale, 2001; Levy, 2008) .",50,51
13997,3934380,"Then, we can describe the tree as an unordered set of symbol/position (or filler/role) bindings: S[1](A,B) ≡ {B/1,S[1]/r,A/0}.",12,13
13998,3934380,"The Competition Constraint prohibits more than one symbol having activation 1 in any given role, so large q values force the model to choose among competitors.",7,8
13999,3934380,"7  We can estimate the expected settling time t c from the old to the new optimum by recalling that, on average, da/dt = ∇ a H, so: ∆H = tc 0 dH(a) dt dt = tc 0 ∇ a H(a) da dt dt ≈ tc 0 ∇ a H(a) 2 dt = t c • E( ∇ a H(a) 2 ) where the approximation symbol indicates we ignore the stochastic term in Eq.",77,78
14000,204800642,"Introduction Traditional models of cognition, and language in particular, have relied heavily on symbol structures and symbol manipulation.",15,16
14001,204800642,"Introduction Traditional models of cognition, and language in particular, have relied heavily on symbol structures and symbol manipulation.",18,19
14002,204800642,NN embedding of symbol structures McCoy et al.,3,4
14003,204800642,"In general, for a symbol structure S with roles {r k } that are respectively filled by the symbols {f k }, e TPR (S) = k e F (f k ) ⊗ e R (r k ).",5,6
14004,204800642,"More precisely, it learns a dictionary of n R d R -dimensional role-embedding vectors, R ∈ R d R ×n R , and, for each input symbol s t , computes a soft-attention vector a t over these role vectors: the role vector assigned to s t is then the attention-weighted linear combination  The fillers (yellow circles) and roles (blue circles) are first vectorized with an embedding layer.",31,32
14005,204800642,"ROLE simultaneously learns a dictionary of n F d F -dimensional symbolembedding filler vectors F ∈ R d F ×n F , the φ th column of which is f φ , the embedding of symbol type φ; φ ∈ 1, . . . ,",35,36
14006,204800642,n F where n F is the size of the vocabulary of symbol types.,12,13
14007,204800642,"The TPR generated by ROLE is thus T(S) = T t=1 f τ (st) ⊗ r t , where τ (s t ) is symbol s t 's type.",28,29
14008,204800642,"Since a TPR for a discrete symbol structure deploys a discrete set of roles specifying discrete structural positions, ideally a single role would be Figure 4 : The role learning module.",6,7
14009,204800642,"The continuous method tests ROLE in the same way as it was trained, with input symbol s t assigned role vector r t = R a t .",16,17
14010,204800642,"In the first stage, the snapped method is used to output one-hot vector roles m t for every symbol in the dataset.",21,22
14011,204800642,"s T that is learned by the SCAN encoder, to an excellent degree of approximation (as measured by substitution accuracy): e(S) = W T t=1 f τ (st) ⊗ r ρ(st) + b, where τ (s t ) is symbol s t 's type, ρ(s t ) is the role assigned to s t by the algorithm discussed next, and the matri- ces W, F = [f 1 . . .",48,49
14012,204800642,"In a discrete symbolic structure, each position can hold at most one symbol, and the final term R 3 in ROLE's regularizer R is designed to encourage this.",13,14
14013,204800642,"In the vector s A = T t=1 a t , the ρ th element is the total attention weight, over all symbols in the string, assigned to the ρ th role: in the discrete case, this must be 0 (if no symbol is assigned this role) or 1 (if a single symbol is assigned this role).",47,48
14014,204800642,"In the vector s A = T t=1 a t , the ρ th element is the total attention weight, over all symbols in the string, assigned to the ρ th role: in the discrete case, this must be 0 (if no symbol is assigned this role) or 1 (if a single symbol is assigned this role).",59,60
14015,17402234,"Conceptually, the probability of a rule for a given non-terminal symbol is the likelihood with which the rule is applied, as opposed to other rules for rewriting the same non-terminal label.",13,14
14016,17402234,"The notion of a grammatical relation, such as precedence or dominance, can be generallsed to mean the propensity of a symbol to relate to another symbol.",22,23
14017,17402234,"The notion of a grammatical relation, such as precedence or dominance, can be generallsed to mean the propensity of a symbol to relate to another symbol.",27,28
14018,17402234,"The likelihood of the derivation of a sentence, W, from the topmost distinguished symbol, S, for an ID/LP grammar, can be determined from the independent likelihoods of each step in the derivation.",15,16
14019,17402234,"Each step is the result of replacing some non-terminal symbol by some other terminal and non-terminal symbols, (the dominance relation), and the result of rearranging those derived symbols in the desired order (the precedence relation).",11,12
14020,17402234,"The advantage of this decomposition is that the size of the rule sets are to some extent governed by the size of the symbol set chosen, and do not involve the huge tail of low frequency rules typical of P-CFG grammars.",23,24
14021,2565824,"A boundary symbol "" b "" was introduced to separate sentences.",2,3
14022,1774141,Consider the following utterances and some of their associated presuppositions (11) (the symbol t> precedes an inference drawn on pragmatic grounds): (9) Either Chris is not a bachelor or he regrets that Mary came to the party. (,15,16
14023,1214376,"After sentence alignment, the English texts are tokenized so that a punctuation symbol is separated from its preceding word.",13,14
14024,3063850,"l:or instance, by adding two special extra argtuncnts to each symbol in the following rules: (1) noun phrase ( Num, NP Str, NP Rest ) --> det(Num,NP Str,I) Rcst),noun(Nun],l) Rest,NP Rest). (",13,14
14025,1788296,"A token can be a word or a punctuation symbol, and each of these neighboring tokens must be in the same sentence as ¢ .",9,10
14026,12515809,"The situation that a verb cannot find a theta role is represented by the symbol ""r--'l "".",15,16
14027,14342090,"Each node is labelled with a terminal symbol, a nonterminal symbol or the empty string.",7,8
14028,14342090,"Each node is labelled with a terminal symbol, a nonterminal symbol or the empty string.",11,12
14029,14342090,A d-edge is removed by merging the nodes at either end of the edge as long as they are labelled by the same symbol.,25,26
14030,61685712,"Tree Adjoining Grammars A Tree Adjoining Grammar (TAG) consists of a quintuple (N, ~ U {~}, I, A, S), where N is a finite set of nonterminal symbols, is a finite set of terminal symbols disjoint from N, is the empty terminal string not in ~, I is a finite set of labelled initial trees, A is a finite set of auxiliary trees, S E N is the distinguished start symbol The trees in I U A are called elementary trees.",86,87
14031,61685712,"Also, every initial tree is labelled at the root by the start symbol S and has leaf nodes labelled with symbols from ~3 U {E}.",13,14
14032,61685712,An auxiliary tree has both its root and exactly one leaf (called the foot node ) labelled with the same nonterminal symbol.,22,23
14033,61685712,"Context Free recognition in O( M(n)) Time The CFG G = (N,~,P, A1), where N is a set of Nonterminals {A1, A2, .., Ak}, is a finite set of terminals, P is a finite set of productions, A1 is the start symbol is assumed to be in the Chomsky Normal Form.",56,57
14034,61685712,"Proof: See (Valiant, 1975) for details The idea behind (Valiant, 1975) is based on visualizing Ak E b+j as spanning a tree rooted at the node Ak with l~aves ai through aj-1 and internal nodes as nonterminals generated from Ak according to the productions in P. Having done this, the following observation is made : Given an input string al...a, and 2 distinct symbol positions, i and j, and a nonterminal Ak such that Ak E b + .,",76,77
14035,61685712,7 Proof of Correctness We will show the proof of correctness of the algorithm by induction on the length of the sequence of symbol positions.,23,24
14036,61685712,"But first, we make an observation, given any two symbol positions (r~, rt), rt > r~ 4-1 , and a node m spanning a tree (i,j, k, l) such that i < rs and i _> rt with j and k in any of the possible combinations as shown in figure 4 .",11,12
14037,61685712,"ml spans a tree (il,jl, kl, ll) such that the last operation to create this tree was an adjunction by an auxiliary tree (or a grown auxiliary tree) (il, j2, ke, Ix), rooted at node me, onto the node ml spanning the tree (je,jl, kl, k2) such that node me has either the property mentioned in Any node satisfying the above observation will be called a minimal node w.r.t, the symbol positions (r,, r0.",92,93
14038,61685712,"Theorem : Given an increasing sequence < rl, r2, .., rp > of symbol positions and given a. V gaps (rq, rq+l), all nodes spanning trees (i,j,k,l} with rq < i < j < k < l < rq+l b. V gaps (rq, rq+l), all nodes spanning trees (i,j,k,l) such that either rq < i < rq+l or rq < l < rq+l c. V gaps (rq,rq+l) , all the minimal nodes for the gap such that these nodes span trees (i,j,k,l) Base Cases : For length = 1, it is trivial as this information is already known as a result of initialization.",16,17
14039,61685712,"Note that since there is only one symbol from the input (namely, ar~), and because an auxiliary tree has at least one label from ~, thus, checking for one adjunction is sufficient as there can be at most one adjunction.",7,8
14040,61685712,"Induction hypothesis : V increasing sequence < rl,r2, ..,r~ > of symbol positions of length < p, (i.e q < p), the algorithm, given the information as (5a) (i,j,k,l) such that {i, l} e { rl, r2, .., rq } and i < j < k < I. Induction : Given an increasing sequence < rl, r~, .., rp, rp+l > of symbol positions together with the information required as per parts a,b,c of the theorem, the algorithm proceeds as follows: 1.",16,17
14041,61685712,"Induction hypothesis : V increasing sequence < rl,r2, ..,r~ > of symbol positions of length < p, (i.e q < p), the algorithm, given the information as (5a) (i,j,k,l) such that {i, l} e { rl, r2, .., rq } and i < j < k < I. Induction : Given an increasing sequence < rl, r~, .., rp, rp+l > of symbol positions together with the information required as per parts a,b,c of the theorem, the algorithm proceeds as follows: 1.",92,93
14042,13328586,"If probabilistic context-free grammars are to be used as the basis of the language model, it will be necessary to compute the probability that successive application of the grammar rewrite rules (beginning with the sentence start symbol s) produces a word string whose initial substring is an arbitrary sequence wl, w2, . . . ,",40,41
14043,13328586,"They include the distinguished phrase marker s, the sentence ""start"" symbol.",13,14
14044,13328586,"A context-free grammar is assumed to generate sentences from top to bottom, starting with some rule s --+ G1G2 that rewrites the sentence symbol s and is used with probability P(s --+ GIG2).",26,27
14045,15326097,"Also, let q0 be a fresh symbol.",7,8
14046,14701220,Any other word within a repair is notated with X. A hyphen affixed to a symbol indicates a word fragment.,15,16
14047,3158796,"1) one who does something, an agent (2) one who is professionally engaged to perform a certain operation (3) a surgeon (4) one who carries on financial operations (5) one who works a machine (6) one who works a business (7) a symbol Figure 1 .",56,57
14048,16212742,"Atom Symbol or number Fun-exp Function, i.e, symbol pointing to a function, or lambda expression interpretable as a function, of type Atom X Atom × ... Atom ---> Atom or else Atom × Atom X ... Atom ---> List-of-atoms (where List-of-atoms will be interpreted as a logical disjunction of atomic values) S-expression Any complete evaluatable expression without internal references to F-PATR nodes The following then is a BNF grammar for F-PATR equations representing feature structures: We will assume the existence of a familiar equivalent notation for these feature equations, in which graph reentrancies (or path equivalences) are expressed by a matrix with integers used for shared reference.",11,12
14049,16212742,"We assume the following data types for nodes in a feature structure graph: :Arc-list a set of attribute labels and associated values, the latter of which may be of any type :null the uninstantiated ""variable"" type :atomic a singleton set of one symbol or number :disjunct a set of 2 or more atomic values :appl an applicative expression :res-var a residuated variable, i.e., a :null type that appears as an argument in at least one predicate :res-disjunct a residuated disjunction, i.e., a :disjunct type that appears as an argument in at least one predicate The node types that may acquire residuations include :null, :disjunct, and :appl (a type for which we do not distinguish residuated from nonresiduated subtypes).",51,52
14050,29905988,"The data types supported by the reference interpreter are integer, floating point number, Boolean, string, symbol, a reference to another annotation, or sets of any of those types.",19,20
14051,29905988,"The reference interpreter does not treat symbols and strings differently, except that if a symbol contains any non-alphanumeric characters, it must be enclosed in string quotes in order to be parsed correctly by the grammar compiler.",15,16
14052,10302625,"They are separated from the rest of the predicate by the symbol I-The different orders in which the two algorithms expand the branches of the derivation tree and generate the terminal nodes are marked, ha italics for SHDGA, and in roman case for EAA.",11,12
14053,1494871,"The third consequence is that the length of the longest candidate context can increase by at most one symbol at each time step, which impairs the model's ability to model complex sources.",18,19
14054,1494871,"In contrast, an extension model maps every history to a sel of contexts, one for each symbol in the alphabet.",18,19
14055,1494871,"Each symbol is predicted in its own context, and the model's current predictions need not be estimated using the same set of histories.",1,2
14056,1494871,"is the set of symbols available in the context w and A(~rlw ) is the conditional probability of the symbol c~ in the context w. Note that )--]o~ A(c~[w) < 1 for all contexts w in the dictionary D. The probability /5(h1¢ ) of a string h given the model ¢, h • E', is calculated as a chain of conditional probabilities (1) /5(h{¢) --"" ~(hnlhl...hn_l,¢)~ (hl...h,~_ll¢) (1) while the conditional probability ih(elh, ¢) of a single symbol ~r after the history h is defined as (2).",19,20
14057,1494871,"is the set of symbols available in the context w and A(~rlw ) is the conditional probability of the symbol c~ in the context w. Note that )--]o~ A(c~[w) < 1 for all contexts w in the dictionary D. The probability /5(h1¢ ) of a string h given the model ¢, h • E', is calculated as a chain of conditional probabilities (1) /5(h{¢) --"" ~(hnlhl...hn_l,¢)~ (hl...h,~_ll¢) (1) while the conditional probability ih(elh, ¢) of a single symbol ~r after the history h is defined as (2).",96,97
14058,1494871,"In contrast, an extension model includes a selection rule s : E* x E --+ D whose inputs include the past history and the symbol to be predicted.",26,27
14059,1494871,"s : E* ---* 2 D , while an extension model would map every history and symbol to a set of candidate contexts, ie.,",18,19
14060,1494871,"The recursion in (2) says that each symbol should be predicted in its longest candidate context, while the expansion factor 6(h) says that longer contexts in the model should be trusted more than shorter contexts when combining the predictions from different contexts.",9,10
14061,1494871,Constraint (4a) states that every symbol has the empty string as a context.,7,8
14062,1494871,This guarantees that every symbol will always have at least one context in every history and that the recursion in (2) will terminate.,4,5
14063,1494871,"The third constraint (4c) states that the sum of the probabilities of the extensions E(w) must sum exactly to unity when every symbol is available in that context (ie.,",25,26
14064,1494871,"Let c(c~[w) be the number of times that the symbol followed the string w in the training corpus, and let c(w) be the sum ~es c(crlw) of all its conditional frequencies.",10,11
14065,1494871,"We estimate )~c(q(w)lw ) as e(w)/(c(w) + #(w)) and )~c(4(w)[w) as #(w)/(c(w)+ #(w)) where #(w) is the total weight assigned to the novel events q(w) in the context w. Currently, we calculate #(w) as min([q(w)l, Iq(w)[) so that highly variable contexts receive more flattening, but no novel symbol in ~(w) receives more than unity weight.",74,75
14066,1494871,if (( n > nm~=) V (ICnl = 0)) then return; The loop in lines 3-5 repeatedly finds the single most profitable symbol a with which to augment the set S of profitable extensions.,30,31
14067,1494871,"Message entropy (in bits/symbol) is for the testing corpus only, as per traditional model validation methodology.",6,7
14068,1494871,The rightmost column contains test message entropy in bits/symbol.,10,11
14069,1494871,all extensions whose symbol is and whose context is a suffix of w. Extension mixing allows us to remove the uniform flattening of zero frequency symbols in our parameter estimates (5).,3,4
14070,19493663,This latter is a well-defined symbol or concept in the Mikrokosmos ontology as described in Mahesh (1996) .,7,8
14071,471453,We assume that the starting symbol has unitary fan-out.,5,6
14072,471453,"For a symbol X, assume , B2, ..., Br) where each of the right-hand side nonterminals has fan-out two.",2,3
14073,15648013,They characterise any string whose analysis involves abstraction over a function symbol as a non-dependency constituent.,11,12
14074,1659667,"Shift-reduce can complete a wellformed symbol table or chart in polynomial time for context-free grammars, but Shake-and-Bake is exponential even with a chart.",7,8
14075,720977,"For each non-anaphoric referential noun phrase (NP) in a question, including a questioned NP itself, LUNAR would create a new constant symbol to represent the new entity, putting an appropriate description on its property list.",27,28
14076,9197677,"   In order to give a rough idea of the density of the space in different locations, the symbol ""1"" is placed before the first neighbor in Table 2 that has a correlation of 0.978 or less with the head word.",19,20
14077,1107100,"Priist calls the operation used to calculate the value for schema the most specific common denominator (MSCD, indicated by the symbol ¢).",22,23
14078,12147910,"This framework uses a variant of the simply typed A-calculus where symbol occurrences can be annotated with so-called colours and substitutions must obey the following constraint: Given a labeling of occurrences as either primary or secondary, the POR excludes of the set of linguistically valid solutions, any solution which contains a primary occurrence.",13,14
14079,12147910,Hence no substitution will ever contain a primary occurrence (i.e. a pe-coloured symbol).,15,16
14080,12147910,"In contrast, (3d) is not a possible solution since it assigns to an -~pe-coloured variable, a term containing a pe-coloured symbol i.e. a term that is not -~pemonochrome. (",28,29
14081,12147910,"We will denote the substitution of a term N for all free occurrences of x in M with [N/x]M. It is crucial for our system that colours annotate symbol occurrences (i.e. colours are not sorts!),",31,32
14082,13248519,"The role of the language model is, for all possible word strings that match the typed phonetic symbol string, to select the word string with the highest language model probability.",18,19
14083,17033030,"A CFG G is a 4-tuple (N, ~, R, S) where N is the set of nonterminals including the start symbol S, ~ is the set of terminal symbols, and R is the set of rules, each of the form A --* a for A c N and a E (N U ~)*.",27,28
14084,17033030,"We will use the symbol ~ for immediate derivation and for its reflexive, transitive closure.",4,5
14085,17033030,"The algorithm consists of a first set of loops to handle the singleton productions, a second set of loops to handle the binary productions, and a return of the start symbol's chart entry.",32,33
14086,17033030,"Thus, the outside probability has the property that when multiplied by the inside probability, it gives the probability that the start symbol generates the sentence using the given item, P (S G Wl .., wi_dAwj... Wn G Wl ... Wn) .",23,24
14087,3266594,Head constraints: The nonterminal symbol V in the source rule must have the verb miss as a syntactic head.,5,6
14088,3266594,The symbol V in the target rule must have the verb manquer as a syntactic head.,1,2
14089,3266594,The head of symbol S in the source (target) rule is identical to the head of symbol V in the source (target) rule as they are co-indexed.,3,4
14090,3266594,The head of symbol S in the source (target) rule is identical to the head of symbol V in the source (target) rule as they are co-indexed.,18,19
14091,3266594,"A head is typically introduced 6 in preterminal rules such as leave ---* V V *--partir where two verbs, ""leave"" and ""partir,"" are associated with the heads of the nonterminal symbol V. This is equivalently expressed as leave:l --~ V:I V:I ~ partir:l which is physically implemented as an entry of an English-French lexicon.",38,39
14092,3266594,"GT has the same set of terminal symbols as T. 6A nonterminal symbol X in a source or target CFG rule X --* X1 ... Xk can only be constrained to have one of the heads in the RHS X1 ... X~. Thus, monotonicity of head constraints holds throughout the parsing process.",12,13
14093,3266594,"For each nonterminal symbol X in T, GT ineludes a set of nonterminal symbols {X~ ]w is either a terminal symbol in T or a special symbol e}.",3,4
14094,3266594,"For each nonterminal symbol X in T, GT ineludes a set of nonterminal symbols {X~ ]w is either a terminal symbol in T or a special symbol e}.",23,24
14095,3266594,"For each nonterminal symbol X in T, GT ineludes a set of nonterminal symbols {X~ ]w is either a terminal symbol in T or a special symbol e}.",29,30
14096,3266594,"If Xj has no head constraint in the above rule, GT includes a set of (N + 1) rules, where Xhj above is replaced with Xw for every terminal symbol w and Xe (Yhj will also be replaced if it is co-indexed with Xj).s Now, L(T) C_ L(GT) is obvious, since GT can simulate the derivation sequence in T with corresponding rules in GT.",33,34
14097,3266594,"The internal representations of these patterns are as follows: leave:V:l NP:2 ~ VP:I VP:I ~--quitter:V:l NP:2 be:V:l year:NP:2 old --+ VP:I VP:I ~ avoir:V:l an:NP:2 These patterns can be associated with an explicit nonterminal symbol such as ""V:*"" or ""ADJP:*"" in addition to head constraints (e.g., ""leave:V:*').",63,64
14098,3266594,"Scan(i) tries to combine inactive charts with the symbol si+l at position i. Complete(n) gives the set of possible parses for the input I. Now, for every inactive chart associated with a nonterminal symbol X for a span of (i~) (1 ~ i, j <_ n), there exists a set P of patterns with the source CFG skeleton, ... --* X. We can define the following ordering of patterns in P; this gives patterns with which we can use head and link constraints for building target charts and translations.",9,10
14099,3266594,"Scan(i) tries to combine inactive charts with the symbol si+l at position i. Complete(n) gives the set of possible parses for the input I. Now, for every inactive chart associated with a nonterminal symbol X for a span of (i~) (1 ~ i, j <_ n), there exists a set P of patterns with the source CFG skeleton, ... --* X. We can define the following ordering of patterns in P; this gives patterns with which we can use head and link constraints for building target charts and translations.",36,37
14100,3266594,"Since the number M of distinct pairs (X,w), for a nonterminal symbol X and a subsequence w of input string s, is bounded by Kn 2, we can compute the mbest choice of pattern candidates for every inactive chart in time O(ITIKn 3) as claimed by Maruyama (Maruyama, 1993) , and Schabes and Waters (Schabes and Waters, 1995) .",16,17
14101,74294," In the special case of BTGs which are employed in the model presented below, there is only one undifferentiated nonterminal category (aside from the start symbol).",28,29
14102,3039886,"A context-free grammar (CFG) is a 4-tuple G = (S, N, P, S), where S and N are two finite disjoint sets of terminal and nonterminal symbols, respectively, S E N is the start symbol, and P is a finite set of rules.",48,49
14103,3039886,"A pushdown automaton (PDA) is a 5-tuple .4 = (Z, Q, T, qi,, q/in), where S, Q and T are finite sets of input symbols, stack symbols and transitions, respectively; qin E Q is the initiM stack symbol and q/i, E Q is the finM stack symbol.",55,56
14104,3039886,"A pushdown automaton (PDA) is a 5-tuple .4 = (Z, Q, T, qi,, q/in), where S, Q and T are finite sets of input symbols, stack symbols and transitions, respectively; qin E Q is the initiM stack symbol and q/i, E Q is the finM stack symbol.",67,68
14105,3039886,"1 Each transition has the form 61 ~-~ 62, where 61,82 E Q*, 1 < 161 l, 1 < 1621 < 2, and z = e or z = a. We generally use symbols q, r, s,... to range over Q, and the symbol 6 to range over Q*.",53,54
14106,3039886,"A configuration of the automaton is a pair (6, w) consisting of a stack 6 E Q* and the remaining input w, which is a suffix of the input string v. The rightmost symbol of 6 represents the top of the stack.",38,39
14107,3039886,"The initial configuration has the form (qi~, v) , where the stack is formed by the initial stack symbol.",21,22
14108,3039886,"The final configuration has the form (qi, q/i,, e), where the stack is formed by the final stack symbol stacked upon the initial stack symbol.",26,27
14109,3039886,"The final configuration has the form (qi, q/i,, e), where the stack is formed by the final stack symbol stacked upon the initial stack symbol.",32,33
14110,3039886,"If the top-most symbols of the stack are 61, then these symbols may be replaced by 62, provided that either z = e, or z = a and a is the first symbol of the remaining input.",37,38
14111,3039886,"We now construct a finite set T~2Lrt as the smallest set satisfying the conditions: (i) {S<l} 6 7~2LR; and (ii) for every q 6 T~2LI:t and X • V, we have goto'(q, X) • T~2LR, provided goto'(q, X) # @. As stack symbols, we take the elements from I2LR and a subset of elements from (V × ~2Lrt): Q2LR = {(X,q) I 3q'[goto'(q',X) = q]} U I2LR In a stack symbol of the form (X, q), the X serves to record the grammar symbol that has been recognized last, cf.",99,100
14112,3039886,"We now construct a finite set T~2Lrt as the smallest set satisfying the conditions: (i) {S<l} 6 7~2LR; and (ii) for every q 6 T~2LI:t and X • V, we have goto'(q, X) • T~2LR, provided goto'(q, X) # @. As stack symbols, we take the elements from I2LR and a subset of elements from (V × ~2Lrt): Q2LR = {(X,q) I 3q'[goto'(q',X) = q]} U I2LR In a stack symbol of the form (X, q), the X serves to record the grammar symbol that has been recognized last, cf.",116,117
14113,11595344,"The operation of the parser is defined in terms of three operations that consult the current set of states and the current input symbol, and add new states to the chart.",23,24
14114,11595344,"For each input symbol and corresponding state set, an Earley parser performs all three operations exhaustively, i.e., until no new states are generated.",3,4
14115,11595344,"After processing the last symbol, the parser verifies that 1: 0 ---~ S. has been produced (among possibly others), where I is the length of the input x. If at any intermediate stage a state set remains empty (because no states from the previous stage permit scanning), the parse can be aborted because an impossible prefix has been detected.",4,5
14116,11595344,"The string probability P(X =g x) (of x given X) is the sum of the probabilities of all left-most derivations X => ... => x producing x from X. s The sentence probability P(S ~ x) (of x given G) is the string probability given the start symbol S of G. By definition, this is also the probability P(x I G) assigned to x by the grammar G. The prefix probability P(S g>L X) (of X given G) is the sum of the probabilities of all sentence strings having x as a prefix, P(S x) = p(s xy) yEY~* (In particular, P(S ~L e) = 1).",58,59
14117,11595344,"A constrained path starting with the initial state contains a sequence of states from state set 0 derived by repeated prediction, followed by a single state from set 1 produced by scanning the first symbol, followed by a sequence of states produced by completion, followed by a sequence of predicted states, followed by a state scanning the second symbol, and so on.",35,36
14118,11595344,"A constrained path starting with the initial state contains a sequence of states from state set 0 derived by repeated prediction, followed by a single state from set 1 produced by scanning the first symbol, followed by a sequence of states produced by completion, followed by a sequence of predicted states, followed by a state scanning the second symbol, and so on.",62,63
14119,11595344,"Delete all null productions, except on the start symbol (in case the grammar as a whole produces c with nonzero probability).",9,10
14120,11595344,"Booth and Thompson (1973) show that the grammar is consistent if and only if the probability that stochastic rewriting of the start symbol S leaves nonterminals remaining after n steps, goes to 0 as n ~ ~. More loosely speaking, rewriting S has to terminate after a finite number of steps with probability 1, or else the grammar is inconsistent.",24,25
14121,11595344,"Certainly P~(X, Y) is bounded above by the probability that the entire derivation starting at X terminates after n steps, since a derivation couldn't terminate without expanding the left-most symbol to a terminal (as opposed to a nonterminal).",35,36
14122,11595344,"Since predicted states only affect the derivation if they lead to subsequent scanning, we can use the next input symbol to constrain the relevant predictions.",20,21
14123,6831086,"x 0 is a special ""wall"" symbol, $, on the left of every sentence.",8,9
14124,5169126,"We resolve this by allowing the user to write expansion rules of the from expand( (symbol), (expansion), (variables)).",17,18
14125,1542925,"The symbol denotes a frontier node of an elementary tree, which must be replaced by the circled root of another elementary tree.",1,2
14126,1542925,"By including as a terminal symbol in this rule, we ensure that distinct elementary trees t with the same states correspond to distinct rules.)",5,6
14127,5848469,"However, in a bottom-up algorithm, we need the extra factor that indicates the probability of getting from the start symbol to the nonterminal in question, which we approximate by the prior probability.",23,24
14128,5848469,"The fastest grammar we can think of we call the terminal grammar, because it has one nonterminal for each terminal symbol in the alphabet.",21,22
14129,5848469,The nonterminal symbol indicates the first terminal in its span.,2,3
14130,5848469,"For technical and practical reasons, we actually wanted a marginally more complicated grammar, which included the ""prime"" symbol of the 6-gram grammar, indicating that a cell is part of the same constituent as its parent.",21,22
14131,18511291,"Combining Equations ( 2 ) and ( 9 ), we see that H(P) is a lower bound on the average number of bits per symbol required to encode a long string of text drawn from P: H(P) <__ lira 1Ep I(X1X2...Xn).",26,27
14132,18511291,"On the other hand, an arithmetic coding scheme (Bell, Cleary, and Witten 1990 ) using model M will encode the sequence xlx2... Xn in IM(XlX2... Xn) = r --logM(XlX2... Xn) + 11 (11) bits, where [r] denotes the smallest integer not less than r. Combining Equations ( 7 ) and ( 11 ) we see that H(P,M) is the number of bits per symbol achieved by using model M to encode a long string of text drawn from P: H(P,M) = lim llM(X1X2...Xn). (",80,81
14133,14270686,"For example, a window of three characters is too small to judge whether the symbol @ is used properly: a@b seems to be a potential OCR error, but is acceptable when it appears in an email address such as lsa@bbb.com.",15,16
14134,5686249,We can solve the problem by balancing the hierarchy: all paths that result in generating a symbol should be of the same length and all distributions should contain the same number of members.,17,18
14135,1598703,"e' is the empty string, and 'S' the start symbol.",12,13
14136,1598703,"A local region of a description is either of the following: • a non4erminal and the child non-terminals that it immediately dominates; • a non-terminal which dominates a terminal symbol (position), along with the terminal and the input segment (if present) filling the terminal position.",35,36
14137,715063,"13 For per-state conditional normalization, let Dj,a be the set of arcs from state j with input symbol a ∈ Σ; their weights are normalized to sum to 1.",22,23
14138,13259913,"These probabilities satisfy where the sum ranges over all French strings f, and failure is a special symbol not in the French vocabulary.",18,19
14139,262262,"We now show that with a suitable additional approxitn ation H(S I T) : Lr { H(n I+) -~(+,t) } + H(S) (~1) use the generic symbol ~ to denote ~ normalizing fa.ctor that norgn com, er!s counts to probabilities.",32,33
14140,67864668,An IPC classification symbol is specified according to a hierarchy of information.,3,4
14141,67864668,"The generic form of the symbol is A01B 1/00, where each component has a special meaning as defined by WIPO (2018) .",5,6
14142,1010,"A derivation of a pair of symbol sequence thus corresponds to the selection of an initial state, a sequence of zero or more transitions (writing the symbols) and a stop action.",6,7
14143,19035620,"Rule#1 sets up the expectation for the VP symbol node, which in turn sets up the expectation for the NP symbol node.",8,9
14144,19035620,"Rule#1 sets up the expectation for the VP symbol node, which in turn sets up the expectation for the NP symbol node.",21,22
14145,19035620,"NP, the first symbol node in the chain, creates the start node S. In subsequent processing, posting an instance of this start symbol would indicate an expectation to instantiate the entire chain of Rule#l, thereby detecting a nonterminal symbol S. Partial The expansion in Figure 1 constructs an LR-RTN.",4,5
14146,19035620,"NP, the first symbol node in the chain, creates the start node S. In subsequent processing, posting an instance of this start symbol would indicate an expectation to instantiate the entire chain of Rule#l, thereby detecting a nonterminal symbol S. Partial The expansion in Figure 1 constructs an LR-RTN.",25,26
14147,19035620,"NP, the first symbol node in the chain, creates the start node S. In subsequent processing, posting an instance of this start symbol would indicate an expectation to instantiate the entire chain of Rule#l, thereby detecting a nonterminal symbol S. Partial The expansion in Figure 1 constructs an LR-RTN.",42,43
14148,19035620,Each symbol node in the RTN denotes a subsequence originating from its lefimost start symbol.,1,2
14149,19035620,Each symbol node in the RTN denotes a subsequence originating from its lefimost start symbol.,14,15
14150,19035620,2) Install the next (leftward) symbol instance.,8,9
14151,19035620,"In substep (1), following selection, a rule node and its immediately downward symbol node are instantiated.",16,17
14152,19035620,Most of the INSERT action is done by instances of symbol and rule RTN nodes.,10,11
14153,19035620,"Using our INSERT method, a new symbol instance in the parse-tree links with predecessor instances, and installs itself.",7,8
14154,19035620,"If the symbol's RTN node leads upwards to a rule node, one new rule instance successor is enqueued; otherwise, not.",2,3
14155,19035620,A rule instance must instantiate and enqueue all RTN symbol nodes from which they could possibly be derived.,9,10
14156,19035620,"At most, this is the set SAME-LABEL(rule) = { N • RTN I N is a symbol node, and the label of N is identical to the label of the rule's nonterminal successor node }.",20,21
14157,19035620,"For every symbol node in SAME-LABEL(rule), instances may be enqueued.",2,3
14158,19035620,The symbol node Y has a left neighbor symbol node X in the RTN.,1,2
14159,19035620,The symbol node Y has a left neighbor symbol node X in the RTN.,8,9
14160,19035620,"Specifically, if the sentence data gives no evidence for a parse-subtree, the associated symbol node instance need never be generated.",17,18
14161,19035620,"Once an RTN node X has been instantiated in some column, it sets up an expectation for • The RTN node(s) Yg that immediately follow it; • For each immediate follower Yg, all those RTN symbol nodes Wg,h that initiate chains that could recursively lead up to Yg.",39,40
14162,19035620,"Therefore, the Follow-Set of a column's symbol nodes can be deferred to Step (3) of the BLR(0) parsing algorithm, after the determination of all the nodes has completed.",10,11
14163,19035620,"The set of symbol RTN nodes that a rule instance r spanning (j+l,k) can enqueue is therefore not SAME-LABEL(rule), but the possibly smaller set of RTN nodes SAME-LABEL(rule) n Follow-Set(j).",3,4
14164,19035620,"For every RTN node Y in Nodes, create and enqueue all instances y inY: Let X be the leftward RTN symbol node neighbor of Y. Let PROD = {x I x an instance of X in column j) x (r), if X exists; {r}, otherwise.",22,23
14165,19035620,"We have: SAME-LABEL(r) = {N 2, N 3 }, i.e, the two symbol nodes labelled N in the sequences of Rules #2 and #3, shown in the LR-RTN of Figure 6 .",20,21
14166,19035620,"All parse-sequences of RTN symbol node X that cover columns i+l through k may be collected into a single equivalence class X(i,k).",6,7
14167,19035620,The parsing merge predicate considers two instantiated sequences equivalent when: (1) Their RTN symbol nodes X are the same. (,16,17
14168,19035620,"The partitioning reduces the cache size: instead of allowing all possible subsets of the RTN, the cache graph nodes contain smaller subsets of identically labelled symbol nodes.",27,28
14169,19035620,Each cache node represents a subset of RTN symbol nodes.,8,9
14170,19035620,The numbers indicate order of appearance; the lettered nodes partition their preceding node by symbol name.,15,16
14171,19035620,"grammar symbol nodes as an already existing Follow-Set node, it is merged into the older node's equivalence class.",1,2
14172,17817178,Acts combined within a turn are joined with a plus (+) symbol.,13,14
14173,4889017,"p i are predicates in the KB's ontology and each v i is either a variable v{x,y 1 ,…,y n } or a symbol in the KB's ontology.",29,30
14174,6960240,"We assume that input predicates are not only lexical predicates, but also unresolved predicates used for, e.g., compound nominals (Alshawi, 1992) , or for unknown words, as was demonstrated in the example above, or synonymous predicates that allow us to represent two or more different words with only one symbol.",57,58
14175,4067972,A slot can be either an atomic symbol or a list of the form (SlotName Option 1 … Option n ) where the options are terms that specify conditions on the fillers of the slot.,7,8
14176,18114032,It extends Prolog with a true negation symbol and the contrapositive forms of each clause.,7,8
14177,18114032,"A default can be given ei- where p is a priority value, d is an atomic symbol with only free variables as arguments, and w is a wtf.",17,18
14178,12859640,There are not enough bits in firing frequency to allow symbol passing between individual units.,10,11
14179,12859640,It is not possible to have such symbol passing in a connectionist network.,7,8
14180,33254038,Language provides a critical test for the hypothesis that physical symbol systems are adequate to perform all human cognitive functions.,10,11
14181,14008641,"Underneath, this discussion seems to be little more than a bad version of the old, familiar anti-AI arguments about Turing tests and Chinese rooms, about the limitations of physical symbol systems, about whether one needs to distinguish between a system that is explicitly following a certain rule from one that merely acts as if it is.",34,35
14182,6522496,"set are padded with a new ""blank"" symbol to make them at least as long as their left-hand sides.",9,10
14183,6522496,"They show that, if S is the recursive symbol in the CF base, the generated language L is predictably enumerable and exponentially bounded.",9,10
14184,6522496,"Using standard notation for CF rules, one example of a metarule that could replace the Apects transformation known as ""particle movement"" is V-~ VNPtX => V-~ VPtN[-PRO]X The symbol X here behaves like variables in structural analyses of Aspects transformations.",32,33
14185,6522496,The root of the adjunct trees is labelled with a nonterminal symbol that also appears exactly once on the frontier of the tree.,11,12
14186,31519895,"After all, our heads do not apply programs on stored symbol arrays but are appropriately wired for understanding or producing language.",11,12
14187,31519895,"In my opinion, the main features to be realized in more adequate computational systems are -concurrency of localized operations (instead of centrally controlled sequential processes), and signal processing (instead of symbol manipulation).",35,36
14188,31519895,"These features cannot be represented by a program on an ordinary von Neumann machine since this type of machine is by definition a sequential,centrally controlled symbol manipulator.",28,29
14189,31519895,"iO. As a first example of the application of these methods, it has been shown in Schnelle (forthcoming) how a complex PLA network composed from AND-planes, OR-planes, ordinary registers, and shift registers can be derived by a general and formal method from any CF-grammar, such that the network generates a sequence of control signals,triggering the production of a corresponding terminal symbol (or of a string of terminal symbols).",75,76
14190,31519895,"The structure derived is a set of units, one for each non-terminal occurring in the grammar and one for each terminal symbol.",24,25
14191,31519895,A unit for a nonterminal symbol occurring to the left of an arrow in the CF gra~muar to be realized which allows m rule alternatives and occurs at n places to the right of the rule arrow has the form of figure 2a .,5,6
14192,31519895,"A unit for a terminal symbol -say ""A"" -occurring at n places to the right of an arrow has the form of figure 2b.",5,6
14193,31519895,"At the real£zations of a terminal symbol a TEST PREDICTION ""a"" is included, as indicated in figure 2b .",6,7
14194,382463,"The symbol used to code hypernyms has been @. That is to say, {peach, drupe,@} has represented ""a peach is a drupe"" or ""all peaches are drupes.""",1,2
14195,382463,A different symbol is needed to code instances.,2,3
14196,6143983,"For example, when the following sentence is given, the evaluation for ~(getphrase 'preph)""in LISP returns one symbol generated for the head prepositional phrase, ~n the machine language"", and determines the slot filler. (",19,20
14197,6143983,"The value for VTYPE-NMPs *MODIFIERS is a pair of VTYPE-NMPs and an individual verbal symbol, for example, ""(PRP-PH verb*l)"".",19,20
14198,6143983,"Those phrases are discriminated from another NMP by a pair of a delimiter ~,"" and a phrase terminal symbol, or, in particular, by proper nouns.",20,21
14199,6143983,"Therfore, the coordinate structure has ""*COORDINATE-OBJECTS"" and ""*OBJ-CAT'"" slot, each of which is filled with any instanciated NP/NHD/NMP symbol or any coordinate type, respectively.",35,36
14200,16892044,"There is an ""escape"" to the DCG formalism: A grammar symbol of the form -NT does analysis with a DCG nonterminal NT, defined by its own DCG rules. (",13,14
14201,16892044,"DCG rules are written with the symbol ---~, whereas MLG rules are written with ft.)",6,7
14202,16892044,"In order to look at right context, one can refer to the next terminal T (without removing T from the word stream) by using the symbol +-T. (Ordinary references to terminals are indicated by +T.) 3.",28,29
14203,16892044,"Also, one can examine the complete right context with a DCG nonterminal NT by use of the symbol -~r.",18,19
14204,16892044,"A symbol 1~ > Syn, where NT is a strong nonterminal, binds Syn to the syntactic structure of the phrase analyzed by ~ (and is otherwise treated like an occurrence of NT) 5.",1,2
14205,16892044,"In the top-level call, MLab is equal to the symbol top.)",12,13
14206,16892044,"The case feature of such a noun premodifier is marked by a special case symbol comb (combining form), which signals that the noun is part of a noun compound and will be given a special form by the German morphological component.",14,15
14207,16892044,Thus the definition can be given (The symbol / denotes the cut in VM/Prolog.),8,9
14208,16519273,"The symbol '="" is interpreted as the association operator ,*.",1,2
14209,22171043,The first mapping transforms dictionary headwords and their inflection class codes into LEXC entries (as sequences of symbols) e.g.: p u r k a a V02* --> p u r {kØ} {aØe} /v The transformation can be represented equivalently as a sequence of symbol pairs where the left symbol is transformed into the right symbol: p:p u:u r:r k:{kØ} a:{aØe} a:0 V02*:/v or in an abbreviated form where pairs (e..g. p:p) of identical symbols are represented by a single symbol (p) without the colon: p u r k:{kØ} a:{aØe} a:0 V02*:/v The inflection code V02* indicates that the entry is a verb of the second inflection class and that the stem is subject to consonant gradation.,52,53
14210,22171043,The first mapping transforms dictionary headwords and their inflection class codes into LEXC entries (as sequences of symbols) e.g.: p u r k a a V02* --> p u r {kØ} {aØe} /v The transformation can be represented equivalently as a sequence of symbol pairs where the left symbol is transformed into the right symbol: p:p u:u r:r k:{kØ} a:{aØe} a:0 V02*:/v or in an abbreviated form where pairs (e..g. p:p) of identical symbols are represented by a single symbol (p) without the colon: p u r k:{kØ} a:{aØe} a:0 V02*:/v The inflection code V02* indicates that the entry is a verb of the second inflection class and that the stem is subject to consonant gradation.,57,58
14211,22171043,The first mapping transforms dictionary headwords and their inflection class codes into LEXC entries (as sequences of symbols) e.g.: p u r k a a V02* --> p u r {kØ} {aØe} /v The transformation can be represented equivalently as a sequence of symbol pairs where the left symbol is transformed into the right symbol: p:p u:u r:r k:{kØ} a:{aØe} a:0 V02*:/v or in an abbreviated form where pairs (e..g. p:p) of identical symbols are represented by a single symbol (p) without the colon: p u r k:{kØ} a:{aØe} a:0 V02*:/v The inflection code V02* indicates that the entry is a verb of the second inflection class and that the stem is subject to consonant gradation.,63,64
14212,22171043,The first mapping transforms dictionary headwords and their inflection class codes into LEXC entries (as sequences of symbols) e.g.: p u r k a a V02* --> p u r {kØ} {aØe} /v The transformation can be represented equivalently as a sequence of symbol pairs where the left symbol is transformed into the right symbol: p:p u:u r:r k:{kØ} a:{aØe} a:0 V02*:/v or in an abbreviated form where pairs (e..g. p:p) of identical symbols are represented by a single symbol (p) without the colon: p u r k:{kØ} a:{aØe} a:0 V02*:/v The inflection code V02* indicates that the entry is a verb of the second inflection class and that the stem is subject to consonant gradation.,101,102
14213,22171043,"Instead, the INPUT that is needed here, consists of the name of the continuation class itself (defined as a multicharacter symbol and preceded by a space that has been quoted with a per cent sign).",23,24
14214,22171043,"The script changed the regular expressions in the definitions and in the regular expressions patterns so that any symbol pair was replaced by the output part only, e.g.: p:{pv} --> {pv} a:0 --> 0 The result was an output projection of the initial transduction.",18,19
14215,5991639,"Illis approach works, since wc can pass akmg this state encoding through the VP (via the complex non-terminal symbol VP/wh) and finally into the embedded S. This complex non-terminal is then used to trigger an expansion of eat into its transitive form.",22,23
14216,17477553,"Relationships between case-frame descriptions are indicated by attaching to each case-frame a ""link"" symbol indicating its relation to the surrounding discourse (either within that sentence, or across the preceding sentence boundary).",19,20
14217,28148522,The rules for the relevant portion of our two theories are () is our symbol for lambda abstraction): 1.,15,16
14218,13994790,"Let w be a string in a language ~ generated by a grannnar G with initial symbol S and production set P. Let B be a nonterminal not used by G. Construct a new grammar G"" with production set P"" = E U {S --> B, B --> w}.",16,17
14219,215822669,"More formally, a grammar is a quintuple G = (//, S, L, C, R), where • ,t/is a finite, nonempty set of nonterminals Nt,..., Nk • S is the set of strings over some alphabet (a fiat domain with an ancillary continuous function concatenation, notated with the symbol .). •",64,65
14220,287454,"For the operators composed of two symbols, the first symbol (""A"" or ""E"") can be thought of as quantifying universally or existentially over branches in time; the second symbol as quantifying over states within the branch.",10,11
14221,287454,"For the operators composed of two symbols, the first symbol (""A"" or ""E"") can be thought of as quantifying universally or existentially over branches in time; the second symbol as quantifying over states within the branch.",36,37
14222,287454,"Since branching is not allowed into the past, past operators have only one symbol.",14,15
14223,17712124,"The semantic rule states that the intensional logic translation of the S-constituent is compounded of the VP-translation (as functor) and the NP-translation (as operand), where the latter is first to be prefixed with the intension operator A. In general, a primed syntactic symbol denotes the logical translation of the corresponding constituent, and a double-primed symbol the logical translation prefixed with the intension operator (thus, NP"" stands for ANP').",54,55
14224,17712124,"The semantic rule states that the intensional logic translation of the S-constituent is compounded of the VP-translation (as functor) and the NP-translation (as operand), where the latter is first to be prefixed with the intension operator A. In general, a primed syntactic symbol denotes the logical translation of the corresponding constituent, and a double-primed symbol the logical translation prefixed with the intension operator (thus, NP"" stands for ANP').",69,70
14225,17712124,"For example, let V denote the semantic valuation function (with a particular interpretation and possible world understood) and let V(P) = {<a,b,c>, <a,b,d>, <e,f,g>}, V(x) = a, V(y) = b, and V(z) = d, where P is a triadic predicate symbol, x, y, and z are individual constants or variables, and a, b ..... g are elements of the individual domain D. Then V((P x)) = {<b,c>, <b,d>}, V((P x y)) = V(((P x) y) = {<c>, <d>}, and V([z V x y]) = V((((P x) y) z)) = {<>}.",73,74
14226,17712124,"This first stage merely distinguishes the formal logical roles of a lexeme, supplying a distinct (but in general still ambiguous) symbol or compound expression for each role, along with syntactic information.",23,24
14227,17436634,"One 'pure' way is to use a Prolog function symbol, say ivar, of one argument, an integer.",11,12
14228,17436634,"Constants could easily be represented using another function symbol, icon.",8,9
14229,17436634,"Application of a l-term to another is represented using the Prolog function symbol lapply, which has two argument places, the first for the function term, the second for the argument term.",14,15
14230,17436634,"Lambda abstraction is represented using a function symbol ~ with two arguments: the ~-variable, and the function body.",7,8
14231,12819449,"If the automaton receives pairs without lexical i it will remain in state 1 (the symbol =-= denotes ""any other pair"").",16,17
14232,9459309,"3 , '*' indicates the remainder symbol.",8,9
14233,9459309,"In this comparison, the remainder symbol operates as a wild card.",6,7
14234,9459309,This means that the comparator also outputs a match signal when the ~-th character memory outputs the remainder symbol.,18,19
14235,5989646,"is an empirical question if one might ever find it useful to write LP rules as in (12}, i.e., rules a < ~/, where a U 3 could be a ~ubset of a complex symbol.",39,40
14236,1182312,The symbol t is neither a terminal nor a nonterminal.,1,2
14237,1182312,"The rules necessary to give a parse tree can be stated informally (i.e., not in terms of turing machine instructions) as follows: When (I) is applied, attach V1 beneath A. When (3) is applied, attach ~he B on alpha B as the right daughter of the top,symbol on gamma.",58,59
14238,1182312,"Intuitively, what NBT does is put the symbols on alpha together in a bottom-up manner with the ultimate goal of constructing a tree that has, at its top, whatever nonterminal symbol is on top of beta.",35,36
14239,1182312,I) finds some phrase structure rule containing the symbol that is on top of alpha immediately after the arrow.,9,10
14240,1182312,"So, if the first symbol on alpha was ""det"", the phrase structure rule NP --> det AdjP N would qualify.",5,6
14241,1182312,"For this to happen, there would need to be two phrase structure rules with the same nonterminal symbol immediately after the arrow.",18,19
14242,1182312,"where : is a special symbol which is neither a terminal or a nonterminal symbol, C1 is a Ci type variable as defined earlier. (",5,6
14243,1182312,"where : is a special symbol which is neither a terminal or a nonterminal symbol, C1 is a Ci type variable as defined earlier. (",14,15
14244,1162820,"The second idea is to choose a unique and unambiguous internal representation for each character : each symbol of each processed language (including the special symbols such as ""/"", ""%"" .o.)",17,18
14245,1162820,"The table would function as follows (at input time) : in the first column, recognition of the current sy~ol of the text, and transformation of this symbol into the corresponding element (in accordance with the storage mode, i.e. adding or not the language code), in the second column.",32,33
14246,776531,"Definite Clause Grammars Given the translation of a context-free grammar G with start symbol S into a set of definite clauses G"" with corresponding predicate s, to say that a string w is in the grammar's language is equivalent to saying that the start goal S{po,pj is a consequence of G"" U W, where Po and p represent the left and right endpoints of u,, and W is a set of unit clauses that represents w. It is easy to generalize the above notions to define DCGs.",15,16
14247,776531,"First, an active edge from r to s labeled with a dotted rule {4) combines with a passive edge from s to t with label a i to produce a new edge from r to t, which will be a passive edge with label X if a i is the last symbol in the right-hand side of the dotted rule; otherwise it will be an active edge with the dot advanced over cr i. Second, the parsing strategy must place into the chart, at appropriate points, new empty active edges that will be used to combine existing passive edges.",55,56
14248,776531,"The ""scanner"" operation amounts to reduction with an input unit clause representing a terminal symbol occurrence, while the ""completer"" operation amounts to reduction with a derived unit clause representing a nonterminal symbol occurrence.",16,17
14249,776531,"The ""scanner"" operation amounts to reduction with an input unit clause representing a terminal symbol occurrence, while the ""completer"" operation amounts to reduction with a derived unit clause representing a nonterminal symbol occurrence.",36,37
14250,7690223,"To compute this expectation, we defined a root symbol 1 which matches 2 all and only trees that include mal-rules.",9,10
14251,7690223,"The parser computes the normalization factor Z r for the set of analyses 3 dominated by each root symbol, using an algorithm similar to the inside algorithm for PCFGs, but keeping track of grandparent contexts as required by the maximum entropy model in a fashion similar to that used by the selective unpacking algorithm.",18,19
14252,2375919,"Therefore, the root of an initial tree was required to be labeled by the symbol S. With the advent of lexicalized TAG and the use of the substitution operation, this assumption is no longer made (see c~3).",15,16
14253,2375919,"Thus, if the root of an auxiliary tree is labeled by a nonterminal symbol, A, then there is a distinguished node, called the foot node, in the frontier of this tree that is also labeled by A. The foot nodes of auxiliary trees, fll and /32, are indicated with an asterisk.",14,15
14254,2375919,npl refers to the subject of v2 and is labeled by the symbol NP.,12,13
14255,2375919,"Of course, with the use of the new notation, the insistence that the root and foot nodes (of auxiliary trees) be labeled by the same nonterminal symbol (as well as for the target of adjunction) is only a stipulation (and not required by the formalism).",30,31
14256,2375919,It must be the case that such a pair of quasi-nodes are labeled by the same atomic symbol (since they correspond to a single node according to the traditional definition of TAG).,19,20
14257,2375919,"Proceeding with the assumption that pairs of quasi-nodes are labeled by the same symbol we note from the definition of adjoining given in Section 2.4, it follows that for any auxiliary tree to be adjoined at this node, the quasi-root of this auxiliary tree and the quasi-foot must also be labeled the same.",15,16
14258,2375919,"If a is a quasi-initial tree, then we will use a predicate symbol ~ to represent this quasi-tree.",15,16
14259,2375919,"tn are terms and P is a n-ary predicate symbol where ¢1, ¢2 are formulae where ¢1, ¢2 are formulae.",11,12
14260,2375919,"From the discussion given in the previous section any FTAG can be stated as Pl(tl,1,..., tin,l) "": ~. "" 01 Pn (h,n,..., tm,n) ~ ¢~ where ¢1,..., Cn are formulae and t1,1,.. • tm,1, tl,n • .., tm,n are terms such that for 1 _< i,j < n, if i ¢ j then the symbol Pi ~ Pj.",85,86
14261,2375919,"Let p be an environment function and I be an interpretation mapping predicate symbols to their denotations, i.e•, if P is a n-ary predicate symbol then I maps P to some set of n-tuples of automata• Given an interpretation function I and an environment p we define ~ in the following way. .,",28,29
14262,2375919,"Then the set of structures derived by a grammar G is given by It(Grammar), where Grammar is the distinguished predicate symbol as defined earlier.",22,23
14263,8893652,The symbol t is neither a terminal nor a nonterminal.,1,2
14264,8893652,"The rules necessary to give a parse tree can be stated informally (i.e. not in terms of turing machine instructions) as follows: When (I) is applied, attach Vl beneath A. When (3) is applied, attach the B on alpha B as the right daughter of the top symbol on gamma.",56,57
14265,8893652,This is by examing the top-most symbol on stack beta of the tuple.,8,9
14266,30400936,"The meaning of such a list should be clear: The Let us indicate a feature which is essential in view of our connectionist implementation: Each piece of information in Earley's system is in fact a triple < list number, dotted symbol, length of dominance >.",44,45
14267,30400936,"The first input symbol is read into the processing space -more correctly into a connected buffer place of space VII, i.e. the unit (-2, .a.,",3,4
14268,30400936,"~. ~-~.A 5-.aft A -a , ~1 -2 -3 Figure 9 An intermediate stage occurring after reading in the first symbol Due to the connectivities in position 0 ( i.e. in space IX) the units (0, S -> .aA , 0) and (0, S -> .Ab, 0) become simuhanously active, and then, depending on them, simultanously the units (0, A-> .aa, 0) and (0, A-> .a, 0).",20,21
14269,65486,It is a symbol that denotes the object referenced by the frame statement.,3,4
14270,36164,"We achieve this by defining a process as a quintuple process= [name, expectation, focus, body, goal] The name is just a symbol used to identify the process.",27,28
14271,215762145,"In particular, the infix function symbol / will be used to form categories of the form Syn/Sem where Syn is the syntactic category of the expression and Sere is an encoding of its semantics as a logical form; the previous rule uses this notation, for example.",6,7
14272,7079142," The recta-symbol ""1-' will prefix formulas that are theorems, i.e.. that are derivable.",4,5
14273,11386125,"Thus, if the root of an auxiliary tree is labeled by a nonterminal symbol, X, then there is a node (called the foot node) in the frontier which is labeled by X. The rest of the nodes in the frontier are labeled by terminal symbols.",14,15
14274,6050262,"Then erasing the arguments of the toplevel terms gives a new rule C 0 ---,¢. Cl....C n where each c i is either a function letter or a terminal symbol.",31,32
14275,6050262,"One's intuition is that rule (1) could not occur in a natural language, because it allows arbitrarily long derivations that end with a single symbol: p(s(0)) ~ p(0) p(s(s(0))) ~ p(s(0)) ~ p(0) p(s(s(s(0)))) ~ p(s(s(0))) ~ p(s(0)) --> p(0) ,,°.",28,29
14276,6050262,"Derivations ending in a single symbol can occur in natural language, but their length is apparently restricted to at most a few steps.",5,6
14277,6050262,"1984) we could allow the grammar writer to choose a maximum depth for the terms in the backbone, and erase every symbol beyond that depth.",23,24
14278,6050262,"Let G be a unification grammar with a set of rules R, a set of terminals T, and a start symbol S. S must be a ground term.",22,23
14279,6050262,"Suppose that the unification grammar G is not depth-bounded; then there is a string a of symbols in G such that cx has arbitrarily deep parse trees in G. If t is a parse tree for a in G, let t' be formed by replacing each non-terminal f(xt...xn) in t with the symbol f. t' is a parse tree for ct in the context-free backbone, and it has the same depth as t. Therefore a has arbitrarily deep parse trees in the context-free backbone, so the context-free backbone is not depth-bounded.",62,63
14280,6050262,"An S-ranked alphabet is a pair (Y~,r) consisting of a set ~ together with a function r :Y~ -+ S* X S assigning a rank (u,s) to each symbol f in I:.",38,39
14281,6050262,"Of course the new algorithm enters an infinite loop for some unification grammars -for example, a grammar containing only the rule 1 p(M) -+ p(s(M)) In the context-free case the algorithm halts because if there are arbitrarily long chains, some symbol derives itself -and the algorithm will eventually detect this.",47,48
14282,6050262,"In a grammar with rules like (1), there are arbitrarily long chains and yet no symbol ever derives itself.",18,19
14283,6050262,"Yet we can show that if the unification grammar G contains no cyclic function letters, the result that holds for cfgs will still hold: if there are arbitrarily long chain derivations, some symbol derives itself.",35,36
14284,6050262,"If the ground grammar of G' allows arbitrarily long chain derivations, then some symbol in the ground grammar derives itself.",15,16
14285,6050262,This derivation begins and ends with the symbol s' (s(Ai)) --s'(s(Aj)).,7,8
14286,6050262,"So this symbol derives itself in the ground grammar for G', which is what we set out to prove.",2,3
14287,1650601,"= (CLOSABCB) + + C (CLOSCF R C B) + R1 + (SYNONYM R R1) B R1 A (CO~ R R1) C + ÷ (CLOSAB C A) where CLOSAB stands for Abstractive Closure and is defined in procedural logic (where the symbol < is shorthand for the reversed implication sign <--, i.e. P < Q S is equivalent to Q "" S --> P): (CLOSAB NI N2) < (OR CINST NI N2) (SUP N1 N2)) (INST N1 N2) < (OR (NI INST N2) (N1 ~* N2)) (INST N1N2) < (INST N1X)(INSTX N2) (SUP Ni N2) < (OR (Ni E~U£V N2)(Ni SUP N2)) (SUP NI N2) < (SUP NI X)(SUPX N2) CLOSCP stands for Complex Product Closure and is defined as (CLOSCP R N1N2) < (TRANSITIVE R)(NI R N2) =N1R N2 is the new A R B"" (CLOSCP R N1N2) < (NI ~OF N2)*~ (CLOSCF R N1N2) < (NI LOC N2)** (CLOSCF R NI N2) < (NI *AND N2) (CLOSCP R N1N2) < (NI *OR N2) ** These two relations turn out not to be universally true complex products; they only give answers that are possibly true, so they have been dropped for most question answering applications.",52,53
14288,9231457,"4 The symbol ""e"" stands for an empty category, or gap, in an argument position.",2,3
14289,6927513,"l ) is to relate the inlmt semantics with the start symbol of the gramnuu"" and then to try to ex-l}and this node in a top-down manner accordiug to the rules specitied in tl,e grammar.",11,12
14290,6927513,"Generation finally succeeds if the root node of I, he current syntax tree is labeled with the start symbol of the grammar and the root of the semantic analysis trec with the input semantics.",19,20
14291,6927513,"Such an extreme case would be a recursive rule for semantically empty particles: ('empty' semantics is represented by the empty list symbol El): /~ /~ _ [] (~) part part X1 X2 x Ilowcver, if we a.ssume that structures of that kind do not occur, a depth-first interpreter will be sufficient, e.g. the inference rules of the algorithm can be encoded and interpreted directly in Prolog.",25,26
14292,16780088,"Counting all possible subsets in addition to other states having to do with predictions, con|pie|ions, and the parser start symbol that some it||p[ententatioas introduce, there will be .14 states in £,. (",21,22
14293,16780088,"Dummy symbol D will be used to soak up excess input symbols, so D ~ a through D ~ d should be rules.",1,2
14294,16780088,"Dummy symbol U will also soak up excess input symbols, but U will be allowed to match only when there are four occurrences in a row of the same symbol {one occurrence for each edge).",1,2
14295,16780088,"Dummy symbol U will also soak up excess input symbols, but U will be allowed to match only when there are four occurrences in a row of the same symbol {one occurrence for each edge).",30,31
14296,16780088,"Hi II2H~II4UUDDDD Each //-symbol will match one of the endpoints of the corresponding edge, each /.r-symbol will correspond to a vertex that was left untouclted by the H-matching, and the D-symbols are just for bookkeeping. (",20,21
14297,16780088,"The gramnlar encodes the underlining procedure by requiring each //-symbol to match one of its endpoints in a. Since the expansion of the START rx, le is unordered, ,an H-symbol can match anywhere in a, hence can match any vertex name (subject to interference from previously matched rules).",34,35
14298,11691908,"Taking as an example the diagram of the copy-subject method, we see that: = − → s b j (ver b × −→ ob j) (7) where the symbol denotes component-wise multiplication and × is matrix multiplication.",36,37
14299,12080584,"AUW is an English term or special symbol (number…) possibly completed by semantic restrictions : the UW ""process"" represents all word meanings of that lemma, seen as citation form (verb or noun here), and ""process(icl>do, agt>person)"" covers only the meanings of processing, working on, etc.",7,8
14300,6122999,"The underscore symbol ""~"" is interpreted as the ""less-than"" relation among time points whereas the comma symbol .... stands for the ""teas-than-or-eQual-to"" relatmn.",2,3
14301,6122999,"The underscore symbol ""~"" is interpreted as the ""less-than"" relation among time points whereas the comma symbol .... stands for the ""teas-than-or-eQual-to"" relatmn.",22,23
14302,6122999,"Note that Hornstem also uses the term ""assoc=ation"" to refer to the comma symbol "","".",17,18
14303,6122999,"The temporal logic cons=stS of seven basic relations and their mveraes (Allen84, D.129, figure 1 ): Relation svmbol symbol for meaninQ inverse X Oefore Y < > XXX YYY X equal Y = = XXX YYY X mee~s Y m mi XXXYYY X overlaps Y o oi XXX YYY X during Y d di XXX YYYYY X starts Y s si XXX YYYY X finishes Y f fi XXX YYYY The reasoning scheme tsa form of constraint propagation in a network of event nodes hnKed by temporal relat,onsmps.",23,24
14304,6122999,"There is a straightforward :ranslat=on of Hornstein's SRE notation =nto the network re=)resenta'Jon, namely, replace each comma symbol "","" by < (or >.",24,25
14305,6122999,"witr the event symbols reverse their roles) and each underscore symbol ""~"" by > (or < with similar a¢liustment on the event symbols).",11,12
14306,35989805,"The symbol < can be read as ""is a"" and the notation {a,,... ,an}<b is an abbreviation for al<b, • • • ,an<b. The grammar writer need not distinguish between instances and classes, or between syntactic and semantic categories when the hierarchy is specified.",1,2
14307,35989805,Each node (both leaf and interior) is annotated with a symbol from the signature.,12,13
14308,35989805,"The next example includes a subterm at agreement:=>: (cat ~ np, agreement ~ (number ~ singular, person ~ third)) In this C-term the head symbol is missing, as is the head symbol of the subterm.",34,35
14309,35989805,"The next example includes a subterm at agreement:=>: (cat ~ np, agreement ~ (number ~ singular, person ~ third)) In this C-term the head symbol is missing, as is the head symbol of the subterm.",42,43
14310,35989805,"When a symbol is missing, the most general symbol of the signature (T) is implied.",2,3
14311,35989805,"When a symbol is missing, the most general symbol of the signature (T) is implied.",9,10
14312,35989805,"In C-terms, the wild card function is filled by the maximal symbol of the signature (T) which will match any C-term during unification.",14,15
14313,35989805,"If a variable appears without a subterm following it, the term consisting of simply the top symbol (T) is assumed.",17,18
14314,35989805,"That is, the GLB of the two symbols in the signature lattice becomes the head symbol of the result.",16,17
14315,35989805,"When the --> symbol is used instead of ""-, the rule is treated as a context-free grammar rule and the interpreter automatically appends two additional arguments (start and end) to facilitate parsing.",4,5
14316,26920658,"A UW is a lexical symbol denoting a (set of) acceptions (meanings), and is constructed by borrowing from English because all developers know English: it is an English term followed by formal restrictions enclosed in parentheses, for example ""chair(icl>thing)"" or ""chair(icl>do, agt>human, obj>entity)"".",5,6
14317,600719,"Rather they are connected to meanings or logical forms via ""--J, which the authors describe as an otherwise uninterpreted binary predicate symbol.",23,24
14318,600719,"The Semantics of QLF The Meaning of As was pointed out earlier, conditional equivalences of the form: if the symbol v-z is interpreted as material or logical equivalence.",21,22
14319,16138926,"LR(1)-parsing, (Knuth, 1965 ) is a well-known and highly-efficient method of shift-reduce (bottom-up) parsing that processes morphemes in a left-to-right manner using a single symbol of lookahead for local disambiguation.",41,42
14320,17413164,"A generalized rewriting system is a quadruple G = (V, s, T,R) where V is a finite set of nonterminal category symbols, s E V is the start symbol, T is a set of terminal symbols, and R C_ (V* x V*) U (V x T) is a finite set of rewriting rules and lexical insertion rules, which are usually expressed in the forms: Definition 17 • vl...vn ----* Ul""""Um where vi:uj E V • v >twherevEVandtET.",35,36
14321,17413164,"We take a basic category for every nonterminal symbol in the generalized rewriting system along with two special symbols; the # is used as a delimiter in representing sequences of nonterminals by means of circular queues, while the s is used as the distinguished category of the grammar (note that s, not s, is the distinguished start symbol of G).",8,9
14322,17413164,"We take a basic category for every nonterminal symbol in the generalized rewriting system along with two special symbols; the # is used as a delimiter in representing sequences of nonterminals by means of circular queues, while the s is used as the distinguished category of the grammar (note that s, not s, is the distinguished start symbol of G).",62,63
14323,17413164,The # symbol keeps track of the true beginning of the sequence being derived.,2,3
14324,8603929,"The usual actions of shift, reduce, and accept are augmented by hypothesised shift: shift a new item on to the stack even if no such action is specified in that state hypothesised unary reduce: reduce a symbol Y as if there was a rule X -~ Y, where the value of X is not yet determined.",40,41
14325,8603929,The value of the X symbol is determined by the next possibilities for reduction.,5,6
14326,8603929,Two more hypothesised actions are used to account for gap threading: hypothesised move: put the current symbol on a separate movement stack (i.e. hypothesise that this constituent has been fronted) hypothesised fill: move the top of the movement stack to to top of the main stack These actions have costs associated with them and a control regime so that the 'cheapest' analysis will always be preferred.,18,19
14327,30733730,"Obviously, a single hour-long session is no more than a symbolic gesture in this direction, even ff the time had not been truncated further by schedule overruns pressing against an inflexible dinner hour, but we feel that the symbol was nevertheless a worthwhile and important one.",43,44
14328,18509607,"In general, each line of slot frame output shows a filled slot frame followed by the symbol < and the frequency for that frame.",17,18
14329,18509607,"The symbol u (for ""unfilled"" or ""unknown"") in the position of the Option indicates that a slot is not filled.",1,2
14330,5693617,"Thus, for instance, the mapping which assigns to each predicate symbol a relation (of the same arity) on the universe, must be viewed as a set of mappings (one for each world), since the denotation of the predicate can vat 3, fiom world to world.",12,13
14331,153315339,"Each DCG uses a counting argument I for a non-terminal symbol to build up a stack of indices i that gives the successive number of occurrences of as, bs, and cs in a string.",12,13
14332,3043299,"The symbol nn is treated as a predicate constant, and the most common possible relations (see Levi, 1978) are encoded in axioms.",1,2
14333,3043299,"The symbol eel is treated as a predicate constant, and there are a number of axioms that specify what the possible coercions are.",1,2
14334,3043299,"Vi,j, k, x,p, args, req, e, c, rel)np(i, j, x) A vp(j, k,p, args, req) ,j,k,z)det(i,j,""a"") A cn(j,k,z,p (Vi,j,w)n(i,j,w) D (3z)cn(i,j,z,w) (Vi,j, k, w, z, c, rel)prep(i, j, w) ^ np(j, k, x) A rel(c, z) In° (V i, j, k, z)det(i, j,""the"") A cn(j, k, z, p) Ap(z) 'm D n1~i,k,z) (Vi ptXi, k, ,~z[w(c, z)], <c>, Req(w)) For example, the first axiom says that there is a sentence from point i to point k asserting eventuality e if there is a noun phrase from i to j referring to z and a verb phrase from j to k denoting predicate p with arguments arg8 and having an associated requirement req, and there is (or, for $3, can be assumed to be) an eventuality e of p's being true of ¢, where c is related to or coercible from x (with an assumability cost of $20), and the requirement req associated with p can be proved or, for $10, assumed to hold of the arguments of p. The symbol c&el denotes the conjunction of eventualities e and el (See Hobbs (1985b) In this approach, s(0, n, e) can be read as saying there is an interpretable sentence from point 0 to point n (asserting e).",288,289
14335,219307941,"Furthermore, Elman's chapter, which is framed as a reply to Fodor and Pylyshyn (1988) , demonstrates just how radical an alternative to traditional symbol-based processing connectionism can be (cf.",28,29
14336,6461701,The message (MSG): A symbol specifying the last action performed on the stack.,6,7
14337,6461701,"In general, this symbol will indicate the type of slot the last input word was inserted in.",4,5
14338,6461701,"The contribution each word makes to the development of the parse is shown to the right of the production symbol ""= ~>"".",19,20
14339,7814335,The MT Machine instances are marked with a special symbol.,9,10
14340,11637792,"So far, well-defined connectionist models all deal with relations over a finite set of elements; at least, no one seems to have shown how to apply such models systematically to the infinite sets of arbitrarily-long symbol-sequences that form the subject matter of classical automata theory.",42,43
14341,11637792,"Connectionist models can deal with sequences of symbols in at least two ways: the first is to connect the symbol sequence to an ordered set of nodes, and the second is to have the network change state in an appropriate way as successive symbols are presented.",20,21
14342,11637792,"Thus the network does not have to parse and/or interrelate the two symbol sequences, but only keep track of the conditional probability of various possible translations of a given letter, given the surrounding letter sequences.",12,13
14343,5113115,"We can note the property ""human subject"" in the following equivalent ways: (2) Nhum V N 1 or N O (:: Nhum) V N t w~ere the symbol :: is used to specify a structure .",36,37
14344,1247839,Each semantic symbol points to such a schema.,2,3
14345,1247839,"For example, the semantic symbol REALISE may translate to ""realisieren"", ""implementieren"" etc.. If the Instrument role of REALISE were filled with an instance of the PROGRAM concept, we would choose the more adequate word sense ""implementieren"".",5,6
14346,1247839,"For example, the semantic symbol ACCIDENT may translate to the German equivalent of ""accident"", ""error"", ""failure"" or ""bug"".",5,6
14347,6515223,"where Xi is an ""internal symbol"" associated with Clty(y)&By(y,Xi) , and )(j is associated with Bay(z).",6,7
14348,18163683,"When the adjacent feature of the second right-hand-side symbol in the CFG-part is nil as in the above case, it is enough just to concatenate both 'orth' feature values of the right-hand-side symbols and make it the 'orth' feature value of the left-hand-side symbol.",12,13
14349,18163683,"When the adjacent feature of the second right-hand-side symbol in the CFG-part is nil as in the above case, it is enough just to concatenate both 'orth' feature values of the right-hand-side symbols and make it the 'orth' feature value of the left-hand-side symbol.",62,63
14350,16369881,The columns having more than one symbol mean that each model performs sequentially.,6,7
14351,52119279,"The following abbreviations are used in the specifications of the schemes: Ana for anaphoric, Det for determiner, Adj for adjective, Pro-Subj for a subject pronoun (referring to an eventuality); the symbol $N$ stands for a variable whose value is given in the lexical head field; optionality is marked with parenthesis and alternatives are written within brackets.",39,40
14352,521075,"As an illustration, consider the pattern in (4a), in which the symbol Ω matches any nonempty sequence of tokens.",15,16
14353,11009209,Our use of the symbol ^ is quite different from that of Montague.,4,5
14354,14288879,"In won/tagging, each of the rurmi.$ words in the coqms text to be processed is associated with a pre-termina/ symbol, denoting word class.",24,25
14355,1069475,E. Types of Inference Rules I. Rules operatin@ on a single TR: (i) the structure of the tree is preserved; the transformation concerns only (a) part(s) of the .o..p~ex symbol of some node of the CDS (i.e. label(s) of some node(s)in the Q-tree of the TR): (ii) a whole subtree is replaced by another subtree: Ex.:,40,41
14356,1069475,"Among other things, the following issues should be taken into consideration: The rules substituting subtrees for subtrees are used rather frequently, as well as those substituting only a label of one node (in the Q-tree, i.e. one element of the complex symbol in the CDS), preserving the overall structure of the tree untouched.",48,49
14357,6896968,"It is a wellestablished convention to mark the ""stress-shifting"" morpheme boundary with a ""+"" symbol and to mark the ""stress-neutral"" boundary with a ""#"" symbol. (",20,21
14358,6896968,"It is a wellestablished convention to mark the ""stress-shifting"" morpheme boundary with a ""+"" symbol and to mark the ""stress-neutral"" boundary with a ""#"" symbol. (",36,37
14359,8493965,"The symbol • is used for a zero pronominal, and its translation equivalent appears in []. (",1,2
14360,67239762,"Also associated with the NIKL system, though not a part of the core language definition, is a symbol table which associates atomic names with the roles or concepts they denote, and concepts and roles with the names denoting them.",19,20
14361,67239762,"If a concept or role does not have a name, the symbol table is able to create and install one for it when demanded.",12,13
14362,67239762,"Similarly, the NIKL system's creating a new concept or role, and creation of a name in the symbol table to stand for it, furnishes us with a new non-logical constant.",20,21
14363,67239762,"The symbol ""<rest>"" denotes a (possibly empty) sequence of formulae.",1,2
14364,67239762,"The symbol ""<quant>"" is to be understood as being replaced by either the operator SETOF or the quantifier EXISTS.",1,2
14365,12981866,We shall use the symbol )4+ to denote this set of terms.,4,5
14366,12981866,"Clauses are written backwards and the symbol :-is used for C. There are, however, some differences.",6,7
14367,12981866,"We need to represent A-abstraction in our language, and we use the symbol \ for this purpose; i.e. AX A is written in AProlog as X \ A. The following program, which defines the operation of mapping a function over a list, illustrates a use of function variables in our language.",15,16
14368,12981866,"An example of these mixed uses of predicate variables is provided by the following set of clauses; the logical connectives A and V are represented in AProlog by the symbols • and ;, true is represented by true and Z is represented by the symbol sigma that has the polymorphic type (A -> O) -> O. The first three clauses define the predicate sublist whose first argument is a predicate and is such that (sublist P L K) is provable if K is some sublist of L and all the members in K satisfy the property expressed by the predicate P. The fourth clause uses sublist to define the predicate have_age which is such that (have_age L K) is provable if K is a sublist of the objects in L which have an age.",46,47
14369,12981866,"Thus the constant man corresponds to two distinct constants, one of type token and another of type i -> b. We have also used the symbol iota that has type (i -> b) -> i. This constant plays the role of a definite description operator; it picks out an individual given a description of a set of individuals.",27,28
14370,5309847,It is saturated when the value of ARGUMENTS is 'closed' with symbol #.,13,14
14371,9612476,"3.1) Among the possible interpretations of this disposition, we shall focus our attention on the following (the symbol rd denotes a restoration of a disposition): rd I A most young men like most young women rd 2 A most young men like mostly young women .",20,21
14372,12792759,"When an attribute has a value-expression which stands for the empty set, the complex category symbol that contains this valueexpression fails to label a possible dcu.",18,19
14373,12792759,"If A, B, C, Y, Z stand for complex category symbols (including attribute/value-expression pairs) and Idcul]¥ and [dcu2]z are legitimate dcu's, the rule ""A => B C"" legitimizes the dcu ~([[dcul]y [dcu2]z]A ) iff the substitution ~ is the most genera/unifier I of the terms <B,C> and <Y,Z>, and G(A) is a legitimate complex category symbol, not containing empty attribute-value expressions.",84,85
14374,12792759,"Notation: Category symbols have the form ""cat [~t:o~t ...... ~n:(~n]"", where ""cat"" is the basic non-terminal symbol of the context-tree grammar, ~t ..... ~n are the attributes, and ~l ..... ~n are expressions which stand for the value-sets of these attributes.",29,30
14375,12792759,"The grammar does not have a particular start symbol; dcu's of all kinds are recognized as well-formed ""discourses"".",8,9
14376,10991030,"A value may be an atomic symbol, hut it may also be a nested feature structure.",6,7
14377,15380152,"Each node of a derivation tree is labeled by a rewriting rule, and in particular, the root must be labeled with a rule with the starting symbol as its left hand side.",28,29
14378,1906653,"SPLITTING RHEMES ( f , f"" ) fram~ f is alpha ancestor to f"" DESCENDING RHEMES ( f , f"" , f'"" ) frame-'f is alpha ancestor to f"" & frame f"" is alpha ancestor to f'"" ~KEN CONSTANT THEME ( f , str ) frame f is beta ancestor=~strlng str SPLITTING THEMES ( f , f', str) fram~ f is alpha ancestor to f"" & frame f"" is beta ancestor to string str CASCADING THEMES ( f , f', f'' , f' '"" , sir ) fram-e f is alpha ancestor f"" & frame f"" is beta ancestor to f'"" & frame f'"" is alpha ancestor to f''"" & frame f''"" is beta ancestor to string str SEPARATOR ( f ) frame f is alpha ancestor to a separator symbol We now illustrate the operation of the word expert designed to handle special cases of text coherence (App.",161,162
14379,58094,For convenience the components of the fielded records which decorate the arcs are separated using the slash symbol.,17,18
14380,798697,"When Xi assumes a word or a special symbol ""0"" as its value, we refer to the corresponding model as a word-based model.",8,9
14381,7972355,The language is defined to be the set of strings on the frontiers of trees whose roots are labeled by a distinguished symbol S. It is easy to see that the set of languages generated by this tree rewriting grammar is exactly the same set as context-free languages.,22,23
14382,7972355,"where a is a tree, dot is the address of the dot in the tree, side is the side of the symbol the dot is on (left or right), pos is the position of the dot (above or below), star is an address in a and l, f~, fr, star, t~, b~ are indices of positions in the input string.",23,24
14383,219302977,"t h e symbol -0 , 91n practice one cannot write a length-0 string, and one cgnnot distinguish a length-?",3,4
14384,219302977,"In the *selnan$ic representations' given here, I arrange the predicate-symbol t o the left of all the arguments, in order to clarify the c~mparison with standard l o g i c a l notation.",14,15
14385,7062713,"1 , where syn, agr, sg, and 3rd are type symbols; agree, hum, per, and subj are feature symbols; and X is a tag symbol.",32,33
14386,7062713,"is the concatenation operator, is said to have sg as its type symbol.",13,14
14387,7062713,"The root feature-address is de- A typed feature, structure is also represented by a rooted, connected, directed graph within which each node corresponds to a typed feature structure and is labeled with a type symbol (and, optionally, a tag symbol) and each arc corresponds to a feature.-value pair and is labeled with a ti'~ature symbol.",39,40
14388,7062713,"The root feature-address is de- A typed feature, structure is also represented by a rooted, connected, directed graph within which each node corresponds to a typed feature structure and is labeled with a type symbol (and, optionally, a tag symbol) and each arc corresponds to a feature.-value pair and is labeled with a ti'~ature symbol.",47,48
14389,7062713,"The root feature-address is de- A typed feature, structure is also represented by a rooted, connected, directed graph within which each node corresponds to a typed feature structure and is labeled with a type symbol (and, optionally, a tag symbol) and each arc corresponds to a feature.-value pair and is labeled with a ti'~ature symbol.",63,64
14390,7062713,"A typed feature structure tl is less than or equal to tu (written as tl <, in) if and only if tt is iuconsistent (that is, if it includes the type symbol ]_) or (i) t~ 's type symbol al is less than or equal to t~'s type symbol a2 (a~ _<7 ap.); (",36,37
14391,7062713,"A typed feature structure tl is less than or equal to tu (written as tl <, in) if and only if tt is iuconsistent (that is, if it includes the type symbol ]_) or (i) t~ 's type symbol al is less than or equal to t~'s type symbol a2 (a~ _<7 ap.); (",47,48
14392,7062713,"A typed feature structure tl is less than or equal to tu (written as tl <, in) if and only if tt is iuconsistent (that is, if it includes the type symbol ]_) or (i) t~ 's type symbol al is less than or equal to t~'s type symbol a2 (a~ _<7 ap.); (",58,59
14393,7062713,Then Unify_A*tx calculates the meet of the type symbol.,8,9
14394,1021605,"It shows that if there is a symbol sequence composed of a tree not-V(erb), a tree N(oun) P(hrase), a tree C(ase-particle), and a tree not-A(dverbial)-P(article) in this order, this is transformed into a tree NP-C. The right side of rewriting rules is a matching pattern which is to be found in the given input symbol string.",7,8
14395,1021605,"It shows that if there is a symbol sequence composed of a tree not-V(erb), a tree N(oun) P(hrase), a tree C(ase-particle), and a tree not-A(dverbial)-P(article) in this order, this is transformed into a tree NP-C. The right side of rewriting rules is a matching pattern which is to be found in the given input symbol string.",71,72
14396,1423474,"We show that any categorial language is a permutation of some context-free language, thus inheriting properties dependent on symbol counting only.",21,22
14397,1423474,"Complex category symbols whose left-most symbol is S (symbols ""headed"" by S) are denoted by Xs, Ys.",7,8
14398,1423474,"This lexicon has the property that any derivable category symbol, either has exactly one S and is Sheaded or does not have an occurrence of S. Hence in x, #S = 1, i.e., w has exactly one e. Let the number of occurrences in x of C/A/C/B and C/D be p and q respectively. ]",9,10
14399,1423474,"5 The second generates only the empty language, since no atomic symbol can be derived using only these two rules.",12,13
14400,1423474,This will enable us to show that properties of context-free languages that depend only on the symbol counts must also hold of categorial languages.,18,19
14401,1423474,6 I-1 Proposition Any one--symbol categorial grammar is regular.,6,7
14402,5189241,"Ad<lod Co that was Clio silence symbol in initial and final position, which adds a, lioChor 70 phoneniic [)aii:s, [gl'OIH this iniCial sol;, l, he pairs of se.lni-vowels wcrc relnow;d.",8,9
14403,5189241,"For handling symbol mapping, a program was written that converts any set of characters to any other set of characters I. The program is developed so that characters coded in octal or decimal code not only can he translated in either code, but also can be input in ascii format for being converted 2 Quite often, there was more than one pronunciation in the phonetic field and the.",2,3
14404,2533830,A dotted symbol is defined as a symbol associated with a dot above or below and either to the left or to 279 the right of it.,2,3
14405,2533830,A dotted symbol is defined as a symbol associated with a dot above or below and either to the left or to 279 the right of it.,7,8
14406,2533830,A dotted tree is defined as a tree with exactly one dotted symbol.,12,13
14407,2533830,"A Adjunction Prediction Move Dot Up Move Dot Down • A transition on a (where a is a terminal symbol) from Si to Sj occurs if and only if in Si there is a dotted tree [6, dot, la, stars] in which the dot is to the left and above a terminal symbol a; Sj consists of the closure of the set of dotted trees of the form [6, dot, ra, stars] . •",20,21
14408,2533830,"A Adjunction Prediction Move Dot Up Move Dot Down • A transition on a (where a is a terminal symbol) from Si to Sj occurs if and only if in Si there is a dotted tree [6, dot, la, stars] in which the dot is to the left and above a terminal symbol a; Sj consists of the closure of the set of dotted trees of the form [6, dot, ra, stars] . •",59,60
14409,14430950,"Some sample rules for the LDOCE machine-readable source, marking the beginning and end of font changes, or making explicit special print symbols, are shown below (to facilitate readability, (*AS) represents the hexadecimal symbol x'AS').",42,43
14410,14430950,"Superfluous font control characters can be simply deleted, when they follow or precede certain data-can'ying tokens which also incorporate typesetting information (such as a homogra.ph superscript symbol or a pronunciation marker indicating the be~finning of the scope of a phonetic font): rk font!",30,31
14411,14430950,"For instance, spurious occurrences of a font marker before a print symbol such as an opening parenthesis, which is not affected by a font dec-' laration, clearly cannot be removed by a retokenization rule font!",12,13
14412,10309583,"However, sentences like the following, which in Pierrehumberts' terms bear a single intonational phrase, are much more ambiguous as to the division that they convey between theme and rheme: The symbol E is a variable ranging over syntactic categories that are (leftward-or rightward-looking) functions into S. al The rule is nondeterministic, so it correctly continues to allow a further analysis of the entire sentence as a single Intonational Phrase conveying the Rheme.",35,36
14413,3052195,"It o o is defined as an 8-tuple T(~ = (q, E, A, II, 6, q, $, F) where: Q is the set of states, ]E is the set of input word symbols, & is the set of stack symbols, II is the set of output symbols (i.e. rules of G), ~l is the initial state, $ is the initial stack symbol, F is the set of final states, 6 is a finite set of transitions of the 4Implementations usually denote these rules by their index in the set II.",81,82
14414,3052195,"Let the PDT be in a configuration p = (p Aa a~ u) where p is the current state, Aa is the stack contents with h on the top, ax is the remaining input where the symbol a is the next to be shifted and x E ~E*, and u is the already produced output.",40,41
14415,3052195,"The application of a transition r = (p A a ~ q B v) results in a new configuration p' = (q Ba x uv) where the terminal symbol a has been scanned (i.e. shifted), A has been popped and n has been pushed, and v has been concatenated to the existing output u. If the terminal symbol a is replaced by e:~ in the transition, no input symbol is scanned.",33,34
14416,3052195,"The application of a transition r = (p A a ~ q B v) results in a new configuration p' = (q Ba x uv) where the terminal symbol a has been scanned (i.e. shifted), A has been popped and n has been pushed, and v has been concatenated to the existing output u. If the terminal symbol a is replaced by e:~ in the transition, no input symbol is scanned.",66,67
14417,3052195,"The application of a transition r = (p A a ~ q B v) results in a new configuration p' = (q Ba x uv) where the terminal symbol a has been scanned (i.e. shifted), A has been popped and n has been pushed, and v has been concatenated to the existing output u. If the terminal symbol a is replaced by e:~ in the transition, no input symbol is scanned.",78,79
14418,3052195,B) is replaced by e~ then no stack symbol is popped from (resp.,9,10
14419,3052195,"Using the terminology of [2] , the algorithm builds an item set Si successively for each word symbol xi holding position i in the input sentence x. An item is constituted of two modes of the form (p A i) where p is a PDT state, A is a stack symbol, and i is the index of an input symbol.",19,20
14420,3052195,"Using the terminology of [2] , the algorithm builds an item set Si successively for each word symbol xi holding position i in the input sentence x. An item is constituted of two modes of the form (p A i) where p is a PDT state, A is a stack symbol, and i is the index of an input symbol.",55,56
14421,3052195,"Using the terminology of [2] , the algorithm builds an item set Si successively for each word symbol xi holding position i in the input sentence x. An item is constituted of two modes of the form (p A i) where p is a PDT state, A is a stack symbol, and i is the index of an input symbol.",65,66
14422,3052195,"The meaning of an item U = ((p A i) (q n j)) is the following: • there are computations of the PDT on the given input sentence that reach a configuration pt where the state is p, the stack top is A and the last symbol scanned is xi; • the next stack symbol is then B and, for all these computations, it was last on top in a configuration p where the state was q and the last symbol scanned was xj; • the rule sequences in l-I* derivable from U in the grammar are exactly those sequences output by the above defined comput~:tions of the PDT between configurations p and p~. In simpler words, an item may be understood as a set of distinguished fl'agments of the possible PDT computations, that are independent of the initial content of the stack, except for its top element.",54,55
14423,3052195,"The meaning of an item U = ((p A i) (q n j)) is the following: • there are computations of the PDT on the given input sentence that reach a configuration pt where the state is p, the stack top is A and the last symbol scanned is xi; • the next stack symbol is then B and, for all these computations, it was last on top in a configuration p where the state was q and the last symbol scanned was xj; • the rule sequences in l-I* derivable from U in the grammar are exactly those sequences output by the above defined comput~:tions of the PDT between configurations p and p~. In simpler words, an item may be understood as a set of distinguished fl'agments of the possible PDT computations, that are independent of the initial content of the stack, except for its top element.",63,64
14424,3052195,"The meaning of an item U = ((p A i) (q n j)) is the following: • there are computations of the PDT on the given input sentence that reach a configuration pt where the state is p, the stack top is A and the last symbol scanned is xi; • the next stack symbol is then B and, for all these computations, it was last on top in a configuration p where the state was q and the last symbol scanned was xj; • the rule sequences in l-I* derivable from U in the grammar are exactly those sequences output by the above defined comput~:tions of the PDT between configurations p and p~. In simpler words, an item may be understood as a set of distinguished fl'agments of the possible PDT computations, that are independent of the initial content of the stack, except for its top element.",91,92
14425,3052195,"Each item is represented by its two modes as (Kh Kh,) without giving the internal structure of modes as a triples (PDT-state × stack-symbol × inputindex).",31,32
14426,3052195,"Ignoring the output, an item (Kh K^,) represent the set of PDT configurations where the current state is p~,, the next input symbol to be read has the index ih + 1, and the stack content is formed of all the stack symbols to be found in the first mode of all items of any chain of items beginning with (Kh Kh,).",28,29
14427,3052195,"standing for one unknown word symbol, and ""*"" standing for an unknown sequence of input word symbols ~. Normally a scanning transition, say (p e a ~ r e z), is applicable to ~tx~ item, say U = ((p A i) (q B j)) in ,-qi, only when a == xi+l, wlmre xi+, is the next input symbol to be shifted.",5,6
14428,3052195,"standing for one unknown word symbol, and ""*"" standing for an unknown sequence of input word symbols ~. Normally a scanning transition, say (p e a ~ r e z), is applicable to ~tx~ item, say U = ((p A i) (q B j)) in ,-qi, only when a == xi+l, wlmre xi+, is the next input symbol to be shifted.",76,77
14429,3052195,When the next input symbol to be shifted is xi+l = ? (,4,5
14430,3052195,"i.e. the unknown input word symbol), then any scanning transition may 6Popping transitions are also the critical place to look at for ensuring O(n a) worst ease complexity.",5,6
14431,3052195,be applied as above independently of the input symbol required by the transition (provided that the transition is applicable with respect to PDT state and stack symbol).,8,9
14432,3052195,be applied as above independently of the input symbol required by the transition (provided that the transition is applicable with respect to PDT state and stack symbol).,27,28
14433,3052195,"When the next input symbol to be shifted is x~+l = * (i.e. the unknowlt input subsequence), then the algorithm proceeds as for the unknown word, except that the new item V is created in item set 8~ instead of b'i+l, i.e. V = ((r A i) (q B j)) in the case of the abow; example.",4,5
14434,3052195,"Thus, in the presence of the unknown symbol subsequence *, scanning transitions may be applied any number of times to the same computation thread, without shifting the input stream s .",8,9
14435,3052195,"Scanning transitions are also used normally on input symbol xi+2 so as to produce also itetns in ,S~+:, for example the item ((r A i+2) (q B j)), assuming a =--xi+~ in the case of the above example 9.",8,9
14436,3052195,There is a remaining difficulty due to tile fact that it may be hard to relate a parse sequence of rules in II to the input sentence because of the unknown nmnber of input symbol actually assumed for all occm'rence of the unknown input subsequence.,34,35
14437,3052195,"In such a parse sequence, the symbol * is included a number of times equal to the assumed length of the corresponding unknown input subsequcnce(s) for that parse (cf.",7,8
14438,3052195,"Hence the output parse grammar ~ produced by our algorithm may be simplified by replacing with the unknown subsequence terminal *, all nonterminals (i.e. items) that deri,e only on (occurrences of) this symbol.",39,40
14439,3052195,"This does not lead to an infinite computation since ohly a finite number (proportional to i) of distinct items can be built in 8~. SWe assume, only for simplicity of exposition, that * is followed by a normal input word symbol.",44,45
14440,3052195,"The output parse sequences would then simplify into a single occurrence of the symbol * qualified by the initial nonterminal I~ of the ]augusta grammar G. the structure of parse forests [4] , and to develop optimization strategies.",13,14
14441,17581953,"A basic category is of the form Head, where Head is an atomic symbol (n(oun), np or s(entence)).",14,15
14442,17581953,"2.3 If P and Q are elements of PARSE, (P+imQ) is a member of PARSE, where I~ is a new symbol} 3.",24,25
14443,17581953,"I In 2.3 the index i is just introduced for notational convenience and will not be used ; k,l.., will denote a valency name or the symbol m. The proof of the theorem is based on the following properties 1 to 3 of the grammar.",30,31
14444,16128307,"The double arrow =¢, denotes a left-most-symbol rewriting Ba =e~ Cfla, using a non-e rule B ---, Cfl.",12,13
14445,16128307,"In the first step, we observe that ambiguity often arises locally: given a certain context C[-], there might be several parse subtrees tl...tk (all deriving the same substring xi+l...xj from the same symbol A) that fit in that same context, leading to the parse trees C[tl], eft2] ..... c [th] for the given string zl...zn.",41,42
14446,16128307,a mapping ¢~ : T~ + ~ V associating a grammar symbol to each number.,11,12
14447,16128307,"Note that 0 is not associated to a symbol in V and is not a possible element of succ,,(k).",8,9
14448,219310292,A packet of grammar r i ~l c s woulcl tlien be explicitly assoclated witli each symbol on tlie right hand sicle of each phrhse structure rule.,16,17
14449,9177876,"The symbol '=~' can now be understood as a textual-replacement ""macro"" operator.",1,2
14450,18779279,The function TYPEOF takes an expression and returns its type; it returns the symbol NULL-SET if the expression is semantically anomalous.,14,15
14451,17429728,"We write the grammar as as a four-tuple (N, E, P, S), where N is the set of non-terminals, E the set of terminals, P the set of production rules, and S 6 N the start symbol.",49,50
14452,17429728,"We assume that only one rule, rule O, of G rewrites the start symbol S. The length of the right-hand side of rule i is given by M~ -1.",15,16
14453,17429728,"We adopt restrictions on NPDA's similarly to [2] , the main one being that one or two symbols be pushed on the stack in a singie move, and each stark symbol is removed when it is read.",34,35
14454,17429728,"If two symbols &re pushed on the sta~k, the bottom one must be identical to the symbol that is removed in the same transition.",20,21
14455,17429728,"The non-terminals of the Lang grammar are the start symbol 3 and four-tuple entities (Lang's 'items') of the form < q, c~,p, ~ >, where p and q axe states, and cr and stack symbols.",11,12
14456,17429728,"The idea is that iff there exists a computation that consumes input symbols zi..zj, starting at state p with a stack ~0 (the leftmost symbol is the top), and ending in state q with stack ~0, and if the stack fl(o does not re-occur in intermediate configura~ tions, then < q,a,p,~ >---"" z~..zj.",28,29
14457,17429728,"Thus, rules are of the form A -* a6, where we use the symbol 6 as a variable over possibly empty sequences of terminals, and a denotes a possibly empty sequence of at most two non-terminals.",16,17
14458,17429728,"BD8 The start symbol of the new grammar, which can be seen as a parametrized version of the tadpole grammar, is defined to be < S, qo >.",3,4
14459,17429728,"The predict set o(A) then is obtainabh as • (s) = {Cl3.~,v,,(a ~. firstnonts(A)A D --CA6)} u {qolS E firstnonts(S)}, where S is the start symbol.",35,36
14460,2167791,"Roles are like variables, and each role can have a pair <a, d> as its value, where the label a is a member of a finite set L = {al,a2,...,at} and the modifiee d is either 1 < d < n or a special symbol nil.",55,56
14461,2167791,"We define the following four functions to represent the various aspect of the role x, assuming that x is an rj-role of the word i: • pos(x)~ f the position i • rid(x)~ r the role id rj • lab(x)d-~ f the label of x • mod(x)d-~ f the modifiee of x We also define word(i) as the terminal symbol occurring at the position i. 1 An individual grammar G =< ~, R, L, C > in the CDG theory determines a set of possible assignments of a given sentence, where • ~ is a finite set of terminal symbols. •",65,66
14462,2167791,"If one introduces context sensitivity by attaching augmentations and controlling the applicability of the production rules, different readings of the same string with the same nonterminal symbol have to be represented by separate edges, and this may cause a combinatorial explosion.",27,28
14463,16688636,"A typical CS production is: ~h A p~ --~ p, 1~ ~h where p,, p~, 13 are strings Of symbols, and A is a non-terminal symbol.",33,34
14464,16688636,"Afterwards the parser returns the graph, kom which ~1 parse trees satisi~ing the tbllowing conditions are extracted: a node covers the entire sentence and Its category Is the root symbol of the grmnmar.",33,34
14465,2878037,"The simplest explanation is achieved in CNL0, where every symbol is transformed into a separate sentence, with an additional sentence at the end giving the reason why a violation occurred.",10,11
14466,2878037,"The core features of the scripting language are: Explaining events: Rather than give a complete sentence for each event represented by a symbol, we split the information into the subject and predicate, enabling us to derive automatically when sequential actions share a subject (thus allowing their combination in a more readable form).",24,25
14467,2878037,"We allow not only for the description of the symbol in this context, but also an explanation of what went wrong and, if relevant, where the responsibility lies: ERROR_EXPLAIN { [<u>|<d>]: { blame: ""due to a lift controller malfunction""; error_reason: context: { default: """"; [<o>[<r1>|<r2>|<r3>|<r4>]]_: ""the lift cannot move with open doors""; } } } Document structure: A way is needed to know how to structure the document by stating how sentences should be formed and structured into paragraphs.",9,10
14468,2878037,"Apart from simply explaining every symbol as a complete sentence, the users also managed to create scripts involving aggregation and summarisation.",5,6
14469,24663,"On the other hand, structure (23b) requires a load of 2 • xrR PLUs since 1) the noun phrase the horse is in a position that can receive a thematic role, but currently does not and 2) the operator Oi is in a position that may be associated with a thematic role, but is not yet sI will prefix sentences that are difficult to parse because of memory limitations with the symbol ""#"".",78,79
14470,5471557,"In looking for a NP VP sequence in ""the happy blageon su'mpled the bait"", however, we can't initially find a complete (initial) NP or (final) VP and hence don't know where in the chart these phrases meeL We express this by <Need NP from 0 to *, VP f~om * to 6>, the symbol ""*"" denoting a position in the chart that remains to be determined.",69,70
14471,5471557,"S, E, the si and the e~ ate positions in the chart (or the special symbol ""*~).",18,19
14472,15821418,The second form of production indicates that if a nonterminal A has a stack containing a sequence a then it can be rewritten to a terminal symbol a. The language generated by a LIG is the set of strings derived from the start symbol with an empty stack.,26,27
14473,15821418,The second form of production indicates that if a nonterminal A has a stack containing a sequence a then it can be rewritten to a terminal symbol a. The language generated by a LIG is the set of strings derived from the start symbol with an empty stack.,43,44
14474,15821418,"For example, the terminal Fm indicates the use of the forward composition rule z/y yllzII2... ImZm and (c, a) indicates the lexical assignment, c to the symbol a. We use one nonterminal, P. An input al...an is accepted if it is the case that ((S, e), -) 6 L [1, n][0, 0] .",34,35
14475,15821418,"Type I Production (inverse of la, 3, and 4a) If for some k such that i _ k < j, some a, 13 such that ~' = a/3, and B E VN we have ((A, a/B), T) E L [i, k][p, q] and ((B,/3), -) E L[k + 1, j][0, 0] then let p be the production • Type $ Production (inverse of lb and 2a) If for some k such that i < k < j, and a,B,T',r,s,k we have ((A,a/B),T') E L [i,k][r,s] where (p,q) = (i, k), ((B, ~'), -) e L[k + 1, j][0, 0], T =/B, and the lengths of a and a' meet the requirements on the corresponding strings in case lb and 2a of the recognition algorithm then then let p be the production A, a/B, i, k)(A, a', i P[..( * Type 5 Production If j = i, then it must be the case that T = -and there is a lexical assignment assigning the category As / to the input symbol given by at.",256,257
14476,14021385,"Other dimensions that enter into this validation method arise from the heterogeneity of contexts in which the grammar can be used, including (5) multiple NLP processing systems (including at least the LKB (Copestake, 2002) and PET (Callmeier, 2000) ); (6) bi-directionality of processing with attendant demands on each of the principal dimensions above for both parsing and generation; (7) multiple configurations of the grammar, including variants of preprocessing, unknown-word handling, root (start symbol) constraints, chart packing (for either parsing or generation, or both), and storage of the lexicon as text file vs relational database; (8) stochastic parse/realization selection or disambiguation (Oepen et al.,",96,97
14477,15650117,"An attribute/value notation for feature constraints is provided for the grammar writer, but this notation is compiled into ordinary term structures by assigning, for each major category symbol, an argument position for each feature that can occur with that category.",31,32
14478,1356957,"For example, atomic formulas in NFLT are constructed of a base-predicate and a set of rolemark-argument pairs, as in the following translation of Tom works in Boston: The explicit representation of roles permits each predicate-and function-symbol in NFLT to take a variable number of arguments, so that different occurrences of a verb are represented with the same predicate-symbol, despite differences in valence (i.e. number and identity of attached complements and adjuncts).",46,47
14479,1356957,"For example, atomic formulas in NFLT are constructed of a base-predicate and a set of rolemark-argument pairs, as in the following translation of Tom works in Boston: The explicit representation of roles permits each predicate-and function-symbol in NFLT to take a variable number of arguments, so that different occurrences of a verb are represented with the same predicate-symbol, despite differences in valence (i.e. number and identity of attached complements and adjuncts).",71,72
14480,8819300,"Sikkel 1997) , with minor changes to adapt it to dependency grammar (for example, the SCANNER always moves the dot over the head symbol * , rather than over a terminal symbol).",26,27
14481,8819300,"Sikkel 1997) , with minor changes to adapt it to dependency grammar (for example, the SCANNER always moves the dot over the head symbol * , rather than over a terminal symbol).",34,35
14482,8819300,"A deduction system describing the transitions of the parser is defined by Nivre, Hall, and Nilsson (2004) , with the following set of rules that describes transitions between configurations (we use the symbol ρ for a stack and the notation ρ :: h for the stack resulting from pushing h into ρ, and β i to represent a buffer of the form w i . . .",37,38
14483,8819300,"The legal combinations of structures for gap degree k will correspond to strings where symbols a and b each appear at most k + 1 times, g appears at most k times and is not the first or last symbol, and there is no more than one consecutive appearance of any symbol.",40,41
14484,8819300,"The legal combinations of structures for gap degree k will correspond to strings where symbols a and b each appear at most k + 1 times, g appears at most k times and is not the first or last symbol, and there is no more than one consecutive appearance of any symbol.",53,54
14485,8819300,"Computational Complexity of MG k Because the string used to generate a COMBINER step can have length at most 3k + 2, and the resulting step contains an index for each symbol of the string plus two extra indices, the MG k parser has complexity O(n 3k+4 ) with respect to the length of the input.",32,33
14486,8819300,"Therefore, we assume that strings are extended with this symbol.",10,11
14487,1916312,The traditional technique for expressing these constraints in a FUG is to define a label for each non terminal symbol in the ~stem.,19,20
14488,15330234,"Throughout this paper, lines beginning with the symbol ** are entered by the user and the following line(s) are the computer response.",8,9
14489,1559535,"The symbol ""o"" shows a possible predicate, and ""x"" means that the possibility has been ruled out.",1,2
14490,6720636,"In LTAG, the root and foot of auxiliary trees must be labeled by the same nonterminal symbol.",17,18
14491,6720636,"Finally, we assume the language includes the equality symbol.",9,10
14492,6720636,"Although feature structures will be used in the linguistic examples presented in Section 4, for the remainder of this section we will assume that each node is labeled with a symbol by the function label.",31,32
14493,6720636,Both of the previous two examples can be extended to give a grammar for strings containing an equal number of any number of symbols simply by including additional components in the elementary d-trees for each symbol to be counted.,37,38
14494,12041769,"Introduction Referring expression generation is a muchexplored task within natural language generation: given an internal symbol that corresponds to an entity in some real or imagined world, we need to work out what properties of that entity should be used to describe the entity so that our hearer will be able to identify it as the intended referent.",16,17
14495,12041769,"Many different algorithms have been developed to address this task, which is generally conceived of as mapping from a symbol-effectively, a referentto a set of properties-a sense.",20,21
14496,12041769,"These algorithms assume that they are given some symbol that corresponds to the intended referent, and then attempt to determine what content should be used to identify this intended referent.",8,9
14497,12041769,"This model is incompatible with the approach proposed here, since the approach we have argued for lacks a distinct stage in the processing where the intended referent is only indicated by some internal symbol.",34,35
14498,12041769,"In order to integrate the generation of one-anaphora into conventional generation algorithms, the assumption that the referring expression generator is given nothing more to work with than the symbol that corresponds to the intended referent has to be abandoned, and the bandwidth of communication between the discourse planner and the referring expression generator increased: ideally, the referring expression generator is told not only what the intended referent is, but also what its function in the discourse is.",31,32
14499,12041769,"Mc-Donald's [1980] work on referring expression generation within mumble includes a facility whereby the expert system driving the generator can specify that a message element (i.e., an internal symbol corresponding to the intended referent) is 'ontologically of a sort that cannot be pronominalized ' [1980:217]: this allows the expert system to specify that some information has to be expressed for descriptive, rather than purely referential, purposes.",35,36
14500,10575078,"< gr ≤ n − 1), such that ≤ p ≤ k, 1 ≤ q ≤ k, 0 ≤ r ≤ k − 1, p + q + r = n, and the string does not contain more than one consecutive appearance of the same symbol.",51,52
14501,10575078,"The legal combinations of structures for gap degree k will correspond to strings where symbols a and b each appear at most k + 1 times, g appears at most k times and is not the first or last symbol, and there is no more than one consecutive appearance of any symbol.",40,41
14502,10575078,"The legal combinations of structures for gap degree k will correspond to strings where symbols a and b each appear at most k + 1 times, g appears at most k times and is not the first or last symbol, and there is no more than one consecutive appearance of any symbol.",53,54
14503,10575078,"As the string used to generate a Combine step can have length at most 3k + 2, and the resulting step contains an index for each symbol of the string plus two extra indexes, the MG k parser has complexity O(n 3k+4 ).",27,28
14504,12018120,"We do not evaluate the relation between any two medications in a sentence; instead, we only considered two medications that are related to each other by a cue word or symbol (including those connected by cue words The results of the two cases are shown in Table 4 .",32,33
14505,8918612,"Arbitrary actions may be performed when certain strings are recognised, although in our case, the value of the token recognised is passed, and an entry in the symbol table created.",30,31
14506,219187100,As is standard in computer science--and explained on page 135 in the appendix on mathematical background---the symbol <_7~ denotes the relation of polynomial time reducibility be-tween computational problems.,20,21
14507,219187100,As is standard in computer science--and explained on page 135 in the appendix on mathematical background-the symbol <:o denotes the relation of polynomial time reducibility between computational problems.,20,21
14508,9087811,"Suppose that we are looking for the symbol below 3,1, i.e., the top of ft.",7,8
14509,9087811,"For example, if a node labeled A (a) is the parent of a node labeled a (i.e., corresponding to the use of the production A (a) --* a where a is a terminal symbol) then obviously this node does not have a terminator.",41,42
14510,9087811,"In order to record the derivation from A (fl), we need to know the top symbol in the stack fl, i.e., the symbol below the top of the stack associated with the primary constituent.",18,19
14511,9087811,"In order to record the derivation from A (fl), we need to know the top symbol in the stack fl, i.e., the symbol below the top of the stack associated with the primary constituent.",27,28
14512,9087811,We need to recover the identity of this symbol from the encoding of the primary category.,8,9
14513,9087811,"Therefore, from Observation 2.1 it follows that the terminator-pointer can only be used to determine the (k+l) st symbol from the top.",23,24
14514,9087811,Thus the (k + 1) st symbol from the top in A (fl-yp) is the same as the symbol below the top of the stack of the terminator.),8,9
14515,9087811,Thus the (k + 1) st symbol from the top in A (fl-yp) is the same as the symbol below the top of the stack of the terminator.),24,25
14516,9087811,A head consists of a nonterminal and a stack symbol. •,9,10
14517,9087811,"We assume that the input given is al ... an, where n>l. Initialization Phase In the initialization phase of the algorithm we store lexical objects (objects deriving a terminal symbol in one step) entirely in a single entry.",33,34
14518,9087811,Therefore the encoding of the new object can easily be derived from that of the primary object by simply modifying the head (to change the top of the stack symbol).,30,31
14519,9087811,"Definition 5.1 A TAG, G, is denoted by (VN, VT~ 57 Iv A) where VN is a finite set of nonterminals symbols, VT is a finite set of terminal symbols, S E VN is the start symbol, I is a finite set of initial trees, A is a finite set of auxiliary trees.",43,44
14520,9087811,"We assume that internal elementary nodes have either a single child labeled by a terminal symbol (or ~), or exactly two children labeled by nonterminals.",15,16
14521,9087811,"If ~ is an elementary node address of a node on the spine of an auxiliary tree, say fl, then any object that has ~ as the top symbol of its stack must be of the form A (~h .. .",30,31
14522,9087811,"Scanning a terminal symbol: If z/is a node labeled by a terminal matching the i th input symbol, ai, then we have (corresponding to Rule 1.L): Scanning empty string: If ~ is a node labeled by e, then we have (corresponding to Rule lx.",3,4
14523,9087811,"Scanning a terminal symbol: If z/is a node labeled by a terminal matching the i th input symbol, ai, then we have (corresponding to Rule 1.L): Scanning empty string: If ~ is a node labeled by e, then we have (corresponding to Rule lx.",20,21
14524,770625,"A morpheme is represented by a tuple (symbol, state, prev, next) , where symbol denotes a morpheme, state is one possible tag for this morpheme, prev and next are sets of indexes, denoting the indexes of the morphemes (of the previous and the next vectors) that precede and follow the current morpheme in the overall lattice, representing the sentence.",8,9
14525,770625,"A morpheme is represented by a tuple (symbol, state, prev, next) , where symbol denotes a morpheme, state is one possible tag for this morpheme, prev and next are sets of indexes, denoting the indexes of the morphemes (of the previous and the next vectors) that precede and follow the current morpheme in the overall lattice, representing the sentence.",18,19
14526,770625,"An emission is denoted in this figure by its symbol, its state index, directed edges from its previous emissions, and directed edges to its next emissions.",9,10
14527,770625,"In practice, we found the average number of emissions per sentence in our corpus (where each symbol is counted as the number of its predecessor emissions) to be 455, where the average number of words per sentence is about 18.",18,19
14528,770625,"The algorithm works in O( Ṫ ) time complexity, where Ṫ is the total number of symbols in the output sequence encoding, where each symbol is counted as the size of its prev set.",27,28
14529,15774184,"The ""smallest UTM"" of Minsky(1967:276-281) has seven states and a four symbol tape alphabet, for a state-symbol product of 28 (!).",16,17
14530,15774184,"The ""smallest UTM"" of Minsky(1967:276-281) has seven states and a four symbol tape alphabet, for a state-symbol product of 28 (!).",24,25
14531,15774184,"M (X~+l) R is false for some odd i (xi) R ~-*M Xi+l is false for some even i Straightforward construction of GVTM will result in a CFG containing on the order of twenty or thirty nonterminals and at least fifteen terminals (one for each UTM state and tape symbol, one for the blank-tape symbol, and one for the instantaneous description separator ""~').",54,55
14532,15774184,"M (X~+l) R is false for some odd i (xi) R ~-*M Xi+l is false for some even i Straightforward construction of GVTM will result in a CFG containing on the order of twenty or thirty nonterminals and at least fifteen terminals (one for each UTM state and tape symbol, one for the blank-tape symbol, and one for the instantaneous description separator ""~').",62,63
14533,15774184,This symbol-production tradeoff becomes clearer when one actually constructs GUTM.,1,2
14534,15774184,Suppose the distinguished start symbol for GVTM is SUTM.,4,5
14535,15774184,B is the terminal blank symbol.,5,6
14536,18306263,"The only other differences are minor: the symbol ""~"" has been replaced by ""DELTA"", the sentence symbol ""P"" has been translated into English ""S"", and accents have been omitted.",8,9
14537,18306263,"The only other differences are minor: the symbol ""~"" has been replaced by ""DELTA"", the sentence symbol ""P"" has been translated into English ""S"", and accents have been omitted.",22,23
14538,17948227,"McCawley's critical remark reflected the gradual development of X-bar theory, which allowed practically any constituent (or, more generally speaking, any arbitrary symbol for a grammatical value) to act as the head, dependent on the needs of the analysis of this or that construction.",28,29
14539,11795965,A special symbol CO is introduced in the complex labels for the coordinated nodes to mark which nodes stand in the coordination relation and which modify the coordination as a whole (see (11')); the lexical value of the restored elements is copied from the antecedents (see (13') above): (11') ((we.,2,3
14540,14841930,"It consists, in addition to the set of nonterminals N, the set of terminals T and the start symbol S, an extraordinary symbol in N, of two different sets of trees, which specify the rules of a TAG.",20,21
14541,14841930,"It consists, in addition to the set of nonterminals N, the set of terminals T and the start symbol S, an extraordinary symbol in N, of two different sets of trees, which specify the rules of a TAG.",25,26
14542,14841930,"This means the start symbol is the root node, all inner nodes are nonterminals and all leaves are terminals (e.g., in Figure 1 tree a).",4,5
14543,14841930,"e E L(G) iff a tree with root node S (NA), the start symbol, which allows no further adjoinings (null adjoining), and a single terminal son e is element in the set of initial trees I (this tree is called the e tree), 2.",17,18
14544,14841930,The number of pointers to elementary trees at each node can be restricted by the number ofoccurences of a nonterminal as the left-hand side symbol of a rule in the context-free kernel (which is a constant).,26,27
14545,14841930,"Each ter-290 minal can be a leaf in a constant number of elementary trees and with an indegree of O(n-1) in row 1 of the triangle matrix, the number of occurences of elementary trees containing the input symbol tl (1 .",39,40
14546,1190899,"The nodes in the frontier of elementary trees are labelled by terminal symbols except for one node in the frontier of each auxiliary tree, the foot node, which is labelled by the same nonterminal symbol as the root.",36,37
14547,1190899,"Head Grammars Head Grammars are string rewriting systems like CFG's, but differ in that each string has a distinguished symbol corresponding to the head of the string.",21,22
14548,1190899,"Unlike a headed string which has a distinguished symbol, a split string has a distinguished position about which it may be split.",8,9
14549,1190899,".)2 ) LLl(ul'd-[u2, u1~22 ~2) LL2(uxh'71u2, vlh-~2v2) LR1(ul-d71u2, vx-d-iv2) LR2 (ux~'lu2, vx'4-~v2) = tt 1~1""1 t/2 t~la2 U 2 : ~1~1~/,2~)1~ They are defined as follows: CI(toITW2, UlTU2 ) = t01TW2UlU 2 C2(WlTW2, u1Tu2) : t/)lt/)2UlTU2 Since the split point is not a symbol (which can be split either to its left or right) but a position between strings, separate left and right wrapping operations are not needed.",64,65
14550,978468,"nbn~n ~ 0} is not profligate, because it has two terminal symbols but there is a grammar for it that has only one nonterminal symbol, namely S. (The rules are: (S --> aSb, S --> e}.)",28,29
14551,978468,A string xcv in (~ + b)*~(~ + A)* will be in this language if any one of the following is met: (a) ~ is longer than Z; (b) K is shorter than ~; (c) ~ is the same length as ~ but there is an such that the ith symbol of K is distinct from the ith symbol of ~. The interesting Condition here is (c).,62,63
14552,978468,A string xcv in (~ + b)*~(~ + A)* will be in this language if any one of the following is met: (a) ~ is longer than Z; (b) K is shorter than ~; (c) ~ is the same length as ~ but there is an such that the ith symbol of K is distinct from the ith symbol of ~. The interesting Condition here is (c).,70,71
14553,978468,"The grammar has to generate, for all ~ and for all pairs <u, v> of symbols in the terminal vocabulary, all those strings in (a + b)*c(a + b)* such that the ~th symbol is ~ and the ~th symbol after ~ is Z. There is no bound on l, so recursion has tO be involved.",41,42
14554,978468,"The grammar has to generate, for all ~ and for all pairs <u, v> of symbols in the terminal vocabulary, all those strings in (a + b)*c(a + b)* such that the ~th symbol is ~ and the ~th symbol after ~ is Z. There is no bound on l, so recursion has tO be involved.",47,48
14555,978468,But it must be recursion through a category that preserves a record of which symbol is crucially going to be deposited at the ~th position in the terminal string and mismatched with a distinct symbol in the second half.,14,15
14556,978468,But it must be recursion through a category that preserves a record of which symbol is crucially going to be deposited at the ~th position in the terminal string and mismatched with a distinct symbol in the second half.,34,35
14557,978468,"But such a grammar has to use recursive nonterminals, one for each terminal, to carry down information about the symbol to be deposited at a certain point in the string.",21,22
14558,978468,"The interesting thing about this, if it is correct, is that it suggests that human languages not only never demand the syntactic string comparison required by string matching languages, they never call for syntactic string comparision over infinite sets of strings at all, whether for symbol-by-symbol checking of identity (which typically makes the language non-CF) or for specifying a mismatch between symbols (which may not make the language non-CF, but typically makes it profligate).",49,50
14559,978468,"The interesting thing about this, if it is correct, is that it suggests that human languages not only never demand the syntactic string comparison required by string matching languages, they never call for syntactic string comparision over infinite sets of strings at all, whether for symbol-by-symbol checking of identity (which typically makes the language non-CF) or for specifying a mismatch between symbols (which may not make the language non-CF, but typically makes it profligate).",53,54
14560,6060644,"We define the following functional expressions: • do(s,a) expresses that agent s has performed the action a; • mistake (s, al,a2 ) expresses that agent s has mistaken an act al for act a2; • and(pl,p2) expresses the conjunction of suppositions Pl and P2, where Pl must be simple (i.e., not formed from others using the function symbol and); • not p expresses the negation of a simple supposition p.17 We also define several suppositions for expressions of knowledge and intention.",73,74
14561,9558665,"Thus, the question of what is a word sense can be addressed with syntactic methods (symbol pushing), and need not address semantics (interpretation), even though the inventory of tags may appear to have semantic values.",17,18
14562,9402339,If a node n[ is tlnked to a node n2 then (1) n2 c-commands nl and (2) nl dominates a null string (or a temi.al symbol in the non-linguistic formal grammar examples).,33,34
14563,9402339,"As we adjoin an auxiliary tree, we augment the length of the terminal string by the length of the terminal string of (not counting the single non-terminal symbol in the frontier of ~ ).Thus for any string, w, of L(G), we have where wgls the terminal string of some initial tree and wg,l ~ i~ m, the terminal string of the [-th auxiliary tree, assuming there are m auxiliary trees.",31,32
14564,18047271,"In (6), the symbol P denotes a description consisting of object language predicates that can be applied to the object being described.",6,7
14565,18047271,"The symbol D* denotes a similar expression, which includes all the descriptors of P conjoined with a set of predicates that describe the focus of thediscourse.",1,2
14566,1375020,"We ensure the input is not inadvertently performing some computation by requiring the one ID rule R allowed in the reduction to be fully specified, with only one 0-1evel category on the left-hand side and one unanalyzable terminal symbol on the right-hand side.",43,44
14567,1375020,"Let w be the string of formula literals in F, and let wl denote the i th symbol in the string w. (2) {[STAGE 2]} --* W (c) I w[ metarules to verify assignments Vi,j,k 1<i<1~ j, l <_ j <_ m, O < k < 2, if wsi-k : xj, then construct the metarule {[yi 1],[ei 0],[STAGE 2]) --+ W (3) {[yj i],[ci 1], [STAGE 2]} --' W Vi,j,k l<i<~ -1, l<_j<_m,O<k<_2, if wsi-k = ~, then construct the metarule {[yj 0], [cl 0], [STAGE 2]} -* W (4) {[yj O],[ci 1],[STAGE 2]}---,W (d) Let the category C = {[ci 1]: 1 < i < l~J}.",18,19
14568,1375020,"Let w be the string of formula literals in F, and w~ denote the i th symbol in the string w. We specify a set K of permissible categories based on A, F, p,.and the set of FCRs R s.t.",17,18
14569,1375020,"The ID rules encode the ATM NextM() relation, i.e. C ---* NextM(C) for a universal configuration C. The reduction constructs an ID rule for every combination of possible head position, machine state, and symbol on the scanned tape square.",40,41
14570,1375020,"In the worst case, every symbol in FC(M,R) is underspecified, and every category in K extends every symbol in the FC(M,R} grammar.",6,7
14571,1375020,"In the worst case, every symbol in FC(M,R) is underspecified, and every category in K extends every symbol in the FC(M,R} grammar.",22,23
14572,1375020,One metarule would be needed for each tape square/tape symbol combination in the ATM.,11,12
14573,7970879,"Also, to handle the terminals, for each t i add the production Ti~t where t is the i th symbol in s. It is straightforward to show inductively that if a nonterminal symbol generates any string at all it generates exactly the substring of s that its subscript determines.",23,24
14574,7970879,"Also, to handle the terminals, for each t i add the production Ti~t where t is the i th symbol in s. It is straightforward to show inductively that if a nonterminal symbol generates any string at all it generates exactly the substring of s that its subscript determines.",36,37
14575,7970879,Also a parse tree of G s can be converted to a parse tree of G by first deleting all terminals (each is dominated by the same symbol with a subscript) and then erasing all superscripts and subscripts on all symbols in the tree.,28,29
14576,7970879,"The C-K-Y recognition algorithm uses the standard bottomup method to determine emptiness of G s. It starts with the terminals and determines which G s nonterminals are productive, eventually finding whether or not the start symbol is productive.",40,41
14577,7970879,It uses a top-down control mechanism to determine the productivity only of nonterminals that are reachable from the start symbol.,21,22
14578,13464725,"A description corresponding to the input condition appears to the left of the --~ symbol, and the description to be included when the input condition is satisfied appears to its right.",14,15
14579,6450767,"G= (V,, VT,S,I,A) where Vjv is a finite set of nonterminals symbols, VT is a finite set of terminal symbols, S E V/v is the start symbol, I is a finite set of initial trees, A is a finite set of auxiliary trees.",40,41
14580,6450767,"That is, the original recognition problem can be turned into one of generating the shared forest grammar, Gw, and deciding whether the start nonterminal, (S, 0, n), of Gw is an useful symbol, i.e., whether there is some terminal string z such that (S,0, n) =~x Ow Here S has been taken to be the start nonterminal of Go.",41,42
14581,6450767,where S is the start symbol of Gw.,5,6
14582,6450767,"Also, if the node r/being visited belongs to an auxiliary tree and is on its spine we can expect the symbol below the top of the stack to give us the node where 3 is adjoined.",23,24
14583,6450767,If r/is not on the spine of an auxiliary tree then it is the only symbol on the stack.,17,18
14584,6450767,"P includes the following set of productions for the start symbol S' iS'[] .---, (T, qo, q/)[r/] I q; e F and t/is root of initial tree In addition, for each elementary node t/do the following.",10,11
14585,6450767,"for each p, q E Q. Note that the stack symbol that appeared below ti will be the node at which fl was adjoined.",11,12
14586,6450767,"This transforms the question of determining whether a symbol is useful into a reachibility question on the graph of Ma.. In particular, for any string of stack symbols % the object A[7] derives a string of terminals if and only if it is possible, in the nfa Ma.., to reach a final state from the state corresponding to A on the input 7.",8,9
14587,6450767,"is the start symbol of Gw, i.e., q0 -S'.",3,4
14588,6450767,If in Go every elementary tree has at least one terminal symbol in its frontier (as in a lexicalized tag) then to derive a string of length n there can beat most n adjunctions.,11,12
14589,6450767,"In the nfa MG, (after useless symbols have been removed) we have (B,p, q) E df ((A, i,j) , ri) if and only if in the cfg shared forest (A, r/, i, j, p, q) is not a useless symbol.",61,62
14590,56751205,"That is, given a prefix, they predict the likelihood of the next symbol.",14,15
14591,56751205,"Consequently, a perfect predictor will yield 100% accuracy (because only one kind of closing parenthesis is acceptable in any given situation), while a random strategy would yield about 20% accuracy (random choice of any closing parenthesis; and otherwise assuming that the end of string symbol does not introduce confusion.)",52,53
14592,5438482,Take START as the start symbol.,5,6
14593,5438482,"Dummy symbol D will be used to soak up excess input symbols, so D -, a through D -, d should be rules.",1,2
14594,5438482,"Dummy symbol U will also be used to soak up excess input symbols, but U will be allowed to match only when there are four occurrences in a row of the same symbol (one occurrence for each edge).",1,2
14595,5438482,"Dummy symbol U will also be used to soak up excess input symbols, but U will be allowed to match only when there are four occurrences in a row of the same symbol (one occurrence for each edge).",33,34
14596,5438482,"HIH2H3H4UUDDDD That is, each H-symbol is supposed to match the name of one of the endpoints of the corresponding edge, in accordance with the rules expanding the H-symbols.",7,8
14597,5438482,"Each U-symbol is supposed to correspond to a vertex that was left untouched by the H-matching, and the D-symbols are just there for bookkeeping.",3,4
14598,5438482,"The grammar encodes the underlining procedure by requiring each H-symbol to match one of its endpoints in o. Since the right-hand side of the START rule is unordered, the grammar allows an H-symbol to match anywhere in the input, hence to match any vertex name (subject to interference from other rules that have already matched).",11,12
14599,5438482,"The grammar encodes the underlining procedure by requiring each H-symbol to match one of its endpoints in o. Since the right-hand side of the START rule is unordered, the grammar allows an H-symbol to match anywhere in the input, hence to match any vertex name (subject to interference from other rules that have already matched).",39,40
14600,5438482,"For example, since simple UCFGs are restricted to be duplicate-free, a difficulty that arises with simple UCFGs cannot result from the possibility that a symbol may occur more than once on the right-hand side of a rule.",29,30
14601,5438482,Designate START as the start symbol.,5,6
14602,5438482,Different parts of step 2 cannot conflict with each other because each one affects a symbol with a different subscript.,16,17
14603,5438482,The states related to the auxiliary start symbol and endmarker that are added by some versions of the Earley parser have been omitted for simplicity.,7,8
14604,5213937,"x k−1 ); we follow convention in denoting generic estimators by P. The maximum likelihood estimator for n-grams is derived from frequency counts for sequence X and symbol c, P ML (c|X) = count(X c)/extCount(X), where count(X ) is the number of times the sequence X was observed in the training data and extCount(X ) is the number of single-symbol extensions of X observed: extCount(X ) = ∑ c∈Char count(X c).",30,31
14605,5213937,"x k−1 ); we follow convention in denoting generic estimators by P. The maximum likelihood estimator for n-grams is derived from frequency counts for sequence X and symbol c, P ML (c|X) = count(X c)/extCount(X), where count(X ) is the number of times the sequence X was observed in the training data and extCount(X ) is the number of single-symbol extensions of X observed: extCount(X ) = ∑ c∈Char count(X c).",69,70
14606,8830794,"The set of features on the left side of a ""<"" symbol is relaxed before the set on the right side.",13,14
14607,7685228,"Thus, the root of an initial trce is labelled by the symbol S. They are required to have a frontier made up of terminals.",12,13
14608,7685228,"Thus, if the root of an auxiliary tree is labelled by a nonterminal symbol, X, then there is a node (called the foot node) in the frontier of this tree which is labelled by X. The rest of the nodes in the frontier are labelled by terminal symbols.",14,15
14609,910368,"After parsing ""1 saw"", the parser is in state $3 and about to visit the NP subnetwork, pushing the current environment (the current state symbol and all registers) onto the stack.",30,31
14610,910368,"Unlike phrase-structure rule based parsers, information about how to reduce constituents is encoded in the complex category symbol of each constituent with functor and argument features.",20,21
14611,15628268,This symbol is for the conven/ence of the human designer; it does not take part in the computation.,1,2
14612,47359055,"Let 3' be a tree with a node n labelled X and let ~ be an auxiliary tree with the root labelled with the same symbol X. (Note that mnst have, by definition, a node (and only one) labelled X on the frontier.)",26,27
14613,47359055,"by definition, no auxiliary trees are adjoinable to a node labelled by a terminal symbol, no constraint has to be stated for node labelled by a terminal.",15,16
14614,47359055,"If 0 is an auxiliary tre~, ""7 E D(0) and the frontier of 3' is w I X w 2 {X is a nooterminsJ.wl.w 2 E ~ r~') then the le~ node having this non-terminal symbol X at the frontier is called the foot of 3'.",44,45
14615,47359055,"L~, once we note that there are no Nxifia~ trees in G rooted with the symbol S, and that N I f3 N, ,m d).",16,17
14616,47359055,"Let S be a symbol not in N t, and let N m N I U {S}.",4,5
14617,47359055,"The OA and NA constraints at X are treated similar to the previous eMes, and so is the cue if either Y o1' Z is labelled by a terminal symbol.",31,32
14618,47359055,"We will annotate with Y the q•adruple (p,qs,qa~t) and the constraint that root of the t~,e which can be adjoined at X should have the quadruple (qt,P~,qt) amucinted with it amen8 the trees which were aflowed in the original grammar, if it is to be adjoined st X. The cm where the original grammar bad null or obligatory constraint amocinted with this node or Y is labelled with a terminsi symbol, are treated similar to how we dealt with them in the previous cuses.",83,84
14619,47359055,"It is more convin/ent for us to think of the headed string (i,at...sl) as the string •t.--~ with tbe head pointing in between the symbok I 4 and 14+t rather than at the symbol 14.",41,42
14620,1651298,We define a dotted symbol as a symbol associated with a dot above or below and either to the left or to the right of it.,4,5
14621,1651298,We define a dotted symbol as a symbol associated with a dot above or below and either to the left or to the right of it.,7,8
14622,1651298,"left above, left below, right above, right below): laura lb ~rb • Then we define a dotted tree as a tree with exactly one dotted symbol.",29,30
14623,1651298,"Any tree ~ will be considered as a function from tree addresses to symbols of the grammar (terminal and non-terminal symbols): if z is a valid address in a, then a(z) is the symbol at address z in the tree a. Definition 2 A state s is defined as a 10-tuple, [a, dot, side,pos, l, ft, fr, star, t~, b~] where: • a: is the name of the dotted tree. •",39,40
14624,1651298,"dot: is the address of the dot in the tree a. • side: is the side of the symbol the dot is on; side E {left, right}. •",20,21
14625,1651298,Suppose that the dot is to the left of and above a terminal symbol (see Figure 5 ).,13,14
14626,1651298,"Then if the terminal symbol matches the next input token, the program should record that a new token has been recognized and try to recognize the rest of the tree.",4,5
14627,1651298,"Therefore ""the scanner applies to s = [a, dot, left, above, 1, ft, L, star, t[, b[] such that ,',(dot) is a terminal symbol and ~(dot) = ~+I or ~(dot) is the empey symbol • Case 1: a(dot) = ai+l The scanner adds [~, dot, right, above, 1, f,, fi, star, t[ , b[ ] ""co SI+I • • Case 2: a(dot) = The scanner adds [tr, dot, right, above, l, ft, fr, star, t[ , b[ ] to S,.",42,43
14628,1651298,"Therefore ""the scanner applies to s = [a, dot, left, above, 1, ft, L, star, t[, b[] such that ,',(dot) is a terminal symbol and ~(dot) = ~+I or ~(dot) is the empey symbol • Case 1: a(dot) = ai+l The scanner adds [~, dot, right, above, 1, f,, fi, star, t[ , b[ ] ""co SI+I • • Case 2: a(dot) = The scanner adds [tr, dot, right, above, l, ft, fr, star, t[ , b[ ] to S,.",54,55
14629,1651298,Left Predictor Suppose that there is a dot to the left of and above a non-terminal symbol A (see Figure 8 ).,18,19
14630,1651298,Substitution Predictor Suppose that there is a dot to the left of and above a non-terminal symbol on the frontier A that is marked for substitution (see Figure 17 ).,18,19
14631,1436190,"The first symbol in ,.~eh_ expression is a labet indicating the function of that item within the plan; embett,bM__ items appearing in angle brackets ere in/ormatiou units from the current-events knowledge base.",2,3
14632,16262303,to denote an arbi-Wary stack whose top symbol is I. This system is called L/near Indexed Grammars because it can be viewed as a restriction of Indexed Grammars in which only one of the non-terminals on the right-hand-side of a production can inherit the stack from the left-hand-side.,9,10
14633,16262303,"Rather than adding or removing a single symbol from the stack, a fixed number of symbols can be removed and added in one production.",7,8
14634,16262303,Each OA nodes on the spine is encoded in c by a slash and nonterminal symbol in the appropriate position.,15,16
14635,5186249,"A rule body is any combination of surlCace terminals, logical terminals, goals, shifted non-terminals, non-tprminals, the symbol 'nil', and the cut symbol '/', using the sequencing operator ':' and the 'or' symbol 'l' (We represent left-to-right sequencing with a colon instead of a comma, as is often done in logic grammars.)",25,26
14636,5186249,"A rule body is any combination of surlCace terminals, logical terminals, goals, shifted non-terminals, non-tprminals, the symbol 'nil', and the cut symbol '/', using the sequencing operator ':' and the 'or' symbol 'l' (We represent left-to-right sequencing with a colon instead of a comma, as is often done in logic grammars.)",33,34
14637,5186249,"A rule body is any combination of surlCace terminals, logical terminals, goals, shifted non-terminals, non-tprminals, the symbol 'nil', and the cut symbol '/', using the sequencing operator ':' and the 'or' symbol 'l' (We represent left-to-right sequencing with a colon instead of a comma, as is often done in logic grammars.)",50,51
14638,5186249,Any rule body element not of the above four forms and not 'nil' or the cut symbol is taken to be a non-terminal.,18,19
14639,14226744,Any symbol with a ?,1,2
14640,15423144,"and the implication symbol Ualnt Dh~ontinuoua Constituents in Grsmamgrs Ahhough the previous section introduced discontinuous constituents in terms of definite cianse grammar, there is no reason we could not invent a notation that abbreviates or implies the ""combines' relationship between mother and daughters, just a.s the CFG in (1) ""implies"" the mother-daughter location relationships made explicit in the DCG (3).",3,4
14641,7122898,"The symbol "".""",1,2
14642,5176865,"Thanks to the properties of the rewriting formalism, the transfer grammar is reversible, and can even generate all possible pairs for the grammar, given only the start symbol TRANSLATE.",30,31
14643,5176865,"This is specified using a trar~late-ag slot with type symbol Pimp, which will be used during the rewriting process (see details in sect 3 and 4).",13,14
14644,5176865,"The type system is interpreted using the rewriting mechanism described in [Ait-Kaci 84], which gives an operational semantics for type inheritance: a feature structure which has a type ~3--v for example is unified with the definition of this type: and the type symbol AG-V is replaced with the supertype VERB in the result of the unification.",51,52
14645,5176865,"Disjunctions like Pt~Dp create a non-deterministic choice for further rewriting: the symbol E,I~Dp is replaced with the disjunction of symbols of the righthand-side creating alternative paths in the rewriting process.",16,17
14646,5176865,"This process of rewriting is applied on every sub-structure of a structure to be evaluated, until no type symbol can be rewritten.",21,22
14647,5176865,"This means that the dictionaries of the system would have to be organized as a single integrated bilingual lexical rhtabas~. Starting from the abstract speech act description, we need only one definition for specifying the direct mapping of Abstract Speech Acts by tagging, which also introduces the type symbol PROP that will trigger the rewriting process for the transfer grmnmar:.",50,51
14648,5176865,"In this simple example, the definition of the symbol PR3P contains the full bilingual dictionary.",9,10
14649,5176865,"One can use the hierarchical type system to restrict the set of candidates to a small sub-set of definitions and instead of using pROP, use the most adequate specific symbol for translating an argument: such a symbol can be viewed as the initial symbol of a sub-grammar which describes the transfer relation on a A STEP BY STEP EXAMPLE We give in this section a trace of a simple example for the sentence in  A lexical definition introduces the PPJ3P symbol for the arguments of a predicate, and the translation relation is defined recursively between argument substructures.",32,33
14650,5176865,"One can use the hierarchical type system to restrict the set of candidates to a small sub-set of definitions and instead of using pROP, use the most adequate specific symbol for translating an argument: such a symbol can be viewed as the initial symbol of a sub-grammar which describes the transfer relation on a A STEP BY STEP EXAMPLE We give in this section a trace of a simple example for the sentence in  A lexical definition introduces the PPJ3P symbol for the arguments of a predicate, and the translation relation is defined recursively between argument substructures.",40,41
14651,5176865,"One can use the hierarchical type system to restrict the set of candidates to a small sub-set of definitions and instead of using pROP, use the most adequate specific symbol for translating an argument: such a symbol can be viewed as the initial symbol of a sub-grammar which describes the transfer relation on a A STEP BY STEP EXAMPLE We give in this section a trace of a simple example for the sentence in  A lexical definition introduces the PPJ3P symbol for the arguments of a predicate, and the translation relation is defined recursively between argument substructures.",47,48
14652,5176865,"One can use the hierarchical type system to restrict the set of candidates to a small sub-set of definitions and instead of using pROP, use the most adequate specific symbol for translating an argument: such a symbol can be viewed as the initial symbol of a sub-grammar which describes the transfer relation on a A STEP BY STEP EXAMPLE We give in this section a trace of a simple example for the sentence in  A lexical definition introduces the PPJ3P symbol for the arguments of a predicate, and the translation relation is defined recursively between argument substructures.",87,88
14653,5176865,The initial symbol that will be rewritten is ~.--'g (symbols to be rewritten are in bold face).,2,3
14654,216804411,Such formalisms can be thought of by analogy to context-free grammars as generalizing the notion of nonterminal symbol from a finite domain of atomic elements to a possibly infinite domain of directed graph structures nf a certain sort.,19,20
14655,216804411,Such formalisms can be thought of by analogy to context-free grammars a.s generalizing the notion of nonterminai symbol from a finite domain of atomic elements to a possibly infinite domain of directed graph structures of a certain sort.,19,20
14656,216804411,"Because the LALR table building algorithm does not in general terminate for complex-featurebased grammar formalisms, the grammar used in that paper was a simple context-free grammar with subcategorization and gap information placed in the atomic nonterminal symbol.",41,42
14657,216804411,"The solution mentioned above of placing more information in lilt, category symbol falls into this class.",12,13
14658,216804411,"One of the aforementioned solutions is to require the grammar writer to put all such significant information in a special atomic symbol--i.e., mandate a context-free backbone.",21,22
14659,216804411,"But when most of the information is not in an atomic category symbol, such prediction is relatively useless and many types of constituents are predicted that could never be involved in a completed parse.",12,13
14660,216804411,"For inslance, in many parsing schemes, edges are indexed by a categ<,ry symbol for elficient retrieval.",14,15
14661,835289,"The same symbol (e.g., [lowl] ) identifies both the situation and its time argument because it is the actual time for which a situation holds which uniquely identifies it.",2,3
14662,1797249,"The symbol ""bel"" is an ordinary predicate letter, not a special modal operator.",1,2
14663,1797249,"It also contains the relation of loving, which is not a symbol or a concept, any more than John or Mary is.",12,13
14664,2100974,"In particular, a context-free grammar is a cuadruple (N, T, P, S), where N is a finite set of string categories; T a finite set of terminal symbols; P a finite set of productions or rewriting rules of the form X--~t~, Xe N, ae (NUT)*; and S a distinguished symbol of N. A binary relation ~ of derivation between strings over the vocalulary NuT of the grammar is defined such that aXl~ ~ ctol3 iff Xo o is a production of P; now, ~* may be defined as the reflexive and transitive closure of ~. The language generated by the grammar, noted L(G), is the set of strings toe T*, such that S ~* to.",66,67
14665,2100974,"An attribute grammar is defined upon a context-free grammar G=(N, T, P, S), by associating each symbol Xe NuT with a finite set A(X) of attributes, and a type or domain dom(a) for each attribute a thus defined (Knuth, 1968) .",23,24
14666,2100974,"Each attribute a of X, noted X.a, takes values over its domain and represents a specific, possibly context-sensitive property of the symbol.",26,27
14667,2100974,"For each category symbol A in the base grammar, we define the attributed symbol A[A.al ..... A.an], where A is the category and A.ai, l<i<n, an attribute occurrence of A. The extended algorithm, in addition to syntactically recognizing the input string, evaluates the attribution associated with each of its possible derivations.",3,4
14668,2100974,"For each category symbol A in the base grammar, we define the attributed symbol A[A.al ..... A.an], where A is the category and A.ai, l<i<n, an attribute occurrence of A. The extended algorithm, in addition to syntactically recognizing the input string, evaluates the attribution associated with each of its possible derivations.",14,15
14669,2100974,"Xi+ll3, f, 8> (i.e., s not final and Xi+l the next input symbol) Si+l := Si+l t..) { < X--->aXi+l.~, f, 8> } end; If Si+l is empty, reject and terminate end; If <¢~S[S.al ..... S.an]_l_.,",17,18
14670,2100974,"For example, the symbol A in the state <A--gao, f, 8> input to the Completer could be shown more explicitly as A=A [A.al ..... A.an] .",4,5
14671,2100974,"Hence, evaluation of the attribute occurrences of A reduces to application of the attribution associated with the production A->a, according to the attribute values in a. This is done by the function eval_s(A, ct, A---~ot), which returns the attributed symbol Ae, identical to A, except that its attribute occurrences have been evaluated, as required.",45,46
14672,2100974,"0, -l-k>, in which the attributed start symbol S[ ...] is already evaluated.",12,13
14673,2100974,"Each instance of the final state corresponds to a different derivation of the initial symbol, leading to a different evaluation of the symbors attributes.",14,15
14674,2100974,"The termination of the Extended Algorithm may be guaranteed while maintaining its full generality, through a finite partition on the attribute domains associated with each cyclic symbol in the grammar.",27,28
14675,14372141,"the numeral 2 being merely an index to permit reference to a specific symbol in the semantics, the metarules, and the rule compiler, and is not a part of the category label); and c is a semantic translation rule stating that the V constituent translates as a function expression taking as its argument the translation of the second N!!,",13,14
14676,5110528,The symbol o~ is a special variable ranging over names.,1,2
14677,5110528,"We allow a variable to appear inside a constant--for example, since v is a variable, q(v) is a constant that denotes the variable v. Enderton explicitly forbids this: ""no symbol is a finite sequence of other symbols"" (p. 68).",36,37
14678,5110528,"Note that the symbol ""believe"" is an ordinary predicate letter, not a special operator• This is a minor technical advantage of the sentential approach: the quotation operator eliminates the need for a variety of special propositional attitude operators• To define this notation precisely, we must have some way of associating dummy variables with the de re arguments• Suppose we have the wff believe(x,[t 1 ... t,],q(p)).",3,4
14679,5110528,"If f is a symbol of the target language, 'f is a symbol of the meta-language.",4,5
14680,5110528,"If f is a symbol of the target language, 'f is a symbol of the meta-language.",14,15
14681,5110528,"These axioms use the constant ""john"" to denote both a terminal symbol of the grammar and a constant of the target language--a convenient abuse of notation.",13,14
14682,5110528,"The rule for verbs says that if a terminal symbol has a subcategorization frame Subcat, then it is a verb: (R7) v(Subcat)~ [Terminal], { has_subcat(Terminal, Subcat) }.",9,10
14683,5110528,"The; derivation is not yet complete, because ""s"" is not the start symbol of our grammar.",16,17
14684,5110528,"Instead we use a special symbol ""start,"" which never appears on the right side of a rule.",5,6
14685,5110528,"Thus, the start symbol derives only top-level sentenees--it cannot derive an embedded sentence.",4,5
14686,11409791,"This explicit representation of roles permits each predicate-symbol in NFLT to take a variable number of arguments, which in turn makes it possible to represent occurrences of the same verb with the same predicate-symbol, despite differences in valence (i.e. number and identity of attached complements and adjuncts).",9,10
14687,11409791,"This explicit representation of roles permits each predicate-symbol in NFLT to take a variable number of arguments, which in turn makes it possible to represent occurrences of the same verb with the same predicate-symbol, despite differences in valence (i.e. number and identity of attached complements and adjuncts).",38,39
14688,11409791,"The denotation of an NFLT-predicate-symbol is a property; thus, although the substitution discussed earlier preserves the extension of 'GENOTYPE-XYZW', it does not preserve the denotation of that predicate-symbol.",8,9
14689,11409791,"The denotation of an NFLT-predicate-symbol is a property; thus, although the substitution discussed earlier preserves the extension of 'GENOTYPE-XYZW', it does not preserve the denotation of that predicate-symbol.",40,41
14690,11409791,"The result of appending the ""conceptual raising"" symbol ' l"" to the constant ""COOKIE' is a new constant, ' TCOOKIE', that denotes the concept that 'COOKTE' expresses (i.e. ' 1""' applies to a constant and forms a standard name of the sense of that constant).",9,10
14691,11409791,"Capitalization, '$'-postfixing, and braces are used there to do the work done here by the symbol ' t'.",20,21
14692,11409791,"In a computer implementation, we model such a conceptual object with a data object of this form: (6) (cookie ;COOKIE} Here the symbol 'cookie' is a surrogate for a phonological representation (in fact we ignore phonology altogether and deal only with typewritten English input).",29,30
14693,11409791,The symbol 'COOKIE' (a basic constant of NFLT denoting the property COOKIE) models the corresponding semantic representation.,1,2
14694,1148651,"followed(sold, by-clause) NMG extends DCG's Horu-clause notation by allowing an ""unless"" term, marked by the symbol O. Such an ""unless"" term appears in (rl), the default case for sold.",25,26
14695,31006279,"This sometimes is called computational llnguistlcs, mechanical linguistics, information processing, symbol manipulation, and so on.",13,14
14696,3204377,"Spanish in this case, which parses the target input words using the given phrasal category (with its associated indices) as its initial symbol.",25,26
14697,3204377,"Figure 2 shows the result of parsing the Spanish input words of our example, using the initial symbol in the phrasal template in (12).",18,19
14698,3204377,"Each node shows the assigned syntactic category, along with the indices, either specified in the initial symbol or instantiated during parsing.",18,19
14699,13350536,The VP rule schema now uses the current selector to choose the appropriate symbol from the complement it is combining with.,13,14
14700,13350536,"npl,pl,ppl}*{np2,p2,pp2}*{np3,p3,pp3}*{fl,f2,f3,f4} There is one symbol for each possible subcat position, plus an extra one to mark the end of the list.",11,12
14701,1578202,"A distinguished symbol, SEXP, indicates that only the occurrence of something expected by preceding words (i.e. for which an impulse was set up) will allow the transition.",2,3
14702,1578202,"The symbol + stands for ""at least one occurrence of what precedes"").",1,2
14703,475492,"To handle the fact that string variables occur on the left-hand side of each clause, we will understand each clause as a function assigning both the formula on its right and the set of individual variables mentioned on the left to the given predicate symbol.",47,48
14704,475492,"The formula ~b will be given as a recursion scheme /xS~. Each state q of M will become a binary predicate variable q(x,y) in ~. The meaning of q(u, v), where u and v are specific strings in F*, is that M is in state q, scanning the first symbol of v, and that u and v are the portions of the work tape to the left and the right of the head, respectively.",58,59
14705,475492,"In order to access the input, we will have, for each symbol a E E, an atomic predicate symbol a(i) of one argument, which will be true iff in the given input x, the symbol x(i) at position i is a. (We number the positions from 0 through n -1).",13,14
14706,475492,"In order to access the input, we will have, for each symbol a E E, an atomic predicate symbol a(i) of one argument, which will be true iff in the given input x, the symbol x(i) at position i is a. (We number the positions from 0 through n -1).",21,22
14707,475492,"In order to access the input, we will have, for each symbol a E E, an atomic predicate symbol a(i) of one argument, which will be true iff in the given input x, the symbol x(i) at position i is a. (We number the positions from 0 through n -1).",40,41
14708,475492,"For example, a test as to whether the scanned symbol on the work tape is 0 or 1 becomes a test of the parity of r, and so on.",10,11
14709,475492,"Then C(ij, k,l) means that the nonterminal symbol C can derive the pair of substrings of the input string between i and j, and between k and l inclusive.",11,12
14710,475492,"Thus, if C ~ LL2(A,B) is a production, our scheme would include a clause C(ij, k,l) ¢:~ (3pq)(A(i,p,q + 1,l) A B(p + ld',k,q)) Similarly, if C --~ LCI(A,B) were a production, we would have C(ij, k,l) ¢~ (3pq)(A(ij, k,p) /~ B(p + 1,q,q + 1,/)) Finally, if C--* (a, bb)were a terminating production, we would have C(ij, k,l) ¢:~ a(i) /~ i = j/~ k = i + 1/~ b(k) Ab(k+ l) Al= k+ 1 The grammar would be defined by the recursion scheme and the assertion 3jS(0jj + 1 ,last), where S is the start symbol of G. It can be seen from this formulation that every head grammar can be written as an ILFP scheme with at most six total variables.",149,150
14711,15983238,"Analogical-ShaDe ,F; t | The set of features on the left side of a ""<"" symbol is relaxed before the set on the rlght side The order that the features inside the braces. "")",21,22
14712,9850163,"4 (Following Steedman, we will sometimes abbreviate this finite verb phrase category as the symbol FVP.)",16,17
14713,257414,"Each rule is of the form ""A --> B : ~,"" where A is a term which denotes a nonterminal symbol.",24,25
14714,257414,"B is either an atom list representing a terminal symbol or a conjunction of terms (separated by commas) corresponding to nonterminal symbols, and y is a semantic rule which may reference the interpretation of the components of ~ in determining the semantics of A. The rule arrow. --",9,10
14715,257414,The /og/col and symbol.,5,6
14716,257414,"For each rule symbol, a matching pattern symbol describes properties that must exist, but not all the properties that may exist.",3,4
14717,257414,"For each rule symbol, a matching pattern symbol describes properties that must exist, but not all the properties that may exist.",8,9
14718,257414,which will bind to the entire structure containing the symbol x. If <[?,9,10
14719,257414,"Similarly, if the pattern symbol vp matched v,v{NumS) in a rule, then the appearance of vp@foo in the template would result in vp(foo~Vumb) appearing in the new rule.",5,6
14720,257414,"N, that matches a symbol 8 i on the left side of the transformation, will appear in the new rule if there is a symbol ~i"" in 8"" that irura-transformation (IT) matches with ~i"" If there are several symbols in 8"" that IT-match ~i"" the leftmost symbol will be selected.",5,6
14721,257414,"N, that matches a symbol 8 i on the left side of the transformation, will appear in the new rule if there is a symbol ~i"" in 8"" that irura-transformation (IT) matches with ~i"" If there are several symbols in 8"" that IT-match ~i"" the leftmost symbol will be selected.",26,27
14722,257414,"N, that matches a symbol 8 i on the left side of the transformation, will appear in the new rule if there is a symbol ~i"" in 8"" that irura-transformation (IT) matches with ~i"" If there are several symbols in 8"" that IT-match ~i"" the leftmost symbol will be selected.",59,60
14723,257414,No symbol on one side of the transformation may IT-match with more than one symbol on the other side.,1,2
14724,257414,No symbol on one side of the transformation may IT-match with more than one symbol on the other side.,16,17
14725,257414,"With the addition of semantics to each rule, another argument is required to represent the semantic interpretation of the current symbol.",21,22
14726,257414,"it'is'repla~gl by a variable bound to the semantic argument of the corresponding symbol, x. in the rule.",14,15
14727,46393371,"Note that even though the same name can occur on both sides of the := symbol, the atomic types and the constants on the left hand side belong to Σ D-STAG while the (possibly complex) types and the terms on the right hand side belong to Λ(Σ TAG ).",16,17
14728,9922498,Define the fan-out of a grammar to be the largest total number of symbol occurrences on the right hand side of any production.,15,16
14729,9922498,"Suppose we have a context-free language, and two strings in that language, each of which has a substring which is the yield of a subtree labeled by the same nonterminal symbol at the respective roots of the subtrees.",34,35
14730,9922498,"Our final technical result concerns an n-symbol analogue of the so-called MIX language, which has been conjectured by Marsh not to be an indexed language (see [4] for discussion.)",8,9
14731,9922498,"First, we notice that the permutation language P,~ really has s counting property: there is exactly one occurrence of each symbol in any string.",22,23
14732,9922498,"Here there must be exactly m occurrences of each symbol in En, in every string.",9,10
14733,9922498,"It is only if we wish to describe permutation-like behavior where the number of occurrences of each symbol is hounded, but with an un-bounded number of symbols, that we encounter difficulties.",19,20
14734,9084138,"Thus the path 12223 leads to an occurrence of the terminal symbol b. Then a formula of the form, say, 12 COUNT -22 COUNT would indicate that these paths lead to the same node.",11,12
14735,9084138,"Thus the atomic symbol ""end"" does not appear as part of any derived string.",3,4
14736,17119706,"An S-ranked alphabet is a pair (E,r) consisting of a set E together with a function r :E ~ S* x S assigning a rank (u,s) to each symbol fin ~'~. The string u in S* is the arity off and s is the type of J; The S-ranked alphabets used in this paper have the following property.",40,41
14737,17119706,"For every symbol f of rank (Ul ...Un, S) and any terms t~...tn, with each ti of sort ui, f(t~ .... tn) is a term of sort s. Since every sort in S includes variables, whose arity is e, it is clear that there are terms of every sort.",2,3
14738,17119706,"Z is called the start symbol of the grammar (the standard notation is S not Z, but by bad luck that conflicts with standard notation for the set of sorts).",5,6
14739,17119706,"The ground grammar for G is the four-tuple (N, T, P', Z), where N is the set of all ground terms of (E,r), T is the set of terminals of G, P' is the set of all ground instances of rules in P, and Z is the start symbol of G. If N and P' are finite, the ground grammar is a context-free grammar.",65,66
14740,17119706,We differ from Hopcroft and Ullman by allowing nonterminals other than the start symbol to label the root of the tree.,13,14
14741,17119706,"As in a cfg, A ~ a iff there is an A-tree with yield a. The language generated by a ground grammar is the set of terminal strings derived from the start symbol.",35,36
14742,17119706,"It has just one terminal symbol, b, and its start symbol is start.",5,6
14743,17119706,"It has just one terminal symbol, b, and its start symbol is start.",12,13
14744,17119706,"In this tree the start symbol derives p(0), which derives p(N) by N applications of the second rule.",5,6
14745,17119706,"Yet the start symbol derives p(N) by a tree of depth N, for every N. Thus trees whose frontier has only one symbol can still be arbitrarily deep.",3,4
14746,17119706,"Yet the start symbol derives p(N) by a tree of depth N, for every N. Thus trees whose frontier has only one symbol can still be arbitrarily deep.",24,25
14747,17119706,"a [i k] is the substring of a from space i to space k, where the space before the first symbol is space zero.",23,24
14748,17119706,"We write x tO y or U(x,y) for the union of sets x and y, and also (U i<j<kflj)) for the union of the sets flj) for all j such that i < j < k. If a is the yield of a tree t, then to every occurrence of a symbol A in ~ there corresponds a leaf of t labeled with A. To every node in t there corresponds an occurrence of a substring in ~----the substring dominated by that node.",64,65
14749,17119706,"Proof: This is intuitively clear, but the careful reader may prove it by induction on the depth of t. OPERATIONS ON SETS OF RULES AND TERMS The parser must find the set of ground terms that derive the input string and check whether the start symbol is one of them.",47,48
14750,17119706,"A grammatical expression, or g-expression, is either a term of L, the special symbol nil, or a pair of g-expressions.",18,19
14751,17119706,"Since the grammar is depth-bounded, there exists a number D such that every derivation tree whose yield contains exactly one symbol has depth less than D. Then C D is empty.",23,24
14752,17119706,"Suppose the rules are (a ~ b) (b -'-> c) (c--, d) (d----> k f) (k ~ g) (f""-~ h) The By the definition of ChainTable, close(S) is the set of symbols that derive a symbol of S. In the example grammar, ChainTable is the union of Cl, C2, and C3--that is, the set { [ a b],[b c],[c d],[a c], [b d],[a d]}.",52,53
14753,17119706,"The start symbol appears on the left side of one of these rules iff a[i k] is a sentence of G. By lemma 2.5 this can be tested, so we have a recognizer for the language generated by G. With a small modification the algorithm can find the set of derivation trees of a. We omit details and speak of the algorithm as a parser when strictly speaking it is a recognizer only.",2,3
14754,17119706,Suppose length(a) > 1 and S is the set of dotted rules that derive a with more than one symbol before the dot.,20,21
14755,17119706,"In our example grammar, the set of dotted rules In our example grammar, NewRules ({k}) = {( d ~ k the set of dotted rules that derive a with one symbol before the dot is NewRules(S).",37,38
14756,17119706,This is the set of dotted rules that derive a with one symbol before the dot.,12,13
14757,17119706,"By lemma 3.2, close(finished(rules1)) is the set of symbols that derive a[i k], so by lemma 3.3 rules2 is the set of dotted rules that derive a[i k] with one symbol before the dot.",36,37
14758,17119706,In other words: A E Ed ÷ i iff there is a rule (A ~ aB/3) such that B E Ed and every symbol of a and /3 is in E' d. Let DR be a set of dotted rules and S a set of symbols.,26,27
14759,17119706,"Clearly AdvanceDot*(DR,S) is the set of rules (A --> a/3.y) such that (A --> a. /33') E DR and every symbol of/3 is in S. Let S 1 = $2 = $3= AdvanceDot*(RuleTable, E'd) AdvanceDot(S l, E'd) AdvanceDot*(S2, E'o) S 4 = finished(S3) S~ is the set of dotted rules (A ---> a./30) such that every symbol of a is in E'd.",30,31
14760,17119706,"Clearly AdvanceDot*(DR,S) is the set of rules (A --> a/3.y) such that (A --> a. /33') E DR and every symbol of/3 is in S. Let S 1 = $2 = $3= AdvanceDot*(RuleTable, E'd) AdvanceDot(S l, E'd) AdvanceDot*(S2, E'o) S 4 = finished(S3) S~ is the set of dotted rules (A ---> a./30) such that every symbol of a is in E'd.",78,79
14761,17119706,$2 is then the set of dotted rules (A ---> aB./3 0 such that B ~ Ed and every symbol of a is in E'd.,22,23
14762,17119706,Therefore $3 is the set of dotted rules (A ---> aB/3./32) such that B E Ed and every symbol of a and/3 is in E'd.,22,23
14763,17119706,"Finally $4 is the set of symbols A such that for some rule (A ---> aBfl), B E E d and every symbol of a and/3 is in E' d. Then $4 is E d + i-In this way we can construct Ed for increasing values of d until the table of empty symbols is complete.",27,28
14764,17119706,C~ is the set of pairs [A B] such that (A ---> aB/3) is a rule and every symbol of a and/3 derives the empty string.,23,24
14765,17119706,We say that the string a derives /3 using one symbol if there is a derivation of/3 from a in which exactly one symbol of a derives a non-empty string.,10,11
14766,17119706,We say that the string a derives /3 using one symbol if there is a derivation of/3 from a in which exactly one symbol of a derives a non-empty string.,23,24
14767,17119706,We say that a derives 13 using many symbols if there is a derivation of/3 from a in which more than one symbol of a derives a nonempty string.,22,23
14768,17119706,"If a string a derives a string/3, then a derives/3 using one symbol, or a derives/3 using many symbols, or both.",13,14
14769,17119706,"In the example grammar, cfc derives r using one symbol, and cfcg derives rs using many symbols.",10,11
14770,17119706,"Note that a dotted rule derives a[i k] using many symbols iff it can be written as (A --~ /3B/3'./30 where/3~a[ij], B ~ a[j k], /3' ~ e, and i < j < k. This is true because whenever a dotted rule derives a string using many symbols, there must be a last symbol before the dot that derives a nonempty string.",64,65
14771,17119706,"Let B be that symbol; it is followed by a/3' that derives the empty string, and preceded by a/3 that must contain at least one more symbol deriving a non-empty string.",4,5
14772,17119706,"Let B be that symbol; it is followed by a/3' that derives the empty string, and preceded by a/3 that must contain at least one more symbol deriving a non-empty string.",29,30
14773,17119706,"If S is the set of symbols that derive a, the set of dotted rules that derive a using one symbol is NewRules'(S). {(",21,22
14774,17119706,"A~ /3C/3'./33) EDRI /3~e/XC~a/%/3'~e} This is the set of dotted rules that derive a using one symbol, by definition.",24,25
14775,17119706,"If PredTable is indeed a complete prediction table, first(S) is the set of symbols B such that some symbol in S can begin with B. IfR is a set of dotted rules let next (R) = {B ] (3 A,/3,/3'. (",20,21
14776,17119706,Let start be the start symbol of the grammar.,5,6
14777,17119706,"Therefore rulesz is the set of dotted rules that follow a[0/] and derive a[i k] using one symbol, by lemma 5.6.",19,20
14778,17119706,"So if the parser with filtering puts the start symbol in dr(O,L), the parser without filtering will do this also, implying that a is a sentence.",9,10
14779,18253936,"This parsing algorithm is similar to the shift-reduce parser except that our algorithms handles ambiguities, parallel processing of each hypothesis, and top-down predictions of possible next input symbol.",33,34
14780,18978709,"Th/s algorithm builds phrases bottom-up from the left-comer, i.e., rules are selected by the first symbol of their r/ght-hand-s/des.",23,24
14781,18978709,"When a non-terminal symbol is encountered, a subparse is initiated.",5,6
14782,16779219,a non-terminal symbol).,4,5
14783,16779219,"Roughly, the states of the PDA are symbolized by the contents of the parser's buffer, and its stack symbols are ordered pairs consisting of a non-terminai symbol (Le.. a stack symbol of the parser) and a set of packets associated with that symbol Let N be the set of non-terminal symbols, and Y"" be the set of terminal symbols of the pazser.",31,32
14784,16779219,"Roughly, the states of the PDA are symbolized by the contents of the parser's buffer, and its stack symbols are ordered pairs consisting of a non-terminai symbol (Le.. a stack symbol of the parser) and a set of packets associated with that symbol Let N be the set of non-terminal symbols, and Y"" be the set of terminal symbols of the pazser.",37,38
14785,16779219,"Roughly, the states of the PDA are symbolized by the contents of the parser's buffer, and its stack symbols are ordered pairs consisting of a non-terminai symbol (Le.. a stack symbol of the parser) and a set of packets associated with that symbol Let N be the set of non-terminal symbols, and Y"" be the set of terminal symbols of the pazser.",50,51
14786,16779219,"r = the set of stack symbols [X.P], where XeN is a non-terminal symbol of the parser and P is a set of packets.",18,19
14787,16779219,"More specifically, there would be a bundle of features associated with each symbol.",13,14
14788,16779219,"When the node X is dropped, its associated features would be copied to the X symbol appea.tinll in the state of the PDA (via first _8-move).",16,17
14789,16779219,The second _8-move allows m to copy the features from the X symbol in the state to the X node dominated by the node 7_ (iii) Accommodation of fC2tur~$; The features used in Marcus' parser are syntactic in nature and have f'mite domains.,15,16
14790,16779219,Feature assitmments can be simulated by .replacing the top stack symbol in the PDA.,10,11
14791,1977251,"For instance, if no rules in (1) and ( 2 ) are invoked by an input pair of tags, where the second input tag denotes some form of verb, then the default rule -VB = Y][V is invoked such that any tag followed by any form of verb closes the constituent left ope n by a previous T-tag look-up rule (where 'Y' is a symbol denoting any hypertag).",76,77
14792,15128029,Since we are considering formalisms with arbitrary structures it is difficult to precisely specify all of the restrictions on the composition operations that we believe would appropriately generalize the concatenation operation for the particular 2 We denote • tree derived from the elemeatany Wee -f by the symbol '~. structures used by the formalism.,47,48
14793,15128029,"Roughly speaking, a language, L, has the property of semillnearity if the number of occurrences of each symbol in any suing is a linear combination of the occurrences of these symbols in some fixed finite set of strings.",20,21
14794,15128029,"Two strings are letter equivalent if they contain equal number of occurrences of each terminal symbol, and two languages are letXer equivalent if every string in one language is letter equivalent to a string in the other language and vice-versa.",15,16
14795,15128029,"A $~p of an ATM consists of reading a symbol from each tape and optionally moving each head to the left or right one tape ceiL A configuration of M consists of a state of the finite control, the nonblank contents of the input tape and k work tapes, and the position of each head.",10,11
14796,15128029,"Suppose M has to determine whether the k substrings zx,..., zk can be derived from some symbol A. Since each zi is a contiguous substrin 8 of the input (say a~x ... a~2), and no two substrings overlap, we can represent zi by the pair of intoge~'s (ix, i2).",19,20
14797,5094470,"It is defined as an 8-tuple T G --(Q, ]~, A, H, 6, ~, ;, F) where: Q is the set of states, ~ is the set of input word symbols, A is the set of stack symbols, H is the set of output symbols s (i.e. rule• of G), q is the initial state, $ is the initial stack symbol, F is the set of final states, 6 is a fnite set of transitions of the form: (p A a ~-* q B u) with p, q E Q, x,s ¢ A u {e}, a E ~: u {~}, and .",79,80
14798,5094470,"Let the PDT be in a configuration p --(p Aa az u) where p is the current state, Aa is the •tack contents with A on the top, az is the remaining input where the symbol a is the next to be shifted and z E ~*, and u is the already produced output.",38,39
14799,5094470,"The application of a transition r = (p A a ~-* qB v) result• in a new configuration p' ----(q Bot z uv) where the terminal symbol a has been scanned (i.e. shifted), A has been popped and B has been pushed, and t, has been concatenated to the existing output ,~ If the terminal symbol a is replaced by e in the transition, no input symbol is scanned.",31,32
14800,5094470,"The application of a transition r = (p A a ~-* qB v) result• in a new configuration p' ----(q Bot z uv) where the terminal symbol a has been scanned (i.e. shifted), A has been popped and B has been pushed, and t, has been concatenated to the existing output ,~ If the terminal symbol a is replaced by e in the transition, no input symbol is scanned.",66,67
14801,5094470,"The application of a transition r = (p A a ~-* qB v) result• in a new configuration p' ----(q Bot z uv) where the terminal symbol a has been scanned (i.e. shifted), A has been popped and B has been pushed, and t, has been concatenated to the existing output ,~ If the terminal symbol a is replaced by e in the transition, no input symbol is scanned.",78,79
14802,5094470,B) is replaced by • then no stack symbol is popped from (resp.,9,10
14803,5094470,"Our algorithm consist• in an Earley-like 9 simulation of the PDT T G. Using the terminology of [1] , the algorithm builds an item set ,~ successively for each word symbol z~ holding position i in the input sentence z. An item is constituted of two modes of the form (p A i) where p is a PDT state, A is a stack symbol, and i.is the index of an input symbol.",35,36
14804,5094470,"Our algorithm consist• in an Earley-like 9 simulation of the PDT T G. Using the terminology of [1] , the algorithm builds an item set ,~ successively for each word symbol z~ holding position i in the input sentence z. An item is constituted of two modes of the form (p A i) where p is a PDT state, A is a stack symbol, and i.is the index of an input symbol.",71,72
14805,5094470,"Our algorithm consist• in an Earley-like 9 simulation of the PDT T G. Using the terminology of [1] , the algorithm builds an item set ,~ successively for each word symbol z~ holding position i in the input sentence z. An item is constituted of two modes of the form (p A i) where p is a PDT state, A is a stack symbol, and i.is the index of an input symbol.",80,81
14806,5094470,Subtree sharing is represented by seVo eral uses of the same symbol in rule right-hand sides.,11,12
14807,5094470,pop) to push a symbol on the stack (reap.,5,6
14808,5094470,"pop one), check~indow to compare the look-ahead symbol(s) to some given symbol, chsckstack to branch depending on the top of the sta~k, scan to read an input word, outpu$ to output a rule number (or a terminal symbol), goto for unconditional jumps, and a few others.",18,19
14809,5094470,"pop one), check~indow to compare the look-ahead symbol(s) to some given symbol, chsckstack to branch depending on the top of the sta~k, scan to read an input word, outpu$ to output a rule number (or a terminal symbol), goto for unconditional jumps, and a few others.",50,51
14810,5094470,"The symbol r stands for the rule used to recognize a constituent s, and ~ri stands for the rule used to recognize its i 'h sub-constituent ei.",1,2
14811,5478765,"If the function returns a non-nil value, the value becomes the semantic of the right-handside of the rule and is forwarded to the left-hand-side nonterminal symbol.",34,35
14812,671232,"For example, according to Table 7 , the noun cargo instantiates Z, while the verb pertencer instantiates X. The symbol ¦ stands for Boolean disjunction.",21,22
14813,274821,"The motivation comes from linguistic phenomena where it is intuitively clear that one symbol (linguistic category) in the rule is noticeably more distinctive than others, so that a parser should not waste time trying to match the rule unless that distinctive element is there.",13,14
14814,274821,"As outlined above, the central idea is to allow different rules to be marked as either top-down (LHS trigger symbol) or bottom-up (RHS trigger symbol(s)), or both.",23,24
14815,274821,Top-down means that the rule can be invoked only if some other rule has established a need for its LHS symbol (or if the LHS symbol is the initial symbol of the grammar).,22,23
14816,274821,Top-down means that the rule can be invoked only if some other rule has established a need for its LHS symbol (or if the LHS symbol is the initial symbol of the grammar).,28,29
14817,274821,Top-down means that the rule can be invoked only if some other rule has established a need for its LHS symbol (or if the LHS symbol is the initial symbol of the grammar).,32,33
14818,274821,"We shall assume that rules of the form A --* w where w is a terminal symbol are never annotated, and can be used whenever needed in the parser (all this is made precise in our formalization in Section 3.3 below).",17,18
14819,274821,"For this informal presentation, and occasionally elsewhere, we shall mark a trigger symbol A by overlining it, thus: A. In the illustrative examples, the distinguished (initial) symbol of the grammar will always be S and terminal symbols will be in lower case.",14,15
14820,274821,"For this informal presentation, and occasionally elsewhere, we shall mark a trigger symbol A by overlining it, thus: A. In the illustrative examples, the distinguished (initial) symbol of the grammar will always be S and terminal symbols will be in lower case.",33,34
14821,274821,"It would be very easy to ensure that annotation does not lose analyses, by stipulating that all rules are marked as top-down, or that all rules are marked as bottom-up with the leftmost symbol as a trigger.",39,40
14822,274821,"With just the smaller grammar, the nonterminal H cannot be expanded as required, since it is on the LHS of a bottom-up rule, and its first symbol B cannot be recognized because it requires a top-down rule.",32,33
14823,274821,"Following the usual conventions (e.g., Aho and Ullman 1972), we will take a context-free grammar (CFG) to be a quadruple (VN, VT, P, S), consisting of a set VN of nonterminal symbols, a set VT of terminal symbols, a set P of rules (productions), and a single distinguished symbol S E VN.",67,68
14824,274821,"Set theoretically, rules can be regarded as being ordered pairs where the first element is a nonterminal symbol and the second is a tuple of symbols, i.e., of the form (A0, (A1 ..... Ak)) where k > 0, but for ease of exposition they will be written as A0 --+ A1 ... Ak We will make the following simplifying assumptions (which do not lose generality): . .",18,19
14825,274821,The root node is labeled with S (the distinguished symbol).,10,11
14826,274821,"A derivation is a sequence of symbol strings w1 ..... wn such that wi ~ wi+l for all 1 < i < n. A rightmost derivation is one in which each step from wi to ~i+1 is made by replacing the furthest right nonterminal symbol in W i USing some rule (i.e., -y in the above definition of directly derives is entirely made up of terminal symbols) (cf.",6,7
14827,274821,"A derivation is a sequence of symbol strings w1 ..... wn such that wi ~ wi+l for all 1 < i < n. A rightmost derivation is one in which each step from wi to ~i+1 is made by replacing the furthest right nonterminal symbol in W i USing some rule (i.e., -y in the above definition of directly derives is entirely made up of terminal symbols) (cf.",44,45
14828,274821,"Top-down initialization) For every rule r in G of the form S --~ B1... Bk, where S is the distinguished symbol of G and 0 c tr(r), there is an edge in C of the form: (0,0,S ~ • ° B1...Bk) 3. (",25,26
14829,274821,"A first approximation to the definition for the property of nonterminals would be the following: Draft Definition: Given a BSCFG (G, tr), a nonterminal symbol A0 is directly analyzable iff every rule r of the form A0 --* ... is either lexical, or of the form Ao --* A1 ... Ak with at least one i E tr(r), i > 0, for which Ai is directly analyzable.",30,31
14830,274821,"This predicate APMAX(G,tr) will assign true to a symbol A if there is some analyzability predicate (for (G, tr)) that makes this assignment.",11,12
14831,274821,"Instead of the draft definition, we can now have the following complete definition: Definition 16 Given a BSCFG (G, tr), a nonterminal symbol A0 is directly APMAX(c, tr) (Ao) = true, where APMAX(G,tr) is as constructed above.",28,29
14832,274821,"For a top-down rule to be invoked, it must be used in a position at which some prediction of its LHS symbol A will be introduced (by some other rule).",24,25
14833,274821,"Proof Let T be a tree in trees(G), spanning the string or, with root node M0 labeled S (the distinguished symbol of G).",24,25
14834,274821,"A strategy-marked grammar (G, tr) causes problems only if there is some purely bottomup rule of the form Ao --* A1... Ak such that every trigger symbol Ai requires a purely top-down rule somewhere in its expansion (see Section 4.2 above).",33,34
14835,274821,"The ""upper"" symbol H cannot be parsed because the ""lower"" symbol B cannot be parsed.",4,5
14836,274821,"The ""upper"" symbol H cannot be parsed because the ""lower"" symbol B cannot be parsed.",15,16
14837,274821,"What salvages this difficulty is the fact that the ""upper"" nonterminal (H in this example) always occurs in a left context, (i.e., a string of symbols to its left) with the following property: Every possible terminal expansion of the left context contains a substring that will, via bottom-up rules, introduce rules that are bound to result in the introduction of an active edge, which starts at the point where the ""upper"" symbol (H) is needed and which is seeking the ""lower"" symbol (B).",87,88
14838,274821,"What salvages this difficulty is the fact that the ""upper"" nonterminal (H in this example) always occurs in a left context, (i.e., a string of symbols to its left) with the following property: Every possible terminal expansion of the left context contains a substring that will, via bottom-up rules, introduce rules that are bound to result in the introduction of an active edge, which starts at the point where the ""upper"" symbol (H) is needed and which is seeking the ""lower"" symbol (B).",101,102
14839,274821,"In the following proofs, we will define constructs for BSCFGs where possible, simply for generality, but where it matters we shall confine attention to LCSCFGs, thereby narrowing the range of contexts relevant to parsing a particular symbol.",40,41
14840,274821,"Left Contexts Following from the informal discussion in Section 5.1 above, we need to define more precisely the notion of a left context of a symbol.",26,27
14841,274821,From this it is trivial to form an essential rightmost derivation for B1 from some symbol Bk (where k < t): Bk ~ Pk-1 ...plBlWl where a;1 E V~ and Pk-I'''plBIOdl ~ 0 where 0 is the substring of ~ spanned by Nk.,15,16
14842,274821,"Bottom-up Derivations Definition 20 In a BSCFG (G, tr), suppose A is a nonterminal symbol, and cr is a string of terminal symbols.",20,21
14843,274821,"Suppose A is a nonterminal symbol, and ~r is a string of terminal symbols, from G .",5,6
14844,274821,"A nonterminal symbol A0 in G is said to be indirectly analyzable iff every rule A0 --* ~; is either lexical, or top-down and left-introducible, or bottom-up of the form A0 --* A-~c~ where A1 is indirectly analyzable.",2,3
14845,274821,"The only symbol X for which X -,z B2 is B2 itself.",2,3
14846,274821,"Intuitively, this is similar to the phenomenon defined earlier as left-introducible, but with the catalytic sequence of rules being triggered top-down from the distin-guished symbol of the grammar.",32,33
14847,274821,"Although some of the head-driven strategies are said to act ""top-down,"" this refers to the parser exploring from a prediction of a specific nonterminal symbol in some region of the input, but not to rules being introduced because the grammar writer has indicated that it is to be introduced top-down in the sense used here.",31,32
14848,274821,"There are similarities between the bidirectional scheme here and the head-corner parser of Sikkel and op den Akker 1996, in which top-down predictions can arise either from the distinguished symbol (predicted to span the whole input) or by working outwards from the specified head constituent (as in the left and right extension principles of Definition 9 in Section 3.3).",34,35
14849,274821,"Sikkel and op den Akker's chart handling principles all have the precondition that the introduction of the new edge can happen only if the region of the input in question is spanned by a predictive edge seeking a symbol A such that A>~B, where B is the label of the constituent or prediction being introduced.",39,40
14850,274821,"Definition 25 A LCSCFG (G, tr) is indirectly analyzable iff for every purely bottom-up rule A0 -* Alc~, the nonterminal symbol A1 is indirectly analyzable.",27,28
14851,274821,"Hence whatever rule licenses M1, it must be either lexical, or top-down and left-introducible, or bottom-up with an indirectly analyzable symbol at the start of its RHS.",29,30
14852,274821,"If the rule is top-down and left-introducible, there is an edge seeking its LHS symbol at the start of M1, and so, by the Inductive Hypothesis, there is a representation of the tree rooted at M1 in C. Since C is fully bidirectional, it contains an edge spanning M1 of the form: Repeated applications of the Inductive Hypothesis and Lemma 1 (Corollary) establish that there is a representation of the tree rooted at M0 in C (i.e., the Inductive Step).",19,20
14853,274821,"Then M1 is labeled with an indirectly analyzable symbol, A1.",8,9
14854,274821,"Since r' is bottom-up and A1 is the leftmost (trigger) symbol of its RHS, this leads to an active edge of the form If a LCSCFG (G, tr) is indirectly analyzable, then tr is complete.",15,16
14855,274821,The distinguished symbol S ~ is distinct from all symbols in V 1 U V 2.,2,3
14856,274821,"The purely bottom-up rules are all the rules of G2, together with where each Si is the distinguished symbol of Gi.",21,22
14857,274821,"The purely top-down rules are all the rules of G1 together with Also we include lexical rules: for some terminal symbols a, b, c. This LCSCFG is indirectly analyzable iff (by definition) every purely bottom-up rule has an indirectly analyzable symbol at the start of its RHS (the trigger position).",49,50
14858,274821,"Hence the grammar is indirectly analyzable iff in the rule B1 --+ B2B3, the trigger symbol B2 is indirectly Appendix A: Computing Direct Analyzability The algorithm is a simple variant of the use of an AND-OR graph in problem solving, as in Nilsson (1971) .",16,17
14859,274821,"The graph will contain a node for each nonterminal symbol A in the grammar, and an OR node for each bottom-up rule.",9,10
14860,274821,"Each node has a label, which is either a nonterminal symbol or OR, and may, optionally, have a marking, which is either SOLVED or FAILED.",11,12
14861,274821,"For each symbol A c VN, create a node NA, and insert arcs and markings as follows: if there is a purely TD rule of the form A --* a then mark NA as FAILED else if all rules of the form A --* a are lexical then mark NA as SOLVED else for each bottom-up rule of the form A --* A1 At this point, each non-terminally labeled node has outgoing arcs for every bottom-up rule that might expand it, and each of these arcs connects to an OR node, which in turn connects to Appendix B: Undecidability Proof nemma For any two context-free grammars G1, G2, it is undecidable whether every member of L(G1) ends in a substring that is a member of L(G2).",2,3
14862,274821,"Let # be a symbol that is not a member of V. Consider the language L~ given by: {#x Ix e L(G1)} and L~ given by: {#YIY c L(G2)} i t These are both context-free languages; assume that grammars G 1, G 2 generate them.",4,5
14863,8473532,"Moreover, during a match process this means that a constituent of the right-hand side has been consumed, and matching the first symbol that-match process is f'mished.",25,26
14864,8473532,"A special symbol is e a, a null category that can occur only in the left-hand side of a reduction rule.",2,3
14865,1983416,Double dashes mark sounds fur which an particular alphabet has no distinctive written symbol. (,13,14
14866,1983416,"For transliteration from one national alphabet into another, these symbol equivalences are needed.",10,11
14867,1983416,"Even in Europe, Spanish accented vowels ~, ~, ~_, _6, ~ show a v~l sup~mpomiti~ of the basic vowels with a functionally independent symbol of accentnation.",34,35
14868,16420577,"However, if a special boundary symbol is permitted, the full family of regular languages can be generated.",6,7
14869,16420577,"This paper establishes the following: (i) (ii) (iii) (iv) (v) an alternative (declarative) statement of the meaning of the high-level rule notation is possible, without recourse to compilation into finite-state transducers; the usual two-level morphological mechanism is more limited than arbitrary transducers in its ability to define relationships between strings; the use of a special boundary symbol slightly increases the generative power of the model; for any two-level morphological grammar, there is an equivalent one in a simpler normal form; the family of languages generated by two-level rules is not closed under union or complementation, but is closed under intersection.",78,79
14870,16420577,"The third level is a similar ""surface tape"" consisting of symbols in a surface alphabet, with the null symbol also appearing at arbitrary positions.",21,22
14871,16420577,"The fourth level, at the bottom in Figure 1 , represents the word as spelt (or phonetically represented) in the surface alphabet; the relationship between the surface tape and the surface form is that the latter can be produced from the former by removing all occurrences of the special null symbol.",54,55
14872,16420577,"The formalism as a whole can be used to describe phenomena involving a surface string of a different length from the lexical forms, via the conventions regarding null symbols, but the rules themselves are written as statements about symbol-strings of equal length.",40,41
14873,16420577,"Hence, the rules can be viewed as statements about sequences of symbol-pairs, where each symbol-pair consists of one symbol from the lexical tape and its counterpart from the surface tape.",12,13
14874,16420577,"Hence, the rules can be viewed as statements about sequences of symbol-pairs, where each symbol-pair consists of one symbol from the lexical tape and its counterpart from the surface tape.",18,19
14875,16420577,"Hence, the rules can be viewed as statements about sequences of symbol-pairs, where each symbol-pair consists of one symbol from the lexical tape and its counterpart from the surface tape.",24,25
14876,16420577,"Conventionally, symbol-pairs are written with an infix colon, rather than as a pair within parentheses.",2,3
14877,16420577,"The rules specify which sequences of symbol-pairs are or are not valid, by supplying contexts (sequences of symbol-pairs) within which a particular symbol-pair may or must occur.",6,7
14878,16420577,"The rules specify which sequences of symbol-pairs are or are not valid, by supplying contexts (sequences of symbol-pairs) within which a particular symbol-pair may or must occur.",21,22
14879,16420577,"The rules specify which sequences of symbol-pairs are or are not valid, by supplying contexts (sequences of symbol-pairs) within which a particular symbol-pair may or must occur.",29,30
14880,16420577,"There are three types of rule, but they all have the same overall form--a single symbol-pair whose behaviour is being specified, followed by an operator (=>, <---or <--->), a left context, a punctuation (""___"" in this paper) to indicate where the symbol pair appears relative to the context, and a right context.",18,19
14881,16420577,"There are three types of rule, but they all have the same overall form--a single symbol-pair whose behaviour is being specified, followed by an operator (=>, <---or <--->), a left context, a punctuation (""___"" in this paper) to indicate where the symbol pair appears relative to the context, and a right context.",61,62
14882,16420577,"The three types of rule are as follows: Context Restriction (operator =>): states that the given symbol-pair can occur only in the given contexts, as in the simple example given above.",20,21
14883,16420577,"Surface Coercion (operator <=): states that if the contexts occur, and the lexical symbol is as given, then the surface symbol must be as specified.",17,18
14884,16420577,"Surface Coercion (operator <=): states that if the contexts occur, and the lexical symbol is as given, then the surface symbol must be as specified.",25,26
14885,16420577,"For example e:l <= l:i +:0 indicates that where a lexical Z corresponds to a surface ±, there is a pairing of lexical + with a surface null symbol one place further to the right, and the intervening symbolpair has a lexical e, then the surface symbol in that pair must be 1.",35,36
14886,16420577,"For example e:l <= l:i +:0 indicates that where a lexical Z corresponds to a surface ±, there is a pairing of lexical + with a surface null symbol one place further to the right, and the intervening symbolpair has a lexical e, then the surface symbol in that pair must be 1.",55,56
14887,16420577,"Composite (operator <=>): a rule of this sort is merely an abbreviation of two separate rules (a context restriction rule and a surface coercion rule) made up of exactly the same symbol-pair, left context and right context.",37,38
14888,16420577,"For example i:y <=> -:-___ e:O +:0 i:i states that lexical ± and surface y are paired only in the context of anything on the left (if we assume that the ""="" symbol means ""any symbol"") and a sequence of pairings on the right as given.",46,47
14889,16420577,"For example i:y <=> -:-___ e:O +:0 i:i states that lexical ± and surface y are paired only in the context of anything on the left (if we assume that the ""="" symbol means ""any symbol"") and a sequence of pairings on the right as given.",50,51
14890,16420577,"For a sequence of symbol-pairs to be acceptable, if any symbol-pair in it was the subject of one or more context restriction rules, then at least one of these rules must apply to the surrounding sequence (i.e. the contexts must match).",4,5
14891,16420577,"For a sequence of symbol-pairs to be acceptable, if any symbol-pair in it was the subject of one or more context restriction rules, then at least one of these rules must apply to the surrounding sequence (i.e. the contexts must match).",13,14
14892,16420577,"Also, if the surrounding contexts of any symbol-pair matched the context parts of any surface coercion rule, then the symbol-pair must obey that rule.",8,9
14893,16420577,"Also, if the surrounding contexts of any symbol-pair matched the context parts of any surface coercion rule, then the symbol-pair must obey that rule.",23,24
14894,16420577,"That is, in a rule-set (a two-level morphological grammar), all the context restriction rules that have the same symbol-pair on the left-hand side of the ~ operator were deemed to be a disjunction of constraints for that symbol-pair, and the surface coercion rules were a conjunction of constraints (regardless of which symbol-pair they constrained).",26,27
14895,16420577,"That is, in a rule-set (a two-level morphological grammar), all the context restriction rules that have the same symbol-pair on the left-hand side of the ~ operator were deemed to be a disjunction of constraints for that symbol-pair, and the surface coercion rules were a conjunction of constraints (regardless of which symbol-pair they constrained).",49,50
14896,16420577,"That is, in a rule-set (a two-level morphological grammar), all the context restriction rules that have the same symbol-pair on the left-hand side of the ~ operator were deemed to be a disjunction of constraints for that symbol-pair, and the surface coercion rules were a conjunction of constraints (regardless of which symbol-pair they constrained).",67,68
14897,16420577,In specifying symbol-pairs in rules (whether in the central pair or within the contexts) various convenient abbreviations were available.,2,3
14898,16420577,"In specifying contexts (left and right), it was possible to supply more complex expressions than just sequences of symbol-pairs.",21,22
14899,16420577,"Essentially, regular expressions of symbol-pairs were allowed (regular pair expressions as Koskenniemi called them), since notation was available to state alternation (disjunction), optionality, and the occurrence of zero or more instances (Kleene star).",5,6
14900,16420577,"1987) , angle brackets indicate sequences of pairs and braces indicate alternatives; also, C, V, C2, and = represent subsets of the relevant symbol alphabets and + is an abstract symbol occurring in certain lexical forms.",29,30
14901,16420577,"1987) , angle brackets indicate sequences of pairs and braces indicate alternatives; also, C, V, C2, and = represent subsets of the relevant symbol alphabets and + is an abstract symbol occurring in certain lexical forms.",36,37
14902,16420577,"One important subtlety is the use of the empty string as an element of a symbol-pair, with the consequence that a ""regular relation"" can associate strings of unequal lengths together, assuming the obvious equivalence between a sequence of symbol-pairs and a pair of symbol-strings (see Section 2 above).",15,16
14903,16420577,"One important subtlety is the use of the empty string as an element of a symbol-pair, with the consequence that a ""regular relation"" can associate strings of unequal lengths together, assuming the obvious equivalence between a sequence of symbol-pairs and a pair of symbol-strings (see Section 2 above).",44,45
14904,16420577,"One important subtlety is the use of the empty string as an element of a symbol-pair, with the consequence that a ""regular relation"" can associate strings of unequal lengths together, assuming the obvious equivalence between a sequence of symbol-pairs and a pair of symbol-strings (see Section 2 above).",51,52
14905,16420577,"In the associated transducer form, the empty symbol corresponds to no symbol being scanned or no symbol being output (i.e. one of transducer's ""heads"" may advance while the other does not).",8,9
14906,16420577,"In the associated transducer form, the empty symbol corresponds to no symbol being scanned or no symbol being output (i.e. one of transducer's ""heads"" may advance while the other does not).",12,13
14907,16420577,"In the associated transducer form, the empty symbol corresponds to no symbol being scanned or no symbol being output (i.e. one of transducer's ""heads"" may advance while the other does not).",17,18
14908,16420577,"It is important to note that here e is a genuine empty string, interpreted by a transducer as a lack of transition; this is different from two-level morphology's ""null"" symbol 0, which acts as an ordinary symbol for the transducers (or for rule-matching), but is treated specially when relating the ""tapes"" to other linguistic levels (see Section 2 above).",36,37
14909,16420577,"It is important to note that here e is a genuine empty string, interpreted by a transducer as a lack of transition; this is different from two-level morphology's ""null"" symbol 0, which acts as an ordinary symbol for the transducers (or for rule-matching), but is treated specially when relating the ""tapes"" to other linguistic levels (see Section 2 above).",44,45
14910,16420577,"2 He also showed that the uniform deletion/insertion of a particular symbol (such as the special null symbol used in two-level rules) at arbitrary points in a string could be expressed as a regular relation, so the entire lexicon-to-surface mapping sketched in Section 2 earlier could be stated as a regular relation.",13,14
14911,16420577,"2 He also showed that the uniform deletion/insertion of a particular symbol (such as the special null symbol used in two-level rules) at arbitrary points in a string could be expressed as a regular relation, so the entire lexicon-to-surface mapping sketched in Section 2 earlier could be stated as a regular relation.",20,21
14912,16420577,"Given any two finite symbolic alphabets, A and A', a symbol-pair from A and A' is a pair (a, a') where a c A and a' c A t. Such symbol-pairs will normally be written as ""a:a"".",13,14
14913,16420577,"Given any two finite symbolic alphabets, A and A', a symbol-pair from A and A' is a pair (a, a') where a c A and a' c A t. Such symbol-pairs will normally be written as ""a:a"".",41,42
14914,16420577,"A symbol-pair sequence from A and A' is simply a sequence (possibly empty) of symbol-pairs from A and A', and a symbol-pair language over A and A' is a set of symbol-pair sequences (i.e. a subset of (A x A')*).",1,2
14915,16420577,"A symbol-pair sequence from A and A' is simply a sequence (possibly empty) of symbol-pairs from A and A', and a symbol-pair language over A and A' is a set of symbol-pair sequences (i.e. a subset of (A x A')*).",19,20
14916,16420577,"A symbol-pair sequence from A and A' is simply a sequence (possibly empty) of symbol-pairs from A and A', and a symbol-pair language over A and A' is a set of symbol-pair sequences (i.e. a subset of (A x A')*).",30,31
14917,16420577,"A symbol-pair sequence from A and A' is simply a sequence (possibly empty) of symbol-pairs from A and A', and a symbol-pair language over A and A' is a set of symbol-pair sequences (i.e. a subset of (A x A')*).",43,44
14918,16420577,"Given two alphabets A and A t, and a symbol-pair sequence G from A and A', a sequence (P1,..Pn) of symbol-pair sequences from A and A' is said to be a partition of iff G = P1P2....Pn (i.e. G is made up of the concatenation of the Pi).",10,11
14919,16420577,"Given two alphabets A and A t, and a symbol-pair sequence G from A and A', a sequence (P1,..Pn) of symbol-pair sequences from A and A' is said to be a partition of iff G = P1P2....Pn (i.e. G is made up of the concatenation of the Pi).",29,30
14920,16420577,"Given two alphabets A and A', a two-level morphological rule over A and A' consists of a pair (P, C) where P is a symbol-pair from A and A', and C is a nonempty set of pairs (LC,RC) where LC and RC are sets of symbol-pair sequences from A and A'; each such set of symbol-pair sequences is called a context set.",32,33
14921,16420577,"Given two alphabets A and A', a two-level morphological rule over A and A' consists of a pair (P, C) where P is a symbol-pair from A and A', and C is a nonempty set of pairs (LC,RC) where LC and RC are sets of symbol-pair sequences from A and A'; each such set of symbol-pair sequences is called a context set.",61,62
14922,16420577,"Given two alphabets A and A', a two-level morphological rule over A and A' consists of a pair (P, C) where P is a symbol-pair from A and A', and C is a nonempty set of pairs (LC,RC) where LC and RC are sets of symbol-pair sequences from A and A'; each such set of symbol-pair sequences is called a context set.",75,76
14923,16420577,"A set 0 is said to match at the right-end a symbol-pair sequence G iff there is a partition (P1, P2) of ~ such that P2 E 0.",13,14
14924,16420577,"A set 0 is said to match at the left-end a symbol-pair sequence G iff there is a partition (P1, P2) of ~ such that P1 c 0.",13,14
14925,16420577,"A set R of two-level morphological rules contextually allows a symbol-pair sequence ~ iff, for every partition (P1, a:a t, P2) of ~, either there is no rule of the form (a:a', C) c R, or there is at least one rule (a:a', C) c R such that C contains a pair (LC, RC) such that LC matches P1 at the right end and RC matches P2 at the left end.",12,13
14926,16420577,"A two-level morphological rule ((a,a'), C) coercively allows a symbol-pair sequence G iff for every possible partition (P1, b:b t, P2) of ~ and every element (LC, RC) E C such that LC matches P1 at the right end, and RC matches P2 at the left end, if b = a, then b' = a'.",19,20
14927,16420577,"An alternative but equally useful variation on the last definition would be that a two-level morphological rule ((a,at), C) coercively disallows a symbol-pair sequence iff there is a possible partition (P1, b:b', P2) of ~ and an element (LC, RC) E C such that LC matches P1 at the right end, RC matches P2 at the left end, b = a and b t # a'.",31,32
14928,16420577,"Roughly speaking, the set of feasible pairs is the set of symbol-pairs that have to be considered as potential elements of symbol-pair sequences.",12,13
14929,16420577,"Roughly speaking, the set of feasible pairs is the set of symbol-pairs that have to be considered as potential elements of symbol-pair sequences.",24,25
14930,16420577,"The normal approach is not to allow the whole cross-product A x A' as possible symbol pairs, but to define a subset of this as being the effective alphabet under consideration.",18,19
14931,16420577,This is normally done in three ways: • Any symbol a that appears in both the lexical alphabet and the surface alphabet gives rise to a feasible pair a'a. •,10,11
14932,16420577,Any symbol-pair that is explicitly mentioned in a context-expression anywhere in a rule is feasible. •,1,2
14933,16420577,Any symbol-pair that is explicitly declared to be feasible (in a list supplied along with the rules) is feasible.,1,2
14934,16420577,"The set of feasible pairs is then used in two ways--any variables or sets occurring in the statement of rules are deemed to range only over feasible pairs, not over arbitrary symbol-pairs; also, any feasible pair may occur freely in a symbol-pair sequence if not otherwise constrained by the rules of the grammar.",34,35
14935,16420577,"The set of feasible pairs is then used in two ways--any variables or sets occurring in the statement of rules are deemed to range only over feasible pairs, not over arbitrary symbol-pairs; also, any feasible pair may occur freely in a symbol-pair sequence if not otherwise constrained by the rules of the grammar.",48,49
14936,16420577,"We are assuming that no variables or symbol-set-mnemonics appear in our rules, so the question of using the feasible pair set to expand or give meaning to such abbreviations is irrelevant, but there is still the question of freedom of occurrence.",7,8
14937,16420577,"A trivial rule like this allows the symbol-pair to occur freely, since any adjacent material will match the empty string. (",7,8
14938,16420577,The exact definition of matching is important here---matching the empty string or sequence means that there is some partition of the surrounding string in which the portion of the partition next to the symbol-pair of interest is the empty sequence; it does not mean that there are no adjacent symbol-pairs).,35,36
14939,16420577,The exact definition of matching is important here---matching the empty string or sequence means that there is some partition of the surrounding string in which the portion of the partition next to the symbol-pair of interest is the empty sequence; it does not mean that there are no adjacent symbol-pairs).,54,55
14940,16420577,"The point is that the original textually-based definitions of feasibility are merely a notational convenience for conveying (to the human reader or a software interpreter/compiler) a set of symbol-pairs that includes at least the pairs from the contexts (and the identity pairs), and we shall abstractly regard that set as being part of the definition of the grammar, regardless of the notation used to make it manifest.",34,35
14941,16420577,"A symbol-pair a:a' is said to be a string-constituent in a set 0 of symbol-pair sequences if there is at least one element s C 0 such that a:a ~ is an element of the sequence s. A symbol-pair a:a ~ is said to occur in a rule (b:b p, C) iff either a:a' = b:b' or for at least one element (LC, RC) of C, a:a' is a string-constituent in at least one of LC and RC.",1,2
14942,16420577,"A symbol-pair a:a' is said to be a string-constituent in a set 0 of symbol-pair sequences if there is at least one element s C 0 such that a:a ~ is an element of the sequence s. A symbol-pair a:a ~ is said to occur in a rule (b:b p, C) iff either a:a' = b:b' or for at least one element (LC, RC) of C, a:a' is a string-constituent in at least one of LC and RC.",21,22
14943,16420577,"A symbol-pair a:a' is said to be a string-constituent in a set 0 of symbol-pair sequences if there is at least one element s C 0 such that a:a ~ is an element of the sequence s. A symbol-pair a:a ~ is said to occur in a rule (b:b p, C) iff either a:a' = b:b' or for at least one element (LC, RC) of C, a:a' is a string-constituent in at least one of LC and RC.",49,50
14944,16420577,"Given such a two-level morphological grammar R = (CR, SC), the set of feasible pairs in R is the set of symbol-pairs: {a : a' ] a : a' occurs in some element of CR U SC} U {a : a I a C A n A'} A two-level morphological grammar is said to be regular if all the rules in it are regular.",27,28
14945,16420577,"Languages Generated Given a two-level morphological grammar R = (CR, SC), a symbol-pair sequence E is generated by R iff all the following hold: 1.",18,19
14946,16420577,all the symbol-pairs in ~ are feasible pairs in R; 2.,2,3
14947,16420577,"the set CR of rules contextually allows E. As mentioned earlier, the two classes of rules are treated slightly differentlym surface coercion rules are conjoined, forming a set of constraints all of which must be met, and the context restriction rules for a given symbol pair are disjoined, giving a set of possible licensing contexts.",47,48
14948,16420577,"If no rules apply to a particular symbol-pair, it is acceptable if and only if it is feasible.",7,8
14949,16420577,"With the above definitions, it is now possible to ask what sorts of symbol-pair languages can be characterized using a two-level morphological grammar.",14,15
14950,16420577,"Let E1 and E2 be symbol-pair sequences such that CR contextually allows El, and CR contextually allows E2.",5,6
14951,16420577,"Proof Let a:a ~ be a symbol-pair occurring in EIE2, such that (P1, a:a',P2) is a partition of E1E2.",8,9
14952,16420577,sequence of symbol-pairs is generated by the grammar if every transducer in the grammar accepts it.,2,3
14953,16420577,"That is, the symbol-pair sequence must be in the intersection of the languages accepted by the transducers (viewed as acceptors); in procedural terms, this is often referred to as ""having the transducers executed in parallel.""",4,5
14954,16420577,"These transducers, like the two-level rules, define a mapping between equal-length symbol sequences, as described earlier; there are no ""empty transitions.""",17,18
14955,16420577,"Under these conditions, since the intersection of a set of regular languages is also a regular language, it follows that these parallel finite-state acceptors define exactly the regular sets of symbol-pair sequences.",34,35
14956,16420577,"The obvious question is whether there is a difference in power; in fact, there is: Theorem 1 There are regular sets of symbol-pair sequences (i.e. symbol-pair languages characterized by regular expressions of symbol-pairs) that cannot be generated by any two-level morphological grammar.",25,26
14957,16420577,"The obvious question is whether there is a difference in power; in fact, there is: Theorem 1 There are regular sets of symbol-pair sequences (i.e. symbol-pair languages characterized by regular expressions of symbol-pairs) that cannot be generated by any two-level morphological grammar.",31,32
14958,16420577,"The obvious question is whether there is a difference in power; in fact, there is: Theorem 1 There are regular sets of symbol-pair sequences (i.e. symbol-pair languages characterized by regular expressions of symbol-pairs) that cannot be generated by any two-level morphological grammar.",40,41
14959,16420577,"Any language L generated by a two-level morphological grammar must have the property that if E2, E3, and EIE2E3E4 C L, then E2E3 E L. There are regular symbol-pair languages that do not have this property, such It was already clear that there are some regular relations that cannot be generated by a two-level grammar, since a regular relation can put into correspondence symbolsequences of different lengths; what the above result shows is that there are some equal-length regular relations that cannot be generated by any two-level grammar.",33,34
14960,16420577,"That is, we have the following proper inclusions: languages generated by regular two-level morphological grammars c regular sets of symbol-pair sequences c regular relations There is another, rather trivial, difference between the power of two-level morphological rules and arbitrary regular expressions.",23,24
14961,16420577,"According to the definitions given here, the empty sequence of symbol-pairs is in every language generated by a two-level morphological grammar, since it conforms to the definition regardless of the content of the rules.",11,12
14962,16420577,It would be straightforward to extend the definition of a two-level morphological grammar to cover this symbol.,18,19
14963,16420577,"The applicability of such rules could be defined thus: Another Type of Rule A symbol-pair sequence E is contextually excluded by a rule (a:b, C) if there is a partition ($1, a:b, $2) of ~ and some context-pair (LC, RC) E C such that LC matches $1 at the right end and RC matches $2 at the left end.",15,16
14964,16420577,"Let El, E2, E3 be symbol-pair sequences such that E1E2E3 is not contextually excluded by R. Then E2 is not contextually excluded by R. Hence, the use of context exclusion rules does not affect any of the results about generative power given earlier, or the lack of closure demonstrated in Section 11 below.",7,8
14965,16420577,"Boundary Markers The two-level formalism, as defined above (and as originally defined by Koskenniemi) has no word-boundary symbol to mark the end of a sequence of symbols.",24,25
14966,16420577,"Although the linguist is free to introduce any symbols that seem empirically useful, none of these symbols has any specially defined status beyond what the linguist chooses to state in the actual twoqevel morphological grammar, and the apparatus does not stipulate that a particular symbol occurs only at the start or end of a complete string.",46,47
14967,16420577,can appear in both the lexical and the surface component of a symbol-pair.,12,13
14968,16420577,"can be paired only with itself, not with any other symbol.",11,12
14969,16420577,"The symbol-pair #:# is not regarded as part of the sequence generated; that is, the rules characterize an extended string with #:# at each end, but the generated sequence is defined to be the sequence without boundary markers.",1,2
14970,16420577,"1987) also allow a lexical boundary marker #, such that the symbol-pair # : 0 meets points 3, 4, and 5 above (which can be achieved by writing suitable two-level rules without any need for special treatment).",13,14
14971,16420577,"We can show that the inclusion of a specially treated boundary symbol slightly alters the generative power of the formalism, since this allows any set that can be included as a context set to constitute the entire language, as follows.",11,12
14972,16420577,"Given two sets A and N, a symbol rx E A, a symbol fl c N, and a two-level morphological grammar G based on alphabets A and A ~, then a symbol-pair sequence E (A x A~) * is said to be generated by G with boundary a:fl iff 1.",8,9
14973,16420577,"Given two sets A and N, a symbol rx E A, a symbol fl c N, and a two-level morphological grammar G based on alphabets A and A ~, then a symbol-pair sequence E (A x A~) * is said to be generated by G with boundary a:fl iff 1.",14,15
14974,16420577,"Given two sets A and N, a symbol rx E A, a symbol fl c N, and a two-level morphological grammar G based on alphabets A and A ~, then a symbol-pair sequence E (A x A~) * is said to be generated by G with boundary a:fl iff 1.",37,38
14975,16420577,"~:fl G a:fl is generated by G. Notice that under this definition, a sequence ~ may well be ""generated by G with boundary c~:fl"" even though G itself is not generated by G. Theorem 2 Let A and A ~ be sets of symbols, let ~ be some symbol not in A, fl some symbol not in A ~ and let L be some set of symbol-pair sequences from A x AE Then there is a two-level morphological grammar G based on A U { (~ } and A' U { fl } such that L --{G I G generates G with boundary rx : fl} include in CR' a rule: (a : b, C1 U C2... Cn).",54,55
14976,16420577,"~:fl G a:fl is generated by G. Notice that under this definition, a sequence ~ may well be ""generated by G with boundary c~:fl"" even though G itself is not generated by G. Theorem 2 Let A and A ~ be sets of symbols, let ~ be some symbol not in A, fl some symbol not in A ~ and let L be some set of symbol-pair sequences from A x AE Then there is a two-level morphological grammar G based on A U { (~ } and A' U { fl } such that L --{G I G generates G with boundary rx : fl} include in CR' a rule: (a : b, C1 U C2... Cn).",61,62
14977,16420577,"~:fl G a:fl is generated by G. Notice that under this definition, a sequence ~ may well be ""generated by G with boundary c~:fl"" even though G itself is not generated by G. Theorem 2 Let A and A ~ be sets of symbols, let ~ be some symbol not in A, fl some symbol not in A ~ and let L be some set of symbol-pair sequences from A x AE Then there is a two-level morphological grammar G based on A U { (~ } and A' U { fl } such that L --{G I G generates G with boundary rx : fl} include in CR' a rule: (a : b, C1 U C2... Cn).",73,74
14978,16420577,"It should be clear from this that (a) any symbol-pair a:b occurs in the rules in G iff it occurs in the rules in G'; (b) any pair of context sets (LC, RC) appears in a rule in CR iff it appears in a rule in CR' with the same symbol-pair; (c) any pair of context sets (LC, RC) appears in a rule in SC iff it appears in a rule in SC' with the same symbol-pair.",11,12
14979,16420577,"It should be clear from this that (a) any symbol-pair a:b occurs in the rules in G iff it occurs in the rules in G'; (b) any pair of context sets (LC, RC) appears in a rule in CR iff it appears in a rule in CR' with the same symbol-pair; (c) any pair of context sets (LC, RC) appears in a rule in SC iff it appears in a rule in SC' with the same symbol-pair.",64,65
14980,16420577,"It should be clear from this that (a) any symbol-pair a:b occurs in the rules in G iff it occurs in the rules in G'; (b) any pair of context sets (LC, RC) appears in a rule in CR iff it appears in a rule in CR' with the same symbol-pair; (c) any pair of context sets (LC, RC) appears in a rule in SC iff it appears in a rule in SC' with the same symbol-pair.",99,100
14981,16420577,"There can be several such rules for any given symbol-pair P (i.e. we have to allow the grammar to be not in the ""normal form"" defined in Section 10 above).",9,10
14982,16420577,"To construct the surface coercion rules for the intersection grammar G use SC = SC1 U SC2 U SC' where SC ~ is defined as follows: SC' : {(a': a', (({c}, {e})})~ (a': a, (({~}, (¢})})} where a' is as above and a is an arbitrary symbol other than a'.",77,78
14983,16420577,There is then a fairly straightforward proof that this grammar characterizes a symbol-pair sequence if and only if that sequence is in L(G1) rl L(G2).,12,13
14984,16420577,"The only slightly complicated step involves the proof that if an occurrence of a symbol-pair matches a rule in CR1, and also matches a rules in CR2, it must match a rule in CR (in CR', in fact).",14,15
14985,16420577,"The Next Stage--Compilation and Complexity The above results define more clearly the family of two-level languages, but they say nothing about mechanisms for recognizing strings of symbol-pairs, nor how such recognition can be integrated with a lexicon of symbol-strings.",31,32
14986,16420577,"The Next Stage--Compilation and Complexity The above results define more clearly the family of two-level languages, but they say nothing about mechanisms for recognizing strings of symbol-pairs, nor how such recognition can be integrated with a lexicon of symbol-strings.",46,47
14987,16420577,"1991) have not been produced, nor is there any characterization of the complexity of determining whether a sequence of symbol-pairs belongs to the language generated by a two-level language (without regard for the lexical segmentation issue).",21,22
14988,16420577,"Let E~, E2, E3 be symbol-pair sequences such that E1E2E3 is coercively allowed by R. Then E2 is coercively allowed by R. Proof If E2 were not coercively allowed by R, it would mean that there is a partition ($1, a:b, $2) of E2 such that for some (LC, RC) in C, LC matches $1 at the right end, RC matches $2 at the left end, and b~ a'.",7,8
14989,16420577,"Lemma 3 (The Concatenation Property) Let G be a two-level morphological grammar (CR, SC), and let L(G) be the set of symbol-pair sequences generated by G. Suppose that there are sequences El, E2, E3, E4 such that E2 E L(G), E3 E L(G), and E1E2EBE4 c L(G).",30,31
14990,16420577,"Since E1E2E3E4 C L(G), all the symbol-pairs in it are feasible with respect to G, hence all the symbol-pairs in E2E3 are feasible.",8,9
14991,16420577,"Since E1E2E3E4 C L(G), all the symbol-pairs in it are feasible with respect to G, hence all the symbol-pairs in E2E3 are feasible.",23,24
14992,16420577,"In the latter case, each transducer deals with some linguistic phenomenon, and a This theorem may sound rather odd, since it implies that any set whatsoever can be generated in this manner (in contrast to the earlier result that there are some regular languages of symbol-pairs that cannot be generated by two-level rules).",49,50
14993,16420577,"However, it is important to note that this theorem not only uses a special boundary symbol, it relies on the generalization of the usual regular-set based two-level rules to rules that allow any set as a context set.",16,17
14994,16420577,"Essentially it says that, with the augmentation of a special boundary symbol, the generative power is limited only by the limits placed on the class of sets that can appear as contexts.",12,13
14995,16420577,"A more useful and natural specialization of this theorem is as follows: Corollary For any regular set L of symbol-pair sequences, there is a regular two-level morphological grammar that generates L with boundary ~:fl, for some symbols c~, fl that are not in the alphabets used for L. Normal Form We define a two-level morphological grammar as being in normal form if there is no symbol-pair that is the subject of more than one context restriction rule or more than one surface coercion rule.",20,21
14996,16420577,"A more useful and natural specialization of this theorem is as follows: Corollary For any regular set L of symbol-pair sequences, there is a regular two-level morphological grammar that generates L with boundary ~:fl, for some symbols c~, fl that are not in the alphabets used for L. Normal Form We define a two-level morphological grammar as being in normal form if there is no symbol-pair that is the subject of more than one context restriction rule or more than one surface coercion rule.",75,76
14997,16420577,"Theorem 3 For any two-level morphological grammar G, there is a corresponding two-level morphological grammar G' in normal form that generates the same symbol-pair language.",29,30
14998,16420577,"If there is exactly one rule (a:b, C) in CR for the symbol-pair a:b, include that rule in CRL For any set of two or more rules in CR with the same symbol-pair a : b (a : b, C1), ...., (a : b, G) CR = CR' U CR"" where CR' is defined to be: and CR"" is defined as: {(a : b, {({a' : a'}, c)} [ a : b is feasible in exactly one of G1 and G2} where a' is a new symbol that is not in a feasible pair in either G1 or G2.",17,18
14999,16420577,"If there is exactly one rule (a:b, C) in CR for the symbol-pair a:b, include that rule in CRL For any set of two or more rules in CR with the same symbol-pair a : b (a : b, C1), ...., (a : b, G) CR = CR' U CR"" where CR' is defined to be: and CR"" is defined as: {(a : b, {({a' : a'}, c)} [ a : b is feasible in exactly one of G1 and G2} where a' is a new symbol that is not in a feasible pair in either G1 or G2.",42,43
15000,16420577,"If there is exactly one rule (a:b, C) in CR for the symbol-pair a:b, include that rule in CRL For any set of two or more rules in CR with the same symbol-pair a : b (a : b, C1), ...., (a : b, G) CR = CR' U CR"" where CR' is defined to be: and CR"" is defined as: {(a : b, {({a' : a'}, c)} [ a : b is feasible in exactly one of G1 and G2} where a' is a new symbol that is not in a feasible pair in either G1 or G2.",126,127
15001,16420577,"There will be no symbol-pair that is the subject of a rule in CR' and in CR"", since the first of these sets covers the jointly feasible pairs and the latter handles others.",4,5
15002,5558941,"Vertices represented by a square are called symbol vertices, and they represent a grammar symbol.",7,8
15003,5558941,"Vertices represented by a square are called symbol vertices, and they represent a grammar symbol.",15,16
15004,5558941,"4 When the parser ""shifts"" a word, it creates a leaf node labeled with the word and the pre-terminal, and, instead of the pre-terminal symbol, a pointer to the newly created leaf node is pushed onto the stack.",33,34
15005,5558941,LOCAL AMBIGUITY PACKING We say that two or more subtrees represent local ambiguity if they have common leaf nodes and their top nodes are labeled with the same non-terminal symbol.,31,32
15006,5558941,"That is to say, a fragment of a sentence is locally ambiguous if the fragment can be reduced to a certain non-terminal symbol in two or more ways.",25,26
15007,5558941,"In the graph-structured stack, if two or more symbol vertices have a common state vertex immediately on their left and a common state vertex immediately on their right, they represent local ambiguity.",11,12
15008,5558941,Nodes pointed to by these symbol vertices are to be packed as one node.,5,6
15009,5558941,"The newly created node is labeled with the Next Word = 'saw' left-hand side symbol of rule 3, namely ""NP"".",18,19
15010,5558941,"The newly Next Word = 'in' created node is labeled with the left-hand side symbol of rule 4, i.e., ""NP"".",18,19
15011,5558941,"A generated function takes a list of arguments, each of which is a value associated with each right-ha~d side symbol, and returns a value to be associated with the left-hand side symbol.",24,25
15012,5558941,"A generated function takes a list of arguments, each of which is a value associated with each right-ha~d side symbol, and returns a value to be associated with the left-hand side symbol.",39,40
15013,391658,Let us call any symbol which occurs as a prefix or a postfix in a string that represents the constituent structure of an expression an affix; the strings themselves affixed strings; and the grammars that generate those strings affix gra1~ars.,4,5
15014,556977,"The "" symbol in (42) stands for the intension operator as defined in Montague 1973 .",2,3
15015,5280555,"The parsing algorithm is sinular to the shift-reduce parser except that our algorithms handle ambiguities, parallel processing of each hypothesis, and top-down predictions of possible next input symbol.",33,34
15016,9459753,"C. Predicates for LFG Primitives The predicates for each LFG primitives are as follows : (d,dl,d2 are designators, s is a set, and "" is a negation symbol) I) dl = d2 -> equate(dl,d2,01d,New) 2) d & s -> include(d,s,Old,New) 3) dl =c d2 -> eonstrain(dl,d2,01dC,NewC) 4) d -> exlst(d,OldC,~lewC) 5) ""(dl =c d2) -> ne&_constraln(dl,d2,01dC,~ewC) 6) ""d -> not_exist(d,OldC,~ewC) The ""Old"" and ""New, are global value assIcnnenta.",34,35
15017,16897771,"The underlying form z will represent the TM input w, while the surface form y will represent the halted state of M on w. The immediate description of the machine (tape contents, head position, state symbol) is represented in the string of units.",39,40
15018,16897771,"The unit representing the currently scanned tape square will also be specified for two additional features, to represent the state symbol of the machine and the direction in which the head will move.",21,22
15019,16897771,"The second observation is that segmental rules can f~eely manipulate (insert and delete) boundary symbols, and thus it is possible to prolong the derivation indefinitely: we need only employ a rule R,~_, at the end of the cycle that adds an extra boundary symbol to each end of the derivation string, unless the simulated machine has halted.",51,52
15020,16897771,"If we use the four-symbol sevenstate ""smallest UTM"" of Minsky (1969) , then the resulting segmental model contains no more than three features, eight specifications, and 36 very simple rules (exact details in Ristad, 1990 ).",6,7
15021,16897771,"We have also seen that symbol-minimization is a poor metric for naturalness, and that the complex norational system of SPE (not discussed here) is an inadequate characterization of the notion of ""appropriate phonological generalisation. """,5,6
15022,16897771,"Worst of all, the UTM simulation suggested above shows that symbol count does not correspond to ""naturalness.""",11,12
15023,969059,"We use the symbol ""-"" to mean 'not.' [",3,4
15024,17667889,"In general, an -gram model can be represented by a stochastic finite automaton, and in a trigram model, each state of an automaton is labeled by a symbol sequence of length 2.",30,31
15025,17667889,The symbol § here indicates no (prior) relationship.,1,2
15026,219301246,"the label being developed (noted by the symbol C) and t h e label associated w i t h the segment which was read in the dictionary (noted by A) The analysis of a word aims to produce a segmentation of t h e woru simultaneously compatible with the segments of t h e d i f f e r e n t dictionaries (the word must be an assembly of dictionary segments) and compatible with a correct evolution of the grammar The purpose of the grammar is to permit or prevent the evolution of label C starting wlth label A Here, the l a b e l will evolve and obtaln the variable category wlth value adjective.",8,9
15027,219304199,"S i n c e these p h o n o l c g i c a l rules use only the immediate c o n t e x t , t h e S D ' s begin a n d end w i t h the unbounded skip symbol ""Xn.",50,51
15028,457176,"For each expansion we distinguish one of the right-hand side labels as the ""middle"" or ""head"" symbol M(c).",22,23
15029,457176,"To the left of M is a sequence of one or more left labels L i (c) including the special termination symbol , which indicates that there are no more symbols to the left, and similarly for the labels to the right, R i (c).",23,24
15030,457176,"j is a ""none-of-the-above"" symbol.",12,13
15031,457176,"As pointed out above, the text in question has been ""speechified"" by removing punctuation and capitalization, and ""simplified"" by allowing only a fixed vocabulary of 10,000 words (replacing all the rest by the symbol ""UNK""), and replacing all digits and symbols by the symbol ""N"".",40,41
15032,457176,"As pointed out above, the text in question has been ""speechified"" by removing punctuation and capitalization, and ""simplified"" by allowing only a fixed vocabulary of 10,000 words (replacing all the rest by the symbol ""UNK""), and replacing all digits and symbols by the symbol ""N"".",54,55
15033,6749484,"First, for each nonterminal symbol in the CFG, a probability distribution is placed on the set of all productions from that symbol.",5,6
15034,6749484,"First, for each nonterminal symbol in the CFG, a probability distribution is placed on the set of all productions from that symbol.",23,24
15035,6749484,"For instance, consider the CFG in Chomsky normal form: S~SS S ~ a (1) where S is the only nonterminal symbol, and a is the only terminal symbol.",26,27
15036,6749484,"For instance, consider the CFG in Chomsky normal form: S~SS S ~ a (1) where S is the only nonterminal symbol, and a is the only terminal symbol.",34,35
15037,6749484,"Definition 1 A context-free grammar (CFG) G is a quadruple (N, T, R, S), where N is the set of variables, T the set of terminals, R the set of production rules, and S E N is the start symbol.",52,53
15038,6749484,Definition 2 Let A C 7 denote that the symbol A occurs in the parse 7.,9,10
15039,6749484,"If A E T, let T A be the left-most maximum subtree of T rooted in A, which is the subtree of 7 rooted in A satisfying the condition that if ;' ~ 7A is also a subtree of v rooted in A, then 7 ~ is either a subtree of ""rA, or a right sibling of 7A, or a subtree of a right sibling of 7A. Let AT be the root of rA, which is the left-most ""shallowest"" instance of A in r. Definition 3 For any two symbols A E N and B c N tO T, not necessarily different, B is said to be reachable from A in G, if there is a sequence of symbols A0, A1 ..... An with A0 = A and An = B, and a sequence of sentential forms oe0 ..... o~n-1, such that each Ai -+ oq is a production in R and each Oq contains the next symbol Ai+l.",179,180
15040,6749484,"For each symbol A ~ S, there is at least one parse T with root S such that A E ~-.",2,3
15041,6749484,"For each A E V, letf(A;T) be the number of nonroot instances of A in T. Given a E (V U T)*, let ai be the ith symbol of the sentential form a. For any A E V Clearly, for every r E A, ff(A; T) = f(A; ~-) whenever A # S and f(S; q-) --f(S; T) -1.",33,34
15042,6749484,"Corollary 1 Under the same assumption of Proposition 1, for each symbol A E N, p(ftA) = 1.",12,13
15043,16432742,"For each nonterminal symbol, a (normalized) probability is placed on the set of all productions from that symbol.",3,4
15044,16432742,"For each nonterminal symbol, a (normalized) probability is placed on the set of all productions from that symbol.",20,21
15045,16432742,"A trivial example is the CFG with one nonterminal and one terminal symbol, in Chomsky normal form: A ~ AA a ~ a where a is the only terminal symbol.",12,13
15046,16432742,"A trivial example is the CFG with one nonterminal and one terminal symbol, in Chomsky normal form: A ~ AA a ~ a where a is the only terminal symbol.",31,32
15047,16432742,"Maximum-Likelihood Estimation More generally, let G --(V, T, R, S) denote a context-free grammar with finite variable set V, start symbol S E V, finite terminal set T, and finite production (or rule) set R. (We use R in place of the more typical P to avoid confusion with probabilities.)",30,31
15048,538122,"For each expansion we distinguish one of the right-hand side labels as the ""middle"" or ""head"" symbol M(c).",22,23
15049,538122,"To the left of M is a sequence of one or more left labels Li (c) including the special termination symbol A, which indicates that there are no more symbols to the left, and similarly for the labels to the right, Ri(c).",22,23
15050,11542450,A sparse Markov transducer is a conditional probability of the form: (5) where ¢ represents a wild card symbol and ti = t-~=lnJ-(i-1).,21,22
15051,11399996,"The ACTION code is executed when the rule fires, and returns either a list of utterances (with an implied reply-wait), an utterance with an indication that no reply wait is necessary, or NIL, the standard Lisp symbol for ""nothing"".",44,45
15052,1269169,"The STOP symbol is added to the vocabulary of nonterminals, and the model stops generating left modifiers when it is generated.",2,3
15053,1269169,"Generate the head label, with probability ~'~ (H I P, h) Generate left modifiers with probability 1-I Pc(L~(li) l Li-I'P'h'H) /=l..n+l where L0 is defined as a special NULL symbol.",41,42
15054,41372336,The symbol nil is used in place of missing information.,1,2
15055,41372336,"Each dependency type is represented in the vector by the outside word 8 it involves, or by the symbol nil, which indicates that this type of dependency does not occur in the phrase under consideration.",19,20
15056,9454249,"For example, our former grammar used the single category symbol S to represent both matrix (or too0 clauses and subordinate clauses.",10,11
15057,6883741,"The probability of a given tree is the product of the probability of all of the rule instances used in the construction of that tree, where rules take the form N → φ, with N a nonterminal symbol and φ a finite sequence of one or more terminals or nonterminals.",39,40
15058,15203074,"It therefore fulfills the function of a match network (Riis, 1998) , expressing a probability that the observed symbol is generated by a certain state.",21,22
15059,15956543,"A simple generative model for this task goes as follows: For each utterance: Repeat until the ""end of utterance"" symbol is chosen Pick the next word according to P(w).",23,24
15060,15956543,"It is assumed here that P(w) is over possible vocabulary items plus a special ""end-ofutterance"" symbol.",20,21
15061,15956543,This means that other less-used categories (foreign word or symbol) get omitted entirely.,12,13
15062,15956543,"Putting it another way, every symbol in a CKY chart is a knowledge barrier between its outside and inside.",6,7
15063,34305886,"Vu investigated two different ways to build a multilingual phone set: Either concatenating the phone sets of the different languages, thus keeping phones distinct between languages, or merging phones which share the same symbol in the IPA table across languages.",36,37
15064,5566545,"Any production of the form A --4 a will be said to be an A-production, and a will be said to be an expansion of A. We will say that a symbol X is a direct left corner of a nonterminal A, if there is an A-production with X as the left-most symbol on the right-hand side.",34,35
15065,5566545,"Any production of the form A --4 a will be said to be an A-production, and a will be said to be an expansion of A. We will say that a symbol X is a direct left corner of a nonterminal A, if there is an A-production with X as the left-most symbol on the right-hand side.",60,61
15066,5566545,"The left-factoring transformation (LF) applies the following grammar rewrite schema repeatedly, until it is no longer applicable: LF: For each nonterminal A, let a be the longest nonempty sequence such that there is more than one grammar production of the form A --+ a~. Replace the set of all productions A-+aft1, ..., A-+a~n with the productions A -+ aA', A' --~ ill, ..., A' --~ fin, where A' is a new nonterminal symbol.",92,93
15067,5566545,The final optimization we have developed for Paull's algorithm is to transform the grammar to combine all the non-left-recursive possibilities for each left-recursive nonterminal under a new nonterminal symbol.,35,36
15068,5566545,"This transformation, which we might call ""non-left-recursion grouping"" (NLRG), can be defined as follows: NLRG: For each left-recursive nonterminal A, let al,...,an be all the expansions of A that do not have a left recursive nonterminal as the left most symbol.",57,58
15069,5566545,"If a terminal symbol a is a proper left corner of A in the original grammar, add A -4 aA-a to the transformed grammar.",3,4
15070,6258391,"One solution is to use an affine function of the form gap(x) = r + sx, where r is the penalty for the introduction of a gap, and s is the penalty for each symbol in the gap.",37,38
15071,3262438,A possible representational enrichment therefore adds a self loop i ~ i labelled with the symbol alphabet E to every state i of the FSA.,15,16
15072,247363311,"This phoneme (or more accurately, phone) set was based on the ARPAbet symbol set.",15,16
15073,1616933,"In what follows, we assume an approach of this type and will use the symbol RcP for such reciprocal quantifiers so that the semantic representation of e.g. Jon and Bill saw each other will be: (1) RcP({jon, saw(x, y)) When antecedent sets of just two members are considered, each set member is required to stand in the scope relation to each other member.",15,16
15074,1723222,"A context-free grammar (CFG) is a tuple G = (VN, VT, P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, S E VN is the start symbol, and P is a finite set of productions having the form A -+ a, where A E VN and a E (VN t3 VT)*.",46,47
15075,1723222,"A CFG G = (VN, VT, P, S[$]) in CNF is called a bilexical context-free grammar if there exists a set VD, called the set of delexicalized nonterminals, such that nonterminals from VN are of the form A[a], consisting of A E VD and a E VT, and every production in P has one of the following two forms: [a] is said to have terminal symbol a as its lexlcal head.",81,82
15076,1723222,"Note that in a parse tree for G, the lexical head of a nonterminal is always ""inherited"" from some daughter symbol (i.e., from some symbol in the right-hand side of a production).",23,24
15077,1723222,"Note that in a parse tree for G, the lexical head of a nonterminal is always ""inherited"" from some daughter symbol (i.e., from some symbol in the right-hand side of a production).",29,30
15078,1723222,"Algorithm R processes the input string from left-to-right, ""consuming"" one symbol ai at a time.",17,18
15079,1723222,"If for some i, 0 < i < n, the set of derivations in G having the form S ~* w[1, i]7, 7 E (VN U VT)*, is empty, then R rejects and halts, and it does so before consuming symbol ai+l, if i < n. In this case, we say that R has detected an error at position i in w. Note that the above property forces the recognizer to do relevant computation for each terminal symbol that is consumed.",51,52
15080,1723222,"If for some i, 0 < i < n, the set of derivations in G having the form S ~* w[1, i]7, 7 E (VN U VT)*, is empty, then R rejects and halts, and it does so before consuming symbol ai+l, if i < n. In this case, we say that R has detected an error at position i in w. Note that the above property forces the recognizer to do relevant computation for each terminal symbol that is consumed.",90,91
15081,1723222,"If for some i, 0 < i < n, w[1, if is not a correct prefix for L(G), then R rejects and halts, and it does so before consuming symbol ai+i,~ if i < n. Note that the latter definition asks for a stronger condition, and the two definitions are equivalent only in case the input grammar G is reduced, i While the above mentioned parsing algorithms satisfy the former definition of CPP, they do not satisfy the latter.",35,36
15082,1723222,"On input (Gq, bq+2bq+i), R does not detect any error at position 1, that is after having read the first symbol bq+2 of the input string.",25,26
15083,1723222,"an, qn be an accepting computation for w in M, and choose some symbol $ ¢ E. We can now encode the accepting computation as ($, q0)(al, ql) • • • (an, qa) where we pair alphabet symbols to states, prepending $ to make up for the difference in the number of alphabet symbols and states.",15,16
15084,26152771,"This phoneme (or more accurately, phone) set is based on the ARPAbet symbol set developed for speech recognition uses.",15,16
15085,41559124,"A typical preference rule is: ""When instructing the hearer, lncremm the accumulating measure by 10 for each occurrence of the symbol 'YOU'"".",23,24
15086,5638681,"The function referent applies to concepts: If a is a concept, then referentla) is either an individual marker in I or the symbol @, which may be read any. •",25,26
15087,5638681,The symbol @ is Codd's notation for null or unknown values in a data base.,1,2
15088,5638681,but the present term and the symbol :: have been adopted from ALGOL-68.,6,7
15089,9564084,"In an analogous fashion, we provide for the translation of a phrase to be synthesized from the translations of its immediate constituents according to a local rule, typically involving symbol/c application and ~-conversiou.",31,32
15090,12453425,"If 7 is of the form (8 T1 T2... Tn) where ~ is an n-ary function symbol, cr 1 is the value for ~-1, or2 is the value for 72,..., an is the value for ~-n, and [or1, or2, • •., an, cr] is an element in the set of n + 1 tuples that M maps to 8, then cr is the value of T. To this we add: 1.",21,22
15091,1433811,"The start symbol for the grammar is S-MAJ, which in the above example consists of a non-terminal node for an S and a final punctuation terminal node.",2,3
15092,1433811,"These useless nodes can be easily pruned after parsing is complete by marking all nodes that participate in a parse beginning with the start symbol, S-MAJ, and freeing those that are unmarked.",24,25
15093,1433811,"The first and simplest method is to prevent two nodes from being packed together (except for the start symbol), if they have different logical forms, as shown in Figure 4 .",19,20
15094,11079685,They used the part of speech information for the words surrounding a punctuation symbol as the input to a feed-forward neural network.,13,14
15095,11079685,They mention that the comma is the most frequently used punctuation symbol and its correct insertion can make a text far more legible.,11,12
15096,1909121,Footnote Search for blocks on the lower part of the page that are smaller than body text; check that they start with a number or other footnoteindicating symbol.,28,29
15097,53299978,"The output values are (d+1)-dimensional though, since they may further contain the termination symbol a, in addition to the symbols in V (i) .",15,16
15098,53299978,"Before the occurrence of the first b in a sequence, the model always predicts a or b (which we notate a/b) whenever it that having a start symbol in the sequence does not affect the learning capabilities of the model.",32,33
15099,53299978,"However, we still use a termination symbol a to encode the end of the sequence in our output samples.",7,8
15100,53299978,"sees an a. However, after it encounters the first b, the rest of the sequence becomes entirely deterministic: Assuming that the model observes n a's in a sequence, it outputs (n 1) b's for the next (n 1) b's and the terminal symbol a for the last b in the sequence.",53,54
15101,10334514,"In Table 2 the symbol '→' indicates use of the dictionary in 'forward' direction (Norwegian -English), and '←' the reverse direction.",4,5
15102,70033909,The format of the entries in the special-symbol index is: English gloss; symbol; pages where the symbol is discussed or used.,9,10
15103,70033909,The format of the entries in the special-symbol index is: English gloss; symbol; pages where the symbol is discussed or used.,16,17
15104,70033909,The format of the entries in the special-symbol index is: English gloss; symbol; pages where the symbol is discussed or used.,21,22
15105,45974637,The first person translates that idea into some symbol system which is transmitted through some medium to the receiver.,8,9
15106,8356430,"Words vs. Concepts Contrary to many ontology designers, who do not seem to distinguish between word (or symbol) and concept, we take an ontology to be an organization of an agent's concepts by some set of ontological relations.",19,20
15107,1552091,The earliest reference to node admissibility appears in Chomsky 1965 (p. 215) ; he suggests the possibility of constructing a rewriting system where the rewriting of a symbol is determined not only by the symbol being rewritten but also by the dominating category symbol.,29,30
15108,1552091,The earliest reference to node admissibility appears in Chomsky 1965 (p. 215) ; he suggests the possibility of constructing a rewriting system where the rewriting of a symbol is determined not only by the symbol being rewritten but also by the dominating category symbol.,36,37
15109,1552091,The earliest reference to node admissibility appears in Chomsky 1965 (p. 215) ; he suggests the possibility of constructing a rewriting system where the rewriting of a symbol is determined not only by the symbol being rewritten but also by the dominating category symbol.,45,46
15110,1552091,"Given a node labeled A in a tree t, we say that DOM(~r q0, qr, ~ E V*, holds of a node labeled A if there is a path from the root of the tree to the frontier, which passes through the node labeled A, and is of the form Results on Local Constraints Theorem 3.1 (Joshi and Levy 1977) Let G be a finite set of local constraint rules and z(G) the set of trees analyzable by G. (It is assumed here that the trees in +(G) are sentential trees; i.e., the root node of a tree in ~-(G) is labeled by the start symbol, S, and the terminal nodes are labeled by terminal symbols.)",122,123
15111,52800576,"The symbol P will serve as a variable over the two LIG grammar nonterminals t and b. The substring of the string wl ... Wn being parsed between indices i and j will be notated as wi+t "".",1,2
15112,52800576,"Rather, the items keep only the single top stack element for each constituent (in addition to the nonterminal symbol).",20,21
15113,52800576,We say that a node p of T is labeled with a symbol s if T(p) = s. A.2 Tree-Adjoining Grammars and Derivations A.2.1 Tree-Adjoining Grammars.,12,13
15114,52800576,"S is a distinguished nonterminal symbol, the start symbol. •",5,6
15115,52800576,"S is a distinguished nonterminal symbol, the start symbol. •",9,10
15116,52800576,"/7 is a finite set of trees, the initial trees, where --interior nodes are labeled by nonterminal symbols, and frontier nodes are labeled by terminal symbols or the special symbol c. (We require that e ~g V, as e intuitively specifies the empty string.) •",32,33
15117,52800576,"~4 is a finite set of trees, the auxiliary trees, where --interior nodes are labeled by nonterminal symbols, and --frontier nodes are labeled by terminal symbols or e, except for one node, called the foot node, which is labeled with a nonterminal symbol. • (",48,49
15118,219301590,"A symbol is an alphanumeric s t r i n g headed by "".",1,2
15119,219301590,"They are constructed using t h e specialization operation, comparable t o CONS i n LISP* (A B) is t h e specialization of A , a concept, by B, a concept o r symbol.",40,41
15120,5754231,Yet another approach seeks some key mathematical property that distinguishes the natural languages from all possible symbol-systems.,16,17
15121,5754231,"In brief, Kimball sets up a base context-free grammar to generate two trees, rooted at S 1 and S 2, corresponding to the two context-free languages demanded by the homomorphism theorem (CFLi and CFL2) and a third tree dominating these two that eventually ""simulates"" the homomorphism H. S 1 dominates a terminal string labeled x and S 2, a terminal string labeled y. Dominating these two subtrees is a third tree that, besides x and y, dominates a terminal string z. The idea is to use a single transformation to successively check that the first member of x matches the first member of y and z; if so, this element is erased in x and y. If all elements match, and we are at the end of z (indicated by a special symbol), then the two strings are identical; this step carries out the intersection of the two context-free languages.",150,151
15122,5754231,"In addition, to write down the annotated surface structure, we must add two bracket labels for each nonterminal symbol.",20,21
15123,5754231,"However, again the addition of each empty category symbol adds just one to the total annotated surface structure length, and there are at most a finite number of such positions (~'positions, such as COMP, as described earlier).",9,10
15124,18828496,"This pair checks two conditions: whether the current input symbol is ""that"" and whether two noun phrases form an appropriate right context for the predicate found.",10,11
15125,218531823,"A tree in T (F, X n ) will be called a context, typically denoted with the symbol C. The notation C[t 1 , . . . ,",20,21
15126,218531823,"A homomorphism h : T (F in ) → T (F out ) is specified by its kernel, a function ĥ : F in → T (F out , X ∞ ) such that ĥ(f ) is a tree in T (F out , X arity(f ) ) for each symbol f ∈ F in .",55,56
15127,218531823,"Further restrictions can be imposed: A tree homomorphism h is LINEAR if ĥ(f ) is linear for all f ∈ F in ; is COM-PLETE if ĥ(f ) contains every variable in X arity(f ) for all f ∈ F in ; is -FREE if ĥ(f ) ∈ X arity(f ) for all f ∈ F in ; is SYMBOL-TO-SYMBOL if ĥ(f ) has exactly one symbol, for all f ∈ F in ; and is a DELABELING if h is complete, linear, and symbol-to-symbol.",73,74
15128,218531823,"Further restrictions can be imposed: A tree homomorphism h is LINEAR if ĥ(f ) is linear for all f ∈ F in ; is COM-PLETE if ĥ(f ) contains every variable in X arity(f ) for all f ∈ F in ; is -FREE if ĥ(f ) ∈ X arity(f ) for all f ∈ F in ; is SYMBOL-TO-SYMBOL if ĥ(f ) has exactly one symbol, for all f ∈ F in ; and is a DELABELING if h is complete, linear, and symbol-to-symbol.",94,95
15129,218531823,"Further restrictions can be imposed: A tree homomorphism h is LINEAR if ĥ(f ) is linear for all f ∈ F in ; is COM-PLETE if ĥ(f ) contains every variable in X arity(f ) for all f ∈ F in ; is -FREE if ĥ(f ) ∈ X arity(f ) for all f ∈ F in ; is SYMBOL-TO-SYMBOL if ĥ(f ) has exactly one symbol, for all f ∈ F in ; and is a DELABELING if h is complete, linear, and symbol-to-symbol.",98,99
15130,218531823,"We use the following abbrevations for the properties: L[inear], C[omplete], [ - ]F [ree], S[ymbol-to-symbol], D[elabeling], M [orphism without restriction].",27,28
15131,218531823,"In order to refer to the substitution nodes of a substitutable tree, we define the substitution paths of a tree t, ↓paths(t) to comprise the paths to substitution nodes in t. A tree-substitution grammar, then, is a triple, F, P, S where F is a ranked alphabet comprising the vocabulary of the grammar, S ∈ F is the start symbol of the grammar, and P ⊆ T ↓ (F) is a set of elementary trees.",70,71
15132,218531823,"As a simple example, we consider the grammar with three elementary trees α 1 S(N P ↓ , V P (V (like), N P ↓ )) α 2 N P (I) α 3 N P (cake) and start symbol S. The arities of the symbols should be clear from their usage.",48,49
15133,218531823,"The tree α r at the root of the derivation tree must be labeled at its root by the start symbol, that is, α r @ = S. For example, the derivation tree α 1 (α 3 , α 2 ) is a wellformed derivation tree for the sample grammar above, assuming that ↓paths(α 1 ) = 2, 2 , 1 .",20,21
15134,218531823,"N n ∈ F} In essence, the transformation replaces the root symbol by pairing it with the state q, and replaces the n variables with new pairs of a state q i and an arbitrarily chosen symbol N i . (",13,14
15135,218531823,"N n ∈ F} In essence, the transformation replaces the root symbol by pairing it with the state q, and replaces the n variables with new pairs of a state q i and an arbitrarily chosen symbol N i . (",39,40
15136,218531823,The tree pairs are schematic in that we use a * to stand for an arbitrary symbol in the appropriate alphabet.),16,17
15137,218531823,"If, for some symbol f , both h in and h out are non--free, then any tree rooted in such a symbol, f (t), is mapped, respectively, to h in (t) and h out (t).",4,5
15138,218531823,"If, for some symbol f , both h in and h out are non--free, then any tree rooted in such a symbol, f (t), is mapped, respectively, to h in (t) and h out (t).",25,26
15139,218531823,"But in that case, we can eliminate the unary symbol f , eliminating transitions in the automaton of the form q(f (x)) → f (q (x)) by adding, for all transitions with q on the left hand side, identical transitions with q on the left-hand side.",10,11
15140,218531823,We will call such a unary symbol ASYM-METRIC.,6,7
15141,218531823,"We arrange that in such cycles, the state and symbol at the root will be identical to the state and symbol at the end of the sequence.",10,11
15142,218531823,"We arrange that in such cycles, the state and symbol at the root will be identical to the state and symbol at the end of the sequence.",21,22
15143,218531823,"For example, suppose we have asymmetric symbols f and g and an -free symbol k with the following automaton transitions: q(k(x)) → k(q(x)) q(f (x)) → f (q(x)) q(g(x)) → g(q (x)) q (f (x)) → f (q (x)) q (f (x)) → f (q (x)) q (g(x)) → g(q (x)) q (k(x)) → k(. . .)",14,15
15144,218531823,Note that the state q and symbol f at the root are duplicated at the bottom.,6,7
15145,218531823,"For each such cycle, we construct a linked tree pair with a trivial input tree labeled with a pair of the state and an arbitrary symbol N from the input alphabet-q , N in the example.",26,27
15146,218531823,"α f = q, F 4 4 4 q , * ↓ q , * ↓ q, D 3 3 3 q , * ↓ D 5 5 q , * ↓ N { 1 2, 1 , 2 1 } α g = q , G q, * ↓ q , E q, * ↓ { 1 1 } α a = q, A q, N {} In addition, for each minimal sequence starting with a symbol that is non--free on the input and leading to such a cyclic state/symbol pair, a tree pair is similarly generated.",86,87
15147,218531823,"α f = q, F 4 4 4 q , * ↓ q , * ↓ q, D 3 3 3 q , * ↓ D 5 5 q , * ↓ N { 1 2, 1 , 2 1 } α g = q , G q, * ↓ q , E q, * ↓ { 1 1 } α a = q, A q, N {} In addition, for each minimal sequence starting with a symbol that is non--free on the input and leading to such a cyclic state/symbol pair, a tree pair is similarly generated.",103,104
15148,218531823,"In the example, the sequence corresponding to the automaton subderivation q(k(f (g(f (x))))) = k(f (g(q (f (x))))) would lead us to generate a tree pair with C( ĥin (k), q, q ), C( ĥin (k)[ ĥout (f )[ ĥout (g)]], q, q ), where links the two leaf nodes labeled with state/symbol pairs.",89,90
15149,218531823,"Similarly, we require elementary tree pairs corresponding to minimal tails of sequences of asymmetric symbols starting in a cyclic state/symbol pair and ending in a symbol non--free on the input.",22,23
15150,218531823,"Similarly, we require elementary tree pairs corresponding to minimal tails of sequences of asymmetric symbols starting in a cyclic state/symbol pair and ending in a symbol non--free on the input.",28,29
15151,9059126,"For example, standard ATNs use a register named ""*"" to hold the input symbol (word) currently being scanned.",16,17
15152,9059126,In Shapiro 1975 the * register was used to hold the string being generated rather than the input symbol being scanned.,18,19
15153,9059126,"We consider parsing a semantic network, as viewed from some node, into a particular linear symbol structure that constitutes a surface string of English.",17,18
15154,9059126,"In ATN grammars written for parsing, a recursive push does not change the input symbol being examined, but when the original level continues, parsing normally continues at a different symbol.",15,16
15155,9059126,"In ATN grammars written for parsing, a recursive push does not change the input symbol being examined, but when the original level continues, parsing normally continues at a different symbol.",32,33
15156,9059126,"The other major motivation is that, in parsing a string of symbols, the ""next"" symbol is defined by the system, but in ""parsing"" a network, ""next"" must be specified in the grammar.",18,19
15157,9059126,Terminal Actions Successful traversal of an ATN arc might or might not consume an input symbol.,15,16
15158,9059126,"When parsing, such consumption normally occurs; when generating it normally does not, but if it does, the next symbol (semantic node) must be specified.",22,23
15159,9059126,JUMP never consumes the input symbol; TO always does.,5,6
15160,9059126,"If the <form> is absent in the TO action, the next symbol to be scanned will be the next one in the input buffer.",14,15
15161,9059126,"If <form> is present, its value will be the next symbol to be scanned.",13,14
15162,9059126,The JUMP arc provides a place to make an arbitrary test and perform some actions without consuming an input symbol.,19,20
15163,9059126,The input symbol is consumed.,2,3
15164,9059126,"The next symbol to be scanned is the value of <form> if it is present, or the next symbol in the input buffer if <form> is missing.",2,3
15165,9059126,"The next symbol to be scanned is the value of <form> if it is present, or the next symbol in the input buffer if <form> is missing.",21,22
15166,9059126,"The PUSH arc makes two assumptions: 1) the first symbol to be scanned in the subnetwork is the current contents of the * register; 2) the current input symbol will be consumed by the subnetwork, so the contents of * can be replaced by the value returned by the subnetwork.",11,12
15167,9059126,"The PUSH arc makes two assumptions: 1) the first symbol to be scanned in the subnetwork is the current contents of the * register; 2) the current input symbol will be consumed by the subnetwork, so the contents of * can be replaced by the value returned by the subnetwork.",32,33
15168,9059126,"If the <test> is successful, all the <action>s of <preaction or action> are performed and a recursive push is made to the state <state> where the next symbol to be scanned is the value of <form> and registers are initialized by the <preaction>s. If the subnetwork succeeds, its value is placed into <register> and the <action>s and <terminal action> are performed.",37,38
15169,9059126,"Just as the normal TO terminal action is the generalized TO terminal action without the optional form, the PUSH arc (which we retain) is equivalent to the following CALL arc: (CALL <state> * <test> <preaction>... • <action>... <terminal action> ) Forms The generalized TO terminal action, the generalized TO arc, and the CALL arc all include a form whose value is to be the next symbol to be scanned.",84,85
15170,9059126,"If this next symbol is a semantic network node, the primary way of identifying it is as the node at the end of a directed arc with a given label from a given node.",3,4
15171,9059126,"The symbol ""#"" represents a SNePSUL function to create this node and make it the value of the variable NOW.",1,2
15172,18612640,"If the original 3-CNF form had n terms, then denoting each by the symbol Ep, p=l, ..., n, this part of the grammar would look like the following:U  (p= 1,2,...,n) The subscripts i, j, and k correspond to the actual subscripts in the original formula.",16,17
15173,3818928,We will assume that the notion of a unifier of such objects and most general unifier (mgu) are well defined; the symbol 0 will be used for unifiers.,24,25
15174,3818928,"Parser Instances Consider a processor to parse a given string built by using this architecture under the following parameterization: • The initialization of the agenda includes axioms for each word in the string (e.g., [O, sonny, 1] and [1,1eft,2] for the sentence 'Sonny left') and an initial prediction for each rule whose lefthand side matches the start symbol of the grammar (e.g., [0, S ~-NP VP, 0]).",69,70
15175,3818928,"Unlike LI~ parsing, 4For formalisms with complex structured nonterminals, the start ""symbol"" need only be unifiable with the left-haud-side nonterminal.",14,15
15176,3818928,"5Again, for formalisms with complex structured nontermiuals, the staxt symbol need only subsume the item's nontermiual.",11,12
15177,3818928,"Generator Instances As a final example of the use of this architecture, we consider'using it for generation by changing the initialization condition as follows: * The initialization of the agenda includes axioms for each word in the lexicon at each position (e.g., [O, sonny, 1] and [0, left, 1] and /1, left, 2/, and so on) and an initial prediction for each rule whose left-hand side is the start symbol of the grammar (e.g., [0, S +-NP VP,0]).",87,88
15178,3818928,"In the case of a grammar formalism with more complex information structures as nonterminals, e.g., definite-clause grammars, the ""start symbol"" might include information about, say, the meaning of the sentence to be generated, We will refer to this as the goal meaning.",25,26
15179,59940,"is a special ""stop"" symbol.",6,7
15180,59940,"wn is the stop symbol ,stop, 2.",4,5
15181,11996398,"The symbol "":="" is used for assignment, lambda is an anonymous-functionforming operator.",1,2
15182,219303310,"This f s a well understood and rel a t i v e l y e f f i c i e n t operation, especially compared to t h e backtracking, intersection, o r u n i f i c a t i o n operations required t o W c , k the consistency of v a r i a b l e substitutione i n n notSEX2seT r?iference rules,  Let us, therefbre, extend our syntax of path-based inference rules t o allow a 41 path of a r c compositions on t h e l e f t of t h e ""+"" symbol.",117,118
15183,244077698,"If we limit ourselves to a practicable length of symbol, the question of adapting the general mathematical solution to actual use becomes one of ingenuity which can probably be solved, but which can only be assessed by practical effort.",9,10
15184,872672,Even the simple view of computer science as the study of symbol manipulation s reveals this bias.,11,12
15185,872672,"Although in this author's view it seems a little far-fetched to call the internal relationships (the ""use"" of a symbol) semantical, it is nonetheless true that we are interested in characterising both, and it is unnecesary to express a preference.",25,26
15186,872672,"For discussion, we will refer to .he "",-semantics of a symbol or expression as its declarative /mp0rt, and refer to its *-semantics as its procedural consequence.",13,14
15187,872672,"The h-calculus and predicate logic systems, furthermore, have no notion of a processor with state; thus the appropriate • involves what we may call local procedural conse.quence, relating a simple symbol or set of symbols to another set.",36,37
15188,61359265,"The solution is to set a pointer to the relevant extensional concept when the function symbol LOCATION is inserted, so that LOCATION will carry the pointer and thus make the information attached to the concept 8ccaesible.",15,16
15189,61359265,We can now see how this pointer may be Set for individual lexical items: it is introduced as a simple relation between a grammatical function symbol and s conceptual role in the iexical entry of e.g. SELL.,26,27
15190,7706905,The parse associated with these chunks is most commonly a single symbol dominating a single word.,11,12
15191,7706905,This symbol is used to compute a ranked set of likely types this symbol is likely to map onto based on how much mutual information it has with each one.,1,2
15192,7706905,This symbol is used to compute a ranked set of likely types this symbol is likely to map onto based on how much mutual information it has with each one.,13,14
15193,7706905,"In the case that this is a new symbol which the net has no information about yet, it will return a ranked list of types based on how frequently those types are the correct output.",8,9
15194,5820758,"All symbols and digits are replaced by the symbol N. Sections 0-20 (929,564 words) are used as the training set for collecting counts, sections 21-22 (73,760 words) as the development set for tuning parameters, and sections 23-24 (82,430 words) for testing.",8,9
15195,15974189,"Since the first symbol of the extension specification of A matches the category of I, an new edge is created by the fundamental rule, as shown in Figure 3c .",3,4
15196,15974189,"For instance for a top-down invocation strategy, the following rule could be used: Top-down Strategy Rule Whenever an active edge is added to the chart, if the first symbol it needs to extend itself is a nonterminal, add an empty active edge at its right hand end for each rule in the gra-s~=r which expands the needed symbol.",35,36
15197,15974189,"For instance for a top-down invocation strategy, the following rule could be used: Top-down Strategy Rule Whenever an active edge is added to the chart, if the first symbol it needs to extend itself is a nonterminal, add an empty active edge at its right hand end for each rule in the gra-s~=r which expands the needed symbol.",67,68
15198,15974189,"With this rule and the fundamental rule in operation, simply adding empty active edges for all rules expanding the distinguished symbol to the left hand end of the chart will provoke the parse.",21,22
15199,244077660,"These elements are lexes which are termed here chemical roots and given the general class symbol R. A subclassification indicated by second and third position symbols has been carried out, but is not given here except by way of example.",15,16
15200,244077660,"These roots are characterized by their immediate co-occurrence with certain suffixes, e.g. The second position symbol for suffixes corresponds in general to the second position symbol of a constitute containing the suffix (e.g. пентоз is a constitute of class Na, -OB is a suffix of class za).",18,19
15201,244077660,"These roots are characterized by their immediate co-occurrence with certain suffixes, e.g. The second position symbol for suffixes corresponds in general to the second position symbol of a constitute containing the suffix (e.g. пентоз is a constitute of class Na, -OB is a suffix of class za).",28,29
15202,245118111,Golem t h e a u t o m a t a is the symbol of man's horror of t h e t h i n g t h a t straddles t h e line bctwccn spirit and flesh.,14,15
15203,1947939,"The symbol ""C"" followed by a number refers to a column in the government pattern; if followed by a dot and a number, it refers to a row in that column.",1,2
15204,1947939,"The symbol ""+"" means ""together with.""",1,2
15205,1947939,"This includes the following (the symbol used by the DECFC is shown inside parentheses): • synonyms (SYN): same meaning and same syntactic category • antonyms (ANTI): same meaning except that the definition of one of the two lexemes includes a negation • syntactic derivates: same meaning but different syntactic categories such as: Nominalization (So), Verbalization (Vo), Adverbialization (Advo), Adjectivization (Adjo) • Converses (CONV): same meaning and same syntactic category, but the order of syntactic actants with respect to semantic actants is different.",6,7
15206,1947939,"We have used the symbol Comp in that case (e.g., if ANTI(A)=B and CONV(B)=C, then the relation resulting from the composition is simply ANTI(CONV(A))----C).",4,5
15207,1947939,"This is clearly different from relation composition, because in many cases more than one result is possible (the symbol * indicates the possibility of having no relation).",20,21
15208,8629740,"An element of a transition pair is either a phoneme, a phoneme class name, the symbol = or the empty string e. A phoneme is a character and is a member of the alphabet set.",17,18
15209,5455910,"Any of these word components can be unspecified, and therefore denoted with the symbol *.",14,15
15210,32031544,"In addition, each dictionary item is provided with a symbol indicating whether it belongs to a SEA, SEC or SE-zero category.",10,11
15211,32031544,"They contain all ambiguous fragments found to be legitimate, and their fully determined counterparts with appropriate ""beginning"" and ""end"" symbol pairs indicating their weight /x/.../y/. A special section indicates the weights of ""isolated words"", whether these weights are fully determined (/x/.../y/) or ambiguous (/w/.../z/).",24,25
15212,32031544,"-Table of positively weighted ""end"" -""beginning"" symbol combinations: In this table all legitimate non-zero . .",9,10
15213,32031544,"-Fourth scanning: At this stage, all /0/ ... /0/s would be set aside and strings of positively weighted fragments examined on the basis of ""end"" -""beginning"" symbol tables.",33,34
15214,32031544,As a first approximation these parts of speech are defined in accordance with the indications given by Webster ' In the main body of the preceding paper fully determined parts of speech are designated by the symbol A i while the ambiguous ones bear the symbol X j .,36,37
15215,32031544,As a first approximation these parts of speech are defined in accordance with the indications given by Webster ' In the main body of the preceding paper fully determined parts of speech are designated by the symbol A i while the ambiguous ones bear the symbol X j .,45,46
15216,4628524,"3 Additionally, {O contains the symbol self which denotes Ihe currently considered lexical item.",7,8
15217,2847571,"Given the necessary complexity and diversity of the symbol processing required by the design decisions above, the programming language and related software support must provide: 1.",8,9
15218,8308091,This step effective replaces singleton variables with a new unique atomic symbol 'ANY'.,11,12
15219,8308091,"Top-down processing fires last, and performs a recursive-descent walk of the grammar starting atthe start symbol , generating a new grammar that propagates features downward through the grammar.",20,21
15220,8308091,"Since each nonterminal is now ground, it is trivial to assign each nonterminal a unique atomic symbol, and rewrite the grammar as a CFG.",17,18
15221,26143700,"Booth and Richens proposed printing only the symbol ""z"" to indicate an unspecified word; others have proposed leaving the word untranslated, and others have proposed always giving the most common translation.",7,8
15222,1333972,The symbol .. stands for the remainder of the stack symbols.,1,2
15223,1333972,Each nonterminal (t[..η] or b[..η]) with the top of the stack symbol in a LIG rule corresponds to a unique node in some elementary tree of the grammar.,19,20
15224,1333972,"r Scanner: b[..η] → Γ • w∆, i, j, k, l b[..η] → Γw • ∆, i, j, k, l + 1 , w = w l+1 , ∅ If w (a terminal symbol) occurs at position l+1, the scanner rule creates a new item whose span extends to l+1.",49,50
15225,1333972,"Given these inference rules, the recognition process is initialized using axioms of the form t[η s ] → •Γ, 0, −, −, 0 for each rule t[η s ] → Γ where η s is the root node of an initial tree labeled with the start symbol.",51,52
15226,1329789,"Given a TAG G, Λ 1 is build as follows: • for every non-termninal symbol X, there are two types X S and X A standing for places where substitution and adjunction can occur respectively; • for every elementary tree γ, there is a constant c γ ∈ C 1 .",18,19
15227,1329789,"Moreover, for every non-terminal symbol X, there is a constant I X : X A .",7,8
15228,1329789,"For any non-terminal symbol X, there are constants X 0 , . . . ,",5,6
15229,1329789,"For any terminal symbol X in G, there is a constant X : τ ∈ C 2 .",3,4
15230,1329789,"The symbol h ≥ l imposes the constraint for a formula that is associated to l to be a subformula of the one associated to h. l : p indicates that a predicate p of SRL is labelled in URL by l. l 1 : All(x, l 2 ) l 4 : Some(y, l 5 ) l 5 : And(l 6 , h 2 ) l 2 : Imp(l 3 , h 1 ) l 3 : dog(x) l 6 : cat(y) h 0 l 1 l 4 l 7 l 7 : chases(x, y) Figure 3 : URL formula for every dog chases a cat We want to underline the difference between URL and SRL because our concern in this paper is not to build and manage SRL formulas, but only URL formulas, that is underspecified representations.",1,2
15231,232021765,"for indeed , who could deny entry into the european house to states such as the poland of copernicus and john paul ii , the hungary bullied in budapest in 1956 or the capital that has become a symbol , prague , when popular democracies threw the head of state out of the window and crushed the nation of jan palach , sacrificed in the name of freedom .",38,39
15232,14797543,Initial trees are trees where leaf-nodes are labelled either by a terminal symbol or by a non-terminal symbol marked for substitution (↓).,14,15
15233,14797543,Initial trees are trees where leaf-nodes are labelled either by a terminal symbol or by a non-terminal symbol marked for substitution (↓).,21,22
15234,7901127,"Following Hopcroft and Ullman (1979) , we define a context-free grammar G as a 4-tuple (N, Σ, A, R), where N is a set of nonterminal symbols, Σ is an alphabet, A is a distinguished start symbol in N, and R is a finite set of rules, in which each rule is of the form X → β for some X ∈ N, β ∈ (N ∪ Σ) * .",50,51
15235,7901127,"We will extend the left and right sequences to include a terminating STOP symbol, allowing a Markov process to model the left and right sequences.",13,14
15236,7901127,"Formally, we will always take a lexicalized nonterminal P(h) to expand deterministically (with probability one) in this way if P is a part-of-speech symbol.",31,32
15237,7901127,Internal rules always have an LHS in which P is not a part-of-speech symbol.,17,18
15238,7901127,"The STOP symbol is added to the vocabulary of nonterminals, and the model stops generating left modifiers when the STOP symbol is generated.",2,3
15239,7901127,"The STOP symbol is added to the vocabulary of nonterminals, and the model stops generating left modifiers when the STOP symbol is generated.",21,22
15240,7901127,"marks the position of the next modifier to be generated-it could be a nonterminal/headword/head-tag triple, or the STOP symbol.",27,28
15241,7901127,"Most importantly, the probability of generating the STOP symbol will be zero when the subcategorization frame is non-empty, and the probability of generating a particular complement will be zero when that complement is not in the subcategorization frame; thus all and only the required complements will be generated.",9,10
15242,7901127,"Because of this, the probability of generating the STOP symbol should be greatly increased when the previous modifier is, for example, a determiner.",10,11
15243,7901127,"The probability of NPB(dog) → DT(the) NN(dog) would be estimated as 11 P h (NN | NPB,dog) × P l (DT(the) | NPB,NN,dog) × P l (STOP | NPB,NN,dog) × P r (STOP | NPB,NN,dog) In making the independence assumption P l (STOP | DT(the), NPB,NN,dog) = P l (STOP | NPB,NN,dog) the model will fail to learn that the STOP symbol is very likely to follow a determiner.",101,102
15244,7650388,"b) If the modifier does not belong to that list, coders are asked to try an insertion test: della cultura + ADJ, (of the ADJ culture) : • lo Spondylus e' un simbolo del Neolitico Danubiano = lo Spondylus e' un simbolo della cultura Neolitica Danubiana (the Spondylus is a symbol of the Danubian Neolithic = the Spondylus is a symbol of the Danubian Neolithic culture). •",59,60
15245,7650388,"b) If the modifier does not belong to that list, coders are asked to try an insertion test: della cultura + ADJ, (of the ADJ culture) : • lo Spondylus e' un simbolo del Neolitico Danubiano = lo Spondylus e' un simbolo della cultura Neolitica Danubiana (the Spondylus is a symbol of the Danubian Neolithic = the Spondylus is a symbol of the Danubian Neolithic culture). •",69,70
15246,17564610,"A-I Rule: Given an active edge A and an inactive edge I with from(I)=to(A), and, having named i toposition(A), with i¢ n (the number of symbols in the right hand side of the rule), cat(I)= Ci +1, i + 1-th symbol of the right hand side of rule(A), then a new edge E can be added to the chart, with from(E)=from(A), to(E)=to(I), and, if i+l=n was the last symbol in rule(A) and fromposition(A)=0, E will be an inactive edge with cat(E) equal to the left hand side of rule(A), if not it will be an active edge with rule(E) = rule(A) and fromposition(E) = fromposition(A), toposition(E) = i + 1.",54,55
15247,17564610,"A-I Rule: Given an active edge A and an inactive edge I with from(I)=to(A), and, having named i toposition(A), with i¢ n (the number of symbols in the right hand side of the rule), cat(I)= Ci +1, i + 1-th symbol of the right hand side of rule(A), then a new edge E can be added to the chart, with from(E)=from(A), to(E)=to(I), and, if i+l=n was the last symbol in rule(A) and fromposition(A)=0, E will be an inactive edge with cat(E) equal to the left hand side of rule(A), if not it will be an active edge with rule(E) = rule(A) and fromposition(E) = fromposition(A), toposition(E) = i + 1.",92,93
15248,17564610,"Similarly, if to(I)=from(A), and having named i fromposition(A), i¢ 0, cat(I) = Ci-I , i-l-th symbol of the right handside of rule(A), then a new edge E can be added to the chart, with from(E)= from(I), to(E)= to(A), and, if i-1 = 0 and toposition(A) is equal to the length of the right handside of rule(A), E will be an inactive edge with cat(E) equal to the left handside of rule(A), if not, it will be an active edge with rule(E)=rule(A), fromposition(E)=i-1, toposition(E) = toposition(A).",28,29
15249,17564610,"We can now state the I/bu Rule: When an inactive edge I, with .withisland(I)=true, is introduced in the chart, a new active edge is introduced for every rule R in the grammar that includes on its right hand side the symbol cat(I) and in relation to R for every position i such that cat(I) is the i + 1-th symbol on the right hand side of R. Let us denote such a generic active edge as A; its characteristics will be from(A)= from(I), to(A)--t0(I), rule(A) = R, fromposition(A) = i, toposition(A) = i + 1, sub-inactives = list(I).",46,47
15250,17564610,"We can now state the I/bu Rule: When an inactive edge I, with .withisland(I)=true, is introduced in the chart, a new active edge is introduced for every rule R in the grammar that includes on its right hand side the symbol cat(I) and in relation to R for every position i such that cat(I) is the i + 1-th symbol on the right hand side of R. Let us denote such a generic active edge as A; its characteristics will be from(A)= from(I), to(A)--t0(I), rule(A) = R, fromposition(A) = i, toposition(A) = i + 1, sub-inactives = list(I).",69,70
15251,17564610,"We have also the usual top-down rule, rivisited consistently with our approach: A/td Rule: When an active edge A is added to the chart, if from the vertex to(A) only edges with withisland=false leave rightward, then introduce a cycling active edge on to(A) for every rule that has on the left handside the symbol that comes after the position toposition(A) for rule rule(A), unless there is already an active edge with that rule or an inactive edge with that category.",66,67
15252,18505916,"It is realized with very simple space management transition networks (SMTNs), in which $EXP, a distinguished symbol on an arc, indicates that only the occurrence of something expected by the preceding context (i.e., for which an impulse was set up) may allow the transition.",21,22
15253,18505916,"Given an active edge A spanning from Va to V b and an inactive edge I spanning from Vb to V e, where A refers to rule R and to position i, and the category of I is just Ci + ~, i + 1-th symbol of the right hand side of R, then a new edge E can be added to the chart, which will span from Va to Vc, and, if Ci + ~ was the last symbol in R, E will be an inactive edge with category equal to the symbol on the left hand side of R; if not, it will be an active edge with rule R and position i + 1.",50,51
15254,18505916,"Given an active edge A spanning from Va to V b and an inactive edge I spanning from Vb to V e, where A refers to rule R and to position i, and the category of I is just Ci + ~, i + 1-th symbol of the right hand side of R, then a new edge E can be added to the chart, which will span from Va to Vc, and, if Ci + ~ was the last symbol in R, E will be an inactive edge with category equal to the symbol on the left hand side of R; if not, it will be an active edge with rule R and position i + 1.",88,89
15255,18505916,"Given an active edge A spanning from Va to V b and an inactive edge I spanning from Vb to V e, where A refers to rule R and to position i, and the category of I is just Ci + ~, i + 1-th symbol of the right hand side of R, then a new edge E can be added to the chart, which will span from Va to Vc, and, if Ci + ~ was the last symbol in R, E will be an inactive edge with category equal to the symbol on the left hand side of R; if not, it will be an active edge with rule R and position i + 1.",103,104
15256,18505916,"If the parser is a top-down one, when, given an active edge with rule R and position i, that has reached the vertex V, there is a rule R' with left hand side equal to Ci + 1, i + 1-th symbol of the right hand side of R, an empty active edge is introduced on the vertex V, with rule R', provided that one such edge is not already present on that vertex.",51,52
15257,18505916,"If the parser is a bottom-up one, when, given an inactive edge with category C that has reached the vertex V, there is a rule R' that has C as the first symbol of its right hand side, an empty active edge is introduced on the vertex V, with rule R', provided that one such edge is not already present on that vertex.",38,39
15258,18505916,"The whole process of parsing aims at getting one or more (if the sentence is ambiguous) inactive edges to span the whole string, with category S the distinguished initial symbol in the grammar.",32,33
15259,18505916,"First, we do not want the parsing process to be overwhelmed by the appearance of spurious constituents, where a rule that prescribes the introduction of the empty symbol was applied.",29,30
15260,18505916,"The symbol + stands for ""at least one occurrence of what precedes"".",1,2
15261,6777241,"Stress is associated with a vowel by suffixing it with the special terminal symbol * , leading to a distinction between stressed ( SSyll ) and unstressed ( USyll ) syllables.",13,14
15262,8942894,"cc s,k is the number of times character k has been observed in the words in labels(s) which, in turn, refers to the words labeling (unigram) tables in model state s. chars is the set of different characters in the language, including the word-boundary symbol #.",54,55
15263,8942894,"In addition, we do not fix the probability for the word-boundary symbol, treating it as a further special character # that may only occur at the end of a word.",14,15
15264,1650722,"Our grammar starts with a root symbol Discourse, which then selects a starting topic through a set of discourse initial rules, Discourse → Discourse t for t ∈ T ′ .",6,7
15265,1650722,"Specifically, the probability of each Earley path is scaled by a constant c i each time it passes through a scanned state generating the input symbol x i .",26,27
15266,1650722,"As noted in section §4.2, the logarithm of c i gives us the surprisal value for the input symbol x i .",20,21
15267,6568949,"An LR parser is basically a pushdown automaton, i.e. it has a pushdown stack in addition to a finite set of internal states, and a reader head for scanning the input string from left to right, one symbol at a time.",40,41
15268,6568949,In state S with lookahead symbol Syra it can: 1.,5,6
15269,6568949,"by goto(Sl,Sym,S2) (or shift(Sl,Sym,S2) if Sym is a terulinal symlml) can be constructed as Ibllows: I. Select all items in state S1 where a particular symbol gym follows immediately afte,' the (lot and move the dot to after this symbol.",38,39
15270,6568949,"by goto(Sl,Sym,S2) (or shift(Sl,Sym,S2) if Sym is a terulinal symlml) can be constructed as Ibllows: I. Select all items in state S1 where a particular symbol gym follows immediately afte,' the (lot and move the dot to after this symbol.",55,56
15271,6568949,In Simple LIt (SLR) the h)okahead is any termiual symbol that can imnlediately follow any symbol of the saltle tylie as the LIIS of tile rule.,11,12
15272,6568949,In Simple LIt (SLR) the h)okahead is any termiual symbol that can imnlediately follow any symbol of the saltle tylie as the LIIS of tile rule.,17,18
15273,6568949,"Secondly, this avoids trivial reduction ambignities where a particular reduction is performed once for each possible ruapping of the next word to a lookahead symbol.",25,26
15274,6568949,The choice made here is to regard phrases that rnap to tile same CF symbol as similar: Definition: Two phrases are similar if they map to the same conic*t-free symbol.,14,15
15275,6568949,The choice made here is to regard phrases that rnap to tile same CF symbol as similar: Definition: Two phrases are similar if they map to the same conic*t-free symbol.,33,34
15276,6568949,"Since the processing is performed by applying coltstraints incrementally and monotonically, where constraints are realized as Prolog terms and these are illstantiated stepwise, it is important that a UG phrase map to tile same CF symbol regardless of its degree of instantiation l'or this delinition to be useful.",37,38
15277,6568949,"For exampie, each entry in the goto table will have as a symbol the generalization of a set of UG phrases.",13,14
15278,6568949,"These UG phrases are those that map to the same contextfree symbol; occur in a UG rule that corresponds to an item where this CF symlml immedhttely follows the clot; and ill such a UC, rule occur at tile position immediately following tile clot.",11,12
15279,6568949,"Again, if there is no lexical ambiguity within the CF symbol, the fllll UO constraints are apl)lied.",11,12
15280,6568949,"Nothing is done about lexical an-lbignities outside of the sltnie CF symbol, though.",13,14
15281,1591992,"The models assume no a priori knowledge or ""universal"" principles-e.g., no preference for aligning a symbol with itself, aligning a vowel with a vowel rather than a consonant, etc.",20,21
15282,1591992,2007) and Hall and Klein (2011) handle context by conditioning the symbol being generated upon the symbols immediately preceding and following.,14,15
15283,1591992,"The simplest form of such alignment at the symbol level is a 1-1 pair (σ : τ ) ∈ Σ × T , a single symbol σ from the source alphabet Σ with a symbol τ from the target alphabet T .",8,9
15284,1591992,"The simplest form of such alignment at the symbol level is a 1-1 pair (σ : τ ) ∈ Σ × T , a single symbol σ from the source alphabet Σ with a symbol τ from the target alphabet T .",28,29
15285,1591992,"The simplest form of such alignment at the symbol level is a 1-1 pair (σ : τ ) ∈ Σ × T , a single symbol σ from the source alphabet Σ with a symbol τ from the target alphabet T .",37,38
15286,1591992,"We can then align word pairs, such as vene-venč (""boat"" in Finnish and Mordvin), in many ways, including, e.g., as in Figure 3 , where the alignment on the right contains symbol pairs: (v : v), (e : e), (n : n), (e : .), (. :",42,43
15287,1591992,"To construct such a code, we ""transmit"" the aligned data by listing the ""events""-the observed symbol pairs (σ : τ ).",19,20
15288,1591992,"We are also extending the presented model to work with more than 1-1 symbol alignment, using, e.g., 2-2 alignments found in (Kondrak, 2003; Wettig et al.,",15,16
15289,934325,except that % (' 0) 1 contains strings in which no additional symbol can be interleaved.,14,15
15290,2070082,"n}, with [0] = ∅. A ranked alphabet is a finite set Σ of symbols, associated with a rank function assigning a number rk Σ (σ) ∈ N to each symbol σ ∈ Σ. We write rk for rk Σ when the alphabet Σ is understood.",37,38
15291,2070082,The set of positions in a tree t labelled by a symbol a ∈ Σ ∪ X is defined as pos a (t) = {p | t(p) = a}.,11,12
15292,16700753,"The training corpus is separated into two parts, the words occurring in Part I but not in Part II and the words occurring in Part II but not in Part I are all replaced by a special symbol #Unknown#.",38,39
15293,6332815,"A derivation starts with a fragment whose root is labeled with the start symbol, and it proceeds by substituting a fragment for the leftmost nonterminal leaf under the constraint that the fragment's root node and the leaf node have the same label.",13,14
15294,14632567,"For example: possessive < state or descriptive < property < shape < NOUN Among them, ""property"" could be further subordered by semantic features: nature or character < sense < geometric < colour Note: the symbol ""<"" means being ahead of .",40,41
15295,2502527,"Introduction Bilexical context-free grammars, or 2-LCFGs for short, are specialized context-free grammars where each nonterminal is associated with a terminal symbol representing a lexical element of the language of interest.",28,29
15296,2502527,"Furthermore, no more than two symbols can be used in the right-hand side of a rule, and the terminal symbol associated with the lefthand side of a rule must also occur on the right-hand side.",23,24
15297,2502527,"A context-free grammar (CFG) is a 4-tuple G = (Σ, N, S, R), where Σ is a finite set of terminals, N is a finite set of nonterminals, disjoint from Σ and including the start symbol S, and R is a finite set of rules.",49,50
15298,2502527,"We associate with G a binary relation called rewrite and denoted by the symbol ⇒ G , defined such that γAγ ⇒ G γαγ if A → α is a rule in R and γ, γ ∈ (Σ ∪ N) * .",13,14
15299,2502527,"Assume a distinguished symbol # ∈ Σ, which we will refer to as a center marker.",3,4
15300,2502527,"The class of linear CFGs with center marker #, or lin-CFG(#) for short, is the class of CFGs in which each rule either has the form A → # or the form A → uBv, where A, B ∈ N and u, v ∈ (Σ \ {#}) * , where symbol ""\"" denotes set difference.",64,65
15301,2502527,"Observe how in a parse tree generated by a 2-LCFG, each occurrence of a lexical element (represented as a terminal symbol) is also part of several head child occurrences of nonterminals above it, up to some unique maximal projection occurrence.",24,25
15302,2502527,"The start symbol is A[a] and the rule set R (A[a]) is specified as follows: r D[a] → B[b] C[a] is in R (A[a]) for each rule D[a] → B[b] C[a] in R; r D[a] → B[a] C[c] is in R (A[a]) for each rule D[a] → B[a] C[c] in R; r D[a] → B[a] is in R (A[a]) for each rule D[a] → B[a] in R; r D[a] → a is in R (A[a]) for each rule D[a] → a in R. Grammar G (A[a]) might contain useless rules, that is, rules that never appear in a derivation of a string of the generated language, but this is irrelevant to the development of the results in this article.",2,3
15303,2502527,"G 1 and G 2 have the same rules rewriting the start symbol, that is, the rules of the form S → A[a]; and 3.",12,13
15304,2502527,"If v or x contain one or more occurrences of the symbol $, we can pump such a string in α and obtain a new string which is not a valid computation, which is a contradiction.",11,12
15305,2502527,"Using similar arguments, we can conclude that neither v nor x contains an occurrence of symbol $.",16,17
15306,10479962,"Attributes may include the ticker symbol, SIC product codes, industry codes and company headquarters information.",5,6
15307,10479962,"If the definition builder provides the company name and ticker symbol to be used as CVTs for some company, as in #CO = Samson Computing Supply lnc. #",10,11
15308,14810479,"A context-free grammar (CFG) is a 4-tuple G = (N, Σ, S, R) where N and Σ are finite disjoint sets of nonterminal and terminal symbols, respectively, S ∈ N is the start symbol and R is a finite set of rules.",46,47
15309,14810479,"Let D be an infinite set of complete derivations using nonterminal symbols in N , start symbol S ∈ N and terminal symbols in Σ. We assume that the set of rules that are observed in D is drawn from some finite set R. Let p D be a probability distribution defined over D, that is, a function from set D to interval [0, 1] such that d∈D p D (d) = 1.",16,17
15310,14810479,Assume now |d| > 1 and let π = (A → α) be the first rule used in d. Note that there must be at least one nonterminal symbol in α.,30,31
15311,4889737,"the theory of context-free grammars, which are often extended with a new start symbol.)",16,17
15312,7145772,"A context-free grammar G is a 4-tuple (Σ, N, S, R), where Σ and N are two finite disjoint sets of terminals and nonterminals, respectively, S ∈ N is the start symbol, and R is a finite set of rules, each of the form A → α, where A ∈ N and α ∈ (Σ ∪ N) * .",43,44
15313,7145772,"In what follows, symbol a ranges over the set Σ, symbols w, v range over the set Σ * , symbols A, B range over the set N, symbol X ranges over the set Σ ∪ N, symbols α, β, γ range over the set (Σ ∪ N) * , symbol ρ ranges over the set R, and symbols d, e range over the set R * .",4,5
15314,7145772,"In what follows, symbol a ranges over the set Σ, symbols w, v range over the set Σ * , symbols A, B range over the set N, symbol X ranges over the set Σ ∪ N, symbols α, β, γ range over the set (Σ ∪ N) * , symbol ρ ranges over the set R, and symbols d, e range over the set R * .",33,34
15315,7145772,"In what follows, symbol a ranges over the set Σ, symbols w, v range over the set Σ * , symbols A, B range over the set N, symbol X ranges over the set Σ ∪ N, symbols α, β, γ range over the set (Σ ∪ N) * , symbol ρ ranges over the set R, and symbols d, e range over the set R * .",60,61
15316,7145772,"With slight abuse of notation, we treat a rule ρ = (A → α) ∈ R as an atomic symbol when it occurs within a string dρe ∈ R * .",22,23
15317,7145772,The symbol denotes the empty string.,1,2
15318,7145772,"In what follows, symbols q, r, s range over the set Q, symbol τ ranges over the set T, and symbol c ranges over the set T * .",16,17
15319,7145772,"In what follows, symbols q, r, s range over the set Q, symbol τ ranges over the set T, and symbol c ranges over the set T * .",25,26
15320,2627881,"The symbol ""y"" along an edge means the node it points to will be kept, and ""n"" means the node will be removed.",1,2
15321,739426,"Because the parsers' moves are determined solely by the top two category labels on the stack and possibly the look-ahead symbol, they are much simpler than stochastic LR parsers (Briscoe and Carroll, 1993; Inui et al.,",23,24
15322,739426,A shift-reduce parse is a sequence of moves which (when composed) map the empty stack to the two-element stack whose top element is ' ' and whose next-to-top element is the start symbol. (,42,43
15323,739426,"To ensure that the next move is in fact a possible move given the current stack, we require that P(reduce 1 (c)| , ) = 0 and P(reduce 2 (c)|c , ) = 0 for all c, c , and that P(shif t( )|s 1 , s 2 ) = 0 unless s 1 is the start symbol and s 2 = .",63,64
15324,739426,"A conditional shift-reduce parser differs only minimally from the shift-reduce parser just described: it is defined by a distribution P(m|s 1 , s 2 , t) over next moves m given the top and next-to-top stack labels s 1 , s 2 and the next input symbol w (w is called the look-ahead symbol).",56,57
15325,739426,"A conditional shift-reduce parser differs only minimally from the shift-reduce parser just described: it is defined by a distribution P(m|s 1 , s 2 , t) over next moves m given the top and next-to-top stack labels s 1 , s 2 and the next input symbol w (w is called the look-ahead symbol).",66,67
15326,739426,"In addition to the requirements on P above, we also require that if w = w then P(shift(w )|s 1 , s 2 , w) = 0 for all s 1 , s 2 ; i.e., shift moves can only shift the current look-ahead symbol.",50,51
15327,739426,"Because the conditional parser predicts its next move on the basis of the lookahead symbol as well as the two top stack categories, one might expect it to predict this next move more accurately than the joint shift-reduce parser.",14,15
15328,3191956,Moore 2000 suggests an additional constraint o n nonterminals D X that can appear in useful productions of LC L G: D must either be the start symbol of G or else appear in a production A !,28,29
15329,3191956,"The left factor operation constructs new nonterminals corresponding to common pre xes of arbitrary length, while left-corner factoring e ectively only factors the rst nonterminal symbol on the right hand side of left-corner productions.",28,29
15330,2639887,"1 Here we de- Initializer: y : [B → • γ, j, j] (y : B → γ) ∈ P 0 ≤ j ≤ n Scanner: x 1 : [A → α • aβ, i, j] x 1 : [A → αa • β, i, j + 1]    (y 1 : A → αaβ) ∈ P 0 ≤ i ≤ j < n a j+1 = a Completer: x 1 : [A → α • Bβ, i, j] x 2 : [B → γ •, j, k] x 1 + x 2 : [A → αB • β, i, k]    (y 1 : A → αBβ) ∈ P (y 2 : B → γ) ∈ P 0 ≤ i ≤ j ≤ k ≤ n Goal items: [S → γ •, 0, n] for any (y : S → γ) ∈ P, where S is the start symbol fine such a weighted deduction system for parsing as consisting of a finite set of inference rules of the form: x 1 : I 1 x 2 : I 2 . . .",196,197
15331,2639887,"The side conditions refer to an input string w = a 1 • • • a n and to a weighted context-free grammar with a set of productions P, each of which has the form (y: A → α), where y is a non-negative real-valued weight, A is a nonterminal, Starter: y : [S → • γ, 0, 0] (y : S → γ) ∈ P, where S is the start symbol Predictor: x 1 : [A → α • Bβ, i, j] y 2 : [B → • γ, j, j]    (y 1 : A → αBβ) ∈ P (y 2 : B → γ) ∈ P 0 ≤ i ≤ j ≤ n Scanner, completer and set of goal items are as in Figure 1 .",92,93
15332,2639887,"Starter: (y, y) : [S → • γ, 0, 0] (y : S → γ) ∈ P, where S is the start symbol Scanner: (z 1 , x 1 ) : [A → α • aβ, i, j] (z 1 , x 1 ) : [A → αa • β, i, j + 1]    (y 1 : A → αaβ) ∈ P 0 ≤ i ≤ j < n a j+1 = a Predictor: (z 1 , x 1 ) : [A → α • Bβ, i, j] (z 1 + y 2 , y 2 ) : [B → • γ, j, j]    (y 1 : A → αBβ) ∈ P (y 2 : B → γ) ∈ P 0 ≤ i ≤ j ≤ n Completer: (z 1 , x 1 ) : [A → α • Bβ, i, j] (z 2 , x 2 ) : [B → γ •, j, k] (z 1 + x 2 , x 1 + x 2 ) : [A → αB • β, i, k]    (y 1 : A → αBβ) ∈ P (y 2 : B → γ) ∈ P 0 ≤ i ≤ j ≤ k ≤ n Set of goal items is as in Figure 1 .",33,34
15333,2639887,"If we assume, without loss of generality, that there is only one goal item, then this goal item becomes the start symbol of c(G, w).",24,25
15334,5959493,"$ , where is a finite set of terminals, called the alphabet, is a finite set of nonterminals, including the start symbol , and # is a finite set of rules having the form & (' 0) with & 21 3 and ) 41 5 6 87 9 $ .",24,25
15335,5959493,"Furthermore, we assume that the input grammars do not contain epsilon rules and that there is only one rule T' U) defining the start symbol .",27,28
15336,5959493,"Thus, a PDA is a 5-tuple `Y a b c ed Cc gf b ih d Gp Sq `r $ , where is the alphabet as above, Y is a finite set of stack symbols including the initial stack symbol b c ed Cc sf and the final stack symbol b ih d Gp Sq , and r is the set of transitions, having one of the following three forms: b ut ' b wv representing stacks.",45,46
15337,5959493,"Thus, a PDA is a 5-tuple `Y a b c ed Cc gf b ih d Gp Sq `r $ , where is the alphabet as above, Y is a finite set of stack symbols including the initial stack symbol b c ed Cc sf and the final stack symbol b ih d Gp Sq , and r is the set of transitions, having one of the following three forms: b ut ' b wv representing stacks.",55,56
15338,5959493,"We remark that in our notation stacks grow from left to right, i.e., the top-most stack symbol will be found at the right end.",20,21
15339,5959493,"We write d { to indicate that there is a computation 9 SE $ 9 f $ of the PDA such that all of the following three conditions hold: (i) either ¦ d P or ¥ d r ; (ii) the computation starts with zero or more push transitions, followed by one scan transition reading E and by zero or more pop transitions; (iii) if ¦ then the top-most symbol of must be in the right-hand side of a pop or scan transition (i.e., top-most in the stack at the end of a previous segment) and if ¦ t , then the topmost symbol of must be the left-hand side of a push or scan transition (i.e., top-most in the stack at the beginning of a following segment).",80,81
15340,5959493,"We write d { to indicate that there is a computation 9 SE $ 9 f $ of the PDA such that all of the following three conditions hold: (i) either ¦ d P or ¥ d r ; (ii) the computation starts with zero or more push transitions, followed by one scan transition reading E and by zero or more pop transitions; (iii) if ¦ then the top-most symbol of must be in the right-hand side of a pop or scan transition (i.e., top-most in the stack at the end of a previous segment) and if ¦ t , then the topmost symbol of must be the left-hand side of a push or scan transition (i.e., top-most in the stack at the beginning of a following segment).",121,122
15341,5959493,"The first inference rule (11) can be easily justified: we want to investigate strings that are both generated by the grammar and recognized by the PDA, so we begin by combining the start symbol and a matching right-hand side from the grammar with the initial stack for the PDA.",37,38
15342,5959493,"We do this one symbol at a time, starting with the symbol just beneath the part of the stack that is already available.",4,5
15343,5959493,"We do this one symbol at a time, starting with the symbol just beneath the part of the stack that is already available.",12,13
15344,5959493,"Rules ( 18 ) and ( 19 ) repeat computations that have been done before, but in a backward manner, in order to propagate the information that deeper stack symbols are needed than those currently available, in particular that we want to know whether a certain stack symbol may occur below the currently available parts of the stack.",50,51
15345,5959493,"If we find the required stack symbol , we propagate the information forward that this symbol may indeed occur at the specified position in the stack.",6,7
15346,5959493,"If we find the required stack symbol , we propagate the information forward that this symbol may indeed occur at the specified position in the stack.",15,16
15347,5959493,"Rule (15) only transfers the top-most symbol b to the consequent, in order to keep the stacks as shallow as possible and to achieve a high degree of sharing of computation.",10,11
15348,1277731,"A context-free grammar G is a 4-tuple (G,N,P,S), where G and N are two finite disjoint sets of terminals and nonterminals, respectively, S E N is the start symbol, and P is a finite set of rules.",43,44
15349,1277731,"The process is initiated at the start symbol, and from there the process descends the grammar in all ways until terminals are encountered, and then transitions are created labeled with those terminals.",7,8
15350,1277731,"1 Assume we have a list of subautomata A1 ..... Am that is ordered from lower-level to higher-level automata; i.e., if an automaton Ap occurs as the label of a transition of automaton Aq, then p < q; Am must be the start symbol S. This order is a natural result of the way that subautomata are constructed during our depth-first traversal of the grammar, which is actually postorder in the sense that a subautomaton is output after all subautomata occurring at its transitions have been output.",51,52
15351,1277731,We assume A is the start symbol and therefore qA becomes the initial state and q~ becomes the final state in the approximating automaton.,6,7
15352,1277731,"Strictly speaking, states qAH and qrH, with [HI < d -1 and I = [A --+ a • fl], will only be needed if AIH ] is the start symbol in the case IH[ > 0, or if A is the start symbol in the case H = c.) The transitions of the automaton that pertain to terminals in right-hand sides of rules are very similar to those in the case of the unparameterized method: For a state qIH with I of the form [A ~ a • aft], we create a transition (q~H, a, qi,H), with I' = [A ~ aa • fl].",35,36
15353,1277731,"Strictly speaking, states qAH and qrH, with [HI < d -1 and I = [A --+ a • fl], will only be needed if AIH ] is the start symbol in the case IH[ > 0, or if A is the start symbol in the case H = c.) The transitions of the automaton that pertain to terminals in right-hand sides of rules are very similar to those in the case of the unparameterized method: For a state qIH with I of the form [A ~ a • aft], we create a transition (q~H, a, qi,H), with I' = [A ~ aa • fl].",50,51
15354,1277731,"If the start symbol is S, the initial state is qs and the final state is q~ (after the symbol S in the subscripts we find empty lists of items).",3,4
15355,1277731,"If the start symbol is S, the initial state is qs and the final state is q~ (after the symbol S in the subscripts we find empty lists of items).",21,22
15356,1277731,"Since reaching A can have three different histories of length shorter than 2 (the empty history, since A is the start symbol; the history of coming from the rule position given by item [A -~ c • A]; and the history of coming from the rule position given by item [B ~ d • Ae]), in Figure 6 we now have three states of the form qI~ for each I --[A ~ a • fl], as well as three states of the form qA~r and q~H"" The higher we choose d, the more precise the approximation is, since the histories allow the automaton to simulate part of the mechanism of recursion from the original grammar, and the maximum length of the histories corresponds to the number of levels of recursion that can be simulated accurately.",23,24
15357,1277731,We again assume A is the start symbol.,7,8
15358,1277731,"However, stacks that contain two occurrences of a stack symbol are identified with the shorter stack that results by removing the part of the stack between the two occurrences, including one of the two occurrences.",10,11
15359,1277731,Each individual stack symbol is now translated to one state of the nondeterministic finite automaton.,3,4
15360,1277731,"We achieve this by introducing three new nonterminals S[1], S[2] and S[3], and by adding modified copies of the original grammar rules, so that we obtain: S[1] S[2] S[3] S The new start symbol is S[1].",43,44
15361,9933866,A checkmark symbol indicates if the relation is symmetric (' ') or not ('x') for a particular syntactic construction.,2,3
15362,1550989,"Left-factoring replaces each production A ~ /3 : p, where p is the production probability and Jill = n > 2, with the following set of binary productions: A ~ '~1,n-l'fln :P 'fll,i' ~ '~l,i-l' ~i : 1.0 '/~1,2' ~ /~1 ~2:1.0 for i e [3, n] In these productions j3i is the ith element of ~3 and '~3i,j' is the subsequence /3i...flj of fl, but treated as a 'new' single non-terminal in the left-factored grammar (the quote marks indicate that this subsequence is to be considered a single symbol).",126,127
15363,505515,"The position of the syntax error can be defined as the rightmost symbol of the shortest prefix of the input that cannot be extended to be a correct sentence in the language L. In formal notation, this prefix for a given erroneous input w ~ L is defined as the string va, where w = vax, for some x, such that vy E L, for some y, but vaz ~ L, for any z. (The symbols v, w .... denote strings, and a denotes an input symbol.)",12,13
15364,505515,"The position of the syntax error can be defined as the rightmost symbol of the shortest prefix of the input that cannot be extended to be a correct sentence in the language L. In formal notation, this prefix for a given erroneous input w ~ L is defined as the string va, where w = vax, for some x, such that vy E L, for some y, but vaz ~ L, for any z. (The symbols v, w .... denote strings, and a denotes an input symbol.)",97,98
15365,36338848,"Additionally, computational phonology may be able to provide formal devices that are useful in phonology proper, as in the case of the information-theoretic evaluation metric (Ellison 1993 ) that is intended to replace the naive symbol-counting version.",40,41
15366,1863789,"The process is initiated at the start symbol, and from there the process descends the grammar in all ways until terminals are encountered.",7,8
15367,1863789,"For example, for --t the rule D -r aC~... YmEt~, which corresponds to the rule A~ ~ CA Y1... YrnEts of the transformed grammar, the filter item D --~ aC~ ... Y,n o E/~ is output, which indicates that an instance of ~ ... Ym (or an approximation thereof) has just been read, which is potentially preceded by an instance of aC and followed by an instance of E/~. On the other hand, upon encountering a rule such as A~ --~ BA, which is an artifact of the grammar transformation, no output symbol is generated.",106,107
15368,1863789,"The transducer was determinized and minimized as if it were a finite automaton, i.e. in a transition (q, vlw , q~) the pair vlw is treated as one symbol, and the pair ele is treated as the empty string.",32,33
15369,1275445,"More precisely, being an 'attribute-value constant' is a property of an individual in an interpretation (i.e., an element of a feature structure), whereas being a constant is a property of a symbol in a formula.",40,41
15370,1275445,"4) Schema (3) requires each symbol in N to denote an attribute-value constant, and schema (4) enforces distinctness in essentially the same manner as that used in the specification systems of algebraic data-type theory (Kapur and Musser 1987) .",8,9
15371,1275445,"points out, one consequence of this is that every model of these constraints will contain individuals corresponding to each attribute-value constant (since each constant symbol will be assigned a denotation).",28,29
15372,1275445,These contain the three-place relation symbol arc and the one-place relation symbols con and 3rd-sg.,7,8
15373,1275445,"Union-Find and Equality As noted above, the equality axioms ensure that the relation that the equality symbol denotes is an equivalence relation and the substitutivity of equals for equals.",19,20
15374,1275445,"If ~ is in clausal form but is not an SB formula then it must contain a function symbol, so its Herbrand universe U is infinite.)",18,19
15375,421607,"We define the set GOAL to be the set consisting of S, the start symbol, and of all nonterminals A which occur in a rule of the form B--* t~ A fl where is not e (the empty sequence of grammar symbols).",15,16
15376,421607,"Initially, the parse stack consists only of the start symbol, which is the first goal, as indicate in Figure 1.",10,11
15377,421607,"If the element on top of the stack is the nonterminal A and if the first symbol of the remaining input is t, then we may remove t from the input and push an item ] and if the input has been completely read, then we may successfully terminate the parsing process.",16,17
15378,421607,The nondeterministie LC parsing algorithm defined above uses one symbol of lookahead in case of terminal left corners.,9,10
15379,421607,"If the element on top of the stack is the nonterminal A and if the first symbol of the remaining input is t, then we may remove t from the input and push an item The same idea can be used in a straightforward way to make generalized LC parsing suitable for 4Actually, an eighth clause is necessary to handle the special case where S, the start symbol, is nonfalse, and the input is empty.",16,17
15380,421607,"If the element on top of the stack is the nonterminal A and if the first symbol of the remaining input is t, then we may remove t from the input and push an item The same idea can be used in a straightforward way to make generalized LC parsing suitable for 4Actually, an eighth clause is necessary to handle the special case where S, the start symbol, is nonfalse, and the input is empty.",70,71
15381,421607,"PARSE: • r ¢= NIL • Create goal element g consisting of S, the start symbol   hidden left-recursive grammars, similar to the way this is handled in [Schabes, 1991] and [Leermakers, 1992] .",17,18
15382,421607,In this paper we have not considered GLC parsing with more lookahead than one symbol for terminal left corners.,14,15
15383,14704022,"We can then derive the string Wil ... wl, from the top symbol S in n+l steps: S ::~ WilSil ~ WilWi2Si2 :=~ ... Wi 1 • . .",13,14
15384,14704022,"The word string wkl ... wk, can be derived from the top symbol S in 2n+l steps: S ~ Ti, Si a =:~ wkxSi, =~ WklTi2Si2 ::~ Wkx Wk2 Si2 =~ • • • =~ Wkx • • • Wk, The interpretation of this is that we start off in the initial state S, select a PoS tag Tia at random, according to the probability distribution in state S, then generate the word wkl at random according to the lexical distribution associated with tag Til, then draw a next PoS tag Ti2 at random according to the transition probabilities associated with state Si~, hop to the corresponding state Si2, generate the word wk2 at random according to the lexical distribution associated with tag Ti2, etcetera.",13,14
15385,14704022,"If we keep the left-hand-side (LHS) symbol of the rule fix, and sum these probabilities over the different RHSs, we get one, since the probabilities are conditioned on the LHS symbol.",12,13
15386,14704022,"If we keep the left-hand-side (LHS) symbol of the rule fix, and sum these probabilities over the different RHSs, we get one, since the probabilities are conditioned on the LHS symbol.",39,40
15387,14704022,"The probability of a derivation step is the probability of rewriting a given symbol using some grammar rule, and equals the rule probability.",13,14
15388,14704022,"Since the same parse tree can be derived in different ways by first rewriting some symbol and then another, or vice versa, we need to specify the order in which the nonterminal symbols of a sentential form are rewritten.",15,16
15389,14704022,"This isn't exactly the same think as an SCFG, since the probabilities are typically not conditioned on the LHS symbol of some grammar rule, but on the current internal state and the current lookahead symbol.",21,22
15390,14704022,"This isn't exactly the same think as an SCFG, since the probabilities are typically not conditioned on the LHS symbol of some grammar rule, but on the current internal state and the current lookahead symbol.",37,38
15391,2645484,"Proposition Let x be a tuple of variables, A be any relation symbol, ~(A) be any formula in which A appears only positively, and ~(x) be a formula in which A does not appear.",13,14
15392,2109704,"l )3 A grammar rule is unfolded downwards on one of the symbols in its right-hand side if it is replaced by a set of rules, each corresponding to the expansion of the chosen symbol by means of another grammar rule.",38,39
15393,2109704,"More formally, let G = (E, EN, S, R) be a context-free grammar, and let r,r' C R, k E .M + such that rhs(r) = aAfl, lal = k -1, lhs(r') = A, rhs(r') = V. The rule adjunction of r I in the k th position of r is defined as a new rule RA(r, k, r ~) = r', such that: lhs(r"") = lhs(r) rhs(r"") = aVfl For unification grammars, we instead require lhs(r') U rhs(r)(k) lhs(r 1') = O(lhs(r)) rhs(r"") = O(oLTfl ) where rhs(r)(k) is the kth symbol of rhs(r), where X t3 Y indicates that X and Y unify, and where 0 is the most general unifier of lhs(r ~) and rhs(r)(k).",135,136
15394,2109704,"function Score was thus formulated as follows: Scorea = Acorr Corra --Aine InCa -~size Sizea where Corr and Inc are the number of correct and incorrect parses allowed by the grammar, and Size is the size of the grammar measured as the total number of symbol occurrences in the right-hand sides of its rules.",47,48
15395,2109704,Any phrase-structure grammar can be represented as a concatenation/or graph -a directed bipartite multigraph with an or-node for each symbol and a concatenation-node for each rule in the grammar.,25,26
15396,2109704,"The present description covers context-free grammars, but the scheme can easily be extended to any unification grammar with a context-free backbone by replacing symbol eqality with unification.",28,29
15397,5199024,"Consider a W-SCFG grammar G for translating between French and English, with initial nonterminal S, and containing among others the following rule: N → A manque à B / B misses A : θ, (1) where the source and target right-hand sides are separated by a slash symbol, and where θ is a non-negative real weight (interpreted multiplicatively) associated with the rule.",57,58
15398,5199024,"2 When we apply this procedure to all the rules of the grammar G, we obtain a new weighted synchronous CFG G , with start symbol Fact 1.",26,27
15399,5199024,We use a terminal symbol $ to denote the end of sentence both on the source and on the target.,4,5
15400,5199024,"Example Let us consider the following W-SCFG (where again, weights are not explicitly shown, and where we use a terminal symbol $ to denote the end of a sentence, a technicality needed for making the grammar compatible with the SA automaton of Figure 1 ): S → NP VP $ / NP VP $ NP → ces N A / these A N VP → sont A / are A A → marrons / brown A → marrons / totally corrupt A → cuits / cooked A → cuits / finished N → avocats / avocadoes N → avocats / lawyers It is easy to see that, for instance, the sentences: these brown avocadoes are cooked $, these brown avocadoes are finished $, and these totally corrupt lawyers are finished $ all belong to the intersection L SA,x L G,x , while the sentences these avocadoes brown are cooked $, totally corrupt lawyers are finished these $ belong only to L SA,x .",25,26
15401,5199024,"Let us call, as before G the raw W-SCFG obtained, its start symbol being t # s # S t $ ,σ all s $ , with σ all the set of all source tokens in x. Fact 2.",16,17
15402,16953314,"The symbol of t at w, denoted by t(w), is defined as σ if w = ε, and by t i (w ) if w = iw .",1,2
15403,1405777,"The distribution is given by P HMM ( x, π) = n i=1 p(x i , π i | π i−1 ) p(STOP | π n ) ( 13 ) π 0 is assumed, for simplicity, to be constant and known; we also assume that every state transition emits a symbol (no arcs), an assumption made in typical tagging and chunking applications of HMMs.",55,56
15404,415973,"The only sense in which it captures regularities is the assumption that the same symbol in two languages represents same sound, which results in assigning a cost of 0 to aligning a symbol to itself.",14,15
15405,415973,"The only sense in which it captures regularities is the assumption that the same symbol in two languages represents same sound, which results in assigning a cost of 0 to aligning a symbol to itself.",33,34
15406,415973,"1-1 symbol model We begin with our ""basic"" model, described in (Wettig and Yangarber, 2011; Wettig et al.,",3,4
15407,415973,"The basic model allows only 1-1 symbol alignments: one source symbol 8 may correspond to one target symbol-or to the empty symbol (which we mark as ""."").",8,9
15408,415973,"The basic model allows only 1-1 symbol alignments: one source symbol 8 may correspond to one target symbol-or to the empty symbol (which we mark as ""."").",13,14
15409,415973,"The basic model allows only 1-1 symbol alignments: one source symbol 8 may correspond to one target symbol-or to the empty symbol (which we mark as ""."").",20,21
15410,415973,"The basic model allows only 1-1 symbol alignments: one source symbol 8 may correspond to one target symbol-or to the empty symbol (which we mark as ""."").",26,27
15411,415973,More advanced models align substrings of more than one symbol to each other.,9,10
15412,415973,"The basic model also ignores context, whereas in reality symbol correspondences are heavily conditioned on their context.",10,11
15413,415973,"This leads to a chickenand-egg problem: on one hand, if we had the best alignment for the data, we could simply read off a set of rules, by observing which source symbol corresponds frequently to which target symbol.",37,38
15414,415973,"This leads to a chickenand-egg problem: on one hand, if we had the best alignment for the data, we could simply read off a set of rules, by observing which source symbol corresponds frequently to which target symbol.",43,44
15415,415973,"We add a special end-of-word symbol, always aligned to itself: (# : #).",9,10
15416,415973,"For the basic 1-1 model, since no information about the context is used, prediction simply means looking for the most probable symbol in target language for each symbol of w A .",25,26
15417,415973,"For the basic 1-1 model, since no information about the context is used, prediction simply means looking for the most probable symbol in target language for each symbol of w A .",31,32
15418,415973,"This is done for easier identification, since the point (a,b) displays the legend symbol for only language a.) Overall, many more points lie below the diagonal, (approximately 10% of the points are above).",18,19
15419,11315192,"One way to do this would be to give the chart an additional dimension so that, for each substring of the input, there comes to be a separate box for each grammatical symbol.",34,35
15420,11315192,"The boxes contain the various structures that the given substring has, and whose top node is labeled with the corresponding symbol.",21,22
15421,11315192,"If this can be done, we can replace the matching sequence with the single symbol that constitutes the left-hand side of the rule, and we attempt to meet the original set of goals with a string modified in this way.",15,16
15422,12780794,"In Greibach normal form, the right-hand side of every rule started with a terminal symbol, so the operation of the parser was to simply take the next word from the input, pair it with the symbol on the top of the stack, and look for matching rules to replace the stack symbol with a sequence of new symbols.",17,18
15423,12780794,"In Greibach normal form, the right-hand side of every rule started with a terminal symbol, so the operation of the parser was to simply take the next word from the input, pair it with the symbol on the top of the stack, and look for matching rules to replace the stack symbol with a sequence of new symbols.",40,41
15424,12780794,"In Greibach normal form, the right-hand side of every rule started with a terminal symbol, so the operation of the parser was to simply take the next word from the input, pair it with the symbol on the top of the stack, and look for matching rules to replace the stack symbol with a sequence of new symbols.",57,58
15425,12780794,"For example, you can replace any nonterminal symbol with the transition network that defines it, as long as the symbol doesn't itself occur in its own transition diagram.",8,9
15426,12780794,"For example, you can replace any nonterminal symbol with the transition network that defines it, as long as the symbol doesn't itself occur in its own transition diagram.",21,22
15427,15971472,In each transition the machine matches the symbol specified for the input tape and writes the one for the output.,7,8
15428,15971472,"The next symbol on the input tape is t. Since this is not a labial, no transition is possible from state 1, and that branch of the process therefore blocks.",2,3
15429,15971472,"When the following symbol is a labial, as in Figure 8 , the process blocks.",3,4
15430,15971472,Notice that the string iNput that would have been written on the intermediate tape before the machines were composed is blocked after the second symbol by constraints coming from the m-machine.,24,25
15431,15971472,"A path-string for any finite-state transducer T is a (possibly empty) sequence of symbol-pairs ul : Vl u2 : v2.. • un : vn that label the transitions of an accepting path in T. The path-language of T, notated as Paths(T), is simply the set of all path-strings for T. Paths(T) is obviously regular, since it is accepted by the finite-state machine constructed simply by interpreting the transition labels of T as elements of an alphabet of unanalyzable pair-symbols.",19,20
15432,15971472,"Also, if P is a finite-state machine that accepts a pair-symbol language, we define the path-relation ReI(P) to be the relation accepted by the fst constructed from P by reinterpreting every one of its pair-symbol labels as the corresponding symbol pair of a transducer label.",15,16
15433,15971472,"Also, if P is a finite-state machine that accepts a pair-symbol language, we define the path-relation ReI(P) to be the relation accepted by the fst constructed from P by reinterpreting every one of its pair-symbol labels as the corresponding symbol pair of a transducer label.",45,46
15434,15971472,"Also, if P is a finite-state machine that accepts a pair-symbol language, we define the path-relation ReI(P) to be the relation accepted by the fst constructed from P by reinterpreting every one of its pair-symbol labels as the corresponding symbol pair of a transducer label.",50,51
15435,15971472,"b/b If a string containing the symbol b is input to this rule, another b will be inserted immediately after it, and that one will serve to trigger the rule again.",8,9
15436,15971472,Consider again the left-to-right rule schema It is often convenient in phonological rules to introduce a special symbol to mark the beginning and end of the string.,21,22
15437,15971472,"For the moment, it can be taken as equivalent to a set of rules whose effect is to replace any segment that is classified as an obstruent by its unvoiced equivalent before the boundary symbol that marks the end of a word.",35,36
15438,15971472,"The boundary symbol # is special in the rule formalism in that it can only appear in the context parts of a rule, never in the input or output patterns, and it never matches an element that appears explicitly in the string.",2,3
15439,15971472,"The problem is that a given string symbol can simultaneously serve several different roles in the application of a rule, and all possible interactions must be accounted for.",7,8
15440,15971472,"We first consider how to apply a rule to strings that have been preprocessed so that every instance of the left context A is followed by the auxiliary symbol < and every instance of the right context p is preceded by the symbol >, where < and > are not in P~. This means that the replacement operator can be defined solely in terms of these distinct context-marking brackets, without regard to what A and p actually specify and what they might have in common with each other or with q~ and ~b.",28,29
15441,15971472,"We first consider how to apply a rule to strings that have been preprocessed so that every instance of the left context A is followed by the auxiliary symbol < and every instance of the right context p is preceded by the symbol >, where < and > are not in P~. This means that the replacement operator can be defined solely in terms of these distinct context-marking brackets, without regard to what A and p actually specify and what they might have in common with each other or with q~ and ~b.",42,43
15442,15971472,The auxiliary symbol 0 is not in ~. or in the set of context brackets.,2,3
15443,15971472,"It will substitute for the empty strings that might appear in the center of rules (in ~ or ~b), but it is a genuine symbol in an expanded alphabet which, unlike the normal e, actually appears as a distinct element in character strings.",27,28
15444,15971472,The Prologue relation is extended to freely introduce 0 as well as the brackets in m: Prologue = Intro(m U {0}) We then construct alternative versions of q~ and ~b in which this special symbol replaces the true empty strings.,38,39
15445,15971472,"It accepts strings that have at least one <, and every 0 or ~ symbol must be followed by a <.",15,16
15446,15971472,"This must map between input and output strings with context-brackets properly located, ensuring that any of the subrule rewrites are possible at each properly marked position but that the rewrite of the k th subrule occurs only between <k and >k. The complete set where the generic symbol <c now stands for {<1 ... c<k}, the set of all left-center brackets, and the generic > is assigned a corresponding meaning.",52,53
15447,15971472,"Since each instantiated rule is formed by applying the same substitution to each of the original rule components, the cross-component correlation of symbol choices is properly represented.",25,26
15448,15971472,"We construct a rewriting grammar that simulates the operation of T, deriving a string y from a string x if and only if the pair Ix, Y/is accepted by T. There will be four rules in the grammar that together implement the provisions that T starts in state q0, makes transitions from state to state only as allowed by 6, and accepts a string only if the state it reaches at the end of the string is in F. Let ~, U Q u {#,$} be the alphabet of the grammar, where # is a boundary symbol not in either G or Q and $ is another distinct symbol that will be used in representing the finality of a state.",108,109
15449,15971472,"We construct a rewriting grammar that simulates the operation of T, deriving a string y from a string x if and only if the pair Ix, Y/is accepted by T. There will be four rules in the grammar that together implement the provisions that T starts in state q0, makes transitions from state to state only as allowed by 6, and accepts a string only if the state it reaches at the end of the string is in F. Let ~, U Q u {#,$} be the alphabet of the grammar, where # is a boundary symbol not in either G or Q and $ is another distinct symbol that will be used in representing the finality of a state.",120,121
15450,15971472,"The first rule in the grammar sequence is the simple start rule: --* qo/# -- (obligatory, left-to-right) The effect of this rule is to introduce the start-state as a symbol only at the beginning of the input string, as specified in the rule by the boundary symbol #.",41,42
15451,15971472,"The first rule in the grammar sequence is the simple start rule: --* qo/# -- (obligatory, left-to-right) The effect of this rule is to introduce the start-state as a symbol only at the beginning of the input string, as specified in the rule by the boundary symbol #.",59,60
15452,15971472,This introduces a state-symbol that makes available at the next position only subrules corresponding to transitions at one of the start-state's successors.,5,6
15453,15971472,"If the last state is not in F, $ will not be inserted and the state will remain as the last symbol in the string.",22,23
15454,15971472,"We must formulate a rule that will ""bleed"" the derivation, producing no output at all if its input ends in a state-symbol instead of $.",26,27
15455,15971472,The individual transducers allowed in two-level specifications do permit the expansion and contraction of strings by virtue of a null symbol 0.,22,23
15456,15971472,"However, the effect of Koskenniemi's literal-matching transition function is achieved by treating 0 as an ordinary tape symbol, so that the individual transducers are C-free.",21,22
15457,15971472,The entire outer relation gives an algebraic model of Koskenniemi's operational method for combining individual transducers and for interpreting the null symbol.,22,23
15458,15971472,"The pairs in 7r contain all the alphabet symbols and 0, but do not contain e except possibly when it is paired with itself in e: ~. The relations corresponding to all the individual rules are all subsets of ~r*, and thus are all of the restricted same-length class (since ~r does not contain e paired with an alphabetic symbol).",66,67
15459,15971472,"For this class of relations, it makes sense to talk about a correspondence between a symbol in one string in a string-pair and a symbol in the other: in the pair (abc, lmn) for example, we can say that a corresponds to l, b to m, and c to n, by virtue of the positions they occupy relative to the start of their respective same-length strings.",16,17
15460,15971472,"For this class of relations, it makes sense to talk about a correspondence between a symbol in one string in a string-pair and a symbol in the other: in the pair (abc, lmn) for example, we can say that a corresponds to l, b to m, and c to n, by virtue of the positions they occupy relative to the start of their respective same-length strings.",27,28
15461,15971472,This problem can be solved with the auxiliary-symbol techniques we developed for rewriting rule overlaps.,9,10
15462,15971472,"Some grammars may make use of boundary-context rules, in which case a special symbol # can appear in contexts to mark the beginning and end of the strings.",16,17
15463,11629097,"We now substitute the constants c'i for the variables xi inside the theories of <i. With a slight abuse of notation, we will use the same symbol <i for the new ordering.",28,29
15464,11629097,"The symbol cr ---* T~,/{qt, ¢d',...} denotes a maximal consistent with {or, ~, k~',...} subtheory of r~ --* T¢, and in general T/T t will be a maximal consistent with T' subtheory of T. ""But"" is then an order to delete from background information everything contradicting • , but to use what remains.",1,2
15465,11629097,"In this case, there is only one: In other words, the yacht in question is a poor quality small and elegant ship serving as an inexpensive status symbol.",30,31
15466,14069948,"For the error rates of our classifiers, the † symbol indicates that the given classifier performs significantly worse when trained on a particular feature set than when trained on the full set.",10,11
15467,14069948,"11 The § symbol indicates that the difference between SVM and the given classifier, either C4.5 or Ripper, is significant.",3,4
15468,16419947,"Such constraints on the allowed syntax and the inferred semantics are often expressed in the form of a ""grammar ""l, a set of Throughout this document, by using the word ""grammar"", we refer to a Context-Free Grammar that consists of a finite set of non-terminals, a finite set of terminals, a unique non-terminal called the start symbol, and a set of production rules of the form A-> a, where A is a non-terminal and a is a string of terminal or non-terminal symbols.",71,72
15469,16419947,Developing tight grammars which trade-off these conflicting constraints is a tedious and accepted by a grammar is the set of all terminal strings that can be generated from the start symbol by successive application of the production rules.,32,33
15470,16419947,"For each non-terminal <N> in the parse tree, except the <<START>> symbol, a. Add a rule to define a new nonterminal <N'> such that <N'> generates all phrases that <N> generates except for the phrase in the counter-example that <N> generates.",20,21
15471,16419947,"Modify the rule that contains the <<START>> symbol in the parse tree, such that the <<START>> symbol no longer generates the given counterexample. ...............................................................................................................................",11,12
15472,16419947,"Modify the rule that contains the <<START>> symbol in the parse tree, such that the <<START>> symbol no longer generates the given counterexample. ...............................................................................................................................",25,26
15473,16356318,"They either use large and complex fragments of the corpus trees (the Data-Oriented Parsing approach), or they introduce specialized non-terminal labels which encode non-local information about the occurrence contexts of the nodes (the symbol-refinement approach). (",43,44
15474,16356318,N is a finite set of non-terminals; a function dim : N → N specifies the unique fan-out for every non-terminal symbol.,28,29
15475,16356318,S is the distinguished start symbol with dim(S) = 1.,5,6
15476,16356318,the start symbol covering the whole input in a single span.,2,3
15477,4247142,"In ATN grmrs written for parsing, a recurstve push does not change the input symbol being examined, but when the original level continues, parsing continues at a different symbol.",15,16
15478,4247142,"In ATN grmrs written for parsing, a recurstve push does not change the input symbol being examined, but when the original level continues, parsing continues at a different symbol.",31,32
15479,4247142,"~ne other major motivation is that, in parsing a string of symbols, the .,next.. symbol is well defined, but in ,.parsing.",17,18
15480,4247142,TERMINAL ACTIONS Successful traversal of an ATN arc might or might not consume an input symbol.,15,16
15481,4247142,"When parsing, such consumpticn normally occurs, when ge~erating it normally does not, but if it does, the next symbol (semantic node) must be specified.",24,25
15482,4247142,JUMP never conswms the input symbol; TO always does.,5,6
15483,4247142,"It the <forw~ is absent in tbe TO action, the nex~ symbol to be scanned will be the next one in the input buffer.",13,14
15484,4247142,"If <form is present, its value will be the next symbol to be scanned.",12,13
15485,4247142,The JUMP arc provides a place to make an arbitrary test and par'form sow actions without consuming an input symbol.,19,20
15486,4247142,"We need such an are that does conmmm its input s~bol, but TST is not adequate since it, ~ CAT, is really a bundle of ares, one for each lexloal entry of the scarmed symbol, should the letter be lexlcall7 ambiguous.",40,41
15487,4247142,The input s~ubol is con~. The next symbol to be scanned is the value OF <form> if it is present or the next symbol in the input buffer if ~fer~ is ~Losing.,9,10
15488,4247142,The input s~ubol is con~. The next symbol to be scanned is the value OF <form> if it is present or the next symbol in the input buffer if ~fer~ is ~Losing.,27,28
15489,4247142,"The PUSH arc mBk~8 two asnn~lo~ms 1 ) the first symbol to be scud in ths ~zheetvoz4c is the cmTent contents of the * registers 2) the cuzTent input symbol will be consuned~oy the subnet~ork, so the content8 of can be replaced by the value returned by the subnet-~ork.",14,15
15490,4247142,"The PUSH arc mBk~8 two asnn~lo~ms 1 ) the first symbol to be scud in ths ~zheetvoz4c is the cmTent contents of the * registers 2) the cuzTent input symbol will be consuned~oy the subnet~ork, so the content8 of can be replaced by the value returned by the subnet-~ork.",34,35
15491,1128981,"In LFGs, each rule scheme corresponds to a particular grammar symbol, since different expansions of the same symbol are expressed as alternatives in the regular expression on its RHS.",11,12
15492,1128981,"In LFGs, each rule scheme corresponds to a particular grammar symbol, since different expansions of the same symbol are expressed as alternatives in the regular expression on its RHS.",19,20
15493,1128981,"Since the grammar formalism requires that each LHS occur only in one rule scheme in the grammar, extracted rules with the same LHS symbol are merged into a single rule scheme with a disjunction operator at its top level.",24,25
15494,1128981,"For both languages, inhibiting pruning on the most variable symbol has the expected effect of increasing both parsing time and coverage.",10,11
15495,1128981,"Inhibiting pruning also on the second most variable symbol has ahnost no effect for French, and only a small effect for English.",8,9
15496,1128981,"Intuitively, nodes with low entropy indicate locations in the trees where a given symbol was expanded using a predictable set of rules, at least most of the times, so that the loss of coverage that derives from ignoring the remaining cases is low.",14,15
15497,1128981,"Nodes with high entropy, on the other hand, indicate positions in which there is a high uncertainty in what rule was used to expand the symbol, so that it is better to preserve all alternatives.",27,28
15498,12695499,"A pattern arc (PAT) has been added to the ATN formalism whose form is similar to that of other arcs: (PAT <pat apec> <test> <act> a <term>) The pattern specification (<pat spec>) is defined as: The pattern (<part>) is either the name of a pattern, a "">"", or a list of ATN arcs, each of which may be preceded by the symbol "">"", while the pattern mode (<mode>) can be any of the keywords, UNANCHOR, OPTIONAL, or SKIP.",88,89
15499,10338752,"In order to achieve this, we place the symbols INI, DNI and CNI immediately after the target word in every annotated sentence, and place the appropriate FE tag on the appropriate symbol.",34,35
15500,219307974,"Schoenfield dismisses the relationship between language and logic with a few terse comments like ""we introduce the symbol & to mean and"" (p. 10).",18,19
15501,6368353,"A concept symbol represents an entity that may be simple or complex, decomposable or not; the symbols  Presentation specifications are formal descriptions of the information that should be expressed in a particular reference, description, or predication.",2,3
15502,6368353,"In generating this sentence, the same concept symbol, say JLDR, would be used to generate both ;f the references.",8,9
15503,6368353,"If a positive response is given, a decomposition inquiry asks for a symbol to represent the available information, such as the location.",13,14
15504,2960672,"The string #1, where # is again a unique symbol, denotes the reading index of the entry (first reading).",11,12
15505,31915229,"There is only one edge to be considered and arc 2 does, in fact, have the same symbol as edge 1.",19,20
15506,8928636,"Each state has only one observation symbol (i.e., out- (1, 2) (1, 1) (2, 1) (2, 2) Φ Figure 4 Example of the absolute hidden Markov model.",6,7
15507,8928636,"The observation symbol set includes all the words in the document, and the observation symbol probabilities are defined as P(W i | P i ) = 1, if word W i is in position P i , and P(W i | P i ) = 0, if word W i is not in position P i .",2,3
15508,8928636,"The observation symbol set includes all the words in the document, and the observation symbol probabilities are defined as P(W i | P i ) = 1, if word W i is in position P i , and P(W i | P i ) = 0, if word W i is not in position P i .",15,16
15509,8928636,"This Markov model is hidden because one symbol sequence can correspond to many state sequences, meaning that many position sequences can correspond to a word sequence, as shown in Figure 1 .",7,8
15510,8928636,"Generally, in a hidden Markov model, one state sequence can also corrrespond to many symbol sequences.",16,17
15511,26325371,Basically the program is a nondeterministic device which applies unrestricted rewriting rules to a family of symbol strings and delivers as output all the strings that can be derived from members of the initial family by means of the rules provided.,16,17
15512,26325371,"Taking one category symbol for each word, it is possible to form 30 different strings, preserving the order of the original sentence.",3,4
15513,26325371,"The above rule will replace the symbol ""VPRSG"" by a string of three symbols ""PRES SG VERB"" whenever it occurs.",6,7
15514,26325371,"When the]eft-hand side of the rule is found to match a particular sub-string, ~ the number associated with a given symbol in the rule becomes a pointer to, or a temporary name for, that symbol.",25,26
15515,26325371,"When the]eft-hand side of the rule is found to match a particular sub-string, ~ the number associated with a given symbol in the rule becomes a pointer to, or a temporary name for, that symbol.",41,42
15516,26325371,"The numbers in parentheses after a symbol on the righthand side of a rule are pointers to items-identified by the left-hand side, and which the new symbol must dominate.",6,7
15517,26325371,"The numbers in parentheses after a symbol on the righthand side of a rule are pointers to items-identified by the left-hand side, and which the new symbol must dominate.",31,32
15518,26325371,"In the example, the symbol ""S"" is to dominate all the symbols mentioned on the left-hand side.",5,6
15519,26325371,"A pointer may refer to a single symbol, as we :have shown, or to a string of symbols.",7,8
15520,26325371,"Pointer number 2 is associated with the symbol ""NUM"" in the second place on the left-hand side, and occurs by itself in the fourth place.",7,8
15521,26325371,"This means that the fourth symbol matched by the rule must be ""NUM,"" and also that it must dominate exactly the same sub-tree as the second.",5,6
15522,26325371,"The use of the symbol ""NULL"" is illustrated in the rule: 4 PPH = NULL This will cause the symbol ""PPH"" to be deleted from any string in which occurs.",4,5
15523,26325371,"The use of the symbol ""NULL"" is illustrated in the rule: 4 PPH = NULL This will cause the symbol ""PPH"" to be deleted from any string in which occurs.",22,23
15524,26325371,"The program is nondeterministic in its treatment of rules of this kind, as elsewhere, so that it zwill consider analyses in which the symbol is deleted, as well as any which can be made by retaining it.",25,26
15525,26325371,"The symbol ""NULL"" is used only on the right-hand sides of rules.",1,2
15526,26325371,"The symbol ""ANY"" is used only on the left-hand sides of rules and has the property that the word implies, namely that it will match any symbol in a string.",1,2
15527,26325371,"The symbol ""ANY"" is used only on the left-hand sides of rules and has the property that the word implies, namely that it will match any symbol in a string.",31,32
15528,26325371,The use of this special symbol is illustrated in the following rule: VERB.I ANY.,5,6
15529,26325371,Elements on the left-hand sides of rules can be specified as optional by writing a dollar sign to the left or right of the symbol as in the following rules: DET.I ADJ$.I NOUN.,26,27
15530,26325371,"Elements can also be specified as repeatable by writing an asterisk against the symbol, as in the following example: VERB.",13,14
15531,26325371,Notice that the asterisk and the dollar sign can be placed before or after the symbol they refer to.,15,16
15532,26325371,"The combination is often useful with symbol ""ANY"" in rules of the following kinds N.I NUM.2 *$ANY.3 V.4 2 = NOUN(I 2) 3 VERB(4 2) This is similar to an earlier example.",6,7
15533,26325371,"The symbol ""ANY"" with an asterisk and a dollar s~g n cor L responds in this system to the so called variables in the familiar notation of transformational grammar. .,.?",1,2
15534,26325371,"Consider now the following rule: SCONJ.1 NP(S).I = NP(1) This will form a noun phrase from a subordinating conjunction followed bya nou~phrase, provided that this dominates only the ""~ymbol ""S."" Any symbol on the left-hand side of the rule may be followed by an expression in parentheses specifying the string of characters that this symbol must directly dominate.",39,40
15535,26325371,"Consider now the following rule: SCONJ.1 NP(S).I = NP(1) This will form a noun phrase from a subordinating conjunction followed bya nou~phrase, provided that this dominates only the ""~ymbol ""S."" Any symbol on the left-hand side of the rule may be followed by an expression in parentheses specifying the string of characters that this symbol must directly dominate.",64,65
15536,26325371,"This noun phrase is replaced by the determiner from the original noun phrase, if there is one, the elements preceding the noun except for the present participle, the noun itself, the symbol '~H,"" the symbol ""DEF~"" another Copy of the noun, the symbol f~E~"" the symbol ""ADJ"" dominating exactly those elements originally dominated by '~RPRT"" and, finally, any following prepositional phrases the original noun phrase may have contained.",35,36
15537,26325371,"This noun phrase is replaced by the determiner from the original noun phrase, if there is one, the elements preceding the noun except for the present participle, the noun itself, the symbol '~H,"" the symbol ""DEF~"" another Copy of the noun, the symbol f~E~"" the symbol ""ADJ"" dominating exactly those elements originally dominated by '~RPRT"" and, finally, any following prepositional phrases the original noun phrase may have contained.",41,42
15538,26325371,"This noun phrase is replaced by the determiner from the original noun phrase, if there is one, the elements preceding the noun except for the present participle, the noun itself, the symbol '~H,"" the symbol ""DEF~"" another Copy of the noun, the symbol f~E~"" the symbol ""ADJ"" dominating exactly those elements originally dominated by '~RPRT"" and, finally, any following prepositional phrases the original noun phrase may have contained.",52,53
15539,26325371,"This noun phrase is replaced by the determiner from the original noun phrase, if there is one, the elements preceding the noun except for the present participle, the noun itself, the symbol '~H,"" the symbol ""DEF~"" another Copy of the noun, the symbol f~E~"" the symbol ""ADJ"" dominating exactly those elements originally dominated by '~RPRT"" and, finally, any following prepositional phrases the original noun phrase may have contained.",58,59
15540,26325371,"The number ""2"" in double parentheses following ""ADJ"" on the right-hand side of this rule specifies that this symbol is to dominate, not the present participle itself, but the elements, if any, that it dominates.",24,25
15541,26325371,"Double parentheses can also be used following a symbol on the left-hand side of a rule, but with a different interpretation.",8,9
15542,26325371,We have seen how single parentheses are used to specify the strin~ in~nediately dominated by a given symbol.,19,20
15543,26325371,DouSle' parantheses enclose a string which must be a proper analysis of the sub-tree dominated by the given symbol.,21,22
15544,26325371,A string is said to be a proper analysis of a sub-tree if each terminal symbol of the.subtree is dominated by some member of the string.,17,18
15545,26325371,"As usual, a symbol is taken to dominate itself.",4,5
15546,26325371,"The initial article and the embeded sentence will be collected as a phrase under the symbol ""DET"" and the final noun will be left unchanged.",15,16
15547,26325371,"n 2 and nx, when present, are interpreted as fol-J lows: Egery symbol in the sub-string matched by the left-hand side of the rule must have been produced by a rule with number i, where ng) i~ nq.",17,18
15548,26325371,The main purpose of the examples that have been given is to show the great power of the program as a processor of symbol strings.,23,24
15549,26325371,"Let us suppose that, before this transformation is applied for the first time, all possible phrases that can be dominated by the symbol ""B"" are describable by context free phrase structure rules of the following form The phrase structure gramma~ needed to describe all the phrases that can exsist after the operation of this transformation must co~tain the following rules, or more accurately rule schemata 41 B~ .",24,25
15550,26325371,"=k Where the asterisk indicates one or more repetitions of the symbol ""A"".",12,13
15551,26325371,"The phrase dominated by the symbol ""B"" is reproduced in the output of this rule with copies of the symbol ""A"" removed from the righthand end and the remainder bounded by the symbols ""B+"" and'tFB "".",5,6
15552,26325371,"The phrase dominated by the symbol ""B"" is reproduced in the output of this rule with copies of the symbol ""A"" removed from the righthand end and the remainder bounded by the symbols ""B+"" and'tFB "".",21,22
15553,26325371,"However the first symbol on the left-hand side of this rule will match any string whatsoever, so that, if the rule can be applied ~t all, it can be applied in a prodigious number of ways.",3,4
15554,26325371,"In analysis we are therefore not only without information about how many times the rule may have been applied but we know nothing about where to insert new copies of the symbol ""A"", except that they must be to the right of the existing copy.",31,32
15555,8125512,A source SPG is assumed to be of'finite degree with ordered rules in which only the initial symbol is recursive.,17,18
15556,8125512,"We may, however, allow a dummy symbol, S', to appear in any rule rewriting some X as a string containing some Y, S' ~ Y ~ X, where S' is replaced after one application of the set of rules by # S #, the axiom, so that the rules then reapply in linear order.",8,9
15557,8125512,"t Thus Z--, [W] {X} conflates four rules, andwill, in any given application, rewrite Z as either WX or WY or Xor Y. Beginning with the first rule rewriting some X and proceeding in rule order, star occurrences of categories, excluding the dummy symbol S I, on the right in such a way that one and only one starred category Y% where Y ~ X, will occur in any application of the rule.",53,54
15558,8125512,"Establish a Table of Substitutes, assigning a unique symbol to every distinct XSn i for which S i ~ 0.",9,10
15559,181820,"The pattern (<patt>) is either a user-assigned pattern name, a "">"", or a list of ATN arcs, each of which may be preceded by the symbol "">"" to indicate a potentially optional arc, while the pattern mode (<mode>) can be any of the keywords, UNANCHOR, OPTIONAL, or SKIP.",36,37
15560,181820,"This is invoked by using the symbol "">"" in place of the pattern name.",6,7
15561,9972666,The mediator isolates the grammar from the symbol system of the environment.,7,8
15562,9972666,"These associations are an extension of the notion of a function symbol, since we can now ask of a function symbol what concept it represents and also what linguistic realization it has.",11,12
15563,9972666,"These associations are an extension of the notion of a function symbol, since we can now ask of a function symbol what concept it represents and also what linguistic realization it has.",21,22
15564,9972666,"The function symbols SPEAKER and HEARER are used in reasoning about pronouns, and the symbol SPEECHACT is used in reasoning about mood.",15,16
15565,9972666,"For example, AGENT would carry the same hub symbol for either ""Someone closed the door"" or ""The door was closed,"" but it would be inserted only in the second case.)",9,10
15566,9972666,Part of the specification of such a question is the function symbol with which the environment's response will be associated.,11,12
15567,9972666,That symbol must not have an existing association when the question is asked.,1,2
15568,34116601,"Here the plus symbol is the logical ""or"" operation.",3,4
15569,3002336,"The newly added tuples contained 60 tuples of the form Aegna, plaats∼lat dir, N (letter) , where the value should have been N (the symbol for latitude on the Northern hemisphere in geographical coordinates), and not the letter N. One page (Akira) was expanded with an incoherent set of tuples, based on tuples for the manga, anime, and music producer with the same name.",29,30
15570,7300113,Consider for instance the following GPSG rule (where H indicates the head of the rule): Example 22 s x 2 [-subj] The symbol S can be analyzed as the feature structure in 23.,26,27
15571,38268823,"He can test the co-ordinativedefinition in other ways too: for example, he can systematically vary the structural environment of a given structural constant for the purpose of recognizing its different meanings, since, as is well-known, natural language systems frequently use the same symbol to represent different entities; he can also vary the denotative environment to observe the effect of different kinds of denotative terms on the meaning of the structural constant.",51,52
15572,38268823,"A language, however, need not employ an explicit and independent symbol corresponding to the operation of negation, but may use implicit means to express negation.",12,13
15573,38268823,Those languages having an independent symbol possess greater structural flexibility.,5,6
15574,38268823,"The German language does not possess a special symbol to express a free variable restricted to two values; in translation, therefore, particularly from English to German, one must know the rules of replacement.",8,9
15575,38268823,"98026) An existential-quantifier restricted to two variables can easily be defined in the predicate calculus, but no special symbol corresponding to this concept occurs in English, hence ""either"" must be replaced by a phrase.",22,23
15576,38268823,"The sentence ""John wants neither book"" corresponds structurally to (27) because the special symbol ""neither"" is a contraction of two logical operators, a negation whose scope includes a restricted-existentialquantifiers ranging over two variables. """,17,18
15577,38268823,"In German, the word ""weder"" does not function as a special symbol representing a contraction of these two operators.",14,15
15578,203595196,"2017a,b) investigated the concept of regular transductions with origin semantics, where the origin semantics of a regular transducer A is a set of the origin graphs that A can create: if A produces a portion v of the output while reading the input symbol at position i, then each position of v is aligned to i. Since the domain of each regular transduction is a regular string language, it cannot capture nonregular syntactic phenomena on the source side of the translation.",48,49
15579,203595196,"An origin graph (over Σ 1 and Σ 2 ) is a triple (w, v, g) where w ∈ Σ * 1 , v ∈ Σ * 2 , and g (origin mapping) maps each position j of v to a position i of w. Intuitively, the pair (j, i) ∈ g indicates that the symbol at position j of v originated from position i of w. Let A be a set of origin graphs and L 1 and L 2 formal languages.",66,67
15580,203595196,"For a given such sequence, each symbol occurrence in v is obtained by application of a transition (q , a, α, q ), and this links the index of that symbol occurrence in v to the index of the corresponding occurrence of a in w. Thereby the sequence of transitions corresponds in a natural way to an origin graph.",7,8
15581,203595196,"For a given such sequence, each symbol occurrence in v is obtained by application of a transition (q , a, α, q ), and this links the index of that symbol occurrence in v to the index of the corresponding occurrence of a in w. Thereby the sequence of transitions corresponds in a natural way to an origin graph.",35,36
15582,203595196,A uni-lexicalized synchronous MCFG is a synchronous MCFG in which each rule either contains exactly one input symbol or contains neither input symbols nor output symbols.,19,20
15583,203595196,"To formalize this, we first define Φ α 1 (r) = r |(α 1 •αε)(r)| for r ∈ R. In words, each register r is mapped to |(α 1 • α ε )(r)| copies of its own name, to encode the size of its contents after reading w. Secondly, we introduce a new symbol †, and define Ψ to be the assignment such that Ψ(r) = r for r ∈ R and Ψ (c) = † for c ∈ Σ 2 .",61,62
15584,227231255,"This is due to the silence between the concatenated utterances which we solved by adding the symbol for silence ""-"" used already in the corpus between to concatenated utterances.",16,17
15585,7521453,We simply treat each lattice as a sequence whose ith symbol corresponds to the set of nodes at distance i from the start node.,10,11
15586,7521453,"We use a simple matching process that finds, for each argument value in the semantic expression, a sequence of lattice nodes such that each node contains a word identical to or a paraphrase of (according to the paraphrase thesaurus) a symbol in the argument value (these nodes are shaded in Figure 2 ).",44,45
15587,16877788,"Following the type, there are generally 1 or more words or other symbols, which we will separate with the symbol .",21,22
15588,34803084,"In addition, each occurrence of a terminal symbol in the string grammar is coupled to an occurrence of a terminal symbol in the tree grammar.",8,9
15589,34803084,"In addition, each occurrence of a terminal symbol in the string grammar is coupled to an occurrence of a terminal symbol in the tree grammar.",21,22
15590,34803084,A ranked set ∆ is a set of symbols associated with a rank function assigning a number rk ∆ (δ) ∈ N to each symbol δ ∈ ∆. A ranked alphabet is a ranked set with a finite number of symbols.,26,27
15591,34803084,"If theoretical frameworks require trees over ranked alphabets in the conventional sense (without s-terms), one may introduce a distinguished symbol cons of rank 2, replacing each s-term of length greater than 1 by an arrangement of subterms combined using occurrences of that symbol.",24,25
15592,34803084,"If theoretical frameworks require trees over ranked alphabets in the conventional sense (without s-terms), one may introduce a distinguished symbol cons of rank 2, replacing each s-term of length greater than 1 by an arrangement of subterms combined using occurrences of that symbol.",50,51
15593,34803084,Another symbol nil of rank 0 may be introduced to replace each s-term of length 0.,1,2
15594,34803084,"A macro grammar (MG) is a tuple G = (N, S, Γ, P), where N is a ranked alphabet of nonterminals, S ∈ N (0) is the start symbol, Γ = Γ (0) is a ranked alphabet of terminals and Γ ∩ N = ∅, and P is a finite set of rules, each of the form: A(x 1,k ) → r (1) where A ∈ N (k) and r ∈ T * N∪Γ (X k ).",39,40
15595,34803084,"A linear context-free rewriting system (LCFRS) is a tuple G = (N, S, Γ, P), where N is a ranked alphabet of nonterminals, S ∈ N (1) is the start symbol, Γ = Γ (0) is a ranked alphabet of terminals and Γ ∩ N = ∅, and P is a finite set of rules, each of the form: A 0 (s 1,k 0 ) → A 1 (x 1,m 1 ), A 2 (x m 1 +1,m 2 ), . . . ,",43,44
15596,34803084,"The number of inherited arguments is the i-rank and the number of synthesized arguments is the s-rank of A; we let rk N (A) = i-rk(A) + s-rk(A) denote the rank of A. The start symbol S has only one argument, which is synthesized-that is, rk N (S) = s-rk(S) = 1 and i-rk(S) = 0.",48,49
15597,34803084,"Example 14 Hybrid trees for cross-serial dependencies as in Figure 5 can be obtained through the following sMG/sCFTG hybrid grammar, with start symbol A: [A → S 1 (ε) , A → S 1 ] [S(x 1 ) → a 1 S 2 (x 1 b 3 ) , S → S(a 1 S 2 b 3 )] [S(x 1 ) → x 1 , S → ε] One can derive, for instance: [A 1 , A 1 ] ⇒ * [a 1 a 2 b 3 b 5 , S(a 1 S(a 2 b 5 ) b 3 )] Analogously to LCFRS/sDCP and sMG/sCFTG hybrid grammars, one can define LCFRS/sCFTG hybrid grammars and sMG/sDCP hybrid grammars, with suitable definitions of ⇒ G obtained straightforwardly by combining elements from the earlier definitions.",27,28
15598,34803084,"Hence, we alter line 9 in Algorithm 5 to: P ← P ∪ { J 0 ( s[s(p i ) 1 ] p i | p ) → } that is, we add an index to the symbol at position p i before selecting the relevant subtree s| p of s. For the indexing of nonterminals, we change line 16 to: P ← P ∪ { J 0 (s 1,k 0 ) → J 1 1 (x (1) 1,k 1 ), . . . ,",40,41
15599,34803084,"Because input strings consist only of POS tags, each terminal symbol in the LCFRS-part of a rule is projected to its first component.",11,12
15600,8265457,"Finite-state transducers are finite-state devices with transitions labeled by pairs of symbols (u:l), u denoting the ""upper"" symbol and l denoting the ""lower"" symbol.",28,29
15601,8265457,"Finite-state transducers are finite-state devices with transitions labeled by pairs of symbols (u:l), u denoting the ""upper"" symbol and l denoting the ""lower"" symbol.",36,37
15602,8265457,"Additionally, either u or l (but not both) can be the symbol, denoting the empty string.",14,15
15603,8265457,"8 How a certain segment of channel is used is indicated by various symbols surrounding the IGs, within the < and > delimiters: • The channel symbol 0 indicates that the channel segment is not used by any dependency link and thus is empty. •",28,29
15604,8265457,The channel symbol 1 indicates that the channel is used by a link that starts at some IG on the left and ends at some IG on the right.,2,3
15605,8265457,"When a link starts from a word-final IG, then a link start symbol is used on the right side of the word-final IG (i.e., between ) and >). •",15,16
15606,8265457,"When a link terminates on an IG, then a link end symbol denoting the syntactic relation is used on the left side of the IG (i.e., between < and ().",12,13
15607,8265457,Each rule has its own brace symbol depending on the relationship of the dependent and the head.,6,7
15608,8265457,"The regular expression LR = [""@"" "")"" ""0"" [""0""]* "">"" ] checks that a. The matching IG is a word-final IG (has a @ marker) b. The right-side topmost channel is empty (channel symbol nearest to ) is 0) c. The IG is not linked to any other in any of the lower channels d. No links in any of the lower channels cross into this segment (that is, there are no 1s in lower channels.)",54,55
15609,8265457,These conditions imply that the only channel symbol that may appear in the right side of a dependent IG is 0.,7,8
15610,8265457,"This transducer modifies the channel symbols to mark a link: • The new (topmost) right channel symbol in the IG just to the right of the opening brace is modified to a link start symbol (one of the symbols s, o, m, p, c, d, t, l, f, i). •",19,20
15611,8265457,"This transducer modifies the channel symbols to mark a link: • The new (topmost) right channel symbol in the IG just to the right of the opening brace is modified to a link start symbol (one of the symbols s, o, m, p, c, d, t, l, f, i). •",37,38
15612,8265457,"The new (topmost) left channel symbol in the IG just to the left of the closing brace is modified to a link end symbol (one of the symbols S, O, M, P, C, D, T, L, F, I).",7,8
15613,8265457,"The new (topmost) left channel symbol in the IG just to the left of the closing brace is modified to a link end symbol (one of the symbols S, O, M, P, C, D, T, L, F, I).",25,26
15614,8265457,For left-to-right braces it will insert the (lowercase) link start symbol to the right side of the left brace and the (uppercase) link end symbol to the left side of the right brace.,16,17
15615,8265457,For left-to-right braces it will insert the (lowercase) link start symbol to the right side of the left brace and the (uppercase) link end symbol to the left side of the right brace.,32,33
15616,8265457,"For right-to-left braces, it will insert the link start symbol to the left side of the right brace and the link end symbol to the right side of the left brace.",14,15
15617,8265457,"For right-to-left braces, it will insert the link start symbol to the left side of the right brace and the link end symbol to the right side of the left brace.",27,28
15618,8265457,"A set of simple regular expressions can recognize if a series of pairs of link start and link end symbols in one direction all appearing in the next-to-top channel (i.e., the second symbol to the left and right of ( and ), respectively, are surrounded by a link end-link start pair for the cycle-inducing link in the other direction) and kill any such configurations.",38,39
15619,8265457,"Since the outermost link symbol on the left side of a conjunction IG identifies the relation (because the left conjunct is immediately to the left of this IG), the link emanating from the conjunction to the right can be made to land on an IG that agrees with the left conjunct in relevant features.",4,5
15620,8265457,"Note that this transducer lets through a sequence of zero or more IGs, none of which have more than one D symbol (indicating an incoming determiner link) among the left channel symbols.",22,23
15621,15860242,"However, instead of simply concatenating the symbol of the right projection to build the output, the transducer uses the fight projection to build a single feature structure by using unification instead of concatenation.",7,8
15622,15860242,"From a generation point of view, the network is traversed based on the fight projection using feature unification instead of symbol equality, and concatenating the suing elements of the left projection to build the inflected word form.",21,22
15623,11999044,numl)erl' ~ 'nuntl)er5' are symbol words and show different senses Of 'llunlber'.,7,8
15624,44039111,"e symbol ""O|"" indicates the pipeline with the Omorfi morphological analyser produced the correct analysis; ""L|"" indicates that the correct analysis was produced by the pipeline with the list-based lexical analyser.",1,2
15625,9432295,"For each corpus version, words with a different analysis were marked with a ""RE-CONSIDER"" symbol.",19,20
15626,9432295,"The ""RECONSIDER"" symbol was also added to a number of other ambiguous words in the corpus.",4,5
15627,9432295,"The linguists were told that some of the words marked with the ""RECON-SIDER"" symbol were analysed differently by them.",17,18
15628,9432295,"To words with a different analysis, a ""NEGOTIATE"" symbol was added.",11,12
15629,1343668,"If, say, the right-context ofa MP is shorter than the left-context, an out-of-bounds symbol (OOB) is used to maintain the mixed-context format.",24,25
15630,7633822,"Thus, for instance, allomorphs that encode the same feature and map to different surfacy features, now map to the same feature symbol.",24,25
15631,7633822,The following information about the distinguished lemma in the paradigm is provided: 6 The alignment produces the following suffix feature 6Upper case characters and the single quote symbol encode specific Russian characters.,28,29
15632,7633822,"The segmented forms have the roots/lemmas and affixes, and the affix boundaries are marked by the + symbol.",20,21
15633,7633822,"we will ignore the escape symbol (Z) that should precede any special characters (e.g., +) used in these rules.",5,6
15634,7633822,"u -> 1 [] LeftContext _ RightContext ; where u(pper) is a symbol in the segmented form, l(ower) is a symbol in the surface form.",15,16
15635,7633822,"u -> 1 [] LeftContext _ RightContext ; where u(pper) is a symbol in the segmented form, l(ower) is a symbol in the surface form.",25,26
15636,7633822,Rules are generated only from those aligned symbol pairs which are different.,7,8
15637,7633822,"+-> 0 II ppy _ e s t # The # symbol denotes a word boundary, to capture any word initial and final phenomena.",13,14
15638,470934,"In other words, all rules can only have either 'S' or 'NP' as their left hand-side symbol.",22,23
15639,44338,"The edit distance between two strings measures the minimum number of unit editing operations of insertion, deletion, replacement of a symbol, and transposition of adjacent symbols (Damerau 1964 ) that are necessary to convert one string into another.",22,23
15640,44338,"Let Z = zl, z2 .... , Zp denote a generic string of p symbols from an alphabet A. Z~] denotes the initial substring of any string Z up to and including the jth symbol.",36,37
15641,44338,"They can be abstracted as finitestate transducers over an alphabet of lexical and surface symbol pairs 1 : s, where either [0] Eo/ a [0] A [0] a [1] a,, [1] b b b [U [2] [1l [1] ( 3 } [0] [1] [1] a [i] [2] ) 2lt l t21 Search graph for matching ababa with threshold 1 Figure 5 Recognizer for (aba + bab)* and search graph for ababa.",14,15
15642,44338,1 or s (but not both) may be the null symbol 0.,12,13
15643,44338,After a successful match with a surface symbol the corresponding lexical symbol is appended to the output gloss string.,7,8
15644,44338,After a successful match with a surface symbol the corresponding lexical symbol is appended to the output gloss string.,11,12
15645,44338,cal/surface symbol approach presented here.,3,4
15646,44338,"First, for morphological analysis, the concurrent generation of the lexical gloss string requires that occasional transitions with an empty surface symbol be taken to generate the gloss properly.",22,23
15647,2952366,"Thus we had to carry out conversions of the original data into the format presented above, which resulted in the so-called Czech ""modified"" corpus, with the following features: tokens ENGLISH TAGSET For the tagging of English texts, we used the Penn Treebank tagset which contains 36 POS tags and 12 other tags (for punctuation and the currency symbol).",66,67
15648,59861657,"Thus, the main task of the analysis module is to choose the correct symbol to represent the meaning of each surface expression.",14,15
15649,59861657,"Conversely, the task of the synthesis module is to choose a surface representation for each conceptual symbol.",17,18
15650,912349,A simple transduction grammar can be written by marking every terminal symbol for a particular output stream.,11,12
15651,912349,"the language L1 emitted on stream 1, while y is a symbol of the language L2 emitted on stream 2.",12,13
15652,912349,We call a matched terminal symbol pair such as x/y a couple.,5,6
15653,912349,The null symbol ¢ means that no output token is generated.,2,3
15654,912349,"Formally, an inversion transduction grammar, or ITG, is denoted by G = (N, W1,W2,T¢,S), where dV is a finite set of nonterminals, W1 is a finite set of words (terminals) of language 1, }4;2 is a finite set of words (terminals) of language 2, T¢ is a finite set of rewrite rules (productions), and S E A/"" is the start symbol.",79,80
15655,912349,"Let W1, W2 be the vocabulary sizes of the two languages, and X = {A1 ..... AN} be the set of nonterminals with indices 1,...,N. (For conciseness, we sometimes abuse the notation by writing an index when we mean the corresponding nonterminal symbol, as long as this introduces no confusion.)",50,51
15656,912349,"Instead, a generic bracketing transduction grammar is employed, containing only one nonterminal symbol, A, which rewrites either recursively as a pair of A's or as a single terminal-pair: A a [A A] A a (A A) A ""~ Ui/V j A ~ ui/¢ b~j A --, (/vj for all i,j English-Chinese lexical translations for all i English vocabulary for all j Chinese vocabulary Longer productions with rank > 2 are not needed; we show in the subsections below that this minimal transduction grammar in normal form is generatively equivalent to any reasonable bracketing transduction grammar.",14,15
15657,912349,"To be legal, a rotation must preserve symbol order on both output streams.",8,9
15658,11467716,"is the symbol for ayn, a strong glottal stop), where each of the successive consonants matches a character in the bare root (for ktb, k matches f, t matches ?",2,3
15659,17397688,"Specifically, all numbers in the text were replaced with a <number> symbol.",14,15
15660,17397688,Punctuation was separated from words and treated as a separate symbol.,10,11
15661,2220955,"In Tomita's LR parsing framework, each such rule must be manually converted into a rule of the following form in which some subpart of each category has been replaced by an atomic symbol.",34,35
15662,2220955,"Constructing the LR(0) sets of items: we compute LR(0) states containing only kernel items (the item IS' --> S], where S' is the start symbol, and all items that have a symbol to the left of the dot), since nonkernel items can be cached in a table and retrieved only if needed.",33,34
15663,2220955,"Constructing the LR(0) sets of items: we compute LR(0) states containing only kernel items (the item IS' --> S], where S' is the start symbol, and all items that have a symbol to the left of the dot), since nonkernel items can be cached in a table and retrieved only if needed.",41,42
15664,2220955,"Taking advantage of the characteristics of this distribution, in each state we represent (in Common Lisp) (a) a set of goto entries as a list of (nonterminal--state) conses sorted into a canonical order, list elements and tails of lists shared where possible between states, (b) a set of shift actions as a list containing a single (large) integer (the list shared when possible between states), where if the state shifts to state s on lookahead t, the element indexed by t in an auxiliary array will contain s together with a number n, and bit n in the binary representation of the integer will be 1, (c) a set of reduce actions as, for each rule involved, a cons whose second element is the rule number and whose first is a bit-vector (shared when possible between states) whose nth bit is 1 if the reduce should occur with the nth terminal as lookahead, (d) an accept action as a cons with the first element being the lookahead symbol.",199,200
15665,17430507,"Throughout, the symbol "":ANY"" indicates a ""don't care"" variable, unifying with anything.",3,4
15666,219306083,"COAL (John), T M ( h a t ) , and also the bLngla ,gapreselltation for both sentences, as below, using a  an & take + book Here 'pf itldicatea past, and is the aepndendy symbol liking a PP to We ACT ( ' t a k e ' ) which i s the hub of the conceptualization, as w i t h Simon& ?'",44,45
15667,8043801,The C symbol signifies the subsumption relationship.,2,3
15668,9794281,The E symbol signifies the subsumption relationship.,2,3
15669,14907272,"The procedures of resolution can be put in the form of a set of phrase-structure rules which produce a nesting of frames of formulae from an initial paragraph symbol P. The rules are given in their generative rather than their analytic form, but I give the ""lowest-level"" rules first, because they are the ones applied at the first stage 0f an~ysis.",30,31
15670,14907272,"The presentation will thus end up, rather than start, with highest level rules P÷..., where P is a ""paragraph symbol"" analogous to the sentence marker, S, in conventional gram~.ar.",24,25
15671,2104869,"Therefore, the output description part is realized through a function called BUILD-ITEM which receives as input the edge variables and a symbol denoting the class of the FST.",24,25
15672,9270830,Finite-state Language Processing Finite-state transducers (FST) are finite-state automata (FSA) where each transition consists of an input and an output symbol.,30,31
15673,9270830,The transition is traversed if its input symbol matches the current symbol in the input and generates the output symbol associated with the transition.,7,8
15674,9270830,The transition is traversed if its input symbol matches the current symbol in the input and generates the output symbol associated with the transition.,11,12
15675,9270830,The transition is traversed if its input symbol matches the current symbol in the input and generates the output symbol associated with the transition.,19,20
15676,9270830,The epsilon symbol is used to indicate when one of these is empty in a given terminal.,2,3
15677,9270830,S is the start symbol for the grammar.,4,5
15678,6976169,"Not only was the meaning of the flag symbol poorly understood, but also subjects did not realise that they could click on it.",8,9
15679,14593074,"If A is an atomic symbol then B A -~A can be checked directly for consistency; otherwise -~A has to be expanded out, yielding (B A -~al) V (B A -~a2) V.. . (",5,6
15680,14593074,"The expressions to be checked will consist either of simple conjunctions of unnegated atomic symbols, or, if we need to support subsumption checking, conjunctions of a single negated atomic symbol with a number of unnegated symbols.",32,33
15681,14593074,We shall illustrate these vectors using strings of symbol specifications.,8,9
15682,14593074,Within such a string there are three possible types of symbol specification: • the presence of a positive specification for a symbol is represented using a boldface capital letter (A1) • the presence of a negated specification for a symbol is represented with a boldface capital letter with a line above (~) • the absence of any specification for a symbol is represented using a lighter lower case font (al).,10,11
15683,14593074,Within such a string there are three possible types of symbol specification: • the presence of a positive specification for a symbol is represented using a boldface capital letter (A1) • the presence of a negated specification for a symbol is represented with a boldface capital letter with a line above (~) • the absence of any specification for a symbol is represented using a lighter lower case font (al).,22,23
15684,14593074,Within such a string there are three possible types of symbol specification: • the presence of a positive specification for a symbol is represented using a boldface capital letter (A1) • the presence of a negated specification for a symbol is represented with a boldface capital letter with a line above (~) • the absence of any specification for a symbol is represented using a lighter lower case font (al).,42,43
15685,14593074,Within such a string there are three possible types of symbol specification: • the presence of a positive specification for a symbol is represented using a boldface capital letter (A1) • the presence of a negated specification for a symbol is represented with a boldface capital letter with a line above (~) • the absence of any specification for a symbol is represented using a lighter lower case font (al).,65,66
15686,14593074,Explicit representation of absent symbols means that every symbol in the repertoire must appear (in some form) in every vector.,8,9
15687,14593074,"The elements of a symbol vector are implicitly conjoined, and there is no mechanism for expressing disjunction.",4,5
15688,1040,"Using terms of a Markov Model, a state is allowed to emit a context-free partial parse tree, starting with the represented non-terminal symbol, yielding part of the sequence of words.",28,29
15689,14039887,"Using types to model this taxonomic hierarchy, the type symbol VP denotes the set of verb phrases, the symbol PH denotes the set of phrases, and we define VP as a subtype of I'H. This description implies that, if we know that a linguistic object is a verb phrase, we can deduce that it is a phrase.",10,11
15690,14039887,"Using types to model this taxonomic hierarchy, the type symbol VP denotes the set of verb phrases, the symbol PH denotes the set of phrases, and we define VP as a subtype of I'H. This description implies that, if we know that a linguistic object is a verb phrase, we can deduce that it is a phrase.",20,21
15691,14039887,"Feature Structures We use the attribute-value matrix (AVM) notation for feature structures, and we write the type symbol for each feature structure in front of the opening square bracket of the AVM.",22,23
15692,14039887,"In the remainder of this section, we shall implicitly refer to some given signature (T, <, .T / where (T, ~, ) is a type hierarchy and ~"" is a set of feature symbols, and we shall also assume a set of variables V. A typed feature structure t is then an expression of the form tl] []A ... : tn where [] is a variable in a set of variables V, A is a type symbol in T, fl,..., fn (with n _> 0) are features in ~-, and h,.-.,",90,91
15693,14039887,A type symbol that does not have any definition is called an atomic type.,2,3
15694,15909369,"For the purpose of comparison, we may compare this model against a baseline model that partitions the set of all supertags into classes so that all of the supertags in one class share the same preterminal symbol, i.e., they are anchored by words which share the same part of speech.",37,38
15695,862713,"Pruning Ideally, every parse theory could be kept in the chart, and when the root symbol has been generated for all theories, the top-ranked one would ""win.""",17,18
15696,291199,"There are many possible formats for specifying a finite-state grammar, and the one used by the Nuance recognition system specifies a single definition for each atomic nonterminal symbol as a regular expression over vocabulary words and other nonterminals, such that there is no direct or indirect recursion in the set of definitions.",30,31
15697,10102985,5 The presence of the symbol √ indicates that the model's class preference for a given verb agrees with its distribution in the corpus.,5,6
15698,9703433,"Two-level morphology is based on three ideas: r Rules are symbol-to-symbol constraints that are applied in parallel, not sequentially like rewrite rules.",13,14
15699,9703433,"Two-level morphology is based on three ideas: r Rules are symbol-to-symbol constraints that are applied in parallel, not sequentially like rewrite rules.",17,18
15700,9703433,"In order to arrive at the point shown in Figure 2 , the analyzer has traversed a branch in the lexicon that contains the lexical string kaN. At this point, it only considers symbol pairs whose lexical side matches one of the outgoing arcs of the current state.",34,35
15701,9703433,"In two-level rules, zero (epsilon) is treated as an ordinary symbol.",15,16
15702,9703433,"In both formalisms, the most difficult case is a rule where the symbol that is replaced or constrained also appears in the context part of the rule.",13,14
15703,9703433,The idea of rules as constraints between a lexical symbol and its surface realization was seen as misguided.,9,10
15704,9703433,It went unnoticed that two-level rules could have the same effect as ordered rewrite rules because the realization of a lexical symbol could be constrained by the lexical side and/or by the surface side.,23,24
15705,1219991,"The part of speech tag of the head of the entire sentence, th, is computed conditioning only on the top-most symbol p:2 P(th I P)- (6) Part of speech tags of modifier constituents, tli and tri, are predicted conditioning on the modifier constituent li or r/, the tag of the head constituent, th, and the word of the head constituent, Wh P (tl, [li, th, Wh) and P(tr~ [ ri, th, Wh) . (",24,25
15706,1219991,"7) The head word of the entire sentence, Wh, is predicted conditioning only on the top-most symbol p and th: P(whlth,p). (",21,22
15707,10461112,"Each head transducer is a finite-state machine that differs from ""standard"" finite-state transducers in that, instead of consuming the input string left to right, it consumes it ""middle out"" from a symbol in the string.",41,42
15708,10461112,"Similarly, the output of a head transducer is built up middle out at positions relative to a symbol in the output string.",18,19
15709,10461112,"The interpretation of q, q', w, and v in transitions is similar to left-to-right transducers, i.e., in transitioning from state q to state qt, the transducer ""reads"" input symbol w and ""writes"" output symbol v, and as usual if w (or v) is e then no read (respectively write) takes place for the transition.",41,42
15710,10461112,"The interpretation of q, q', w, and v in transitions is similar to left-to-right transducers, i.e., in transitioning from state q to state qt, the transducer ""reads"" input symbol w and ""writes"" output symbol v, and as usual if w (or v) is e then no read (respectively write) takes place for the transition.",48,49
15711,10461112,"For a derivation to be valid, it must read each symbol in the input string exactly once.",11,12
15712,10461112,"Relationship to Standard FSTs The operation of a traditional left-to-right transducer can be simulated by a head transducer by starting at the leftmost input symbol and setting the positions of the first transition taken to a = 0 and fl = 0, and the positions for subsequent transitions to o~ = 1 and fl = 1.",28,29
15713,10461112,"Since the empty string may appear in a transition in place of a source or target symbol, the number of source and target dependents can be different.",16,17
15714,41415,computed conditioning only on the top-most symbol p:6 PL(Iz P(th I P). (,8,9
15715,41415,"7) The head word of the entire sentence, Wh, is predicted conditioning only on the top-most symbol p and th.",21,22
15716,41415,"9) The word feature of the head of the entire sentence, fh, is predicted conditioning on the topmost symbol p, its head word, wh, and its head tag, th: P(fh [Wh, th,p) . (",21,22
15717,6467318,"The roles of P , P U , ¥ , and in transitions are similar to the roles they have in left-to-right transducers, i.e. in transitioning from state P to state P p , the transducer 'reads' input symbol ¥ and 'writes' output symbol , and as usual if ¥ (or ) is then no read (respectively write) takes place for the transition.",45,46
15718,6467318,"The roles of P , P U , ¥ , and in transitions are similar to the roles they have in left-to-right transducers, i.e. in transitioning from state P to state P p , the transducer 'reads' input symbol ¥ and 'writes' output symbol , and as usual if ¥ (or ) is then no read (respectively write) takes place for the transition.",52,53
15719,6467318,"Since the empty string may appear in a transition in place of a source or target symbol, the number of source and target dependents can be different.)",16,17
15720,6467318,"In this probabilistic lexicon, may be , the empty symbol, so source words may have different probabilities of being deleted.",10,11
15721,1460,"The symbol, o. in Figure 1 denotes the composition operation.",1,2
15722,2444688,"1 represent states, the arcs (state transitions) are labelled by a pair of symbols: a lexical symbol and a surface symbol.",20,21
15723,2444688,"1 represent states, the arcs (state transitions) are labelled by a pair of symbols: a lexical symbol and a surface symbol.",24,25
15724,7580918,"The symbol "" ¶"" represents a paragraph break; ""objective-sentence"" and ""subjective-sentence"" represent objective and subjective sentences, respectively; ""sentence"" alone represents either a subjective or objective sentence; the symbol ""*"" means 0 or more occurrences, ""÷"" means 1 or more occurrences, and ""..."" represents any number of paragraph breaks and sentences (but not scene breaks, since only what has appeared since the start of the current scene is shown).",1,2
15725,7580918,"The symbol "" ¶"" represents a paragraph break; ""objective-sentence"" and ""subjective-sentence"" represent objective and subjective sentences, respectively; ""sentence"" alone represents either a subjective or objective sentence; the symbol ""*"" means 0 or more occurrences, ""÷"" means 1 or more occurrences, and ""..."" represents any number of paragraph breaks and sentences (but not scene breaks, since only what has appeared since the start of the current scene is shown).",42,43
15726,5180342,"Hiero is formally based on a weighted synchronous context-free grammar (CFG), containing synchronous rules of the form X → ē, f , φ k 1 ( f , ē, X) (2) where X is a symbol from the nonterminal alphabet, and ē and f can contain both words (terminals) and variables (nonterminals) that serve as placeholders for other phrases.",45,46
15727,5180342,"We use the following features in our induced English-to-English grammar: 3 • The joint probability of the two English hierarchical paraphrases, conditioned on the nonterminal symbol, as defined by this formula: p( ē1 , ē2 |x) = c(X → ē1 , ē2 ) ē1 , ē2 c(X → ē1 , ē2 ) = c(X → ē1 , ē2 ) c(X) (4) where the numerator is the fractional count of the rule under consideration and the denominator represents the marginal count over all the English hierarchical phrase pairs. •",31,32
15728,5389622,"The INSERT operations add a field with a symbol to the output level, more exactly: to the close neighborhood of the visited field.",8,9
15729,10360549,"The head is always able t0 read or write symbols to the item of the list on which it stands (the current item) and, in addition, it is able to read (but not %o write) the symbol On the item immediatelly to the left in the llst.",43,44
15730,10360549,"All pointer values of the new item are set to NIL, the information part of the item is copied from the current item WRITE(1)...i is a symbol fro~ the alphabet of the automaton; the value of Cat of the current Item Is set to i All oper~tlons are performed relatively to the current item (i.eo, ""x"" in their descriptlon means ""x of the current item"").",29,30
15731,44077352,"Classification Layer We make two assumptions of how texts affect the usage of emojis, and how emojis are chosen to express emotions as an auxiliary symbol for the text.",26,27
15732,15240200,"The acyclic weighted finite digraph has the properties; each arc is labeled with a word or a null symbol with a non-Sept. 1999 negative real number W being assigned , and at least one of the vertexes is defined to be a starting vertex, and an accepting vertex respectively.",19,20
15733,15240200,"In our language translation tutoring system, we compute the probabilities of grammars by the following formula giving scores for the tree structures chosen: F i denotes the frequency of the structure Struc i in the corpus, T i the Total frequency of the structures starting with the same symbol of Struc i .",51,52
15734,10351190,w n where w 0 is a special root symbol $ 0 .,9,10
15735,10351190,"In the example, information about the topmost stack element is attached to the corresponding vertex marked with a non-terminal symbol X. Weights are omitted in the example.",22,23
15736,10351190,We introduce the * symbol to indicate that the root node of the topmost element on the stack has not been scanned yet.,4,5
15737,10351190,"Table 13 shows the recall, precision and F-measure of grandchild structures whose grand parent is a sentence root symbol $.",21,22
15738,2488776,"Note that for each nonterminal symbol, the probabilities of its alternative expansions sum to 1.",5,6
15739,2488776,"Tbis captures the fact that in a context-free derivation, each instance of a nontermina] symbol must be rewritten according to exactly one of its expansions.",18,19
15740,2488776,"tile rule that generated this instance of nontermihal symbol A, and 2.",8,9
15741,2488776,"An auxiliary tree, by definition, has a uonterminal node on its frontier -tile fool node --that matches the nonterminal symbol at its root.",21,22
15742,2488776,The substitution operation corresponds to the rewriting of a symbol in a context-free derivation: a nonterminal node at the frontier of a tree is replaced with au initial tree having the same nonterminal symbol at its rout.,9,10
15743,2488776,The substitution operation corresponds to the rewriting of a symbol in a context-free derivation: a nonterminal node at the frontier of a tree is replaced with au initial tree having the same nonterminal symbol at its rout.,36,37
15744,2488776,"Adjunction may take place at any node r/ in labelled by a nonterminal symbol, so long as *7 s(~).",13,14
15745,1048006,"We use six character types: alphabet, numeral (Arabic and Chinese numerals), symbol, Kanji (Chinese character), Hiragana (Japanese script) and Katakana (Japanese script).",16,17
15746,5990443,"Introduction Chinese is a tonal syllabic and character (symbol) language, in which each character is pronounced as a tonal syllable.",9,10
15747,9816686,"All words with POS '記号' (symbol) such as ""、, 。, 「, 」""",8,9
15748,9299698,"If the translation equivalent of a symbol exist in the Japanese equivalent sentence, its weight is defined according to the number of words in edges, otherwise it is zero.",6,7
15749,7335081,"Unknown Words The speech recognizers of the VERBMOBIL system are able to recognize input as unknown to the system; if such a fragment is encountered the symbol UNK_ followed by the SAMBA-transcription (SAM, 1992) of the fragment (e.g. <UNK_maI62> for the unknown spoken input Maier) is inserted into the output of the recognizers.",27,28
15750,988839,"In the extreme, it can hypothesize a rule which directly derives the input string of words from the start symbol 's'.",20,21
15751,6876666,The difference between the notion of span here and [9] is that the latter requires that a span consists of al l the words that are dominated by a single non-terminal symbol .,35,36
15752,3065371,The symbol biases represent a kind of lexical probabilities for given word equivalence classes. •,1,2
15753,3236499,The symbol {++} indicates a morpheme boundary.,1,2
15754,10146013,"Thus, for instance, allomorphs that encode the same feature and map to different surfacy features now map to the same feature symbol. .",23,24
15755,10146013,"The function ed(v,w) (ed for edit distance), where v and w are strings, measures the minimum number of symbol insertions and deletions (but not substitutions) that can be applied to v to obtain w (Damerau 1964) .",25,26
15756,10146013,The segmented forms contain the citation forms and affixes; the affix boundaries are marked by the + symbol.,18,19
15757,10146013,"From each segmented pair we generate rewrite rules of the sort is u-> i [] LeftContext, RightContext ; where u(pper) is a symbol in the segmented form, l(ower) is a symbol in the surface form.",26,27
15758,10146013,"From each segmented pair we generate rewrite rules of the sort is u-> i [] LeftContext, RightContext ; where u(pper) is a symbol in the segmented form, l(ower) is a symbol in the surface form.",36,37
15759,10146013,Rules are generated only from those aligned symbol pairs that are different.,7,8
15760,10146013,"For the sake of readability, we will ignore the escape symbol (%) that should precede any special characters (e.g., ÷) used in these rules.",11,12
15761,10146013,y -> i y -> i y -> i + -> 0 + -> 0 + -> 0 + -> 0 p_ y- The # symbol denotes a word boundary and is intended to capture any word-initial and word-final phenomena.,33,34
15762,10146013,which would be resolved with the appropriate surface symbol by the rules learned.,8,9
15763,14844507,The symbol % denotes the glottal stop. (,1,2
15764,3519367,"2003) ), and each lemma shall get the special symbol @fn@ to set it apart from lemmas of the surrounding sentence.",11,12
15765,1174611,We compare the symbol distributions across di↵erent fields and calculate the jargon distance between fields.,3,4
15766,18977171,"A complex query might look like in the following example (with > denoting direct dominance, > * denoting general dominance, the dot denoting immediate precedence, and the # symbol introducing variables).",32,33
15767,61290341,"Symbols for new channels (upper channels in Figure 3 ) are stacked so that the symbols for the topmost channels are those closest to the (...).a The channel symbol 0 indicates that the channel segment is not used while 1 indicates that the channel is used by a link that starts at some IG on the left and ends at some IG on the right, that is, the link is just crossing over the IG.",32,33
15768,61290341,"If a link starts from an IG (ends on an IG), then a start (stop) symbol denoting the syntactic relation is used on the right (left) side of the IG.",20,21
15769,61290341,"A typical linking rule looks like the following: 5 For instance, LI~ is the pattern ""© .... ) .... 0"" [""0"" I 1]* "">"" which checks that (i) this is a word-final IG (has a ""©""), (ii) the right side ""topmost"" channel is empty (channel symbol nearest to "")""is ""0""), and (iii) the IG is not linked to any other in any of the lower channels (the only symbols on the right side are 0s and ls.)",70,71
15770,61290341,"This happens when the left side channel symbols of the IG immediately right of a open bracket contains the symbol 1 for one of the lower channels, indicating a link entering the region, or when the right side channel symbols of the IG immediately to the left of a close bracket contains the symbol 1 for one of the lower channels, indicating a link exiting the segment, i.e., either or both of the following patterns appear in the bracketed segment: A second configuration that may appear is the following: A rule may attempt to put a link in the topmost channel even though the corresponding segment is not utilized in a previous channel, e.g., the corresponding segment one of the previous channels may be all Os.",19,20
15771,61290341,"This happens when the left side channel symbols of the IG immediately right of a open bracket contains the symbol 1 for one of the lower channels, indicating a link entering the region, or when the right side channel symbols of the IG immediately to the left of a close bracket contains the symbol 1 for one of the lower channels, indicating a link exiting the segment, i.e., either or both of the following patterns appear in the bracketed segment: A second configuration that may appear is the following: A rule may attempt to put a link in the topmost channel even though the corresponding segment is not utilized in a previous channel, e.g., the corresponding segment one of the previous channels may be all Os.",55,56
15772,61290341,"RemoveTempBrackets; The transducer MarkChannels modifies the channel symbols in the bracketed segments to either the syntactic relation start or end symbol, or a 1, depending on the IG.",21,22
15773,61290341,"AtMost0neDet = [ ""<"" [ ~ [[$""D""]'I] & LeftCharmelSymbols* ] ""("" AnyIG (""@"") "")"" RightChannelSymbols* "">"" ]*; The FST for this regular expression makes sure that all configurations that are produced have at most one D symbol among the left channel symbols, n Many other syntactic constraints (e.g., only one object to a verb) can be formulated similar to above. ""(""",57,58
15774,988489,pp All strings starting with # are variables and the > symbol is the dominance operator.,11,12
15775,10220410,The bilanguage string consists of source-target symbol pair sequences as shown in Equation 3 .,8,9
15776,15057877,"If we formulate our search query as something like the following schematic regular expression:  The proposed noun phrases are given on indented lines, each marked with the symbol 'np:'.",30,31
15777,15057877,"The candidate noun phrases are then subjected to further routines: all candidate noun phrases with at least one occurrence in the output of both the NP-hostile and NP-friendly parsers are labelled with the symbol 'ok:', and the remaining candidates are labelled as uncertain, with the symbol '?:'.",38,39
15778,15057877,"The candidate noun phrases are then subjected to further routines: all candidate noun phrases with at least one occurrence in the output of both the NP-hostile and NP-friendly parsers are labelled with the symbol 'ok:', and the remaining candidates are labelled as uncertain, with the symbol '?:'.",55,56
15779,9987841,6The *head* symbol--used for modifiers--is a placeholder that points to the root (cause) of the overall lex-icaJ entry.,4,5
15780,18043676,The processing of an input sentence is divided into three phases: a) Positive projective This phase is in fact a standard parser --it checks if it is possible to represent a given input sentence by means of a projective syntactic tree not containing any negative symbol (these symbols represent the application of a grammar rule with relaxed constraints or an error anticipating rule).,47,48
15781,18043676,"In other words, each symbol used by RFODG belongs to exactly one set from each pair of sets under a), b) and c).",5,6
15782,18043676,"Whenever a metarule describing syntactic inconsistency is used during the parsing process, a negative symbol is inserted into the tree created according to the grammar.",15,16
15783,575386,"An ITG can always be written in a 2normal form and it is represented by a tuple ⟨N, V 0 , V 1 , R, S⟩ where N is a set of nonterminals, V 0 and V 1 are the bitokens of L 0 and L 1 respectively, R is a set of transduction rules and S ∈ N is the start symbol.",66,67
15784,690921,"If the source word is not aligned to any target word, a special symbol 'INF' is used to indicate such a case.",14,15
15785,690921,"In our model, this symbol is also a part of the target distribution.",5,6
15786,14091090,"Node Information All nodes have several kinds of semantic information, represented by a CP semantic symbol.",16,17
15787,17007944,"For example, in order to add support for a new kind of entity, for example, bars, a category bar is added to the ontology as a subtype of located entity along with specification of the head nouns used for this new category, the attributes that apply to it, the symbol to use for it in the gesture representation, and a reference to the appropriate table to find bars in the underlying application database.",55,56
15788,17007944,These symbol sequences can be concatenated in order to represent sequences of gestures and assembled into a lattice representation in order to represent a range of possible segmentations and interpretations of a sequence of ink strokes.,1,2
15789,17007944,Table 2 provides the full set of eight gestures supported and the symbol sequences used to represent them in the gesture lattice.,12,13
15790,17007944,"For symbolic gestures and selections, the gesture symbol complexes have the basic form: G FORM MEANING (NUMBER TYPE) SEM.",8,9
15791,17007944,The epsilon symbol ( ) is used to indicate when one of these is empty within a given terminal.,2,3
15792,17007944,"In addition to the gesture symbols (G area loc ...), G contains a symbol SEM used as a placeholder for specific content (see Section 3.3).",16,17
15793,17007944,The first step is to represent the gesture input as a transducer I:G where the input side contains gesture symbols and the specific content and the output side contains the same gesture symbols but a reserved symbol SEM appears in place of any specific gestural content such as lists of points or entity identifiers.,38,39
15794,17007944,"For each path through the meaning lattice we concatenate symbols from the output M side, unless the M symbol is SEM in which case we take the input I symbol for that arc.",19,20
15795,17007944,"For each path through the meaning lattice we concatenate symbols from the output M side, unless the M symbol is SEM in which case we take the input I symbol for that arc.",30,31
15796,17007944,"Essentially, the I:G transducer provides an index back from the gesture symbol sequence associated with each meaning in the meaning lattice to the specific content associated with each gesture.",14,15
15797,17007944,"  The meaning is generated by reading off and concatenating meaning symbols from the output of the I:M transducer, except for cases in which the output symbol is SEM, where instead the input symbol is taken.",29,30
15798,17007944,"  The meaning is generated by reading off and concatenating meaning symbols from the output of the I:M transducer, except for cases in which the output symbol is SEM, where instead the input symbol is taken.",37,38
15799,17007944,A spoken expression such as these three restaurants is aligned with the gesture symbol sequence G area sel 3 rest SEM in the multimodal grammar.,13,14
15800,17007944,Selecting the language symbol of each arc (projecting the FSM on the speech component) results in an FSM that can be used as the language model acceptor (G gram ) for speech recognition.,3,4
15801,9961130,"<) < 'r, ro~. vj su,: H that i < j < i+ k + m-1, where P(Xj I Xj-v,...Xj-+)is ,~+-th order Markov chain probability which denotes prol> ability of occurrence of sueeessiw+ character Xj when string Xj .... • • • Xj-t has occurred, mtd X,, denotes a space symbol if u < 0.",67,68
15802,2350772,"The o-symbol is a composition operator, and (R o Rs) basically means that the output of applying the rule R forms the input to the application of the rules Rs.",3,4
15803,8829260,"In the p-TBL system's rule formalism, conditions may refer to different symbol features, and complex conditions may be composed from simpler ones.",15,16
15804,8829260,"Rules that can be learned in TBL are instances of templates, such as ""replace tag A with B if tho symbol (e.g. the word) immediately to the left has tag C, where A, B and C are variables.",22,23
15805,6204420,We call the token containing the symbol which marks a putative sentence boundary the Candidate. ',6,7
15806,219309934,"On page 43 the authors state that in trigram tables the symbol ( 1 ) stands for a word space, in the tables them '-selves (pp.",11,12
15807,219309934,"Actually, there are no differences between first-grade speech and the ""adult"" speech samples in phoneme redundancy--the curves of relative sequential constraint across second-symbol positions (Figure 3 : 3 .",31,32
15808,13225085,The infix function symbol '/' is used in each category to separate tile syntactic from the semantic part.,3,4
15809,13225085,ZNote the different use of the symbol '/': here it denotes the category-valued feature 'slash'.,6,7
15810,13276834,We will use the equal-sign (=) to mark links which come from a bilingual dictionary and the star symbol (*) to mark comprehension correspondences.,21,22
15811,45066870,The set of parse rules effectively forms a phrase-structure (context free) grammar where each symbol has a list of typed features.,18,19
15812,45066870,This LHS symbol LHS assignment Rule title Unique ID RHS conditions listed in brackets RHS symbol Transfer (gen) rules LHS condition Gen RHS sumbol RHS assignment mechanism of overrides extends the expressive power of the grammar beyond that of pure context-free formalisms.,2,3
15813,45066870,This LHS symbol LHS assignment Rule title Unique ID RHS conditions listed in brackets RHS symbol Transfer (gen) rules LHS condition Gen RHS sumbol RHS assignment mechanism of overrides extends the expressive power of the grammar beyond that of pure context-free formalisms.,15,16
15814,45066870,"Beyond grammar modification, the developer also provides an interface through which detailed information can be queried about the last parse chart: rules that fired, edges that were created from a given symbol, instantiated kills, removed edges etc.",34,35
15815,12018700,"Every terminal and non-terminal symbol (or what is equivalent, the corresponding node in the syntactic tree under construction) has a well-defined set of features.",6,7
15816,12018700,Every symbol that is created and is not eliminated by an overriding pattern is retained even if it does not form part of a correct sentence's syntactic tree.,1,2
15817,1007835,We use the attribute-value matrix (AVM) notation for feature terms and we write the type symbol for each feature term in front of the opening square bracket of the AVM.,19,20
15818,1007835,A type symbol which does not have any feature defined for it is atomic.,2,3
15819,1007835,A type definition has the following form: the type symbol to be defined appears on the lefthand side of the equation.,10,11
15820,1007835,"The righthand side feature term may contain the lefthand side type symbol in a subterm (or in the condition), thus defining a recursive type equation which gives the system the expressive power needed to describe complex linguistic structures.",11,12
15821,9720898,"In addition, from the initial symbol S C of the structure, we obtain an identification result that shows that the example request sentence corresponds to Request C. Our retrieval method Our approach allows us to identify user requests with high accuracy, but lacks robustness when applied to a new request sentence.",6,7
15822,9720898,"Here, the request ID of the input sentence can be known from feature i of the initial symbol σ (∈V X , V X １ , …, V X N ) of the syntactic structure.",18,19
15823,1125140,"We find the most likely tags after seeing word i using the following where s is a traversal string, the symbol Bi-I indicates the set of tag-traversal string pairs that is being considered for word wi-l, and ~ indicates the ""forward probability"" according to the HMM.",21,22
15824,5212140,"In Section 2, the same symbol 'S' was used for the rnea.ning of 'seine' which is relatively defined ill the {A, M, S, F, N } (list (2)). ]",6,7
15825,1977923,"The special symbol ""*"" denotes the position that remains to be determined.",2,3
15826,1977923,"This argument also applies to the start symbol, as that symbol may be created depending the constituent hypothesized by error recovery rules and the fundamental rule.",7,8
15827,1977923,"This argument also applies to the start symbol, as that symbol may be created depending the constituent hypothesized by error recovery rules and the fundamental rule.",11,12
15828,1977923,"Second, so that the search process can be anchored by the start symbol, a data structure is created that can represent global plausibility.",13,14
15829,1977923,"The location operation proceeds by refining a need into more precise one, and it starts from the global need that refers to the start symbol, S, from 0 to n, where n is the length of the given input.",25,26
15830,7552909,"For example, if a general term is a company' s name, then its subsidiaries, its products, its stock symbol, its industrial code, etc.",23,24
15831,10178418,A view pattern consists of names of the attributes to consider with the symbol # representing the character data field of the element.,13,14
15832,690958,"The role is set (with the equality symbol ""="") in the case of ordinary complement arguments and restricted (with ""<"") in the ease of iterable temporal or locative adjunct moditiers (""in Harvard Square at Out of Town News next to the foreign magazine section"").",8,9
15833,56548665,"The f o l l a w i n g t r e e i l l u s t r a t e s t h e n o t a t i o n a l v a r i a t i o n s of (a) drawing i n t e r s e c t i n g t r e e s a s c r o s s -r e f e r e n c e d s e t s of normal, t r e e s , (b) drawing t h e j u n c t i o n symbol between t h e b r o t h e r s (subo r d i n a t e nodes) i n s t e a d of n e x t t o t h e f a t h e r ( s u p e r o r d i n a t e ) node, and (c) o p t i o n a l l y l e a v i n g o u t t h e j u n c t i o n symbol when i t i s a d j u n c t i o n (+) .",115,116
15834,56548665,"The f o l l a w i n g t r e e i l l u s t r a t e s t h e n o t a t i o n a l v a r i a t i o n s of (a) drawing i n t e r s e c t i n g t r e e s a s c r o s s -r e f e r e n c e d s e t s of normal, t r e e s , (b) drawing t h e j u n c t i o n symbol between t h e b r o t h e r s (subo r d i n a t e nodes) i n s t e a d of n e x t t o t h e f a t h e r ( s u p e r o r d i n a t e ) node, and (c) o p t i o n a l l y l e a v i n g o u t t h e j u n c t i o n symbol when i t i s a d j u n c t i o n (+) .",214,215
15835,13342424,"We have also experimented with the following initial grmnmar which defines a large number of rules (I 10640): X i ~ XjX k X i ~ t i In this grammar, each non-terminal symbol is uniquely ,associated with a terminal symbol.",38,39
15836,13342424,"We have also experimented with the following initial grmnmar which defines a large number of rules (I 10640): X i ~ XjX k X i ~ t i In this grammar, each non-terminal symbol is uniquely ,associated with a terminal symbol.",46,47
15837,13342424,"In the case no rule in the gnunmar introduces a terminal symbol found in the input string, we assigned a lexical rule (X i ~ tin) with very low • probability for all non-terminal symbols.",11,12
15838,13342424,"When the sentence is not recognized from the starting symbol, we considered ,all possible non-terminal symbols as starting symbols ,and considered as starting symbol the one that yields the most likely ,'malysis.",9,10
15839,13342424,"When the sentence is not recognized from the starting symbol, we considered ,all possible non-terminal symbols as starting symbols ,and considered as starting symbol the one that yields the most likely ,'malysis.",28,29
15840,13342424,"However, the same performance is achieved on these test sets by using only 450 rules (the top 20 binary branching rules X i ~ XjXk for each non-terminal symbol ,and the top 10 lexical rules X i ~ I m for each non-terminal symbol), Pereira and Schabes (1992) note that the training ,algorithm behaves in linear time (with respect to the sentence length) when the training material consists of fully bracketed sentences.",32,33
15841,13342424,"However, the same performance is achieved on these test sets by using only 450 rules (the top 20 binary branching rules X i ~ XjXk for each non-terminal symbol ,and the top 10 lexical rules X i ~ I m for each non-terminal symbol), Pereira and Schabes (1992) note that the training ,algorithm behaves in linear time (with respect to the sentence length) when the training material consists of fully bracketed sentences.",50,51
15842,57774,"In addition to rewriting non-terminals, the rules of the grammar can have the effect of pushing or popping symbols on top of the stacks that are associated with each non-terminal symbol.",35,36
15843,57774,LIGs [15] restrict The productions of a LIG are restricted to copy the stack corresponding to the non-terminal being rewritten to at most one stack associated with a non-terminal symbol on the right hand side of the production.,35,36
15844,57774,"A stochastic linear indexed grammar, G, is denoted by (VN, VT, Vt, S, Prod), where VN is a finite set of nonterminal symbols; VT is a finite set of terminal symbols; Vi is a finite set of stack symbols; S E VN is the start symbol; Prod is a finite set of productions of the form: x0[$p0] a Xo[..po] Xl[..pd x0[..p0] xl[$pl] z0[$p0] Xl[$pl] x2[$p2] where Xk E VN, a G VT and Po E Vi, Pl,P2 G V~; 2 LIGs have been shown to be weakly equivalent to Tree-Adjoining Grammars [16] .",57,58
15845,57774,"The non-terminal symbols of Gstia are the two symbols 'top' (t) and 'bottom' (b), the set of terminal symbols is the same as the one of Gtag, the set of stack symbols is the set of nodes (not node labels) found in the elementary trees of Gta9 augmented with the bottom of the stack ($), and the start symbol is 'top' (t).",75,76
15846,57774,"If 77 is a node labeled by a non-terminal symbol and if it does not have an obligatory adjoining constraint, then we need to consider the case that adjunction might not take place.",11,12
15847,12997979,"The nonterminal names in the logical form grammar are mnemonic for Formula, Relation (or function) symbol, Term, and Quantifier.",18,19
15848,11408822,Punctuation: If the post contains a question mark symbol ('?').,9,10
15849,5429505,"Linear Indexed grammar (LIG) (Alto, 1968; Gazdar, 1985 ) is a rewriting system in which the non-terminal symbols are augmented with a stack, in addition to rewriting non-terminals, the rules of the grammar can have the effect of pushing or popping symbols on top of tile stacks that are associated with each non-terminal symbol.",67,68
15850,5429505,The productions of a LIG are restricted to copy the stack corresponding to tile non-terminal being rewritten to at most one stack associated with a non-terminal symbol on tile right hand side of the production?,30,31
15851,5429505,"A stochastic linear indexed grammar, G, is denoted by (VN, VT, VI, S, Prod), where VN is a finite set of nonterminal symbols; VT is a finite set of terminal symbols; VI is a finite set of stack symbols; S E VN is the start symbol; Prod is a finite set of productions of the form: Xo[$po] --* a Xo[..po] --.",57,58
15852,5429505,"The non-terminal symbols of Gstia are the two symbols 'top' (t) and 'bottom' (b), tile set of terminal symbols is the same as the one of Gta9, the set of stack symbols is the set of nodes (not node labels) found in the elementary trees of Gla~ augmented with the bottom of tile stack ($), and tile start symbol is 'top' (t).",75,76
15853,5429505,"is a node labeled by a non-terminal symbol and if it does not have an obligatory adjoining constraint, then we need to consider the case that adjunetion might not take place.",9,10
15854,5429505," In the above grammar, a node S'k in a tree c~ a or/~ associated with the symbol a is referred as t/~, and a node S~ in a tree associated with b as r/~. We also conducted a similar experiment with the inside-outside algorithm for context-free grammar (Baker, 1979) , starting with all pc~sible Chomsky Normal Form rules over 4 non-terminals and the set of terminal symbols {a,b} (72 rules).",18,19
15855,8332406,"These rankings are indicated by the symbol "">"".",6,7
15856,8854836,"Trie is an indexing tree structure in which each node represents a symbol (in our case, words) and each link represents the sequence of the symbols (in our case n-gram).",12,13
15857,14018427,$ is the general wildcard: it matches any single word or other symbol.,13,14
15858,2124752,"Character type information Character type, like Kanji, Hiragana, Katakana, alphabet, number or symbol, etc.",17,18
15859,9001557,"Unlike other tagging systems (e.g., Flickr 1 ), hashtags in Twitter are used more as a symbol of community membership than for organizing a message content, as remarked in Huang, Thornton, and Efthimiadis (2010) .",19,20
15860,7766722,"Today's generators confront the problem regularly, as for example when a knowledge-based system passes just the symbol 'red-porsche' to the generator and its designer wants the phrase ""the red porsche,"" or ""that car,"" or ""the red one"" produced as is contextually appropriate.",20,21
15861,34812294,"If not, it rejects x as not acceptable to T. If there is a string, T' consults the table of T and determines by random choice a symbol s defined for T that drives T to a state for which there is a string leading to a final state.",30,31
15862,34812294,"Then the undefined symbol u is taken to be s, and the computation of the string x continues.",3,4
15863,59916558,"Combining Symbolic and Statistical Approaches to Language Our research is particularly relevant to this workshop, as it is clear we will have to combine rulebased symbol-mapping knowledge with statistical disambiguation models.",26,27
15864,59916558,We envisage that the bulk of a mapping can be done using simple one-to-one symbol replacement rules; but some rules will map one source symbol onto a set of more than one target symbol.,18,19
15865,59916558,We envisage that the bulk of a mapping can be done using simple one-to-one symbol replacement rules; but some rules will map one source symbol onto a set of more than one target symbol.,29,30
15866,59916558,We envisage that the bulk of a mapping can be done using simple one-to-one symbol replacement rules; but some rules will map one source symbol onto a set of more than one target symbol.,38,39
15867,8663865,2004) assigns a node to each word and punctuation symbol in the sentence.,10,11
15868,219306908,"As mntioned before, we append t o case frama in eha w r b d i c t i ~n a v d e s c r i p t i o n s of any ~b j e c t s which m y be TQMSU (dieeolve) h a the ease frame: ((ACT huaun) ( 0 B J material) (IN liguld) ) and t h i s case fratne has the a d d x t i s a a l description: 'aolutksn ('solvent (X LN)) The symbol $ in this description is a LISP function which fills the (a) I n p u t senteaca: d i a e e result of the a n a l y s i s of t h e second s e n t e n c e * f i n a l result abtained a f t e r searching F i g u r e 6 .",105,106
15869,62082058,"M A R T I N Massachusetts Institute of Technology In OWL, an implementation of conceptual gramar, the two types of data items are sy~nbols and concepts and the two basic data composition operations are specialization and restriction A symbol is an alphanumeric string headed by "" Symbols correspond to words, suffixes, prefixes, and Qord stems in English and the programmer can introduce them at will.",43,44
15870,62082058,"They are constructed using the specialization operation, comparable to CONS in LISP* (A B) is the special~zation of A, a concept, by B, a eoncept or symbol OWL forms a branching tree w d e r specialization, with SOMETHING at the top.",35,36
15871,18013136,"However, it is not clear how well the directed methods are applicable to grammars which do not depend on concatenation and have no unique 'left cornet"" which should be connected to the start symbol.",36,37
15872,14584513,The post-generator is usually dormant (just copies the input to the output) until a special alarm symbol contained in some targetlanguage surface forms wakes it up to perform a particular string transformation if necessary; then it goes back to sleep.,20,21
15873,219308390,A 16:l compaction r a t i o was achieved by s t o r i n g only the f i r s t instance of each p a t t e r n c l a s s and t h e r e a f t e r s u b s t i t u t i n g this exemp l a r for every subsequent occurrence of t h e symbol.,78,79
15874,1379432,"When the user presses the vowel key, the appropriate symbol is displayed based on the context.",10,11
15875,236145019,"An example of elements that had to be isolated in input processing were bracketed items called ""number/symbol strings.""",19,20
15876,237558710,"Searle replies that this implies that understanding is solely a matter of formal symbol manipulation, which is one of the tenets of objectivism.",13,14
15877,237558710,Searle counters the symbol-manipulation theory by noting that an essential element of understanding is conscious intentionality.,3,4
15878,812442,"A symbol preceded by a ""$"" represents an ATN Variable and ""*"" will again stand for ~he current constituenE. Thus in the state S in the grammar: (S (PUSH NP (UNIFY ""($SUBJVAR gYP $S) *) (TO S/SUBJ))) the parser pushes to the state NP co parse a noun phrase.",1,2
15879,186219479,Such formalisms can be thought of by analogy to context-free grammars as generalizing the notion of nonterminal symbol from a finite domain of atomic elements to a possibly infinite domain of directed graph structures of a certain sort.,19,20
15880,186219479,"By isolating from this survey those characteristics that were found to be most neuropsychologically adequate, this investigation advanced a definition of ""knowing"", and a description of the cognitive symbol that were consonant with the most recent evidence from neuropsychology.",32,33
15881,186219479,"By articulating the characteristics that were common to both thought and language, this investigation advanced a perspective from which the symbol itself could be viewed as inherently epistemic.",21,22
15882,186219479,"Of the theories examined, most were found to advance claims about the nature of symbolic interaction that were at variance with the characteristics they imputed to the symbol itself.",28,29
15883,186219479,"Only one theory developed a position that was based on an interactional perspective toward the symbol, and only that one made a viable claim to knowing.",15,16
15884,186219479,"This investigation concluded that neither a theory of language, nor a. theory of meaning has as yet been developed that can adequately address the interactional characteristics of the symbol itself.",29,30
15885,219302371,"and, if it is, finding all derivations of x from the starting symbol of G. We hope that posing t h e parsing problem i n purely algebraic terms will provide a basis for examination and comparison of parsfng algorithms and grammar classes.",14,15
15886,219302371,"I f the r e s u l t i n g p o l y n o m i a l c o n t a i n s a t e r m ( S , A ) w h e r e S i s t h e s t a r t i n g symbol i n A precise formulation of the polynomial and t h e operations on it is given b e l a w .",59,60
15887,219302371,"The N symbol S is the distinguished nonterminal from which all derivations b e g i n , and P 2 s the set of productions of G. A context-free grammer is proper if it does not contain productions of thz form A -+ c (erasures) or A B where A and E are both nonterminals.",2,3
15888,4758185,"A second reading is the following: 5 (b't) Max believes thaq(to(Socrates is mortal and Bruce Springsteen is The Boss), Wor, to(Socrates is mortal and it is not the case that Bruce Springsteen is The Boss)) Here we have a that, thatl, that is a symbol for a ternary function that takes as arguments a proposition, a binary function on propositions, and another proposition and yields the ordered triple whose first element is the binary function and whose next two are the two propositions: (too,-, to(Socrates is mortal and Bruce Springsteen is The Boss), to(Socrates is mortal and it is not the case that Bruce Springsteen is The Boss)) One more case should (more than) suffice: (b'2) Max believes that2 ((to(Socrates is mortal), to,,nd, to(Bruce Springsteen is The Boss)), toot, to(Socrates is mortal and it is not the case that Bruce Springsteen is The Boss)).",54,55
15889,4758185,that 2 is a symbol for a ternary function that takes the following as arguments.,4,5
15890,4758185,"7 Different thats, applied to one and the same sentence, yield names of different n-tuples; these tuples consist of an n-ary function and its arguments, some of which might 5 to is a symbol for the intension of the associated expression.",42,43
15891,14478849,"We show that any categorial language is a permutation of some context-free language, thus inheriting properties dependent on symbol counting only.",21,22
15892,6660159,An omitted preposition is marked in the analysis by the symbol prep and the semantic checking routine has been modified to accept prep in place of a particular preposition (but not to match positional markers such as subject or object).,10,11
15893,7589965,"A complex term is a variable followed by a ""such that"" symbol ""[ "", followed by a conjunction of one or more propositions?",13,14
15894,219310277,"T h i s can be accomplished i n a 3 -s t e p p r o c e s s : f i r s t , * r e p l a c e r u l e s o f t h e form u + abv ( a , b e V , u , v d ) by r u l e s u + a a , a -+ bv where a i s a new symbol; r e p e a t t h i s until a l l r u l e s have r h s .",83,84
15895,219310277,"But i n t h a t c a s e w E & ( L ( G ) , R ) i 1 because by d e f i n i t i o n x = o > ~ y i m p l i e To c o n s t r u c t G O , remember t h a t by d e f i n i t i o n o f R t h e r e i s an a n t i -s y m m e t r i c r e l a t i o n < on V R' Using t h i s , we a s s i g n t o  have on t h e r h s : a t e r m i n a l symbol of index s t r i c t l y g r e a t e r t h a n i .",148,149
15896,8993046,"If X is a non-terminal symbol of our grammar, we may generate several separate edges in the chart representing X spanning words wl to w2, each representing a different analysis.",7,8
15897,11582782,"Thus, by specifying in the search request ""computer"" as a word with its synonyms, ""center"" as a stem to allow for plurals, a distance delimiter to allow three or four intervening words, a disjunction of ""in"" and ""at"", and a capitalizatlon symbol we hoped to identify some (but certainly not a11) of the computer centers ~entloned in our date base that were identified with certain Instltutlons or locations.",54,55
15898,507186,Natural language is an organic symbol system; it does not submit to cast-iron control.,5,6
15899,219305200,F i r s t to appear i n a TYPE node is an a t t r i b u t e giving t h e t e x t r e p r e s e n t a t i o n f o r t h e symbol.,51,52
15900,219305200,This t e x t is repeated h e r e s i n c e t h e symbol given ilP t h e WQRDLIST i s i n 6 c h a r a c t e r chunks l i n k e d t o g e t h e r -a form not s u i t a b l e f o r p r i n t i n g .,19,20
15901,219305200,The second a t t r i b u t e always p r e s e n t is t h e DEFN l i n k used t o chain together a l l d e f i n i t i o n s of t h e symbol.,51,52
15902,14038085,"With the first-order representation for John's salary given above, as a first guess we might imagine that RISE({z I 3x3y emp(John,x,y,z)}) would represent this new query, where RISE is a predicate symbol.",45,46
15903,14038085,"In particular they constitute a definition of the function P: P :/QE-III\ --> M U {ERROR}, which assigns to any derivation tree of a meaningful expression a of QE-III, either an object in the model M or the distinguished symbol ""ERROR"" as its pragmatic interpretation.",50,51
15904,14038085,"Variable symbols Type of variable symbol Before illustrating some of the added features of the QE-III database query fragment, we present a simple example within the syntactic range of the PTQ fragment (up to vocabulary differences) in order to contrast the way these two fragments derive and translate it.",5,6
15905,219300194,"For applications in which the tree is searched for existing items as well as having new items added to it (e,g. in the construction of a symbol table), the tree can be made to exhibit stacklike behaviour so that the fewest comparisons are required to locate the most recently used i terns.",29,30
15906,219303858,"It is given a grammar a s a s e t of context-free rules with various extensions, most i m p ~r t a n t of which a r e t h a t LISP functions may be used as predicates instead of terminals, and thay each rule may be followed by operations t h a t are defined i n tbnns of t h e s y n t a c t i c elements a f t h e r u l e i n question, *for pe&~ogic polyglot A n example of t h i s notation i s as follows: S -+ NP VP => [AGREE [REF NP] [VB VP] ] [ S U M = [REF NP] ] [OW = [REF VP] [VB = [VB VP] ] S -* NP [BE] [VPASS] BY NP => [AGREE [REF NP] [VB [BE] ]I [SUM = [REF NP I ] ] [OBJ = REF NP] ] [VB = [VB [VPASS] ] ] NP -+ [DET] [N] => [REF = [N]] W + [VINP => [VB = [V]] [REF = [REF NP]] Here, each bracketed symbol i s the name of a recognition predicate ( e .g.,",249,250
15907,219303858,"S p e c i f i c a l l y , when t h e parser encounters a non-terminal symbol, t h a t symbol i s t h e left-hand s i d e of various r u l e s .",23,24
15908,219303858,"S p e c i f i c a l l y , when t h e parser encounters a non-terminal symbol, t h a t symbol i s t h e left-hand s i d e of various r u l e s .",29,30
15909,219303858,"I n PEDAGLOT, t h e choice of which r u l e t o t r y can be defined a s t h e r e s u l t of the c a l l t o a 'choosef function (or it can be l e f t uncontrolled] , W e have des&ned various approaches t o such p r e d i c t i o n s (e.g., a l i m i t e d key-word scan of the incoming s t r i n g , and the use of 'language s t a t i s t i c s such as t h e s e t of rules which can generate the next symbol i n t h e s t r i n g as t h e i r l e f t most symbol).",133,134
15910,219303858,"I n PEDAGLOT, t h e choice of which r u l e t o t r y can be defined a s t h e r e s u l t of the c a l l t o a 'choosef function (or it can be l e f t uncontrolled] , W e have des&ned various approaches t o such p r e d i c t i o n s (e.g., a l i m i t e d key-word scan of the incoming s t r i n g , and the use of 'language s t a t i s t i c s such as t h e s e t of rules which can generate the next symbol i n t h e s t r i n g as t h e i r l e f t most symbol).",156,157
15911,219303858,"Ambiguity i s discovered when two parses from the same symbol, cbvering t h e same s t r i n g segment axe found.",10,11
15912,3992521,"In this paper, we applied an information retrieval (IR) based method for human gene symbol disambiguation and studied different methods to combine various types of information from available knowledge sources.",17,18
15913,3992521,"2004) , only 30% of gene symbols in abstracts and 18% in full text were accompanied by their corresponding full names, which makes the task of gene symbol normalization much harder.",31,32
15914,3992521,"Gene symbol disambiguation (GSD) is a particular case of word sense disambiguation (WSD), which has been extensively studied in the domain of general English.",1,2
15915,3992521,"2005) , developed at NCBI (National Center for Biotechnology Information), which have been used for gene symbol disambiguation.",20,21
15916,3992521,Figure 1 Overview of an IR combination-based gene symbol disambiguation approach using different types of information.,10,11
15917,3992521,"For each ambiguous gene that was detected, a pair was created consisting of the PMID of the article and the gene symbol, so that each pair would be considered a possible testing sample.",22,23
15918,3992521,"Repeated gene symbols in the same article were ignored, because we assumed only one sense per gene symbol in the same article.",18,19
15919,3992521,"Using this method, 69,111 PMID and ambiguous human gene symbol pairs were identified from the above collection of abstracts.",10,11
15920,3992521,"The list of candidate PMID/gene symbol pairs generated from the articles was then compared with the list of gene identifiers known to be associated with the articles based on ""gene2pubmed"".",7,8
15921,3992521,Then the PMID/gene-symbol pair was tagged with that sense and set aside as a testing sample.,6,7
15922,3992521,"Following a similar procedure as was used for step 2 in the testing set 1 (except that the reference standard in this case was the manually annotated results obtained from Bio-CreAtIvE instead of ""gene2pubmed""), we obtained 124 PMID/gene-symbol pairs with the corresponding tagged senses, which formed testing set 2.",48,49
15923,3992521,"In testing, coefficients obtained from training were used to predict each candidate gene's probability of being the correct sense for a given ambiguous symbol.",25,26
15924,3992521,We evaluated precision and coverage of different combined methods for gene symbol disambiguation on both testing sets.,11,12
15925,3992521,It used the majority sense of an ambiguous gene symbol as the correct sense.,9,10
15926,3992521,All IR-based gene symbol disambiguation approaches showed large improvements when compared to the baseline method.,5,6
15927,3992521,"For example, an ambiguous gene symbol PDK1 (in the article with PMID 10856237), which has two possible gene senses ('GeneID:5163 pyruvate dehydrogenase kinase, isoenzyme 1' and 'GeneID:5170 3phosphoinositide dependent protein kinase-1'), was incorrectly predicted as 'GeneID: 5163' when only ""word"" was used.",6,7
15928,3992521,Coverage of the Methods The IR-based gene symbol disambiguation method described in this paper aims to resolve intraspecies gene ambiguity.,9,10
15929,3992521,The data set from the BioCreAtIvE II GN task therefore is a valuable testing set that enables evaluation and comparison of other gene symbol disambiguation methods.,23,24
15930,3992521,"According to the 'gene_info' file, the gene symbol ""IL-1"" is a synonym for both ""GeneID: 3552 interleukin 1, alpha"" and ""GeneID: 3553 interleukin 1, beta"".",10,11
15931,3992521,"Therefore, the NLP system identified it as an ambiguous gene symbol.",11,12
15932,3992521,"In this paper, we focused on the study of improvements in precision of the gene symbol disambiguation system.",16,17
15933,3992521,"Conclusion and Future Work We applied an IR-based approach for human gene symbol disambiguation, focusing on a study of different methods for combining various types of information from available knowledge sources.",14,15
15934,3992521,Results showed that combination of multiple evidence usually improved the performance of gene symbol disambiguation.,13,14
15935,11764214,"The apprentice's utterances are preceded by the symbol ""#"" and numbered for purposes of discussion.",8,9
15936,219307920,The function LISTVC examines an expression and const'ructs a list of all the variables and constants used in the expression and the number of times each symbol appears.,26,27
15937,14214926,A rewriting of some start symbol into a set of lexical symbols is called a derivation.,5,6
15938,219302696,"Documents indicated by the symbol "" [ # ) "" are available on request to the AFIPS IVashington Office: Where price is noted, make checks payable to I1AFIPS.""",4,5
15939,10504311,It is represented using the symbol where (¼ Ã) is used to refer to a particular alignment configuration.,5,6
15940,1784389,"Given that the notion of a symbol, let.",6,7
15941,1784389,"alone an 'internal symbol', is itself a slippery one, it may he unwise to build our theories of language, or even tl,.",4,5
15942,1784389,"where r is a relation symbol, wi is the lexical head of a phrase and wj is the lexical head of another phrase (typically a subphrase of the phrase headed by w~).",5,6
15943,1784389,"In addition to words, it only required symbols for dependency relations, whereas the qualitative model required symbol sets for linguistic categories and features, and a set of word sense symbols.",18,19
15944,219305084,"P o i n t e r ~?urnl~)or 2 i s a s s o c i a t e d w i t 1 1 t h e symbol ""NUPI"" i n t h e second p l a c e on t h e l e f t -h a n d sic'e, and o c c u r s : ~y i t s e l f i n t h e f o u r t h ?",29,30
15945,219305084,T h i s means t h a t t h e f o u r t h symbol ~o t c h e c !,18,19
15946,219305084,rrotice t h a t t h e a s t e r i s k and t h e d o l l a r s i g n can be p l a c e d 5 e f o r e o r a f t e rt h e symbol t h e y r e f e r t o .,53,54
15947,219305084,"The c o m l ~i n a t i o n i s o f t e n u s e f u l w i t h symbol '""NQY"" i n r u l e s of tile f o l l o w i n q k i n d T h i s ims s i m i l a r t o a n e a r l i e r example.",29,30
15948,219305084,"The symbol ""LNY"" w i t h on a s t e r i s k and a d o l l a r s i q n corresponds i n t h i s s y s t e ?",1,2
15949,219305084,"Consider now t h e f o i l o y ~i n g r u l e : T h i s w i l l form a noun p h r a s e from a s u b o r d i n a t i n g c o n j u n c t i o n followed by a noun p h r a s e , provided that t h i s dominates o n l y t h e symbol ""Sn.",87,88
15950,219305084,Any symhol on t h e l e f thand s i d e o f t h e r u l e may be followed by a n e x p r e s s i o n i n p a r e n t h e s e s s p e c i f y i n g t h e s t x i n g o f c h a r a o t e r s t h a t t h i s symbol must c:ircctly c?oninntc.,91,92
15951,219305084,"i ) o l ""ADJ"" clominntinq e x a c t l y symbol i s t a k e n t o dominate i t s e l f .",15,16
15952,219305084,The main purpose of t h e examples t h a t have been g i v e n i s t o show t h d g r e a t power o f t h e program a s a p r o c e s s o r of symbol s t r i n g s .,52,53
15953,219305084,"r u l n schclv.ta where t h e a s t e r i s k i n d i c a t e s one o r p o r e repetitions of t h e symbol ""A"".",38,39
15954,219305084,"The p h r a s e dominated by t h e synbol ""B"" i s reproduced i n t h e o u t p u t o f t h i s r u l e w i t h c o p i e s o f t h e symbol ""A"" removed from t h e right-hand end and t h e remainder bounded by t h e symbols ""B+"" and ""+B"".",55,56
15955,219305084,"However t h e f i r s t symbol on t h e l e f t -h a n d s i d e of t h i s r u l e w i l l match any s t r i n g \ i h a t s o e v e r , s o t l ~a b , i f t h e r u l e c a n b c applicc?",9,10
15956,219305084,"times t h e r u l e may have been a p p l i e d b u t we know n o t h i n g a b o u t where t o i n s e r t new c o p i e s of t h e symbol ""A"", e x c e p t t h a t t h e y must be t o t h e r i g h t o f t h e e x i s t i n g copy.",55,56
15957,219305084,"Key documents discussed or cited in the Washlnyton Report will be, made available through the Washington Office upon request, whcre feasible; availability will be indicated by a "" ( # ) ' I symbol i n the text of the Washington Report.",36,37
15958,219305084,"Documents indicated by the symbol ""(B)"" are available on request to the AFIPS Washington Office.",4,5
15959,219305084,"Documents indicated by the symbol ' I ( # ) "" a r e available on r q u e s t t o t h e AFIPS Washington Office.",4,5
15960,219305084,"Documents indicated by the symbol 'I(#)"" are available on request to the AFIPS Washington Office.",4,5
15961,219305084,"Both the Regulation J amendments and the interim guidelines for access RFIPS societies have permission t o use material in the AFIps Washington Report for t h e i r own publications, except that where an a r t i c l e t i t l e appears with an ""(*)"" clearan-must f i r s t be obtained from the AFIPS Washingtbn Office.. Documents indicated by the symbol ' I ( # ) "" are available on request t o the AFIPS IVashingon Office.",77,78
15962,2957155,"If the output of processing LDOCE definitions was in the form of meaning postulates, then the logic expressions produced would have a new symbol for the word sense being defined along with symbols corresponding to the senses of words in the definition vocabulary.",24,25
15963,2957155,"For example immediately after the definition of the sense of hornbeam given earlier, the following dependent word sense definition is present, for which the system produces a structure containing the special symbol '*previoussense*'. (",33,34
15964,3265918,"The symbol I indicates a substitution site, while [ ] stands for a particular subordinate conjunction and its feature structure. (",1,2
15965,219308776,"The N symbol S is the distinguished nonterminal from which all derivations b e g i n , and P 2 s the set of productions of G. A context-free grammer is proper if it does not contain productions of thz form A -+ c (erasures) or A B where A and E are both nonterminals.",2,3
15966,7701661,"Let D-S be a domain and R a representation language for D. A natural language understanding system UR/D in R on D is an algorithm that computes a total u R function gR/D:E---2 U {_L} (into 2Ru {±}), where ± is U called the undefined symbol, gR/D(e) = ± denotes that U is unable to assign a meaning to the expression e, that is, that it fails in computing gR/D(e) (not that e has no meaning in the domain D!).",61,62
15967,219303943,"The d a t a p a r t of a r e c o r d i s d i v i d e d i n t o f i e l d s , e a c h o f which i s t a g g e d by a 3digit number-; s u b -f i e l d s a r e headed by a symbol of t h e form $ n , where n i s a n i d e n t i f i e r code.",72,73
15968,1344619,"In this approach, the lexicon associates with a word the entire structure it selects (as shown in Figure 1 ) and not just a (non-terminal) symbol as in context-free grammars.",31,32
15969,1344619,"defgrammar demol (: start-symbol ""S"" :start-feature ""<mode> = ind"") (:tree-files ""lex"" ""modifiers"" (:type ""trees"")) ( : f amily-f iles ""TnxOV"" ""TnxOVa"" ""TnxOVnxl"" ""TnxOVdnl"" ""TnxOVnxlpnx2"" ""TnxOVpnxl"" ""TnxOVsl"" (:type ""trees"")) (:lexicon-files ""lexicon"" (:type ""lex"")) (:example-files ""examples"" (:type ""ex""))) The morphological lexicons for English [Karp el al.,",5,6
15970,219304689,"However, in investigating the information theoretic properties of a long text is convenient to use a scfg-based system which returns to the start symbol and begins the eeneration of another sentence as soon as it completes the generation on one sentence.",26,27
15971,219304689,A predicate symbol followed by a series of constant terms or variable terms and its complement is a literal.,2,3
15972,219304689,"distribution of themes through scenes, and proportion o f occurrence of a theme~-defined in terms of the percentage of words in scene with 3 certain classification symbol (a theme indicator) divided by the percentage of words of that class in the play as a whole.",27,28
15973,219304689,"K is a member of V and is the initial symbol, and S is a set of 21 production rules.",10,11
15974,10736122,"1997) ) is a tuple G = N, Σ, P, S where N is a ranked, finite set of nonterminal labels, Σ is a finite set of terminal labels such that Σ ∩ N = ∅, S ∈ N is the designated start symbol, and P is a finite set of rules.",50,51
15975,10736122,"The hypergraph language of a grammar G is the (possibly infinite) set of hypergraphs that can be derived from the start symbol S. L(G) = (S → H, ) ∈ P {H ⇒ * G H ′ |H ′ has only terminals } We will show examples for HRG derivations below.",23,24
15976,12816225,"For our formalism, this means that the regular expression in a production is such that each string in its denoted language contains at least one terminal symbol.",27,28
15977,12816225,It is interesting to notice the difference between the rewriting operation of a nonterminal symbol as defined for a ECFG or a GDG and the equivalent rewriting steps with a weakly equivalent CFG.,14,15
15978,12816225,"A GDG rewriting operation of a symbol X using a rule r is decomposed in two stages, the first stage consists in choosing a string w which belongs to the set denoted by the right-hand side of r. During the second stage, X is replaced by w. These two stages are of a different nature, the first concerns the generation of CFG rules (and hence a CFG) using a GDG while the second concerns the generation of a string using the generated CFG.",6,7
15979,12816225,"If i = j, t i,j also contains the in- put symbol w i .",14,15
15980,12816225,"Completion: If t i,j contains either the input symbol w or an item (M, q) such that q is a final state of M , and M is a C-rule-FSM, then add to t i,j all (M , q ) such that M is a rule-FSM which transitions from a start state to state q on input w or C. Add a single backpointer from (M , q ) in t i,j to (M, q) or w in t i,j . •",11,12
15981,12816225,"Scanning: If (M 1 , q 1 ) is in t i,k , and t k+1,j contains either the input symbol w or the item (M 2 , q 2 ) where q 2 is a final state and M 2 is a C-rule-FSM, then add (M 1 , q) to t i,j (if not already present) if M 1 transitions from q 1 to q on either w or C. Add a double backpointer from (M 1 , q) in t i,j to (M 1 , q 1 ) in t i,k (left backpointer) and to either w or (M 2 , q 2 ) in t k+1,j (right backpointer).",24,25
15982,9570554,"~ The sentence form corresponding to the passive transformation, N t be en V by N is then interrupted by the sequence AN before the last AN is a deformation of the kernel sentence form N t be A not a sentence form, but, when substituted for a symbol in a sequence (of ~et I or 2 or 3), preserves the character of that sequence (e.g. er V or/L , n A of N).",51,52
15983,9570554,Another body o£ such material is the dlctlonar~. • The dictionary associates to each English word a symbol representing the wordts grannnatlcal class~ together with markers of certain additional characteristics the word may reveal by restricting its environment in the sentence.,17,18
15984,9570554,2.3 The process of analyzing a sentence begins in postulatlng (in turn) all those strings in the grammar which may occur at the beglnnlngP--q~o//~ of a sentence (and whose initial symbol is the s~me as the first symbol in the data). (,37,38
15985,9570554,2.3 The process of analyzing a sentence begins in postulatlng (in turn) all those strings in the grammar which may occur at the beglnnlngP--q~o//~ of a sentence (and whose initial symbol is the s~me as the first symbol in the data). (,46,47
15986,6510171,The symbol CR in the tables corresponds to the blank space between words and is used as a word delimiter.),1,2
15987,6510171,"The process of generating the output sequence of a model can then be seen as a random traversal of the graph according to the probability weights on the arrows, with an output symbol generated randomly each time a state is visited, according to the output distribution associated with that state.",33,34
15988,6510171,Each state corresponds to exactly one output symbol (a letter or word delimiter).,7,8
15989,1840531,"Local String Adjunct Language (LAL) The meaning of an adjunction rule, say, u = (ui, a i, ~k) is that fr~n u i we can derive a new string by adjoining u~Vto the left of the kth symbol in u i. Thus, for example if u = (abe, t, ~2) we can derive a string atbc.",49,50
15990,1840531,"Further a. A~N =~; b. If ~ ¢ Zthen s c (A~N) (AUN)*; c. There is no rule in J which adjoins adjuncts to the left or right of a null symbol, i.e., null symbols have no points of adjunction.",43,44
15991,1840531,"~ 2.8 We say that a local a~junction rule is uniform if in a rule u the adjunct aj adjoins to the left (or right) of some symbol ag ~ A in the host of u, then a~ adjoins to the left (or right) Of ag wherever ag occurs in 8n~ string in ~. An LAG, G, is uniform if all its rules are uniform.",31,32
15992,1840531,"But these auxiliary symbols are used purely as position markers and do not have the same interpretation as the nonterminals in a PSG (i.e., the auxiliary symbols ~k's do not correspond to phrase types) • If we consider the marking symbol, A, used in making precise the definition in Section 2.2, (see Joshi, Kosaraju, yamada (1968)), also as an auxiliary symbol then one can posslbly consider a i (a i e A) as a nonterminal which can be interpreted as a phrase ^type but with the added interpretation that a rhrase type a i has a i as the 'heed' (or 'center') of the phrase.",44,45
15993,1840531,"But these auxiliary symbols are used purely as position markers and do not have the same interpretation as the nonterminals in a PSG (i.e., the auxiliary symbols ~k's do not correspond to phrase types) • If we consider the marking symbol, A, used in making precise the definition in Section 2.2, (see Joshi, Kosaraju, yamada (1968)), also as an auxiliary symbol then one can posslbly consider a i (a i e A) as a nonterminal which can be interpreted as a phrase ^type but with the added interpretation that a rhrase type a i has a i as the 'heed' (or 'center') of the phrase.",74,75
15994,1840531,"The null symbols as nontcrmlnals are highly restricted because they are never 'rewritten' (in the sense of a PSG) into any other string except the null string, i.e., the only 'rewrite rule' associated with a null symbol, say~, is ~ ~ £.",44,45
15995,1840531,"Of course, ~f an adjunct string, say, ~ is a complex basic string and has a specified segmentation, then ~ach symbol S in oj must be included in some segment of aj. *",24,25
15996,1840531,"Actually, it is possible to define the recursive extension of the adjunction rules such that the null symbol associated with any basic string is erased at the time the string is ad.ioined to some host string.",18,19
15997,1840531,The null symbol associated with the center string is then erased at the end.,2,3
15998,1840531,If R is not empty then we have replacement rules associated with the symbol S. The generation still proceeds in an 'inside out' manner.,13,14
15999,1840531,"We say that a string s' is a deformed a if every symbol of ~' is either a constant (i.e., is in C^) or is a symbol of a or both, That is, a' is obtained from a Ey one or more of the operations of permuting, deleting, or repeating symbols of a or adding one or more symbols from C .",13,14
16000,1840531,"We say that a string s' is a deformed a if every symbol of ~' is either a constant (i.e., is in C^) or is a symbol of a or both, That is, a' is obtained from a Ey one or more of the operations of permuting, deleting, or repeating symbols of a or adding one or more symbols from C .",31,32
16001,1840531,"Each oci can then be extended to the derived hosts and derived adjuncts in u, and to derived hosts and derived replacers in p in the obvious way (i.e., if a symbol of ~ is permuted it carries with it its adjuncts, and their adjunct~ etc.~ if a symbol of G~ is deleted then we delete its adjuncts, and their adjuncts etc.,",34,35
16002,1840531,"Each oci can then be extended to the derived hosts and derived adjuncts in u, and to derived hosts and derived replacers in p in the obvious way (i.e., if a symbol of ~ is permuted it carries with it its adjuncts, and their adjunct~ etc.~ if a symbol of G~ is deleted then we delete its adjuncts, and their adjuncts etc.,",52,53
16003,198622,"Completion: If T i,j contains either the input symbol w or an item (M, q) such that q is a final state of M , and M is a t-FSM, then add to T i,j all (M , q ) such that M is a FSM which transitions from a start state to state q on input w or t. Add a single backpointer from (M , q ) in T i,j to (M, q) or w in T i,j .",11,12
16004,198622,"Scanning: If (M 1 , q 1 ) is in T i,k , and T k+1,j contains either the input symbol w or the item (M 2 , q 2 ) where q 2 is a final state and M 2 is a t-FSM, then add (M 1 , q) to T i,j (if not already present) if M 1 transitions from q 1 to q on either w or t. Add a double backpointer from (M 1 , q) in T i,j to (M 1 , q 1 ) in T i,k (left backpointer) and to either w or (M 2 , q 2 ) in T k+1,j (right backpointer).",24,25
16005,219302120,a g e o m e t r i c a l d e s i g n which c o n s t i t u t e s a word-symbol ( l e x i c a l item) of t h e ~e r k i s h l a n g u a g e .,33,34
16006,13192969,"Techniques which have been used in augmentative and alternative communication (AAC) often involve the use of alternative symbol systems for input, in particular Minspeak (Baker, 1982) .",19,20
16007,13192969,"Such users would prefer to continue using their original language, rather than to learn an alternative symbol system.",17,18
16008,5413386,"In the next level analysis (the term we give to the analysis of its parent CS or of the whole sentence if no parent CS exists), the CS node is handled as a symbol.",36,37
16009,219306412,"I i s the meaning function, which assigns t o each sentence, phrase form, and symbol, one o r more meanings according t o the semantics d .",18,19
16010,219306412,"For c f g ' s we s h a l l follow t h e u s u a l convention t h a t a l l symbols which do n o t appear on t h e left-hand side o f some grammar r u l e are terminal symbols, and t h a t t h e axiom i s the f i r s t symbol appearing in t h e f i r s t rule. )",73,74
16011,219306412,"A l l t h a t i s needed i s t o assume t h a t i f a symbol appears i n both semantics, i t r e p r e s e n t s t h e same semantic e n t i t y i n each, whatever that e n t i t y is.",22,23
16012,219306605,"T h e four t y p e s are : (0) Develops a s i n g l e parse tree a t a t i m e ; a t any i n s t a n t t h e store holds a set o f nodes c o r r e s p o n d i n g t o the nodes of an incomplete potential parse tree f t ) The s t o r e holds a s e t of n o d e s , each of which represents t h e fact t h a t some substring of t h e s e n t e n c e , from word f to word R , can be a n a l y z ~d as some symbol N. ( 2 ) T h e s t o r e holds a s e t of nodes, each of which r e p r e s e n t s an analysis of some substring of t h e s e n t e n c e , from word f to word a , as some symbol N ( i f there are several different analyses of words f t o 11 as some symbol N , there w i l l be several nodes corresponding t o a single node i n a t y p e 1 p a r s e r ) .",141,142
16013,219306605,"T h e four t y p e s are : (0) Develops a s i n g l e parse tree a t a t i m e ; a t any i n s t a n t t h e store holds a set o f nodes c o r r e s p o n d i n g t o the nodes of an incomplete potential parse tree f t ) The s t o r e holds a s e t of n o d e s , each of which represents t h e fact t h a t some substring of t h e s e n t e n c e , from word f to word R , can be a n a l y z ~d as some symbol N. ( 2 ) T h e s t o r e holds a s e t of nodes, each of which r e p r e s e n t s an analysis of some substring of t h e s e n t e n c e , from word f to word a , as some symbol N ( i f there are several different analyses of words f t o 11 as some symbol N , there w i l l be several nodes corresponding t o a single node i n a t y p e 1 p a r s e r ) .",202,203
16014,219306605,"T h e four t y p e s are : (0) Develops a s i n g l e parse tree a t a t i m e ; a t any i n s t a n t t h e store holds a set o f nodes c o r r e s p o n d i n g t o the nodes of an incomplete potential parse tree f t ) The s t o r e holds a s e t of n o d e s , each of which represents t h e fact t h a t some substring of t h e s e n t e n c e , from word f to word R , can be a n a l y z ~d as some symbol N. ( 2 ) T h e s t o r e holds a s e t of nodes, each of which r e p r e s e n t s an analysis of some substring of t h e s e n t e n c e , from word f to word a , as some symbol N ( i f there are several different analyses of words f t o 11 as some symbol N , there w i l l be several nodes corresponding t o a single node i n a t y p e 1 p a r s e r ) .",220,221
16015,219306605,"The f i r s t t i m e the p a r s e r t r i e s t o analyze a p o r t i o n of the s e n t e n c e b e g i n n i n g a t word f as an instance of symbol N, this mechanism records any and all trees constructed below node N. The n e x t time t h e p a r s e r tries symbol N at word f, the saving m e c h a n i s m retrieves this i n f o r m a t i o n so mat t h e trees below N n e e d not actually be r e b u i l t .",60,61
16016,219306605,"The f i r s t t i m e the p a r s e r t r i e s t o analyze a p o r t i o n of the s e n t e n c e b e g i n n i n g a t word f as an instance of symbol N, this mechanism records any and all trees constructed below node N. The n e x t time t h e p a r s e r tries symbol N at word f, the saving m e c h a n i s m retrieves this i n f o r m a t i o n so mat t h e trees below N n e e d not actually be r e b u i l t .",90,91
16017,219306605,A t y p e 2 rather t h a n a type 1 a l g o r i t h m was u s e d because t h e r e s t r i c t i o n s can o11e analysis of a portion of the s e n t e n c e as a particular symbol while accepting another a n a l y s i s of the same p o r t i o n of the s e n t e n c e a s the same s y x b o l .,63,64
16018,219306605,How f r e q u e n t l y can a p o r t i o n of i+ s e n t e n c e be a n a l y z e d as a particular symbol i n s e v e r a l ways?,42,43
16019,219306605,"s > I each of whose components 1' n is either a symbol (name of a n o d e ) o r "" X u -The s t r u c t u r a l change is a tuple <sclr. . . ,",13,14
16020,219306605,"Each of t h e s c i j in, is e i t h e r a t e r m i n a l symbol or an integer between 1 and n. The a p p l i c a t i o n of f r a n s f o m a t i o n a l rules i s based dh t h e n o t i o n of a p r o p e r anatysis, which is in t u r n based on t h e concept of a cut o f a tree.",27,28
16021,219306605,"If s c i j is an integer,between 1 and n, t h e new node is t h e node matched to the s c i j -t h e l e m e n t of t h e s t r u c t u r a l i n d e x ; i f SCij is a t e r m i n a l symbol, the new node is a t e r m i n a l node w i t h t h a t name.",73,74
16022,219306605,"I n a s i n g u l a r y t r a n s f o r m a t i o n , the s t r u c t u r a r index does not contain the symbol #.",43,44
16023,219306605,"if t h o s p e c i f i e d koot symbol is ' X' tbc r o u t i n e creates a s e t of parse trees, e a c h containi~qj a single node, named X, s p a ~l n i n g a l l possible s u b s t r i n g s with f i r s t w6rd = STARTWD 3.",14,15
16024,14801993,"We might come up with a mapping z from properties mentioned in the network to logical terms which includes the following assignments (variables whose names are of no importance in the logical terms are denoted here by the symbol ""_""): ~(animate) = pr(q(an),c(_),no) •(case) = pr(_,c(_),_) •(far) = pr(d(far),no,n(_, _)) ""r(gender) = pr(p(3),c(_),n(s,_)) r(neuter) = pr(p(3),c(_),n(s,n)) ~(numb) = pr( .... n(_,_)) r(personal) = pr(p(_),c(_),n(_,_)) ""r(reflexive) = pr(_,c(refl),_) r(singular) = pr(_,_)) •(third) = pr(p(3),c(_),n(_,_)) This mapping is not purely random, but has been chosen so that the logical relationship of subsumption is ""echoed"" in the ""degree of instantiation"" of the terms.",39,40
16025,14801993,"Each such term also has the same function symbol, the constant ""0"" as its first argument and the constant ""1"" as its last argument.",8,9
16026,14801993,"In the logical terms, this can be allowed for by giving the *(neg) = vb(aux(neg,_,_),_) *(be) = vb(aux( .... be),_) *(pres) = vb(_,pres) Notice how the ""aux"" function symbol has arity 3; this corresponds to the three independent sub-classifications introduced by the ""{"" connective.",54,55
16027,14801993,"The algorithm works for networks using the connectives ""l"", ""{"" and ""}"", but does not handle networks that use it],,, For this algorithm, we require in advance a function A which associates with each node n of the system A is required to be I-1, except that it should map all the nodes appearing around a ""{"" connective to the same function symbol.",78,79
16028,14801993,"The values for C and P are: C = a set of ""constraints"" describing the term P = a ""path"" indicating a (possibly embedded) component of the term which can become further instantiated for terms representing properties subsumed by the current property A path is simply a sequence of alternating function symbols and numbers, either empty or starting with a function symbol, indicating a specific position in a term and the function symbols that appear on the route from the outside of the term to this position.",69,70
16029,14801993,"A symbol in the sequence indicates a function symbol that is present, whereas a number selects one of the argument positions of the last function symbol.",1,2
16030,14801993,"A symbol in the sequence indicates a function symbol that is present, whereas a number selects one of the argument positions of the last function symbol.",8,9
16031,14801993,"A symbol in the sequence indicates a function symbol that is present, whereas a number selects one of the argument positions of the last function symbol.",26,27
16032,14801993,"For instance, in f(a, g(X,h(b),i(d,e j(c)))) the symbol ""c"" appears at the place indicated by the path (f,2,g,3,i,3,j,1) and the symbol ""b"" at the place indicated by (f,2,g,2,h, 1).",17,18
16033,14801993,"For instance, in f(a, g(X,h(b),i(d,e j(c)))) the symbol ""c"" appears at the place indicated by the path (f,2,g,3,i,3,j,1) and the symbol ""b"" at the place indicated by (f,2,g,2,h, 1).",34,35
16034,14801993,"We will make use of two basic types of extensions of a path P: an extension of P to f, for some function symbol f: If P is 0 then {f) Otherwise if P is (p~, P2 .... i) for some number i then (Pl, P2 .... i,f) Otherwise if P is (Pl, P2 .... s) for some symbol s then iP~, P2 .... s,i,f) for some number i an extension of P beyond f, for some function symbol f: If P is i) then if, i) for some number i Otherwise if P is iPl, P2 .... i) for some number i then iPl, P2 .... i,f,j) for some number j Otherwise if P is (Pl, P2 .... s) for some symbol s then (Pl, P2 .... s,i) for some number i For instance, for the path if,2,g), two possible extensions to h are if,2,g,33,h) and if,2,g, 1,h).",25,26
16035,14801993,"We will make use of two basic types of extensions of a path P: an extension of P to f, for some function symbol f: If P is 0 then {f) Otherwise if P is (p~, P2 .... i) for some number i then (Pl, P2 .... i,f) Otherwise if P is (Pl, P2 .... s) for some symbol s then iP~, P2 .... s,i,f) for some number i an extension of P beyond f, for some function symbol f: If P is i) then if, i) for some number i Otherwise if P is iPl, P2 .... i) for some number i then iPl, P2 .... i,f,j) for some number j Otherwise if P is (Pl, P2 .... s) for some symbol s then (Pl, P2 .... s,i) for some number i For instance, for the path if,2,g), two possible extensions to h are if,2,g,33,h) and if,2,g, 1,h).",74,75
16036,14801993,"We will make use of two basic types of extensions of a path P: an extension of P to f, for some function symbol f: If P is 0 then {f) Otherwise if P is (p~, P2 .... i) for some number i then (Pl, P2 .... i,f) Otherwise if P is (Pl, P2 .... s) for some symbol s then iP~, P2 .... s,i,f) for some number i an extension of P beyond f, for some function symbol f: If P is i) then if, i) for some number i Otherwise if P is iPl, P2 .... i) for some number i then iPl, P2 .... i,f,j) for some number j Otherwise if P is (Pl, P2 .... s) for some symbol s then (Pl, P2 .... s,i) for some number i For instance, for the path if,2,g), two possible extensions to h are if,2,g,33,h) and if,2,g, 1,h).",101,102
16037,14801993,"We will make use of two basic types of extensions of a path P: an extension of P to f, for some function symbol f: If P is 0 then {f) Otherwise if P is (p~, P2 .... i) for some number i then (Pl, P2 .... i,f) Otherwise if P is (Pl, P2 .... s) for some symbol s then iP~, P2 .... s,i,f) for some number i an extension of P beyond f, for some function symbol f: If P is i) then if, i) for some number i Otherwise if P is iPl, P2 .... i) for some number i then iPl, P2 .... i,f,j) for some number j Otherwise if P is (Pl, P2 .... s) for some symbol s then (Pl, P2 .... s,i) for some number i For instance, for the path if,2,g), two possible extensions to h are if,2,g,33,h) and if,2,g, 1,h).",160,161
16038,14801993,"Finally, we will have two ways of describing paths that differ, according to the type of the first component where they disagree: PI and P2 are independent iff their first disagreement is on a number P1 and P2 are inconsistent iff their first disagreement is on a function symbol Thus (f,2,g,3,h) is independent of if,2,g,4,d,3), and if,2,g,5,a) is inconsistent with if,2,h,4).",51,52
16039,14801993,"First of all, each function symbol (element of the range of A) is taken to have the same arity (number of arguments) in all the terms derived from the network.",6,7
16040,14801993,"If the connective is ""l"", the extra constraints added for each node on the right amount to forcing the place indicated by the path to have the function symbol associated with that node.",31,32
16041,14801993,"For instance, in the verb network, checking that a descrip-tion of a verb is compatible with ""pres"" simply amounts to seeing whether the third component of the term is not a function symbol different from ""pres"".",38,39
16042,219302531,Sakharw is f o r us--and should be f o r the world--a symbol of redemption.,17,18
16043,28708574,In this case each character must be considered a function symbol.,10,11
16044,28708574,"The rules are organized in the form of a ""symbol tree"" in which a symbol is either a part of speech or a function symbol, a node carrying a single symbol.",10,11
16045,28708574,"The rules are organized in the form of a ""symbol tree"" in which a symbol is either a part of speech or a function symbol, a node carrying a single symbol.",16,17
16046,28708574,"The rules are organized in the form of a ""symbol tree"" in which a symbol is either a part of speech or a function symbol, a node carrying a single symbol.",26,27
16047,28708574,"The rules are organized in the form of a ""symbol tree"" in which a symbol is either a part of speech or a function symbol, a node carrying a single symbol.",33,34
16048,28708574,The tree arrangement matches the parsing strategy; as the parser advances or backtracks in the parsing graph it performs identical actions in the symbol tree.,24,25
16049,219310220,"Documents i n d i c a t e d by t h e symbol ""(#)I1 a r e a v a i l a b l e on r e q u e s t t o the Washington Office.",14,15
16050,219310220,"Documents i n d i c a t e d by t h e symbol l l ( # ) t l a r e a v a i l a b l e on r e q u e s t According to the executive order establishing the new Assistant Secretary, a'nd the first budget submission of the NTIA, the Secretary of Commerce and the Assistant Secretary of Commerce or the NTIA are expected to: Act as principal adviser to the President on telecommunications;.",14,15
16051,219310220,"Documents i n d i c a t e d by the symbol "" ( # ) I t are a v a i l a b l e on request t o t h e ' Washington Office.",12,13
16052,1032431,"We thus overload the symbol E (""is-more.",4,5
16053,7617879,"Formalism In brief, an HMM is a doubly stochastic process that generates sequence of symbols S = { Si, S2,...,ST}, Si E W I<i<T, where W is some finite set of possible symbols, by composing an underlying Markov process with a state-dependent symbol generator (i.e., a Markov process with noise), i Th Markov process captures the notion of sequence depen dency and is described by a set of N states, a matrix c transition probabilities A = {aij} 1 <_ i, j <_ N where ai is the probability of moving from state i to state j, and vector of initial probabilities H = {rq} 1 < i < N where is the probability of starting in state i. The symbol ger erator is a state-dependent measure on V described by matrix of symbol probabilities B = {bjk} 1 _< j <__ N an 1 < k < M where M = IWI and bjk is the probability generating symbol s~ given that the Markov process is i state j.2 In part-of-speech tagging, we will model word order d, pendency through an underlying Markov process that ot crates in terms of lexical tags,'yet we will only be ab to observe the sets of tags, or ambiguity classes, that aJ possible for individual words.",56,57
16054,7617879,"Formalism In brief, an HMM is a doubly stochastic process that generates sequence of symbols S = { Si, S2,...,ST}, Si E W I<i<T, where W is some finite set of possible symbols, by composing an underlying Markov process with a state-dependent symbol generator (i.e., a Markov process with noise), i Th Markov process captures the notion of sequence depen dency and is described by a set of N states, a matrix c transition probabilities A = {aij} 1 <_ i, j <_ N where ai is the probability of moving from state i to state j, and vector of initial probabilities H = {rq} 1 < i < N where is the probability of starting in state i. The symbol ger erator is a state-dependent measure on V described by matrix of symbol probabilities B = {bjk} 1 _< j <__ N an 1 < k < M where M = IWI and bjk is the probability generating symbol s~ given that the Markov process is i state j.2 In part-of-speech tagging, we will model word order d, pendency through an underlying Markov process that ot crates in terms of lexical tags,'yet we will only be ab to observe the sets of tags, or ambiguity classes, that aJ possible for individual words.",147,148
16055,7617879,"Formalism In brief, an HMM is a doubly stochastic process that generates sequence of symbols S = { Si, S2,...,ST}, Si E W I<i<T, where W is some finite set of possible symbols, by composing an underlying Markov process with a state-dependent symbol generator (i.e., a Markov process with noise), i Th Markov process captures the notion of sequence depen dency and is described by a set of N states, a matrix c transition probabilities A = {aij} 1 <_ i, j <_ N where ai is the probability of moving from state i to state j, and vector of initial probabilities H = {rq} 1 < i < N where is the probability of starting in state i. The symbol ger erator is a state-dependent measure on V described by matrix of symbol probabilities B = {bjk} 1 _< j <__ N an 1 < k < M where M = IWI and bjk is the probability generating symbol s~ given that the Markov process is i state j.2 In part-of-speech tagging, we will model word order d, pendency through an underlying Markov process that ot crates in terms of lexical tags,'yet we will only be ab to observe the sets of tags, or ambiguity classes, that aJ possible for individual words.",162,163
16056,7617879,"Formalism In brief, an HMM is a doubly stochastic process that generates sequence of symbols S = { Si, S2,...,ST}, Si E W I<i<T, where W is some finite set of possible symbols, by composing an underlying Markov process with a state-dependent symbol generator (i.e., a Markov process with noise), i Th Markov process captures the notion of sequence depen dency and is described by a set of N states, a matrix c transition probabilities A = {aij} 1 <_ i, j <_ N where ai is the probability of moving from state i to state j, and vector of initial probabilities H = {rq} 1 < i < N where is the probability of starting in state i. The symbol ger erator is a state-dependent measure on V described by matrix of symbol probabilities B = {bjk} 1 _< j <__ N an 1 < k < M where M = IWI and bjk is the probability generating symbol s~ given that the Markov process is i state j.2 In part-of-speech tagging, we will model word order d, pendency through an underlying Markov process that ot crates in terms of lexical tags,'yet we will only be ab to observe the sets of tags, or ambiguity classes, that aJ possible for individual words.",193,194
16057,7617879,"However, in part-of-speech tagging, the problem structure dictates that the matrix of symbol probabilities B is sparsely populated.",18,19
16058,7617879,"That is, bij 3£ 0 iff the ambiguity class corresponding to symbol j includes the part-of-speech tag associated with state i. In practice, the degree of overlap between ambiguity classes is relatively low; some tokens are assigned unique tags, and hence have only one non-zero symbol probability.",13,14
16059,7617879,"That is, bij 3£ 0 iff the ambiguity class corresponding to symbol j includes the part-of-speech tag associated with state i. In practice, the degree of overlap between ambiguity classes is relatively low; some tokens are assigned unique tags, and hence have only one non-zero symbol probability.",56,57
16060,7617879,The sparseness of B leads one to consider restructuring equations 1-6 so a check for zero symbol probability can obviate the need for further computation.,18,19
16061,7617879,"However, this fails if any two 4An equivalent approach maintains a mapping from states i to non-zero symbol probabilities and simply avoids, in the inner iteration, computing products which must be zero [Kupiec, 1992] .",20,21
16062,7617879,"states are indistinguishable (in the sense that they had the same transition probabilities and the same symbol probabilities at start), because states are then not matched across trained models.",17,18
16063,14437379,"The following description relates to sentence level: Initially, the input appears on the first tape with each of the tape's cells containing either a word, a blank (symbolized below by a single ""B""), or a left or right sentence boundary symbol.",49,50
16064,17586164,Each elementary tree is constrained to have at least one terminal symbol which acts as its anchor.,11,12
16065,17586164,"It should be emphasized that in our approach the category of a word is not a non-terminal symbol but a multi-level structure corresponding to minimal linguistic structures: sentences (for predicative verbs, nouns and adjectives) or phrases (NP for nouns, AP for adjectives, PP for prepositions yielding adverbial phrases).",19,20
16066,6754541,"Considering the use of symbols from AAC (augmentative and alternative communication) designed for speech-impaired disabled users by patients with limited English, we noticed that AAC symbol sets have a systematic iconicity that regular users learn, but which may be opaque to first-time (or one-time) untrained users (Johnson, 2004) .",30,31
16067,15668195,Vowel bilabial alveolar retroflex palatal velar plosive [p] p [t] t [֒] t [c] tj [k] k nasal [m] m [n] n ‫][‬ n [ْ] ny [ŋ] ng trill/flap [r] r lateral [l] l [և] l [ѭ] ly approximant [w] w [ӆ] r [j] y Table 1 : Pitjantjatjara consonants: phonetic symbol in [ ] and standard orthography in italics.,86,87
16068,8642915,"Due to the presence of the special symbol skip in the right side of the rules, this grammar turns to be a special, very simple case of a discontinuous grammar (Dahl, 1989) .",7,8
16069,10103878,"In the pattern ^AB, the ^ symbol indicates adjectival form, so for example nihon may be Japan or Japanese.",7,8
16070,5203920,"Simply put, an ACFG is a Context-Free Grammar where each non-terminal symbol has a (finite) sequence of attributes, each of which can have a set of a finite number of symbols as its value.",16,17
16071,2197535,"As a shorthand convenience in what follows, if the circle contains a symbol, then it is assumed that only that symbol is ever generated by the state. (",13,14
16072,2197535,"As a shorthand convenience in what follows, if the circle contains a symbol, then it is assumed that only that symbol is ever generated by the state. (",22,23
16073,2197535,A single symbol is generated by a transition to a terminal state.,2,3
16074,2197535,"Only one network may be assigned as the top-level network, and it is analogous to the root symbol of a grammar.",20,21
16075,5083798,In most programs today a sufficient representation of the object it names could be just the symbol 53RD-MECHANIZED-DIVISION.,16,17
16076,5083798,"The print name of the symbol conveys all the information that a person reading the code needs to know, without it actually playing a role in the program's reasoning.",5,6
16077,5083798,"If all we cared about were a single communicative context, we might consider implementing the link between the symbol and the description as though the phrase were one long word without any internal structure.",19,20
16078,5083798,"Specifications are implemented as structured objects, indicated by the ""#< ... >"" convention of CommonLisp; the first symbol after the ""<"" gives the object's type.",22,23
16079,7657542,"Convenient notational reductions of other specialized sublanguages to formal symbol systems come to mind, as for example the notational systems of logic and mathematics, whose most complex formulae are nevertheless always stateable somehow in sentences of the corresponding sublanguages.",9,10
16080,7657542,"In informal, intuitive terms, a constructive grammar generates sentences from the bottom up, beginning with word entry, whereas a generative grammar generates sentences from the top down, beginning with the abstract symbol S. The grammatical apparatus of constructive grammar (the rules together with their requirements and exceptions) is very simple and parsimonious.",36,37
16081,7184544,"Th, right-hand side of the rule contains one distinguishe~ terminal symbol called governor, while the other sym bols are called dependents.",13,14
16082,7184544,The star symbol in each rule represents the governor position.,2,3
16083,7184544,"symbol in the solution represents a possibly missing function word ignored during parsing, that is expanded into a set of candidates, according to the grammar, to be fed back to the recognizer.",0,1
16084,5166150,Extensive measurements were made on a parallel version of the 1 We also differ from CYK in that we do not merge different analyses of the same string as the same symbol.,31,32
16085,5166150,"2For grammar #4 given below and a 15-word sentence, the matrix would have roughly 15,000 entries (one entry for each substring and each symbol in the equivalent Chomsky normal form $ranunar), of which only about 1000 entries are filled.",28,29
16086,5166150,"Y Z from nl to n2, and this is the first time we have tried to match symbol Y starting at n2 (there are no edges labeled Y originating at n~), we perform a seek on symbol Y at n2: we create an active edge for each production which expands Y, and try to extend these edges.",18,19
16087,5166150,"Y Z from nl to n2, and this is the first time we have tried to match symbol Y starting at n2 (there are no edges labeled Y originating at n~), we perform a seek on symbol Y at n2: we create an active edge for each production which expands Y, and try to extend these edges.",40,41
16088,5166150,We begin by doing a seek for the sentence symbol S starting a node 0.,9,10
16089,2044913,"l1, e1] {[l1, x12] subj , [l1 : give(e1, x12, x13, x14)] {} [l1, x13] obj , [l1, x14] iobj } a cat : [l4, x2] {} [l2 : a(x2, h2, h3), l1 : cat(x1)] {l3 = l1, x2 = x1} a rat : [l7, x5] {} [l5 : a(x5, h5, h6), l4 : rat(x4)] {l6 = l4, x5 = x4} iobj : [l1, e1] {[l1, x12] subj , [l1, x13] obj } [l1 : give(e1, x12, x13, x14), {l3 = l1, x2 = x1, l2 : a(x2, h2, h3), l1 : cat(x1)] l1 = l4, x14 = x2} obj : [l1, e1] {[l1, x12] subj } [l1 : give(e1, x12, x13, x14), {l3 = l1, x2 = x1, l2 : a(x2, h2, h3), l1 : cat(x1), l1 = l4, x14 = x2, l5 : a(x5, h5, h6), l4 : rat(x4)] l1 = l7, x13 = x5} One alternative would be to use an n-ary conjunction symbol.",264,265
16090,2044913,"Schematically: AT lexrel q(x) NN1 lexrel n(x) AT1 lexrel q(x) VVD lexrel v(e past ) JJ lexrel j(x) II lexrel p(e) Here, 'lexrel' is a special symbol, which is to be replaced by the individual lemma (with a leading underscore) -e.g.,",35,36
16091,186232840,"After all, our heads do not apply programs on stored symbol arrays but are appropriately wired for understanding or producing language.",11,12
16092,186231583,"Artificial Intelligence, as well as neurophysiological studies of recent decades, suggests a unitary view of language, in which natural language constitutes only part of the total language of the organism; the latter is a single but complex structure containing also the symbol-systems and their transformations responsible for mental processes.",45,46
16093,2209224,"Although the generative and the computational paradigms share an interest in the knowledge possessed by an individual who uses language and in formal symbol manipulation, they differ in the degree of attention to process organization (low in the generative paradigm) and the inclusion of non-linguistic knowledge into the sphere of interest of linguistics (liberal in the computational paradigm -cf.",23,24
16094,1826534,"Definition 1 A Tree Adjoining Grammar (TAG) is a 5-tuple G=(VN, Vy, S, l, A), where VN is a finite set of non-terminal symbols, Vy is a finite set of terminal symbols, Se VN is the start symbol, 1 and A are two finite sets of trees, called initial trees and auxiliary trees respectively.",51,52
16095,1826534,"For every tree a=(N, E) and every node n~N, a function domaina is defined such that domaindn)-'~, where/3 is the maximal subtree in a whose root is n. IThe symbol ""_1_"" denotes here the undefined element.",35,36
16096,1826534,"For any TAG G and for every node n in some tree in G, we will write cat(n)=X, X~ VNuVZ, whenever X is the symbol associated to n in G. For every node n in some tree in G, such that cat(n)~ VN, the set Adjoin(n) contains all root nodes of auxiliary trees that can be adjoined to n in G. Furthermore, a function x is defined such that, for every tree a~ luA, it holds that z(a)=n, where n indicates the anchor node of a. In the following we assume that the anchor nodes in G are not labelled by the null (syntactic) category symbol e. The set of all nodes that dominate the anchor node of some tree in IuA will be called Middle-nodes (anchor nodes included); for every tree a=(N, E), the nodes nEN in Middle-nodes divide a in two (possibly empty) left and right portions.",27,28
16097,1826534,"For any TAG G and for every node n in some tree in G, we will write cat(n)=X, X~ VNuVZ, whenever X is the symbol associated to n in G. For every node n in some tree in G, such that cat(n)~ VN, the set Adjoin(n) contains all root nodes of auxiliary trees that can be adjoined to n in G. Furthermore, a function x is defined such that, for every tree a~ luA, it holds that z(a)=n, where n indicates the anchor node of a. In the following we assume that the anchor nodes in G are not labelled by the null (syntactic) category symbol e. The set of all nodes that dominate the anchor node of some tree in IuA will be called Middle-nodes (anchor nodes included); for every tree a=(N, E), the nodes nEN in Middle-nodes divide a in two (possibly empty) left and right portions.",117,118
16098,1826534,"The set Left-nodes (Right-nodes) is defined as the set of all nodes in the left (right) portion of some tree in IuA. Note that the three sets Middle-nodes, Left-nodes and Right-nodes constitute a partition of the set of all nodes of trees in IuA. The set of all foot nodes in the trees in A will be called Foot-nodes: Let w---a I ... anw, nw >1, be a symbol string; we will say that nw is the length of w. Definition 2 A state is defined to be any 8-tuple [n, ldot, lpos, rdot, rpos, fl, fr, m] such that: n, ldot, rdot are nodes in some tree ~ IuA; lpos, rpos~ {left, right}; fl, fr are either the symbol ""-"" or indices in the input string such thatfl<fr; mE {-, rm, Ira}.",91,92
16099,1826534,"The set Left-nodes (Right-nodes) is defined as the set of all nodes in the left (right) portion of some tree in IuA. Note that the three sets Middle-nodes, Left-nodes and Right-nodes constitute a partition of the set of all nodes of trees in IuA. The set of all foot nodes in the trees in A will be called Foot-nodes: Let w---a I ... anw, nw >1, be a symbol string; we will say that nw is the length of w. Definition 2 A state is defined to be any 8-tuple [n, ldot, lpos, rdot, rpos, fl, fr, m] such that: n, ldot, rdot are nodes in some tree ~ IuA; lpos, rpos~ {left, right}; fl, fr are either the symbol ""-"" or indices in the input string such thatfl<fr; mE {-, rm, Ira}.",164,165
16100,1826534,"A binary operator indicated with the symbol • is defined to combine the components fl, fr in different states; such an operator is defined as follows: f~f equalsfiff= -, it equalsf if f= -, and it is undefined otherwise.",6,7
16101,1826534,"Add state s~ [n, ldot, left, rdot, rpos, i', i, -] to 2Given a generic set ;1, the symbol P(.,q) denotes the set of all the subsets of .,~ (the power set of ,~).",30,31
16102,1826534,"In order to simplify the computation, we have somewhat reduced the initial tree a and we have considered the constituent PP as a terminal symbol.",25,26
16103,219300218,"Or do people think about mutual belief via some simple internal symbol, analogous to a modal operator?",11,12
16104,219300218,"In this last case, does the symbol stand for the official notion of mutual belief, or is that notion just an idealization of a much more commonsensical notion that people think in terms of?",7,8
16105,10165340,"Levelt might well have added ""from the symbol-manipulation school of computer science.""",8,9
16106,11277320,"We manually add the wild card symbol to our mapping of tokens to integers, and map it to the integer 0, so that a search for a query with a wild card symbol would be unsuccessful but would point to the first record in the file that replaces the wild card symbol with a real token as the key for the wild card symbol is guaranteed to be the smallest.",6,7
16107,11277320,"We manually add the wild card symbol to our mapping of tokens to integers, and map it to the integer 0, so that a search for a query with a wild card symbol would be unsuccessful but would point to the first record in the file that replaces the wild card symbol with a real token as the key for the wild card symbol is guaranteed to be the smallest.",34,35
16108,11277320,"We manually add the wild card symbol to our mapping of tokens to integers, and map it to the integer 0, so that a search for a query with a wild card symbol would be unsuccessful but would point to the first record in the file that replaces the wild card symbol with a real token as the key for the wild card symbol is guaranteed to be the smallest.",53,54
16109,11277320,"We manually add the wild card symbol to our mapping of tokens to integers, and map it to the integer 0, so that a search for a query with a wild card symbol would be unsuccessful but would point to the first record in the file that replaces the wild card symbol with a real token as the key for the wild card symbol is guaranteed to be the smallest.",65,66
16110,7032506,"We have listed five atomic event structures below, along with their symbol.",12,13
16111,16724404,"Other DG formalisms, such as the one proposed by Gaifman (1965) and Hays (1964) , mark the position of the head among its dependents by a special symbol in the body.",32,33
16112,16724404,"The DUG parser can be adapted to follow this convention by accepting the symbol self in the rule body as in n(sleep, noun(N)) :> n(_, noun(N)), self.",13,14
16113,1678415,If R is a relation symbol for an n-place predicate and x 1 . . .,5,6
16114,38837143,"Oliphant addresses the trivial case of agents with a fixed, finite set of unstructured meanings, each of which is to be paired with a single symbol from another finite set.)",27,28
16115,38837143,"I found the various WBHK mechanisms interesting-though I see much further work required to explain the biological evolution whereby hominids came to have mechanisms for mapping open sets of meanings to symbol strings, and I would like to see models exploring how language might guide the discovery of concepts, rather than expressing concepts that are already built in, let alone formalized.",33,34
16116,51996995,"In this method, two vocabularies are used: training vocabulary and symbol vocabulary.",12,13
16117,51996995,Words in training vocabulary are charactersequences followed by an end-of-word symbol.,14,15
16118,51996995,"At first, all characters are added to symbol vocabulary.",8,9
16119,51996995,"This step is followed by adding the most frequent symbol bigram to the vocabulary, and all its occurrences are replaced by a new symbol (merged symbol bigram).",9,10
16120,51996995,"This step is followed by adding the most frequent symbol bigram to the vocabulary, and all its occurrences are replaced by a new symbol (merged symbol bigram).",24,25
16121,51996995,"This step is followed by adding the most frequent symbol bigram to the vocabulary, and all its occurrences are replaced by a new symbol (merged symbol bigram).",27,28
16122,51996995,"Starting from character level as the number of merge operations is increased, primarily frequent character-sequences and then full words are also added as a single symbol.",28,29
16123,51996995,Hyperparameter selection of BPE Higher number of merge operations adds almost all words to symbol vocabulary.,14,15
16124,51996995,Step 2: Find and replace all occurrences of these words with their segmented form (symbol '**' is used to keep information of segmenting positions).,16,17
16125,51996995,Use symbol '@@' for these segmentations.,1,2
16126,51996995,"It will not merge already segmented subwords followed by symbol'**', because it'll treat already segmented subwords as different elements.",9,10
16127,51996995,Step 4: Replace symbol '**' with the symbol '@@'.,4,5
16128,51996995,Step 4: Replace symbol '**' with the symbol '@@'.,11,12
16129,219303097,This class has the following property: the number of null constituents preceding a symbol in a sentence of the language cannot in general be inferred using bounded lookahead.,14,15
16130,9830226,"Let w =-dld2""""dn, n > 0, be a string over some alphabet; symbol pWq denotes the substring dpdp+l ... dq for 1 _< p < q < n and is undefined otherwise.",16,17
16131,9830226,"Non-null element aik is mapped into an auxiliary tree ""~1 having its root (and foot node) labeled by a symbol including integers i1 and kl.",24,25
16132,9830226,String xl is the smallest substring of w including the symbol in the /3th position within slice w (1) and the symbol in the k3th position within slice w (2).,10,11
16133,9830226,String xl is the smallest substring of w including the symbol in the /3th position within slice w (1) and the symbol in the k3th position within slice w (2).,23,24
16134,9830226,"Furthermore, string yl is the smallest substring of w including the symbol in the (k2 + 1)-th position within slice w (5) and the symbol in the /2th position within slice w (6).",12,13
16135,9830226,"Furthermore, string yl is the smallest substring of w including the symbol in the (k2 + 1)-th position within slice w (5) and the symbol in the /2th position within slice w (6).",28,29
16136,9830226,"At the same time 5 r maps non-null element bk,j into an auxiliary tree 3'2 having its root labeled by a symbol including integers k~ and jl.",25,26
16137,9830226,String x2 is the smallest substring of w including the symbol in the (k~ + 1)-th position within slice w (2) and the symbol in the j3th position within slice w (3).,10,11
16138,9830226,String x2 is the smallest substring of w including the symbol in the (k~ + 1)-th position within slice w (2) and the symbol in the j3th position within slice w (3).,26,27
16139,9830226,"Furthermore, string y2 is the smallest substring of w including the symbol in the j2th position within slice w (4) and the symbol in the k~th position within slice w (s).",12,13
16140,9830226,"Furthermore, string y2 is the smallest substring of w including the symbol in the j2th position within slice w (4) and the symbol in the k~th position within slice w (s).",25,26
16141,9830226,"At the same time, the equality test on the least significative digits obtained from the intermediate indices has been transferred to the requirement on the matching of the derived string with w. In fact, after the adjunction of ""/1 and ""/2 into "")'3 takes place, no terminal symbol can intervene between the internal boundaries of the yield of ""Y1 and the external boundaries of the yield of ""Y2 in slices w (2) and w (s) (see again Figure 7b ).",55,56
16142,9830226,The fact that these indices have range in {1..n} forces the choice of the length of these slices to ~r = n + 1; the example in Figure 7 actually uses the (n + 1)-th symbol of w (2).,41,42
16143,9830226,"We first observe that, in order to derive any terminal symbol from '/4, auxiliary trees in F~ n), F~ n) and F~ n) must be used.",11,12
16144,9830226,"As a second step, we observe that 73 can be adjoined into 74 only if u' =fl(i) and v' = A q) and 3'3 can host 71 and ""/2 just in case u' = u, v' = v, and k I = k~ = t. We also observe that, after these adjunctions take place, the leftmost terminal symbol in the yield of 3'4 will be the leftmost terminal symbol in the yield of 71, that is dp.",70,71
16145,9830226,"As a second step, we observe that 73 can be adjoined into 74 only if u' =fl(i) and v' = A q) and 3'3 can host 71 and ""/2 just in case u' = u, v' = v, and k I = k~ = t. We also observe that, after these adjunctions take place, the leftmost terminal symbol in the yield of 3'4 will be the leftmost terminal symbol in the yield of 71, that is dp.",81,82
16146,9830226,Such a grammar can be obtained from families F~ n) and F~ n) defined in Section 3 by deleting the integer components in each nonterminal symbol.),27,28
16147,6648670,"It consists of an ordinary PDA augmented with a special stack symbol, which is de-noted ~, and a special type of instruction to check for reduplication.",11,12
16148,6648670,"The symbol $ is inserted in the stack just like any other stack symbol, and the stack grows above this symbol just as in an ordinary PDA.",1,2
16149,6648670,"The symbol $ is inserted in the stack just like any other stack symbol, and the stack grows above this symbol just as in an ordinary PDA.",13,14
16150,6648670,"The symbol $ is inserted in the stack just like any other stack symbol, and the stack grows above this symbol just as in an ordinary PDA.",21,22
16151,6648670,"To check for an occurrence of a simple reduplication ww, the RPDA pushe,; $ onto the stack and then pushes the first w onto the stack symbol by symbol.",29,30
16152,6648670,"To check for an occurrence of a simple reduplication ww, the RPDA pushe,; $ onto the stack and then pushes the first w onto the stack symbol by symbol.",31,32
16153,6648670,"At that point the stack contains + with w on top of it, but while the stack symbols are ordered so that it would be easy to compare the w in the stack with w n (i.e., w reversed), they are in the wrong order to compare them with w. To overcome this; problem an RPDA is allowed, in one step, to corapare the entire string above the $ to an initial segment of the remaining input and to do so in the order starting at the special symbol ~ rather than at the top of the stack. (",96,97
16154,6648670,"One way to view this is to say that the RPDA can decide to treat the stack above the symbol like a queue, but once it decides to do so, all that it can do is empty the queue.",19,20
16155,6648670,"While placing symbols on top of the symbol $, the stack may grow or shrink just like the stack on an ordinary PDA.",7,8
16156,6648670,"Without this, or some similar generality, the only reduplication allowed would be exact symbols by symbol identity.",17,18
16157,6648670,"A Reduplication PDA (abbreviated RPDA) consists of the following items: (i) A finite set of states S, an element qo in S to serve as the start state, and a finite subset F of S to serve as the accepting states; (ii) A finite input alphabet E; (iii) A finite pushdown store alphabet Fsuch that E C F, a distinguished symbol Z o in F to serve as the start pushdown symbol, and a distinguished stack marker ~, which is an element of F-E; (iv) A transition function 8 that maps triples (q, a, Z) consisting of a state q, an input symbol a, and a pushdown store symbol Z, onto a finite set of instructions, where each instruction is in one of the following two forms: (1) An ordinary PDA move: (p, push a, A), where p is a state, a is a string of pushdown symbols, and A is one of the two instructions + 1 and 0 standing for ""advance the input head"" and ""do not advance the input head,"" respectively. (",74,75
16158,6648670,"A Reduplication PDA (abbreviated RPDA) consists of the following items: (i) A finite set of states S, an element qo in S to serve as the start state, and a finite subset F of S to serve as the accepting states; (ii) A finite input alphabet E; (iii) A finite pushdown store alphabet Fsuch that E C F, a distinguished symbol Z o in F to serve as the start pushdown symbol, and a distinguished stack marker ~, which is an element of F-E; (iv) A transition function 8 that maps triples (q, a, Z) consisting of a state q, an input symbol a, and a pushdown store symbol Z, onto a finite set of instructions, where each instruction is in one of the following two forms: (1) An ordinary PDA move: (p, push a, A), where p is a state, a is a string of pushdown symbols, and A is one of the two instructions + 1 and 0 standing for ""advance the input head"" and ""do not advance the input head,"" respectively. (",85,86
16159,6648670,"A Reduplication PDA (abbreviated RPDA) consists of the following items: (i) A finite set of states S, an element qo in S to serve as the start state, and a finite subset F of S to serve as the accepting states; (ii) A finite input alphabet E; (iii) A finite pushdown store alphabet Fsuch that E C F, a distinguished symbol Z o in F to serve as the start pushdown symbol, and a distinguished stack marker ~, which is an element of F-E; (iv) A transition function 8 that maps triples (q, a, Z) consisting of a state q, an input symbol a, and a pushdown store symbol Z, onto a finite set of instructions, where each instruction is in one of the following two forms: (1) An ordinary PDA move: (p, push a, A), where p is a state, a is a string of pushdown symbols, and A is one of the two instructions + 1 and 0 standing for ""advance the input head"" and ""do not advance the input head,"" respectively. (",128,129
16160,6648670,"A Reduplication PDA (abbreviated RPDA) consists of the following items: (i) A finite set of states S, an element qo in S to serve as the start state, and a finite subset F of S to serve as the accepting states; (ii) A finite input alphabet E; (iii) A finite pushdown store alphabet Fsuch that E C F, a distinguished symbol Z o in F to serve as the start pushdown symbol, and a distinguished stack marker ~, which is an element of F-E; (iv) A transition function 8 that maps triples (q, a, Z) consisting of a state q, an input symbol a, and a pushdown store symbol Z, onto a finite set of instructions, where each instruction is in one of the following two forms: (1) An ordinary PDA move: (p, push a, A), where p is a state, a is a string of pushdown symbols, and A is one of the two instructions + 1 and 0 standing for ""advance the input head"" and ""do not advance the input head,"" respectively. (",135,136
16161,6648670,"The top symbol is at the left end of 3"", which is the same convention as that normally used for ordinary PDA's.)",2,3
16162,6648670,"ffp, a, Z); (p, aw, Za) F (q, aw, /3a), provided (q, push/3, O) 8(p, a, Z) ; (p, axw, Z3"" ~ a) F (q, w, a), provided q E 8 (p, a, Z) and ax = (Z3,) R, where (Z3') R denotes Z3"" written backwards (so the a matches the symbol just above the stack marker $.",94,95
16163,6648670,"Note that Z3' cannot contain the symbol $, because $ is not in the input alphabet).",8,9
16164,6648670,"If the center of the string is marked with a punctuation symbol, then the RPDA can be deterministic, so L~ is a deterministic RPDA language.",11,12
16165,6648670,The language L2 illustrates the fact that reduplication need not be symbol-by-symbol identity.,11,12
16166,6648670,The language L2 illustrates the fact that reduplication need not be symbol-by-symbol identity.,15,16
16167,6648670,"For example, af'~ ~, 'y can be transformed into fla $ T by moving one symbol at a time from the top of the stack to just above the n~tarker $.",18,19
16168,6648670,"The top symbol is moved by remembering the symbol in the finite-control, flipping the stack, placing the remembered symbol on the stack, and flipping again.",2,3
16169,6648670,"The top symbol is moved by remembering the symbol in the finite-control, flipping the stack, placing the remembered symbol on the stack, and flipping again.",8,9
16170,6648670,"The top symbol is moved by remembering the symbol in the finite-control, flipping the stack, placing the remembered symbol on the stack, and flipping again.",22,23
16171,6648670,A realistic model would have to read input one symbol per unit time.,9,10
16172,6648670,"In the case of an RPDA this is a bit more complicated, since the RPDA does not read its input symbol by symbol.",21,22
16173,6648670,"In the case of an RPDA this is a bit more complicated, since the RPDA does not read its input symbol by symbol.",23,24
16174,6648670,"In one move it does all of the following: either read a symbol or move without consuming any input (called moving on the empty input) and then, on the basis of this symbol or the empty input, as well as the state of the finite-state machine, it changes state and outputs a string of symbols. (",13,14
16175,6648670,"In one move it does all of the following: either read a symbol or move without consuming any input (called moving on the empty input) and then, on the basis of this symbol or the empty input, as well as the state of the finite-state machine, it changes state and outputs a string of symbols. (",36,37
16176,6648670,This change of English can be obtained by first replacing all plural nouns with a special symbol (a in the definition) and then performing a reduplication substitution as described in the definition.,16,17
16177,6648670,"Let L be a language, a a symbol, and T a finite-state transduction.",8,9
16178,6648670,"Define the language L' to A Formal ldodd for Context-Free Languages Augmented with Reduplication consist of all strings w that can be written in the form Xo Yo xl Yl """"Xn -1 Yn -I Xn where (i) each xi contains no a, s (ii) Xo axl a """"Xn -I ax, E L, and (iii) each Yi is of the form vv' where v' is a finite-state transduction of v via T. A language L', obtained in this way, is called a reduplication substitution of the language L via T by substituting reduplication strings for a. More simply, L' is called a reduplication substitution of L provided there is some symbol a and some such finite-state transduction T such that L' can be obtained from L in this way.",134,135
16179,6648670,"Without loss of generality, we will assume that M pushes at most one symbol onto the stack during each move.",14,15
16180,6648670,"A reduplication context-free grammar (RCFG) is a grammar consisting of terminal, nonterminal, and start symbols as in an ordinary context-free grammar, but instead of a finite set of productions, it has a finite set of rule schemata of the following form: (A --~ ct, T) where A is a nonterminal symbol, a is a string of terminal and/or nonterminal symbols, and where T is a finite-state transducer. (",64,65
16181,6648670,"As usual, ~ is the reflexivetransitive closure of ~. The language generated by an RCFG, G, is defined and denoted in the usual way: L(G) = { w L w a string of terminal symbols and S ~ w}, where S is the start symbol.",51,52
16182,6648670,Recall that a context-free grammar in Greibach Normal Form is one in which each production is of the form A --~ act where a is a terminal symbol and ct is a string consisting entirely of nonterminals.,29,30
16183,6648670,"The schemata described in the definition of RCFGs have some similarity to context-free rules in Greibach Normal Form, except that they start with a reduplication string, rather than a single terminal symbol, and the remaining string may contain terminal symbols.",35,36
16184,6648670,"Let G' be a context-free grammar, T a finitestate transduction and a a symbol such that L is obtained from L(G') via T by substituting reduplication strings for a. Without loss of generality, we can assume that G' is in Greibach Normal Form.",17,18
16185,6648670,"To obtain G from G' we replace each G' rule of the form A --~ aA 1 A 2 ...A n, where a is the symbol used for the reduplication substitution, by the schema (A --~ Al A2 """"An, T) The remaining rules of G' are left unchanged except for the technicality of adding a finite-state transduction that accepts only the empty string as input and output, and so leaves the rule unchanged for purposes of generation.",29,30
16186,6648670,"A simple RPDA is an RPDA such that, in any computation: (i) there is at most one occurrence of the marker $ in the stack at any one time, and (ii) as long as the marker symbol $ is in the stack, the RPDA never removes a symbol from the stack.",43,44
16187,6648670,"A simple RPDA is an RPDA such that, in any computation: (i) there is at most one occurrence of the marker $ in the stack at any one time, and (ii) as long as the marker symbol $ is in the stack, the RPDA never removes a symbol from the stack.",55,56
16188,6648670,Pushes the marker symbol $ onto the stack.,3,4
16189,6648670,"If the top symbol is a terminal and there is no ~ in the stack, then it simply matches the stack symbol to the input symbol, consuming both the stack symbol and the input symbol.",3,4
16190,6648670,"If the top symbol is a terminal and there is no ~ in the stack, then it simply matches the stack symbol to the input symbol, consuming both the stack symbol and the input symbol.",22,23
16191,6648670,"If the top symbol is a terminal and there is no ~ in the stack, then it simply matches the stack symbol to the input symbol, consuming both the stack symbol and the input symbol.",26,27
16192,6648670,"If the top symbol is a terminal and there is no ~ in the stack, then it simply matches the stack symbol to the input symbol, consuming both the stack symbol and the input symbol.",32,33
16193,6648670,"If the top symbol is a terminal and there is no ~ in the stack, then it simply matches the stack symbol to the input symbol, consuming both the stack symbol and the input symbol.",36,37
16194,6648670,"Conversely, suppose that M is a simple RPDA such that L(M) = L. Without loss of generality, we will assume that M always pushes at least one symbol on the stack after pushing the marker symbols $, that every marker symbol $ on the stack is eventually used in a copy-check move, and that the marker symbol ~ is not left in the stack at the end of any accepting computation.",30,31
16195,6648670,"Conversely, suppose that M is a simple RPDA such that L(M) = L. Without loss of generality, we will assume that M always pushes at least one symbol on the stack after pushing the marker symbols $, that every marker symbol $ on the stack is eventually used in a copy-check move, and that the marker symbol ~ is not left in the stack at the end of any accepting computation.",44,45
16196,6648670,"Conversely, suppose that M is a simple RPDA such that L(M) = L. Without loss of generality, we will assume that M always pushes at least one symbol on the stack after pushing the marker symbols $, that every marker symbol $ on the stack is eventually used in a copy-check move, and that the marker symbol ~ is not left in the stack at the end of any accepting computation.",63,64
16197,6648670,"M' is defined as follows: M' has all the input symbols of M plus one new symbol, denoted <q, p>, for each pair of M states (q, p).",19,20
16198,6648670,"Intuitively, a new symbol <q, p> is used to stand in for a reduplication string that M would process starting in state q and ending up in state p after a successful check-copy move.",4,5
16199,6648670,"IfM' reads a new symbol <q, p>, and M' is simulating M in the state q, then M' guesses an input symbol a of M and simulates M on input a. If M would consume the input symbol a without pushing the marker ~ on the stack, then M' aborts its computation.",5,6
16200,6648670,"IfM' reads a new symbol <q, p>, and M' is simulating M in the state q, then M' guesses an input symbol a of M and simulates M on input a. If M would consume the input symbol a without pushing the marker ~ on the stack, then M' aborts its computation.",29,30
16201,6648670,"IfM' reads a new symbol <q, p>, and M' is simulating M in the state q, then M' guesses an input symbol a of M and simulates M on input a. If M would consume the input symbol a without pushing the marker ~ on the stack, then M' aborts its computation.",45,46
16202,6648670,"Remember that, intuitively, a new symbol <q, p> is used to stand in for a reduplication string that M would process starting in state q and ending up in state p after a successful check-copy move.",7,8
16203,13422571,"For example, the substring ough is pronounced /o~5/ when its left context is th in the word although,/u/when its left context is thr in the word through, and /Af/ when its left context is en in the word enough: in each case, the right context is the word delimiter symbol.",55,56
16204,13422571,"The ""-"" symbol is the null phoneme, introduced to give a strict one-to-one alignment between letters and phonemes to satisfy the training requirements of NETtalk.",4,5
16205,219301355,This is the problem of finding the referent of a symbol in one modality using information present either in the same or in other modalities.,10,11
16206,219301355,"The denotation of the symbol rl in G that is related to the word France in L through PG-L, and to a particular region in P through pG-P, is also France, as translation is a meaning-preserving relation between expressions.",4,5
16207,219301355,"For the production of emergent objects in P there is a well-defined computational geometry algorithm associated with an operator symbol of G, as will be seen below.",21,22
16208,201747457,"Marconi's answer to Searle is that, yes, the Chinese room is merely doing symbol processing without understanding, but this does not preclude there being some linguistic competence.",16,17
16209,7046575,"We use the letters A, B, C to denote atomic categories, the letters X, Y, Z to denote arbitrary categories, and the symbol | to denote slashes (forward or backward).",28,29
16210,7046575,"We use the symbol as a placeholder for that part of a primary input category that is unconstrained by rule restrictions, and therefore may consist of an arbitrary sequence of arguments.",3,4
16211,7046575,"These trees can be characterized as rooted, ordered trees in which internal nodes are labeled with nonterminal symbols-including a distinguished start symbol S-and leaf nodes are labeled with nonterminals, terminals, or the empty string.",24,25
16212,7046575,"The vocabulary of G is the set of all terminal symbols of G; the set of atomic categories consists of all symbols of the form A t , where either A is a nonterminal symbol of G and t ∈ {a, c}, or A is a terminal symbol of G and t = a. The distinguished atomic category of G is S a , where S is the start symbol of G. Lexicon.",35,36
16213,7046575,"The vocabulary of G is the set of all terminal symbols of G; the set of atomic categories consists of all symbols of the form A t , where either A is a nonterminal symbol of G and t ∈ {a, c}, or A is a terminal symbol of G and t = a. The distinguished atomic category of G is S a , where S is the start symbol of G. Lexicon.",52,53
16214,7046575,"The vocabulary of G is the set of all terminal symbols of G; the set of atomic categories consists of all symbols of the form A t , where either A is a nonterminal symbol of G and t ∈ {a, c}, or A is a terminal symbol of G and t = a. The distinguished atomic category of G is S a , where S is the start symbol of G. Lexicon.",74,75
16215,7046575,"Additionally, for each terminal symbol x of G, one constructs a lexicon entry x := x a .",5,6
16216,7046575,The only categories that qualify as secondary input categories of the new instances are atomic categories of the form B c where B is a nonterminal of the TAG G. Now the lexical categories of G either are of the form x a (where x is a terminal symbol) or are non-atomic.,49,50
16217,7046575,"As the terminal alphabet of H we choose the vocabulary of G; as the nonterminal alphabet we choose the set K; and as the start symbol we choose the distinguished atomic category S. Every transformed derivation of G corresponds (in an obvious way) to some derivation in H , which proves that Y ⊆ L(H ).",27,28
16218,15136612,"A linear context-free rewriting system (LCFRS) is a structure G = (N, Σ, P, S) where N is a set of nonterminals, Σ is a set of function symbols, P is a finite set of production rules, and S ∈ N is a distinguished start symbol.",57,58
16219,15136612,A m ) ( 1 ) where f is a function symbol and the A i are nonterminals.,11,12
16220,15136612,"For this, every function symbol f comes with a yield function that specifies how to compute the yield of a term f (t 1 , . . . ,",5,6
16221,15136612,"We call this tuple the template of the yield function f , and use it as the canonical function symbol for f .",19,20
16222,15136612,"Formally, precedence is the lexicographical order on occurrences: (i 1 , j 1 ) < (i 2 , j 2 ) if and only if either i 1 < i 2 or (i 1 = i 2 and j 1 < j 2 ) Operations on Dependency Trees A yield function f is called lexicalized if its template contains exactly one yield symbol, representing a lexical item; this symbol is then called the anchor of f .",67,68
16223,15136612,"Formally, precedence is the lexicographical order on occurrences: (i 1 , j 1 ) < (i 2 , j 2 ) if and only if either i 1 < i 2 or (i 1 = i 2 and j 1 < j 2 ) Operations on Dependency Trees A yield function f is called lexicalized if its template contains exactly one yield symbol, representing a lexical item; this symbol is then called the anchor of f .",75,76
16224,15136612,"The fan-out of a nonterminal is the fan-out of g. The agenda is initialized with the pair (S, x ) where x is the identity function; this pair also represents the start symbol of G .",39,40
16225,15136612,"By this item, there exists a term that can be derived from the start symbol S and yields the full string w .",15,16
16226,15136612,"There is one rather complex rule to rewrite the start symbol S; this rule sets up the general topology of w. Let I be the m × n matrix with entries I i,j = (j − 1)m + i. Define x 1 to be the sequence of variables of the form x h,1 , where the argument index i is taken from a row-wise reading of the matrix I; in this case, the argument indices in x will simply go up from 1 to mn.",10,11
16227,8137902,"Like any generative grammar, it describes a language by means of the rules of an algorithm that reads the input string symbol by symbol and at some moment either accepts it as grammatical or rejects it as ungrammatical.",22,23
16228,8137902,"Like any generative grammar, it describes a language by means of the rules of an algorithm that reads the input string symbol by symbol and at some moment either accepts it as grammatical or rejects it as ungrammatical.",24,25
16229,8137902,"After a symbol is read from the input, a rule is sought that allows it to be accepted given the current internal state (the whole contents of the tape).",2,3
16230,8137902,"The manner in which the rules decide whether or not the new symbol is compatible with the current state is not specified by the definition of the LA-formalism, the only requirement being that the corresponding Boolean function be recursive, i.e., computable in principle.",12,13
16231,5574231,r S ∈ L is a start symbol.,7,8
16232,5574231,"nonterminal A of the rule, matrix multiplication will produce an entry in the matrix cell where the row address corresponds to the endpoints from B, and the column address corresponds to the endpoints from C. To capture this partition of the endpoints of A, we define config 1 (r) = {2i | α i,n i = β j for some j} ∪ {2i − 1 | α i,1 = β j for some j} where the first set defines right ends of spans of A that are formed from B, and the second set defines left ends of spans of A that are formed from B. For example, any CFG rule r has configuration, config 1 (r) = {1}, because only the first endpoint of A is derived from B. For the TAG rule t shown in Figure 1 , config 1 (t) = {1, 4} because, of A's four endpoints, the first and fourth are derived from B. Definition of Multiplication Operator We need to define a multiplication operator ⊗ between a pair of elements R, S ⊂ M. Such a multiplication operator induces multiplication between matrices of the type of T, just by defining for two such matrices, T 1 and T 2 , a new matrix of the same size T 1 ⊗ T 2 such that: [T 1 ⊗ T 2 ] ij = k∈N [T 1 ] ik ⊗ [T 2 ] kj (5) We also use the ∪ symbol to denote coordinate-wise union of cells in the matrices it operates on.",280,281
16233,5574231,"The first multiplication involves the nonterminal A in cell (i, j) in the left matrix, and a symbol in cell (j, insert(j, x)) in the right matrix, resulting in a matrix with nonterminal A in cell            T 1 (2, 7) (1, 8) {. . . ,",21,22
16234,5574231,"To remove x from the row address, we multiply on the left with a matrix containing the symbol in cell (remove(i, x), i), resulting in a matrix with nonterminal A in cell (remove(i, x), insert(j, x)).",18,19
16235,5574231,"Finally, we multiply by a third matrix to replace the marked index x with the unmarked index x. This is done by multiplying on the right with a matrix containing the symbol in cell (insert(j, x), insert(j, x)).",32,33
16236,5574231,"In order to guarantee that our operations copy nonterminals only into cells with equivalent addresses, the seed matrix contains the special symbol only in cells (j, k) such that k = insert(j, x) for some x. When in cell (j, k) combines with a nonterminal A in cell (i, j), the result contains A only if x ∈ i, guaranteeing that the index added to the column address was originally present in the row address.",22,23
16237,5574231,These matrices indicate the positions in which each symbol appears in the seed matrix T defined in Figure 4 : 1.,8,9
16238,5574231,"Finally, it checks whether the start symbol appears in a cell with an address that spans the whole string.",7,8
16239,5574231,"Production rules with LIGs copy the stack on the left-hand side to one of the nonterminal stacks in the right-hand side, 5 potentially pushing or popping one symbol in the new copy of the stack.",32,33
16240,5574231,"Once the transitive closure of T is computed, we can backtrack to find such a parse, starting with the start symbol in a cell spanning the whole string.",22,23
16241,248095,"In contrast, we present the structural tag representation, where the symbol representing the word simultaneously represents the classification of that word (McMahon and Smith [1994] make connections between this and other representations; Black et al. [",12,13
16242,1531965,This model is also known as symbol-refined HMM (SR-HMM).,6,7
16243,1531965,"For example, the NR tag may be split into NR-1 and NR-2, and the corresponding symbol-refined tag sequence for ""Mr./NR Smith/NR saw/VV Ms./NR Smith/NR"" can be denoted as ""Mr./NR-2 Smith/NR-1 saw/VV-2 Ms./NR-2 Smith/NR-1.""",17,18
16244,1531965,The objective of training a symbol-refined bigram tagger is to solve the LA-involved emission and transition parameters by maximizing the likelihood of the training data.,5,6
16245,1531965,"In contrast with a non-symbol-refined HMM tagger, where the POS tags are observed, the latent annotations are unseen variables.",6,7
16246,1531965,"In this table, the symbol ""+"" in the Features column means that the current configuration contains both the baseline features and new cluster-based features; the number is the total number of the clusters; the number in the #Sent column means how many millions of raw sentences are used to cluster words.",5,6
16247,2280293,"A PTSG is a 5-tuple V, Σ, R, ♦, θ where c ∈ V are non-terminals; t ∈ Σ are terminals; e ∈ R are elementary trees; 5 ♦ ∈ V is a unique start symbol; and θ c,e ∈ θ are parameters for each tree fragment.",46,47
16248,6267446,"For each (x, y) ∈ E max t , we assign a new label to (x, y) as follows, Case 1: x → g y, add the original label in G(V, E) to the new edge x → t y; Case 2: y → g x, add the original label with symbol R to x → t y; Case 3: x → g y ∧ y → g x, add label as Case 1; Case 4: x g y ∧ y x, add label None to the edge x → t y. To improve the coverage of original edges, a variant model with modified labels in trees to help encode more edges in graphs.",65,66
16249,6267446,x is great-grandparent node of y: New label is the label of x → g y following the symbol 'g'. •,21,22
16250,6267446,x is grandparent node of y: New label is the label of x → g y following the symbol 'f '. •,19,20
16251,6267446,"If the two siblings are on the same side of z, P will be the distance of the two siblings' positions in the sorted children sequence and extra symbol will be 'y'.",30,31
16252,6267446,"If the two siblings are on the different sides of z, extra symbol will be 'n' and P will be x's rank in the same side's nodes in the sorted children sequence.",13,14
16253,6267446,"New label is the symbol 'b' with extra symbol followed and the label of x → g y. If node y is higher than x in the dependency tree, we would add symbol R to indicate the additional edge is reversed.",4,5
16254,6267446,"New label is the symbol 'b' with extra symbol followed and the label of x → g y. If node y is higher than x in the dependency tree, we would add symbol R to indicate the additional edge is reversed.",10,11
16255,6267446,"New label is the symbol 'b' with extra symbol followed and the label of x → g y. If node y is higher than x in the dependency tree, we would add symbol R to indicate the additional edge is reversed.",35,36
16256,8400322,"An LCFRS is defined as a tuple G = (V T , V N , P, S), where V T is a set of terminal symbols, V N is a set of nonterminal symbols, P is a set of productions, and S ∈ V N is a distinguished start symbol.",56,57
16257,14586568,"The symbol grounding problem, that is, the problem of mapping symbols to their counterparts in the real world, is not considered here, because it affects all languages, including both natural and formal ones.",1,2
16258,14586568,"These are languages that do not look natural, making heavy use of symbol characters, brackets, or unnatural keywords.",13,14
16259,1344525,"The beginning or end of the input word is indicated by the symbol ""o"".",12,13
16260,1344525,"The operation obtaining one or more strings is denoted by the symbol ""+'.",11,12
16261,14241519,"In our proposal, we memo only those categories that are maximal projections, i.e., projections of a head that unify with the top category (start symbol) or with a nonhead daughter of a rule.",28,29
16262,14241519,The start symbol of this grammar is nt6.,2,3
16263,14241519,"Root nodes have a nonterminal symbol before the colon, and the corresponding rule identifier after the colon.",5,6
16264,14241519,"It is assumed that state names are integers; to rule out cyclic word-graphs we also require that, for all transitions from P0 to P, it is the case that P0 < P. Transitions in the word-graph are represented by clauses of the form wordgraph:trans (P0, Sym, P, Score), which indicate that there is a transition from state P0 to P with symbol Sym and acoustic score Score.",76,77
16265,14241519,The parser is modified in such a way that it finds all derivations of the start symbol anywhere in the input.,16,17
16266,14241519,"Furthermore, the start symbol should be defined in such a way that it includes all categories considered useful for the application.",4,5
16267,14241519,The top category (start symbol) of the OVIS grammar is defined as the category max (gem).,5,6
16268,14241519,"As explained in the previous section on robustness, each of the parsers finds all derivations of the start symbol anywhere in the input (this is the case in each of the OVIS experiments).",19,20
16269,14241519,Both the left-corner and head-corner parser use a goal-weakening operator that only leaves the functor symbol.,21,22
16270,14241519,The goal-weakening operator used for the left-corner and head-corner parser removes all features (only leaving the functor symbol of each category); again this simplifies the maintenance of the goal table considerably.,24,25
16271,15852664,"Then 7 itself is the minimal Lisp expression in (1) containing only the internal symbol $screw, and (so (fo $screw ,)) is the maximal Lisp expression in (1) containing only the internal symbol $screw.",16,17
16272,15852664,"Then 7 itself is the minimal Lisp expression in (1) containing only the internal symbol $screw, and (so (fo $screw ,)) is the maximal Lisp expression in (1) containing only the internal symbol $screw.",43,44
16273,15852664,"The symbol s refers to an entire sentence, and correspondingly ¢ to an entire internal representation.",1,2
16274,15852664,"On any trial, let s be associated to c~, let a be in the set of words of s not associated to any internal symbol of a, and let ~ be in the set of internal symbols not currently associated with any word of s. Then pairs (a, a) are sampled, possibly using the current denotational value, and associated, i.e. a ~ ~. The probabilistic sampling in the case Get the screw could lead to the incorrect associations get ~,, $screw, the ,,~ $get and no association for screw, for there are only two symbols to be associated to in the internal representation.",26,27
16275,15852664,"If at the end of trial n, a word a in the presented verbal stimulus is associated with some internal symbol or, then d(a), the denotational value of a, increases and if a is not so associated, d(a) decreases.",21,22
16276,15852664,"Moreover, if a word a does not occur on a trial, then d(a) stays the same unless the association of a to an internal symbol c~ is broken on the trial, in which case d(a) decreases.",27,28
16277,15852664,"c) If no internal representation is generated from the occurrence of a sentence s, cr is then given as the correct internal representation, and if there are several words in s associated to an internal symbol c~ of cr such that the number of occurrences of these words is greater than the number of occurrences of c~ in or, then these associations are deleted.",38,39
16278,15852664,"On any trial n, let s be associated to cr in accordance with Background Assumption 1, let A be the set of words of s not associated to any internal symbol of or, let d, (a) be the current denotational value of each such a in A and let A be the set of internal symbols not currently associated with any word of s. Then (i) (ii) an element a is uniformly sampled without replacement from A, at the same time, an element a is sampled without replacement from A with the sampling probability: (iii) The sampled pairs are associated, i.e. a ,-~ c~. (iv) Sampling continues until either the set A or the set A is empty.",32,33
16279,15852664,"b) If (i) wX is a substring of g with g ~ 3` such that w = a, which is a word with low denotational value, or if X is preceded by a variable, or is the first symbol of g, w = ~, the empty symbol, and (ii) 3`' (X) is the maximal Lisp form of 3` containing the occurrence of X and no other occurrence of categories, then: wX -3`'(x). (",45,46
16280,15852664,"b) If (i) wX is a substring of g with g ~ 3` such that w = a, which is a word with low denotational value, or if X is preceded by a variable, or is the first symbol of g, w = ~, the empty symbol, and (ii) 3`' (X) is the maximal Lisp form of 3` containing the occurrence of X and no other occurrence of categories, then: wX -3`'(x). (",55,56
16281,15852664,"We also want to note that in the Japanese case we deleted all the sentences translating $above as an internal symbol (ten sentences), because in Japanese above and on are expressed by the same word ue.",21,22
16282,15852664,This created an important problem: individual words in the corpus of a given natural language sometimes denoted more than one internal symbol.,22,23
16283,6752466,We treat the points of syntactic encoding of noun phrases as forward references that are temporarily maintained in a symbol table for later binding.,19,20
16284,6752466,"The trick is to delay the evaluation of encoding schema of constituent NPs till an appropriate moment, while maintaining a persistent data structure, such as a symbol table, to keep track of the points of forward reference (at which actual function names get instantiated) and their local environments (the internal f-structure of the constituent NPs).",28,29
16285,6752466,"metavariables generate placeholders for hitherto anonymous grammatical functions, which we shall call nameholders, and denote them by actual name variables nl, n2,.... Locate-ing of schemata (4) creates such a nameholder (n, say) in the scope of the functional placeholder (f, say) for the T metavariable and simultaneously stores the pair (f, n) in the symbol table.",72,73
16286,6752466,"Simultaneously, the pair (f, n) is entered as a new entry of the symbol table.",17,18
16287,6752466,"A symbol table entry (f, n) satisfies an m-structure schemata (# g qi) = vi projected by the verb V of a sentence S, iff is the f-structure of S, and the structure (f n), where n is treated as an atom, contains the pair [qi vi].",1,2
16288,6752466,"If a symbol table entry satisfies all m-structure schema for a function g, by our proposed scheme, the nameholder n that points to the entry is bound to the function name g. Also, the satisfying symbol table entry is deleted.",2,3
16289,6752466,"If a symbol table entry satisfies all m-structure schema for a function g, by our proposed scheme, the nameholder n that points to the entry is bound to the function name g. Also, the satisfying symbol table entry is deleted.",40,41
16290,6752466,Testing of symbol table entries with m-structure schema and resulting binding of nameholders to actual function names are carried out by a newly introduced operator Search.,2,3
16291,6752466,"If more than one symbol table entry satisfies the m-structure schema for a particular function g, the one earlier in order of occurrence is chosen.",4,5
16292,6752466,This would generate symbol table entries corresponding to NPs annotated with the ?,3,4
16293,6752466,"A sentence is well formed if and only if all the m-structure schema for the verb are satisfied and all nameholders in the scope of the sentence are bound to names (i.e., at the end, the symbol table is empty).",41,42
16294,2736514,"He used met* to refer collectively to metonymy and metaphor: ""*"" is a match-anything symbol in the Unix operating system; hence, the token ""met*"" matches the two tokens ""metonymy"" and ""metaphor.""",20,21
16295,1041782,"Cloned states are modified versions of the original states q ∈ Q − {⊥}: All of their outgoing transitions point to the corresponding intact states in Q , that is, (δ(q, a), ⊥ w ), except for the transition with symbol a : xa ∈ Pr(w), which now points to the corresponding cloned state (δ(q, a), xa), that is, δ ((q, x), a) = (δ(q, a), xa) if xa ∈ Pr(w) (δ(q, a), ⊥ w ) otherwise (5) Cloned states are in F if the corresponding original states are in F; in addition, if there is a cloned state of the form (q, w), then it is in F .",49,50
16296,1107343,The follower graph places directed edges between users who have chosen to follow each other's updates; the message graph places a directed edge between users who have addressed messages to each other (using the @ symbol).,38,39
16297,33921041,"These languages can be described by grammars in Chomsky normal form, that is, by context-free grammars G = (N, T, S, P) with nonterminal vocabulary N, terminal vocabulary T, and start symbol S where every rule in P is of the form A → BC with B, C ∈ N, or A → a with a ∈ T. For the proof we first define for each context-free grammar G in Chomsky normal form two LFG grammars that both derive L(G) and that associate with each derivable terminal string feature structures (f-structures) that provide slightly different encodings of the derivable string.",42,43
16298,213719646,"In erasure noise, a symbol x is stochastically erased (replaced with a special erasure symbol E) with some probability e. The noise model here further assumes that the erasure rate increases with time: I call this noise model progressive erasure noise.",5,6
16299,213719646,"In erasure noise, a symbol x is stochastically erased (replaced with a special erasure symbol E) with some probability e. The noise model here further assumes that the erasure rate increases with time: I call this noise model progressive erasure noise.",16,17
16300,213719646,"Each symbol m j , called a memory symbol, is equal either to the context word w j or to the erasure symbol E. The surprisal of a word w i given the memory representation m 1:i−1 can be written in two terms: − log p(w i |m 1:i−1 ) = − log p(w i ) − pmi(w i ; m 1:i−1 ), where pmi(w i ; m 1:i−1 ) = log p(w i |m 1:i−1 ) p(w i ) is the pointwise mutual information (Fano, 1961; Church and Hanks, 1990) of the word and the memory representation, giving the extent to which the particular memory representation predicts the particular word.",1,2
16301,213719646,"Each symbol m j , called a memory symbol, is equal either to the context word w j or to the erasure symbol E. The surprisal of a word w i given the memory representation m 1:i−1 can be written in two terms: − log p(w i |m 1:i−1 ) = − log p(w i ) − pmi(w i ; m 1:i−1 ), where pmi(w i ; m 1:i−1 ) = log p(w i |m 1:i−1 ) p(w i ) is the pointwise mutual information (Fano, 1961; Church and Hanks, 1990) of the word and the memory representation, giving the extent to which the particular memory representation predicts the particular word.",8,9
16302,213719646,"Each symbol m j , called a memory symbol, is equal either to the context word w j or to the erasure symbol E. The surprisal of a word w i given the memory representation m 1:i−1 can be written in two terms: − log p(w i |m 1:i−1 ) = − log p(w i ) − pmi(w i ; m 1:i−1 ), where pmi(w i ; m 1:i−1 ) = log p(w i |m 1:i−1 ) p(w i ) is the pointwise mutual information (Fano, 1961; Church and Hanks, 1990) of the word and the memory representation, giving the extent to which the particular memory representation predicts the particular word.",23,24
16303,213719646,"We can now use the chain rule to break the pointwise mutual information into separate terms, one for each symbol in the memory representation: pmi(w i ; m 1:i−1 ) = i−1 j=1 pmi(w i ; m j |m 1:j−1 ) = i−1 j=1 pmi(w i ; m j ) − j=1 pmi(w i ; m j ; m 1:j−1 ) = i−1 j=1 pmi(w i ; m j ) − R, (6) where pmi(x; y; z) is the three-way pointwise interaction information of three variables (Bell, 2003) , indicating the extent to which the conditional pmi(w i ; m j |m 1:j−1 ) differs from the unconditional pmi(w i ; m j ).",20,21
16304,213719646,"5, we get an expression for processing difficulty in terms of the pmi of each memory symbol with the current word: Difficulty(w i |w 1:i−1 ) ∝ E m|w 1:i−1 [− log p(w i |m)] (5) = E m|w 1:i−1   − log p(w i ) − i−1 j=1 pmi(w i ; m j ) + R   = − log p(w i ) − E m|w 1:i−1   i−1 j=1 pmi(w i ; m j ) + R   = − log p(w i ) − i−1 j=1 E m j |w j [pmi(w i ; m j )] + E m|w 1:i−1 [R] . (",17,18
16305,213719646,7) It remains to calculate the expected pmi of the current word and a memory symbol given the distribution of possible memory symbols.,16,17
16306,213719646,Recall that each m j is either equal to the erasure symbol E (with probability e i−j ) or to the word w j (with probability 1 − e i−j ).,11,12
16307,213719646,Therefore the expected pmi between a word w i and a memory symbol m j is (1 − e i−j )pmi(w i ; w j ).,12,13
16308,18307754,"A fundamental concept in linguistic knowledge is the predicate, by which we mean a word or other symbol that combines with one or more arguments to produce a composite representation with a composite meaning (by the principle of compositionality).",18,19
16309,219308087,"It is certainly rare for a book author to give himself ""ten tlhousand thanks"" for his own work on a standardization committee; and we do not accept the yin-yang-based symbol of the I Ching as evidence that the Chinese invented the fundamental theory of computation.)",36,37
16310,3220249,"Experiments for English CCG-grounded analysis were performed using automatically assigned POS-tags that are generated by a symbol-refined generative HMM tagger 3 (SR-HMM; Huang, Harper, and Petrov 2010) .",20,21
16311,16182950,"It is certainly rare for a book author to give himself ""ten tlhousand thanks"" for his own work on a standardization committee; and we do not accept the yin-yang-based symbol of the I Ching as evidence that the Chinese invented the fundamental theory of computation.)",36,37
16312,237102589,"3) a. P i → $ p i occurs in C j j=1,..,m (↑ C j ) = TRUE b. P i → $ ¬p i occurs in C j j=1,..,m (↑ C j ) = TRUE (The conjunction symbol is usually omitted.)",47,48
16313,248780323,"To handle out-of-vocabulary items, I include an unknown-word symbol UNK in the target word set W and full vocabulary V .",15,16
16314,14308836,"8 The cover symbol "" is used to permit the upstream reconstruction of Tukche forms in which the tone of the modern form is not precisely known.",3,4
16315,14308836,"The ""fuzzy file,"" which states that TGTM proto tones A and ~ should be equated to the ""cover symbol"" X, and that TGTM *k, *k h, and *g should be conflated into ""6, a velar stop, unspecified for aspiration and voicing.",22,23
16316,14308836,"The feature sets represent a statement of the significant distinctions in each language; we note in passing that RE thus implements the notion that the phonemes of each language pattern in their own way, and that even though the same symbol may be used to transcribe words in different languages, this does not necessarily indicate that they share features in common.",42,43
16317,5392356,"For example, a question mark will be identified as a punctuation symbol, but its function (question cue; end of a sentence) will not.",12,13
16318,36338,"The parameters satisfy the following normalization constraints: a,h p(a(h)) = 1, and for all a ∈ N and h ∈ [m]: a(h)→b(h 2 ) c(h 3 ) p(a(h) → b(h 2 ) c(h 3 )) + a(h)→x p(a(h) → x) = 1 Note that for simplicity, we consider the case where every nonterminal symbol has the same number m of hidden state values.",67,68
16319,36338,"One of the transformations introduced was that of ""parent annotation"" where each nonterminal is annotated with its parent symbol.",20,21
16320,236425703,"While these marks started out as logographic representations of words whose denotations could be easily depicted with drawings, the scribes who developed the first writing soon discovered the rebus principle, and started to use symbols not for what they depicted, but rather for how the symbol was pronounced.",48,49
16321,236425703,"Daniels (2018, p. 155) : The closest thing to a definition is here: ""logogram: a symbol (often a pictogram) denoting the meaning but not the pronunciation of a word or morpheme"" Handel (2019, pp.",21,22
16322,236425703,"This is more precise than the vague notion presented in the literature that a given word is written, at least in part, with a symbol that somehow represents that whole word.",26,27
16323,236425703,"Thus in the case at hand, the word pig that is synonymous with ingot (as in pig iron), should in that case be written with a different symbol, say .",31,32
16324,236425703,"A priori this seems unlikely to be able to discover the function of those symbols, any more than the distribution of symbols is likely to tell you that a given symbol system represents language (i.e., is a true writing system) versus some other kind of information (cf.",31,32
16325,236425703,"To be sure, traditional approaches to decipherment have often relied on a very simple extrinsic measure-the size of a symbol set-for making an initial guess as to what kinds of information the symbols represented.",22,23
16326,236425703,"For example, Pope (1999, page 138) describes the early work of A. H. Sayce, one of the pioneers of Luvian hieroglyphic decipherment, who in 1876 suggested that the script must be a syllabary, with an ideographic element present as well, based, among other evidence, on the close similarity between the symbol inventory sizes between Luvian and the recently deciphered Cypriot syllabary.",60,61
16327,236425703,We need rather to consider not only the symbol but some representation of the linguistic information it encodes.,8,9
16328,236425703,"The task of the decoder is to predict the next symbol of the output, given the inputs and the previous outputs-that is, to find the probability p of predicting output y i given the history y 1 , . . . ,",10,11
16329,236425703,"In practical terms, attention reflects the importance each portion of the input has for predicting each output symbol.",18,19
16330,236425703,"It would at least be theoretically possible to have a system where it is almost always straightforward to phonetically transcribe a text automatically, because each written symbol has only one pronunciation; but where determining how to spell a given phoneme sequence requires one to consider the context.",27,28
16331,236425703,"Default Neural Architecture Details As discussed in Section 4, we utilize a neural sequence-to-sequence model (Sutskever, Vinyals, and Le 2014) , where the input side corresponds to the phonemes in a discrete phoneme sequence and the output side represents the discrete orthographic symbol sequence.",51,52
16332,236425703,"The encoder embeds the inputs into a sequence of vectors in a continuous vector space, while the decoder component predicts the next symbol of the output, given the inputs and the previous outputs.",23,24
16333,245436731,A sequence of l copies of a symbol s is denoted by s l .,7,8
16334,245436731,"For a set T of trees we denote by Σ(T) the set of trees which have a symbol from Σ at their root, with direct subtrees in T, more precisely, {f [t 1 , . . . ,",18,19
16335,245436731,"In the following, let ∈ Σ be a special symbol of rank 0.",10,11
16336,245436731,"q k ] w → q will also be viewed as a symbol of rank k, turning R into a ranked alphabet.",12,13
16337,245436731,"In addition, we view every state q ∈ Q as a symbol of rank 0.",12,13
16338,245436731,Directed arcs point from the consumed symbol to the right-hand side state of the transition in question.,6,7
16339,5756693,"The approach presented here, vertical integration, shows how to construct PDP computers that can process symbols and how to design symbol systems so that they will run on more brainlike computers.",22,23
16340,5756693,The operations on tensors in turn are interpreted as operations on symbol structures.,11,12
16341,5756693,A wide range of tensor manipulation architectures are presented with the goal of inducing constraints on the symbol structures that it is possible for the mind to possess.,17,18
16342,5756693,"As a demonstration of what is possible with constrained symbol structures, a program, CRAM, that uses and acquires thematic knowledge is presented.",9,10
16343,5567214,"The length of a string w ∈ Σ * is denoted |w|, and the ith symbol (starting from one) in the string w is denoted w i .",16,17
16344,5491644,"For Entrez Gene we extracted all entries for the following types: gene locus, protein name, protein description, nomenclature symbol and nomenclature fullname, creating a SimString database for each.",22,23
16345,9549569,Often a tweet may include one or more words immediately preceded with a hash symbol (#).,14,15
16346,235458160,"The symbol * means our score is significantly higher than the best previous system while † means our score is not significantly different from the best previous system, according to paired t-test with p < 0.05.",1,2
16347,235458160,"Besides, Normalization rules Examples Replace the punctuation ""&"" with ""and"" Bay Subs & Deli → Bay Subs and Deli If the entity contains any symbol of "" -"", "", "" or ""/"", split this entity by this symbol and remove the second part Hard Knox Cafe -Potrero Hill → Hard Knox Cafe Replace ""guesthouse"" with ""guest house"" ARBURY LODGE GUESTHOUSE → ARBURY LODGE GUEST HOUSE If the entity contains a place name such as ""Fisherman's Wharf"" and ""San Francisco"" in the end, remove it (since the entities in the knowledge base do not contain these place names) Data is limited to 50MB per day with no option of additional data.",29,30
16348,235458160,"Besides, Normalization rules Examples Replace the punctuation ""&"" with ""and"" Bay Subs & Deli → Bay Subs and Deli If the entity contains any symbol of "" -"", "", "" or ""/"", split this entity by this symbol and remove the second part Hard Knox Cafe -Potrero Hill → Hard Knox Cafe Replace ""guesthouse"" with ""guest house"" ARBURY LODGE GUESTHOUSE → ARBURY LODGE GUEST HOUSE If the entity contains a place name such as ""Fisherman's Wharf"" and ""San Francisco"" in the end, remove it (since the entities in the knowledge base do not contain these place names) Data is limited to 50MB per day with no option of additional data.",48,49
16349,5843825,"The hashtags in the tweets are stripped of the # symbol, and each of the hashtags are treated as regular unigrams in the corpus.",10,11
16350,15943041,"Non-terminals preceded with the symbol ""@"" are created through binarization (see Section 3.3).",6,7
16351,15943041,"A context-free grammar G = (V, T, S † , P) consists of: a set of nonterminal symbols V, including a special start symbol S † ; a set of terminal symbols T; and a set of rule productions P of the form A → β for A ∈ V and β ∈ (V ∪ T) * , i.e., a single non-terminal on the left-hand side of the rule production, and a sequence of 0 or more terminals or non-terminals on the right-hand side of the rule.",31,32
16352,5446291,"For each rule justifying a local mother-daughters configuration, all occurrences of the ↑ symbol (called a metavariable) in the functional annotations of the daughters are replaced by the mother node, and for each of the daughter categories, all occurrences of the ↓ metavariable in its annotations are replaced by the corresponding daughter node.",16,17
16353,5446291,The start rules expand the root category of the new grammar to the original start symbol augmented by root and one of the instantiated-rule collections determined in the second step.,15,16
16354,5446291,"Thus the graph indicates, for example, that the interpretation function assigns to the attribute symbol PRED the (unary) partial function {(a, b), (e, h), (f, i), (g, j)} and to the attribute symbol ELE the partial function {(f, d), (g, d)}.",16,17
16355,5446291,"Thus the graph indicates, for example, that the interpretation function assigns to the attribute symbol PRED the (unary) partial function {(a, b), (e, h), (f, i), (g, j)} and to the attribute symbol ELE the partial function {(f, d), (g, d)}.",53,54
16356,5446291,"Furthermore, it interprets the atomic value symbol 'FALL (SUBJ) ' as denoting b and PAST as denoting c. Now, let M be a minimal model of our original f-description.",7,8
16357,5446291,The grammar G F expands the root symbol S F to complex categories of the form S:root:IR root containing the root category S of G as their first component.,7,8
16358,5446291,"We see then that the left daughter of ( 29 ) matches the mother of ( 31 ) that derives the terminal symbol ""John:⊥:∅"". (",22,23
16359,5446291,"By applying (32) to the left daughter of (30) we first derive the terminal symbol ""fell:⊥:∅"". (",18,19
16360,5446291,These cannot play a role in any derivation either because they are unreachable from the root symbol S F or because they do not lead to a terminal string.,17,18
16361,5446291,A top-down approach to grammar construction is the simplest way of avoiding categories and rules that are unreachable from the root symbol.,23,24
16362,5446291,"At that point the algorithm terminates with R containing a subset of R F sufficient to simulate all and only the LFG derivations for F. As indicated, this algorithm has the desirable property of creating just those categories and rules of G F that are accessible from the root symbol.",50,51
16363,5446291,"It ensures that every category we construct can derive a terminal string, but it does not guarantee that every bottom-up sequence will reach the root symbol.",28,29
16364,13943154,"Hidden Markov models (HMMs), for example, can be understood as a random walk through a probabilistic finite-state network, with an output symbol sampled at each state.",28,29
16365,13943154,"PCFGs generate phrase-structure trees by recursively rewriting nonterminal symbols as sequences of ""child"" symbols (each itself either a nonterminal symbol or a terminal symbol analogous to the emissions of an HMM).",24,25
16366,13943154,"PCFGs generate phrase-structure trees by recursively rewriting nonterminal symbols as sequences of ""child"" symbols (each itself either a nonterminal symbol or a terminal symbol analogous to the emissions of an HMM).",28,29
16367,13943154,The grammar's start symbol is S n .,4,5
16368,9179033,Figure 1 also shows the probabilities of the Pictish symbol script sets.,9,10
16369,9179033,"As stated in the paper (Lee, Jonathan, and Ziman 2010) , one of the corpora of Pictish symbol types gives values of the structure variables (U r and C r ) defined in the original paper that are consistent with digram communication encoding at the constrained vocabulary level such as name lists.",21,22
16370,10254955,"Recall that a CFG is in Greibach normal form if the right-hand side of every rule in the grammar starts with a terminal symbol, representing an overt lexical item.",25,26
16371,10254955,"We use the symbol γ as a variable ranging over elementary trees, α as a variable ranging over initial trees, and β as a variable ranging over auxiliary trees.",3,4
16372,10254955,"A derivation tree d is called sentential if d is of some type γ, and the root node of γ is labeled with the start symbol of the grammar, denoted as S. A node u in an elementary tree γ may be annotated with an adjunction constraint, which for purposes here is a label in the set {NA, OA}.",26,27
16373,10254955,"Lexicalization In a tree, a node labeled with a terminal symbol is called a lexical node.",11,12
16374,10254955,"For a derived tree t and a terminal symbol a, we write Nodes(a, t) to denote the set of all nodes in t that are labeled with a. Furthermore, for a node u of t we write depth(u, t) to denote the length of the unique path from the root node of t leading to u. Intuition In order to convey the basic idea behind Schabes's proof and our alternative version herein, we first consider a specific candidate grammar for the lexicalization of G 1 .",8,9
16375,10254955,"Under this view, substitution is essentially context-free rewriting: It replaces a nonterminal symbol in the yield of a derived tree with a new string consisting of terminals and nonterminals, representing the yield of the tree that is substituted.",16,17
16376,10254955,"But just as substitution can be seen as context-free rewriting on tree yields, adjunction can be seen as context-free rewriting on the paths of trees: It replaces a nonterminal symbol in some path of a derived tree with a string representing the spine of the tree that is adjoined-the unique path from the root node of the tree to the foot node.",35,36
16377,10254955,Note that the left parenthesis symbol '(' and the right parenthesis symbol ')' are nonterminal symbols.,5,6
16378,10254955,Note that the left parenthesis symbol '(' and the right parenthesis symbol ')' are nonterminal symbols.,13,14
16379,16294146,"Introduction In the last issue of this journal, I presented a piece that called into question some of the techniques reported in two papers in high-profile journals that purported to provide statistical evidence for the linguistic status of some ancient symbol systems (Sproat 2010a) .",43,44
16380,16294146,"Misunderstandings about Nonlinguistic Symbols A large part of the discussion in Rao's response centers, as it should, on the question of the ""Type 1"" (random) and ""Type 2"" (rigidly ordered) models, which I argued did not accurately characterize any nonlinguistic symbol systems.",52,53
16381,16294146,All the hierarchy implies is that a symbol Y that is lower on the hierarchy should come after a symbol X that is higher on the hierarchy.,7,8
16382,16294146,All the hierarchy implies is that a symbol Y that is lower on the hierarchy should come after a symbol X that is higher on the hierarchy.,19,20
16383,16294146,"In my piece, I made the point that relevant nonlinguistic symbol systems abound, a point that Rao seems to be calling into question.",11,12
16384,16294146,"As Rao highlights, Vidale notes the ""systematic, large-scale redundancy"" of symbol distributions of some of the systems he examines.",16,17
16385,16294146,A symbol system has as many symbols as it needs.,1,2
16386,16294146,"The Indus certainly was one of the most advanced civilizations of the third millenium BCE: Would it therefore not be reasonable to suppose that they could have invented a rich nonlinguistic symbol system, even if their less advanced neighbors could not?",32,33
16387,16294146,"Although they are possibly ""the two 'most ancient' nonlinguistic systems,"" the topic of discussion surely is symbol systems that were products of the human mind.",21,22
16388,16294146,"Finally, while we are on the topic of types of symbol systems, I note that Rao makes a point of enumerating the list of properties that they deem relevant for considering the linguistic status of the Indus symbols.",11,12
16389,16294146,3) Deity symbols are often ligatured together: One symbol may be joined with another. (,10,11
16390,16294146,"5 ) Deity symbols clearly have language-like properties in that certain symbols display positional preference (symbols for the more important gods coming earlier in the text), and certain glyphs have an affinity for each other-for example, some glyphs such as the ""horned crown"" seem to like to be joined together with the ""symbol base."" (",63,64
16391,16294146,"This would seem like a fairly compelling list of script-like properties: Contrary to what Rao suggests, it is certainly possible to find very script-like nonlinguistic symbol systems.",31,32
16392,16294146,How many such misclassified systems are there among plausible nonlinguistic symbol systems?,10,11
16393,16294146,The problem with the broader view expressed by the quoted sentence from Powell is that it would seem to classify as writing any meaningful conventionalized symbol system.,25,26
16394,2704059,"Preliminaries A ranked alphabet is an alphabet where each symbol has an integer rank, denoting the number of children the symbol takes in a tree.",9,10
16395,2704059,"Preliminaries A ranked alphabet is an alphabet where each symbol has an integer rank, denoting the number of children the symbol takes in a tree.",21,22
16396,2704059,"Similarly, given a ranked alphabet Σ and a set X, Σ(X) denotes the set of trees consisting of a single symbol of Σ of rank k dominating a sequence of k elements from X. We use T Σ (X) to denote the set of arbitrarily sized trees constructed from ranked alphabet Σ having items from set X at some leaf positions.",23,24
16397,2704059,"A linear context-free rewriting system (LCFRS) is defined as a system (V N , V T , P, S), where V N is a set of nonterminal symbols, V T is a set of terminal symbols, P is a set of productions, and S ∈ V N is a distinguished start symbol.",62,63
16398,2704059,"Following Melamed, Satta, and Wellington (2004) , we represent translation in LCFRS by using a special symbol # to separate the strings of the two languages.",20,21
16399,2704059,"Our LCFRS grammars will only generate strings of the form s#t, where s and t are strings not containing the symbol #, and we will identify s as the source string and t as the target string.",21,22
16400,2704059,We use the notation trans(LCFRS) to denote the set of translations that can be produced by taking the string language of some LCFRS and splitting each string into a pair at the location of the # symbol.,37,38
16401,2704059,Let S 0 be the state symbol at the root of the right-hand-side (r.h.s.),6,7
16402,2704059,"Finally, we add a start rule rule S → g(S i ), g( e, f ) = e#f for each S i ∈ F to generate all final states S i of the MBOT from the start symbol S of the LCFRS.",41,42
16403,2704059,t k contain the k yields of the k MBOT output subtrees (subtrees of O) that are found as children of the root (state symbol) of the MBOT rule's right-hand side.,27,28
16404,2704059,t k returned by LCFRS combination functions contain the k yields of the k MBOT output subtrees found as children of the root (state symbol) of the MBOT rule's r.h.s.,25,26
16405,2704059,"For any transduction of the MBOT, from Lemma 1, there exists an LCFRS derivation which produces a string consisting of the yield of the MBOT's input and output trees joined by the # symbol.",36,37
16406,2704059,"t m,1 • • • t m,n m where each α i is a string of terminals, and each symbol t i,j is either a variable f i ,j , or a single terminal, we construct the MBOT rule: S α 0 B 1 f 1,1 . . .",22,23
16407,2704059,"Grammars of the class 1-m-LCFRS have the property that, for any nonterminal A (other than the start symbol S) having fan-out ϕ(A), one span is always realized in the source string (to the left of the # separator), and ϕ(A) − 1 spans are always realized in the target language (to the right of the separator).",23,24
16408,2704059,"For a 1-m-LCFRS rule constructed from an MBOT, we define the rule's source language projection to be the rule obtained by discarding all the target language spans, as well as the separator symbol # in the case of the start productions.",39,40
16409,2704059,"t ϕ(S),1 • • • t ϕ(S),n ϕ(S) where each symbol t i,j is either some variable x i ,j or a terminal from the alphabet of the LCFRS, we construct the MBOT rule: S B 1 x 1,1 . . .",11,12
16410,2704059,"x r,ϕ(B r ) → S S 1 t 1,1 • • • t 1,n 1 • • • S ϕ(S) t ϕ(S),1 • • • t ϕ(S),n ϕ(S) where the MBOT's input alphabet contains a symbol S for each LCFRS nonterminal S, and the MBOT's output alphabet contains ϕ(S) symbols S i for each LCFRS nonterminal S. This construction for converting an LCFRS to an MBOT shows that LCFRS ⊂ target(yield(MBOT)).",39,40
16411,2704059,"We define an SCFG to be a system (V, Σ, ∆, P, S) where V is a set of nonterminals, Σ and ∆ are the terminal alphabets of the source and target language respectively, S ∈ V is a distinguished start symbol, and P is a set of productions of the following general form: X 0 → X 1 1 • • • X n n , X π(1) π(1) • • • X π(n) π(n) Figure 6 Second MBOT in composition.",49,50
16412,2704059,"We apply Lemma 5 a second time, with all terminals of the kth run as the distinguished positions, to the derivation (A, A) ⇒ * (γ, γ ) by taking A as the start symbol of the grammar.",41,42
16413,248780553,"Specifically, a span (i, j) of width k is represented by the vector s ij = h i ⊗ h j ⊗ z k where h i and h j are respectively the representation of the words at indexes i and j, and z k corresponds to the embedding vector for spans of width k; the ⊗ symbol denotes the concatenation operation.",63,64
16414,108289542,Let 2 ∈ Σ be a special symbol.,7,8
16415,108289542,"Hence, these subtrees must have been processed earlier, to the ex- The result of applying a rule at α deletes the subtrees covered by the left-hand side and turns the label of α (a state or input symbol) into a state q. tent necessary to make the part to be processed identical to s. Applying the rule then removes the subtrees and turns α into a state (or turns it from one state into another, if it already was a state due to an earlier step).",42,43
16416,108289542,"More generally, rules in which the root of the left-hand side is an input symbol (with or without children) can be viewed as initializing the processing of the remaining children of that node by turning their parent into an ""initial"" state.",17,18
16417,108289542,"Grounding The first step annotates every occurrence of a symbol in t with its address, yielding t, and constructs a new t2g transducer Θ t = (Σ , ∆, Q , R , µ, F ) with domain language { t} and output graphs that are the translations of t by Θ. Let N ( t) = N (t) and t(α) = t(α) α for all α ∈ N (t).",9,10
16418,1611078,1 People have used the existence of quasi-Zipfian distributions in symbol systems to argue for their status as writing; such claims figure in the work of Rao and colleagues.,12,13
16419,1611078,And if the symbols come from an undeciphered or previously unknown symbol system it is common to ask what language the symbols supposedly represent and whether the system can be deciphered.,11,12
16420,1611078,"A writing system, as linguists usually define it, is a symbol system that is used to represent language.",12,13
16421,1611078,"But symbol systems that do not encode language abound: European heraldry, mathematical notation, labanotation (used to represent dance), and Boy Scout merit badges are all examples of symbol systems that represent things, but do not function as part of a system that represents language.",1,2
16422,1611078,"But symbol systems that do not encode language abound: European heraldry, mathematical notation, labanotation (used to represent dance), and Boy Scout merit badges are all examples of symbol systems that represent things, but do not function as part of a system that represents language.",33,34
16423,1611078,"Patterns of symbol distribution might suggest that a symbol system is not linguistic: For example, odd repetition patterns might make it seem that a symbol system is unlikely to be writing.",2,3
16424,1611078,"Patterns of symbol distribution might suggest that a symbol system is not linguistic: For example, odd repetition patterns might make it seem that a symbol system is unlikely to be writing.",8,9
16425,1611078,"Patterns of symbol distribution might suggest that a symbol system is not linguistic: For example, odd repetition patterns might make it seem that a symbol system is unlikely to be writing.",26,27
16426,1611078,"'s claim, which we will describe in more detail in the next section, was that one could use conditional entropy as evidence that the famous symbol system of the third millenium BCE Indus Valley civilization was most probably writing, and not some other kind of system.",27,28
16427,1611078,"The first serious arguments against the idea that the Indus symbols were part of a writing system were presented in work that Steve Farmer, Michael Witzel, and I published in Farmer, Sproat, and Witzel (2004) , which reviews extensive support for that view from archaeological evidence and comparisons with other ancient symbol systems.",57,58
16428,1611078,"A trend, it seems, has been established: We now have a set of statistical techniques that can distinguish among ancient symbol systems and tell you which ones were writing and which ones were not.",23,24
16429,1611078,"The plot purports to show that bigram conditional entropy , defined as The Fallacies H(Y|X) = − x∈X ,y∈Y p(x, y)logp(y|x) ( 1 ) can distinguish between non-linguistic symbol systems and linguistic symbol systems, and that the Indus Valley symbols behave like linguistic symbol systems.",34,35
16430,1611078,"The plot purports to show that bigram conditional entropy , defined as The Fallacies H(Y|X) = − x∈X ,y∈Y p(x, y)logp(y|x) ( 1 ) can distinguish between non-linguistic symbol systems and linguistic symbol systems, and that the Indus Valley symbols behave like linguistic symbol systems.",38,39
16431,1611078,"The plot purports to show that bigram conditional entropy , defined as The Fallacies H(Y|X) = − x∈X ,y∈Y p(x, y)logp(y|x) ( 1 ) can distinguish between non-linguistic symbol systems and linguistic symbol systems, and that the Indus Valley symbols behave like linguistic symbol systems.",50,51
16432,1611078,The sample sizes are small because the Indus corpus against which all other symbol systems are compared is very small.,13,14
16433,1611078,The problem is that there is little evidence that either of these types accurately characterized any ancient symbol system.,17,18
16434,1611078,The take-home message appears to be that in principle symbol systems could vary as widely as being completely rigid or completely random and equiprobable.,11,12
16435,1611078,"The problem with this argument is that it is highly unlikely that there were ever any functional symbol systems that had either of these properties, and one can argue this point on basic information theoretic grounds.",17,18
16436,1611078,A symbol system that was completely rigid-had an entropy of 0-would convey no information whatsoever.,1,2
16437,1611078,"If whenever symbol x occurred, sym-bol y always followed, there would be little point in having more than just symbol x, except perhaps for decorative purposes.",2,3
16438,1611078,"If whenever symbol x occurred, sym-bol y always followed, there would be little point in having more than just symbol x, except perhaps for decorative purposes.",23,24
16439,1611078,"So although Rao is technically correct that his Types 1 and 2 do represent the logical extremes of the distribution, it is not likely that any meaningful symbol systems were ever created that had either of these properties.",28,29
16440,1611078,"If one allows that symbols have a quasi-Zipfian distribution-something that is surely true of linguistic symbol systems, but of many other things too-then one finds curves that look very similar to what Rao et al.",19,20
16441,1611078,"But this has not been demonstrated: Nobody has done the legwork of putting together the needed corpora of ancient linguistic and nonlinguistic symbol systems, and demonstrated that one can in fact use such measures to do a better than chance job of classifying systems.",23,24
16442,1611078,"'s (2009a) work, they compare the symbols to a variety of known writing systems, as well as symbol systems like Morse code, and European heraldry, and randomly generated texts-by which, again, is meant random and equiprobable.",21,22
16443,1611078,"Lee, Jonathan, and Ziman (2010) use C r and U r to train a decision tree to classify symbol systems.",22,23
16444,1611078,"In order to put the Lee, Jonathan, and Ziman (2010) theory to a serious test, I looked to another symbol system, namely, Mesopotamian deity symbols from kudurrus (boundary stones) catalogued in Seidl (1989) .",24,25
16445,1611078,"For the Pictish symbols, Lee, Jonathan, and Ziman computed values for C r and U r under various assumptions of what the symbol type set was, with the largest values being C r = 6.16 and U r = 1.45.",25,26
16446,1611078,I must stress that I do not wish to argue that it is impossible that one could come up with a sound statistical argument to show that a particular symbol system is not linguistic.,29,30
16447,1611078,"If one took a large sample of known linguistic and non-linguistic symbol systems, and showed that a particular set of measures could reliably distinguish between them with very high accuracy, then such measures could presumably be applied in the case of unknown systems such as the Indus or Pictish systems.",13,14
16448,1611078,"Yet, with the publication of these papers, and their promotion by the all-too-eager popular science press, non-specialists might easily believe that ""artificial intelligence"" methods can provide crucial evidence for a symbol system's status as writing.",41,42
16449,1611078,"It is more exciting to learn that a statistical method can tell you that such-and-such an ancient symbol system was writing, than to learn that in fact the proposed methods do not work.",21,22
16450,16567212,"Refinements such as the Growing Cells technique (Fritzke 1993) might be preferable to a move to higher dimensionality, so as to retain transparency of the notation and a possible link to symbol-based stages of operation.",34,35
16451,9770874,"When representing empty categories in dependency trees, we can use a null symbol to depict the idea that there is a mental category at the level being represented.",13,14
16452,7255096,The feature value is also not defined (ND) if the token itself is a punctuation symbol or contains any special symbol or digit.,17,18
16453,7255096,The feature value is also not defined (ND) if the token itself is a punctuation symbol or contains any special symbol or digit.,22,23
16454,7255096,"For each tag T inserted in the training corpus, the algorithm generates a lexical pattern p using a context window of maximum width 6 (excluding the tagged NE) around the left and the right tags, e.g., p = [l -3 l -2 l -1 <T> ...</T> l +1 l +2 l +3 ], where, l ±i are the context of p. Any of l ±i may be a punctuation symbol.",82,83
16455,13449499,"Synchronous Context-Free Grammars As usual, a CFG has a finite set of rules having the general form A → α, where A is a nonterminal symbol to be rewritten and α is a string of nonterminal and terminal symbols.",29,30
16456,186232078,It is presented abstractly in the form of principles that are to be understood as a specification rather than a symbol-level description for a representation scheme.,20,21
16457,233364999,"The structure, or ""grammar"" of an aks ̣ara is based on the follow ing common principles: an aks ̣ara often consists of a consonant symbol 𝐶, by default bearing an unmarked inherent vowel or attached diacritic (de pendent) vowel sign 𝑣 (𝐶 𝑣 ); but it may also be an independent vowel symbol 𝑉 , or a consonant sym bol with its inherent vowel ""muted"" by a special virama diacritic ∅ (𝐶 ∅ ).",28,29
16458,233364999,"The structure, or ""grammar"" of an aks ̣ara is based on the follow ing common principles: an aks ̣ara often consists of a consonant symbol 𝐶, by default bearing an unmarked inherent vowel or attached diacritic (de pendent) vowel sign 𝑣 (𝐶 𝑣 ); but it may also be an independent vowel symbol 𝑉 , or a consonant sym bol with its inherent vowel ""muted"" by a special virama diacritic ∅ (𝐶 ∅ ).",61,62
16459,900582,The colon is a punctuation symbol to display roles.,5,6
16460,17981421,"An HMM state h t may then be defined as a WFSA state, or a symbol position in a corresponding regular expression.",16,17
16461,17981421,"Like HMM states, the states at each level in a simple HHMM also correspond to weighted FSA (WFSA) states or symbol positions in regular expressions, except that some states can be nonterminal states, which introduce corresponding sub-expressions or sub-WFSAs governing state transitions at the level below.",23,24
16462,250390850,"Each symbol is embedded, resulting in a data structure with n embedding dimensions per symbol, corresponding to the ""channels"" in an image 5 .",1,2
16463,250390850,"Each symbol is embedded, resulting in a data structure with n embedding dimensions per symbol, corresponding to the ""channels"" in an image 5 .",15,16
16464,250390850,"The logits, in turn, can be used to predict the most likely character at each position, or to calculate a sparse categorical cross-entropy loss during training, given a target symbol.",35,36
16465,250390850,B Tuning the Inpainting Model For the cognate inpainting model there are six tunable hyperparameters: • The symbol embedding dimension. •,18,19
16466,2120538,"In the rest of this paper, a structured symbol a will be written as a (f).",9,10
16467,2120538,"The definite article der is described by the complex symbol [Art-cat(def), Gender(<masc,neutr>)i case(nominative), number(singular)]}, Feature grammars are defined as formal grammars manipulating strings of complex symbols and the derivability concept is modified according to the structures of the complex symbols.",9,10
16468,18106982,"According to Equation ( 7 ), the following data are to be maintained: c k,1 and c k,2 given any order k, N 1+ (•), and c X (•) (see Section 3.4 for the meaning of each symbol).",47,48
16469,58248297,"For example, n2duty is the symbol for the second noun sense of the word duty.",6,7
16470,611341,"Recurrent neural language models akin to TEXTUAL, which are trained to predict the next symbol in a sequence, are relatively well understood, and there have been some attempts to analyze their internal states (Elman 1991; Karpathy, Johnson, and Li 2016, among others) .",15,16
16471,611341,In all the models the full sentences are represented by the activation vector at the end-of-sentence symbol (h end ).,20,21
16472,10390315,The symbol YP stands for any maximal projection admitted by linguistic theory.,1,2
16473,6237722,Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.,23,24
16474,6237722,"A CFG G = (V, T, P, St), consists of a set of nonterminal symbols V, a set of terminal symbols T, a start symbol St E V, and a set of rule productions P of the form: A ~ a, where a c (V U T)*.",32,33
16475,6237722,"These context-free rules can be interpreted as saying that a nonterminal symbol A expands into one or more either nonterminal or terminal symbols, a = Xo... Xk} A sequence of context-free rule expansions can be represented in a tree, with parents expanding into one or more children below them in the tree.",13,14
16476,6237722,"Consider, for example, the parse tree shown in (a) in Figure 1 : the start symbol is St, which expands into an S. The S node expands into an NP followed by a VP.",19,20
16477,6237722,"A CFG G defines a language Lc, which is a subset of the set of strings of terminal symbols, including only those that are leaves of complete trees rooted at St, built with rules from the grammar G. We will denote strings either as w or as WoW1 ... wn, where wn is understood to be the last terminal symbol in the string.",63,64
16478,6237722,"be the set of all complete trees rooted at the start symbol, with the string of terminals w~ as leaves.",11,12
16479,6237722,"We will adopt the convention that an explicit beginning of string symbol, (s/, and an explicit end symbol, </s), are part of the vocabulary, and a string wg is a complete string if and only if w 0 is (s) and wn is (/s).",11,12
16480,6237722,"We will adopt the convention that an explicit beginning of string symbol, (s/, and an explicit end symbol, </s), are part of the vocabulary, and a string wg is a complete string if and only if w 0 is (s) and wn is (/s).",20,21
16481,6237722,"Since the beginning of string symbol is not predicted by language models, but rather is axiomatic in the same way that S ~f is for a parser, we can safely omit it from the current discussion, and simply assume that it is there.",5,6
16482,6237722,"While a complete string of words must contain the end symbol as its final word, a string prefix does not have this restriction.",10,11
16483,6237722,"To avoid confusion between sets and sequences, 0 will not be used for empty strings or sequences, rather the symbol ( ) will be used.",21,22
16484,6237722,"Note that the script $ is used to denote stacks, while St is the start symbol.",16,17
16485,12445390,"Therefore, before popping the S from the STACK, ASSEMBLE-NP returns the symbol ""AS"" to PARSE-WORD.",15,16
16486,12445390,"20) (S (v <""buy"", +PAST, Q>) (AGNT (NP1 ""John"") (PTNT (NP4 (HEAD (NP3 (HEAD (NP2 ""book"") (MOD (ADJ ""good"")))) (DET <-DEF>)))) The next thing PARSE-WORD sees in the INPUT BUFFER is EOS (end-of-sentence symbol).",80,81
16487,12445390,"a. Create a TPIC node which is directly dominated by the topmost S node and attach a ""copy"" (i.e., the category symbol and its index) of the TOS to this node.",25,26
16488,7592530,"The following rather simple Linear Basic Grammar (and, of course, Basic Grammar) productions generate J (where the symbol 'A' refers to the empty string and the symbol '1' is a disjunction of right-hand sides of rules having a common left-hand side): S ---+ F(wu zhao, ,k) F(x,y) --* F(xzhao, y) I F(xzhao, xy) I xy However, Fischer conjectures that the class of Linear Basic Languages does not include the class of CFLs.",22,23
16489,7592530,"The following rather simple Linear Basic Grammar (and, of course, Basic Grammar) productions generate J (where the symbol 'A' refers to the empty string and the symbol '1' is a disjunction of right-hand sides of rules having a common left-hand side): S ---+ F(wu zhao, ,k) F(x,y) --* F(xzhao, y) I F(xzhao, xy) I xy However, Fischer conjectures that the class of Linear Basic Languages does not include the class of CFLs.",33,34
16490,7592530,"We use the symbol '&' to refer to the null string, i.e. ""do not read,"" and '#' as a right-edge marker.",3,4
16491,3505719,"Having extracted rules from the training data, we could let X be the grammar's start symbol and translate new sentences using only the extracted rules.",17,18
16492,3505719,"We formalize this inside a synchronous CFG using the rules ( 14 ) and ( 15 ), which we call the glue rules, repeated here: S → S 1 X 2 , S 1 X 2 (14) S → X 1 , X 1 (15) These rules analyze an S (the start symbol) as a sequence of Xs which are translated without reordering.",60,61
16493,3505719,"6   The axioms would be X γ : w (X w − → γ) ∈ G and the inference rules would be Z f i+1 : w [Z, i, i + 1] : w Z XY : w [X, i, k] : w 1 [Y, k, j] : w 2 [Z, i, j] : w 1 w 2 w and the goal would be [S, 0, n], where S is the start symbol of the grammar and n is the length of the input string f .",96,97
16494,3505719,"First, assume that the LM expects a whole sentence to be preceded by (m − 1) start-of-sentence symbols s and followed by a single end-of-sentence symbol /s .",36,37
16495,3505719,"The grammar can be made to do this simply by adding a rule S' → S 1 , s m−1 S 1 /s (26) and making S' the new start symbol.",34,35
16496,3505719,"First, we define two functions p and q which operate on strings over T ∪ { }, where T is the English terminal alphabet, and is a special placeholder symbol that stands for an elided part of an English string.",32,33
16497,3505719,Here w[x/X] means the string w with the string x substituted for the symbol X. The function q is defined in the text.,16,17
16498,59775664,"We used the WordNet notation in which the symbol 'x>y' means x is a superconcept of y and the symbol 'x<y' means x is a subconcept of y. For example, vicereine<wife means the concept vicereine is a specialization of the concept wife.",8,9
16499,59775664,"We used the WordNet notation in which the symbol 'x>y' means x is a superconcept of y and the symbol 'x<y' means x is a subconcept of y. For example, vicereine<wife means the concept vicereine is a specialization of the concept wife.",23,24
16500,7588509,"Let R be the set of production rules in the grammar and R A be the set of production rules with left-hand nonterminal symbol A. Generative process A parse tree (or derivation) in this formalism is a tree where every interior node is labeled with a nonterminal symbol, every leaf is labeled with a terminal, and the root node is labeled with the root nonterminal S. Moreover, every node in the tree is associated with a logical form: let x n be the logical form assigned to the tree node n, and x 0 = x for the root node 0.",25,26
16501,7588509,"Let R be the set of production rules in the grammar and R A be the set of production rules with left-hand nonterminal symbol A. Generative process A parse tree (or derivation) in this formalism is a tree where every interior node is labeled with a nonterminal symbol, every leaf is labeled with a terminal, and the root node is labeled with the root nonterminal S. Moreover, every node in the tree is associated with a logical form: let x n be the logical form assigned to the tree node n, and x 0 = x for the root node 0.",51,52
16502,7588509,"This process repeats recursively with every right-hand side nonterminal symbol, until there are no unexpanded nonterminal nodes.",11,12
16503,7588509,from a uniform distribution over a finite set of terminals and a special stop symbol with probability φ A .,14,15
16504,7588509,"Once the stop symbol is drawn, we have finished gen-erating the rule.",3,4
16505,7588509,"For example, a lexicon can be provided where each entry is a terminal symbol y i with a corresponding logical form label x i .",14,15
16506,7588509,"At every iteration, the algorithm pops an item from the agenda and adds it to the chart, and considers the next right-hand side symbol B k . •",27,28
16507,7588509,"For any terminal symbol T , we define I (T,i,j) = 0.",3,4
16508,7588509,"B K : f K after completing parsing for the symbol B k with logical form set X, we create a new agenda item with the same contents as the old item, but with the rule position increased by one.",10,11
16509,202540912,"All input text is first lower-cased and tokenised, numbers with 5 or more digits get their digits replaced by a wildcard symbol #, while words longer than 16 characters are replaced by a wildcard token LONGWORD.",24,25
16510,6667804,"Lexicon The VW-CCG lexicon is a set of pairs σ := X, where σ is a word (formalized as a symbol from some finite vocabulary) and X is a category.",25,26
16511,6667804,"We denote the resulting rule using the ""$"" notation of Steedman (2000) , where the symbol $ is used as a variable for the part of the category stack below the topmost stack element: Y/Z S $ /Y ⇒ S $ /Z (8) Example 3 Backward crossed composition (Equation ( 6 )) can be used for the analysis of heavy NP shift in sentences such as Kahn blocked skillfully a powerful shot by Rivaldo (example from Baldridge 2002) .",19,20
16512,6667804,"The only thing to note is that in a formal derivation tree, leaf nodes correspond to lexicon entry σ := X, whereas in our graphical notation, leaf nodes are split into a parent node with the category X and a child, leaf node with the symbol σ.",50,51
16513,6667804,"Reading from the leaves to the root, for every symbol v j in w, the derivation nondeterministically chooses between two lexicon entries, c 0 v 1 • • • v n v n+1 .",10,11
16514,6667804,"The derivation then uses compositions ( 13 ) and ( 14 ) to ""push"" these variable-specific categories to the argument stack of the lexical category for the special symbol c 0 , and a final application (15) to yield a complex category that encodes the complete assignment.",32,33
16515,6667804,"This resource-restricted model is well-known in the literature, and it exactly characterizes the class of all decision problems that are solvable by a deterministic Turing machine (i.e., a Turing machine where there is at most one possible transition given a state and a tape symbol) working in exponential time (Chandra, Kozen, and Stockmeyer 1981) .",51,52
16516,6667804,"Formally, an alternating Turing machine is a structure M = (Q, Σ, δ, q 0 , g) where: Q is a finite set of states; Σ is an alphabet of tape symbols, which we assume includes the special blank symbol #; δ ⊆ (Q × Σ) × (Q × Σ) is the transition relation; q 0 ∈ Q is the initial state; and g: Q → {∃, ∀, A, R} is a function that assigns a type to each state.",48,49
16517,6667804,"To simplify the proof, we also require that for every universal state q and tape symbol a, there are exactly two transitions with lefthand side (q, a).",16,17
16518,6667804,"A configuration of M relative to w is a pair c = (q, α), where q ∈ Q is some state and α ∈ Σ * is a sequence of tape symbols with length |α| = m. The intended interpretation of c is that the current state of M is q, the content of the circular tape is represented by α, and the tape head is positioned to read the first symbol of α.",77,78
16519,6667804,"In particular, the initial configuration of M for w, denoted by I M (w), takes the form I M (w) = (q 0 , w# m−n ), meaning that, at the start of the computation, the machine is in the initial state, the tape consists of the n symbols of the input string w followed by m − n blanks, and the tape head is positioned to read the first symbol of w. A configuration c is called existential, universal, accepting, or rejecting based on the type of its state q. Successors.",84,85
16520,6667804,"The intended interpretation of t is that if M is in state q and reads tape symbol a, then overwrites a with a , moves its tape head one cell to the right (which is always possible because the tape is circular), and continues the computation in state q .",16,17
16521,6667804,"Formally, let c = (q, aα) be a configuration of M. The successor of c with respect to t, denoted by t(c), is the configuration c = (q , αa ), where the string αa encodes the fact that the symbol a has been overwritten with a and the circular tape has been rotated one position to the right, so that the head now is posititioned to read the first symbol of α.",49,50
16522,6667804,"Formally, let c = (q, aα) be a configuration of M. The successor of c with respect to t, denoted by t(c), is the configuration c = (q , αa ), where the string αa encodes the fact that the symbol a has been overwritten with a and the circular tape has been rotated one position to the right, so that the head now is posititioned to read the first symbol of α.",80,81
16523,6667804,"Note that in this encoding, the target of E(c) is an atomic category representing the current state, and the arguments of E(c) represent the circular tape, with the innermost argument corresponding to the symbol under the tape head.",38,39
16524,6667804,"More generally now, assume that the initial configuration for M on w is c I = (q 0 , a 1 • • • a m ), where w = a 1 • • • a n and a h = # for each h with n < h ≤ m. To support fragments as the one in Figure 10 , we introduce lexicon entries ε := [init]/[q 0 ] and ε := [a], where a ∈ Σ is any tape symbol.",90,91
16525,6667804,"We also introduce the following rules: [init] $ /[a] [a] ⇒ [init] $ (20) [init] $ /[q 0 ] [q 0 ]/[a 1 ] • • • /[a m ] ⇒ [init] $ /[a 1 ] • • • /[a m ] (21) The $ symbol is, as usual, a variable for the part of the category stack below the topmost stack element.",63,64
16526,6667804,A rule of the form (20) allows the application of a category with target [init] to any atomic category [a] representing a tape symbol; this implements the nondeterministic pushing to the tape stack that we introduced above.,29,30
16527,6667804,The derivation then extends the tape stack by the new symbol a (26).,10,11
16528,6667804,In a last step it simultaneously discards the category for the previous tape symbol a and changes the target to [q ] (27).,13,14
16529,6667804,"Finally, we introduce the following rules: [q] $ /[t] [t]/X 1 • • • /X m ⇒ [q] $ /X 1 • • • /X m (25) [t] $ /[a ] [a ] ⇒ [t] $ (26) [t] $ /[q ] [q ]/X 1 • • • /X m ⇒ [t] $ /X 1 • • • /X m (27) A rule of the form ( 25 ) is a composition rule of degree m that simultaneously restricts the target of its primary input to q and the target of its secondary input to t. A rule of the form ( 26 ) is an application rule that matches t (the target of its primary input) with the tape symbol a produced by t (its secondary input).",146,147
16530,6667804,Let also b ∈ Σ be any tape symbol.,8,9
16531,6667804,"More formally now, let b ∈ Σ be any tape symbol.",11,12
16532,6667804,"To implement it, we introduce lexicon entries ε := [π; +]/[t 1 ] and ε := [b]/[b]/ [t 2 ], where b ∈ Σ is any tape symbol.",37,38
16533,6667804,"For instance, we add to G a number |Σ| • m of rules of types ( 30 ) and ( 31 ), because there is a single rule for each choice of a tape symbol a and index i with 1 ≤ i < m. Similar analyses can be carried out for the remaining elements.",36,37
16534,207780015,BPE splits words into symbols (a symbol is a sequence of characters) and then iteratively replaces the most frequent sequences of symbols with a new merged symbol.,7,8
16535,207780015,BPE splits words into symbols (a symbol is a sequence of characters) and then iteratively replaces the most frequent sequences of symbols with a new merged symbol.,28,29
16536,207780015,"In essence, frequent character n-gram sequences merge to form one symbol.",13,14
16537,10807721,We do not have a special end-of-sentence symbol.,11,12
16538,10807721,The second feature template has the form: The last verb is v and the current word is w and w has been tagged as a particle and the current tag is t. The last verb is the pseudo-symbol NA if there is no verb in the previous three positions.,40,41
16539,15287969,"This representation is called ""CFG form"", and is formally defined in the usual way as G = N, T, R, S , where N is a set of nonterminal symbols (corresponding to source-side phrase and partof-speech tags); T is a set of source-side terminals (the lexicon); R is a set of production rules of the form η → γ, with η ∈ N and γ a sequence of terminal and nonterminal symbols; and S ∈ N is the distinguished symbol.",99,100
16540,1967279,"The symbol Γ refers to the Gamma function, an extension of the factorial function to real numbers.",1,2
16541,3537181,"But the only reason the above example achieves this is because one hairpin has only as and us and the other has only cs and gs-that is, each symbol indicates overtly which hairpin it belongs to.",31,32
16542,3537181,α m ) where A is a predicate (nonterminal) symbol and the α j are strings of terminal symbols and variables (which range over strings of terminal symbols).,11,12
16543,3537181,"The proof is very simple: given two grammars, we rename apart their predicate symbols; let S 1 and S 2 be the renamed start symbols and S be a new start symbol, and add the new clause S (x) S 1 (x), S 2 (x).",34,35
16544,3537181,"We can generate these as well, but we must keep track of the direction of each strand so as not to generate any Möbius strips: Here B has three arguments: the first strand, the middle part, and the last strand; there is an additional predicate symbol B which is the same as B, except that B is for sheets with antiparallel first and last strands, whereas B is restricted here to sheets with parallel first and last strands.",51,52
16545,204826204,"To compensate the differences of sequence lengths, CTC introduces an additional ""blank"" symbol.",15,16
16546,3537908,"S ∈ V is a distinguished start symbol, 3.",7,8
16547,3537908,"Definition 2 We say that a formalism F is a context-free rewriting system (CFRS) if its derivation sets can be characterized by generalized CFGs, and its derived structures are produced by a function • F from terms to strings such that for each function symbol f , there is a yield function f F such that f (t 1 , . . . ,",49,50
16548,3537908,"For each function symbol φ, there is a function φ such that φ(t 1 , . . . ,",3,4
16549,3537908,Each component is associated with a symbol called its type.,6,7
16550,3537908,S ∈ Σ is a distinguished start symbol.,7,8
16551,3537908,"The value of a subderivation t of G under • s is a tuple of partial derivations of G, one for each symbol in the root label of t , in order.",23,24
16552,3537908,"The tree set that is left behind is the elementary tree set corresponding to η (rather, the function symbol that labels η); this process is repeated recursively on the children of η, if any.",20,21
16553,3537908,"a m , where A is the left-hand side of the GCFG production that generated η , and a i = j, k if η gets its ith field from the kth field of η j , or * if η produces a function symbol in its ith field.",47,48
16554,3537908,"Ĝ trivially simulates G. Since each tree of Ĝ corresponds to a function symbol (though not necessarily one-to-one), it is easy to write a trivial simulating interpretation • : T ( Ĝ) → T (G).",13,14
16555,17862685,"We used top 16,000 source and target vocabularies in the model and mapped the other words into a single OOV symbol, while the original paper [21] used part-of-speech classes.",20,21
16556,3421955,We use a '$' marker to indicate the word boundary between the components and a '#' symbol to mark the beginning and ending of the first and the final components respectively.,20,21
16557,4887222,We write Σ k to denote the set of all k-ary symbols in Σ. We use the special nullary symbol e ∈ Σ 0 to syntactically represent the empty string ε.,21,22
16558,4887222,"For every r ∈ T R (X), we let base(r ) denote the run obtained from r by replacing each symbol (q , l) s•s 0 •...•s k −−−−−→ (q 1 • • • q k , r) by just (q, l) s → (q 1 • • • q k , r) ∈ R. Thus, we replace a rule (which is a symbol) of R by the underlying rule of R. We start with a general lemma, which we believe to be self-evident.",23,24
16559,4887222,"For every r ∈ T R (X), we let base(r ) denote the run obtained from r by replacing each symbol (q , l) s•s 0 •...•s k −−−−−→ (q 1 • • • q k , r) by just (q, l) s → (q 1 • • • q k , r) ∈ R. Thus, we replace a rule (which is a symbol) of R by the underlying rule of R. We start with a general lemma, which we believe to be self-evident.",79,80
16560,6182054,Longer texts are truncated and shorter ones are padded with a special PAD symbol.,13,14
16561,219310067,"In HMMs, the state transition probabilities and output symbol probabilities are uniformly initialized.",9,10
16562,14108286,"A legal dependency tree has n + 1 vertices, each corresponding to one word plus a ""wall"" symbol, $, assumed to be the hidden root of the sentence.",20,21
16563,52289309,"3 The symbols sigh and tanh denote hard sigmoid and hard tan nonlinear functions, respectively, and the symbol denotes an element-wise product of two vectors.",19,20
16564,8020400,"In the discussion that follows, we use A, B, and C to denote arbitrary nonterminal symbols, S to denote the start nonterminal symbol, and a to denote a terminal symbol.",26,27
16565,8020400,"In the discussion that follows, we use A, B, and C to denote arbitrary nonterminal symbols, S to denote the start nonterminal symbol, and a to denote a terminal symbol.",34,35
16566,11705966,We further specialize symbol =U by writing =U < (=U > ) to indicate that the missing upper tree should have its head to the left (right) of its gap.,3,4
16567,30084790,"Consider, for example, the second line of the ""abstract syntax"" part of Figure 3 , with the symbol ComplV2.",21,22
16568,30084790,This symbol is one of the functions that are used for building the abstract syntax tree.,1,2
16569,14951123,The grammar has the following lexicon: w 1 WD A w 5 WD E=H =C w 2 WD B w 6 WD F =G =B w 3 WD C =A=F w 7 WD G w 4 WD S=E w 8 WD H The start symbol is S .,55,56
16570,18637494,"A tree over ˙takes the form t D f .t 1 ; : : : ; t n /, where f j n 2 ˙and t 1 ; : : : ; t n are trees over ˙. We write T ˙for the set of all trees over ˙. The nodes of a tree can be identified by paths 2 N from the root: The root has the address "", and the i th child of the node with the address has the address i. We write t. / for the symbol at path in the tree t, and t # for the subtree of t at .",94,95
16571,18637494,"C OEt is the tree in T ˙which is obtained by replacing the hole in C by some tree t 2 T ˙. We write C ˙for the set of all contexts over ˙. A ˙-algebra A consists of a non-empty set A called the domain and, for each function symbol f j n 2 ˙, a total function f A W A n !",53,54
16572,18637494,"t 1 A ; : : : ; t n A / : One algebra that we will use throughout the paper is the string algebra A over some alphabet A. Its elements are the strings over A; it has one binary operation "" "", which concatenates its two arguments, and one constant for each symbol in A which evaluates to itself.",58,59
16573,18637494,"Another important algebra is the term algebra T ˙over some ranked signature ˙. The domain of the term algebra is T ˙, and for each symbol f j n 2 ˙, we have f T ˙.t 1 ; : : : ; t n / D f .t 1 ; : : : ; t n /, i.e. every term evaluates to itself.",26,27
16574,18637494,"Formally, h is specified by pairs .f; h.f //, where f 2 ˙is a symbol with some rank n, and h.f / 2 T [fx 1 ;:::;x n g is a term with variables.",17,18
16575,18637494,"Formally, an RTG is a construct G D .N; ˙; P; S / where N and ˙are signatures of nonterminal and terminal symbols, S 2 N is a start symbol, and P is a finite set of production rules of the form A !",34,35
16576,18637494,"We write t(π) for the symbol at π in the tree t. Σ-algebra A consists of a non-empty set A d the domain and, for each symbol f ∈ Σ with m, a total function f A : A m → A, called the tion associated with f .",6,7
16577,18637494,"We write t(π) for the symbol at π in the tree t. Σ-algebra A consists of a non-empty set A d the domain and, for each symbol f ∈ Σ with m, a total function f A : A m → A, called the tion associated with f .",32,33
16578,18637494,"Formally, such a grammar is a structure (N, Σ, P, S), where N is a signature of nonterl symbols, all of which are taken to have rank 0, a signature of terminal symbols, S ∈ N is a guished start symbol, and P is a finite set of ctions of the form B → t, where B is a nonnal symbol, and t ∈ T N ∪Σ .",50,51
16579,18637494,"Formally, such a grammar is a structure (N, Σ, P, S), where N is a signature of nonterl symbols, all of which are taken to have rank 0, a signature of terminal symbols, S ∈ N is a guished start symbol, and P is a finite set of ctions of the form B → t, where B is a nonnal symbol, and t ∈ T N ∪Σ .",72,73
16580,18637494,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",6,7
16581,18637494,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",34,35
16582,18637494,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",54,55
16583,18637494,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",76,77
16584,18637494,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",6,7
16585,18637494,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",34,35
16586,18637494,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",54,55
16587,18637494,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",76,77
16588,18637494,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",6,7
16589,18637494,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",34,35
16590,18637494,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",54,55
16591,18637494,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",76,77
16592,18637494,"Formally, such a gram G = (N, Σ, P, S), where N is a minal symbols, all of which are ta Σ is a signature of terminal sym distinguished start symbol, and productions of the form B → t, terminal symbol, and t ∈ T N ∪Σ of a regular tree grammar are use on terms.",38,39
16593,18637494,"Formally, such a gram G = (N, Σ, P, S), where N is a minal symbols, all of which are ta Σ is a signature of terminal sym distinguished start symbol, and productions of the form B → t, terminal symbol, and t ∈ T N ∪Σ of a regular tree grammar are use on terms.",50,51
16594,18637494,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",6,7
16595,18637494,"We write t(π) for the symbol at path π in the tree t. A Σ-algebra A consists of a non-empty set A called the domain and, for each symbol f ∈ Σ with rank m, a total function f A : A m → A, called the operation associated with f .",34,35
16596,18637494,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",54,55
16597,18637494,"Formally, such a grammar is a structure G = (N, Σ, P, S), where N is a signature of nonterminal symbols, all of which are taken to have rank 0, Σ is a signature of terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of productions of the form B → t, where B is a nonterminal symbol, and t ∈ T N ∪Σ .",76,77
16598,18637494,"Formally, such a gram G = (N, Σ, P, S), where N is a minal symbols, all of which are ta Σ is a signature of terminal sym distinguished start symbol, and productions of the form B → t, terminal symbol, and t ∈ T N ∪Σ of a regular tree grammar are use on terms.",38,39
16599,18637494,"Formally, such a gram G = (N, Σ, P, S), where N is a minal symbols, all of which are ta Σ is a signature of terminal sym distinguished start symbol, and productions of the form B → t, terminal symbol, and t ∈ T N ∪Σ of a regular tree grammar are use on terms.",50,51
16600,18637494,"In the formulation of Schmitz and Le Roux (2008) , we can obtain an RTG G describing the derivation trees by using a nonterminal set fN S ; N A j N nonterminal of Gg; the start symbol is S S .",40,41
16601,18637494,"2a Notice that every node at which an adjunction may take place is represented by a nonterminal symbol, which must be expanded by a production rule.",17,18
16602,18637494,Consider a ˙0-algebra A L whose values are the derived trees of the TAG grammar G. A L interprets each symbol in the derivation tree as a complex tree-building operation which spells out the derived trees.,22,23
16603,18637494,"In the derivation tree algebra, we use a special symbol @, which inserts its second argument into the hole of its first argument.",10,11
16604,18637494,"˛.B 1 S ; : : : ; B k S ; B kC1 A ; : : : ; B n A / be the (unique) rule in G that contains the symbol ˛. Let i be a function that maps each substitution node in ˛to the position of the nonterminal occurrence that corresponds to in the right-hand side of this rule, i.e. to a number between 1 and k. Likewise, let i map each node at which an adjunction may take place to the position of the adjunction nonterminal, i.e. a number between k C 1 and n. We define a function h ˛for each ˛that maps nodes of ˛to terms over T D , and let h t .˛/ D h ˛.""/. Then h ˛. / D x i. / if is a substitution node; h ˛. / D a if is a lexical leaf with label a; h ˛. / D if is a foot node, and h ˛. / D @.x i. / ; f .h ˛. 1/; : : : ; h ˛. n// if is a non-leaf with label f .",35,36
16605,18637494,"h st effectively reads off the yield of a tree or context, and is defined by mapping each operation symbol of T D to a term over A T .",20,21
16606,18637494,"The start symbol of D.w/ is the span that corresponds to the entire string or string pair w. If w is a string of length n, it is OE1; n C 1; for a string pair w D .w 1 : : : w m 1 ; w m : : : w n / 2 A T , the start symbol is OE1; m; m; n C 1.",2,3
16607,18637494,"The start symbol of D.w/ is the span that corresponds to the entire string or string pair w. If w is a string of length n, it is OE1; n C 1; for a string pair w D .w 1 : : : w m 1 ; w m : : : w n / 2 A T , the start symbol is OE1; m; m; n C 1.",64,65
16608,18637494,"Again, the start symbol is simply the representations of itself.",4,5
16609,18637494,"If is a tree, it is A "" ; for a context with hole , the start symbol is B ""= .",18,19
16610,18637494,"First, the parser computes the decomposition grammar D.w/. As explained above, this grammar has the start symbol OE1; 4 and rules given in Fig.",18,19
16611,18637494,The grammar has the start symbol OE1; 4 and the following rules:  An algorithm that computes G 0 is given by Koller and Kuhlmann (2011) .,5,6
16612,55565528,The default is the category symbol itself.,5,6
16613,9046374,The state transition probabilities and the output symbol probabilities are uniformly initialized.,7,8
16614,218516694,"TriviaQA From the Greek for color, what element, with an atomic number of 24, uses the symbol Cr?",19,20
16615,1615409,"We lowercase word and lemma n-grams and replace each digit by the same symbol (e.g., 12,345 → 00,000), as proposed in Markov et al. (",15,16
16616,6727884,The symbol | stands for an arbitrary (forward or backward) slash; it is understood that the slash before each B i above the line is the same as below.,1,2
16617,6727884,"Formally, a regular tree grammar (RTG) is a construct Γ = (N, Σ, S, P ), where N is an alphabet of non-terminal symbols, Σ is an alphabet of ranked term constructors called terminal symbols, S ∈ N is a distinguished start symbol, and P is a finite set of production rules of the form A → γ, where A ∈ N and γ is a term over Σ and N , where the nonterminals can be used  as constants.",54,55
16618,6727884,The grammar Γ generates trees from the start symbol by successively expanding occurrences of nonterminals using production rules.,8,9
16619,6727884,We also take the atomic categories of G as our nonterminal symbols; the start category s of G counts as the start symbol.,23,24
16620,12273076,"Recall that each nonterminal symbol A of G comes with a positive integer called the fan-out of A, and that a production p of G has the form A !",4,5
16621,12273076,"g.A 1 ; : : : ; A r / I g. E x 1 ; : : : ; E x r / D E ˛; where A; A 1 ; : : : ; A r are nonterminals with fan-out f; f 1 ; : : : ; f r , respectively, g is a function symbol, and the equation to the right of the semicolon specifies the semantics of g. For each i 2 OEr, E x i is an f i -tuple of variables, and E ˛D h˛1; : : : ; ˛f i is a tuple of strings over the variables on the left-hand side of the equation and the alphabet of terminal symbols in which each variable appears exactly once.",65,66
16622,6695204,Auxiliary symbol We can use auxiliary symbols to denote the head phrase position in a CFG rule.,1,2
16623,6695204,The first choice is to conveniently use a H symbol to indicate that current phrase is the head of its parent node.,9,10
16624,6695204,"The second choice is to practically use an L or R symbol to indicate the head of current node is its left or right child, in a binarized tree.",11,12
16625,6695204,"With head symbol With left/right symbol X l → X l #H, X r X l #L → X l , X r X r → X l , X r #H X r #R → X l , X r Three conversions Taking into account the above strategies, we propose three concrete DS to CS conversions: Flat conversion with H auxiliary symbol (FlatH).",2,3
16626,6695204,"With head symbol With left/right symbol X l → X l #H, X r X l #L → X l , X r X r → X l , X r #H X r #R → X l , X r Three conversions Taking into account the above strategies, we propose three concrete DS to CS conversions: Flat conversion with H auxiliary symbol (FlatH).",7,8
16627,6695204,"With head symbol With left/right symbol X l → X l #H, X r X l #L → X l , X r X r → X l , X r #H X r #R → X l , X r Three conversions Taking into account the above strategies, we propose three concrete DS to CS conversions: Flat conversion with H auxiliary symbol (FlatH).",71,72
16628,6695204,"Just as shown as the third tree in Figure 3 , we can learn a grammar from very flat constituency trees where the auxiliary symbol H is used for extracting dependencies.",24,25
16629,6695204,Right-to-left binarizing with H auxiliary symbol (BinH).,9,10
16630,6695204,Auxiliary symbol H is chosen.,1,2
16631,6695204,Right-to-left binarizing with LR auxiliary symbol (BinLR).,9,10
16632,34529108,This procedure can be understood as computing the alignment probability between the t-th target symbol and k-th source symbol.,16,17
16633,34529108,This procedure can be understood as computing the alignment probability between the t-th target symbol and k-th source symbol.,22,23
16634,34529108,"The hidden state annotation h t , together with the previous target symbol y t−1 and the context vector c t , is fed into a feedforward neural network to result in the conditional distribution and the whole network, consisting of the encoder, decoder and soft-alignment mechanism, is then tuned endto-end to minimize the negative log-likelihood using stochastic gradient descent.",12,13
16635,34529108,"For the source vocabulary, we constrained the size of BPE symbol vocabulary to 30, 000 tokens.",11,12
16636,16284829,"The ASJP Code collapses distinctions in vowel length, stress, tone and reduces all click sounds to a single click symbol.",21,22
16637,56657857,"Each response ends with a special end-of-sentence symbol, EOS.",11,12
16638,56657857,"Each response ends with a special end-of-sentence symbol, EOS.",11,12
16639,196105,A special STOP symbol is generated to terminate the sequence of children for a given parent.,3,4
16640,18013307,"VI?,eDAG VseV~ (r~ (~, s) iff s is derivable from the start symbol S and s has a-structure ~) Since this condition implicitly assmnes that F~ -1 is determined by an adequate parsing algorithm, this extension of the formalism is neutral with respect to the problem of the creativity 19 of the extension.",16,17
16641,18013307,"7 A gives a slightly simplified example of a lexical entry from [Halvorsen 87], written as the expansion of a lexical category symbol.",25,26
16642,3240710,"Symbol γ and δ are the buckets whose errors are at any position except γ and δ; symbol φ is the base syllable distance and φ∈{1, 2}; symbol ω represents bigram (ω=2) or trigram bucket index (ω= 3).",18,19
16643,3240710,"Symbol γ and δ are the buckets whose errors are at any position except γ and δ; symbol φ is the base syllable distance and φ∈{1, 2}; symbol ω represents bigram (ω=2) or trigram bucket index (ω= 3).",31,32
16644,4904798,The generation process ends with the EOS symbol.,7,8
16645,4904798,"Output words, y t , are selected in order and each caption ends with special symbol <EOS>.",16,17
16646,12525406,"2We assume here a single type of phrase, and thus each input symbol is either in a phrase or outside it.",13,14
16647,12525406,Local signals can indicate that an input symbol o is inside or outside a phrase (IO modeling) or they can indicate that an input symbol o opens or closes a phrase (the OC modeling) or some combination of the two.,7,8
16648,12525406,Local signals can indicate that an input symbol o is inside or outside a phrase (IO modeling) or they can indicate that an input symbol o opens or closes a phrase (the OC modeling) or some combination of the two.,26,27
16649,195766892,The σ-label matches any symbol and maps it to itself at no cost.,6,7
16650,9157199,"As noted previously, we add grammatical functions simply by concatenating them to the dependent phrasal categories and calling each unique symbol a PCFG nonterminal; this is an obvious way to adapt an existing PCFG parser, but not a sophisticated model of grammatical functions.",21,22
16651,5280402,"For the current state j, the conditional probability of drawing the k th author K k j and the i th topic Z i j pair, given all the hyperparameters and all the obeserved documents and authors except the current assignment (the exception is denoted by the symbol ¬j), is defined in Equation 2 .",50,51
16652,8867529,The syncbeam variation of beam search compares competing hypotheses only after consuming a special word boundary symbol rather than after each token.,16,17
16653,9995495,"In [3, 4] , however, compound parts were marked with a symbol, to separate them from normal words, resulting in improved translation quality compared to an unsplit baseline.",15,16
16654,9995495,"-e -a pojkvän (pojke + vän) boyfriend Combinations -a/-s -a/-e -a/-u -a/-o -e/-a -e/-s -el/-la -la/-el -ra/-er arbetsgrupp (arbete + grupp) working group All but the last part of a word were marked with a symbol, which was used in the merging step.",39,40
16655,9995495,"In addition we add the symbol '#' to all but the last part, to separate compound parts from other words, since compounds are not always compositional in meaning.",5,6
16656,4371490,"The generated string when traversing a path is not relevant for the computations involved in this article, so, to simplify, the symbol information is omitted in subsequent definitions.",24,25
16657,4371490,"Given a state of a left-to-right PFA that has two loops, each of which is labeled with a different symbol (like state 2 in Figure 2 ) the substrings that can be composed using the loops in state 2 in increasing length and their probability is: p(a) + p(b) = 0.4 + 0.2 p(aa) + p(ab) + p(ba) + p(bb) = 0.4 2 + 2 • 0.4 • 0.2 + 0.2 2 p(aaa) + p(aab) + p(aba) + p(abb) + p(baa)+ p(bab) + p(bba) + p(bbb) = 0.4 3 + 3 • 0.4 2 • 0.2 + 3 • 0.4 • 0.2 2 + 0.4 3 . . .",24,25
16658,4371490,"Note that the previous terms are related to binomial coefficients and the product of a polynomial with two terms (there are two terms because there are two loops, each of which has a different symbol).",36,37
16659,4371490,"Thompson (1974) points out a problem when the start symbol of the grammar is not proper (see Condition 4, page 611), although he mentioned an ""intermediation"" operation to overcome this problem.",11,12
16660,52088192,We briefly describe the BPE preprocessing procedure here: First the symbol vocabulary is initialized with all characters in the training set.,11,12
16661,52088192,"The frequency of each symbol pair is calculated, and we iteratively merge the most frequent pair to create a new symbol.",4,5
16662,52088192,"The frequency of each symbol pair is calculated, and we iteratively merge the most frequent pair to create a new symbol.",21,22
16663,52088192,"Very frequent character n-grams, such as frequent words, eventually become a single symbol.",16,17
16664,4311889,"Throughout this article, we also require that the root of a smooth tree decomposition contains k + 1 copies of the special symbol $, with vertices of G being added one at a time in the bags below the root.",23,24
16665,4311889,"Add a new root containing k + 1 instances of the special symbol $, and intermediate bags connecting the root to B adding one vertex of B at a time, and removing instances of $.",12,13
16666,4311889,"v n ], ∅), meaning that the stack and edge set are initially empty, and the cache is filled with m occurrences of the special symbol $.",29,30
16667,4311889,"If we consider the first element of the buffer as an additional stack element sitting on the top of the top-most stack symbol, the two formulations are equivalent.)",24,25
16668,464827,"The French word is predicted conditioned only on the English word, and each English word can generate at most one French word, or can generate a NULL symbol, representing deletion.",29,30
16669,40807060,"The traditional formalisms are impoverished in their absence of an explicit representation of the denotations of each symbol, and the statistics-based word distributions do not support the compositionality required of semantics since it is unclear how to link together two separate word distributions in a semantically meaningful way.",17,18
16670,4366526,a k } that contain the same number of occurrences of each symbol in this alphabet.,12,13
16671,4366526,"In order to introduce our algorithm for DAG automata, we therefore consider the analogous problem for finite automata: Given an input string w, find the total weight of all runs of a nondeterministic weighted finite automaton M on w. Let Q be the state set of M. A naïve algorithm for this problem would consider all possible assignments of states in Q to the |w| + 1 inter-symbol positions of w, under the restriction that the first position is assigned the unique starting state for M. For each such assignment, we then check against M's transitions that it corresponds to a run of M and, if this is the case, we add in the weight of that run.",72,73
16672,4366526,"We view w as a sequence of tokens w i from the alphabet of M. Symbols s and F denote the initial state and the final state set, respectively, of M. Symbol δ denotes the transition function, mapping a pair of states and an input symbol from M to a weight.",48,49
16673,4366526,"We call each chart entry α[i, r] a partial analysis of w. Observe that each partial analysis of w is uniquely identified by the inter-symbol position i we have reached on w, and by the state r we have reached on M. The complexity analysis of Algorithm 1 is rather straightforward.",28,29
16674,4366526,"Let D be an input DAG and let M be our DAG automaton with state set Q. In order to strengthen the similarity with the string case, we view the nodes of D like the tokens of w and the edges of D like the inter-symbol positions of w. A naïve algorithm, similar to the one for finite automata, can be developed for computing the total weight for all runs of M on D. We iterate over all possible assignments of states from Q to edges in E, that is, over all runs, and sum up their weights.",48,49
16675,4366526,"For every symbol σ ∈ Σ, we let σ be a fresh copy of σ.",2,3
16676,4366526,"Define a unary language to be a language that only uses one symbol; that is, a language L ⊆ M({q}) for some symbol q ∈ Q. Next, we give two equivalent characterizations of a class of regular (or recognizable) languages of multisets, analogous to regular expressions and finite automata for languages of strings.",12,13
16677,4366526,"Define a unary language to be a language that only uses one symbol; that is, a language L ⊆ M({q}) for some symbol q ∈ Q. Next, we give two equivalent characterizations of a class of regular (or recognizable) languages of multisets, analogous to regular expressions and finite automata for languages of strings.",26,27
16678,4366526,"Note also that we are overloading symbol Q, which is used to denote the state set of a DAG automaton as well as the input alphabet of a multiset automaton.",6,7
16679,4366526,"For the second statement, now let A 1 = (S 1 , Q 1 , τ 1 , s 1 , ρ 1 ) and A 2 = (S 2 , Q 2 , τ 2 , s 2 , ρ 2 ) recognize L 1 and L 2 , respectively, where Q 1 ∩ Q 2 = ∅. Then the shuffle product (Hopcroft and Ullman 1979, p. 142) of A 1 and A 2 is the automaton that simulates A 1 and A 2 together, feeding each input symbol to either machine but not both.",97,98
16680,4366526,"Let Q be the state set of M and let Q A = Q ∪ Q , where Q = {q | q ∈ Q} is a set of fresh copies of the states in Q. We compile all the transitions on an input symbol σ ∈ Σ into a single m-automaton A σ over Q A , using the states in Q and Q to distinguish between incoming and outgoing edges.",46,47
16681,4366526,"Let σ ∈ Σ be an input symbol of M. The first two rules here apply at the root of a treelet T v derived from a vertex v of D labeled with σ, and stipulate initial and final states of some computation in A σ : The next two rules apply at nodes that are internal to a treelet T v .",7,8
16682,4366526,"16) Thus, as with the algorithm of Section 6 for non-extended DAGs, the running time is exponential in the treewidth of the input DAG, linear in the total size of the input DAG, and, for a fixed treewidth, polynomial in the number of states of the extended automaton and in the number of states of the largest m-automaton A σ for the transitions on a single input symbol σ.",78,79
16683,4366526,"Similarly to Section 7.4.1, we compile all transitions of M on an input symbol σ ∈ Σ into a single m-automaton A σ = (Ξ σ , Q A , τ σ , s σ , ρ σ ).",14,15
16684,1471139,"Following Collins, 1997 , words occurring fewer than four times in training were replaced with the symbol *UNKNOWN* and tagged with the output of the part-of-speech tagger described in Ratnaparkhi, 1996 .",17,18
16685,2956077,"Four binary features are also extracted with value 1 if t i is a stop word, punctuation symbol, proper noun or numeral.",18,19
16686,2846882,In Query 2 the shorthand '/' symbol represents the hierarchy operation shown in Translation 2.,8,9
16687,16108973,"The most widely known phonetic symbol sets used to transcribe Mandarin Chinese are the Mandarin Phonetic Alphabet (MPA, also called Zhu-in-fu-hao) and Pinyin (Han-yu-pinyin), which have been officially used in Taiwan and Mainland China, respectively, for many years.",5,6
16688,584,"Every symbol used in the algorithm is replaced by a pair of symbols, where the second member of the pair is either a 0 or a 1 depending on whether the first member is a marker or not.",1,2
16689,584,"2 As the first step in the algorithm, O's are inserted after every symbol in the input string to indicate that initially every symbol is a non-marker.",15,16
16690,584,"2 As the first step in the algorithm, O's are inserted after every symbol in the input string to indicate that initially every symbol is a non-marker.",25,26
16691,584,"Similarly, the following macro can be used to insert a 0 after every symbol in an arbitrary expression E. 2This approach is similar to the idea of laying down tracks as in the compilation of monadic second-order logic into automata Klarlund (1997, p. 5) .",14,15
16692,584,"In fact, this technique could possibly be used for a more efficient implementation of our algorithm: instead of adding transitions over 0 and 1, one could represent the alphabet as bit sequences and then add a final 0 bit for any ordinary symbol and a final 1 bit for a marker symbol.",45,46
16693,584,"In fact, this technique could possibly be used for a more efficient implementation of our algorithm: instead of adding transitions over 0 and 1, one could represent the alphabet as bit sequences and then add a final 0 bit for any ordinary symbol and a final 1 bit for a marker symbol.",54,55
16694,584,"For example, the sequence [i,0] represents a non-marker use of the symbol I. Utilities Before describing the algorithm, it will be helpful to have at our disposal a few general tools, most of which were described already in Kaplan and Kay (1994) .",16,17
16695,584,introduce 0 after every symbol % (a b c => a 0 b 0 c 0). %,4,5
16696,2452020,"Symbols can be intervals of symbols, or the 'Any'variable which matches any symbol.",14,15
16697,680807,"Wyoming, who became an overnight symbol of antigay violence after he was found dangling from the fence by a passerby from a bar by saying they too were gay and one of their girlfriends said the The slight, soft-spoken 21-year-old Shepard, a freshman at the University of Wyoming, who became an overnight symbol of anti-gay violence after he was found dangling from the fence by a passerby had embarrassed one of the new ads in that supposedly hate-free crusade.",6,7
16698,680807,"Wyoming, who became an overnight symbol of antigay violence after he was found dangling from the fence by a passerby from a bar by saying they too were gay and one of their girlfriends said the The slight, soft-spoken 21-year-old Shepard, a freshman at the University of Wyoming, who became an overnight symbol of anti-gay violence after he was found dangling from the fence by a passerby had embarrassed one of the new ads in that supposedly hate-free crusade.",62,63
16699,18578354,A reserved symbol separates sentences.,2,3
16700,2816192,"Where standard semantic formalisms would map the verb write to a write' symbol, we map it to a cluster identifier such as relation37, which the noun author may also map to.",13,14
16701,3879136,"w i−1 ) (4) where w is a vector of input tokens, f t−1 is the index of the previous fixation, f t is the index of the current fixation, T is a random variable over syntactic trees and T i is a terminal symbol in a tree.",49,50
16702,3879136,"w i−1 ) (6) where again w is a vector of input tokens, f t is the index of the current fixation, f t+1 is the index of the next fixation, T is a random variable over syntactic trees and T i is a terminal symbol in a tree.",50,51
16703,203611841,"x n−1 , where symbols are drawn from the alphabet Σ, a probabilistic model S assigns to the next symbol x n ∈ Σ the conditional probability p s [x n | x n−1 . . .",20,21
16704,203611841,"Further, to avoid redundancy that leads to inefficiency, we assume the target model is deterministic, which requires at each state there is at most one transition labeled with a given symbol.",33,34
16705,203611841,"A probabilistic model p over Σ is a probabilistic distribution over the next symbol x n , given the previous symbols x n−1 , such that 1 x∈Σ p(x n = x|x n−1 ) = 1 ∧ ∀x ∈ Σ, p(x n = x|x n−1 ) ≥ 0.",13,14
16706,203611841,"Without loss of generality, we assume that the model maintains an internal state q and updates it after observing the next symbol.",22,23
16707,203611841,1) The symbol $ is used as a stopping criterion.,3,4
16708,203611841,"Similar to Equation 1, we assume a symbol $ ∈ Σ such that L(A) ⊆ {x n ∈ Σ * : x n = $ and x i = $ , i < n}.",8,9
16709,203611841,Thus all successful paths are terminated by the symbol $ .,8,9
16710,203611841,"For a symbol x ∈ Σ and a state q ∈ Q of a deterministic, probabilistic WFA A, define a distribution p a (x|q) w if (q, x, w, q ) ∈ E and p a (x|q) 0 otherwise.",2,3
16711,203611841,We assume each string in L(A) is terminated by the symbol $ as before.,11,12
16712,203611841,"A deterministic ϕ-WFA is backoff-complete if a failure transition from state q to q implies L[q] ∩ Σ ⊆ L[q ] ∩ Σ. Further, if ϕ / ∈ L[q ], then the containment is strict: L[q] ∩ Σ ⊂ L[q ] ∩ Σ. In other words, if a symbol can be read immediately from a state q it can also be read from a state failing (backing-off) from q and if q does not have a backoff arc, then at least one additional label can be read from q that cannot be read from q. For example, the topology depicted in Figure 1 has this property.",56,57
16713,203611841,"For a symbol x ∈ Σ and a state q ∈ Q of a deterministic, probabilistic ϕ-WFA A, define p * a (x|q) w if (q, x, w, q ) ∈ E * [q] and p * a (x|q) 0 otherwise.",2,3
16714,203611841,"It is convenient to define a companion distribution p a ∈ P (A) to p * a as follows: 3 given a symbol x ∈ Σ ∪ {ϕ} and state q ∈ Q, define When A = (Σ, Q, E, i, f ) is an unweighted deterministic, backoff-complete ϕ-WFA, we denote by P * (A) the set of all probabilistic models p * a representable as a weighted ϕ-WFA Â = (Σ, Q, Ê, i, f ) of same topology as A with Ê ={(q, x, p a (x|q), q ) : (q, x, 1, q ) ∈ E, x ∈ Σ} ∪ {(q, ϕ, α(q, q ), q ) : (q, ϕ, 1, q ) ∈ E} where p a ∈ P (A) is the companion distribution to p * a and α(q, q ) = p a (ϕ|q)/d(q, q ) is the weight of the failure transition from state q to q with d(q, q ) = 1 − x∈L[q]∩Σ p a (x|q ).",25,26
16715,233365123,In the GUM data set the X symbol is nonetheless more sparse.,7,8
16716,221097905,We use the + symbol to denote a combined model.,4,5
16717,5242162,"2008) discussed treatment of hyphened compounds in translation into German by splitting at hyphens and treat the hyphen as a separate token, marked by a symbol.",27,28
16718,5242162,"They marked split parts with a symbol, and merged every word in the output which had this symbol with the next word.",6,7
16719,5242162,"They marked split parts with a symbol, and merged every word in the output which had this symbol with the next word.",18,19
16720,5242162,"They also mark morphs with a symbol, and in addition normalize affixes to standard form.",6,7
16721,5242162,"Three types of marking have been investigated, no marking at all (unmarked), a marking symbol that is concatenated to all parts but the last (marked), or using a separate symbol between parts (sepmarked).",18,19
16722,5242162,"Three types of marking have been investigated, no marking at all (unmarked), a marking symbol that is concatenated to all parts but the last (marked), or using a separate symbol between parts (sepmarked).",36,37
16723,5242162,"Parts are normalized in the unmarked and sepmarked schemes, but left in their compound form in the marked scheme, since the symbol separates them from ordinary words in any case.",23,24
16724,5242162,"In summary, the three markup schemes use the following combinations, exemplified by the result of splitting the word begrüßenswert (welcome, literally worth to welcome) • Unmarked: no symbol, normalization, special POS-tags begrüßen ADJ-PART wert ADJ • Marked: symbol on parts, no normalization, special POS-tags begrüßens# ADJ-PART wert ADJ • Sepmarked: symbol as separate token, normalization, ordinary POS-tags begrüßen VV @#@ COMP wert ADJ Merging There is no guarantee that compound parts appear in a correct context in the translation output.",33,34
16725,5242162,"In summary, the three markup schemes use the following combinations, exemplified by the result of splitting the word begrüßenswert (welcome, literally worth to welcome) • Unmarked: no symbol, normalization, special POS-tags begrüßen ADJ-PART wert ADJ • Marked: symbol on parts, no normalization, special POS-tags begrüßens# ADJ-PART wert ADJ • Sepmarked: symbol as separate token, normalization, ordinary POS-tags begrüßen VV @#@ COMP wert ADJ Merging There is no guarantee that compound parts appear in a correct context in the translation output.",50,51
16726,5242162,"In summary, the three markup schemes use the following combinations, exemplified by the result of splitting the word begrüßenswert (welcome, literally worth to welcome) • Unmarked: no symbol, normalization, special POS-tags begrüßen ADJ-PART wert ADJ • Marked: symbol on parts, no normalization, special POS-tags begrüßens# ADJ-PART wert ADJ • Sepmarked: symbol as separate token, normalization, ordinary POS-tags begrüßen VV @#@ COMP wert ADJ Merging There is no guarantee that compound parts appear in a correct context in the translation output.",71,72
16727,5242162,"For the sepmarked system, coordinated compounds are handled as part of the symbol algorithms, by using the special markup symbol that indicates them.",13,14
16728,5242162,"For the sepmarked system, coordinated compounds are handled as part of the symbol algorithms, by using the special markup symbol that indicates them.",21,22
16729,5242162,The POSmatch and symbol algorithms make additional errors on coordinated compounds.,3,4
16730,5242162,"The simpler symbol-based methods, often have similar scores, and in a few cases even better.",2,3
16731,5242162,There are small differences between the POS-match and symbol algorithms.,10,11
16732,531808,"E.g., the two senses of letter ""The conventional characters of the alphabet used to represent speech"" and ""A symbol in an alphabet, bookstave"" (taken from WN and WKT, respectively) are clearly equivalent and should be aligned.",22,23
16733,203688987,"For example, a very large character set, such as that used for Chinese, can be impractical to represent on a keyboard requiring direct selection of characters; hence specialized encoding methods are generally used based on smaller symbol sets.",40,41
16734,203688987,"Given a lexicon with words in the native script and possible romanizations of those words (see §4.1 for specifics on our data), expectation maximization is used to derive pairwise symbol alignments.",33,34
16735,203688987,"For example, ाचार and ""bhrashtachar"" may yield a pairwise symbol alignment of: भ:b ◌् :h र:r ϵ:a ष:s ◌् :h ट:t ◌ा:a च:c ϵ:h ◌ा:a र:r where each symbol is composed of an input (native script) unicode codepoint (or ϵ, denoting the empty string) and an output (Latin script) unicode codepoint (or ϵ).",12,13
16736,203688987,"For example, ाचार and ""bhrashtachar"" may yield a pairwise symbol alignment of: भ:b ◌् :h र:r ϵ:a ष:s ◌् :h ट:t ◌ा:a च:c ϵ:h ◌ा:a र:r where each symbol is composed of an input (native script) unicode codepoint (or ϵ, denoting the empty string) and an output (Latin script) unicode codepoint (or ϵ).",54,55
16737,203688987,These symbol pairs then become tokens in an n-gram language model encoded as an automaton.,1,2
16738,203688987,"To read a string off of the input side of such a lattice, we take the last symbol of the bikey at each transition, with ϵ representing the empty string.",18,19
16739,203688987,"For each symbol in the (Latin script) input strings, we sample a touch point from Gaussian distributions in two dimensions, with mean value at the center of the key.",2,3
16740,5199824,"A Mention is easily recognizable because all of them start with the symbol ""@"" followed by the user name.",12,13
16741,5199824,"Anybody can begin a new topic by typing the name of the topic preceded by the symbol ""#"".",16,17
16742,5416706,"Beginning at the bottom, we assign to these constituents the relative scoping ranges of 1-2, 2-3, and 3-$, respectively, where $ is a terminal symbol.",33,34
16743,479235,"The regular expressions of XFST The standard set of regular expressions, originally introduced by Kleene, has just six syntactic forms: the one-symbol expression c, the empty string expression 0, the empty language expression e, the concatenation A B, the union A I B, and the Kleene closure A*.",26,27
16744,479235,"In addition, there are some categories not directly visible to the user: l~E of regular match constraint expressions, RCR of regular context expressions, ROE of regular operation expressions, and RSE of regular symbol expressions.",39,40
16745,479235,"For instance, if we have an interface implementing translation from XFST to natural language, we can type in the string [a J b]+ and get the following output (actually as ~ code, here typeset): English expressions for language : nonempty sequence of symbols from the list 'a', 'b' French expressions for language : sgquence non vide de symboles de la liste 'a', 'b' English expressions for relation : accept a nonempty sequence of symbols from the list 'a"" 'b' as such ~vpeatedly accept a symbol from the list 'a ', 'b' as such, as long as applicable but at least once l~epeatedly not only accept 'a' as such but also accept 'b' as such, as long as applicable but at least once French expressions for relation : accepter une sgquence non vide de symboles de la liste 'a"" 'b' telle quelle [aisant rgpgtition, accepter un symbole de la liste 'a', 'b ' tel quel aussi longtemps qu'applicable mais au moins une lois faisant rdp~tition, non seulement accepter 'a' tel quel mais aussi accepter 'b' tel quel aussi longtemps qu 'applicable mais au moins une lois Because the input expression is ambiguous between a language and a relation, it can be expressed both as a common noun and as an instruction.",104,105
16746,479235,"Both the formal code and the corresponding English and French texts are easier to understand if organized in sequences of shorter definitions: A vowel is a symbol from the list %', 'e', 'i', 'o', 'u', 'y'.",27,28
16747,479235,"A consonant is a symbol from the list 'd', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'r', 's', 't', 'v'.",4,5
16748,160009505,"x n−1 , where symbols are drawn from the alphabet Σ, a probabilistic model S assigns probability to the next symbol x n ∈ Σ by p s [x n |x n−1 . . .",21,22
16749,160009505,"Further, to avoid redundancy that leads to inefficiency, we assume the target model is deterministic, which requires that at each state there is at most one transition labeled with a given symbol.",34,35
16750,160009505,"A probabilistic model p over Σ is a probabilistic distribution over the next symbol x n , given the previous symbols x n−1 , such that 2 x∈Σ p(x n = x|x n−1 ) = 1 and p(x n = x|x n−1 ) ≥ 0, ∀x ∈ Σ Without loss of generality, we assume that the model maintains an internal state q and updates it after observing the next symbol.",13,14
16751,160009505,"A probabilistic model p over Σ is a probabilistic distribution over the next symbol x n , given the previous symbols x n−1 , such that 2 x∈Σ p(x n = x|x n−1 ) = 1 and p(x n = x|x n−1 ) ≥ 0, ∀x ∈ Σ Without loss of generality, we assume that the model maintains an internal state q and updates it after observing the next symbol.",71,72
16752,160009505,"Let the language L(p) ⊆ Σ * defined by the distribution p be L(p) {x n ∈ Σ * : p(x n ) > 0 and x n = $ and x i = $, ∀ i < n} (2) The symbol $ is used as a stopping criterion.",48,49
16753,160009505,"D(p s ||p a ) = ∞ n=1 x n p s (x n ) log p s (x n ) p a (x n ) = ∞ n=1 x n p s (x n ) n i=1 log p s (x i |x i−1 ) p a (x i |x i−1 ) = ∞ n=1 n i=1 x n p s (x n ) log p s (x i |x i−1 ) p a (x i |x i−1 ) = ∞ n=1 n i=1 x n p s (x i−1 )p s (x i |x i−1 )p s (x n i+1 |x i ) log p s (x i |x i−1 ) p a (x i |x i−1 ) = ∞ i=1 x i−1 p s (x i−1 ) x i p s (x i |x i−1 ) log p s (x i |x i−1 ) p a (x i |x i−1 ) • n≥i x n i+1 p s (x n i+1 |x i ) = ∞ i=1 x i−1 p s (x i−1 ) x i p s (x i |x i−1 ) log p s (x i |x i−1 ) p a (x i |x i−1 ) By definition, the probability of the next symbol conditioned on the past just depends on the state.",234,235
16754,160009505,"Similar to Equation (2), we assume a symbol $ ∈ Σ such that L(A) ⊆ {x n ∈ Σ * : x n = $ and x i = $, ∀ i < n} Thus all successful paths are terminated by the symbol $.",10,11
16755,160009505,"Similar to Equation (2), we assume a symbol $ ∈ Σ such that L(A) ⊆ {x n ∈ Σ * : x n = $ and x i = $, ∀ i < n} Thus all successful paths are terminated by the symbol $.",49,50
16756,160009505,"For a symbol x ∈ Σ and a state q ∈ Q of a deterministic, probabilistic WFA A, define a distribution p a (x|q) w if (q, x, w, q ) ∈ E and p a (x|q) 0 otherwise.",2,3
16757,160009505,We assume each string in L(A) is terminated by the symbol $ as before.,11,12
16758,160009505,"A probabilistic (or stochastic) ϕ-WFA satisfies e∈E * [q] w[e] = 1 and w [e] ≥ 0, ∀q ∈ Q − {f } A deterministic ϕ-WFA is backoff-complete if a failure transition from state q to q implies L[q] ∩ Σ ⊆ L[q ] ∩ Σ. Further, if ϕ / ∈ L[q ], then the containment is strict: L[q] ∩ Σ ⊂ L[q ] ∩ Σ. In other words, if a symbol can be read immediately from a state q it can also be read from a state failing (backing-off ) from q and if q does not have a backoff arc, then at least one additional label can be read from q that cannot be read from q. For example, both topologies depicted in Figure 2 have this property.",88,89
16759,160009505,"For a symbol x ∈ Σ and a state q ∈ Q of a deterministic, probabilistic ϕ-WFA A, define p * a (x|q) w if (q, x, w, q ) ∈ E * [q] and p * a (x|q) 0 otherwise.",2,3
16760,160009505,"It is convenient to define a companion distribution p a ∈ P (A) to p * a as follows: 7 Given a symbol x ∈ Σ ∪ {ϕ} and state q ∈ Q, define p a (x|q) p * a (x|q) when x ∈ L[q] ∩ Σ, p a (ϕ|q) 1 − x∈L[q]∩Σ p * a (x|q), and p a (x|q) 0 otherwise.",25,26
16761,160009505,"Each approach relies on the training corpus and 32k vocabulary, with every out-of-vocabulary word replaced by a single OOV symbol .",24,25
16762,160009505,"Baseline corpus trained models: We count character 5-grams from the k-anonymized corpus, then remove all n-grams that include the symbol (in any position) prior to smoothing and normalization.",27,28
16763,160009505,Word trigram sampled model: First we count word trigrams in the k-anonymized corpus and discard any n-gram with the symbol (in any position) prior to smoothing and normalization.,24,25
16764,160009505,"As this model is still closed vocabulary (see below), we additionally smooth the unigram distribution with a character trigram model trained from the words in the symbol table (and including the 50 extra counts for every printable ASCII character as with the other methods).",29,30
16765,160009505,"Briefly, for every state q in the n-gram automaton, the set of words labeling transitions leaving q are represented as a trie of characters including a final end-of-word symbol.",36,37
16766,160009505,"Each resulting transition labeled with the end-of-word symbol represents the last transition for that particular word spelled out by that sequence of transitions, hence is assigned the same next state as the original word transition.",11,12
16767,160009505,"To make it open vocabulary, we further back off the character trie leaving the unigram state to a character n-gram model estimated from the symbol table (and additional ASCII character observations).",27,28
16768,160009505,"We encode the test set as a sequence of characters, without using the symbol table because our models are intended to be open vocabulary.",14,15
16769,1109319,"爸) m [m a] (媽) f [f a] (花) t [t a] (打) tS [tS y] (朱) n [n a] (拿) s [s a] (沙) S [S y] (書) l [l a] (啦) j [j åu] (憂) k [k a] (加) N [pH a N] (烹) w [w a] 蛙 h [h a] (蝦) I [s I k] (色) i [s i] (絲) E [s E] (借) U [s U N] (鬆) u [f u] (夫) pH [pH a] (扒) tH [tH a] (他) ts [ts i] (之) tsH [tsH i] (痴) tSH [tSH y] (處) kH [kH a] ( 卡 ) kW [kW a] (瓜) kWH [kWH a] (誇) y [S y] (書) ø [h ø] (靴) a [s a] (沙) å [s å p] (濕) P [s P t] (恤) ç [s ç] (梳) ei [h ei] (稀) Eu [t Eu] (投) ai [w ai] (威) Py [s Py] (衰) åi [s åi] (西) ui [f ui] (灰) iu [s iu] (燒) åu [s åu] (收) au [s au] (筲) çi [s çi] (鰓) ou [s ou] (鬚) English phonemes IPA symbol Example p [p aI] (pie) m [m aI] (my) f [f l aI] (fly) t [t aI] (tie) tS [tS I n] (Chin) n [n E t] (net) s [s Q t] (sat) S [S aI] (shy) l [l aI] (lie) j [j u] (you) k [k aI t] (kite) N [h Q N] (hang) w [w aI] (why) h [h aI] (high) I [b I d] (bid) i [b i t] (beat) E [b E d] (bed) U [g U d] (good) u [b u t] (boot) b [b aI] (buy) v [v aI] (vie) T [T I N] (thing) D [D e I] (they) d [d aI] (die) z [z u] (zoo) ® [® E n t] (rent) dZ [p e I dZ] (page) Z [Q Z '] (azure) g [g aI] (guy) e [b e I t] (bait) Q [b Q d] (bad) ' [b ' d] (bird) o [b o t] (boat) A [p A d] (pod) √ [b √ d] (bud) aU [k aU] (cow) aI [b aI] (buy) çI [b çI] (boy) In this section, we use IPA symbols to facilitate an intuitive comparison between Cantonese and English.",352,353
16770,912359,"Since for every labelling σ compatible with a lexicon for every type A, σ(A) contains at least one symbol different from 1, the set of labelled hyperconfigurations such that their yield equals a given α is finite.",20,21
16771,912359,"We want to prove that the language recognized by Lex G with distinguished symbol S is the permutation closure of the language generated by G: L(Lex G , S) = P erm(L(G, S)).",13,14
16772,912359,"This entails in particular L(Lex G , S) ⊆ P erm(L(G, S)) where S ∈ N is the distinguished nonterminal symbol.",24,25
16773,49557398,"Languages that are learnable by a finite-state-automaton learner using this notion of locality as its criterion for determining whether two states in a prefix tree are equivalent (and hence mergeable) are termed k, l-local, where k and l are coefficients determining how much context (in our case, how many syllables) to the left and the right, respectively, are considered at each edge of a symbol corresponding to a state.",79,80
16774,49557398,"incoming edges with the same symbols for both states (describing left context of the state under consideration for merging) and l, representing the length of the sequence of outgoing edges with the same symbol for both states (describing its right context in the prefix tree).",36,37
16775,45900206,Note that the fourth symbol from the end is a comma in all tag questions.,4,5
16776,218595858,"First, we select sarcastic or non-sarcastic tweets only when they appear in a dialogue (i.e., begins with ""@""-user symbol) and at least have two or more prior turns as conversation context.",24,25
16777,308827,A a(f ) ∈ N are called argument categories and f ∈ F is the function symbol.,16,17
16778,308827,The parser is incremental because all active items span up to position k and the only way to move to the next position is the SCAN rule where a new symbol from the input is consumed.,30,31
16779,5289670,"w n , and assume that τ 0 is a special start symbol, and that τ n+1 is a special stop symbol.",12,13
16780,5289670,"w n , and assume that τ 0 is a special start symbol, and that τ n+1 is a special stop symbol.",22,23
16781,245838240,"The core part, hours and minutes, are typically spelled as one string, looking like a decimal number (although they actually use the sexagesimal system, and sometimes a different punctuation symbol).",34,35
16782,37011029,The probabilities of the rules are conditioned on the parent rule and on the trigram centered at the first input symbol that would be covered by the rule.,20,21
16783,6796226,"In a CFG, there is a fixed finite set of categories N (usually called ""non-terminal symbols""), and one of them is designated as the initial symbol S ∈ N .",33,34
16784,6796226,The first result is their closure under quotient with a single symbol.,11,12
16785,6796226,"Lemma B. If L is a language over Σ described by a conjunctive grammar, and a ∈ Σ is any symbol, then there exists a conjunctive grammar that describes the language a −1 L = {w | aw ∈ L}. (",21,22
16786,6796226,"A conjunctive grammar G with the initial symbol S is in the odd normal form, if all its rules are of the following form, with A ∈ N , a ∈ Σ, B i , C i ∈ N , and a i ∈ Σ. A → a A → B 1 a 1 C 1 & . . . &",7,8
16787,6796226,"For each symbol a i , by Lemma B, there is a conjunctive grammar G i that describes the quotient a −1 i L. By Theorem C, this grammar can be assumed to be in the odd normal form.",2,3
16788,6796226,Let S i be the initial symbol of G i .,6,7
16789,6796226,"For the sake of uniformity, rules of the form S i → aA are replaced with S i → Y and Y → aA, and rules of the form A → a are replaced with A → Z and Z → a. Finally, a new conjunctive grammar for L is obtained by joining these grammars together, for all i, adding the following extra rules for the new initial symbol S. S → a 1 S 1 , . . . ,",73,74
16790,6796226,"Next, as observed by Kanazawa (1992) , its symbol-to-symbol homomorphic image h(CVP) must have an MALC-grammar as well.",11,12
16791,6796226,"Next, as observed by Kanazawa (1992) , its symbol-to-symbol homomorphic image h(CVP) must have an MALC-grammar as well.",15,16
16792,6796226,"a i h−1 ) can be derived in the conjunctive categorial grammar, and, since Now, the conjunctive categorial grammar is extended by adding a new symbol a n+1 to the original alphabet Σ = {a 1 , . . . ,",28,29
16793,6796226,"a i h−1 a i h has been substituted for a fresh symbol b = a n+1 , can be derived in the original conjunctive categorial grammar.",12,13
16794,6796226,"On the other hand, the class of languages generated by MALC-grammars is, by definition, closed under symbol-to-symbol homomorphisms.",21,22
16795,6796226,"On the other hand, the class of languages generated by MALC-grammars is, by definition, closed under symbol-to-symbol homomorphisms.",25,26
16796,6796226,"be a homomorphism that maps both digits to the question mark symbol, leaving all other symbols intact: h(0) = h(1) = ?,",11,12
16797,214612871,"= or <> as inequality symbol; • the order of constituents, for example, if one uses infix (2 + 3), postfix (2 3 +) or prefix (+ 2 3) notation; and • the number of tokens, for example, if one includes the keyword then in ""if-then-else"" statements or not.",6,7
16798,214612871,"This is why we use the category symbol V2 rather than transitive verb (TV): transitivity, which governs a direct object case, is a language-dependent feature that can be left to the concrete syntax.",7,8
16799,14543144,Each nonterminal symbol A ∈ (N G − pre(N G ) ) is a left-hand side in at least one ordered non-recursive rule.,2,3
16800,14543144,"In addition, the empty string cannot be derived from any nonterminal symbol and cycles are not allowed.",13,14
16801,14543144,"S ∈ N G is the start nonterminal symbol, and ∀A ∈ N G , S A (we use the same notation for the reflexive, transitive closure of ).",8,9
16802,14543144,"The language of a grammar G is the set of all strings generated from the start symbol S, i.e., L(G) = {w|w ∈ Σ + , S * G ⇒ w}.",16,17
16803,40458752,"Each symbol is represented by a row in an embedding matrix W e of size V × d, where V is the vocabulary size and d is the dimensionality of the embeddings.",1,2
16804,40458752,"This structure implies that the model's score for a text is the mean score over each symbol, which means that the score q(s i...j ) can be computed for any subsequence s i...j of a text without depending on the length of the sequence.",17,18
16805,2942573,"The final tagging for every word s i ∈ s is taken from a sub-sequence where s i is the last symbol, so that we compose the final tagging for the sequence s from the tags for words s i listed in bold: [s 1 ; s 1 s 2 ; s 1 s 2 s 3 ; ... ; s 1 s 2 ...s n ].",23,24
16806,14734025,"In Okanohara (2007) bad sentences used as negative training instances are drawn from the distribution P (w i |w i−N +1 , ..., w i−1 ): first the start symbol < s > is generated, then the next words are taken based on the word probability given the already generated words.",33,34
16807,13064315,"The c-structure in Figure 1 is derived by applying a sequence of rules from 2 to rewrite the symbol S, the grammar's start symbol, and then rewriting the preterminal categories according to the lexical rules.",20,21
16808,13064315,"The c-structure in Figure 1 is derived by applying a sequence of rules from 2 to rewrite the symbol S, the grammar's start symbol, and then rewriting the preterminal categories according to the lexical rules.",27,28
16809,13064315,"First, all occurrences of the symbol"" in the functional annotations of the daughters are replaced by a v ariable standing for the f-structure unit that assigns to the mother node.",6,7
16810,13064315,"Then for each o f the daughter categories, all occurrences of the symbol in its annotations are replaced by a variable pred 0 fallhsubji 0 tense past 3 7 7 7 7 7 7 5 Figure 1 : Piecewise c-and f-structure correspondence.",13,14
16811,13064315,"Our recipe for constructing G F may produce many categories and expansion rules that cannot play a role in any derivation, either because they are inaccessible from the root symbol, they do not lead to a terminal string, or because they involve individual descriptions that F does not satisfy.",31,32
16812,13064315,"With this strategy we insure that every category we construct can derive a terminal string, but we h a ve no guarantee that every bottom-up sequence will reach the root symbol.",33,34
16813,13064315,"If we start with an agenda containing the root symbol, create rules only to expand categories on the agenda, and place categories on the agenda whenever they appear for the rst time on the right side of a new rule, we get the e ect of a top-down exploration of the grammar.",9,10
16814,13064315,"We will only create categories and rules that are accessible from the root symbol, but we m a y still produce categories that derive no terminal string.",13,14
16815,9226412,"The second type of constraints relate to the FunQL-grammar itself, ensuring that the generated logical forms are meaningful for execution: • The type of argument expected by each non-terminal symbol must follow the FunQL grammar; • The number of arguments expected by each non-terminal symbol must follow the FunQL grammar; • When the expected number of arguments for a non-terminal symbol is reached, a RED operation must be called for the top-down system; for the bottom-up system this constraint is built within the NT-RED operation, since it reduces the expected number of arguments based on a specific non-terminal symbol.",35,36
16816,9226412,"The second type of constraints relate to the FunQL-grammar itself, ensuring that the generated logical forms are meaningful for execution: • The type of argument expected by each non-terminal symbol must follow the FunQL grammar; • The number of arguments expected by each non-terminal symbol must follow the FunQL grammar; • When the expected number of arguments for a non-terminal symbol is reached, a RED operation must be called for the top-down system; for the bottom-up system this constraint is built within the NT-RED operation, since it reduces the expected number of arguments based on a specific non-terminal symbol.",53,54
16817,9226412,"The second type of constraints relate to the FunQL-grammar itself, ensuring that the generated logical forms are meaningful for execution: • The type of argument expected by each non-terminal symbol must follow the FunQL grammar; • The number of arguments expected by each non-terminal symbol must follow the FunQL grammar; • When the expected number of arguments for a non-terminal symbol is reached, a RED operation must be called for the top-down system; for the bottom-up system this constraint is built within the NT-RED operation, since it reduces the expected number of arguments based on a specific non-terminal symbol.",72,73
16818,9226412,"The second type of constraints relate to the FunQL-grammar itself, ensuring that the generated logical forms are meaningful for execution: • The type of argument expected by each non-terminal symbol must follow the FunQL grammar; • The number of arguments expected by each non-terminal symbol must follow the FunQL grammar; • When the expected number of arguments for a non-terminal symbol is reached, a RED operation must be called for the top-down system; for the bottom-up system this constraint is built within the NT-RED operation, since it reduces the expected number of arguments based on a specific non-terminal symbol.",121,122
16819,56595641,"The symbol ""*ldd"" indicates long-distance dependencies; ""subj*ldd"" between the word ""涉及/involve"" and the word ""文件/documents"" represents a long-range subject-predicate relation.",1,2
16820,52995432,"In contrast to the original illustration, lost sounds are displayed with help of the dash ""-"" as a gap symbol, while missing words (where no reflex in Gothic or Latin could be found) are represented by the ""∅"" symbol.",22,23
16821,52995432,"In contrast to the original illustration, lost sounds are displayed with help of the dash ""-"" as a gap symbol, while missing words (where no reflex in Gothic or Latin could be found) are represented by the ""∅"" symbol.",46,47
16822,52995432,"While we use the dash as a symbol for gaps in alignment sites, we will use the character Ø (denoting the empty set) to represent missing data in correspondence patterns and alignment sites.",7,8
16823,52995432,"In the following, we will assume that two alignment sites are compatible, if they (a) share at least one sound that is not a gap symbol, and (b) do not have any conflicting sounds.",29,30
16824,18147858,"Each single block containing the hash symbol represents the actual word alignment points, whereas the large block represents phrase alignments.",6,7
16825,18147858,"Phrase alignments will always appear as rectangles and may include blocks that were not originally aligned (coloured, but no hash symbol).",22,23
16826,1909236,"We formally define a CFG in the usual way as G = N, T, R, S , where N is a set of nonterminal symbols (corresponding to source-side phrase and partof-speech tags); T is a set of source-side terminals (the lexicon), R is a set of production rules of the form η → γ, with η ∈ N and γ, which is a sequence of terminal and nonterminal symbols; and S ∈ N is the distinguished symbol.",94,95
16827,799726,"Firstly, it checks each pair if it contains any malicious word, say, a noisy symbol.",17,18
16828,201742900,"Moreover, we disable the word-final epsilon symbol, which had been introduced by Grönroos et al. (",9,10
16829,16437722,"The character-based approaches thus enable us to significantly speed up the softmax computation for generating a symbol, and we can train the NMT model and generate translations much faster.",18,19
16830,11908897,"<o> i indicates that o is the outcome number i in the EDU, the symbol // is used to separate the two annotation levels and brackets indicate how outcomes are attached.",17,18
16831,2603210,"There are several categories which speech for non-alphabet symbol ""/"" are silence; the duration for silence in prosodic parameter is still different to other senses.",10,11
16832,2603210,"w l count s pos w l count s pos 公車 -6 1 1 Na 公車 +5 4 3 Na 公車 -5 2 1 Na 公車 -7 1 6 Na 公車 -2 1 1 Na 公車 -5 2 6 Na 公車 -1 3 1 Na 公車 -2 1 6 Na 公車 -4 2 3 Na 公車 -1 2 6 Na 公車 -1 1 3 Na 公車 +5 8 6 Na W l count s pos 公車 p 7 1 Na 公車 p 3 3 Na 公車 f 4 3 Na 公車 f 5 6 Na 公車 f 8 6 Na , _ ) ( 1 set possi E = Φ 2 1 and Φ Φ ) ( max arg ) _ ( _ 2 j set possi s s TScore set possi j ∈ = Φ Based on the voting schemes, each token in CH L and CH R have a statistical probability value, which looks like the voting suffrage, assigned to each category of the non-alphabet symbol.",172,173
16833,2603210,"The example E is composed of word sequence W and contains three parts: chunk-L(CH L ), non-alphabet symbol TS (target symbol) and chunk-R(CH R ).",23,24
16834,2603210,"The example E is composed of word sequence W and contains three parts: chunk-L(CH L ), non-alphabet symbol TS (target symbol) and chunk-R(CH R ).",27,28
16835,2603210,Thus: (7) where N and M denote the number of sense category of target symbol and token (word) in word sequence W. Two problems should be considered for the Eq. (,17,18
16836,2603210,"8 ) can be expressed as: (9) where C(s,w) denotes the count of token w occurred in feature database for certain sense category s. TC(w) is the total count of token w in feature database for target symbol.",44,45
16837,2603210,"When computing the probability score of each word w for sense category s, we just need to use token count C(s,w) and total count TC(w) with respect to the sense category s and target symbol.",39,40
16838,2603210,The probability can be regarded further as a score for each token in CH L and CH R to vote for each category of non-text symbol.,27,28
16839,2603210,"10 ), the score 1 Score L and Score R of each token in CH L and CH R voting for sense category s j of non-text symbol can be computed as: , where J denotes the number of sense category for target symbol.",30,31
16840,2603210,"10 ), the score 1 Score L and Score R of each token in CH L and CH R voting for sense category s j of non-text symbol can be computed as: , where J denotes the number of sense category for target symbol.",47,48
16841,2603210,"The example (E1) contains 15 individual tokens (including symbol "":"").",11,12
16842,2603210,"The minimum difference between is +1.4% for outside testing of target symbol "":"".",12,13
16843,2603210,"Therefore sense category date 3 was excluded for target symbol ""/"" by the 1 st layer classifier.",9,10
16844,2603210,"s n Evaluation for the Effect of Word's Location In previous Section, the location of each token is just labeled two types: preceding (p) and following (f) the target symbol.",36,37
16845,2603210,"10 )-( 12 ) can be changed as follow: , where i is the location of word with respect to the non-text symbol , -m<= -i <=-1 and 1<=+i<=n.",27,28
16846,2603210,"Evaluation for Effect of Token Unit Until now, the sentence will be divided into two chunks: chunk-L(CH L ) and chunk-R(CH R ), which are in the left and right side of target symbol TS in sentence.",40,41
16847,2603210,"The location of each character will be considered same as described in previous 10 )-( 12 ) of probability scoring can be rewritten as: , ( 21 ) , ( 22 ) ( 23 ) where i is the location of character with respect to the non-text symbol , -m<= -i <=-1 and 1<=+i<=n.",52,53
16848,2603210,"At first, the weighting of token in different location with respect to the target symbol will be analyzed.",15,16
16849,2603210,"Weights for Individual Token It is our intuition that the nearer a token is to target symbol, the higher prediction capability to token is.",16,17
16850,2603210,"The target symbol ""/"" in ""24/7"" will be a silence.",2,3
16851,235482407,"However, in Chinese writing, people often concatenate many subsentences by commas into a long sentence (ended with a period symbol or a question mark).",22,23
16852,18993073,"Indeed, the addition of the constraints on types is done by the symbol ""&"".",13,14
16853,18993073,"Besides, the coindexations are preceded by the symbol ""#"".",8,9
16854,18993073,"The comments are preceded by the symbol "";"".",6,7
16855,18993073,"Moreover, a new type definition is done with the assistance of the symbol "":="".",13,14
16856,233029492,The character (symbol) having highest probability is given as the output.,3,4
16857,16169363,"CFG's rules present a single symbol on the left-hand-side, are a sufficiently powerful formalism to describe most of the structure in natural language.",6,7
16858,16169363, A start symbol S  V.  A finite set P  V  (V T)* of productions.,3,4
16859,16169363,Topdown parsing begins with the start symbol and attempt to derive the input sentence by substituting the right hand side of productions for non terminals.,6,7
16860,12139147,"The remaining terms are the so-called secondary features, in which the secondary feature ""buy|買"" is prefixed with the symbol ""*.""",23,24
16861,12139147,"For each of them, its prefixed symbol provides a relation between the lexical entry and the concept to which it corresponds.",7,8
16862,1241654,"We then have a Korean linguist transcribe the broadcaster's speech into Korean phoneme representation forms, and we then convert them into IPA symbol strings as broad transcriptions.",24,25
16863,1241654,We compare the result strings of our system and the human transcriptions based on the IPA symbol characters standing for Korean phonemes.,16,17
16864,8402389,"Ideally, no antecedent knowledge about the grammar of a language is necessary for successful extraction by means of our method, the 1) prescriptive form of the figure-to-be-extracted and 2) the symbol representing phrase and/or clause boundaries is the only information necessary.",40,41
16865,8402389,"In such a way could our PCRFs possibly become something little bit more than just another tool for stylometric analysis of textual corpora -in such a way they could possibly help answering a somewhat more fundamental question: ""What is the essence of figures of speech and how could they be represented within&by an artificial and/or organic symbol-manipulating agent?""",58,59
16866,29866654,"We applied lowercasing, symbol removal, stop word elimination, and stemming in processing of each token in the documents.",4,5
16867,16227604,"On the other hand, there are other instances, such as the phrasal verbs and word reduplications (as shown above), which require a combination of Kleene Closure, the Don't-Care symbol and Back-Referencing (a special device by which we can encode a class of word-reduplication as the pattern ( s rrir) where r is the reduplicated word) [4] .",36,37
16868,16227604,These exercise all aspects of RL operators and the Don't-Care symbol.,12,13
16869,9193821,1) It consists of a non-terminal symbol A on the left-hand side and a string of symbols β on the right-hand side.,9,10
16870,9193821,"For (1), this is C R = (1 + |β|) log |Σ|, ( 2 ) where Σ is the symbol set and |Σ| is the number of symbols in Σ. The second kind is the cost to derive the sentences given the rules.",25,26
16871,9193821,"Otherwise, we expand the left-most non-terminal symbol, say X, in α by one of its derivation bodies 3 .",11,12
16872,9193821,"The cost is C D = m k=1 log |R(s k )| = log |R(S)| + log |R(X)| + log |R(X)|, ( 5 ) where m is the number of rules in the derivation sequence, s k is the non-terminal symbol for the kth derivation, and |R(s k )| is the number of rules in the CFG using s k as the left-hand side.",45,46
16873,9193821,"Combining ( 2 ) and ( 5 ), the total cost is C = p i=1 C R (i) + q j=1 C D (j) = p i=1 n i log |Σ| + q j=1 m j k=1 log |R(s k )|, (6) where p is the number of rules, q is the number of sentences, n i is the number of symbol tokens in rule i, and m j is the length of the derivation sequence for sentence j. Special-Case Analysis We will analyze the costs for two special CFGs in this section.",73,74
16874,9193821,"The first CFG, which we call the exhaustive CFG, uses every distinct sentence in the corpus as a direct derivation body of the start symbol S. The corpus is thus covered trivially.",26,27
16875,9193821,"The number of symbols for a rule is simply the number of words of the corresponding sentence n w , plus 1 (for the start symbol S), and |Σ| is the vocabulary size |V | of the corpus plus 1 (again for the start symbol).",26,27
16876,9193821,"The number of symbols for a rule is simply the number of words of the corresponding sentence n w , plus 1 (for the start symbol S), and |Σ| is the vocabulary size |V | of the corpus plus 1 (again for the start symbol).",48,49
16877,9193821,"The kind of rules we investigate in this study is of the form X → Y Z. The introduction of such a rule to the exhaustive CFG described in Section 2.2 has the following impacts on the cost: • Each occurrence of Y Z is replaced by X, so the total number of symbol tokens in the S derivation rules is reduced. •",55,56
16878,9193821,"Since there are two symbols on the right-hand side, the number of candidate rules is |Σ × Σ| = |Σ| 2 , where Σ is the current symbol set.",30,31
16879,9193821,"To choose one, we compute the bigram counts of all bigrams and use the bigram with the highest count as the right-hand side of the new rule, whose left-hand side is a new symbol.",39,40
16880,9193821,The rule cost reduces because the decrease in the number of tokens in the rules outweighs the increase in the size of symbol set.,22,23
16881,9193821,The symbol M 66 can be identified as a noun phrase (NP).,1,2
16882,9193821,"Finally, from the data compression and information theory [10] , one can design a different cost function that takes the symbol frequencies into account and achieves further reduction on the number of bits.",23,24
16883,2071397,POS Tagger The imported tagger produces a part-of-speech tag as an annotation for each word or symbol.,20,21
16884,2654419,"In other words, String numeral, denoted by S n : a true/false evaluation on whether the string contains any numeral symbol.",24,25
16885,2513777,"2) VJ(J c [7:00, 8:00] -4 (York_eathis_sandwich) (J)) If we indicate the tenseless sentence to be the symbol and the two point of time of subStevent to be s 1 and s2, we can have (TL.",26,27
16886,8092089,"Following Sag (1996) , the symbol 0 designates the ""sequence union"" or ""shuffle"" operation.",7,8
16887,219310049,A clitic is a symbol consisting of one to three letters that can be attached to the beginning or the end of a word.,4,5
16888,9152030,"For example, instead of one syntactic class for comma, and one for each o f the other punctuations, I have combined this into symbol, but each symbol has a head feature which is the symbol.",26,27
16889,9152030,"For example, instead of one syntactic class for comma, and one for each o f the other punctuations, I have combined this into symbol, but each symbol has a head feature which is the symbol.",30,31
16890,9152030,"For example, instead of one syntactic class for comma, and one for each o f the other punctuations, I have combined this into symbol, but each symbol has a head feature which is the symbol.",38,39
16891,14363866,"The symbol 1 stands for the empty string of types and will be usually omitted, as will be the dot that stands for multiplication.",1,2
16892,14363866,"At each stage, we choose a symbol v i represented by its index i, a type t in D(v i ) and a position p in t. We examine the simple type(s) placed just to the left of this position in some type assignment and store it (them) in the memory, where they are kept as a 'left parenthesis' waiting to be contracted with a simple type that might come later.",7,8
16893,52957274,"Data Manipulation and Batch Processing When training each model, we use all sentences in the respective training set, but we truncate all sentences longer than 35 words and pad all sentences shorter than 35 words with a special symbol so all have the same length.",40,41
16894,14361964,"which are denoted in Figure 2 by three nodes as follows: M (for emitting an aligned pair of symbols), X (for deleting a symbol from one word), and Y (for inserting a symbol) in the other word.",28,29
16895,14361964,"which are denoted in Figure 2 by three nodes as follows: M (for emitting an aligned pair of symbols), X (for deleting a symbol from one word), and Y (for inserting a symbol) in the other word.",40,41
16896,14361964,"The sc i and tc i nodes enforce consistency constraints by having a fixed observed value 1 where the only configuration of their parents is such that the source component of the edit operation variable z i is s i or an empty symbol and the target component of z i is t i or an empty symbol , and that z i does not generate ( , ).",43,44
16897,14361964,"The sc i and tc i nodes enforce consistency constraints by having a fixed observed value 1 where the only configuration of their parents is such that the source component of the edit operation variable z i is s i or an empty symbol and the target component of z i is t i or an empty symbol , and that z i does not generate ( , ).",57,58
16898,10500486,"A is a non-terminal symbol which could be one of the following: a prefix/infix/suffix, or (for duplication), Z, or X. Star (*) indicates 0 or more occurrences of the set in the bracket.",6,7
16899,245838318,"Sentences were presented region by region, and every sentence was preceded by a '+' symbol to signal where the sentence started.",17,18
16900,210936463,"In order to provide a compact notation, we introduce the following convention: let us denote a set of word types contained in a sentence or a textual n-tuple t by a symbol ||t||.",35,36
16901,8193839,Each sequence in C XNP consists of XTAG elementary trees and have to contain at least one UNKNOWN symbol regarding this fact that NPI contains the sentences that couldn't be modeled in XTAG grammars.,18,19
16902,13244912,"The probability of a symbol w with respect to this model M and to a context c can be estimated by: Pax, A c) = f (w, M, c) The information of a symbol w with respect to the model M and to a context c is defined by: I (w M c) _ -loge p (w I M , c) The entropy of a context c with respect to this model M is defined by: -I+ 3 Algorithm H(M, Contextual Entropy To find Chinese words we look for character sequences that are stable in the corpus in the sense that the components of a word are strongly correlated but appear in various contexts in the corpus.",4,5
16903,13244912,"The probability of a symbol w with respect to this model M and to a context c can be estimated by: Pax, A c) = f (w, M, c) The information of a symbol w with respect to the model M and to a context c is defined by: I (w M c) _ -loge p (w I M , c) The entropy of a context c with respect to this model M is defined by: -I+ 3 Algorithm H(M, Contextual Entropy To find Chinese words we look for character sequences that are stable in the corpus in the sense that the components of a word are strongly correlated but appear in various contexts in the corpus.",40,41
16904,6687003,"Each word has one or more senses, represented as a root symbol, which i s generally the concatenation of the English token, the ""^"" character, and the PAKTUS lexica l category (e.g., ""Set^Monotrans""), or as a simple structure involving a root, lexical category , inflectional mark, and sometimes a conceptual derivation (e .g.,",12,13
16905,234487218,"Here, the symbol ""Positive"" indicates that a sentence can be converted into relevant questions, that is, it should be extracted, and ""Negative"" the opposite.",3,4
16906,65091,"First of all, divide the text into some sentences according to the symbol of pause, such as: comma, full stop, etc.",13,14
16907,7309228,"The left-hand side of a colon symbol represents a meaning expression, and the right-hand side provides a logical formula.",8,9
16908,7309228,"The body of a meaning expression is shown in typewriter fonts, in which a capital letter represents a variable and the backslash symbol stands for lambda abstraction.",23,24
16909,7309228,A symbol consisting of a bar and a circle stands for implication in linear logic.,1,2
16910,7009825,The attribute ST (sentence type) refers to the sentence type denoted by a ST-marker and its value is either an atomary symbol like DECO°.,25,26
16911,7009825,The attribute SL (= speech level) is used for all parts of a sentence ender and its value is either an atomary symbol like INTIM.,23,24
16912,1645887,"The LHS of a generation rule can be any non-terminal symbol, whereas the RHS is a combination of non-terminal symbols and terminal symbols.",12,13
16913,1645887,A terminal symbol is a word or a string that appears in the output text.,2,3
16914,1645887,A non-terminal symbol do not appears in the output.,4,5
16915,1645887,"In order to produce flexible output texts, a non-terminal symbol in the RHS of a rule can also be a frame.",12,13
16916,1645887,"The rule set also has an [s_ symbol, which represents for sentence.",9,10
16917,15717994,"For example, the mapped conceptual symbol from the word 'think' is 'v80', and the mapped conceptual symbol from the word 'idea' is 'r80'.",6,7
16918,15717994,"For example, the mapped conceptual symbol from the word 'think' is 'v80', and the mapped conceptual symbol from the word 'idea' is 'r80'.",22,23
16919,15717994,"The close relation between the two words is uncovered by the common symbol '80', as their relation in the fixed phrase 'think an idea'.",12,13
16920,15717994,"The framework unit of situation must be described as the following: EK name[EK conceptual symbol] | GBKm name[GBK conceptual symbol], the value of m is from 1 to 3.",15,16
16921,15717994,"The framework unit of situation must be described as the following: EK name[EK conceptual symbol] | GBKm name[GBK conceptual symbol], the value of m is from 1 to 3.",21,22
16922,39590661,The symbol -is similar to the minus.,1,2
16923,6996543,"As the HNC conceptual primitives give the words meaning in concept, so some words with different form can be expressed in same symbol when they have the same meaning, such as the ""distress"" and ""sad"" they have the same symbol in HNC conceptual primitives.",23,24
16924,6996543,"As the HNC conceptual primitives give the words meaning in concept, so some words with different form can be expressed in same symbol when they have the same meaning, such as the ""distress"" and ""sad"" they have the same symbol in HNC conceptual primitives.",45,46
16925,6996543,"The HNC conceptual primitives are a kind of artificial semantic symbol, their meaning is well defined, and the natural language words always have ambiguous meaning.",10,11
16926,204812180,"3 ) performs the deletions, substituting a special symbol δ for each deleted character.",9,10
16927,6593851,The symbol α will denote a specific part of speech tag sequence.,1,2
16928,6593851,"All occurrences of α is identified within a context of two adjacent tags or sentence boundaries: x α y. x denotes the symbol directly before α, and y directly after.",23,24
16929,5546857,"Also the wild card ""*"" is introduced as a symbol to match any attribute value.",11,12
16930,5546857,"The symbol Y signifies that the boundary in focus is a bunsetsu boundary, and N signifies that is not.",1,2
16931,3842045,"then set the symbol feature to 1, otherwise set it to 0.",3,4
16932,10608814,are the symbol name of the landmarks. (,2,3
16933,9937581,"Depending on the concepts applicabl e to a message, the inference engine activates appropriate rules of the rule base, whose terminal symbol s correspond to the various concepts acquired .",23,24
16934,7020117,"For example, if we mask (or ignore) the last bit of the phonetic symbol "" "" and ""17 "", both bit representations will be the same.",16,17
16935,17357371,"General notation is as follows: Zero anaphors are marked by a zero symbol '0' inside brackets [], followed by an equal sign '=' and the arrow symbols '<' and '>', corresponding to anaphora and cataphora relations, respectively, and a word indicating the head of the antecedent noun phrase (NP).",13,14
16936,6568714,"Also, because time (date of incident) played such an important role in the MUC-3 task, a new feature was added to th e preprocessor which bracketed time expressions in the text, calculated a date, and passed it on to th e parser as a symbol .",50,51
16937,29542818,"Each word has one or more senses, represented as a root symbol, which is generally the concatenation of the English token, the "" A"" character, and the PAKTUS lexical category (e .g., """,12,13
16938,31462105,"Let f u, be the valuation assigned to w, then for every n-ary predicate symbol P of 1,, f,(P) is a set of n-tuples of constants in L. Intuitively a context is a set of topics (thoughts) relevant to the conversation.",18,19
16939,16144093,Katakana is one of Japanese writing symbol types.,6,7
16940,16144093,Hiragana is one of the writing symbol types of Japanese.,6,7
16941,16144093,Kanji is another symbol type which is used to represent the words in graphic form.,3,4
16942,47385985,"V In order to make difference with the symbol in the article, the numbers in the structure tree are in the square brackets.",8,9
16943,3263023,"We define a total of 30 semantic relation types for WSD by referring mainly to the SELK (Sejong Electronic Lexicon of Korean) (Hong & Pak, 2001) and the Mikrokosmos ontology (Mahesh, 1996) , as shown in Table 1 headed-by, operated-by, controls, owner-of, represents, symbol-of, name-of, producer-of, composer-of, inventor-of, make, measured-in These semantic relation types cannot express all possible semantic relations existing among concepts, but experimental results demonstrated their usefulness for WSD.",62,63
16944,834625,"The same structural information can be represented in the following table form where the terminal nodes Mia, loves, and Kim are marked with a dummy symbol t: (9) Here, a unique code number is assigned to each node from which one can obtain information on the dominance and precedence relations among the nodes.",27,28
16945,834625,"In Phrase Structural Grammar, the terminal nodes Mia, loves, and Kim in a tree like (8) are first marked with a dummy symbol like t, as in (9).",26,27
16946,14378859,"However in order to use GATE for Vietnamese, we need to define our own tokenizer rules, token types, word, number, symbol, punctuation, etc.",25,26
16947,1314252,"But the interpretation function 7 of predicate symbols are splitted into 7r + and 7r -which give a predicate symbol its positive and negative domain respectively, i.e. the domains which return T and F respectively if the n-tuple of individuals which the arguments of the predicate denote belongs to the respective domains.",19,20
16948,1314252,"I.e., A, 10 B. Proof i) A model which assigns the empty set to every predicate symbol P, i.e. where 7r+ P) = cb, and 7r -(P) = 0, assigns the truth value N to every formula.",19,20
16949,13972464,The Phonetic and the Homophone Error problem in a language have been characterized as a symbol substitution problem.,15,16
16950,13972464,Phonetically equivalent symbols or symbol combinations in the language are grouped together.,4,5
16951,13972464,The Homophone Error problem can also be looked upon as a substitution problem in which the substitution of a correct symbol in a valid word by its phonetic equivalent generates another valid word in the language.,20,21
16952,13972464,"Internal Storage of a Word : The UNIT Concept An Unique Identification Number is assigned to each symbol in Bengali, i.e., vowels, consonants, conjuncts, matras, diacritics, numerals and special symbols, based on the collating sequence.",17,18
16953,13972464,This Symbol Number is derived from the ISCII codes of each basic symbol.,12,13
16954,13972464,c) The method of storage for numerals and special symbols is useful during editing when such a symbol may have to be replaced by a letter. (,18,19
16955,13972464,"The equivalence between symbols or symbol combinations in a group depends on several factors like the position of the symbol in a word (start, middle or end) and the matra attached to the symbol.",5,6
16956,13972464,"The equivalence between symbols or symbol combinations in a group depends on several factors like the position of the symbol in a word (start, middle or end) and the matra attached to the symbol.",19,20
16957,13972464,"The equivalence between symbols or symbol combinations in a group depends on several factors like the position of the symbol in a word (start, middle or end) and the matra attached to the symbol.",36,37
16958,13972464,The symbol in the input word is changed to the corresponding symbol in the dictionary word.,1,2
16959,13972464,The symbol in the input word is changed to the corresponding symbol in the dictionary word.,11,12
16960,13972464,If a word contains only one symbol belonging to one of the Phonetic Equivalence Groups then it will be included in the corresponding dictionary.,6,7
16961,13972464,Methodologies have been developed to distinctly identify the dictionary when a word has more than one symbol belonging to the same or different Equivalence Groups.,16,17
16962,13972464,"Whenever, two corresponding units mismatch in the input word and in the dictionary word but there is equivalence either in the character or in the matra then the input word symbol can be corrected to that in the dictionary word.",31,32
16963,13972464,"If a member of a unit does not have a corresponding procedure to execute, i.e., the symbol cannot have any phonetic error then it is explicitly validated.",18,19
16964,13972464,A symbol may also be explicitly validated by a detection and correction procedure.,1,2
16965,13972464,A procedure is called for execution when a particular symbol or one of the symbols in the associated Phonetic Equivalence Group appears in the input word.,9,10
16966,13972464,"Finally, if the modification is not successful, either it will be kept or the original symbol may be restored.",17,18
16967,18939724,"Translation System Description The procedure for constructing English-Arabic pairs is as follows: First, beginning with English name list which obtains each words English phonetic representations since these seemed to be most common English pronunciation symbol.",38,39
16968,18939724,Each diacritized symbol is mapped to the corresponding character.,2,3
16969,18939724,"First, the system should map the English phonetic symbols to the Diacritized Arabic Phonetic symbols which a scan has been made for the universal phonetic symbols for (vowels), and linked it with the audio symbols for Vowels in Arabic so that we can convert the English names to the universal phonetic symbols, and then convert the universal phonetic symbols to the Arabic phonetic symbol.",68,69
16970,18939724,"The set of symbols is called International Phonetic Alphabet (IPA) (International Phonetic Association, 2005) which its symbol is based on the Roman letters as shown in Figure 1 .",21,22
16971,5611528,A set of rules have been carried out to tell the researcher how to construct the WCK using concept symbol system.,19,20
16972,17407340,"Formula (2) describes the rule for the first pattern in detail: > (Na | Nb | Nca | Ncb) > Connective > (Na | Nb | Nca | Ncb) > (2) where symbol "">"" represents several words or no word, symbol ""|"" represents logical operator ""OR"".",41,42
16973,17407340,"Formula (2) describes the rule for the first pattern in detail: > (Na | Nb | Nca | Ncb) > Connective > (Na | Nb | Nca | Ncb) > (2) where symbol "">"" represents several words or no word, symbol ""|"" represents logical operator ""OR"".",52,53
16974,11523489,"In order to establish the natural language interactive engine which simulates the intelligence of human being's brain, HNC frames a space for language concepts which stresses the concept association vein, which constructs a digitized symbol system for natural language [2] .",37,38
16975,14478108,"For example, the mapped conceptual symbol from the word 'think' is 'v80', and the mapped conceptual symbol from the word 'idea' is 'r80'.",6,7
16976,14478108,"For example, the mapped conceptual symbol from the word 'think' is 'v80', and the mapped conceptual symbol from the word 'idea' is 'r80'.",22,23
16977,14478108,"The close relation between the two words is uncovered by the common symbol '80', as that 'have an idea' is one meaning of the word 'think'.",12,13
16978,14478108,"ConceptCategory((0)).INCLUDE.""vv"" , _ObjType((1),""CC"").IS.""v"" => _SETOBJTYPE( (1), ""CHUNK"",""Ek"" ) In formal rules, a function begins with the symbol '_'.",28,29
16979,14478108,"Therefore, the above formal rule means that if the symbols of the conceptual category of the current node (such as a word) include the symbol 'vv', and the next node is a 'v' concept word, then the next node would be assumed as the Ek.",27,28
16980,12056330,The first one is how to represent the sentence with simple symbol with enough semantic information.,11,12
16981,12056330,"T3=TA+T+TB+ [#T3C#] (1) In this example, T3 is the name and the symbol of the sentence category.",21,22
16982,203697858,"The final symbol, '<,>', represents a pause.",2,3
16983,35652008,"The first set contains sentences translated by Humans, which is assigned the symbol H. The second set contains Indirectly translated sentences or Ideal MT system translated sentences, which is assigned the symbol I. The third set is the set produced by the MT system, that is, the set of sentences translated by the target MT system.",13,14
16984,35652008,"The first set contains sentences translated by Humans, which is assigned the symbol H. The second set contains Indirectly translated sentences or Ideal MT system translated sentences, which is assigned the symbol I. The third set is the set produced by the MT system, that is, the set of sentences translated by the target MT system.",33,34
16985,35652008,"This is the set of correct sentences chosen in the analysis phase, and it is assigned the symbol S. The Source-Text Analysis Ratio In this evaluation step, the evaluator analyzes sentences translated by the MT system, by comparing them with the gold standard (the sentences previously translated by humans).",18,19
16986,209516390,It is an encoder-decoder architecture where the encoder takes a sequence of symbol representations x (i) . . .,14,15
16987,209516390,"y (n) , one symbol at a time.",6,7
16988,219310249,"DiaSim can capture any and all regular relations between strings in the specified symbol alphabet, whether that alphabet is the provided IPA default, or another supplied by the user.",13,14
16989,44237430,"A one-hot vector comprises either hiragana, katakana, kanji, alphabet, number, symbol, start symbol, or terminal symbol.",17,18
16990,44237430,"A one-hot vector comprises either hiragana, katakana, kanji, alphabet, number, symbol, start symbol, or terminal symbol.",20,21
16991,44237430,"A one-hot vector comprises either hiragana, katakana, kanji, alphabet, number, symbol, start symbol, or terminal symbol.",24,25
16992,7365891,"Japanese sheds light on this issue of derivation from a different angle: the language is very well known for having rich derivational and in-Unless powerful morphophonological rules manipulating the underlying phonological strings are assumed to apply, 4 morphological considera-  3 In the following, the symbol Ø indicates either the presence of a phonologically null morpheme or the absence of a morpheme, depending on the approach one endorses.",50,51
16993,7365891,"This conclusion, then, further justifies another conclusion: the symbol Ø in (2)k-(2)p must represent a phonologically null morpheme, not the absence of a causative or inchoative morpheme.",11,12
16994,8248149,"FSTs read their input symbol by symbol; each time a symbol is read, they move to a new state, and write one or more output symbols.",4,5
16995,8248149,"FSTs read their input symbol by symbol; each time a symbol is read, they move to a new state, and write one or more output symbols.",6,7
16996,8248149,"FSTs read their input symbol by symbol; each time a symbol is read, they move to a new state, and write one or more output symbols.",11,12
16997,9236719,This continues until the end-of-sequence symbol is obtained.,9,10
16998,235788521,"Since parts of a text according to SGML philosophy can again represent identifiable texts, such a text can, in the extreme, consist of -one symbol only -non-linguistic representations only.",27,28
16999,9572751,So it is with the symbol emission function.,5,6
17000,9572751,"A finite set of (observed) symbols Σ. In the above definition, t is the time variable indicating when state transition or symbol emission occurs.",24,25
17001,9572751,"Representation of Time Information Since time information is to describe when the events of Hmm (e.g. state transition or symbol emission) occur, a natural way is to use the event index in Markov chain to represent the time information.",20,21
17002,9572751,"For simplicity and convenience, we define some notations below: − The hidden state number n − The observed symbol number m − The bin number for NSHmm k In Hmm and NSHmm, all system parameters are devoted to simulate the three functions of ρ, α and θ.",20,21
17003,220446145,"HamNoSys uses Signing Gesture Markup Language (SiGML), which gives a special XML tag to each Ham-NoSys symbol.",21,22
17004,52142442,"Bracketing Transduction Grammar (BTG) (Wu, 1997) is a binary and simplified synchronous context-free grammar with only one non-terminal symbol.",27,28
17005,52142442,"It has three types for the right hand side of the rules γ: S-straight keeps the order of child nodes, I-inverted reverses the order, and Tterminal generates a terminal symbol.",36,37
17006,218974252,"For example, in written money tokens, the currency symbol can only precede or follow the numbers, while in decimal numbers like '1.23', the fractional part can typically only be read as a sequence of digits, as in 'one point two three', or a cardinal number, as in 'one point twenty three', or both.",10,11
17007,218974252,"Rather than supporting nearidentical hand-written verbalizers for each language, we have developed verbalization templates which offer options to set parameters like 'currency symbol precedes numbers' and 'decimal fractional part is read as a digit sequence'.",26,27
17008,218974252,"Written Domain Data Written domain data are details about writing conventions in the target language, such as: • whether the full stop or the comma is used to separate decimal numbers (e.g. '1.2' or '1,2'); • which symbol is used to separate numbers in dates (e.g. '1.2.2020' or '1/2/2020'); • the order of elements in written dates (e.g. DDMMYYYY or MMDDYYYY); • which currency symbols are used and whether they precede or follow the numbers (e.g. '$21' or '21C'); • common phone number formats, including the number of digits in a block, and the separator used (e.g. '1-800-234-5678' or '07123 456 789').",46,47
17009,218974252,"The first is 'free' variation, where a symbol can be verbalized in more than one way in the same context.",10,11
17010,218974252,"In US English for example, the hyphen-minus symbol in negative numbers like '-1' can be read as either 'minus' or 'negative'.",10,11
17011,218974252,The more complex kind of variation is the case in which the same written symbol is verbalized differently depending on the context or type of numeric token in which it is used.,14,15
17012,218974252,"For example, if a respondent fills out the currency symbol field with the dollar sign '$', and selects the parameter 'currency symbol precedes number', the follow up questions will then be automatically updated to reflect these facts, for example 'How do you say '$1'?'.",10,11
17013,218974252,"For example, if a respondent fills out the currency symbol field with the dollar sign '$', and selects the parameter 'currency symbol precedes number', the follow up questions will then be automatically updated to reflect these facts, for example 'How do you say '$1'?'.",26,27
17014,218974252,"Then in follow up questions, we ask them to identify which parts of the verbalization refer to each symbol, for example ""Which part of this is '='?"".",19,20
17015,218974252,This exposes the overlaps and splits in the use of different words for the same symbol in different contexts.,15,16
17016,218973760,"In this discussion, the outer product will be denoted by symbol.",11,12
17017,10783269,"In fact, each symbol is treated as a ground type name that represents itself in the type lattice.",4,5
17018,10783269,"Since we assume that we have an imaginary type for each ground type name in the type lattice such that it is a parent of that ground type name and the empty string, the score of the generalization of a symbol with the empty string is assumed to be 2.",41,42
17019,10783269,The generalization of a symbol a and the empty string is represented by nullor(a).,4,5
17020,235729184,"Among the new technologies for very large-scale processing now attracting attention are memory base inference, which is a parallel knowledge processing method[8], and technology for the application of neurocomputing to massive symbol manipulation.",36,37
17021,6630937,The correspondence specifies a lexical symbol that corresponds to a surface symbol.,5,6
17022,6630937,The correspondence specifies a lexical symbol that corresponds to a surface symbol.,11,12
17023,75817,Each state of machine represents a phonetic event and a symbol is generated each time a transition is made from one state to the next.,10,11
17024,919225,"If a student is not familiar with a specific Yami sound, a similar phonetic symbol is provided to him/her.",15,16
17025,32935727,"A text sentence is simply a written form of speech, and a character is nothing but a symbol to express the sound of a spoken word.",18,19
17026,32935727,"They may read the symbol in different ways, according to their familiar spoken languages.",4,5
17027,32935727,"Secondly, define common way of reading for each visual symbol.",10,11
17028,32935727,"Surely it will make the communication much more efficient, because now one need not write or show the symbol any more, and one need just say its common way of reading.",19,20
17029,32935727,An important point is that a Han character is a kind of icon -a highly developed visual symbol.,17,18
17030,32935727,"As it is a highly developed visual symbol system, each Han character can be used as an icon, and works as a keyword when inserted into a text of phonograms.",7,8
17031,17247338,"6 ) or the exit symbol &NUL, and the edges bear tree conditions.",5,6
17032,17247338,"The lower left part of the window shows the structure an input subtree should have to be processed by the rule ; the lower right part shows the structure of the subtree after application of the rule ; the upper left part describes the detailed rule as written in the ROBRA specialized language for linguistic processing (clicking on a symbol like $CP in the rule opens another window explaining the symbol); the upper right part may contain a comment to the rule, a comment which is local to the user's CASH interface.",60,61
17033,17247338,"The lower left part of the window shows the structure an input subtree should have to be processed by the rule ; the lower right part shows the structure of the subtree after application of the rule ; the upper left part describes the detailed rule as written in the ROBRA specialized language for linguistic processing (clicking on a symbol like $CP in the rule opens another window explaining the symbol); the upper right part may contain a comment to the rule, a comment which is local to the user's CASH interface.",72,73
17034,38354366,The symbol ⌅ indicates no space between two neighboring word pieces.,1,2
17035,52140009,"Moreover, a single symbol is a basic unit in the monolingual setting.",4,5
17036,52140009,"For the bilingual setting, we could extend to choose single symbol pairs as basic units.",11,12
17037,219303000,"The optional and non-optional nodes are necessary for the generation algorithm, which will be described in Subsection 3.2.. The lower part of Figure 2 shows the tree leaves which bear empty, representing an empty string or symbol in the grammar.",41,42
17038,218974399,"For example, the character '屌' has many meanings (where the original meaning is ""pxnis"") as follows: So we put context into the input sequences in the format of PREC SEP PRFN SEP FOLW, where PREC is the preceding context, PRFN is the target profane expression, FOLW is the following context, and SEP is a separating symbol.",67,68
17039,218974424,"We removed @ symbol (it means that we also removed all mentions), we also removed hashtags (#), URLs, and emojis.",3,4
17040,218973958,"To get the output of the respective tasks is trivial, we just have to separate the labels by the '-' symbol, where the first word corresponds to sub-task A and second word corresponds to sub-task B. The models using this technique are labeled with (C) in the results table below. •",23,24
17041,2363925,"Currently four different formats are recognized by the system: raw text without annotation, POS tagged format where each sentence is simply a sequence of word and POS tag pairs separated by some symbol like underscore, 'bracket form' which allows POS tagged and chunked data to be represented (including recursion), and Shakti Standard Format (SSF) 2 .",34,35
17042,12103545,"--Spelled 'b'; usually (not always) after a vowel stem --> </Mo:Allomorph> <Mo:inflectionFeatures> <Fs:f name=""Tense""><Fs:symbol value=""Future""/></Fs:f> <Fs:f name=""Mood""><Fs:symbol value=""Indicative""/></Fs:f> <Fs:f name=""Person""><Fs:symbol value=""1""/></Fs:f> </Mo:inflectionFeatures> /Mo:InflectionalAffix> <!",32,33
17043,12103545,"--Spelled 'b'; usually (not always) after a vowel stem --> </Mo:Allomorph> <Mo:inflectionFeatures> <Fs:f name=""Tense""><Fs:symbol value=""Future""/></Fs:f> <Fs:f name=""Mood""><Fs:symbol value=""Indicative""/></Fs:f> <Fs:f name=""Person""><Fs:symbol value=""1""/></Fs:f> </Mo:inflectionFeatures> /Mo:InflectionalAffix> <!",43,44
17044,12103545,"--Spelled 'b'; usually (not always) after a vowel stem --> </Mo:Allomorph> <Mo:inflectionFeatures> <Fs:f name=""Tense""><Fs:symbol value=""Future""/></Fs:f> <Fs:f name=""Mood""><Fs:symbol value=""Indicative""/></Fs:f> <Fs:f name=""Person""><Fs:symbol value=""1""/></Fs:f> </Mo:inflectionFeatures> /Mo:InflectionalAffix> <!",54,55
17045,218974094,"In fact, this symbol was available on the Keyman layout, but required a long press to be accessed and so went unnoticed by this participant.",4,5
17046,9863151,"In an attribute-value pair, the attribute corresponds to the name of a grammatical symbol (e.g. NUM, TENSE) or a syntactic function (e.g. SUBJ, OBJ) and the value is the corresponding feature possessed by the concerning constituent.",16,17
17047,234487220,"As Vietnamese words are morphologically invariable, the morphological information in VCL is only related to the word formation: a word can be either single, or compound, or redoubled, otherwise it can be a loan word, or an abbreviation, or a symbol.",47,48
17048,221097958,"We use the symbol S to denote the set of all S-formulas, and similarly for R and RE.",3,4
17049,221097936,"symbol is omitted for perspicuity, and parentheses in types are of a semantic paradigm, is the FraCaS test suite (Cooper et al.",0,1
17050,55488801,"An additional label, kind, was introduced for usages of the concept as a kind, where quantification does not apply (e.g. beaver symbol-of-Canada).",25,26
17051,6984666,Straight keeps the same order in the source and the target languages; Inverted exchanges the order; Terminal just stands for the production of a nonterminal symbol both in the source and target languages.,27,28
17052,17236603,"The tape is marked off a series of squares, each of which can hold a single symbol.",17,18
17053,17236603,"The tape head, or readwrite head, can read a symbol from the tape, write a symbol to the tape, and move one square in either direction at a time.",11,12
17054,17236603,"The tape head, or readwrite head, can read a symbol from the tape, write a symbol to the tape, and move one square in either direction at a time.",18,19
17055,236447816,"What we call corpus-based features contains the frequency of a current word in the whole corpus, position label for first (begin), last (end) and remaining (middle) words of the headline, if the current word is capitalized, or entirely in upper or lowercase, if the token is a number, a punctuation symbol, or in the list of 50 most frequent words in our corpus.",64,65
17056,13967959,"The problem of translation can be defined as: P r(f J 1 |e I 1 ) = a J 1 P r(f J 1 , a J 1 |e I 1 ) (1) Here we use the symbol P r(•) to denote general probability distributions.",40,41
17057,11881791,The caret symbol (ˆ) is a path operator and points to the newly built FS.,2,3
17058,41464328,"The special character # denotes a word boundary, and the symbol C denotes the set of all consonants.",11,12
17059,32515458,"In this paper, the symbol L 1000 indicates the lowest-level semantic classes, L 1OO the next higher level, and so on.",5,6
17060,55447902,"32  The chain of the verb ending morphs can be modeled as a Markov chain of the first order, i.e., as a sequence of symbols where the probability of occurrence of a symbol depends only on the immediately preceding element or, perhaps more precisely, cannot depend on preceding elements lying further than the immediately preceding one.",35,36
17061,55447902,"33  As for the initial distribution (i.e., the distribution at t = 1, if we label the first item as x 1 ), we can posit that it is identical to the distribution of an element preceded by an A or C symbol (i.e., we rule out C morphs as possible initial elements since these constitute the second part of an ending).",47,48
17062,55447902,"Indeed, for a Markov process it is possible to calculate the entropy rate H(X ), a quantity that represents the average information content per symbol (see Cover and Thomas 2006: p. 77 ).",26,27
17063,6283393,"'s method which splits a sentence into pieces (and supplies a subject for each piece if necessary), our rewriting actions consist of addition, deletion and substitution of English expressions as well as insertion of preediting symbols, such as a symbol for disambiguation of the word classes and one for disambiguation of the scope of a phrase and a clause.",44,45
17064,14608464,"However, due to the flexibility of Thai language, the order of a keyword, a polarity word and a stock symbol may not be the same in the news.",22,23
17065,14608464,"First, the selected stock news must have a stock symbol.",10,11
17066,14608464,"Hence, the stock news contains only a stock symbol and recommended price will not be selected.",9,10
17067,14608464,ขึ ้ น khun 'up' + ลง lng 'down' - กำรลงทุ น karn-lng-thun 'investment' ฟื ้ นตั ว fun-taw 'recover' + ซบเซำ sob-sea 'stagnant' - คงที ่ khong-thi 'stable' 0 แนะน ำ nae-num 'recommend' ขำย khai 'sell' - ซื ้ อ sux 'buy' + ถื อ thux 'hold' 0 Experimental Design and Result We hypothesized that wordpairs affecting the stock news sentiment can be located near the stock symbol.,100,101
17068,14608464,"Hence, we design experiments, using the stock symbol as a center, with different window sizes varying from 20, 40, 60 and 80 letters.",9,10
17069,14608464,"เรำยั งคงแนะน ำซื ้ อ MINT CENTEL และ ERW ←←10→MINT←10→ ←←←←←←20→CENTEL←20→→→ ←←←←←←←←←←←←←30→ERW←30→→→ range 20 extracts: [แนะน ำ][ซื ้ อ][MINT] → '[recommend][buy][MINT]' range 40 extracts: [แนะน ำ][ซื ้ อ][MINT] → '[recommend][buy][MINT]' [แนะน ำ][ซื ้ อ][CENTEL] → '[recommend][buy][CENTEL]' range 60 extracts: [แนะน ำ][ซื ้ อ][MINT] → '[recommend][buy][MINT]' [แนะน ำ][ซื ้ อ][CENTEL] → '[recommend][buy][CENTEL]' [แนะน ำ][ซื ้ อ][ERW] → '[recommend][buy][ERW]' Figure 1 : Effects of varying window sizes for extracting wordpairs features We found that the optimal window size for extracting wordpairs features is 60, with average 1-3 wordpairs for each stock symbol, as shown in Figure 2 .",131,132
17070,14608464,"We also found that 80-letter window size sometimes extracts irrelevant wordpairs, such as wordpairs of the next symbol.",20,21
17071,14608464,"For example, the stock news on Figure 2 has a stock symbol (1) and three wordpairs.",12,13
17072,14608464,"1) MAKRO ก ำไร (2) ดี (2) กว่ ำคำด (3) คงก ำไร (3) ปี นี ้ / คงค ำแนะน ำ (4) ถื อ (4) In short, there are 6 combinations of {symbol (S), keyword (K), polarity (P)} as shown in Table 7 .",46,47
17073,14608464,"Moreover, we found six combination patterns of a stock symbol, a keyword and a polarity (S-K-P) in stock news.",10,11
17074,11549832,"Further tag sets are Anaphora Classes (personal pronoun, demonstrative pronoun, abbreviation, special symbol) and Numbers.",16,17
17075,11549832,"Special symbols like $ have been considered as a separate anaphora class as in many target language headlines the transliteration of the the full form of the symbol, i.e., dollar, is used.",27,28
17076,11549832,"Acronyms in short news headlines can be identified by looking for words which are all capital or may include a vowel in small case (e.g., HoD) or a special symbol (e.g., J & K).",32,33
17077,9344827,"Deterministic FSA (DFA) is efficient because a unique ""next state"" is determined when given an input symbol and the current state.",20,21
17078,29938854,Method We apply part-speech tagging to a symbol tokenized and word segmented Arabic and symbol-tokenized English parallel corpus.,9,10
17079,29938854,Method We apply part-speech tagging to a symbol tokenized and word segmented Arabic and symbol-tokenized English parallel corpus.,16,17
17080,29938854,"We then viterbi-align the part-of-speech tagged parallel corpus, using translation parameters obtained via Model 1 training of word segmented Arabic and symbol-tokenized English, to derive the conditional probability of an English part-of-speech tag given the combination of an Arabic prefix and its part-of-speech or an Arabic suffix and its part-of-speech.",28,29
17081,15000287,"observation log-pdfs identified as the B = bj,k matrix, where j identifies the arc (transition between states) and k identifies the observation symbol. (",29,30
17082,202776713,"To train our system, any arbitrary symbol would work.",7,8
17083,16131800,"An example of a simple pattern rule for Bamabara is a rule for analyzing privative participle forms ending in -bali: ( This rule states that it is applicable for forms having the part-of-speech tag ""verb"" or ""participle"" or an unspecified tag as well, ending in -bali (left part of the rule before the | symbol).",65,66
17084,37374645,"It includes the element, basic concept element, related function and additional symbol.",13,14
17085,6395143,The symbol ˆDB represents a derivational boundary and splits the parse into chunks called inflectional groups (IGs).,1,2
17086,38866455,"Therefore, there is a necessity to designate all verbs with some common symbol.",13,14
17087,38866455,"For example, the symbol ""s"" in the 3 rd column means that for the noun code-word (code-word which is beginning 002) the stem of the English translation of the word-form in Azerbaijani must be taken in plural, but for the verb code-word (code-word which is beginning 001) the third person in singular.",4,5
17088,221097938,The @@ symbol is placed at the end of each BPE sub-word when it is not the last sub-word of a token.,2,3
17089,221097970,"Furthermore, the engine might be attempting to translate the bullet symbol (e.g. ""State of the Union Address 2016 …"" translated as ""barbara, govor o stanju v Uniji 2016 …"" -translation of ""-"" (""bar""?)",11,12
17090,41845669,"Although that an illegal-character error can happen where writing by hand, i.e. the written symbol is not a legal Chinese character and thus not collected in a dictionary, such an error cannot happen in a digital document because only legal Chinese characters can be typed or shown in computer.",17,18
17091,221097285,"4 .2) and a deep output that combines the context vector, the recurrent hidden state and the embedding of the previous symbol.",23,24
17092,6030778,symbol).,0,1
17093,6030778,"Argument names preceded by the @ symbol, e.g., @theme, require the specification of an event type (denoted by :type).",6,7
17094,16730107,"In case a target word has multiple aligned source words (such as ""again""), we separate these partners by symbol ""|"" after putting the prefix ""B-"" at the beginning.",23,24
17095,16730107,"The lack of a symbol indicates an exact match (we replace it with ""E"" thereafter).",4,5
17096,1722348,"Definitions A SCFG is a quadruple Go = (N, E, P, S), where N is a finite set of no,terrainalsymbols, ~ is a finite set of terminal symbols disjoint from N, P is a finite set of productions of the form H ~ a, H 6 N, c~ 6 (SUN)*, and S 6 Nis a special symbol called Jtart symbol.",72,73
17097,1722348,"Definitions A SCFG is a quadruple Go = (N, E, P, S), where N is a finite set of no,terrainalsymbols, ~ is a finite set of terminal symbols disjoint from N, P is a finite set of productions of the form H ~ a, H 6 N, c~ 6 (SUN)*, and S 6 Nis a special symbol called Jtart symbol.",75,76
17098,1722348,"It is also possible to assodate with each derivation tree the probability that it was generated from a nonterminal symbol H by the grammar G,.",19,20
17099,1722348,"Given a string z 6 E*, the notation H < z >, H 6 N, indicates the set of all trees with root H generated by Go and spanning z. Therefore Pr(H < z >) is the sum of the probabilities of these subtrees, i.e. the probability that the string z ha, been generated by G, starting from symbol H. We assume that the grammar G, is consistent.",66,67
17100,13430235,"Roughly speaking, as each new symbol is accepted (or shifted-in) the parser decides how to incorporate it into a subtree and perhaps how to link several existing subtrees together (i.e. reduce).",6,7
17101,13430235,"The three possible ""moves"" for reacting to the next input symbol 'z' are listed below.",12,13
17102,13430235,"The ""input cycle"" of a shift-reduce parser is typically to shift in a new symbol via move 1, use move 2 to give that symbol a nonterminal root, and then to perform some number of moves of type three.",18,19
17103,13430235,"The ""input cycle"" of a shift-reduce parser is typically to shift in a new symbol via move 1, use move 2 to give that symbol a nonterminal root, and then to perform some number of moves of type three.",29,30
17104,13430235,"Happily, this observation leads to a closed form solution to the problem of calculating all the necessary probabilities for the To illustrate the use of Markov chain theory for the left-edges of trees, we compute the probability of the event that the left edge of a randomly generated subtree terminates in a specified terminal symbol a, given that the root is a specified nonterminal symbol A. This event is the disjoint union of the events that a is the n th symbol in the left-edge sequence, for all n > 1.",58,59
17105,13430235,"Happily, this observation leads to a closed form solution to the problem of calculating all the necessary probabilities for the To illustrate the use of Markov chain theory for the left-edges of trees, we compute the probability of the event that the left edge of a randomly generated subtree terminates in a specified terminal symbol a, given that the root is a specified nonterminal symbol A. This event is the disjoint union of the events that a is the n th symbol in the left-edge sequence, for all n > 1.",69,70
17106,13430235,"Happily, this observation leads to a closed form solution to the problem of calculating all the necessary probabilities for the To illustrate the use of Markov chain theory for the left-edges of trees, we compute the probability of the event that the left edge of a randomly generated subtree terminates in a specified terminal symbol a, given that the root is a specified nonterminal symbol A. This event is the disjoint union of the events that a is the n th symbol in the left-edge sequence, for all n > 1.",86,87
17107,13430235,"Correspondingly, we want the sum P(the n th left-edge symbol is a I the root is A) n>l which is the sum of the (A, a) th entries in the sequence M, M 2, M 3, etc.,",12,13
17108,13430235,Note our convention that the root is the 0 th symbol along the left edge of the tree.,10,11
17109,13430235,"As another illustration we compute the probability that the left edge of a subtree T terminates in some specific subtree ~', again given that the root of T is A. More precisely, we compute the conditional probability that the subtree ~-appears as a subtree of T, with its root B somewhere in the left-edge of T, given that the root of T is A. This is the disjoint union of the events, as n varies, that B is the n th symbol in the left edge of T and that T then appears rooted at this B. If (just for a moment) we exclude the possibility that v is identical to T, then n must be at least 1.",90,91
17110,13430235,"For each n > 1, the conditionM probability that ~-appears rooted at the n m symbol B, is P(r]B) multiplied by the (A, B) th entry of the n th power of M. In this case we can find, much as in the preceding illustration, that the sum from 1 to infinity of these l~obabilities is P(rlB ) x the (A, B) th e~try of M(I-M) -1.",16,17
17111,13430235,"When in the Midst of the Input When there may be more input to be processed, the calculation of the probability of a parser hypothesis with only one subtree is exactly the equation (15 in which the start symbol of the grammar, S, takes the place of the symbol A in the formula.",40,41
17112,13430235,"When in the Midst of the Input When there may be more input to be processed, the calculation of the probability of a parser hypothesis with only one subtree is exactly the equation (15 in which the start symbol of the grammar, S, takes the place of the symbol A in the formula.",52,53
17113,13430235,"The desired probability is the sum of the entries in Vn and Vii n multiplied by the conditional probability of the subtrees already constructed: [I e(~ln~) i=1 When at the End of the Input If there is no more input and the hypothesis has only one subtree, then either the root of the subtree is the start symbol of the grammar, and hence the hypothesis has yielded a wellformed sentence with probability P(rlSS ) or the hypothesis must be abandoned since it has not yielded a sentence and no further changes to it are possible.",61,62
17114,13430235,"For our score, a ""cheap"" grammar is one in which each symbol is the left child in relatively few rules.",14,15
17115,7493352,"For example, in the word speech (/s p iy ch/), both left and right contexts for /p/ and /iy/ are known, while the left context for /s/and the right context for /ch/ are a special symbol for ""word boundary"".",42,43
17116,219302482,"In addition to the description of using the symbol system to convert language expertise and heuristic knowledge into a knowledge base to cope with a frame-based corpus and a tone sandhi processor, the procedure of connecting the 1 本論文承蒙中正大學語言學研究所麥傑教授提供諸多建議，謹此致謝。",8,9
17117,220837154,"We use symbol normalization and tokenization schemes for French in Moses (Koehn et al.,",2,3
17118,15230543,The target sequence y is tagged with the none symbol (i.e. O) or the name of the relationship (e.g. Interaction.,9,10
17119,215800433,"The symbol ""np"" stands for ""noun phrase"", and ""det"" for determiner (an article, such as ""a"" or ""the"", or a quantifier, such as ""some"" or ""every"").",1,2
17120,215800433,"The two-character set has stuck, and is becoming the phonetic symbol set of the computer age.",13,14
17121,215800433,The Identity of the vowel is shown by the symbol plotted.,9,10
17122,215800433,"The symbols cluster, and the regions where they fall are indicated by a surrounding balloon and a phonetic symbol to show what vowel (mainly) 4oils in that region.",19,20
17123,53444920,"Therefore, we also use an unknown character symbol and train it analogously to the unknown word symbol, that is, we randomly replace character with [UNK] during training with probability p CHARUNK .",8,9
17124,53444920,"Therefore, we also use an unknown character symbol and train it analogously to the unknown word symbol, that is, we randomly replace character with [UNK] during training with probability p CHARUNK .",17,18
17125,53444920,The process is initialized by setting c 0 to an end-of-sequence symbol [EOS].,15,16
17126,53444920,"Additionally, we replace characters with an [UNK] symbol with probability 0.1 during training.",10,11
17127,1170238,"Furthermore, multi-character word i8 still subject to homophone, i.e. phonetic symbol ""xingshi"" represents both the Chinese words ""situation"" and ""form"", among others, though the number of homophones is much reduced.",14,15
17128,1170238,"The key to the question ks the software of this Chinese Input System which must be developed as to identify different Chinese words properly on the basis of the same phonetic s~mbol, i.e., different string of Chinese oharacters must be generated from the same phonetic symbol occuring in different contexts.",49,50
17129,1170238,"When considered as without any connection with other words a Chinese word with one or more homophones sharing the same phonetic symbol is really a trouble, but when we try to grasp the proper word not merely by itself but in connection with its context with the background knowledgy and/or with the very topics of the whole text or corresponding paragraph, we find, as a rule, the phonetic ambiguity (i.e. the different homophones which cause language ambiguities) would dissolve.",21,22
17130,1539972,"If there is no path can be searched in the current branch point or all routes are not passed through, the parsing fails and output the final error symbol.",29,30
17131,6558616,"Similar to a context free grammar, an RTG uses production rules with terminal symbols and nonterminal symbols (NTs), whereby the lefthand side (LHS) is always a nonterminal and the right-hand side (RHS) contains at least one terminal symbol.",47,48
17132,6558616,One NT is the start symbol.,5,6
17133,6558616,The start symbol is {1-7}.,2,3
17134,6558616,"For an arbitrary pure chain with n fragments, the filter grammar generating the LCC is constructed as follows: S is the start symbol.",24,25
17135,12900947,"then the replacement of these two rules by the three rules (where d_tc is a new nonterminal symbol, which represents a kind of ""transitive c(osure"" of d): g(X) -, t(Y), d_tc(r, X). ,",18,19
17136,13146776,"The simple phrasestructure grammars that suffice to describe the syntax of formalized languages simply do not work for natural languages; to dascribe the surface syntactic structure of a natural language requires a system, such as a relational phrase-structure grammar (Bellert, [4] ) or a complex-feature-symbol grammar, with the power of expressing the various relations of grammatical agreement among constituents.",56,57
17137,6339908,"J. Pohl, 23, [453] [454] [455] [456] [457] [458] [459] [460] [461] [462] [463] [464] [465] [466] [467] [468] ° A new definition of vleme is given on this occasion: pleme absolute, relative (semi-symbol),.",72,73
17138,6339908,"in the bicipher (twofigure) numeral notation, syntactical synergy and line~ arrangement of plemes are encountered in a very profuse measure when compared to, for example, the decimal notation where the symbol O when placed to the right of a figure serves to multiply it by 10.",35,36
17139,227231132,"The preprocessing stage includes the following steps: separating hyperlinks from the adjacent text, normalize twitter-specific tokens, extracting text from '*' (e.g. *good* > good), replacing & symbol, lower-casing the text, normalize multiple occurrences of vowels and consonants, normalize emojis and numbers, spiting 'number' and 'emoji' when adjacent to text, removing non-alphanumeric characters, removing very long words >= 15 and short words < 2 to reduce sparsity, removing multiple sequential occurrences of the same token.",38,39
17140,9402109,"alone, or transliterate them, or insert a dummy symbol, such as ""X"".",10,11
17141,227230490,"We pre-process the data as follows: 1) the HTML symbol &amp; are removed as per ( Úbeda et al.,",13,14
17142,11054519,"Four distinctly original transducer models are introduced: symbol networks, interlingua transducers, the joint distribution/conditional operations tranducer, and acquired alphabetic identity transducers.",8,9
17143,11054519,"Their core model, which we reproduce here, was a singlestate transducer having a self-loop transition: an emission function on the transition encodes a joint distribution over individual symbol substitutions, deletions and insertions.",32,33
17144,11054519,"Their second variant, also a memoryless transducer, had a radically reduced tied parameter set, consisting of 4 parameters: one each for any insertion, any deletion, any substition of and identical symbol, and any non-identity substition.",36,37
17145,11054519,"A convenient way to work with these models is to factor P (α, β, γ) as P (α|β, γ)P (β|γ)P (γ) and then assume α to be conditionally independent of β given γ, simplifying the model structure to: P (α|γ)P (β|γ)P (γ) For language 1 and language 2 alphabets Σ 1 and Σ 2 respectively, and for interlingua alphabets Σ 0 , for alphabet symbols a ∈ { Σ 1 } , b ∈ { Σ 2 } , c ∈ Σ 0 : we can model symbol generation for languages 1 and 2 as P (a|c)P (b|c)P (c) leaving the ""interlingua language model"" P (γ) to be specified as desired.",103,104
17146,11054519,"We investigated two variants of the unigram interlingua model: model UIT is exactly as described above, whereas UIT2 is a variant which allows atomic generation of two-character language 1 (a i a i+1 ) or language 2 (b j b j+1 ) sequences from a single interlingua symbol c k .",53,54
17147,11054519,"The number of parameters for the UIT model is (language 1 insertions + language 2 insertions + ""substitutions"" + probabilities of performing each operation) 1 + |Σ 1 | + 1 + |Σ 2 | + 25 + 25 * (|Σ 1 | + |Σ 2 |) = 27 + 26 * (|Σ 1 | + |Σ 2 |) Symbol Networks We define a Symbol Network (henceforth, SN) transducer as follows: Symbol network transducers have one state per alphabet symbol per language.",91,92
17148,11054519,These states emit the symbol by which they are denoted with probability 1.,4,5
17149,11054519,"An interesting property of this transducer structure is that some symbol states learn to allocate most of their outgoing transition probability to symbols internal to the language, whereas other symbols more strongly prefer transitions cross-language.",10,11
17150,11054519,"The allowed operations in that transducer are restricted to substitutions, where each English sound The transducer pictured has one state per alphabet symbol per language.",23,24
17151,11054519,These states emit the symbol by which they are denoted with probability 1.,4,5
17152,11054519,"Given a string α, we probabilistically choose from operations I (insert in β), D (delete from α), or S (substitute a symbol b given current symbol a).",29,30
17153,11054519,"Given a string α, we probabilistically choose from operations I (insert in β), D (delete from α), or S (substitute a symbol b given current symbol a).",33,34
17154,11054519,"We start by adding a special start symbol /s/ to each string in the pair: for example, considering the Spanish-Italian cognate pair delegación -delegazione, we get /s/delegación -/s/delegazione.",7,8
17155,11054519,All substitutions not in this set are treated as in R&Y: each individual symbol-to-symbol substitution has its own parameter.,14,15
17156,11054519,All substitutions not in this set are treated as in R&Y: each individual symbol-to-symbol substitution has its own parameter.,18,19
17157,11054519,"We can draw the conclusion that, when there is a near one-to-one symbol correspondence across languages, this model is hard to beat, whereas the mapping cardinality of orthographic symbols and phonetic strings is not amenable to such representational simplification.",17,18
17158,1390700,"5 The symbol ""S"" donates sentences with a common end mark, while ""s"" denotes structures orthographically end with one of the PMs studied here. """,2,3
17159,1390700,"Finally, we decided to use Period, the End-of-line symbol, and these six marks (Question Mark, Exclamation Mark, Colon, Semicolon, Ellipsis and Dash) as delimiters of our EUDA.",14,15
17160,1925415,The output has been scored on words correct and also on symbol score (i.e. phonemes correct) using the Levenshtein (1966) string-edit distance as shown in Table 1 .,11,12
17161,1925415,"This problem is entirely avoided with the Sullivan and Damper style of lattice, because the shortest-length arc corresponds to a single-symbol mapping rather than to a bigram (which may be unique).",25,26
17162,1925415,"Thus, there will always be a 'default' single-symbol mapping corresponding to the commonest pronunciation of the letter.",12,13
17163,13372028,"We have to distinguish three cases: GT': If ~"" is tile next input symbol, we push ~a(q,~).",17,18
17164,13372028,presence now points on the first symbol in ft. (,6,7
17165,53579762,"As is typically currently done in computational modeling of language, data items with fewer than a preselected number of items are replaced with an unknown symbol label (<UNK>) to ensure that all items found in test and development sets are present in training.",26,27
17166,773710,"In peculiar, in many Asians writings systems (Japanese, Chinese or Thai, for example), there is not a special symbol to delimit words (such as the blank in most non-Asian writing systems).",24,25
17167,11200749,"A Mandarin syllable comprises 1 to 3 such constituents, the first symbol is usually a consonant.",12,13
17168,16761534,"A phonological word is one joined by a ""+"" symbol.",11,12
17169,8507267,We differentiate F category symbols from features by putting the latter in [ The symbol X denotes strings having no relevance to the rules.,14,15
17170,237366092,"The random ""words"" were then extracted based on the word boundary symbol.",13,14
17171,237366092,"First, almost all the studies generated random texts with the assumption that each symbol appears with equal probability and thus the frequency of a sequence should decrease monotonically with its length (Manin, 2008) .",14,15
17172,237366092,"Punctuation usually includes one symbol such as ""?""",4,5
17173,17725568,"To make the AUG notation compact, we introduce recursively defined adjoined symbols (Shaumyan 1987: 199) : A type symbol is called adjoined if it is introduced into tile type system by a definition of file form: Z = Oxy where z denotes ,'m adjoined type and Oxy denotes a type where x and y are either other ,adjoined type symbols, oft, ors. (",22,23
17174,18072599,"These represent the lower and upper bounds on the number of fillers for a given RoleSet, In Figure 1 , we have arbitrarily estimated that people's mouths have a minimum of 6 and a maximum of 5 functions, Notiee that every Concept has a diamond-shaped symbol associated with it.",50,51
17175,18072599,This symbol is not part of the KL-ONE language.,1,2
17176,34890335,"If the si~Lfiant is, semiottcally, a symbol, we have to do with a namin~ construction, if it is an icon, we have a picture construction!",10,11
17177,237365386,"Specifically, those topic-relevant messages is commonly clustered by the hashtag, which is marked with ""#"" symbol (e.g., #fantasticfour in in Table 1 ) and ubiquitous in social media domain.",21,22
17178,15416717,Subtrees which dominated only empty categories were collapsed into a single empty element symbol.,13,14
17179,10194386,"£2 is the class of context free languages generated by grammars whose productions are restricted such that the LHS of each is a single nonterminal symbol, and each RHS is a sequence of terminals and nonterminals.",26,27
17180,10194386,"Finally, the regular languages, £3 are those produced by regular grammars, characterized by rules that have a single nonterminal symbol on the LHS and on the RHS, either a terminal symbol or a terminal and a single nonterminal.",23,24
17181,10194386,"Finally, the regular languages, £3 are those produced by regular grammars, characterized by rules that have a single nonterminal symbol on the LHS and on the RHS, either a terminal symbol or a terminal and a single nonterminal.",35,36
17182,10194386,"The two languages induce different dependency relationships which is best described as nesting in the context free case and cross-serial in the indexed case: An important property of the each of the language classes is that it is closed under bottl intersection with regular languages (e.g., the intersection of a context free language and a regular language is no more expressive than a context free language) and homomorphism (e.g., an order preserving map of each symbol in a language to a single element (possibly a string) of a context free language implies that the first language is also context free).",83,84
17183,10194386,"Edges in the chart are marked with a category (some nonterminal or preterminal symbol from the grammar), constituents, subs|ring span and expectations (along with a unique identifier for each edge).",14,15
17184,10194386,"In the completer step, when active edges combine with adjacent inactive edges whose category satisfies the current expectation of the active, the usual process of creating a new edge with one less expectation is augmented with another: if the current expectation has an associated copy feature, then the new edge is marked with a constraint interpreted by the parser as indicated above --the nonterminal symbol and tile string spanned by the inactive edge are noted so 5We take a local domain, in tree terms, as a node and tile set of nodes that it immediately dominates.",68,69
17185,10194386,"An I~PDA is just a PDA which has a special type of symbol thai, can tie put onto the stack to nlake the machine treat the part of the stack above it ms if it were a queue.",14,15
17186,10194386,"The model of comput~ttion here is an RPDA in which only (me spe, cial symbol is allowed on the stack at any one, time.",18,19
17187,14712638,an index to the symbol in SYMBOL that is being processed.,4,5
17188,14712638,A convenient way to handle this problem would be to use a special symbol (pethaps ' !'),13,14
17189,16907615,"The null symbol ¢ was used to indicate, that a given dimension had no instance.",2,3
17190,16584852,"To illustrate: bolezEn decl_subst_f2 / bv=subst gen=fem; (1) (1) -the stem of the lexeme ""illness""; the lexical symbol ""E"" denotes an unstressed ""e"" (schwa sound), deleted in word forms with non-null endings (""bolezen"" -nom.",30,31
17191,44819581,"As Martin Kay once put it, until recently there existed only one symbol-manipulating system in the world; now there exist two.",13,14
17192,11992179,"symbol indicates that ""the word has some restriction, either a restriction to one meaning or a caution that the word is not at eight-grade level and should only be used with care.""",0,1
17193,227230577,"2019a) , for the combined multi-task learning setup the label will be treated as a single task where its components are separated by the symbol ""@"".",27,28
17194,9661304,The symbol -can be interpreted as a need and + as the corresponding resource. .,1,2
17195,9661304,"This is the simplest way to model linear order and precedence rules: X precedes Y iff the end of X is the beginning of Y. The initial category S of the grammar gives us the initial structure: A terminal symbol a corresponds to a positive edge: A rewriting rule ABC → DE gives us the elementary structure: This elementary structure is a ""cell"" whose upper frontier is a string of positive edges corresponding to the left part of the rule, while the lower frontier is a string of negative edges corresponding to the right part of the rule.",41,42
17196,8863035,Pred Predicate-symbol] [Case-labell Complement i] [Cas e-label2 Complement2] [Case-label3 Complement3] • . . ],3,4
17197,15334175,An atomic type consists of a type symbol and a complex type of a type symbol with a set of pairs of features and TFSes (values).,7,8
17198,15334175,An atomic type consists of a type symbol and a complex type of a type symbol with a set of pairs of features and TFSes (values).,15,16
17199,15334175,"Definitions have the form T = TFSt v ... v TFS m :-C, where T is a type symbol, the TFS~ are TVSes of type F i (T _> Fi) and C is a conditional constraint, which may be omitted and is expressed by a logical conjunction of TFSes.",20,21
17200,170187,"Suppose the string 'e d cm' is to be analysed, After the terminal symbol 'e' has been read, it .is reduced to (e,~,_).",16,17
17201,170187,"Now the telxninal symbol 'era' is read and reduced to (c,-,_).",3,4
17202,170187,"If instead the next input symbol after the string 'e d' is 'cp', no problems with the LP-consistency arise.",5,6
17203,170187,Suppose that the next input symbol after the string 'e d' is 'cp'.,5,6
17204,170187,"This terminal symbol is reduced to (c,+,_) which can be unified with (c,X,2) in t. 2 and the variable X is instantiated with '+'.",2,3
17205,11813565,"However, the previous intuition seems to hold anyway when the two analogies to be concatenated do not have any symbol in common.",20,21
17206,11813565,"Languages of analogical strings 4.1 Analogical Derivation In order to show how some languages, i.e., some sets of symbol strings, can be characterised by a device based on analogy, w e rst introduce analogical derivations.",20,21
17207,227230338,"To use this modelling feature a special pipe symbol, |, is required between each of the token's features 5 .",8,9
17208,219307482,"Explanation of the diagram Each connection line is marked by an arrow symbol, meaning regular bus or train line taxi C oling charter The mode of communication is indicated by T for (underground) train, Ship for ship, Plane for plane; unmarked represents bus or car.",12,13
17209,219307482,"Below each line is given the time(s) of departure; the symbol & means 'continuous' flow, i.e. less than |5 max.",12,13
17210,18106511,"Thie qenerator (+ lJU+ildo+n StOP'e gener/Itiv@ +P'aP, I+"" l,:ar} d~veloped irl~id~2 the functional geoerattve dest:vip +lion ([.'I.'.~)) rJf language irl Prague it+ tb~÷ f+rsi: part 6~rnm the vieeiiuiut o.~ tJte geoeration efa sentet'+ce) o~; the +ho~e stratificatinoal FGIJ,~ the output uf tile generator (a generatL~d U,+J) iS ~[E~n,Jduced to tilL~ Iu~+er levels t'+¢ F(~t) ill the directiell f roli~ functiun to }or,l ( +~ ro~ t:l[q:~tlJli~j v generally~ to its rep~e~ontation) in order to+ +t:hieve tlie final iJhouetic (.qrapbeeit:) stands for a symbol of the shape ~ Jff on the left= haud side of the same rule the variable k h~s the value 8{ etherwlse m stands for an empty sequence~ stands ~or 0 iff h~ on the loft-hand side o¢ the ~a~e rule has the value to otherwise ~ etands for i~ the priam and bar sy~ibols (e.g. ~', ~) have a similar meaning mS their simpla counterparts (ioe~ g) (here it ~eaes that al~o ~ ~e {0~1} )o If a superscript or a subscript uf a wriabln lias the va~ue 0, it may be absent in the notation used.",135,136
17211,18106511,"F is the defining function of 6, It has two parts= (able II and Limiting Cundltions~ i.eo conditions limitleg tbe possibility of using individual rsles as given in  G starts in Ko~ the rules are applied in an arbitrary order, the only condition being that the current state and the PB's top symbol agree with the left-hand side of the applied ru~e. In some cases a choice between the rules is possible (i.e. B is nondeterministic).",57,58
17212,30594460,"sink(at) where arakis a property symbol of I, and a~ is the statement on(b~,b~.), amk(on(b~,b2)) means that it is assumed to be mutuM lmowledge that the block b t is on the block b> Other examples of statements of L~ are id(b~) meaning that the block b I has been identified in a TAKE action, neq(TAKE) meaning that TAKE can be inte~preteal unequivocally arid apl(li) meaning that li is assumed to be a potential position tbr the moved blocks.",6,7
17213,8497667,"M""is the initial symbol, ""#"" is the boundary marker, and the subscript will be explained later.",3,4
17214,8497667,"and that the form of the function is generated in the definition --as the expansion of the symbol -3-""functmention"" --generating place-holders for instances of use of the function.",17,18
17215,8497667,initial symbol of the phrase-structure grammar.,1,2
17216,7931227,"Who:l a non-letter symbol is found the word is then conver-'e ,~ to storage for~:~.: 5 letters arc plnc,,q in the first 25 bitu of "" c ~-i i t:.:",7,8
17217,7931227,"support is the notation"" ""X"", meaning any string of units not containlng the boundary symbol ""#"".",18,19
17218,29849726,"They are frequently discussed under the title of ""symbol table techniques"", or ""scatter storage techniques"" as used by Morris as the title of his article.",9,10
17219,29849726," The first available address in the Available Space List, J, for storing the entry is placed as The Overflow INDEX (I) = J $5 The entry is stored in the Available Space List (ASL) sequentially starting at ASL(J) and with a special symbol EOE placed at the end of the entry in ASL, and exit on success.",51,52
17220,237366093,"Assuming that there are cd convolution kernels, the formula for the n-th convolution operation of the m-th convolution kernel is as follows: h mn = w • c n:n+k−1 + b, The size of the sliding window contains k characters, which is represented by the symbol c n:n+k−1 , w represents the convolution kernel, each time the feature is obtained by sliding k characters h mn , that is, the red box and the yellow box in the figure 2 .",55,56
17221,227230266,"Arabic and Latin punctuation removal: We removed a list of Arabic punctuation symbol that we prepared manually, whereas we used the Latin punctuation symbol list provided by the string library (string.punctuation).",13,14
17222,227230266,"Arabic and Latin punctuation removal: We removed a list of Arabic punctuation symbol that we prepared manually, whereas we used the Latin punctuation symbol list provided by the string library (string.punctuation).",25,26
17223,227231222,"In Korean, when UPOS tags are allocated by the eojeol unit, postpositions and verbal endings are only annotated as ADP and PART when they are separated by a punctuation mark or an identifying symbol.",35,36
17224,227231222,"The only exception to this is if a postposition is separated from the dependent word using a punctuation mark or a symbol, then it is recognized as an independent unit.",21,22
17225,245587,"Note that the derivation D of sentence s is then simply E start,s[1..|s|] , where start is the start symbol of the MRL's context free grammar, G. Our procedure to find the most probable partial derivation E * n,s[i..j] considers all possible subtrees whose root production has n as its LHS nonterminal and which cover s from index i to j. Mathematically, the most probable partial derivation E * n,s[i..j] is recursively defined as: E * n,s[i..j] = makeT ree( arg max π = n → n1..nt ∈ G, (p1, .., pt) ∈ partition(s[i..j], t) (Pπ(s[i..j]) k=1..t P (E * n k ,p k ))) where partition(s[i..j], t) is a function which returns the set of all partitions of s[i..j] with t elements including their permutations.",24,25
17226,953076,"The first sets are defined for a non terminal symbol A over a string c~ of preterminals as the potential preterminal symbols which can occur in the leftmost position of the string: F~IRST(a) = {a E Y, I c~==~afl} u {el ,~=~e} The follow sets of a nonterminal A are defined as the first sets of the preterminals which may occur after the nonterminal A: FOLLOW(A) := {a E $ I S=~aA~ A a E FIRST(fl)} u {$1 S:~A*}.",9,10
17227,953076,"The first and follow sets allow to define the reach relations, which provide the information for a nonterminals A (in the stack) and for a preterminal symbol (located in the input string a) by which production rule(s) the preterminal can be accessed: REACH(A,a,P)~3P67 > with P = A ~ ~ : a 6 FIRST(~) ^ ~(a = ~) Va 6 FOLLOW(A) ^ a =~e The reach relations are valid for all context free languages and extend the applicability of LL(1)-tables for them in general.",29,30
17228,953076,At this point the nodes relevant to the functional assignments are easily accessible as the left hand side symbol (for the metavariable T) and the right hand side symbols (for the metavariables ~} in the rules.,18,19
17229,36147261,"per:;on singular conjunctive present active"" giving a cover symbol ""verb, 1.",9,10
17230,36147261,"the basic wordclass, if this has not been defined as belonging to a cover symbol 2.",15,16
17231,36147261,"file applicable covet"" symbol otherwise and given a 2D-matrix that contains relative frequencies of transitions from any label (wordclass or cover symbol) to any other label in the text, then some useful rueastn'es are the branclfing factor for a given label, that tells how many different labels actually followed/preceeded it in an analysed text.",4,5
17232,36147261,"file applicable covet"" symbol otherwise and given a 2D-matrix that contains relative frequencies of transitions from any label (wordclass or cover symbol) to any other label in the text, then some useful rueastn'es are the branclfing factor for a given label, that tells how many different labels actually followed/preceeded it in an analysed text.",25,26
17233,36147261,"tile correlation between different rows/columns of the matrix, that gives information about how similarly the labels behave in a general right/left context, i.e. how much itffomtation will be lost by combining two labels into a new cover symbol.",43,44
17234,36147261,"he symbols nsed h~ th~,~ examph,x ca~ l,:-intcq~reted as: the subclass cannot be specified for the wordclass in question the subclass is specifiable, but has uot been specified Example 1: If a user works on a 3D-matrix with the matl/x editor aid considers inclusion of all conjunctions into one cover symbol in the first scope, but wants to leave the most frequent labels out, he/she will look e.g. at a part of the matrix by a comm,'u~d DISPLAY C ......... ;; which will give a display of only those parts of the matlix where a conjtmction stands in the first position of the Markov chain.",62,63
17235,36147261,"Let us assume that the ,nest frequent labels ,-u'e C(K)#######, C02..##### and 'all labels C01 but without C01..#####, the,l he/she could define the cover symbol 'ZCON' for scope I in the following way: The very low standard deviation of the label A17.....## casts considerable doubt upou its significance; it will probably be included into a cover symbol.",53,54
17236,36147261,"Let us assume that the ,nest frequent labels ,-u'e C(K)#######, C02..##### and 'all labels C01 but without C01..#####, the,l he/she could define the cover symbol 'ZCON' for scope I in the following way: The very low standard deviation of the label A17.....## casts considerable doubt upou its significance; it will probably be included into a cover symbol.",92,93
17237,36147261,Exanple 4: The labels M02####### and B02####### have a high correlation and are therefore candidates to be put into the same cover symbol.,37,38
17238,36147261,"This gives the consttaint use of cover symbol notations within a cover symbol definition, E.g. in an expression Z1 = <expl>!(<exp2>!<exp3>), the cover symbol set becomes inconsiste.t, if another cover symbol Z2 occurs included in <expl> or <exp3>, cover symbols occuning on the right side of a definition must be defined in the same file. """,7,8
17239,36147261,"This gives the consttaint use of cover symbol notations within a cover symbol definition, E.g. in an expression Z1 = <expl>!(<exp2>!<exp3>), the cover symbol set becomes inconsiste.t, if another cover symbol Z2 occurs included in <expl> or <exp3>, cover symbols occuning on the right side of a definition must be defined in the same file. """,12,13
17240,36147261,"This gives the consttaint use of cover symbol notations within a cover symbol definition, E.g. in an expression Z1 = <expl>!(<exp2>!<exp3>), the cover symbol set becomes inconsiste.t, if another cover symbol Z2 occurs included in <expl> or <exp3>, cover symbols occuning on the right side of a definition must be defined in the same file. """,28,29
17241,36147261,"This gives the consttaint use of cover symbol notations within a cover symbol definition, E.g. in an expression Z1 = <expl>!(<exp2>!<exp3>), the cover symbol set becomes inconsiste.t, if another cover symbol Z2 occurs included in <expl> or <exp3>, cover symbols occuning on the right side of a definition must be defined in the same file. """,36,37
17242,36147261,"constraint use of CS-notation"" la} order to support order in the cover symbol definitio.s cover symbols that ate to be included into other cover symbols (i.e. they have only attxifiaty function, but will not occur ha a map) are notated differently from cover symbols, that will occur hi a map: Auxili,'u'ies lmve a name preceeded by a Additional notations are used in a textual definition to specify the scope for subsequently defined cover symbols, Cover symbol definitio, fries may include other cove,' symbol definition fries by a C-like ""#include"" command.",16,17
17243,36147261,"constraint use of CS-notation"" la} order to support order in the cover symbol definitio.s cover symbols that ate to be included into other cover symbols (i.e. they have only attxifiaty function, but will not occur ha a map) are notated differently from cover symbols, that will occur hi a map: Auxili,'u'ies lmve a name preceeded by a Additional notations are used in a textual definition to specify the scope for subsequently defined cover symbols, Cover symbol definitio, fries may include other cove,' symbol definition fries by a C-like ""#include"" command.",85,86
17244,36147261,"constraint use of CS-notation"" la} order to support order in the cover symbol definitio.s cover symbols that ate to be included into other cover symbols (i.e. they have only attxifiaty function, but will not occur ha a map) are notated differently from cover symbols, that will occur hi a map: Auxili,'u'ies lmve a name preceeded by a Additional notations are used in a textual definition to specify the scope for subsequently defined cover symbols, Cover symbol definitio, fries may include other cove,' symbol definition fries by a C-like ""#include"" command.",95,96
17245,227231049,Adapted API script Latin script MA phoneme API symbol ḥ 7 ‫ح‬ ħ ḍ/ đ d ‫ض‬ dˁ ɛ / Ɛ 3 ‫ع‬ ʕ ġ gh ‫غ‬ ɣ ħ h ‫ه‬ h ḫ / x kh ‫خ‬ x ḷ l ‫ل‬ (geminated) l ṟ / ṛ / ř r ‫ر‬ r ṣ s ‫ص‬ sˁ š / ṧ ch ‫ش‬ š ț / ṭ t ‫ط‬ tˁ ž j ‫ج‬ ʒ ẓ / ż z ‫ز‬ z â a ‫ا‬ Ɂ / a ə e - - î i ‫ي‬ i û ou ‫و‬ u Table 2 .,8,9
17246,227231049,"Conversion rules from adapted API to Latin script used for MA dialect in SM and its equivalent in MA phoneme with the associated API symbol Word Embedding model generation We use three word-embedding models, namely, word2vec CBOW (continuous bag of words) and Skipgram (Mikolov et al.,",24,25
17247,854276,Each symbol in the string corresponds to a word in the original sentence.,1,2
17248,854276,"However, if the cost of insertion is equal to the cost of deletion for any given symbol then symmetry follows as a consequence.",17,18
17249,854276,"The edit cost is also computed from the match set M. In a manner similar to the Fischer/Wagner measure it is assumed that each grammatical symbol has two associated unit costs, the cost of insertion and the cost of deletion.",27,28
17250,854276,In other words the cost of a unit edit operation is independent of the symbol being edited.,14,15
17251,237433853,The set of elements hi:ai forms the head of a rule and the set of elements bi the body; the head and the body are separated by an if-symbol (:-).,33,34
17252,227230505,Input characters that are not part of the vocabulary are mapped to a special symbol for unknown characters.,14,15
17253,9438924,"Now we are able to compute the a-priori p r o b a b i 1 i t y of a (partial) derivation T starting with the symbol S in the following recursive manner : p(S <- s) -- I p(xYz <-T-S)= p(xAy<-S)*q , if there is a rule < A -> Y, q> in R In our implementation, these a-priori probabilities are weighted with the scores delivered for individual words by the acoustic-phonetic componem to yield accumulated grammaticalacoustic scores for whole phrases.",31,32
17254,226226849,3) The shaddah is the gemination symbol used to indicate consonant doubling.,7,8
17255,1653737,"MULTILEX entry contains the following information: head word in the dictionary form; part-of-speech symbol; word-forms including spelling variants, abbreviations, or stem/stems; target language equivalents (T~); reliability index of TE; notations which help a user to choose the necessary TE of several registered in the entry; terminological notations which specify the subject-field of the TE; definition of the head word; illustrative phrases; multiterms and collocations which include the head word.",19,20
17256,1653737,"The following information is obligatory for all the entries: head-word, partof-speech symbol, word forms or stems, subject-field notation, target language equivalents and reliability index.",17,18
17257,246702336,The if-symbol (:-) separates the head and the body of the rule.,3,4
17258,1696711,"All word boundary sequences, including those which account for the assinfilatory processes described in 2.1, were placed in one file and the medial word boundary symbol was removed.",27,28
17259,1696711,"z a/. Informally, (3) states that if/z a/cannot begin words (according to the Morphology-lexicon),/z/must be an inflectional suffix of the previous word: therefore place an 'M' (morpheme boundary) before /Z] and shift the # symbol to the right of/z].",52,53
17260,1696711,symbol were considered separately.,0,1
17261,1696711,symbol is e~panded into two alternatives.,0,1
17262,1696711,"ai/. In eliminating the alternatives, a slight modification has to be made to the rules: rather than referring to two segments to the left and right of #, they refer to the two segments to the left of an M symbol (if present) and to two segments to the right of #.",43,44
17263,14856543,"Every node is labelled by a representation of a single (autosemantic) word form, having the shape of a complex symbol containing its syntactic, morphological and lexical parts, corresponding to the character of the particular level.",22,23
17264,14102745,"But over the years, the usage of these characters particularly, ਖ਼, ਗ਼, ਜ਼, and ਫ਼ has been on the decline as many Punjabi speakers do not make a distinction between ਖ ਖ਼, ਗ ਗ਼ and ਫ ਫ਼. The result is that most of the words in Gurmukhi are now written without nukta symbol.",58,59
17265,14102745,The symbol ਸ਼ is an exception. •,1,2
17266,14300645,"In Joshua, an SCFG can be represented as a set of rules given as: C i →<α i , γ i , ~i , φ i > (1) where C i is a non-terminal symbol of the grammar, α i and γ i are sequences of terminal and nonterminal symbols for the source and target sides respectively, ~i is a correspondence between the non-terminals of α i and γ i , and φ i is a feature vector defining the probability of translation from α i to γ i .",40,41
17267,14300645,This is corrected by removing the plural symbol of Persian words.,7,8
17268,14276458,"These have ts be listed, and liaison is prevented before them through the use of a macro symbol referring to their list.",18,19
17269,14276458,"By preventing liaison before vowel initial words belonging to specific parts of speech represented by other macro symbols, and included in lists available to the system~ the use of the macro symbol L in liaison rules allows for a correct transcription, without liaison, of ' usually a determiner, in such a context We: Le premier/el man second se ressemblent.",32,33
17270,53593097,"The primitive finite state machines in this example include the set of all possible worlds W , as well as worlds 57 in which the center has the symbol CAT, worlds where the center has the symbol DOG, and worlds where the center is adjacent to the pericenter. (",28,29
17271,53593097,"The primitive finite state machines in this example include the set of all possible worlds W , as well as worlds 57 in which the center has the symbol CAT, worlds where the center has the symbol DOG, and worlds where the center is adjacent to the pericenter. (",37,38
17272,53593097,Each world in W is simply preceded by a symbol indicating whether it is true or false.,9,10
17273,2481675,"Each word depends on a ""parent"" word or a root symbol.",12,13
17274,14249050,Sometimes Nexidia does not provide a symbol to express certain distinctions.,6,7
17275,14249050,Language Orthographic symbol IPA SAMPA Dari Language Conclusion We have achieved some success searching for language and dialect-specific pronunciations using the Audio Gazetteer tool.,2,3
17276,5903320,We are using the symbol _ to close off tuples and the symbol # to close off relation states.,4,5
17277,5903320,We are using the symbol _ to close off tuples and the symbol # to close off relation states.,12,13
17278,5903320,They are chained together using a transition on the symbol # from the last state of one gobbler to the first state of the next.,9,10
17279,10531943,"IPA is organized in such a way that each symbol on a chart can be visualized as a hierarchical structure of features (Peter Ladefoged, 1988) .",9,10
17280,7674916,The plus symbol (+) functions as a control character that switches from the text entry mode to the vocabulary entry mode.,2,3
17281,7674916,"The subsequent character sequence (n-n-e) represents the required feature structure, followed by a colon (:), the actual word form (laptop), and a plus symbol (+) that quits the vocabulary entry mode.",35,36
17282,2922053,"A meaning is a finite sequence of variable atoms, this is an expression formed by applying a predicate symbol to the correct number of variables as arguments.",19,20
17283,9072766,"In this stage we apply a parallel rewrite rule which transforms all sequences of joiner symbols into a single joiner symbol, i.e. ""condenses"" consecutive joiners into one joiner.",20,21
17284,13908757,The underlying form is represented by the stem plus a unique symbol for each affix; these symbols are then realized by (converted to) the allomorph appropriate to their phonological environment.,11,12
17285,13908757,"The unique symbol can be the affixs gloss, which is typically something like ¡PL¿ or ¡1.Sg¿; the angled brackets indicate to sfst that this is a single symbol.",2,3
17286,13908757,"The unique symbol can be the affixs gloss, which is typically something like ¡PL¿ or ¡1.Sg¿; the angled brackets indicate to sfst that this is a single symbol.",33,34
17287,13908757,"The <PL> symbol bracketing the allomorphs on the surface side ensures that the algorithm applies allomorph constraints of a given affix only to the allomorphs of that affix, and not to sequences of phonemes which happen to be identical to the allomorph but which belong to another affix, or to a stem.",4,5
17288,17781526,"Next, we construct FSTs I G and I P which insert the respective delimiter symbol between grapheme and phoneme segments from M into words and phonetic transcriptions, respectively.",15,16
17289,17781526,It generates all admissible segmentations by inserting the delimiter symbol at the appropriate location(s) on its output tape.,9,10
17290,13247233,This symbol is used to indicate clustering of Iyek Ipee characters to produce a combined sound.,1,2
17291,201628782,But inserting a symbol into the form would mean that the form no longer analyses; we need to somehow mark the split-point.,3,4
17292,201628782,"If the lexicon contains the symbol-pairs/arcs: then, since the form-side of this analysis is 17.,",5,6
17293,201628782,"will match, but since there was a backtrack-symbol, we trigger a retokenisation.",10,11
17294,201628782,The input-mark symbol says where the form should be split.,4,5
17295,201628782,"We also insert a backtracking symbol with the space, so that the tokenisation tool knows that the compound analysis is not necessarily the only one (but without having to explicitly list all possible alternative tokenisations).",5,6
17296,29221852,"V.KP * KP → K.DP DP → D.N + D.N.M odP M odP → K.DP K → t + ..... V → padang + ..... V oice → mi + ..... N → suwal + ..... D → u + ..... Our goal is to compare the possible derivation trees of the sentence mi-padang t-u suwal n-ira tatakulaq and to use the Content Vector to infer the ""most likely"" tree in the grammar G. Stochastic grammars In a stochastic grammar Manning and Schütze (1999) , derivations with the same non terminal symbol have a probability p such that the sum of the probabilities for each non terminal is 1.",101,102
17297,195776180,"To lexicalise patterns of this type, we extract the nodes (i.e. words) from the path, arrange them as per their order in the sentence and replace the entity mentions by a symbol denoting simply their entity types.",35,36
17298,26645651,"We generate a caption as a series of words (encoded as 1-hot vectors), terminated by the end of sentence symbol </s>.",24,25
17299,28821415,"The set I lex of initial trees for the lexicalized d-TAG G lex is the maximal subset of T (NR) that only contains d-trees whose root is labeled by the start category S. As the empty string is not generated by G d , the initial trees of G lex have at least one terminal symbol on the frontier.",61,62
17300,53245763,"It consists of more than 1,600 symbol-vector pairs, each associating a Unicode character to a real 300−dimensional vector.",6,7
17301,406115,"We use the methods that are based on pattern matching to extract character aliases Lu and HOU (2006) , as is shown below following methods: 1) Synonymy keywords + Synonyms + End identifier Synonymy keywords: 本名| 别 号 | ，字 |^(字)|，号|^(号) |又号|^(名)|笔名|自号 |又名|乳名|别名|原名|艺名|本名|曾用名| 俗称|亦称|又称, the symbol ""|""means choose, ""^""means that matches the beginning of the string.",53,54
17302,406115,which mean comma symbol and full stop.,3,4
17303,10311210,"Here are examples of Magn type collocations as they would appear in an English Explanatory Combinatorial Dictionary [ECD] (where collocates are listed in the entries of their headwords): Magn: high, significant, < huge, << astronomical, << exorbitant SPENDING (N) Magn 2 : strong [AntiBon+Magn 2 ]: lavish The symbol ""//"" precedes a fused element of the value of an LF, expressing together, i.e., in one word, the meaning of the headword and the intensification; thus, myriad means 'huge number'.",63,64
17304,202235289,Hashtags were preprocessed by removing the # symbol and keeping the words. •,7,8
17305,201739614,"The tool used here is the commonly used tokenizer.perl, which can separate the words and punctuation in English and convert the special symbols to keep the same symbol.",28,29
17306,14474392,Every tag label becomes a nonterminal symbol.,6,7
17307,14474392,"To every nonterminal symbol A, we associate a nonnegative integer h(A), called the head of A, such that h(A) ≤ χ.",3,4
17308,17872284,How exactly one goes about this has no bearing on the translation procedure as long as every interior node with a non-terminal symbol is a projection of some LI.,24,25
17309,17872284,"In the other direction, π(l) := p 1 • • • p n is the string of all projections of l such that each p 1 is the parent of l and each p i is immediately dominated by p i+1 , 1 ≤ i < n. A node n is a maximal projection of l iff l = n or n the last symbol of π(l).",67,68
17310,17872284,"For some distinguished symbol, µ(f i ) := .",3,4
17311,6667629,In order to distinguish reachability in G from reachability in some g ∈ τ (G) I sometimes use the symbol instead of .,21,22
17312,6667629,Suppose we have a string transduction τ that can only insert -or | after a symbol.,15,16
17313,34878614,Symbol What the symbol means a Marks an argument valent; the absence of this subscript indicates that the valent is not an argument of its governor The machinery given in the table is just enough to address control and raising and distinguish between them.,3,4
17314,9595259,Function Pref2N is called |w| times for each symbol a in the prefix.,8,9
17315,9595259,"Note that in the superimposed coding (sparse matrix) representation, the for loop must run for every symbol of the alphabet.",19,20
17316,2395785,"Moreover, the operation corresponding to this connection is noted down in the rewriting rule (i.e. the algebraic inscription) by the symbol ""+"".",23,24
17317,2395785,"This corresponds to the symbol ""→"" in the algebraic inscription.",4,5
17318,2395785,"8  Barnard's diagrams have no discrete means to express individual part-whole relations: the brace 1836 ) is equivalent to Chomsky's rewriting operator as well as the ""+"" symbol, linking a phrase with the entire set of its immediate constituents.",35,36
17319,2395785,"Thus, if we make just one IC-cut in each sentence, ignoring any smaller constituents for the moment, then all four sentences conform to pattern X. Hockett's boxes can be typed by an additional symbol, ""<"" or "">"", ""placed at each junction of ICs, pointing from attribute to head"" (fig.",39,40
17320,195064798,"The 8 syllabification rules are applied to this sequence to derive a symbol sequence in terms of c, c1, c2 and v. The symbol sequence is the input to the FSM in Figure 1 .",12,13
17321,195064798,"The 8 syllabification rules are applied to this sequence to derive a symbol sequence in terms of c, c1, c2 and v. The symbol sequence is the input to the FSM in Figure 1 .",25,26
17322,195064798,"Suppose the symbol pair associated with an arc is ""x/y"".",2,3
17323,195064798,"This indicates that whenever a symbol ""x"" is fed to the state at the beginning of the arc, the system makes a transition along the arc and outputs the symbol ""y"".",5,6
17324,195064798,"This indicates that whenever a symbol ""x"" is fed to the state at the beginning of the arc, the system makes a transition along the arc and outputs the symbol ""y"".",32,33
17325,195064798,"If part of a symbol string reaches to the final state F, then it is consumed, and a transition from F to I with arc label e/b takes place, where e is null and b denotes the syllable boundary of the consumed string.",4,5
17326,195064798,"The symbol c, v s and v l stand for consonants, short vowels and long vowels respectively.",1,2
17327,195064798,The output symbol along an arc is either a syllable label or the reflection of the input itself.,2,3
17328,6248369,"Under this framework, a tailored PCFG grammar we used for generation can be described as a 6-tuple: G = Np, Nc, T, S, L, λ (1) where N p is a finite set of non-terminal symbols produced by a common parser, N c is a finite set of 1 A demo can be found at http://www.aidc.org.cn:8008/WebContent/ concept symbols related to specific record fields, T is a finite set of NL terminal symbols (words), S ∈ N p is a distinguished start symbol, L is a lexicon which consists of a finite set of production rules, and λ is a set of parameters that defines a probability distribution over derivations under G. Grammar Induction In this section, we present a learning procedure for the grammar described above.",99,100
17329,6248369,"Then for each phrase in the table, we find the minimal subtree spanning it, and modify its ancestor node attached directly below the subtree's root node to the conceptual symbol of its aligned field.",32,33
17330,6248369,"It first fills the diagonal cell of the chart with terminal words with the top scoring words emitted by the unary rules of the type A → α, where A is a non-terminal symbol, and α is a terminal word.",36,37
17331,199582240,"A specific breakthrough in mathematical language was the invention of the equals symbol (=) that is now universally accepted in mathematics, which was first recorded by the Welsh mathematician Robert Recorde in The Whetstone of Witte (1557) 1 .",12,13
17332,199582240,"Instead, every mathematical symbol has various types of relations, and these relations are vertically and visually represented in 2D space.",4,5
17333,18790184,"However, these consonants are not separated by any special symbols (as in the case of Indian languages where the consonants within the conjugate are separated by a special symbol called halant).",30,31
17334,18790184,The syllables are separated by a special symbol called tsheg.,7,8
17335,10128510,"For the final word in the sentence, we add a sentence-end symbol to the final state of both of these decoders − → r |F |+1 = enc(embed(⟨s⟩), − → r |F | ) ← − r |F |+1 = enc(embed(⟨s⟩), ← − r 1 ).",14,15
17336,10128510,"At each step of beam search, the best-scoring hypothesis remaining in the beam that ends with the sentence-end symbol is saved.",23,24
17337,836555,"For example, consider extracting data from Figure 1 into numerical relations of the form ts tick abs (TS symbol, numerical value), e.g. ts tick abs (US Unemployment, 4.9%), or ts tick rel (TS symbol, change in num.",20,21
17338,836555,"For example, consider extracting data from Figure 1 into numerical relations of the form ts tick abs (TS symbol, numerical value), e.g. ts tick abs (US Unemployment, 4.9%), or ts tick rel (TS symbol, change in num.",44,45
17339,836555,"It is not rare for the parser to generate an extraction candidate ts tick abs (TS symbol, numerical value) in which the numerical value fits into the time series of the time series symbol, but the extraction is nonetheless incorrect.",17,18
17340,836555,"It is not rare for the parser to generate an extraction candidate ts tick abs (TS symbol, numerical value) in which the numerical value fits into the time series of the time series symbol, but the extraction is nonetheless incorrect.",36,37
17341,2881080,"Stemming, language detection, and symbol removing are these pre-processing steps.",6,7
17342,201762525,"It takes surface forms produced by LEXC and applies rules on them; the rules vary depending on morphological alteration of stem, morphologically or phonologically conditioned deletion of suffix, morphologically or phonologically conditioned insertion, morphologically or phonologically conditioned symbol change.",41,42
17343,44084379,"Every emotion and valence adopts different ensemble methods, the symbol '-' means that the component is not used in the ensemble method in this emotion or valence.",10,11
17344,62330738,The skip-grams are indicated by a star symbol * in between the sequences.,9,10
17345,9382184,"Morph resource in Lttoolbox has three important sections (i.e. for automatic conversion ) namely symbol definition section, paradigm definition section and lexicon dictionary.",15,16
17346,9382184,These symbols are defined within symbol definition (sdef).,5,6
17347,9382184,"<sdef n=""gen:m"" c=""masculine"" /> Listing 3: ""Example symbol definition"" In the above example, gen is the symbol for gender, m is the corresponding feature value.",15,16
17348,9382184,"<sdef n=""gen:m"" c=""masculine"" /> Listing 3: ""Example symbol definition"" In the above example, gen is the symbol for gender, m is the corresponding feature value.",26,27
17349,9382184,"In algorithm 1, we are converting symbol definitions and paradigm definitions into IR (i.e features and paradigms).",7,8
17350,174799296,"2016) on the concatenated training data of all three languages, append an EOS symbol to each sentence, and train the translation model.",15,16
17351,215716044,"Introduction Connecting human language predicates like ""red"" and ""heavy"" to machine perception is part of the symbol grounding problem (Harnad, 1990) , approached in machine learning as grounded language learning.",20,21
17352,219310273,"When an English-English alignment is one-tomany or many-to-one, a special symbol is added in between two adjacent sentences.",19,20
17353,14996102,symbol in the form of compound tags like N.SPT and ADJ.TMP mentioned earlier.,0,1
17354,13939295,Hashtags: the hash symbol (#) is removed from hashtags and they are treated as regular words afterwards. •,4,5
17355,6152986,"Pre-Processing The query pre-processing for snippets retrieval is the same to the strategies for document retrieval, which includes unnecessary symbol removal, stop-words removal, case-folding, noun extraction and concept extraction with Metamap.",24,25
17356,12381273,Then we give each input symbol a weight which reflects the probability of seeing this symbol when looking at the appropriate attribute for all the members of this class.,5,6
17357,12381273,Then we give each input symbol a weight which reflects the probability of seeing this symbol when looking at the appropriate attribute for all the members of this class.,15,16
17358,12381273,"The weights for the St symbol represent the relative frequency of the ID in relation to the other ID's of this phoneme, the weight for the rule (surrounded by curly braces) is the size of the group it represents as a fraction of all the ID's of this phoneme and the weights of each attribute is calculated by sampling over the range of the input phoneme corresponding to the appropriate attribute.",5,6
17359,53609310,PPM is based on conditional probabilities of the upcoming symbol given several previous symbols.,9,10
17360,53609310,"Cross-entropy is the entropy calculated for a text if the probabilities of its characters have been estimated on another text (Teahan, 1998) :     1 log n m m m d i i i H p x p x    (2) where n is the number of symbols in a text d, H d m is the entropy of the text d obtained by model m, p m (x i ) is a probability of a symbol x i in the text d obtained by model m. The cross-entropy can be used as a measure for document similarity; the lower cross-entropy for two texts is, the more similar they are.",91,92
17361,16984085,We also found mistyped tags (identifiable as tags not in the defined tagset) and a spurious $ symbol before some punctuation signs.,19,20
17362,1607717,"CRF-based segmentation Post-processing Punctuation, Consecutive and identical punctuation, Dot, Emotional symbol, Hyperlink, Quantifier, Ordinal number.",17,18
17363,1607717," Dot: when the character ""•"" appears in the training data, it is generally used as a connection symbol in a foreign personal name, such as ""奥黛丽•赫本"".",22,23
17364,1607717,"A similar rule is designed to join consecutive digits on the sides of the symbol ""."",",14,15
17365,1607717, Emotion symbol: some consecutive punctuations have special meanings.,2,3
17366,196594525,In Kannada 'laːɲʧana' widely used to describe 'Emblem visible symbol representing an abstract idea' This concept is not carried in Malayalam.,12,13
17367,196594525,In Kannada ' widely used to describe 'Emblem -A visible symbol representing an abstract idea' 1 .,11,12
17368,219304211,"The same is possible for the target language, although in the example above the only symbol shown is the ""Gàidhlig"" symbol for switching to Gàidhlig-Gàidhlig monolingual dictionaries.",16,17
17369,219304211,"The same is possible for the target language, although in the example above the only symbol shown is the ""Gàidhlig"" symbol for switching to Gàidhlig-Gàidhlig monolingual dictionaries.",23,24
17370,219304211,"In the sl field in the dictParam table, a ""¤"" symbol is placed, and this indicates to Multidict to refer to a separate table dictLang to obtain a list of the n languages which this particular n×n dictionary handles.",13,14
17371,219304211,"The tl field in the dictParam record for the n×n dictionary also contains a ""¤"" symbol if this is truly an n×n dictionary, including monolingual pairs such as English-English.",17,18
17372,15584709,2014) in that the seq2seq model supplies the hidden representation of the input sequence to predict the first symbol in a target sequence and then uses the predicted target symbol as an input to the LSTM layer to predict the current symbol.,19,20
17373,15584709,2014) in that the seq2seq model supplies the hidden representation of the input sequence to predict the first symbol in a target sequence and then uses the predicted target symbol as an input to the LSTM layer to predict the current symbol.,30,31
17374,15584709,2014) in that the seq2seq model supplies the hidden representation of the input sequence to predict the first symbol in a target sequence and then uses the predicted target symbol as an input to the LSTM layer to predict the current symbol.,42,43
17375,15308556,"For feature (d), it checks whether C n is a punctuation symbol (such as ""?"", ""-"", "","")",14,15
17376,15308556,"Because of their specific meanings in micro-blog, for exam-ple, ""#"" is a start or end symbol of a topic and they are often appeared in pairs.",23,24
17377,16260313,"When the symbol ""-"" or ""_"" is between English and Chinese.",2,3
17378,16260313,If the left is Chinese and the right is English the symbol should be segmented alone.,11,12
17379,646355,"In each iteration, for splitting, the symbol could be split into subsymbols.",8,9
17380,13482248,"Each entry in EntrezGene is provided with a rich range of information 5 , such as official symbol, corresponding organism and a short manually created summary.",17,18
17381,13482248,"In the document retrieval step, we relied on the PubMed API to search for the official symbol, official full name and the whole name of the gene.",17,18
17382,7838719,"In the hybrid decoder, a word level decoder outputs frequent words as they are, while replacing infrequent words with a special <UNK> symbol.",26,27
17383,7838719,"a t,i (h, s) = exp(align(h t , s i )) ∑ j exp(align(h t , s j )) align(h t , s i ) = v ⊤ a tanh(W a [h t ; s i ]) (2) In effect, at each timestep the attention mechanism scans the entire source to decide which parts are relevant to focus on when generating the next output symbol.",76,77
17384,7838719,"First, at each step, for each hypothesis to be extended, we prune the list of candidates for the next symbol based on local probability, to only keep beam_width + 1 candidates.",22,23
17385,7838719,"We collapse numbers into a single number symbol, remove special characters, and cluster the remaining lemmas into 10 000 clusters with word2vec (Mikolov et al.,",7,8
17386,10481901,"Mathematically, bigram score for a 'Name' is given by B(Name), where 'Namei' is the i th token/letter of the first name given that one start symbols (*) was added before each first name and an end symbol (#) was added after each first name before experiment, 'n' is the length of given first name.",47,48
17387,10481901,"Mathematically, trigram score for a 'Name' is given by T(Name), where 'Namei' is the i th token/letter of the first name given that two start symbols (*) were added before each first name and an end symbol (#) was added after each first name before experiment, 'n' is the length of the given first name.",47,48
17388,12831752,"Decision target sentence We define a ""sentence"" as a line of text marked off by the Japanese periodical symbol, ""。"".",20,21
17389,8250881,This calculation was done on Tweets which had URLs and mentions (which contain the @ symbol) removed.,16,17
17390,202677159,"Since we need a symbol for each sentence representation, we insert the [CLS] token before each sentence.",4,5
17391,202677159,"As a result, the vector for the i-th [CLS] symbol from the top BERT layer corresponds to the i-th sentence representation h i .",14,15
17392,14726684,PPM is based on conditional probabilities of the upcoming symbol given several previous symbols.,9,10
17393,14726684,"As a compression algorithm PPM is based on the notion of entropy introduced as a measure of a message uncertainty (Shannon, 1948) :     1 log n d i i i H p x p x    (2) where H dentropy of text d; p(x i ) -probability of character x i (i = 1…n) for all characters in the text d. Cross-entropy is the entropy calculated for a text if the probabilities of its characters have been estimated on another text (Teahan, 1998) :     1 log n m m m d i i i H p x p x    (3) where n is the number of symbols in a text d, H d m is the entropy of the text d obtained by model m, p m (x i ) is a probability of a symbol x i in the text d obtained by model m. The cross-entropy between two texts is greater than the entropy of a text itself, because probabilities of characters in diverse texts are different: m dd HH  (4) The cross-entropy can be used as a measure for document similarity; the lower cross-entropy for two texts is, the more similar they are.",168,169
17394,14726684,"While the entropy for the texts written in the same language was in average 2 ± 0.5 bit/symbol, for the different languages the average entropy varied from 4 to even 12 bit/symbol.",19,20
17395,14726684,"While the entropy for the texts written in the same language was in average 2 ± 0.5 bit/symbol, for the different languages the average entropy varied from 4 to even 12 bit/symbol.",36,37
17396,14145275,"We then automatically create semantic content for all lexicalized elementary trees by assigning a single semantic literal to each tree in the treebank, using the lexical anchor as the predicate symbol and variables for each substitution node and the 'self' role as arguments.",31,32
17397,9191554,symbol is used for the representation of morphology and semantics at POS level.,0,1
17398,9191554,"After taking a sentence as input, variables are initialized along with a starting value of the chart as ROOT @ S. In place of a dot symbol '•' used in the Earley algorithm, here an '@' symbol is used because the dot symbol is extensively used in the hierarchal annotation of the URDU.KON-TB treebank, from which the grammar productions are extracted.",27,28
17399,9191554,"After taking a sentence as input, variables are initialized along with a starting value of the chart as ROOT @ S. In place of a dot symbol '•' used in the Earley algorithm, here an '@' symbol is used because the dot symbol is extensively used in the hierarchal annotation of the URDU.KON-TB treebank, from which the grammar productions are extracted.",42,43
17400,9191554,"After taking a sentence as input, variables are initialized along with a starting value of the chart as ROOT @ S. In place of a dot symbol '•' used in the Earley algorithm, here an '@' symbol is used because the dot symbol is extensively used in the hierarchal annotation of the URDU.KON-TB treebank, from which the grammar productions are extracted.",48,49
17401,9191554,"The '@' symbol before a non-terminal on the RHS of the production is the case of predictor and the nonextended PREDICTOR() adds all the available L type productions of N.PROP into the chart from the grammar, even they are not required.",4,5
17402,5310075,"We search the dictionary for the longest sequence of symbols s that matches the current input, we output the dictionary entry for s, remove s from the input and add s followed by the next input symbol to the dictionary.",38,39
17403,5310075,"As symbol inventory, we use bytes, not unicode symbols.",1,2
17404,16585956,"of acceptable standard prefixes, acceptable suffixes present in the word, NE salutations, NE, POS i-1 , POS i, POS i+1 word length, word frequency, digit feature, symbol feature, MWE, reduplicated MWE} In the above feature list W i represents current and surrounding words, RW i represents root words, NE represents the name entity, POS represents the part of speech and MWE represents the multiword expression.",34,35
17405,18311191,People use the hashtag symbol # before a relevant keyword or phrase without space in the tweets to facilitate automatic categorization and search.,4,5
17406,5471567,If Word Contains Any Symbol This feature is boolean in nature and represents if the current word contains any symbol.,19,20
17407,5471567,Presence of symbol in a word gives a possible hint about the part-of-speech of the word. •,2,3
17408,17393896,"For each nonterminal symbol A of rank m, we compute a set Q A ⊆ (Sub 0, * ({t})) 1+m of (1 + m)-tuples of subtrees of t so that t 0 , t 1 , . . . ,",3,4
17409,17393896,"We initialize those sets Q A to be empty and then monotonically and recursively expand the sets by referring to the rules of G. We have t ∈ L(G) if t ∈ Q A for some initial symbol A ∈ I. When all sets converge without satisfying this condition, we conclude t / ∈ L(G).",38,39
17410,199566033,"In the simulated task environment, each of 7 locations was represented as an angle (in radian) in a circular space [0, 2π) and associated with a symbol.",32,33
17411,199566033,"The center location was associated with an empty symbol , representing the absence of a target.",8,9
17412,199566033,A selection of a symbol was treated as the fixation on its associated location.,4,5
17413,199566033,We use the superscript to denote the position of a symbol in a sequence of symbols and the subscript to denote a particular element in a sample space.,10,11
17414,199566033,"The perception module was implemented as a Hidden Markov Model (HMM), where the hidden variable S (k) represents states after processing the k-th symbol x * (k) in a sequence of symbols, assuming the agent is equipped with a perfect language model.",30,31
17415,199566033,X (k) representing symbol identities is conditioned on S (k) .,5,6
17416,199566033,"Y (k) represents the observations of the input symbol (i.e., a particular location in the task environment [see Figure 4 ]).",10,11
17417,199566033,"In this simple language, each state s (k) uniquely specifies the present input symbol x (k) ; p(x s (k) = x (k) x ( j |s i ) = 1 if s i = x j x k change is as follows : c -f -f -a-a -d-d -b.",16,17
17418,199566033,Noisy Input Channel Let y * be the noise-free observation of the target symbol x * .,15,16
17419,199566033,"4 We will use the same conditional prob- 4 The likelihood of observation y is also conditioned on the present fixation location, which is modeled as the symbol chosen by the decision making module at the previous timestep (see Section 4).",28,29
17420,199566033,"The likelihood function in the target present condition (see Table 3 ) assumes that the present fixation is on the center, which is true at the beginning of each target-present trial; the measure of first saccade onset is accu-ability distribution when the module updates the posterior probability of symbol x given noisy observation y. Parameters α and β are false positive and false negative rates, respectively.",55,56
17421,199566033,"2a: sim1) symbol-type similarity; e.g., a, b, c are similar because they occur at the same position in a sequence (i.e., as the first word of a two-word sentence) so a can be recalled as a , b , or c . (",4,5
17422,199566033,"+ symbol-type similarity; for example, a can be confused as a, b, and c. More specifically, we consider p(s j |s i ) = (1 − η noise )δ ij + η noise {η rand p rand (s j |s i ) + (1 − η rand )((1 − η trans ) p sim1 (s j |s i ) + η trans p sim2 (s j |s i ))}; p type (s j |s i ) (where type ∈ {rand, sim1, sim2}) was set to the reciprocal of the number of transitions corresponding to the type of confusion if s i → s j is allowed and 0 otherwise.",1,2
17423,199566033,Step 1: Each trial begins with the instantaneous update of input symbol from x * (k−1) to x * (k) .,12,13
17424,199566033,Step 2 and Step 3 are iterated until (1) the decision making module (see the next section) selects the target symbol x * correctly or (2) the maximum number of timesteps (= 100) has passed.,24,25
17425,199566033,"For each sentence, the model processed each symbol over 50 timesteps.",8,9
17426,199566033,"When a new symbol (i.e., f ) is presented, the perception module uses the last parser state to reset log priors, which determine different starting points before evidence integration.",3,4
17427,199566033,"When the race begins, the symbol candidate with a low surprisal value is many steps ahead of its competitors with high surprisal values.",6,7
17428,199566033,"When the action chosen at timestep t(≤ 100) corresponds to the target symbol, it terminates the present trial and the new target symbol is presented in the task environment.",13,14
17429,199566033,"When the action chosen at timestep t(≤ 100) corresponds to the target symbol, it terminates the present trial and the new target symbol is presented in the task environment.",24,25
17430,199566033,"If the module selects a non-target symbol (which is different from its previous selection), the model receives a penalty (= -1).",8,9
17431,199566033,"If the model selects the same wrong symbol as in the previous timestep (i.e., a t = a t−1 ), the model is not penalized; the reward is 0 in this case.",7,8
17432,199566033,"For example, let us suppose the decision making module made a sequence of choices , , a, a, , b, , c when the target symbol was c, assuming the previous trial ended at the selection of the previous target .",29,30
17433,199566033,"If the model fails to choose the target symbol for 100 timesteps, the task environment is updated to present a new target symbol.",8,9
17434,199566033,"If the model fails to choose the target symbol for 100 timesteps, the task environment is updated to present a new target symbol.",23,24
17435,199566033,"Thus, the decision making module has an option not to select any new symbol; technically, the model can keep choosing the previous target symbol over 100 timesteps.",14,15
17436,199566033,"Thus, the decision making module has an option not to select any new symbol; technically, the model can keep choosing the previous target symbol over 100 timesteps.",26,27
17437,199566033,"This suboptimal policy is better than choosing a non-target symbol; while the maximum reward per trial is 0.99 (if the model chooses the correct target at the first timestep after the task environment update), the model is given -1 for a single wrong selection.",11,12
17438,199566033,"In the text, we report the behavior of the best model that achieved the highest reward over 2400 four-symbol sentences because we are interested in the optimal agent's behavior.",21,22
17439,199566033,It makes sense that the model took a safer approach for the target f in the LoS context c given that symbol f in the LoS context was three times more frequent than f in each HiS context.,21,22
17440,5828958,"Bottom-up prediction In bottom-up parsing we predict a nonterminal constituent only when its first symbol has been found (Ljunglöf and Wirén, 2010, section 4.4.4) .",18,19
17441,5828958,"There are three possibilities, depending on whether the first symbol is a terminal, a nonterminal, or if the constitutent is empty.",10,11
17442,1741394,"2012) , where a method for symbol refinement of probabilistic tree substitution grammars is presented.",7,8
17443,1741394,"Given a finite set A and alphabet Σ, let Σ A denote the alphabet of all σ a with σ ∈ Σ, a ∈ A. Note that the new symbol σ a is merely a syntactic construct and should be identified neither with σ nor a. Presuming an alphabet Σ and set A, the set U Σ (A) of unranked trees over Σ indexed by A is the smallest set U such that A ⊆ U and for every n ∈ N, t 1 , . . . ,",31,32
17444,1741394,"For every symbol σ ∈ Σ and variable y i ∈ Y k , we define σ y i (t 1 , . . . ,",2,3
17445,1741394,y j ) in a symbol σ x i (resp.,5,6
17446,1741394,"A tree-adjoining grammar (TAG) is a tuple G = (Σ, S, S, A) where Σ is an alphabet, S ∈ Σ is called the start symbol, S ⊆ U Γ (∆) the set of initial trees, and A ⊆ U Γ (∆ ∪ Z) the set of auxiliary trees, with Γ = Σ ∪ Σ Y and ∆ = Σ X .",35,36
17447,1741394,The foot node of β is indicated by z. We denote the conditional probability that an elementary tree t with root symbol A is used to rewrite substitution or adjoining sites that are labeled with A by P (t).,21,22
17448,1741394,"These vertices are just copies of the symbols of G: for every symbol A ∈ Σ, a vertex A is introduced.",13,14
17449,1741394,"2 , the only relevant vertex of this form is B , whose hyperpaths represent derivations of auxiliary trees with root symbol B. Lastly, for every adjoining site that appears in an elementary tree t and is tagged with y i , we include a vertex S(t, y i ).",21,22
17450,1741394,Its head vertex is its root symbol.,6,7
17451,1741394,"2 , we see, among others, s(α 1 ), whose head vertex A corresponds to α 1 's root symbol, while its two tail vertices C stand for its respective substitution sites, and S(α 1 , y 1 ) for its adjoining site.",22,23
17452,1741394,"Its head vertex is B because β's root symbol is B, and it has no tail vertices, since there are no sites in β.",9,10
17453,1741394,"1 , there are only two possible derivations of trees with root symbol A: in the first one, we substitute two instances of α 2 into α 1 , and, after activation of the adjoining site in α 1 , adjoin β into this site.",12,13
17454,1741394,"In a hyperpath representing a PTAG derivation, the appearance of the hyperedge y(α 1 , y 1 ) 11 signifies that the adjoining site in α 1 , labeled with the split symbol B 1 , is activated, and next, an auxiliary tree with equal root symbol must be adjoined into it.",33,34
17455,1741394,"In a hyperpath representing a PTAG derivation, the appearance of the hyperedge y(α 1 , y 1 ) 11 signifies that the adjoining site in α 1 , labeled with the split symbol B 1 , is activated, and next, an auxiliary tree with equal root symbol must be adjoined into it.",49,50
17456,1741394,"This hyperedge can be interpreted as activation of the mentioned adjoining site, labeled with B 1 , and preparation of adjoining an auxiliary tree with root symbol B 2 .",27,28
17457,1741394,"This stands in conflict to the concept of adjoining, where the label of the node to be replaced must be identical to the symbol at the root of the auxiliary tree.",24,25
17458,1741394,"We construct G so that the trees of G are relabelings of those in G, generated by incorporating the annotations to the hyperedges in G to substitution and adjoining sites: is the result of replacing the root symbol A of t by A a , every substitution site A x i in t by A b i x i , and every adjoining site A y j by A c j y j , for i ∈ [rk 1 (t)] and j ∈ [rk 2 (t)].",39,40
17459,51997874,"Icon-based systems can have higher selection speeds, and can be easier for individuals with neuromuscular impairments to operate; there are a wide variety of symbol sets and symbol-based communication systems used (Patel, 2011) .",28,29
17460,51997874,"Icon-based systems can have higher selection speeds, and can be easier for individuals with neuromuscular impairments to operate; there are a wide variety of symbol sets and symbol-based communication systems used (Patel, 2011) .",31,32
17461,51997874,"We also share an experi-1 One notable exception to this trend is the system used in SymbolPath (Wiegand and Patel, 2012b) , which uses semantic frames to attempt non-sequential symbol prediction (Wiegand and Patel, 2012a) .",34,35
17462,51997874,"Our contributions in this paper are: • A proposed approach to synthesize a pseudocorpus with which to learn language models from a corpus-less symbol set • An experimental evaluation of the impact of various pieces of our corpus synthesis methodology on icon prediction accuracy Symbolstix dataset The Symbolstix (Clark, 1997) icon set is used in several commercial AAC applications.",26,27
17463,51997874,"However, this basic approach describes a generic process to produce models form a corpus-less symbol-set, and should translate to other situtions.",17,18
17464,12700615,"The OOV words are replaced with a ""UNK"" symbol.",10,11
17465,219299733,"Memory task The input of an instance of this task is a sequence of T = N + 20 discrete symbols in a ten symbol alphabet a i : i ∈ 0, . . .",24,25
17466,219299733,"a 7 , followed by N − 1 ""blank"" a 8 symbols, then a distinguished ""run"" symbol a 9 , followed by 10 more ""blank"" a 8 symbols.",21,22
17467,219299733,"Therefore the model has to remember the 10 ""data"" symbol string over the temporal gap of size N , which is challenging for a recurrent neural network when N is large.",11,12
17468,16150164,"In Chinese, text is a stream of characters (symbols) that could be interpreted differently based on their context where one symbol could be an independent word or part of a word.",23,24
17469,8821211,"This is roughly analogous to the distinction in linguistics and semiotics between denotation, the literal meaning of a term or symbol, and connotation, its sociocultural associations, famously articulated by Barthes (1957) .",21,22
17470,2980099,"The input sentence is represented as: S 2 = [w 1 + w 2 + ... + w n ] Here, the plus + symbol denotes the concatenation of the word vector.",27,28
17471,208853,"The weighted transitions of T form the set E ⊆ Q×Σ×∆×K×Q, where each transition includes a source state from Q, input symbol from Σ, output symbol from ∆, cost from the weight set K, and target state from Q. For each state q ∈ Q, let E[q] denote the set of edges leaving state q. For each transition e ∈ E[q], let p[e] denote its source state, n[e] its target state, and w[e] its weight.",23,24
17472,208853,"The weighted transitions of T form the set E ⊆ Q×Σ×∆×K×Q, where each transition includes a source state from Q, input symbol from Σ, output symbol from ∆, cost from the weight set K, and target state from Q. For each state q ∈ Q, let E[q] denote the set of edges leaving state q. For each transition e ∈ E[q], let p[e] denote its source state, n[e] its target state, and w[e] its weight.",28,29
17473,19523276,"We should now get rigorous about this distinction because logicality is a property of an object language symbol, and not of its interpretation in a particular model.",17,18
17474,19523276,"Henceforth, let's take BE and A to be object language symbols such that for every model Westerståhl (1985) , an object language symbol is logical if and only if it has the two properties called constancy and topicneutrality.",26,27
17475,6433532,"A relational model U, R 1 , R 2 , ..., R n is a representation of some structure with a universe U of elements and n relations R i ⊆ U k for some finite k. We can represent a string w ∈ Σ * with a finite relational model M w = U, ≺, (P σ ) σ∈Σ where U = {1, 2, ..., |w|} is an initial segment of the natural numbers representing the positions in the string, ≺ is a binary relation representing the natural order over the positions in the string, and each P σ is a unary relation representing the set of positions containing the symbol σ.",123,124
17476,7317741,This in one way address the classical problem of symbol grounding which is addressed by Harnard (1990) .,9,10
17477,6230079,"They are user-defined topics or keywords that are denoted by the hash symbol ""#"", followed immediately by a single word or multi-word phase joined without spaces (Qadir and Riloff, 2013) .",14,15
17478,6230079,"Tweets containing at least one hashtag of a Pre-processing Training hashtags are stripped of their hash symbol, ""#"".",18,19
17479,7584264,Only the last symbol differs in the transducer.,3,4
17480,7584264,"For synthesis, the empty symbol on the input level changes to form ID on the output level (1), but for all word form generation, the form ID on the input level changes to the part-of-speech tag on the output level (2).",5,6
17481,3115534,"The asterisk represents the Kleenstar operation, and • S is the start symbol used to represent the whole sentence, i.e. γ 0 : S → X , X .",13,14
17482,14768937,The symbol */mopo represents a degree modifier (e.g. 很 、 非常 、 十分 ).,1,2
17483,14768937,○ 2 */mone + */pxn = #2:-0.5 The symbol */mone represents a negative modifier (e.g. 没有、毫无、缺乏).,11,12
17484,14768937,The sentiment value assignment rule samples: ○ 3 质量|性能|像素|分辨率|清晰度|安全系数 /% + #[*/!(w|mone)] + 高/a = #3:0.5 The symbol #[*/!(w|mone)] means that the rule can cross arbitrary segmentations here except the punctuation(w) or negative modifier(mone) .,26,27
17485,220445617,"2015) , and the Generate function is a feed-forward network to compute a score for each symbol in target vocabulary.",19,20
17486,6972314,"For example, the symbol (code E8a) was added as variant of (code E8).",4,5
17487,6972314,"A mixed query containing both Latin and hieroglyphic text (top of right-hand panel) has been composed, the latter with the assistance of the symbol palette (left-hand panel).",28,29
17488,6972314,"This palette also functions as a catalog of symbols organized according to Gardiner's List classification (Section 3.1), so the user can navigate through it and consult the information and variants associated with each symbol.",37,38
17489,6972314,"For its implementation we have made use of the libraries provided with JSESH, including its symbol palette.",16,17
17490,219300889,"Regarding the challenging problems where there was more disagreement (less than 35% agreement) and, therefore, presented more difficulties for the tokenization tools are, the hypertext markup symbol, URLs and chemical substances.",32,33
17491,14898670,"For this topic, the top-5 most relevant terms given λ = 1 (ranking solely by probability) q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q P(Token | Topic) (log scale) water exhaust q q q q q q q q q q Top 10 Most Relevant Boundary are {out, #emailaddress, #twodigitnumer, up, #onedigitnumber}, where a ""#"" symbol denotes a term that is an entity representing a class of things.",1153,1154
17492,4954741,"Our main result is that forward application preserves recognizability if the STSG G is outputproductive, which means that each rule of G contains at least one output symbol that is not a state.",28,29
17493,4954741,Each symbol that occurs in such a tree has a fixed rank that determines the number of children of nodes with this label.,1,2
17494,4954741,"To avoid consistency issues, we assume that a symbol σ that occurs in both Σ and ∆ has the same rank in Σ and ∆; i.e., rk Σ (σ) = rk ∆ (σ).",9,10
17495,4954741,"For every WRTG G there is an equivalent WRTG G in normal form, in which the right-hand side of every rule contains exactly one symbol of Σ. Proof.",27,28
17496,4954741,"If G is input-productive, then each derivation step creates at least one input symbol.",16,17
17497,16491812,"According to the same lexical resource, this LU has 12 lexical senses and the first one (i.e. ""The Crown (or the reigning monarch) as the symbol of the power and authority of a monarchy"") could evoke other frames, like LEADERSHIP.",30,31
17498,30919873,The rank of a symbol is the number of child elements it needs to have in the tree.,4,5
17499,30919873,"A node in a dependency tree can have any number of children, therefore the alphabet in the tree can be assumed to contain each symbol with multiple different ranks to adhere to the formalism.",25,26
17500,812036,"APGs derived from concatenation Alphabets of graph primitives As Engelfriet and Vereijken (1997) observe, given a concatenation operation a class of graphs can be seen as an interpretation of a set of strings, where each symbol in the string corresponds to a graph primitive.",39,40
17501,7138760,"Ignore a PSA when (i) at least one symbol from the meaning of the utterance is absent from all P(w), and (ii) not all N(w) contribute to the meaning of the utterance.",10,11
17502,7138760,"For each word w of the utterance, remove from P(w) any symbol not included in the utterance meaning.",13,14
17503,7138760,"For each word w of the utterance, add to N(w) any conceptual symbol exclusively in P(w) (thus, absent from the P set of the remaining words).",14,15
17504,7138760,"For each word w in the utterance, remove from P(w) any conceptual symbol that appears only once in the utterance meaning and is included in the N(w ) for some other word w of the utterance.",14,15
17505,7138760,"For each word w in the utterance, if w converged for its conceptual symbol set, that is, N(w) = P(w), remove from D(w) any expression that does not involve the conceptual symbols in N(w); if the word has not yet converged, remove from D(w) any expression that includes a symbol not in P(w).",14,15
17506,7138760,"For each word w in the utterance, if w converged for its conceptual symbol set, that is, N(w) = P(w), remove from D(w) any expression that does not involve the conceptual symbols in N(w); if the word has not yet converged, remove from D(w) any expression that includes a symbol not in P(w).",60,61
17507,7138760,"The original Rule 1 would discard relevant PSAs because at some point the symbol DECL would be absent from all P(w) (the set P for a word w), that is, at some point there would be no word in any utterance which could possibly contribute DECL.",13,14
17508,7138760,The updated lexicon shows the following configuration: The fourth heuristic compares the necessary symbol sets of the utterance words.,14,15
17509,7138760,"Thus, the conceptual symbol ball can be removed from P(john) and P(the), as shown below: The learner is ready for what Siskind (1996) calls ""stage two"": once the relevant conceptual symbols were discovered, a structured meaning is calculated for words that have more than one conceptual symbol.",4,5
17510,7138760,"Thus, the conceptual symbol ball can be removed from P(john) and P(the), as shown below: The learner is ready for what Siskind (1996) calls ""stage two"": once the relevant conceptual symbols were discovered, a structured meaning is calculated for words that have more than one conceptual symbol.",57,58
17511,14194494,"Is stop word: whether the target word is a stop word, punctuation symbol, proper name or number.",14,15
17512,7641692,A common way is to attach a symbol to all morphs on the right (or left) side of the morph boundary.,7,8
17513,7641692,Marking affixes by category and compound boundaries with a special linking token is called the compound-symbol strategy.,17,18
17514,18184607,It is one of flexional forms of essere ('be') instead of the symbol (SYM).,16,17
17515,17492419,"1987; Weir, 1988) is a tuple G = (N, T, V, P, S) where N is a finite set of non-terminals with a function dim: N → N determining the fan-out of each A ∈ N ; T and V are disjoint finite sets of terminals and variables; S ∈ N is the start symbol with dim(S) = 1; and P is a finite set of rewriting rules A(α 1 , . . . ,",69,70
17516,17492419,"All-accepting grammars usually have only one non-terminal symbol, but we need a distinction between pre-terminal constituents T and general constituents A for simulating SLCFRS in normal form as well as the full class.",11,12
17517,348276,If an expanded rule contains a pointer list (indicated by a + symbol) then the rule is duplicated for each element of the pointer list associated to the current index value before dereferencing.,13,14
17518,15280593,"The text is converted via the following regular ex-POS tags: noun (n), verb (v), adjective (a), adverb (adv), article (art), preposition (prep), conjunction (conj), interjection (interj), pronoun (pron), numeral (num), noun phrase (np), verb phrase (vp), symbol or special character (sym), and idiom (idiom).",75,76
17519,15763622,"Σ is a fi- nite alphabet, V is a set of non terminals (Σ ∩ V = ∅), P ⊆ V × (V ∪ Σ) + is a finite set of productions, S ∈ V is the start symbol.",45,46
17520,16122245,"Each author is identified by a unique color; each text is identified by an unique symbol; each point represents a 10,000-words window.",16,17
17521,305321,"In defining the shape of an Akshara, one of the consonant symbols acts as pivotal symbol (referred to as semi-full form).",16,17
17522,305321,"Depending on the context, an Akshara can have a complex shape with other consonant and vowel symbols being placed on top, below, before, after or sometimes surrounding the pivotal symbol (referred to as half-form).",33,34
17523,38155999,"This examines language at the level of tokens, which here is either a single word, emoticon, or symbol.",20,21
17524,10344351,"The shape of a token is given by a set of characters that represent its morphology: ""a"" for lower case letters, ""A"" for upper case letters, ""1"" for numbers, ""g"" for Greek letters, ""p"" for stop-words 3 , ""$"" for identifying 3-letters prefixes or suffixes or any other symbol represented by itself.",69,70
17525,10344351,"Here are some few example for the shape feature: ""Dorsal"" would be represented by ""Aa"", ""Bmp4"" by ""Aa1"", ""the"" by ""p"", ""cGKI(alpha)"" by ""aAAA(g)"", ""patterning"" by ""pat$a"" ('$' symbol separating the 3-letters prefix) and ""activity"" by ""a$vity"" ('$' symbol separating the 4letters suffix).",58,59
17526,10344351,"Here are some few example for the shape feature: ""Dorsal"" would be represented by ""Aa"", ""Bmp4"" by ""Aa1"", ""the"" by ""p"", ""cGKI(alpha)"" by ""aAAA(g)"", ""patterning"" by ""pat$a"" ('$' symbol separating the 3-letters prefix) and ""activity"" by ""a$vity"" ('$' symbol separating the 4letters suffix).",78,79
17527,10344351,"No repetition is allowed in the case of the ""a"" symbol for the lower case letters.",12,13
17528,3198964,"1987) is a tuple G = (N, T, V, P, S) where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N ; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rewriting rules A(α 1 , . . . ,",72,73
17529,3198964,"The language L(G) of some LCFRS G consists of all words w = w 1 • • • w n for which it holds that there is a rule with the start symbol on the LHS which can be instantiated to 0, n and rewritten to ε.",33,34
17530,1406849,"S n has been seen within the set of fragments sharing the same signature, and p(S i ) is the relative frequency of seeing the symbol S i in the i-th position of the signature within the same set of fragments.",26,27
17531,195742086,"We consider a simple set of rules: for hierarchical mentions we only allow transitions to lower levels in the hierarchy if the upper levels exist in the mention buffer, meaning transitions of the form TRANSITION(a > b) where the symbol > indicates that b is a lower level hierarchy of class a and is only admitted if TRANSITION(a) exists in the mention stack.",42,43
17532,1851460,The machine marks the dancer with a symbol in order to remember the data point (w t ← w t + y i x i ).,7,8
17533,1851460,Marking of dancers is done explicitly where the machine dancer attaches a round or triangular symbol to the data points: round is for Ballet and triangle is for Modern (see Figure 1 ).,15,16
17534,7511759,"The forward LSTM reads the beginning of word symbol, but not the end of word symbol, and the backward LSTM reads the end of word symbol but not the beginning of word symbol.",8,9
17535,7511759,"The forward LSTM reads the beginning of word symbol, but not the end of word symbol, and the backward LSTM reads the end of word symbol but not the beginning of word symbol.",16,17
17536,7511759,"The forward LSTM reads the beginning of word symbol, but not the end of word symbol, and the backward LSTM reads the end of word symbol but not the beginning of word symbol.",27,28
17537,7511759,"The forward LSTM reads the beginning of word symbol, but not the end of word symbol, and the backward LSTM reads the end of word symbol but not the beginning of word symbol.",34,35
17538,1843242,This script provides a simple scanner and symbol table to replace file names with standardized placeholders from the grammar.,7,8
17539,51871106,"While fastText returned rs-prefixed terms as most similar terms to the reference SNP identifier rs2243250 (which refers to the SNP Interleukin 4 -590C/T polymorphism), word2vec recalled terms 590C>T and 590C/T; the nucleotide polymorphism specified by the identifier itself (Supp 1,2-dichloromethane 1-(dimethylamino)-2-methyl-3,4-diphenylbutane-1,3-diol ZNF560 1,2-dichloroethane 8-(N,N-diethylamino)octyl-3,4,5-trimethoxybenzoate ZNF580 1,2-dichlorobenzene 1,3-dimethylamylamine ZNF545 Dibromochloromethane 8-(diethylamino)octyl ZNF582 1,2-dichloropropane 2-cyclohexyl-2-hydroxy-2-phenylacetate ZNF521 water/1,2-dichloroethane diethylamine SOX1 Table 1 : Top 5 most similar words to a selection of out-of-vocabulary terms (two chemical systematic names and a protein symbol; top row).",126,127
17540,6483979,"1987; Weir, 1988) is a tuple G = (N, T, V, P, S) where N is a finite set of non-terminals with a function dim: N → N determining the fan-out of each A ∈ N ; T and V are disjoint finite sets of terminals and variables; S ∈ N is the start symbol with dim(S) = 1; and P is a finite set of rewriting rules A(α 1 , . . . ,",69,70
17541,6483979,A derivation starts with the start symbol S instantiated to the input string w. All strings that can Kaeshammer (2013) .,6,7
17542,69323098,"In Figure 1 , transitions between states are labeled (i, o, d) where i is the input symbol, o is the output string, and d is the directional parameter.",21,22
17543,69323098,The boundary symbol ⇠ in the output plays no crucial function; it visualizes the boundary between the two copies.,2,3
17544,69323098,"In the input string, we underline the input symbol which the 2-way FST will read next.",9,10
17545,69323098,The symbol marks the empty string.,1,2
17546,69323098,"But for other tuples, the form of the arc is: input state input symbol:output string !",15,16
17547,69323098,"The meaning of the configuration (wqx, u) is that the input to T is wx and the machine is currently in state q with the read head on the first symbol of x (or has fallen off the right edge of the input tape if x = ) and that u is currently written on the output tape.",33,34
17548,11997088,One exception is that for the English WSJ corpus we did not remove the $ symbol because we found that removing it significantly decreased the accuracy of the learned grammar.,15,16
17549,14675184,"For instance, in the judgement A true a symbol A is treated as a proposition assumed to be true, whereas in a : A it is treated as a type.",9,10
17550,5383070,"The notation ⊸, the linear implication symbol of linear logic, signifies that if there is an attribute VAR (v) in the s-structure (↑ σ ) then there is also an attribute RESTR (r) in that same s-structure.",7,8
17551,17092573,indicator of token being a punctuation symbol These features were evaluated both in isolation and in combination on the trial set.,6,7
17552,69638941,When output activations are quantized to discrete values they percolate to the symbolic level which contains symbolic descriptions that are familiar from symbol-based linguistic theory.,22,23
17553,3256350,"Table 1 gives sample derivations; the I row gives the current symbol in the input read by the FST, Q the state the machine is in, and O the string written to the output.",12,13
17554,15558278,The duplication symbol ''ៗ'' is put after the word to indicate the duplication.,2,3
17555,3005862,We replace these with a <url> symbol to reduce noise in the vocabulary.,8,9
17556,3005862,"In the validation, development and test data, words that do not appear in the vocabulary are replaced with an <unk> symbol.",24,25
17557,3005862,An identity rule is also added for the <unk> symbol.,11,12
17558,16915423,"This is construed as a pause, and receives its own symbol in the scansion, even though there is no corresponding word or syllable.",11,12
17559,17083956,"represents the empty symbol for both alphabets, i.e., ∈ Σ, ∈ ∆ and (˜ : ˜ ) ∈ Γ. To simplify let ˜ be . •",3,4
17560,17083956,"A transition is labelled with an input symbol, a context condition and an attribute evaluation function.",7,8
17561,10565326,"By simply adding a hash symbol (#) before a string of letters, numerical digits or underscore signs (_), it is possible to tag a message, helping other users to find tweets that have a common topic.",5,6
17562,10565326,"Tweets with the term ""basketball"" (without the hash symbol) do not appear in a search for hashtags.",11,12
17563,11088200,"It will be convenient to assume that the automaton A has a special form: (i) it has exactly one initial state q 0 and one final state q f , (ii) there is a special ""end-of-string"" symbol $ in V , (iii) $ only appears on transitions to q f , and no other symbol can appear on such a transition, (iv) transitions labeled with $ have weight 1 or 0.",47,48
17564,11088200,"It will be convenient to assume that the automaton A has a special form: (i) it has exactly one initial state q 0 and one final state q f , (ii) there is a special ""end-of-string"" symbol $ in V , (iii) $ only appears on transitions to q f , and no other symbol can appear on such a transition, (iv) transitions labeled with $ have weight 1 or 0.",67,68
17565,11088200,"The weighted automaton A can be viewed as associating, with each symbol a ∈ U a transition matrix, that we will also call a, of dimension D × D over the non-negative reals R ∞ + , where D is the number of non-final states in A; the coordinate a ij of that matrix is equal to the weight of the transition of label a between q i and q j , this weight being null if there is no such transition.",12,13
17566,11088200,"Due to our assumptions on the symbol $, we can identify $ with a D-dimensional column vector (w 0 , w 1 , ..., w D−1 ) , where w i is equal to 1 or to 0.",6,7
17567,11088200,"On line 4, for each symbol a in U such that vector.a is not null, we add to W the pair consisting of the prefix string prefix.a and of the vector vector.a.",6,7
17568,14545725,First the symbol labeled A is realized.,2,3
17569,202618058,"The model is allowed to predict any symbol from its output vocabulary, although only two symbols are valid at any given timestep: the boundary symbol or the actual next character.",7,8
17570,202618058,"The model is allowed to predict any symbol from its output vocabulary, although only two symbols are valid at any given timestep: the boundary symbol or the actual next character.",26,27
17571,202618058,"Because our data set contains two morph categories, STM and SUF, this only increases the size of the output vocabulary from 5 (BMES + end symbol) to 10.",28,29
17572,202618058,"This is achieved by manipulating the probability of the end symbol, setting it to zero if the sequence is still too short and to one when the correct length is reached.",10,11
17573,202668949,"If the lexicon contains the symbol-pairs/arcs 1:1 7:7 ϵ:@PMATCH_BACKTRACK@ ϵ: @PMATCH_INPUT_MARK@ .:A ϵ:Ord then, since the form-side of this analysis is 17.,",5,6
17574,202668949,"will match, but since there was a backtrack-symbol, we trigger a retokenization.",10,11
17575,202668949,The input-mark symbol says where the form should be split.,4,5
17576,16025853,"On the other hand, there is also an illegalcharacter error where a hand-written symbol is not a legal Chinese character (thus not collected in a dictionary).",16,17
17577,10606105,The stressed syllable is signaled by the prosody mark before its first symbol.,12,13
17578,11647004,"We modified all the stories such that a manually introduced symbol (like 'CCC': not in the vocabulary) separates the first four sentences from the fifth sentence, and its representation is learned by training a word2vec model on the data.",10,11
17579,11647004,"On the test and validation set, the hypothesis whose representation is the closest to the sum of the vectors of the context and the connective symbol is chosen as the prediction.",26,27
17580,16945118,"n (l) dk,¬i + γ L l=1 n (l) dk,¬i + Lγ .n (k) d,¬i + α (1) where n (t) kl,¬i is the number of times term t was assigned to topic k and the viewpoint l in the corpus; n (l) dk,¬i is the number of times viewpoint l of topic k was observed in document d; and n (k) d,¬i is the number of times topic k was observed in document d. All these counts are computed excluding the current token i, which is indicated by the symbol ¬i.",107,108
17581,5312836,"Pairwise testing of values for statistically significant differences is shown with symbols: in each column, the first value marked with a particular symbol is not signifi-cantly different from any subsequent value marked with the same symbol.",24,25
17582,5312836,"Pairwise testing of values for statistically significant differences is shown with symbols: in each column, the first value marked with a particular symbol is not signifi-cantly different from any subsequent value marked with the same symbol.",39,40
17583,48374376,The symbol < s denotes lesser sonority.,1,2
17584,3042003,"The subtree determines a substructure s = σ of d and the tree-context determines a contextual structure c = χ in which the substructure is plugged to form the derived object d = c ⊙ s, where we represent the plugging operation by ⊙. In the CFG case, c is a string pair ⟨l, r⟩ and s is a string u and ⟨l, r⟩ ⊙ u = lur, which may correspond to a derivation I * ⇒ lXr * ⇒ lur where I is the initial symbol and X is a nonterminal symbol.",93,94
17585,3042003,"The subtree determines a substructure s = σ of d and the tree-context determines a contextual structure c = χ in which the substructure is plugged to form the derived object d = c ⊙ s, where we represent the plugging operation by ⊙. In the CFG case, c is a string pair ⟨l, r⟩ and s is a string u and ⟨l, r⟩ ⊙ u = lur, which may correspond to a derivation I * ⇒ lXr * ⇒ lur where I is the initial symbol and X is a nonterminal symbol.",99,100
17586,3042003,An X-derivation context is obtained by replacing an occurrence of an X-derivation tree in a complete derivation tree by a special symbol □ σ(X) .,25,26
17587,3042003,"In particular since c X for X ∈ I is the identity function □ O * , the corresponding nonterminal [[c X ]] = [[□ O * ]] is the initial symbol of G, too.",37,38
17588,3042003,Then every Σ i -grammar G can be seen as a special type of Σ 0 -grammar by adding a new initial symbol Z and rules of the form Z ← □ Σ i ⟨X⟩ for all initial symbols X of G. We have L(Σ 1 ) ∪ L(Σ 2 ) ⊆ L(Σ 0 ).,22,23
17589,3042003,"Note that C O * is the singleton of the identity function in Σ 0 , which means any element of L(G) is a 1-kernel of the new initial symbol Z. In this way, from two signatures, one can obtain a richer learnable class of languages.",32,33
17590,15822567,"Interestingly, set intersection now corresponds to element-wise vector multiplication (in this work denoted by symbol ) and the vector space equivalent of Eq.",18,19
17591,1786290,"E.g., the term Niere [spleen] gets assigned the non-terminal symbol ANATOMIE.",14,15
17592,1786290,Only non-terminal symbols used for classification are directly derived from the start symbol (S).,14,15
17593,1786290,"Currently, only the mapping of the word Prostata to the non-terminal symbol ANATOMIE can be derived from the lexicon.",14,15
17594,1786290,"The ambiguity is resolved at the top-most parsing level: The sentence is annotated as 'pathological', hence, only rewritings that include the corresponding non-terminal symbol PATH are allowed.",32,33
17595,5658536,"Protein entities are represented by BEL terms, consisting of the abundance function, the normalized entity and optionally modifications expressed as additional arguments within the abundance function: BEL statement: p(HGNC:AKT1, pmod(P, S, 21)) Entity: AKT1 Namespace: HGNC Optional modification: pmod(P,S,21) BEL Expression The used namespace denotes the approved symbol of HUGO Gene Nomenclature Committee 10 .",64,65
17596,16255230,In sorted first order logic every term has a sort and each function symbol f specifies the sorts of its arguments and the sort of its value.,13,14
17597,16255230,t n ) | t 1 = σ t 2 | Φ 1 ∨ Φ 2 | ¬Φ | ∀x : σ Φ[x] Note that in the above grammar the equality symbol = σ is subscripted with a sort σ to which it applies.,32,33
17598,16255230,In formal type systems this is done with symbol declarations.,8,9
17599,16255230,We write Σ t : σ to indicate that the symbol declarations in Σ imply that t is a well-formed expression of type σ.,10,11
17600,16255230,The sequent Σ Θ says that judgement Θ holds in context Σ. We allow a context to contain both symbol declarations and Boolean assumptions.,19,20
17601,9963840,"We take a symbol ∈ Σ, and using this we define a context as an element of Σ * Σ * , written l r. We define as l r w = lwr, and extend these to sets of strings and contexts in the usual way.",3,4
17602,9963840,"The empty context λ λ = is particularly important: of course w = w. Grammars We define CFGs standardly as a tuple Σ, V, S, P where S ∈ V is a single start symbol, V is the set of nonterminals and P is a finite subset of V × (Σ ∪ V ) * , written as N → α.",38,39
17603,9963840,"For each language L ∈ L P we define the grammar G P (L) as the tuple Σ, V, S, P where • S is a distinguished symbol, • V = {S} ∪ P(L) λ .",32,33
17604,9963840,"For every language L in L MJ , we define a grammar G MJ (L) = V ∪ {S}, S, P L ∪ P B ∪ P S where • V is the set of MSI-JSI-primes of L. • S is a distinguished symbol.",53,54
17605,1467434,The feature DIAL holds a symbol that uniquely identifies the dialogue which the corresponding utterance occurs in.,5,6
17606,2098562,C j=l The term under the summation symbol represents the selectivity of feature i for class j. It equals 1 if either all or none of the cases have value 1 for this feature.,9,10
17607,2098562,"In that case, this feature allows for no prediction of the class and the term under the summation symbol becomes 0.",19,20
17608,6614339,"Each entry is tagged as 0 (useful: total 1337), 1 (stopword: 671), s (symbol: 88), 6 (numeric: 37), 4 (punctuation: 9), and 2 or 3 for the rules below.",22,23
17609,6602395,"The collocation list has thus an entry such as: it X-make difference to Obj that-clause • cataphoric it (Subj) • antecedent = that-clause This sort of knowledge is extended to cleft sentences, adding to the collocation list an entry like: it X-be SubjC that-clause • cataphoric it (Subj) • antecedent = that-clause In order to resolve the second and third tokens of it, the entry to be accessed in the collocation list is: it X-VERB Objl Adj for Obj2 NF-clause • cataphoric it (Subj) • antecedent = NF-clause • if VERB = make and Objl = it - it (Objl) nonreferential The X-symbol means any inflected form of the verb, optionally including tense, aspect and modality.",135,136
17610,16335673,"A ""database"" of n observations is compiled for each language of interest and each successive symbol of the message stream of interest is used as the starting point for the maximal prefix to be found within the database.",17,18
17611,10163540,"Theoretically, for such a grammar, a weakly equivalent grammar using only a single nonterminal symbol exists (Franzen, 1983) .",16,17
17612,249538440,"On the other hand, the MLM pre-training objective can utilize the bidirectional context since it is based on replacing a certain portion of tokens by a special symbol [MASK] and the model is trained to recover the original tokens at these corrupted positions.",30,31
17613,250390990,"The grammar was composed by a set of recursive rules of the form in (1): (1) S → tok i S tok i S [P1] Where S denotes the start symbol, tok i a given terminal symbol sampled from the vocabulary, and P1 the probability assigned to the application of the rule.",36,37
17614,250390990,"The grammar was composed by a set of recursive rules of the form in (1): (1) S → tok i S tok i S [P1] Where S denotes the start symbol, tok i a given terminal symbol sampled from the vocabulary, and P1 the probability assigned to the application of the rule.",43,44
17615,250390990,"Rules of this form are said to be recursive since the same non-terminal symbol S appears on both sides of the formula, which enables it to be reapplied to its own output.",15,16
17616,250390990,"The rule in (1) allows for both right and central recursion, since the non-terminal symbol S is rewritten into itself both within a pair of terminal symbols and in the rightmost part of the formula.",19,20
17617,250390990,"This set of rules was complemented by the rule in (2), where the start symbol was rewritten into the empty string ε.",17,18
17618,8161857,Word-boundaries are indicated using the special symbol #.,8,9
17619,8161857,The negative tag (N) is assigned to the special symbol # used to indicate the word boundary.,11,12
17620,12809229,"A symbol sequence "" 1 , , n x x "" is denoted as "" 1 n x "".",1,2
17621,12809229,"The probability corresponding to the candidate is evaluated by applying the SCFG (Sto- chastic Context-free Grammar) model (Fujisaki, 1989) as follows: , 0 , 1 0 0 0 ( ) ( ) max ( ) max ( | ) C y C T T T A T P o P T P T P A α α → ∈ = ≈ = ∑ ∏ (5) where T stands for one possible parse tree that derive the candidate, A α → indicates a rule in the parse tree T , A stands for the left-handside symbol of the rule and α stands for the sequence of right-hand-side symbols of the rule.",107,108
17622,12809229,"In this figure, the symbol ""S"" denotes the start symbol, the symbol ""SNG"" denotes the nonterminal deriving surname characters and the symbol ""GNC"" denotes the nonterminal deriving given name characters.",5,6
17623,12809229,"In this figure, the symbol ""S"" denotes the start symbol, the symbol ""SNG"" denotes the nonterminal deriving surname characters and the symbol ""GNC"" denotes the nonterminal deriving given name characters.",12,13
17624,12809229,"In this figure, the symbol ""S"" denotes the start symbol, the symbol ""SNG"" denotes the nonterminal deriving surname characters and the symbol ""GNC"" denotes the nonterminal deriving given name characters.",15,16
17625,12809229,"In this figure, the symbol ""S"" denotes the start symbol, the symbol ""SNG"" denotes the nonterminal deriving surname characters and the symbol ""GNC"" denotes the nonterminal deriving given name characters.",27,28
17626,9624366,With an extra symbol in the alphabet H0 will rise.,3,4
17627,9624366,"However, as the space symbol will prevent ""irregular"" letter sequences between words, and thus reduce the unpredictability H~ and Ha do in fact decline.",5,6
17628,9624366,"We expect • H0 will be higher with a marker, since the alphabet size increases • H1, which takes into account the single element probabilities, will increase or decrease depending on the frequency of the new symbol. •",39,40
17629,6700158," pronunciation by clicking on one of several options, represented in the International Phonetic Alphabet (IPA) or by clicking on a speaker-symbol, so that he can hear the options spoken.",26,27
17630,143326,"One of these models can predict which symbol will following any sequence of four symbols, while the other can predict which symbol will precede any such sequence.",7,8
17631,143326,"One of these models can predict which symbol will following any sequence of four symbols, while the other can predict which symbol will precede any such sequence.",22,23
17632,143326,2A symbol refers to both words and non-words.,1,2
17633,143326,"Note that P(w]s) is the probability of word w following the symbol sequence s, according to the Markov model.",12,13
17634,16514634,In its basic form it extracts mentions matching the regular expression (S N?(\+|-)?)+ where S denotes a chemical symbol and N a natural number greater one.,19,20
17635,6890211,"The overall structure of the CF is defined as follows: (I) c(g, A, P), where C and K are the concept identifier and the key-phrase respectively, A represents a list of attribute values of the concept, and P is the concept pattern which is a sequence of several terms: variables, constants, and the symbol * which represents the key-phrase itself or one of its derivative expressions.",67,68
17636,6890211,"These codes, each of which is preceded by the symbol +, are classified into three categories: (a) constraints, (b) roles, and (c) instruction codes to be used by the CCM processing system.",10,11
17637,6890211,"Besides the symbol • which represents the key-phrase break or one of its derivatives, the pattern includes two variable terms ($1 and $2), which are called the immediate constituents of the concept breakOlO. The appended attributes to these variables impose conditions on the CFs substituted for them.",2,3
17638,6890211,"The concept pattern consists of the symbol *, for which box or boxes is to'be substituted.",6,7
17639,8226137,"ing/ plague), the same does not happen with (1): (1a) ºDois coelhos foram mortos pelo João de uma cajadada (Two rabbits were killed by John with one blow), since the meaning of the sentence becomes literal (this is shown by symbol 'º').",50,51
17640,2977235,"Hash-tags with sentiment: These features are implemented by getting all the possible substrings of the string after the symbol # and checking if any of them match with any word from S + , S − , W + and W − (4 features).",21,22
17641,14202905,"This is in contrast to strings, where a single symbol has length 1.",10,11
17642,2506008,"AJ0 NN1 AT0 CJC AT0 AJ0 AJ0 NN1 PRF AT0 CJC AT0 AJ0 NN1 PRP AT0 CJC CRD NN1 AT0 AJ0 CJC DPS NN1 AT0 CJC PRP AT0 NN1 CJC AJ0 PRF AJ0 Table 2 : Some of the sequencess in two bad clusters Mutual Information The criterion I propose is that with real constituents, there is high mutual information between the symbol occurring before the putative constituent and the symbol after -i.e.",63,64
17643,2506008,"AJ0 NN1 AT0 CJC AT0 AJ0 AJ0 NN1 PRF AT0 CJC AT0 AJ0 NN1 PRP AT0 CJC CRD NN1 AT0 AJ0 CJC DPS NN1 AT0 CJC PRP AT0 NN1 CJC AJ0 PRF AJ0 Table 2 : Some of the sequencess in two bad clusters Mutual Information The criterion I propose is that with real constituents, there is high mutual information between the symbol occurring before the putative constituent and the symbol after -i.e.",71,72
17644,2506008,"For each terminal or non-terminal symbol ¨we define four distributions, ¤ © © ¦© ! """,7,8
17645,2506008,"Two of these, © and ¦© are just the prefix and suffix probability distributions for the symbol (Stolcke, 1995) : the probabilities that the string derived from ¨begins (or ends) with a particular tag.",18,19
17646,2506008,"Clearly if ¨is a terminal symbol, the strings derived from it are all of length 1, and thus begin and end with ¨, giving © and ¦© a very simple form.",5,6
17647,2506008,The index is necessary since the same non-terminal symbol might occur more than once on the right hand side of the same rule.,10,11
17648,2506008,"So for each # , $ &% can take only those values of © ' 2 0) 3 where # is the ) th symbol on the right hand side of ' .",26,27
17649,2506008,"then the terminal symbol that appears before # will be distributed exactly according to the symbol that occurs before ¨, i.e. ¤ A© B# DC E¤ © .",3,4
17650,2506008,"then the terminal symbol that appears before # will be distributed exactly according to the symbol that occurs before ¨, i.e. ¤ A© B# DC E¤ © .",15,16
17651,2506008,"The non-terminal symbol that occurs after # will be distributed according to the symbol that occurs at the beginning of the symbol that occurs after # in the right hand side of the rule, so ! """,4,5
17652,2506008,"The non-terminal symbol that occurs after # will be distributed according to the symbol that occurs at the beginning of the symbol that occurs after # in the right hand side of the rule, so ! """,15,16
17653,2506008,"The non-terminal symbol that occurs after # will be distributed according to the symbol that occurs at the beginning of the symbol that occurs after # in the right hand side of the rule, so ! """,23,24
17654,2506008,"We could instead create a new symbol that represents , and rewrite the corpus using this abbreviation.",6,7
17655,2506008,"Since we would use it @ times, each symbol would require d gf # 2 @ nats.",9,10
17656,2506008,"Where it is not, I leave the symbol produced by the program which starts with NT-.",8,9
17657,14027861,"This is indicated by the "" ° "" symbol, which is automatically prefaced to all sentences prior to annotation.",8,9
17658,12624704,"When processing the data, any words in between a negative adverb or verb, a 'negation key' (e.g. never, not, can't) and the next end of sentence indicator, in this case, any punctuation symbol, are negated.",43,44
17659,44154136,"The ""#"" symbol is removed and the word itself is retained.",4,5
17660,4068052,"In the concatenated feature, commonly known to most of the population, each orthographic symbol represents a consonant and a vowel, except for the sixth order 2 , which is sometimes realized as a consonant without a vowel and at other times a consonant with a vowel.",15,16
17661,4068052,"This representation of concatenated speech sounds by a single symbol has been the basis for the claim made of the writing system, as syllabary.",9,10
17662,4068052,"Amharic orthography does not indicate gemination, but since there are relatively few 2 An order in Amharic writing system is a combination of a consonant with a vowel represented by a symbol.",32,33
17663,4068052,"For example, the dictionaries used the same symbol for the syllable [rI] in the word [dЗəmərInI] 'we started', whose vowel part may not be realized, and in the word [bərIzo] 'he diluted with water' that is always realized with its vowel sound.",8,9
17664,245855707,"As a reminder, we make use of the symbol to refer to any language in the set {eng, f ra, ibo, f on}.",9,10
17665,5635466,"Hypotheses generation: - Then, for each tag in this first sequehce, a second stochastic process generates a second symbol: in our case, these symbols correspond to actual words in the language.",21,22
17666,184483248,"The ""#"" symbol is removed and the word itself is retained for hashtags. •",4,5
17667,14612319,"The symbol ≈ represents equivalence between two relationships, including identicality.",1,2
17668,3200817,"The table given in figure 5 show the results: The first line in table 5 show the size of each file in terms of symbols (word, punctuation, formatting symbol, etc.),",32,33
17669,16966886,This theorem helps make clear how ISL functions are Markovian: the output for input symbol a depends on the last (k − 1) input symbols.,15,16
17670,16966886,"Following Beesley and Karttunen (2003) , the symbol '?'",9,10
17671,16966886,"For example, for final devoicing k = 2, so each state will be merged with the state that represents its final symbol.",23,24
17672,17676963,"Using dynamic programming, a Levenshtein edit distance can be calculated between each word's lineage strings in the candidate parse and the gold standard, by determining the smallest number of symbol insertions, deletions and substitutions required to transform one of the strings into the other.",32,33
17673,18603875,"The word is a number, a letter, a punctuation sign or a symbol.",14,15
17674,250390481,"Throughout this paper we refer to echo posts as reposts, not to confuse with the ((())) (echo) hate symbol (Arviv et al.,",26,27
17675,235790370,"Embodied Reasoning: Embodied agents have been proposed as a solution to the symbol grounding problem (Harnad, 1990) , or the problem of how concepts acquire real-world meaning.",13,14
17676,235790370,"Humans likely resolve symbol grounding at least partially by assigning semantics to concepts through perceptually-grounded mental simulations (Barsalou et al.,",3,4
17677,235790370,"Using embodied agents that take in perceptual data and perform actions in real or virtual environments offers an avenue for studying semantics and symbol grounding empirically (Cangelosi et al.,",23,24
17678,235790370,"2020b) help control what kind of knowledge is acquired, and better operationalize the study of symbol grounding.",17,18
17679,17041842,"Associated with each of the external nodes of the embedded tree structure are feature structures such as inflection, case information, head symbol, semantic constraints as well as a difference list for surface expressions.",23,24
17680,17041842,"If the read ahead is an illocutionary-act marker or the ending sentence symbol, and the inflection of SIT is appropriate, parsing terminates.",14,15
17681,46603543,"Although the algorithm has been developed in Prolog, we are going to expose it in a pseudocode, similar to that used in PASCAL, which uses the following notation: the symbols "","", ""/"" and ""7"" represent the conjunction, disjunction and negation of terms, respectively; the brackets ""(...)"" delimitate optional units; braces ""{... }"" are used to establish the prelation of logical operators; ""::="" is the symbol chosen for the definition and "":="" refers to the assignation of values.",91,92
17682,2310516,"In order to use the data in our experiments, we tokenized 1 the corpus, converted all words to lowercase, and collapsed all numerical symbols into one special symbol.",30,31
17683,1113227,"For example, given a parent node such as NP (noun phrase) containing the attribute 'p1' (first person), a guarded syntax rule (these are headed by the symbol <) will determine that the only possible child node is a pronoun.",35,36
17684,1113227,"However, in the default case (these are headed by the symbol >), the child may take either the form of a pronoun or a full noun phrase.",12,13
17685,7790153,"The different datafields are always separated by a # symbol, which functions as a delimiter.",9,10
17686,235097222,"Follow the practice in BERTSUMM paper, we insert a [CLS] token before each sentence and a [SEP] token after each sentence and the [CLS] is used as a symbol to aggregate features from one sentence.",35,36
17687,184482804,VCF uses the symbol : − for representing the implication relationship between statements.,3,4
17688,4890780,"There are several unique identifiers in use by the gene curation organizations, we chose to use the official symbol as a default, but it is easy to use other database identifiers as needed.",19,20
17689,4890780,"For each of these databases, the official symbol, unique identifiers, name, symbol, synonym, and alias fields were extracted.",8,9
17690,4890780,"For each of these databases, the official symbol, unique identifiers, name, symbol, synonym, and alias fields were extracted.",15,16
17691,4890780,"Symbols, synonyms, and aliases corresponding to the same official symbol were combined into a single list.",11,12
17692,4890780,"Therefore, combining terms corresponding to the same official symbol is justified, even if one database is composed of genes and the other proteins.",9,10
17693,4890780,"The final product of the four preceding steps are two dictionaries, a main dictionary and a confusion dictionary, each which map terms to the unique identifier for the gene/protein symbol corresponding to that term.",33,34
17694,4130937,"The symbol n means unification, while the symbol & following the phonology means that the phonology feature is distributively associated with each of the entries (there are as many specializations of the phonology as there are entries).",1,2
17695,4130937,"The symbol n means unification, while the symbol & following the phonology means that the phonology feature is distributively associated with each of the entries (there are as many specializations of the phonology as there are entries).",8,9
17696,4130937,"The operation which combines helping paradigms together is an extension of Cartesian product, denoted by the symbol (X).",17,18
17697,2700604,"After annotating morphological, syntactical and semantic labels, we will apply the above templates in which Z condition has one of following formats: • The i th -word to the left/right of the ambiguous word is a certain ""word form W"" or a certain symbol. •",50,51
17698,5710819,"The t.erm involving morpho-syntactic information, i.e. the probability to produce MS2 from MS1 knowing the suffixation operation S, can be directly rewritten as: p(MS~ --+ MS2]S) = ~(MS1, MSo)5(MS2, MSd) where 5 is the Kronecker symbol (6(x, y) equals 1 if the two arguments are equal and 0 otherwise).",44,45
17699,16582821,"The mapping relation between phonemes and graphemes in practical orthographies is purposely shallow, i.e. there is a faithful mapping from a unique sound to a unique symbol.",27,28
17700,16582821,A grapheme is the unit of writing that represents a particular abstract representation of a symbol employed by a writing system.,15,16
17701,16582821,"A grapheme is the basic, minimally distinctive symbol of a writing system.",8,9
17702,6134532,"Categories were formed with an informal ""diagnostic test"" of substitution of the category's theme (e.g., ""this proper name"", ""this translation"", ""this symbol"", ""this quotation"") in the place of the candidate token or tokens.",33,34
17703,12756685,"The alignment score is the sum of the substitution score between the correspondence at the aligned position, minus the sum of the gap penalty for the case that '-' symbol is inserted.",32,33
17704,12756685,"All the questions are labeled into 6 coarse grained categories and 50 fine grained categories: Abbreviations (abbreviation and expansion), Entity (animal, body, color, creation, currency, medical, event, food, instrument, language, letter, plant, product, religion, sport, substance, symbol, technique, term, vehicle, word) , Description (definition, description, manner, reason), Human (description, group, individual, title), Location (city, country, mountain, state) and Numeric (code, count, date, distance, money, order, percent, period, speed, temperature, size, weight).",57,58
17705,8032424,"Introduction The modelling of a symbol sequence requires some assumptions about the nature of the process which generated it, and the modelling of English text would, for example, commonly make the assumption that the text consists of words, short strings which usually recur and which are separated by whitespace, and punctuation symbols.",5,6
17706,8032424,"The whitespace symbol (which we shall represent explicitly by A) and its distinctive function do not seem to occur in spoken English, and would not seem to be essential in written English.",2,3
17707,8032424,"Entropic Chunking A predictive model M is a model which, when presented with a sequence of symbols s, is able to make a prediction about the next symbol in the sequence in the form of a probability distribution over the alphabet E (for the purposes of this investigation, is the set of ASCII characters).",29,30
17708,8032424,The information of a symbol w with respect to a statistical model M and a context s is defined in Equation 1.,4,5
17709,8032424,"Intuitively we may think of the information as the surprise the model experiences upon receipt of the symbol w; it is low if the model's expectations are vindicated, high if they are erroneous (Shannon and Weaver, 1949) .",17,18
17710,8032424,"The entropy is a measure of the model's uncertainty about the future; it will be low if the model expects one particular symbol to occur with a high probability, and it increases as the estimated probability distribution approaches the uniform.",24,25
17711,8032424,"The A symbol precedes the majority of sud- The -symbol occurs within hyphenated words, which were usually broken up into their constituents, while the "" symbol occurs as a chunk separator whenever two pieces of dialogue appear backto-back.",2,3
17712,8032424,"The A symbol precedes the majority of sud- The -symbol occurs within hyphenated words, which were usually broken up into their constituents, while the "" symbol occurs as a chunk separator whenever two pieces of dialogue appear backto-back.",27,28
17713,10503145,"∑ ∑ ∑ = = = )] ( [ )], ( [ , 1 1 )], ( [ )], ( [ , exp[ )], ( [ )], ( [ , exp[ | f T SM e T SM e M m m m M m m m f f T SM e T SM e f f T SM e T SM ( ) ∏ = = k i i i f T RM e T RM P f e h 1 1 )]) ( [ | )] ( [ ( log , (9) ( ) ∏ = = k i i i e T RM f T RM P f e h 1 2 )]) ( [ | )] ( [ ( log , (10) Features to model lexical transformation processes, and its inverted version, where the symbol L (RM i [T(S)]) denotes the words belonging to this sub-structure in the sentence.",163,164
17714,10503145,"When reordering at the RM level, this model just takes an RM as a symbol, and it can perform a long distance reordering job according to the knowledge of RM alignments.",15,16
17715,44165418,"Hashtags All the tokens begin with ""#"" symbol are called hashtags.",9,10
17716,44165418,"We extracted all the hashtags, removed its ""#"" symbol and built unigram features for them.",11,12
17717,4696194,"Moreover, all tokens between brackets [] and parentheses () were deleted, and 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Samples Word distance between entities Using entity tags In order to provide the neural networks with explicit cues of where an entity started and ended, we used a single symbol, represented as an XML tag <e> before and after the entity, to indicate it (Dligach et al.,",70,71
17718,8559688,The algorithm processes as follows: (1) the array of string is partitioned into three parts based on the first symbol of each string.,22,23
17719,8559688,"In order to process the split a pivot element is chosen just as in the classical quicksort giving rise to: one part with elements smaller than the pivot, one part with elements equal to the pivot and one part with elements larger than the pivot; (2) the smaller and the larger parts are recursively processed in exactly the same manner as the whole array; (3) the equal part is also sorted recursively but with partitioning starting from the second symbol of each string; (4) the process goes on recursively: each time an equal part is being processed, the considered position in each string is moved forward by one symbol.",87,88
17720,8559688,"In order to process the split a pivot element is chosen just as in the classical quicksort giving rise to: one part with elements smaller than the pivot, one part with elements equal to the pivot and one part with elements larger than the pivot; (2) the smaller and the larger parts are recursively processed in exactly the same manner as the whole array; (3) the equal part is also sorted recursively but with partitioning starting from the second symbol of each string; (4) the process goes on recursively: each time an equal part is being processed, the considered position in each string is moved forward by one symbol.",121,122
17721,44137695,"We use only words with POS 6 matching adjective, interjection, noun, symbol, verb, and other. •",14,15
17722,44141822,"The model implicitly modify a hidden variable r t to regulate the attention distribution at timestep t as M t =tanh(W y Y + (W h h t + W r r t−1 ) ⊗ e L ) α t =softmax(w T M t ) r t =Yα T t + tanh(W t r t−1 ) Here we have maintained the original symbol usage of Rocktaschel et al.,",66,67
17723,18253563,"1 ) and the two improvements, we formulated our model as: (2) Here, we use the approximation symbol, because the right hand side is not guaranteed to be normalized.",22,23
17724,17482255,"Formally, ALC is given by the following formation rules, where c denotes a concept symbol and r a role symbol (Schild, 1991) : C, D −→ c | | C D | ¬C | ∀R.C R −→ r DL SHIQ is implemented in the RACER system (Haarslev and Moller, 2003) .",16,17
17725,17482255,"Formally, ALC is given by the following formation rules, where c denotes a concept symbol and r a role symbol (Schild, 1991) : C, D −→ c | | C D | ¬C | ∀R.C R −→ r DL SHIQ is implemented in the RACER system (Haarslev and Moller, 2003) .",21,22
17726,184482670,"We used down arrow symbol ↓ to show that the value is below of our baseline, so is not valuable include it.",4,5
17727,184482670,"Furthermore, emojis were normalized using its CLDR short name, For instance, we changed for ':smiling face with smiling eyes:' cleaned (without any additional symbol).",31,32
17728,1851166,"A segment is safe, when there is a syntactic category symbol N P dominating the segment and the segment can be combined with adjacent segments under a given grammar.",11,12
17729,11328858,"Comparing the official symbol with the first synonym, it becomes clear that ""OR"" abbreviates ""Olfactory receptor"" using S&H. Comparing the synonyms, we find direct correspondences between both ""1""s and ""G"".",3,4
17730,11328858,"As the official symbol clearly is an abbreviation (single word, upper case letters, numbers) and matches the last part of the synonym, we can assume that the first part is either another synonym or a mere descriptive element that explains the real gene name.",3,4
17731,2741432,"If the learner has formed a legitimate word, the task becomes one of determining whether or not it 4 These include transitions to states on no input symbol (IN-SERTION), transitions to states on a different symbol from the next input symbol (SUBSTITUTION), and consumption of an input symbol without transition to a new state (DELETION).",28,29
17732,2741432,"If the learner has formed a legitimate word, the task becomes one of determining whether or not it 4 These include transitions to states on no input symbol (IN-SERTION), transitions to states on a different symbol from the next input symbol (SUBSTITUTION), and consumption of an input symbol without transition to a new state (DELETION).",41,42
17733,2741432,"If the learner has formed a legitimate word, the task becomes one of determining whether or not it 4 These include transitions to states on no input symbol (IN-SERTION), transitions to states on a different symbol from the next input symbol (SUBSTITUTION), and consumption of an input symbol without transition to a new state (DELETION).",46,47
17734,2741432,"If the learner has formed a legitimate word, the task becomes one of determining whether or not it 4 These include transitions to states on no input symbol (IN-SERTION), transitions to states on a different symbol from the next input symbol (SUBSTITUTION), and consumption of an input symbol without transition to a new state (DELETION).",56,57
17735,75637,"Start symbol I ° • A set of production rules P~ containing the following type of rules: -I~ ""* ~I~, where f E El _ _.",1,2
17736,75637,"Each constructed attributed subgrammar agi will have a start symbol J'T/. First, however, we define two new attributed subgrammars that have no direct relation with a subgrammar of a given M-grammar: the start subgrammar and the terminal subgrammar.",9,10
17737,75637,The terminal subgrammar agt with start symbol ~ contains a rule of the form [ ~<o>--*~ O=Z for each basic expression z of the M-grammar.,6,7
17738,75637,The start subgrammar ago with start symbol S contains a rule of the form [ S < o >~/~.° <p> o = p A cat(p) E ezportcat$(i) for the start symbol of each attributed subgrammar.,6,7
17739,75637,The start subgrammar ago with start symbol S contains a rule of the form [ S < o >~/~.° <p> o = p A cat(p) E ezportcat$(i) for the start symbol of each attributed subgrammar.,36,37
17740,75637,"F~ contains all relations defined by the M-rules of subgrammar i. s The set of production rules of a9i can be constructed as follows: -If r9i contains a rule of the form I~ --* fI~, where f corresponds with an n-ary meaningful M-rule r, agi contains the following attribute grammar rule: Ii <o >-.~I~ <pl > S<p2 >... • ..S<pn> I> (o,(P, .... ,P. )) e Rr Here, ~ and [/k are non-terminals of the attributed sugrammar agi, S is the start symbol of the complete grammar, the terminal is the name of the M-rule and Rr is the binary relation between S-trees amd tuples of S-trees which is defined by M-rule t. The terminal symbol I:> marks the end of the scope of the production rule in the strings generated by the grammar.",113,114
17741,75637,"F~ contains all relations defined by the M-rules of subgrammar i. s The set of production rules of a9i can be constructed as follows: -If r9i contains a rule of the form I~ --* fI~, where f corresponds with an n-ary meaningful M-rule r, agi contains the following attribute grammar rule: Ii <o >-.~I~ <pl > S<p2 >... • ..S<pn> I> (o,(P, .... ,P. )) e Rr Here, ~ and [/k are non-terminals of the attributed sugrammar agi, S is the start symbol of the complete grammar, the terminal is the name of the M-rule and Rr is the binary relation between S-trees amd tuples of S-trees which is defined by M-rule t. The terminal symbol I:> marks the end of the scope of the production rule in the strings generated by the grammar.",155,156
17742,75637,"S-tree pl is passed to another nonterminal of the current subgrammar, whereas p2, ..., pn are offered to the start symbol of the attribute grammar.",25,26
17743,75637,The terminal symbol O is used for this purpose.,2,3
17744,75637,The attribute of the start symbol of an attribute grammar is called the designated attribute (Engelfriet (1986) ) of the attribute grammar.,5,6
17745,75637,"Computational Aspects Because each meaningful attributed rule r produces the terminal symbol ~ and because each terminal rule x produces terminal symbol ~, the strings of £(X), the language defined by an arag X, will contain the derivational history of the string itself. :",11,12
17746,75637,"Computational Aspects Because each meaningful attributed rule r produces the terminal symbol ~ and because each terminal rule x produces terminal symbol ~, the strings of £(X), the language defined by an arag X, will contain the derivational history of the string itself. :",21,22
17747,52917394,"For IWSLT, we observe that many segments make sense statistically (frequent or rare patterns) and linguistically to some extent: Many of the frequent segments include whitespace (itself a frequent symbol); 2-gram segments amongst others include frequent word suffixes ('en', 'in', 'er'), but also frequent diphthongs ('ei' and 'ie'); 3-grams start with rare characters like 'x' and 'y' or single dashes; 4-grams combine single characters with whitespaces and double dashes; 5-grams cover numbers, in particular, years.",34,35
17748,51784165,"Hence, the RNNLM2 LM was trained over around 12.7 M words, mapping the singletons into the ""<unk>"" symbol.",23,24
17749,10988880,5) X is also the start symbol.,7,8
17750,10988880,"The expected counts can be computed using inside probabilities β ijkl (X) and outside probabilities α ijkl (X) defined as follows: β ijkl (X) = P (X ⇒ * e j i ; f l k ) (9) α ijkl (X) = P (S ⇒ * e i−1 1 , X, e M j+1 ; f k−1 1 , X, f N l+1 ) (10) In other words, the β ijkl (X) represents the probability of deriving the two parallel sequences e j i and f l k from X, while the α ijkl (X) is the probability of all derivations of the remaining parts of the sentence pair e, f , which are not spanned by X. Since there is only one non-terminal symbol X, we can omit it from the following text.",150,151
17751,10988880,Repeat the non-terminal symbol on all spanned positions.,5,6
17752,34193903,"Indeed, each sequence of symbols ends when the end-of-sequence (<eos> ) symbol is generated with this architecture, and nothing prevents the lemma generator to output the <eos> symbol before or after the factors generator.",19,20
17753,34193903,"Indeed, each sequence of symbols ends when the end-of-sequence (<eos> ) symbol is generated with this architecture, and nothing prevents the lemma generator to output the <eos> symbol before or after the factors generator.",38,39
17754,34193903,This implies that to ignore the <eos> symbol for factors (to avoid shorter factors sequence) and stop the generation of factors when the lemma sequence has ended (to avoid longer factors sequence).,9,10
17755,34193903,The first GRU cell of the decoder is fed by its previous hidden state and the feedback (i.e. the previous generated symbol) with the following formulation.,22,23
17756,247363307,This algorithm iteratively merges the most frquent pair of symbols into a single symbol.,13,14
17757,2680313,"The authors describe the syllabification task as a tagging problem, in which each phonetic symbol of a word is tagged as either a syllable boundary ('B') or as a non-syllable boundary ('N').",15,16
17758,248780458,"To do so, we turn the target name into a dummy symbol [TARGET_NAME] .",12,13
17759,232097626,"Generation and Evaluation At inference time we provide the prefix x and generate until reaching the end-of-sentence symbol, using Nucleus Sampling (Holtzman et al.,",21,22
17760,232270100,"It should be mentioned that as the hashtags encountered within a tweet can indicate a misogynistic content, we removed the hashtag symbol while retaining the hashtag words.",22,23
17761,232270100,"For the preprocessing of the text, we remove all special characters, URLs, users mentions, and hashtags to ensure that the evaluation models are not biased to any Twitterinherited symbol.",32,33
17762,11615707,"A → a, with a terminal symbol 2.",7,8
17763,11615707,"A → aB, with B a non terminal symbol and a a terminal symbol.",9,10
17764,11615707,"A → aB, with B a non terminal symbol and a a terminal symbol.",14,15
17765,11615707,"A → Ba, with B a non terminal symbol and a a terminal symbol.",9,10
17766,11615707,"A → Ba, with B a non terminal symbol and a a terminal symbol.",14,15
17767,13413603,"Each transition is a 4-tuple <c, n, i, o> where c is current state, n is the next state, i is the input symbol and o is the output.",32,33
17768,13413603,"Here, a transition arc is added depicting the rule which says that on receiving an input symbol ya at state 3, go to state 5 with an output i+a → ya.",17,18
17769,17507643,"Then, we have linearly interpolated LM ted with the baseline LM (LM base ) and, as for the automatic selection methods, we have rescored the WGs generated in the second ASR decoding pass with the ""adapted"" LM (i.e. LM base ⊕ LM ted , where symbol ⊕ denotes interpolation according to equation 1).",52,53
17770,233365144,"We have removed the following patterns: Users (any name after @ symbol), English letters, Numbers (English or Arabic), Special characters / Punctuations, replace each emoji with its Arabic name without duplicating, Leading spaces (before and after the tweet), dropping empty tweets.",13,14
17771,8861531,"Call an index-symbol finside the index-stack ix a w iindex if f is discharged into a terminal constituting a parenthesized w i in (8) (or equivalently, if f encodes a symbol of the peripheral Xl..Xr).",4,5
17772,8861531,"Call an index-symbol finside the index-stack ix a w iindex if f is discharged into a terminal constituting a parenthesized w i in (8) (or equivalently, if f encodes a symbol of the peripheral Xl..Xr).",37,38
17773,8861531,"a dia is only allowed to begin to travel down the stack or enter the stack reading mode, if a tape,symbol A on top of the stack has been deleted and stored in a special stack-reading-state qA, and the stack-reading mode has to be terminated as soon as the first indexsymbol f from above is being scanned, in which case the index-symbol concerned is deleted and an embedded stack is created, provided the transition-function gives permission.",23,24
17774,8861531,"a dia is only allowed to begin to travel down the stack or enter the stack reading mode, if a tape,symbol A on top of the stack has been deleted and stored in a special stack-reading-state qA, and the stack-reading mode has to be terminated as soon as the first indexsymbol f from above is being scanned, in which case the index-symbol concerned is deleted and an embedded stack is created, provided the transition-function gives permission.",74,75
17775,8861531,"Thus, every occurrence of an indexsymbol on the stack can only be ""consumed"" once, and only in combination with a ""matching"" non-index-symbol.",31,32
17776,8861531,"Moving through the stack in the stack reading mode, a dia is not allowed to pass or skip an index symbol.",21,22
17777,8861531,"Formally, a di-automaton is a 10-tuple D ={q, Q17 T,F, Z~,z~s,¢,#), where q is the control state for the pushdown mode, QI-={qA; A e,/""} a finite set of stack reading states, T a finite set of input symbols, /'a finite set of storage symbols, I a finite set of index symbols where Ir-d""=~, ZoeFis the initial storage symbol, $ is the top-of-stack marker on the storage tape, ¢ is the bottom-of embedded stack marker on the storage tape, # marks the bottom of the storage tape, where $,¢,# f~F~ T~I, Dir = {-1,0,1} (for ""1 step upwards"",""stay"",""l step downwards"", respectivly, E = {0,1} (""halt input tape"", ""shift input tape"", respectively), T'= Tu {#}, l'= Fu {¢}, d~is a mapping 1) in the push down mode: from {q} x T' x SFinto finite subsets of {q} x O x $1""((FuI) *) 2) in the stack reading mode: for everyA ~/"" (a)from {qA} x 7"" x 1-"" into subsets of {qA} x {0} x {1} (for walking down the stack) (b)from {q} xT' x $(A} into subsets of (qA} x (0} x {1} (for initiating the stack reading mode) (c) from {q} x T' x {,4} into subsets of {q} x {0} x (-1} (for climbing up the stack) 3) in the stack creation mode: from QFx T' x I into finite subsets of {q} x {0} x $F((l""u1) *)¢, and from QFx T' x $1 into finite subsets of {q} x {0} x $$F((F~l)*)¢ (for re-placing index symbols by new stacks, preserving the top-of-stack marker $) 4) in the stack destruction mode: from {q} x T' x {$¢} into subsets of {q} x {0}.",84,85
17778,8861531,"I'D* is the reflexive and transitive closure of ~'D.. N(D) or the language accepted by empty stack by D is defined as follows N(D)={w; weT* & (q,w#,l,$^Z0 #) (q,w#,lwl+l,$ ^#) To illustrate, the DI-automaton DI 3 accepting L 3 by empty stack is specified: DI 3 = (q (state for pda-mode), (QF =) {q~qM, qz, q$} (states for stack reading mode),('/'=) {a,b, [,]} (---input  alphabet), (G=){S,M,Z,a,b,[,],} (-- tape symbols for Ixtamode),(l=){f,g} (--tape symbols representing indices), 5,S,S,¢,#) where for every x e T: 8(q,x,$S) = {(q, O, SaSfa), (q, O,$bSgb), (q, O, CM),), (for the G3-ndes: S --~ aSfa, S ~bSgb, S ~ M) 8(q,x,$M) = ((q, O,S[MJ), (q, O, SMM), (q, O, SZ),}, (for: M-+[M], M-+MM, M---~Z) 8(q,x,$x) = {(q,1,$)} (i.e.: if input symbol x = ""predicted"" terminal symbol x, then shift input-tape one step (""1"") and delete successful prediction"" (replace Sx by $)) 8(q,x,$Z) contains {(qz, 0,$)}, (i.e.: change into stack reading mode in order to find indices belonging to the nonterminal Z) 5(qz, x,$Y ) = 5 (qz, x,Y ) contain {(qz, O,1)} (for every x T, Y ~/) (i.e.seek first index-symbol belonging to Z inside the stack) 5(qz, x, $J9 = {(qz, o, $$Za¢), (qz, O, $$a¢)), 5(qz, x, Sg) = {(qz, O, SSZb¢),(qz, 0,$$b¢)}, 5(qz, xJ) = {(q,x, $Za¢), (q,x, SAC)}, 5(qz, x,g) = {(q,x, SZb¢), (q,x, Sb¢)}, (i.e. simulate the index-rules Zf~Za, Zf~a by creation of embedded stacks) 5(q,x,S¢) = {(q, O)}, (i.e. delete empty sub-stack) 8(q,x,Y) = {(q,O,-1)} (forx ~ T, Y ~ G-~g}) (i.e. move to top of (sub-)stack).",259,260
17779,8861531,"I'D* is the reflexive and transitive closure of ~'D.. N(D) or the language accepted by empty stack by D is defined as follows N(D)={w; weT* & (q,w#,l,$^Z0 #) (q,w#,lwl+l,$ ^#) To illustrate, the DI-automaton DI 3 accepting L 3 by empty stack is specified: DI 3 = (q (state for pda-mode), (QF =) {q~qM, qz, q$} (states for stack reading mode),('/'=) {a,b, [,]} (---input  alphabet), (G=){S,M,Z,a,b,[,],} (-- tape symbols for Ixtamode),(l=){f,g} (--tape symbols representing indices), 5,S,S,¢,#) where for every x e T: 8(q,x,$S) = {(q, O, SaSfa), (q, O,$bSgb), (q, O, CM),), (for the G3-ndes: S --~ aSfa, S ~bSgb, S ~ M) 8(q,x,$M) = ((q, O,S[MJ), (q, O, SMM), (q, O, SZ),}, (for: M-+[M], M-+MM, M---~Z) 8(q,x,$x) = {(q,1,$)} (i.e.: if input symbol x = ""predicted"" terminal symbol x, then shift input-tape one step (""1"") and delete successful prediction"" (replace Sx by $)) 8(q,x,$Z) contains {(qz, 0,$)}, (i.e.: change into stack reading mode in order to find indices belonging to the nonterminal Z) 5(qz, x,$Y ) = 5 (qz, x,Y ) contain {(qz, O,1)} (for every x T, Y ~/) (i.e.seek first index-symbol belonging to Z inside the stack) 5(qz, x, $J9 = {(qz, o, $$Za¢), (qz, O, $$a¢)), 5(qz, x, Sg) = {(qz, O, SSZb¢),(qz, 0,$$b¢)}, 5(qz, xJ) = {(q,x, $Za¢), (q,x, SAC)}, 5(qz, x,g) = {(q,x, SZb¢), (q,x, Sb¢)}, (i.e. simulate the index-rules Zf~Za, Zf~a by creation of embedded stacks) 5(q,x,S¢) = {(q, O)}, (i.e. delete empty sub-stack) 8(q,x,Y) = {(q,O,-1)} (forx ~ T, Y ~ G-~g}) (i.e. move to top of (sub-)stack).",266,267
17780,8861531,"I'D* is the reflexive and transitive closure of ~'D.. N(D) or the language accepted by empty stack by D is defined as follows N(D)={w; weT* & (q,w#,l,$^Z0 #) (q,w#,lwl+l,$ ^#) To illustrate, the DI-automaton DI 3 accepting L 3 by empty stack is specified: DI 3 = (q (state for pda-mode), (QF =) {q~qM, qz, q$} (states for stack reading mode),('/'=) {a,b, [,]} (---input  alphabet), (G=){S,M,Z,a,b,[,],} (-- tape symbols for Ixtamode),(l=){f,g} (--tape symbols representing indices), 5,S,S,¢,#) where for every x e T: 8(q,x,$S) = {(q, O, SaSfa), (q, O,$bSgb), (q, O, CM),), (for the G3-ndes: S --~ aSfa, S ~bSgb, S ~ M) 8(q,x,$M) = ((q, O,S[MJ), (q, O, SMM), (q, O, SZ),}, (for: M-+[M], M-+MM, M---~Z) 8(q,x,$x) = {(q,1,$)} (i.e.: if input symbol x = ""predicted"" terminal symbol x, then shift input-tape one step (""1"") and delete successful prediction"" (replace Sx by $)) 8(q,x,$Z) contains {(qz, 0,$)}, (i.e.: change into stack reading mode in order to find indices belonging to the nonterminal Z) 5(qz, x,$Y ) = 5 (qz, x,Y ) contain {(qz, O,1)} (for every x T, Y ~/) (i.e.seek first index-symbol belonging to Z inside the stack) 5(qz, x, $J9 = {(qz, o, $$Za¢), (qz, O, $$a¢)), 5(qz, x, Sg) = {(qz, O, SSZb¢),(qz, 0,$$b¢)}, 5(qz, xJ) = {(q,x, $Za¢), (q,x, SAC)}, 5(qz, x,g) = {(q,x, SZb¢), (q,x, Sb¢)}, (i.e. simulate the index-rules Zf~Za, Zf~a by creation of embedded stacks) 5(q,x,S¢) = {(q, O)}, (i.e. delete empty sub-stack) 8(q,x,Y) = {(q,O,-1)} (forx ~ T, Y ~ G-~g}) (i.e. move to top of (sub-)stack).",360,361
17781,8861531,"II.(""iP) If L is accepted by a DI-automaton D=(q, QFZF d,L,Z6$,¢,#), then we can assume without loss of generality a) that D writes at most two symbols on a stack in eifiler the push down mode or the stack creation mode (it follows from the Di-automaton definition that the first one of the two symbols cannot be a index symbol from I), b) that T and F are disjunct.",76,77
17782,8861531,"Thus, if the cell Z i ; of the CKY-table contains an entry beginning with ~l<A,fl, (B,f2,q,p),..>"", then we know that Att=*=>ai_.a j with tt--fltt 1 eF* is valid, and further that the top index symbol f2 on Bl(i.e.",58,59
17783,8861531,"That is why a DI-entry for a node K in a CKY-ceU requires an additional pointer to the entry for a descendant C, which contains the end-of-stack symbol of K and which eventually has to be supplemented by an intersemital continuation pointer.",36,37
17784,8861531,"E.g. the entry (14) < Bl,f2,(D,f3,p,q),(C,ft,r,s) > in Z i,j indicates that the next symbol f3 below f2 on the index stack belonging to B 1 can be found in cell ZO q in the entry for the nonterminal D; the second ~l{~druple (C,f t r,s) points to the descendant C of Blcarrying the last ~ndex ft of Bland containing a place where a continuation pointer to a neighbouring path can be added or has already been added.",31,32
17785,17299215,"A token is a set of characters that can be classified into one of these categories: word, punctuation, number, contraction, possessive, symbol without taking into account any additional context.",27,28
17786,10080250,The natural language query is separated from the restrictions by the : | : symbol.,14,15
17787,17630430,"here is similar to standard DRT, because a new subordinated DRS affixed with a negation symbol is introduced in case of negation.",16,17
17788,16784668,"5 Note that we require a punctuation symbol between both arguments to prevent the template from extracting, e.g., coordinated NPs such as chairman and chief executive. •",7,8
17789,222133356,"Then, the message m is generated symbol by symbol: the current sequence is fed to the LSTM cell that outputs a new hidden state.",7,8
17790,222133356,"Then, the message m is generated symbol by symbol: the current sequence is fed to the LSTM cell that outputs a new hidden state.",9,10
17791,222133356,"During the training phase, the next symbol is sampled from this distribution.",7,8
17792,222133356,"During the testing phase, the next symbol is deterministically selected by taking the argmax of the distribution.",7,8
17793,222133356,"As shown in Figure 1 , Impatient Listener consists of a modified Standard Listener that, instead of guessing i after consuming the entire message m = (m 0 , ..., m t ), makes a prediction îk for each symbol m k .",43,44
17794,222133356,We consider a symbol to be informative if replacing it randomly has an effect on Listener's prediction.,3,4
17795,222133356,"To evaluate the information contained in the symbol at position k, m k , we substitute it randomly by drawing another symbol r k uniformly from the vocabulary (except the EOS token).",7,8
17796,222133356,"To evaluate the information contained in the symbol at position k, m k , we substitute it randomly by drawing another symbol r k uniformly from the vocabulary (except the EOS token).",22,23
17797,222133356,"Then, we feed this new message m = (m 1 , ..., r k , ..., m t ) into Listener that outputs õm,k (index m indicates that the original message was m, index k indicates that the k th symbol of the original message has been replaced).",48,49
17798,222133356,"We define Λ m,k a boolean score that evaluates whether the symbol replaced at position k has an impact on the prediction, such that Λ k,m = 1(õ m,k = i).",13,14
17799,222133356,"If Λ m,k = 1, the k th symbol of message m is considered as informative.",11,12
17800,222133356,"We assign a score Λ .,k for each position k that counts the proportion of informative symbols over all the messages of a language: Λ .,k = 1 N (k) m∈M Λ m,k , (7) where N (k) is the number of messages that have a symbol (different from EOS) at position k. Effective length L ef f : measures the mean number of informative symbols by message: L ef f = 1 N m∈M l(m)−1 k=1 Λ m,k . (",55,56
17801,222133356,"On the other hand, Impatient Listener aims to succeed at the game as soon as possible, by predicting Speaker's input at each message's symbol.",27,28
17802,186671,"Moreover, parsing also starts from the rules that build the final symbol in the grammar.",12,13
17803,186671,The chart is searched for complete edges with the final symbol of the grammar (e.g. SBAR) as their category.,10,11
17804,219708219,"Together, these provide embedding for a symbol s at position i given by f (f b (s), pos(i)), often taken to be simply f b (s) + pos(i).",7,8
17805,219708219,Vector s ∈ Q m denotes one-hot encoding of a symbol s ∈ Σ. RNNs We follow Siegelmann and Sontag (1992) in our definition of RNNs.,12,13
17806,219708219,"After the last symbol s n has been fed, we continue to feed the RNN with the terminal symbol f b ($) until it halts.",3,4
17807,219708219,"After the last symbol s n has been fed, we continue to feed the RNN with the terminal symbol f b ($) until it halts.",19,20
17808,219708219,"Now, using the last but one coordinate in y t representing the time t + 1, the attention mechanism Att(p t , K e , V e ) can retrieve the embedding of the t-th input symbol x t .",40,41
17809,219708219,"At this point, O dec (•) has at its disposal the hidden state h t (coming from y t via p t and the residual connection) and the input symbol x t (coming via the attention mechanism and the residual connection).",34,35
17810,219708219,"We maintain in vector ω t ∈ Q m , with a coordinate each for symbols in Σ, the fraction of times the symbol has occurred up to step t. Now, at a step t ≤ n, for the difference ω t − ω t−1 (which is part of the query vector), it can be shown easily that only the coordinate corresponding to s t is positive.",24,25
17811,219708219,"In the decoder-encoder attention block, we give equal attention to all the t + 1 values, which along with O enc (•), leads to z (1) t = [h t−1 , 0 d h , 0 d b , δ t , 1 2 t+1 , 0 m , 0 m , ω t ], where essentially δ t = σ(ω t − ω t−1 ), except with a change for the last coordinate due to special status of the last symbol $ in the processing of RNN.",93,94
17812,219708219,"At this point, O dec (•) has at its disposal the hidden state h t (coming from z (1) t via p (2) t and the residual connection) and the input symbol x t (coming via the attention mechanism and the residual connection).",40,41
17813,219708219,We found that the model with absolute positional encodings during training overfits on the fact that the 13th token is always the terminal symbol.,23,24
17814,219708219,The input embedding is obtained by summing the symbol and positional encodings which we next describe.,8,9
17815,219708219,"We will use the symbol encoding f symb : Σ → Q d which is essentially the same as f b except that the dimension is now larger: f symb (s) = [0 d h , f e (s); 0 d h , 0, 0].",4,5
17816,219708219,"s n in the first n + 1 steps followed by the embedding of the symbol $ for the subsequent steps, which is in accordance with the requirements of (Siegelmann and Sontag, 1992).",15,16
17817,219708219,"Thus the encoder-encoder attention in ( 5 ) is redefined as where Z (0) = X. Similarly the decoder-encoder attention in ( 7 ) is redefined by where in a t denotes the layer and we use v ( ,b) to denote any intermediate vector being used in -th layer and b-th block in cases where the same symbol is used in multiple blocks in the same layer.",68,69
17818,219708219,"2017) in the attention mechanism in our construction, For the computation of the Transformer, we also use a vector sequence in Q |Σ| defined by where φ t,k denotes the number of times the k-th symbol β k in Σ has appeared till the t-th step.",42,43
17819,219708219,"Note that ω t,0 = 1 t+1 since the first coordinate corresponds to the proportion of the start symbol # which appears only once at t = 0.",18,19
17820,219708219,"Similarly, ω t,|Σ| = 0 for 0 ≤ t < n and ω t,|Σ| = 1/(t + 1) for t ≥ n, since the end symbol $ doesn't appear till the end of the input and it appears only once at t = n. We define two more sequences of vectors in Q |Σ| for 0 ≤ t ≤ n: Here ∆ t denotes the difference in the proportion of symbols between the t-th and (t − 1)-th steps, with the applicatin of sigmoid activation.",28,29
17821,219708219,The last coordinate in ω t indicates the proportion of the terminal symbol $ and hence the last value in ∆ t denotes the change in proportion of $.,12,13
17822,219708219,We set the last coordinate in δ t to an exponentially decreasing sequence so that after n steps we always have a nonzero score for the terminal symbol and it is taken as input in the underlying RNN.,27,28
17823,219708219,"This leads to, In words, the change in the proportion of a symbol is positive from step t − 1 to t if and only if it is the input symbol at the t-th step.",14,15
17824,219708219,"This leads to, In words, the change in the proportion of a symbol is positive from step t − 1 to t if and only if it is the input symbol at the t-th step.",32,33
17825,6019059,"We first preprocess the text in the following ways: (1) map rare words (with counts less than T ) to the symbol ""UNKNOWN""; (2) map words in a name dictionary to the symbol ""NAME.""",25,26
17826,6019059,"We first preprocess the text in the following ways: (1) map rare words (with counts less than T ) to the symbol ""UNKNOWN""; (2) map words in a name dictionary to the symbol ""NAME.""",41,42
17827,6019059,"Similarly, all the examples of caller's identities are collected, and a structure is induced for them To further simplify the task, we replaced number strings by the single symbol ""NUMBER+"", and person-names by the symbol ""PERSON-NAME"".",32,33
17828,6019059,"Similarly, all the examples of caller's identities are collected, and a structure is induced for them To further simplify the task, we replaced number strings by the single symbol ""NUMBER+"", and person-names by the symbol ""PERSON-NAME"".",43,44
17829,6291852,"Let G be any non-terminal symbol of the grammar; n(G) the number of productions rewriting G and P(ilG ) the probability that the ith of these productions takes place, then (10) P(iIG ) = n(G) It is assumed that for all i --1, 2 .... , n(G), P(iIG ) is a positive number and that ~iP(ilG) --1.",7,8
17830,6291852,"9Although not in the grammar, this symbol is used to make it possible to describe the possibility of a word being of a certain category in terms of (5).",7,8
17831,14713623,"7 Within the phrasal type system of (Ginzburg and Sag, 2000) root-cl constitutes the 'start' symbol of the grammar.",22,23
17832,535600,The corpus was pre-processed so that every word appearing less than three times was replaced by a special UNK symbol.,21,22
17833,1692958,"Thus, we could for example have a permutation modality rap with the following proof rule (in addition to [rapL] and [OpR'] as before): r[oeA, B] ~ C r[8, opA] ~-C The symbol ~ here indicates that the inference is valid in both directions.",43,44
17834,201590,"1 ; the start symbol is B, and we want the root to have index 1.",4,5
17835,201590,"ϕ n }, using the grammar G. The input ""sentence"" of the parsing problem we construct is the sequence {start} ∪ S, where start is a special start symbol.",34,35
17836,201590,"The start symbol, start, gets a special lexicon entry: Its labels entry is the empty set (i.e. it must be the root of the tree), and its valency entry is the set {subst S,k,1 }, where k is the semantic index with which generation should start.",2,3
17837,201590,"Then we construct all partitions of the input semantics in which each block in the partition is covered by a lexical entry, and build a parsing problem in which each block is one symbol in the input to the parser.",34,35
17838,222133301,"For Biaffine, the correlations are much weaker ranging from 0.2 (AUX) to 0.6 (CCONJ, coordinating conjunction, and SYM, symbol) for those which are statistically significant.",25,26
17839,15632730,"Tree Substitution Grammars A tree substitution grammar (TSG) is a 4-tuple V n , V t , S, T where V n is the set of nonterminals; V t is the set of of terminals; S ∈ V n is the start symbol; and T is the set of elementary trees, having root and internal nodes in V n and leaf nodes in V n ∪V t .",49,50
17840,15632730,"When the tree resulting from a series of substitution operations is a complete parse tree, i.e. the root is the start symbol and all leaf nodes are terminals, we define the sequence of the elementary trees used as a complete derivation.",22,23
17841,196177225,"In particular, the suffixation of derivational and inflectional morphemes in Yupik is conditioned by morphophonological rules that apply at each morpheme boundary and obscure them, rendering a surface form that may be unrecognizable from the glossed form, as in ( 7 ): (7) kaanneghituq kaate--nghite--u--q arrive--did.not--INTR.IND--3SG 'he/she did not arrive' (Jacobson, 2001, p.43) Moreover, each morphophonological rule has been assigned an arbitrary symbol in the Yupik literature (Jacobson, 2001) , and so, every derivational and inflectional suffix can be written with all of the rules associated with it, as in (8).",86,87
17842,196177225,"Since minimal pairs do exist to differentiate the phonological conditions under which each symbol applies (see ( 9 )), inclusion of the symbols may in fact assist the system in learning the morphophonological changes that are induced by certain suffixes.",13,14
17843,196195753,A considerable weakness in these NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single < unk > symbol that represents every possible out-of-vocabulary (OOV) word.,35,36
17844,196195753,This OOV is then crudely considered the same as an < unk > symbol.,13,14
17845,196195753,"In VIN, the < unk > symbol corresponds to a low training reward, whereas the low training reward corresponds to a low value.",7,8
17846,196195753,"The reason for this is that the individual Mongolian-case is not obviously 'helpful' to the production of < unk > symbol in Mongolian, so the screener is insensitive to it. •",24,25
17847,2983348,"Total number of productions/rules to recognize Myanmar syllable structure is ""111"" and we found that the director symbol sets (which is also known as first and follow sets) for same nonterminal symbols with different productions are disjoint.",21,22
17848,2983348,"This is the property of LL(1) grammar which means for each non terminal that appears on the left side of more than one production, the directory symbol sets of all the productions in which it appears on the left side are disjoint.",28,29
17849,2983348,"The first L means reading from Left to right, the second L means using Leftmost derivations, and the ""1"" means with one symbol of lookahead. (",26,27
17850,2983348,"However, because of these contracted forms, single lookahead symbol in our proposed LL(1) grammar does not refer exactly to one character and it may be a combination of two or more characters in parsing Myanmar syllable.",10,11
17851,15861242,Twitter users can interact between them by using the @ symbol followed by the username they want to mention.,10,11
17852,13890168,"c k ], plus a sentinel symbol $ marking either the left or the right boundary of the word, depending on the direction of the model.",7,8
17853,6781837,"Each EP has a predicate symbol which, in the case of lexical predicates, encodes information about lemma, part-of-speech, and sense distinctions.",5,6
17854,6781837,"Two event relations are the same or similar if they share the same predicate symbol, or if their predicate symbols contain the same lemma and part-of-speech.",14,15
17855,196173439,"For prefix concatenation, the backtick symbol (`) was used, on the other hand, for suffix concatenation, the hyphen symbol (-) was used.",6,7
17856,196173439,"For prefix concatenation, the backtick symbol (`) was used, on the other hand, for suffix concatenation, the hyphen symbol (-) was used.",24,25
17857,196173439,"深刻さ (Seriousness) 3 Implementation The Dictionaries The dictionaries needed for this work were a list of non-inflectional words and various lists of inflectional word stems: The Non-Inflectional Word Dictionary (NIWD) was formed by unifying the IPADIC files into a single list; omitting verb, adjectival verb, adjectival noun and symbol files.",60,61
17858,196173439,Return the obtained edges in order while adding a separation symbol between them such as a backtick (') or hyphen (-) for affixes or a white space for other words.,10,11
17859,2474014,"The required tag-pair dictionary will therefore require only an entry for each cover-symbol pair (together with a list of exceptions, where the tag rather than the cover symbol is diagnostic of the appropriate T-tags).",16,17
17860,2474014,"The required tag-pair dictionary will therefore require only an entry for each cover-symbol pair (together with a list of exceptions, where the tag rather than the cover symbol is diagnostic of the appropriate T-tags).",33,34
17861,1436872,S denotes a start symbol of the formal grammar (in the sense of a term-rewriting system): {S−→ ProsodicFeatures SegmentalFeatures; ProsodicFeatures −→ Pitch Intensity Jitter Shimer; SegmentalFeatures −→ Energy Formants; Pitch −→ Min Max Quantile Mean Std MeanAb-soluteSlope; etc. },4,5
17862,549022,"Let also t i symbol-ize term i used to index the documents in the collection, with i = 1, .., n. The VSM assumes that for each term t i there exists a vector t i in the vector space that represents it.",4,5
17863,226283838,"Consider the question: ""What was 1 Instead of numbering examples, we identify them with a unique semantic symbol in brackets.",20,21
17864,1412909,"A copy of t~s list has a special symbol substituted for the relevant element, thus: i1 1 ll ,ll 1 181] and this symbol is inserted into the main list.",10,11
17865,1412909,"A copy of t~s list has a special symbol substituted for the relevant element, thus: i1 1 ll ,ll 1 181] and this symbol is inserted into the main list.",29,30
17866,1412909,There is a straight forward substitution of the special symbol in the list seen above for the phoneme that occupies the same position in the phonetic representation that has just had syllabic stress assigned to it.,9,10
17867,1412909,"The result of all this is a list ve~j similar to the original phonetic rendition of the English text, but with a special symbol substituted at the point that has been chosen to have stress assigned to it.",26,27
17868,1412909,"For this process, the list with the special symbol and the list representing the general intonation trend for the required sentence are both searched down recursively; if the symbol is found at the head of the phonetic list, the relevant stress peak value (an fO value obtained from recorded speech) is inserted in its place in a third list.",9,10
17869,1412909,"For this process, the list with the special symbol and the list representing the general intonation trend for the required sentence are both searched down recursively; if the symbol is found at the head of the phonetic list, the relevant stress peak value (an fO value obtained from recorded speech) is inserted in its place in a third list.",30,31
17870,1412909,"To assign duration the following algorithm was adopted: Search down the phonetic list after stress peak assignment, doing: (1) If the head of the list is the special symbol, increase the standard duration of the element by one. (",33,34
17871,2484382,"Finally, I follow [Bierwisch, 1989] in using the symbol We(a) for the 'norm' expected for amount a in context C. This reflects the usual assumption that the positive expresses a relation to a context-dependent standard.",12,13
17872,2484382,"The symbol '+' is + in the unmarked case and -in the marked case, and 'x' stands for scalar multiplication.",1,2
17873,233365157,"Bear in mind that a 6-dot Braille notation is also feasible, provided that we introduce compound characters, i.e., characters made of a prefix followed by the main symbol, a practice that is current in many other Braille notations.",32,33
17874,233365157,"To achieve this goal, we will construct a brief presentation of each Braille symbol, consisting of its Braille representation in 6 and 8-dot Braille, the explanation of its meaning and some examples of its use in semantic formulae.",14,15
17875,241583464,"enumerating expressions (Arabic numbers, Roman numbers and consecutive letters) and the section symbol ( §) with keywords that are typically part of US legislation references (statute, code, section, etc.).",15,16
17876,241583556,"Given a set of entities E and relations R, a knowledge graph embedding (KGE) model embeds the elements of E ∪ R as elements in R k for a fixed positive integer k. For brevity, we abuse notation and will represent the elements and the embeddings of the elements in E and R with the same symbol.",60,61
17877,248780157,"Introduction In the age of multimedia information technology, massive network data is a symbol of people's freedom of speech,and these messages contain a lot of positive or negative sentiments.",14,15
17878,15246702,We use the → symbol to express that this mapping is a default.,4,5
17879,237532582,"For the LSTM we penalize the generation of the unknown-word symbol, reducing its output score by a factor of 10 16 .",12,13
17880,235097312,"The condition with the best performance is ToneSep, where the tone symbol is kept separate (HIGH, FALL, RISE), the low tone is left out as a default, and the nasal feature remains connected to the vowel symbol (i.e.: A versus A-NAS).",12,13
17881,235097312,"The condition with the best performance is ToneSep, where the tone symbol is kept separate (HIGH, FALL, RISE), the low tone is left out as a default, and the nasal feature remains connected to the vowel symbol (i.e.: A versus A-NAS).",43,44
17882,235097312,"With ToneSep, the tone symbols are surrounded by the vowel the tone belongs to and the following consonant or vowel (or at the nasal symbol).",26,27
17883,235097312,"First, providing an explicit symbol for the low tone might force DeepSpeech to look for more words in the transcription.",5,6
17884,235097312,"The explicit symbol for the low tone appears to force the CTC algorithm to keep looking for tones, and therefore words, whereas, in the other conditions, the CTC algorithm gives up on the search sooner.",2,3
17885,236772808,"The annotator inserts metadata by clicking the '+' symbol on the left side of each counterfactual hypothesis (below label drop-down), as shown in Figure 3b .",10,11
17886,2033596,"The function f maps each word w ∈ s to a symbol based on its frequency in C as follows: f (w) =    S if w is a seed word H otherwise if w is a frequent word X otherwise For example, the sentence: A and B are usually mediated by an overproduced C. might be mapped to the sequence (S, H, X, H, H, X, H, H, X, X), which we will write as SHXHHXHHXX for brevity.",11,12
17887,2276635,"At each transition the FST may output the same symbol on both streams, a symbol on the left stream only, or a symbol on the right stream only.",9,10
17888,2276635,"At each transition the FST may output the same symbol on both streams, a symbol on the left stream only, or a symbol on the right stream only.",15,16
17889,2276635,"At each transition the FST may output the same symbol on both streams, a symbol on the left stream only, or a symbol on the right stream only.",24,25
17890,2276635,The central idea is to use a generative model to extract finite-dimensional features from a symbol sequence.,17,18
17891,220057395,H M eSH = [H GCN : e M eSH ] (7) where the symbol : means the concatenated operation; e M eSH is the word embedding of MeSH terms.,17,18
17892,23330239,The symbol λ stands for abstraction and • stands for application 4 .,1,2
17893,23330239,Definitions of types i : type symbol denoting the type of individuals p : type symbol denoting the type of propositions e : type symbol denoting the type of events ent: type symbol denoting the type of natural numbers inst : type symbol denoting the type of instants inter : type symbol denoting the type of extended intervals dur: type symbol denoting the type of durations Type symbols may be omitted when no ambiguity is introduced.,6,7
17894,23330239,Definitions of types i : type symbol denoting the type of individuals p : type symbol denoting the type of propositions e : type symbol denoting the type of events ent: type symbol denoting the type of natural numbers inst : type symbol denoting the type of instants inter : type symbol denoting the type of extended intervals dur: type symbol denoting the type of durations Type symbols may be omitted when no ambiguity is introduced.,15,16
17895,23330239,Definitions of types i : type symbol denoting the type of individuals p : type symbol denoting the type of propositions e : type symbol denoting the type of events ent: type symbol denoting the type of natural numbers inst : type symbol denoting the type of instants inter : type symbol denoting the type of extended intervals dur: type symbol denoting the type of durations Type symbols may be omitted when no ambiguity is introduced.,24,25
17896,23330239,Definitions of types i : type symbol denoting the type of individuals p : type symbol denoting the type of propositions e : type symbol denoting the type of events ent: type symbol denoting the type of natural numbers inst : type symbol denoting the type of instants inter : type symbol denoting the type of extended intervals dur: type symbol denoting the type of durations Type symbols may be omitted when no ambiguity is introduced.,33,34
17897,23330239,Definitions of types i : type symbol denoting the type of individuals p : type symbol denoting the type of propositions e : type symbol denoting the type of events ent: type symbol denoting the type of natural numbers inst : type symbol denoting the type of instants inter : type symbol denoting the type of extended intervals dur: type symbol denoting the type of durations Type symbols may be omitted when no ambiguity is introduced.,43,44
17898,23330239,Definitions of types i : type symbol denoting the type of individuals p : type symbol denoting the type of propositions e : type symbol denoting the type of events ent: type symbol denoting the type of natural numbers inst : type symbol denoting the type of instants inter : type symbol denoting the type of extended intervals dur: type symbol denoting the type of durations Type symbols may be omitted when no ambiguity is introduced.,52,53
17899,23330239,Definitions of types i : type symbol denoting the type of individuals p : type symbol denoting the type of propositions e : type symbol denoting the type of events ent: type symbol denoting the type of natural numbers inst : type symbol denoting the type of instants inter : type symbol denoting the type of extended intervals dur: type symbol denoting the type of durations Type symbols may be omitted when no ambiguity is introduced.,62,63
17900,226283813,NOUN(infection) The symbol (W)?,3,4
17901,13426678,"The ITG is a synchronous PCFG, consisting of five types of rules: j i j i e c e c AA AA A / | / | / | | ] [ ε ε > < ⎯→ ⎯ (1) Where A is the non-terminal symbol, [] and <> represent the two operations which generate outputs in straight and inverted orientation respectively.",50,51
17902,8851964,Caption generation by iterative word estimation A caption is generated by estimating the words one-byone repeating Steps 2 and 3 until either the length of the sentence exceeds the predefined maximum or the terminal symbol of a sentence is output.,36,37
17903,16610454,"Specifically, each document, x i , is transformed into a 'POS-translated' version, x i , such that every word or punctuation symbol from the original document is replaced with its respective POS or punctuation token in the translated version.",28,29
17904,228375878,"due to the fact that abbreviations, which can appear in a legitimate sentence, typically include a period symbol.",19,20
17905,226221766,symbol is appended to the end.,0,1
17906,6234837,The symbol * denotes zero or more matches.,1,2
17907,6234837,The symbol ?,1,2
17908,248780511,"A PDF document actually contains I) a list of fonts, and for each there is a) a mapping between a CID (Characterd IDentifier) and the symbol as a 2D bitmap (glyph), and b) a mapping from CID to Unicode value; II) a list of physical lines, which are themselves made of an ordered list of tuples (page, font, character, bounding box) describing where to to draw each symbols in the 2D coordinates of each pages, as given by the bounding box of that symbol.",30,31
17909,248780511,"A PDF document actually contains I) a list of fonts, and for each there is a) a mapping between a CID (Characterd IDentifier) and the symbol as a 2D bitmap (glyph), and b) a mapping from CID to Unicode value; II) a list of physical lines, which are themselves made of an ordered list of tuples (page, font, character, bounding box) describing where to to draw each symbols in the 2D coordinates of each pages, as given by the bounding box of that symbol.",101,102
17910,248780511,"As such all the realisation of a symbol will be linked to the exact same CID, and the task of recovering the document is equivalent to the simpler task of recovering the Unicode translation table.",7,8
17911,248780511,In Figure 1 we report an example of a corrupted font encoding for Nenets taken from the UDHR corpus: The letters in gray are the Unicode letters associated to the symbol below them.,31,32
17912,233365109,The n-grams located at the beginning or at the end of a comment were distinguished from the others by the presence of a specific symbol.,26,27
17913,233365115,"For MLM, the objective is to predict one or more randomly chosen characters in the input sequence, which are replaced with a special masking symbol before the embedding stage.",26,27
17914,233365115,"Characters that are not in this vocab are replaced with a special symbol reserved for unknown characters, before the encoding stage.",12,13
17915,14220869,"First, users can replace one query term with a placeholder symbol ' * ' (wildcard, henceforth), which will return the ten most frequent expansions of the wildcard in the corpus for the specified year range.",11,12
