,corpus_id,sentence,start_idx,end_idx
0,14382934,"We then expanded the coverage to less well known human diseases, zoonotic diseases, animal diseases and diseases caused by toxic substances such as sarin, hydrogen sulfide, sulfur dioxide and ethylene.",21,22
1,247958012,"2021 ), using large language models for data augmentation presents several challenges: they exhibit social biases and are prone to generating toxic content.",23,24
2,248780022,"2021) detect toxic language in a user utterance, they stop producing output (Xu et al.,",3,4
3,15424937,"After spawning, the fish die in their thousands -delivering their toxic cargo to the lake sediment and increasing its PCB content by more than sevenfold when the density of returning salmon is high.' (",11,12
4,645466,"The identification of fertile translations is useful because (i) they frequentlty correspond to non-canonical translations, e.g. paraphrastic variants and (ii) they tend to correspond to vulgarized forms of technical terms (e.g. « cytotoxic » vs. « toxic to the cells ») which are useful when the translator translates lay science texts.",44,45
5,645466,"For example, toxic can be translated to toxique 'toxic', toxicité 'toxicity' or vénéneux 'poisonous'.",3,4
6,645466,"For example, toxic can be translated to toxique 'toxic', toxicité 'toxicity' or vénéneux 'poisonous'.",10,11
7,645466,"For example, if the system generates the target words {toxique, cellule} 'toxic, cell' from cytotoxic, it will match ""toxique pour les cellules"" 'toxic to the cells'.",16,17
8,645466,"For example, if the system generates the target words {toxique, cellule} 'toxic, cell' from cytotoxic, it will match ""toxique pour les cellules"" 'toxic to the cells'.",33,34
9,645466,"For example, toxique pour les cellules and toxique pour la cellule 'toxic to the cell' correspond to the same translation.",13,14
10,645466,The term cytotoxic will be translated into German as zytotoxisch whereas in French it can be translated as cytotoxique or toxique pour les cellules 'toxic to the cells'.,25,26
11,13983070,"Comp en conf = {-cyto-} ; Comp en f ree = {cytotoxic, cytotoxicity, toxic} ; Comp f r conf = {-cyto-} ; Comp f r f ree = {cellule, toxique} ; T rans = {{-cyto-→ -cyto-, cellule}, {toxic → toxique}} ; V ar en = {cytoxic → cytoxicity} ; Stop f r = {pour, le} ; Corpus f r = ""le/DET cytotoxicité/N Decomposition function The decomposition function D works in two steps D 1 and D 2 .",18,19
12,13983070,"Comp en conf = {-cyto-} ; Comp en f ree = {cytotoxic, cytotoxicity, toxic} ; Comp f r conf = {-cyto-} ; Comp f r f ree = {cellule, toxique} ; T rans = {{-cyto-→ -cyto-, cellule}, {toxic → toxique}} ; V ar en = {cytoxic → cytoxicity} ; Stop f r = {pour, le} ; Corpus f r = ""le/DET cytotoxicité/N Decomposition function The decomposition function D works in two steps D 1 and D 2 .",54,55
13,13983070,"S(R(T (D 2 (D 1 (""cytotoxic""'))))) = S(R(T (D 2 ({cyto, toxic})))) Step 2 of decomposition (D 2 ) gives out all possible decompositions of the single-word term by enumerating the different concatenations of its minimal components.",26,27
14,13983070,"S(R(T (D 2 ({cyto, toxic})))) = S(R(T ({cyto, toxic}, {cytotoxic}))) The concatenation of the minimal components into bigger components increases the chances of finding translations.",8,9
15,13983070,"S(R(T (D 2 ({cyto, toxic})))) = S(R(T ({cyto, toxic}, {cytotoxic}))) The concatenation of the minimal components into bigger components increases the chances of finding translations.",20,21
16,13983070,"For example, consider the single-word term non-cytotoxic and a dictionary having translations for non, cyto and cytotoxic but no translation for toxic.",27,28
17,13983070,"If we stick to the sole output of D 1 {non-,-cyto-,toxic}, the translation of noncytotoxic will fail because there is no translation for toxic.",26,27
18,13983070,"= S({cytotoxique}, {toxiquecyto}, {cellule, toxique}, {celluletoxique}, {toxique, cellule}, {toxiquecellule}, {cytotoxicité}) = ""cytotoxicité/N"", ""toxique/A pour/PREP le/DET cellule/N"" 'cytotoxicity', 'toxic to the cells' In other words, L is a subsequence of the lemmas of T and we allow at maximum L1 closed-class words between two tokens which match the lemmas of L. For a given sequence of lexical items L, we collect from the target corpus all sequences of tokens T 1 , T 2 , ...T p which match L according to our abovementioned definition.",58,59
19,12082660,"In this paper, we address the analysis point of view in an experiment we made in the processing of a corpus consisting of a collection of texts from the Agency for Toxic Substances and Disease Registry (ATSDR) describing different toxic products 1 .",42,43
20,12082660,"In these texts, multiple ways of describing toxic products are present (see 2.1 below), which makes this text collection particularly interesting for the task of paraphrase detection.",8,9
21,12082660,"The analysis phase can thus be seen as a paraphrase detection phase, as it unifies in a same representation different ways of expressing similar information about toxic products.",27,28
22,12082660,Corpus Analysis and Expected Output Corpus study The corpus on which we work consists of a collection of texts presenting toxic products from ATSDR that are meant to be read by general public.,20,21
23,12082660,We have concentrated on the first paragraphs containing in average between 6-7 sentences and consisting in the general presentation of a toxic product.,23,24
24,12082660,"They give information about the name, the appearance (colour, smell), some physical properties and possible synonyms of a toxic product.",23,24
25,12082660,"We focus on the different ways of expressing the information relative to the appearance, physical properties, synonyms, use and origin of toxic products.",24,25
26,12082660,This predicate is the result of the normalization of strings expressing the physical form of the toxic product.,16,17
27,12082660,This predicate is the result of the normalization of strings describing the colour of the toxic product.,15,16
28,12082660,This predicate is the result of the normalization of strings describing the smell of toxic product.,14,15
29,12082660,"This predicate expresses that the second argument is a synonym of the first, which is the name of the toxic product.",20,21
30,12082660,The PROPERTY predicate is the result of the normalization of strings expressing physical or chemical properties of the toxic product.,18,19
31,12082660,"For instance, we want to obtain the same normalized predicate for the two utterances ProductX is a colorless, nonflammable liquid and ProductX is a liquid that has no colour and that does not burn easily namely: The input to our paraphrase detection system is the whole paragraph that describes the toxic product.",53,54
32,12082660,"Since toxic products can have names like 2,3-Benzofuran, which the general tokenizer does not consider as one unique token, we add a local grammar layer dedicated to the detection of these kinds of names.",1,2
33,12082660,"In our corpus, the pronoun it and the possessive its always refer to the toxic product that is described in the text.",15,16
34,12082660,"In the present example, since we have detected that aniline is the described toxic product (SUBSTANCE(aniline)), since an ISAJ relation exists between soluble and dissolve (ISAJ(soluble,dissolve)) and finally since the deep syntactic analysis of the sentence has given to us the dependency ATTRIB(aniline,soluble), the final predicate is created.",14,15
35,12082660,"Example of output When applied on an input text describing a toxic substance, such as the following one : Acetone is a manufactured chemical that is also found naturally in the environment.",11,12
36,12082660,"Performance of the whole system for information extraction In order to evaluate the results of the information extraction system, we apply the full chain of information extraction on an unseen collection of 30 texts describing toxic substances.",36,37
37,226283669,"We consider all posts abusive which are marked as either ""toxic"" or ""severely toxic"", following Wiegand et al. (",11,12
38,226283669,"We consider all posts abusive which are marked as either ""toxic"" or ""severely toxic"", following Wiegand et al. (",16,17
39,225062782,"2 Related work Corpus Some previous works have discussed how to identify the offensive language, but in that literature, the offensive language is ranging from aggression to cyberbullying, toxic comments, and hate speech.",31,32
40,225062782,"The dataset in this competition was extracted from the comments of Wikipedia, and it was formed in six categories: toxicant, severe toxic,identity hate, threat, insult,obscene .",24,25
41,245855939,"This error can happen because toxicity is introduced in the translation when it is not in the source, deleted in the translation when it was in the source, or mistranslated into different (toxic or not) words, or not translated at all (i.e. the toxicity remains in the source language or it is transliterated). •",35,36
42,245855939,"In all 6 https://www.kaggle.com/c/ jigsaw-toxic-comment-classification-challenge/ data 7 https://meta.wikimedia.org/wiki/ Research:Detox/Data_Release#Wikipedia_ Comments_Corpus cases, the XLM-RoBERTa transformer was used for the encoding (predictor) part of the architecture, using xlm-roberta-base for all experiments.",6,7
43,219176615,"Apart from a growing volume of press articles concerning toxicity online, 1 there is increased research interest on detecting abusive and other unwelcome comments labeled 'toxic' by moderators, both for English and other languages.",27,28
44,219176615,"In the top example, the target comment (the one being annotated) was labeled as toxic only when context was given.",17,18
45,219176615,"In the bottom example, the target comment was considered toxic only without its parent comment. •",10,11
46,219176615,"Here we use the term 'toxic' as an umbrella term, but we note that the literature uses several terms for different kinds of toxic language or related phenomena: 'offensive' (Zampieri et al.,",6,7
47,219176615,"Here we use the term 'toxic' as an umbrella term, but we note that the literature uses several terms for different kinds of toxic language or related phenomena: 'offensive' (Zampieri et al.,",26,27
48,219176615,Table 3 lists all currently available public datasets for the various forms of toxic language that we are aware of.,13,14
49,219176615,"Mikolov and Zweig (2012) , for example, used LDA to encode the preceding sentences and pass the en-  For each comment and group of annotators, the toxicity scores of the annotators were averaged and rounded to the nearest binary decision (toxic, non-toxic) to compute the number of toxic comments (#toxic).",46,47
50,219176615,"Mikolov and Zweig (2012) , for example, used LDA to encode the preceding sentences and pass the en-  For each comment and group of annotators, the toxicity scores of the annotators were averaged and rounded to the nearest binary decision (toxic, non-toxic) to compute the number of toxic comments (#toxic).",50,51
51,219176615,"Mikolov and Zweig (2012) , for example, used LDA to encode the preceding sentences and pass the en-  For each comment and group of annotators, the toxicity scores of the annotators were averaged and rounded to the nearest binary decision (toxic, non-toxic) to compute the number of toxic comments (#toxic).",57,58
52,219176615,"Mikolov and Zweig (2012) , for example, used LDA to encode the preceding sentences and pass the en-  For each comment and group of annotators, the toxicity scores of the annotators were averaged and rounded to the nearest binary decision (toxic, non-toxic) to compute the number of toxic comments (#toxic).",61,62
53,219176615,"5 We addressed this problem by asking the annotators an extra question: ""Was the parent comment less, more, or equally toxic?""",24,25
54,219176615,"Figure 1 shows that the toxicity ratio (toxic comments over total) of CAT-SMALL is higher when annotators are given context (GC), compared to when no context is provided (GN).",8,9
55,219176615,"We fix the bias term of the single output neuron to log T N , where T and N are the numbers of toxic and non-toxic training comments, respectively, to counter-bias against the majority (non-toxic) class.",23,24
56,219176615,"We fix the bias term of the single output neuron to log T N , where T and N are the numbers of toxic and non-toxic training comments, respectively, to counter-bias against the majority (non-toxic) class.",27,28
57,219176615,"We fix the bias term of the single output neuron to log T N , where T and N are the numbers of toxic and non-toxic training comments, respectively, to counter-bias against the majority (non-toxic) class.",43,44
58,219176615,"We report ROC AUC, because both datasets are heavily unbalanced, with toxic comments being rare (Fig.",13,14
59,219176615,We collected and share two datasets for investigating our research questions around the effect of context on the annotation of toxic comments (RQ1) and its detection by automated systems (RQ2).,20,21
60,184482657,"The literature contains many terms for different kinds of offensive language: toxic, abusive, hateful, attacking, etc.",12,13
61,184482657,"However, other researchers have created different taxonomies based on sub-kinds of toxic language (Table 2 ).",14,15
62,236460230,The Toxic Spans Detection task of SemEval-2021 required participants to predict the spans of toxic posts that were responsible for the toxic label of the posts.,14,15
63,236460230,The Toxic Spans Detection task of SemEval-2021 required participants to predict the spans of toxic posts that were responsible for the toxic label of the posts.,21,22
64,236460230,"The task could be addressed as supervised sequence labeling, using training data with gold toxic spans provided by the organisers.",15,16
65,236460230,"It could also be treated as rationale extraction, using classifiers trained on potentially larger external datasets of posts manually annotated as toxic or not, without toxic span annotations.",22,23
66,236460230,"It could also be treated as rationale extraction, using classifiers trained on potentially larger external datasets of posts manually annotated as toxic or not, without toxic span annotations.",27,28
67,236460230,"For the supervised sequence labeling approach and evaluation purposes, posts previously labeled as toxic were crowd-annotated for toxic spans.",14,15
68,236460230,"For the supervised sequence labeling approach and evaluation purposes, posts previously labeled as toxic were crowd-annotated for toxic spans.",20,21
69,236460230,"Introduction Discussions online often host toxic posts, meaning posts that are rude, disrespectful, or unreasonable; and which can make users want to leave the conversation (Borkan et al.,",5,6
70,236460230,"Current toxicity detection systems classify whole posts as toxic or not (Schmidt and Wiegand, 2017; Pavlopoulos et al.,",8,9
71,236460230,"2019) , often to assist human moderators, who may be required to review only posts classified as toxic, when reviewing all posts is infeasible.",19,20
72,236460230,"In such cases, human moderators could be assisted even more by automatically highlighting spans of the posts that made the system classify the posts as toxic.",26,27
73,236460230,"As a first step along this direction, Task 5 of SemEval 2021 provided the participants with posts previously rated to be toxic, and required them to identify toxic spans, i.e., spans that were responsible for the toxicity of the posts, when identifying such spans was possible.",22,23
74,236460230,"As a first step along this direction, Task 5 of SemEval 2021 provided the participants with posts previously rated to be toxic, and required them to identify toxic spans, i.e., spans that were responsible for the toxicity of the posts, when identifying such spans was possible.",29,30
75,236460230,Note that a post may include no toxic span and still be marked as toxic.,7,8
76,236460230,Note that a post may include no toxic span and still be marked as toxic.,14,15
77,236460230,"On the other hand, a non toxic post may comprise spans that are considered toxic in other toxic posts.",7,8
78,236460230,"On the other hand, a non toxic post may comprise spans that are considered toxic in other toxic posts.",15,16
79,236460230,"On the other hand, a non toxic post may comprise spans that are considered toxic in other toxic posts.",18,19
80,236460230,"We provided a dataset of English posts with gold annotations of toxic spans, and evaluated participating systems on a held-out test subset using character-based F1.",11,12
81,236460230,"The task could be addressed as supervised sequence labeling, training on the provided posts with gold toxic spans.",17,18
82,236460230,"2016) , using classifiers trained on larger external datasets of posts manually annotated as toxic or not, without toxic span annotations.",15,16
83,236460230,"2016) , using classifiers trained on larger external datasets of posts manually annotated as toxic or not, without toxic span annotations.",20,21
84,236460230,Civil Comments contains about 30k comments marked as toxic by a majority of at least three crowd raters.,8,9
85,236460230,"We undertook an effort to reannotate this subset of comments at the span level, using the following instructions: For this task you will be viewing comments that a majority of annotators have already judged as toxic.",37,38
86,236460230,"Extract the toxic word sequences (spans) of the comment below, by highlighting each such span and then clicking the right button.",2,3
87,236460230,"If the comment is not toxic or if the whole comment should have been annotated, check the appropriate box and do not highlight any span.",5,6
88,236460230,"That is, we computed the mean pairwise Kappa per post, by using character offsets as instances being classified in two classes, toxic and non-toxic.",24,25
89,236460230,"That is, we computed the mean pairwise Kappa per post, by using character offsets as instances being classified in two classes, toxic and non-toxic.",28,29
90,236460230,"Each span is assigned a binary (toxic, nontoxic) label, based on whether the respective rater found the span to be insulting, threatening, identitybased attack, profane/obscene, or otherwise toxic.",7,8
91,236460230,"Each span is assigned a binary (toxic, nontoxic) label, based on whether the respective rater found the span to be insulting, threatening, identitybased attack, profane/obscene, or otherwise toxic.",37,38
92,236460230,"If the span was annotated with any of those types, the span is considered toxic according to the rater, otherwise not.",15,16
93,236460230,"For each post, we extracted the character offsets of each toxic span of each rater.",11,12
94,236460230,"In each post, the ground truth considers a character offset as toxic if the majority of the raters included it in their toxic spans, otherwise the ground truth of the character offset is non-toxic.",12,13
95,236460230,"In each post, the ground truth considers a character offset as toxic if the majority of the raters included it in their toxic spans, otherwise the ground truth of the character offset is non-toxic.",23,24
96,236460230,"In each post, the ground truth considers a character offset as toxic if the majority of the raters included it in their toxic spans, otherwise the ground truth of the character offset is non-toxic.",37,38
97,236460230,A toxic span (Table 1 ) in the ground truth of a post is a maximal sequence of contiguous toxic character-offsets.,1,2
98,236460230,A toxic span (Table 1 ) in the ground truth of a post is a maximal sequence of contiguous toxic character-offsets.,20,21
99,236460230,Most of the toxic spans in the training set are single-word terms.,3,4
100,236460230,"Task description The objective of this task is the detection of the spans that make a post toxic, when detecting such spans is possible.",17,18
101,236460230,"Systems had to extract a list of toxic spans, or an empty list, per post.",7,8
102,236460230,A toxic span was defined to be a sequence of words that attribute to the post's toxicity.,1,2
103,236460230,Table 2 : Examples of toxic test posts and their ground truth toxic spans (shown in red).,5,6
104,236460230,Table 2 : Examples of toxic test posts and their ground truth toxic spans (shown in red).,12,13
105,236460230,The left column shows the character offsets of the toxic spans.,9,10
106,236460230,"The top three posts have no toxic spans, the next three have one each, while the remaining three posts have two toxic spans each.",6,7
107,236460230,"The top three posts have no toxic spans, the next three have one each, while the remaining three posts have two toxic spans each.",23,24
108,236460230,"In both approaches, word-level BIO tags were used, i.e., words were labelled as B (beginning word of a toxic span), I (inside word of a toxic span), or O (outside of any toxic span).",24,25
109,236460230,"In both approaches, word-level BIO tags were used, i.e., words were labelled as B (beginning word of a toxic span), I (inside word of a toxic span), or O (outside of any toxic span).",34,35
110,236460230,"In both approaches, word-level BIO tags were used, i.e., words were labelled as B (beginning word of a toxic span), I (inside word of a toxic span), or O (outside of any toxic span).",44,45
111,236460230,"Roughly speaking, in this case BERT produces probabilities indicating how likely it is for each token to be the beginning or end of a toxic span.",25,26
112,236460230,"2019) , selects the best combinations of candidate begin and end tokens, aiming to output the most likely set of toxic spans per post.",22,23
113,236460230,"That is, if any two systems considered a character to be part of a toxic span, then the ensemble classified the character as toxic, otherwise the ensemble classified it as non-toxic.",15,16
114,236460230,"That is, if any two systems considered a character to be part of a toxic span, then the ensemble classified the character as toxic, otherwise the ensemble classified it as non-toxic.",25,26
115,236460230,"That is, if any two systems considered a character to be part of a toxic span, then the ensemble classified the character as toxic, otherwise the ensemble classified it as non-toxic.",35,36
116,236460230,"The latter is first fine-tuned to classify posts as toxic or nontoxic, using three Kaggle toxicity datasets.",11,12
117,236460230,"6 For toxic span detection, RoBERTa's subword representations from three different layers (1, 6, 12) are summed to produce the corresponding word embeddings.",2,3
118,236460230,"A binary classifier on top of RoBERTa, operating on the word embeddings, predicts whether a word belongs to a toxic span or not.",21,22
119,236460230,"The second component of the ensemble used the RoBERTa model as a teacher to produce silver toxic spans for 30,000 unlabelled toxic posts (Borkan et al.,",16,17
120,236460230,"The second component of the ensemble used the RoBERTa model as a teacher to produce silver toxic spans for 30,000 unlabelled toxic posts (Borkan et al.,",21,22
121,236460230,RoBERTa was then retrained as a student on the augmented dataset (30k posts with silver labels and the training posts provided by the organisers) to predict toxic offsets.,28,29
122,236460230,The ensemble returns the intersection of the toxic spans identified by the two components.,7,8
123,236460230,Rationales Some participants experimented with training toxicity classifiers on external datasets containing posts labeled as toxic or non-toxic; and then employing model-specific or model-agnostic rationale extraction mechanisms to produce toxic spans as explanations of the decisions of the classifier.,15,16
124,236460230,Rationales Some participants experimented with training toxicity classifiers on external datasets containing posts labeled as toxic or non-toxic; and then employing model-specific or model-agnostic rationale extraction mechanisms to produce toxic spans as explanations of the decisions of the classifier.,19,20
125,236460230,Rationales Some participants experimented with training toxicity classifiers on external datasets containing posts labeled as toxic or non-toxic; and then employing model-specific or model-agnostic rationale extraction mechanisms to produce toxic spans as explanations of the decisions of the classifier.,36,37
126,236460230,The model-specific rationale mechanism of Rusert (2021) used the attention scores of an LSTM toxicity classifier to detect the toxic spans.,23,24
127,236460230,"All the above mentioned approaches used a threshold to turn the explanation scores (e.g., attention or LIME scores) of the words into binary decisions (toxic/non-toxic words).",28,29
128,236460230,"All the above mentioned approaches used a threshold to turn the explanation scores (e.g., attention or LIME scores) of the words into binary decisions (toxic/non-toxic words).",32,33
129,236460230,"2020) and it was simply employed as a list of toxic words for lookup operations (Palomino et al.,",11,12
130,236460230,"Second, the lexicon was compiled using the set of tokens labeled as toxic in our span-annotated training set and it was used as a lookup table (Burtenshaw and Kestemont, 2021) , possibly also storing the frequency of each lexicon token in the training set (Zhu et al.,",13,14
131,236460230,"Third, the least supervised lexicons were built with statistical analysis on the occurrences of tokens in a training set solely annotated at the comment level (toxic/nontoxic post) (Rusert, 2021 ).",27,28
132,236460230,"If a false prediction was located near a ground truth toxic span, then it would contribute less to the overall loss for that post, compared to one located further away.",10,11
133,236460230,"2021) , or to filter posts (Luu and Nguyen, 2021) that were not labeled as toxic by a toxicity classifier.",19,20
134,236460230,"It is based on a RoBERTa model, fine-tuned to predict if a post is toxic or not (Section 4.2) and further fine-tuned to predict toxic spans by using a CRF layer on top.",17,18
135,236460230,"It is based on a RoBERTa model, fine-tuned to predict if a post is toxic or not (Section 4.2) and further fine-tuned to predict toxic spans by using a CRF layer on top.",31,32
136,236460230,"2021) , which extracts likely toxic words from the training data and simply tags them during inference.",6,7
137,236460230,The lexicon comprises words that appear frequently inside ground truth toxic spans and not outside.,10,11
138,236460230,"BENCHMARK III is a random baseline, which assigns a random label (toxic/non-toxic) per character offset (50% chance of being toxic).",13,14
139,236460230,"BENCHMARK III is a random baseline, which assigns a random label (toxic/non-toxic) per character offset (50% chance of being toxic).",17,18
140,236460230,"BENCHMARK III is a random baseline, which assigns a random label (toxic/non-toxic) per character offset (50% chance of being toxic).",28,29
141,236460230,"The system that was ranked second (S-NLP) also employed an ensemble, using a RoBERTa model initially fine-tuned to classify posts as toxic or nontoxic as the starting point (Nguyen et al.,",28,29
142,236460230,"The ensemble combined (i) the resulting RoBERTa model, now fine-tuned to predict toxic spans, with additional FLAIR and FastText embeddings, and (ii) a RoBERTa model retrained as a student to predict toxic spans (Section 4.2).",17,18
143,236460230,"The ensemble combined (i) the resulting RoBERTa model, now fine-tuned to predict toxic spans, with additional FLAIR and FastText embeddings, and (ii) a RoBERTa model retrained as a student to predict toxic spans (Section 4.2).",40,41
144,236460230,"The binary toxic post classifiers that were used were LSTM, Logistic Regression (LR), Support Vector Machines (SVM), and BERT.",2,3
145,236460230,"The best rationale-based method employed a BERT model, fine-tuned for toxic post classification, and SHAP.",15,16
146,236460230,Error analysis A common theme across many competitor reports was the serious challenge posed by comments with no toxic spans.,18,19
147,236460230,"But the majority of the short spans comprises common cuss or clearly abusive words, which can be directly classified as toxic (Ghosh and Kumar, 2021) ; by contrast, the infrequent longer spans are rather context dependent and more challenging to detect.",21,22
148,236460230,"Toxicity, when expressed with subtle language, can appear through non-local text features: some comments are toxic without showing any obvious toxic span in them.",20,21
149,236460230,"Toxicity, when expressed with subtle language, can appear through non-local text features: some comments are toxic without showing any obvious toxic span in them.",25,26
150,236460230,Type Description INCONSISTENCIES Not all the occurrences of the same toxic span are annotated in the same post.,10,11
151,236460230,FALSE POSITIVES Non-toxic words labelled.,4,5
152,236460230,"2021) , Hoang and Nguyen (2021), Ding and Jurgens (2021) , and Ghosh and Kumar (2021) , where an additional effort was made to examine their model's ability to correctly tag words in toxic and non-toxic contexts.",42,43
153,236460230,"2021) , Hoang and Nguyen (2021), Ding and Jurgens (2021) , and Ghosh and Kumar (2021) , where an additional effort was made to examine their model's ability to correctly tag words in toxic and non-toxic contexts.",46,47
154,236460230,"Interestingly Sans and Farràs (2021) also noted in their analysis that racial and ethnic terms are labeled in biased ways that reflect patterns not only in the training toxic spans, but also in external data used to pre-train underlying Transformer models.",30,31
155,236460230,"Conclusions We provided 10,629 posts that were annotated for toxic spans and we defined the task of toxic span detection.",9,10
156,236460230,"Conclusions We provided 10,629 posts that were annotated for toxic spans and we defined the task of toxic span detection.",17,18
157,236460230,"Long toxic spans were more likely contextdependent and less frequent in the dataset compared to single-word spans, which made their detection a challenge.",1,2
158,236460230,"The winners included in their ensemble an approach that performed better on long spans, but we note that the problem of detecting long uncommon toxic spans is far from solved.",25,26
159,236460230,"Future competitions could also require participants to both classify posts as toxic or not, and detect toxic spans only when posts are classified as toxic, instead of providing the participants only with posts already classified as toxic.",11,12
160,236460230,"Future competitions could also require participants to both classify posts as toxic or not, and detect toxic spans only when posts are classified as toxic, instead of providing the participants only with posts already classified as toxic.",17,18
161,236460230,"Future competitions could also require participants to both classify posts as toxic or not, and detect toxic spans only when posts are classified as toxic, instead of providing the participants only with posts already classified as toxic.",25,26
162,236460230,"Future competitions could also require participants to both classify posts as toxic or not, and detect toxic spans only when posts are classified as toxic, instead of providing the participants only with posts already classified as toxic.",38,39
163,236460230,"Finally, future competitions could require participants to distinguish toxic posts of different kinds (e.g., insult, threat, profanity, along with supporting spans), which are sometimes easier to define compared to the more general umbrella toxicity term we (and others) have used.",9,10
164,236486094,"These posts are called toxic (Borkan et al.,",4,5
165,236486094,"For instance, the post ""Keep the hell out"" may be considered as toxic by a moderator, if the previous (parent) post ""What was the title of that 'hell out' movie?""",15,16
166,236486094,"To obtain a single toxicity score per post, we calculated the percentage of the annotators who found the post to be insulting, profane, identity-attack, hateful, or toxic in another way (i.e., all toxicity sub-types provided by the annotators were collapsed to a single toxicity label).",33,34
167,236486094,"2017) , who also found that training using the empirical distribution (over annotators) of the toxic labels (a continuous score per post) leads to better toxicity detection performance, compared to using labels reflecting the majority opinion of the raters (a binary label per post).",18,19
168,236486094,"2020) , where we observed that when the parent post is provided, the majority of the annotators perceive fewer posts as toxic, compared to showing no context to the annotators.",23,24
169,236486094,"For each post p, we define s ic (p) to be the toxicity (fraction of coders who perceived the post as toxic) derived from the IC coders and s oc (p) to be the toxicity derived from the OC coders.",25,26
170,236486094,A positive δ means that raters who were not given the parent post perceived the target post as toxic more often than raters who were given the parent post.,18,19
171,236486094,"This means that clearly context sensitive posts (e.g., in an edge case, ones that all OC coders found as toxic while all IC coders found as non toxic) are rare.",22,23
172,236486094,"This means that clearly context sensitive posts (e.g., in an edge case, ones that all OC coders found as toxic while all IC coders found as non toxic) are rare.",30,31
173,236486094,"Experimental Study Initially, we used our dataset to experiment with existing toxicity detection systems, aiming to investigate if context-sensitive posts are more difficult to automatically classify correctly as toxic or nontoxic.",32,33
174,236486094,Toxicity Detection We employed the Perspective API toxicity detection system to classify CCC posts as toxic or not.,15,16
175,236486094,"Experimenting with artificial parent posts (long or short, toxic or not) confirmed that the error increases for context-sensitive posts.",10,11
176,236486094,"2019) , this work uses toxicity as an umbrella term for hateful, identity-attack, insulting, profane or posts that are toxic in another way.",25,26
177,248780005,"We study the task of toxic spans detection, which concerns the detection of the spans that make a text toxic, when detecting such spans is possible.",5,6
178,248780005,"We study the task of toxic spans detection, which concerns the detection of the spans that make a text toxic, when detecting such spans is possible.",20,21
179,248780005,"Moreover, methods that add generic rationale extraction mechanisms on top of classifiers trained to predict if a post is toxic or not are also surprisingly promising.",20,21
180,248780005,"Finally, we use TOXICSPANS and systems trained on it, to provide further analysis of state-of-the-art toxic to non-toxic transfer systems, as well as of human performance on that latter task.",23,24
181,248780005,"Finally, we use TOXICSPANS and systems trained on it, to provide further analysis of state-of-the-art toxic to non-toxic transfer systems, as well as of human performance on that latter task.",27,28
182,248780005,"Introduction In social media and online fora, toxic content can be defined as rude, disrespectful, or unreasonable posts that would make users want to leave the conversation (Borkan et al.,",8,9
183,248780005,"2019) exist, most of them classify whole posts, without identifying the specific spans that make a text toxic.",20,21
184,248780005,"But highlighting such toxic spans can assist human moderators (e.g., news portal moderators) who often deal with lengthy comments, and who prefer attribution instead of a system-generated unexplained toxicity score per post.",3,4
185,248780005,Locating toxic spans within a text is thus a major step towards successful semi-automated moderation and healthier online discussions.,1,2
186,248780005,"To promote research on this new task, we release the first dataset of English posts with annotations of toxic spans, called TOXICSPANS.",19,20
187,248780005,1 We discuss how it was created and propose an evaluation framework for toxic spans detection.,13,14
188,248780005,"We consider methods that (i) perform sequence labeling (tag words) or (ii) rely on an attentional binary classifier to predict if a post is toxic or not, then invoke its attention at inference time to obtain toxic spans as in rationale extraction.",30,31
189,248780005,"We consider methods that (i) perform sequence labeling (tag words) or (ii) rely on an attentional binary classifier to predict if a post is toxic or not, then invoke its attention at inference time to obtain toxic spans as in rationale extraction.",43,44
190,248780005,"The latter approach allows leveraging larger existing training datasets, which provide gold labels indicating which posts are toxic or not, without providing gold toxic span annotations.",18,19
191,248780005,"The latter approach allows leveraging larger existing training datasets, which provide gold labels indicating which posts are toxic or not, without providing gold toxic span annotations.",25,26
192,248780005,"We then study some characteristics of supervised and self-supervised toxic-to-civil transfer models (Laugier et al.,",11,12
193,248780005,"2021) by comparing them on several datasets, including a recently released parallel toxic-to-civil dataset (Dementieva et al.,",14,15
194,248780005,"Lastly, by applying toxic span detection systems, we assess the performance of human crowdworkers on the toxic-to-civil task.",4,5
195,248780005,"Lastly, by applying toxic span detection systems, we assess the performance of human crowdworkers on the toxic-to-civil task.",18,19
196,248780005,"2019) are typically trained on datasets annotated at the post level (a text is annotated as toxic or not) (Wulczyn et al.,",18,19
197,248780005,Table 1 : Examples of toxic posts and their ground truth toxic spans (also shown in bold red).,5,6
198,248780005,Table 1 : Examples of toxic posts and their ground truth toxic spans (also shown in bold red).,11,12
199,248780005,"In the left column, toxic spans are shown as sets of character offsets.",5,6
200,248780005,No toxic spans are included in the ground truth of the last post.,1,2
201,248780005,"in that we detect toxic spans, instead of assigning toxicity labels to entire texts.",4,5
202,248780005,"2020) , but specifically for toxic posts, a task that has never been considered in general toxicity detection before.",6,7
203,248780005,"Suggesting civil rephrases of posts found to be toxic (Nogueira dos Santos et al.,",8,9
204,248780005,"We show how toxic spans detection can contribute in the assessment of toxic-to-civil transfer, linking the two tasks together for the first time.",3,4
205,248780005,"We show how toxic spans detection can contribute in the assessment of toxic-to-civil transfer, linking the two tasks together for the first time.",12,13
206,248780005,"We followed the toxicity definition that was used in Civil Comments, i.e., we use 'toxic' as an umbrella term that covers abusive language phenomena, such as insults, hate speech, identity attack, or profanity.",17,18
207,248780005,"For the purposes of our experiments, we collapsed all the subtypes into a single toxic class, and we did not study them further; but the subtypes are included in the new dataset we release.",15,16
208,248780005,"Annotation From the original Civil Comments dataset (1.2M posts), we retained only posts that had been found toxic by at least half of the crowdraters.",21,22
209,248780005,30k toxic posts.,1,2
210,248780005,We selected a random 11k subset of the 30k posts for toxic spans annotation.,11,12
211,248780005,The raters were asked to mark the toxic word sequences (spans) of each post by highlighting each toxic span on their screen.,7,8
212,248780005,The raters were asked to mark the toxic word sequences (spans) of each post by highlighting each toxic span on their screen.,19,20
213,248780005,"If the raters believed a post was not actually toxic, or that the entire post would have to be annotated, they were instructed to select appropriate tick-boxes in the interface, without highlighting any span.",9,10
214,248780005,"Hence, when no toxic spans are provided (for a particular post by a particular rater), it is clear if the rater thought that the post was not actually toxic, or that the entire post would have to be annotated.",4,5
215,248780005,"Hence, when no toxic spans are provided (for a particular post by a particular rater), it is clear if the rater thought that the post was not actually toxic, or that the entire post would have to be annotated.",32,33
216,248780005,It is not possible to annotate toxic spans for every toxic post.,6,7
217,248780005,It is not possible to annotate toxic spans for every toxic post.,10,11
218,248780005,"For example, in some posts the core message being conveyed may be inherently toxic (e.g., a sarcastic post indirectly claiming that people of a particular origin are inferior) and, hence, it may be difficult to attribute the toxicity of those posts to particular spans.",14,15
219,248780005,"In such cases, the posts may end up having no toxic span annotations, according to the guidelines given to the annotators; see the last post of Table 1 for an example.",11,12
220,248780005,"In other cases, however, it is easier to identify particular spans (possibly multiple per post) that make a post toxic, and these toxic spans often cover only a small part of the post (see Table 1 for examples).",23,24
221,248780005,"In other cases, however, it is easier to identify particular spans (possibly multiple per post) that make a post toxic, and these toxic spans often cover only a small part of the post (see Table 1 for examples).",27,28
222,248780005,"We calculated the mean pairwise (for a pair of annotators) Cohen's kappa per post, using character offsets as instances being classified as toxic (included in a toxic span) or non-toxic; we then averaged over the posts.",26,27
223,248780005,"We calculated the mean pairwise (for a pair of annotators) Cohen's kappa per post, using character offsets as instances being classified as toxic (included in a toxic span) or non-toxic; we then averaged over the posts.",31,32
224,248780005,"We calculated the mean pairwise (for a pair of annotators) Cohen's kappa per post, using character offsets as instances being classified as toxic (included in a toxic span) or non-toxic; we then averaged over the posts.",37,38
225,248780005,"Although our dataset contains only posts found toxic by at least half of the original crowd-raters, only 31 of the 87 posts were found toxic by all five of our annotators, and 51 were found toxic by the majority of our annotators; this is an indicator of the well-known subjectivity of toxicity detection.",7,8
226,248780005,"Although our dataset contains only posts found toxic by at least half of the original crowd-raters, only 31 of the 87 posts were found toxic by all five of our annotators, and 51 were found toxic by the majority of our annotators; this is an indicator of the well-known subjectivity of toxicity detection.",27,28
227,248780005,"Although our dataset contains only posts found toxic by at least half of the original crowd-raters, only 31 of the 87 posts were found toxic by all five of our annotators, and 51 were found toxic by the majority of our annotators; this is an indicator of the well-known subjectivity of toxicity detection.",39,40
228,248780005,"On the 31, 51, and 87 posts, the average kappa score was 65%, 55%, 48%, respectively, indicating that when the raters agree (at least by majority) about the toxicity of the post, there is also reasonable agreement regarding the toxic spans.",52,53
229,248780005,Note that the toxic spans are typically short.,3,4
230,248780005,"This leads to class imbalance (most offsets are marked as non-toxic), increases agreement by chance (on the non-toxic offsets), and leads to low kappa scores (kappa adjusts for chance agreement).",13,14
231,248780005,"This leads to class imbalance (most offsets are marked as non-toxic), increases agreement by chance (on the non-toxic offsets), and leads to low kappa scores (kappa adjusts for chance agreement).",25,26
232,248780005,Another reason behind this modest (compared to other tasks) inter-annotator agreement is the inherent subjectivity of deciding if a post is toxic or not.,25,26
233,248780005,"Our kappa score is in fact slightly higher than in previous work on toxicity detection, classifying posts as toxic or not (Sap et al.,",19,20
234,248780005,"We then assigned a toxicity score to each character offset of t, computed as the fraction of raters who annotated that character offset as toxic (included it in their toxic spans).",25,26
235,248780005,"We then assigned a toxicity score to each character offset of t, computed as the fraction of raters who annotated that character offset as toxic (included it in their toxic spans).",31,32
236,248780005,"The dataset TOXICSPANS contains the 11,035 posts we annotated for toxic spans.",10,11
237,248780005,"Furthermore, about 5k of the 11k posts have an empty ground truth set of toxic character offsets (as in the last post of Table 1), even though all the posts of our dataset had been found toxic by the original raters.",15,16
238,248780005,"Furthermore, about 5k of the 11k posts have an empty ground truth set of toxic character offsets (as in the last post of Table 1), even though all the posts of our dataset had been found toxic by the original raters.",40,41
239,248780005,This is partly due to the fact that we include in the ground truth only character offsets that were included in the toxic spans of the majority of our annotators.,22,23
240,248780005,It also confirms it is not always possible to attribute (at least not by consensus) the toxicity of a post to particular toxic spans.,24,25
241,248780005,A dense toxic span of a post is a maximal sequence of contiguous toxic characters.,2,3
242,248780005,A dense toxic span of a post is a maximal sequence of contiguous toxic characters.,13,14
243,248780005,"There exist posts with more than one dense toxic span, but most posts include only one.",8,9
244,248780005,"Evaluation framework for toxic spans For the newly introduced toxic spans detection task, we evaluate systems in terms of F 1 score, as in the work of Da San Martino et al. (",3,4
245,248780005,"Evaluation framework for toxic spans For the newly introduced toxic spans detection task, we evaluate systems in terms of F 1 score, as in the work of Da San Martino et al. (",9,10
246,248780005,"Given a test post t, let system A i return a set S t A i of character offsets, for parts of the post found to be toxic.",29,30
247,248780005,"5 Methods for toxic spans detection 5.1 Simplistic baselines TRAIN-MATCH, is a simple lookup-based model that classifies as toxic any tokens encountered inside toxic spans of the training data.",3,4
248,248780005,"5 Methods for toxic spans detection 5.1 Simplistic baselines TRAIN-MATCH, is a simple lookup-based model that classifies as toxic any tokens encountered inside toxic spans of the training data.",23,24
249,248780005,"5 Methods for toxic spans detection 5.1 Simplistic baselines TRAIN-MATCH, is a simple lookup-based model that classifies as toxic any tokens encountered inside toxic spans of the training data.",28,29
250,248780005,"A naive baseline, RAND-SEQ, randomly classifies tokens as toxic or not.",12,13
251,248780005,"We call this model CNN-SEQ and fine-tune it on dense toxic spans, treated as 'entities'.",14,15
252,248780005,"2020) for toxic spans (BERT-SEQ, SPAN-BERT-SEQ).",3,4
253,248780005,4 These methods require training data manually annotated with toxic spans.,9,10
254,248780005,"Weakly supervised learning We trained binary classifiers to predict the toxicity label of each post, and we employed attention as a rationale extraction mechanism at inference to obtain toxic spans, an approach Pavlopoulos et al. (",29,30
255,248780005,"To detect toxic spans, we used the attention scores of the BILSTM and the attention scores from the heads of BERT's last layer averaged over the heads, respectively.",2,3
256,248780005,"In both cases, we obtain a sequence of binary decisions (toxic, nontoxic) for the tokens of the post (inherited by their character offsets) by using a probability threshold (tuned on development data) applied to the attention scores.",12,13
257,248780005,These methods require training posts annotated only with toxicity labels per post (no toxic span annotations).,14,15
258,248780005,These results show that the tokens of toxic spans are context-dependent and their meaning is not captured well by context-unaware look-up lexicons.,7,8
259,248780005,"BERT+ARE performs worse than BILSTM+ARE, despite the fact that the underlying BERT classifier is much better (ROC AUC 96.1%) at separating toxic from nontoxic posts than the underlying BILSTM (90.9%).",25,26
260,248780005,"Interestingly, the BILSTM binary toxicity classifier with the attention-based toxic span detection mechanism (Pavlopoulos et al.,",12,13
261,248780005,"2017b) is close in performance with BILSTM-SEQ, despite the fact that the latter is directly trained on toxic span annotations, whereas the former is trained with binary post-level annotations only (toxic, non-toxic post).",21,22
262,248780005,"2017b) is close in performance with BILSTM-SEQ, despite the fact that the latter is directly trained on toxic span annotations, whereas the former is trained with binary post-level annotations only (toxic, non-toxic post).",38,39
263,248780005,"2017b) is close in performance with BILSTM-SEQ, despite the fact that the latter is directly trained on toxic span annotations, whereas the former is trained with binary post-level annotations only (toxic, non-toxic post).",42,43
264,248780005,"Therefore, attribution-based toxic span detectors, such as BILSTM+ARE, can in principle perform even better if the underlying binary classifier is trained on a larger existing dataset.",5,6
265,248780005,"We added to the training set of each cross-validation fold 80k further toxic and non-toxic posts (still equally balanced, without toxic spans) from the dataset of Borkan et al. (",14,15
266,248780005,"We added to the training set of each cross-validation fold 80k further toxic and non-toxic posts (still equally balanced, without toxic spans) from the dataset of Borkan et al. (",18,19
267,248780005,"We added to the training set of each cross-validation fold 80k further toxic and non-toxic posts (still equally balanced, without toxic spans) from the dataset of Borkan et al. (",26,27
268,248780005,"The ROC AUC score of the underlying BILSTM (in the task of separating toxic from nontoxic posts) improved from 90.9% to 94.2%, and the F 1 score of BILSTM+ARE (in toxic spans detection) improved from 57.7% to 58.8%, almost reaching the performance of BILSTM-SEQ.",14,15
269,248780005,"The ROC AUC score of the underlying BILSTM (in the task of separating toxic from nontoxic posts) improved from 90.9% to 94.2%, and the F 1 score of BILSTM+ARE (in toxic spans detection) improved from 57.7% to 58.8%, almost reaching the performance of BILSTM-SEQ.",36,37
270,248780005,"6   7 Toxic spans in toxic-to-civil transfer As shown in Section 6, a toxic span detection method can be used to highlight toxic parts of a post, to assist, for instance, human moderators.",6,7
271,248780005,"6   7 Toxic spans in toxic-to-civil transfer As shown in Section 6, a toxic span detection method can be used to highlight toxic parts of a post, to assist, for instance, human moderators.",19,20
272,248780005,"6   7 Toxic spans in toxic-to-civil transfer As shown in Section 6, a toxic span detection method can be used to highlight toxic parts of a post, to assist, for instance, human moderators.",28,29
273,248780005,"The new TOXICSPANS dataset and toxic span detection methods, however, can assist in more ways.",5,6
274,248780005,"This section describes how we combined the new dataset and the best-performing toxic span detector (SPAN-BERT-SEQ) to show how they can be useful in toxic-to-civil text transfer (Nogueira dos Santos et al.,",14,15
275,248780005,"This section describes how we combined the new dataset and the best-performing toxic span detector (SPAN-BERT-SEQ) to show how they can be useful in toxic-to-civil text transfer (Nogueira dos Santos et al.,",32,33
276,248780005,"In the context of detoxifying comments to nudge users towards healthier conversations online, this task aims at suggesting civil rephrasings of toxic posts.",22,23
277,248780005,"More specifically, we study the following research question: ""Can TOXICSPANS data and toxic span detectors be used to assess the mitigation of explicit toxicity in toxic-to-civil transfer?""",15,16
278,248780005,"More specifically, we study the following research question: ""Can TOXICSPANS data and toxic span detectors be used to assess the mitigation of explicit toxicity in toxic-to-civil transfer?""",28,29
279,248780005,"To answer this question, we proceeded in two ways: (i) evaluating the transfer of toxic spans in system-detoxified posts, and (ii) studying any remaining toxic spans in human-detoxified posts.",18,19
280,248780005,"To answer this question, we proceeded in two ways: (i) evaluating the transfer of toxic spans in system-detoxified posts, and (ii) studying any remaining toxic spans in human-detoxified posts.",33,34
281,248780005,"System-detoxified posts We first compare the performance of two toxic-tocivil transfer models, CAE-T5 and SED-T5, both based on the T5 transformer encoder-decoder architecture (Raffel et al.,",11,12
282,248780005,"SED-T5 is a datasets, used to train the SED-T5 and CAE-T5 toxic-to-civil models, respectively.",18,19
283,248780005,"2021) , consisting of pairs of comments: a toxic comment and a detoxified paraphrase written by a crowdworker.",10,11
284,248780005,"Table 5 summarizes statistics of the two datasets (P, NP) and highlights a trade-off between the level of supervision and number of samples: there is a 1:40 ratio between toxic comments in P (direct supervision, parallel data) and NP (indirect supervision, no parallel data).",35,36
285,248780005,"Accuracy measures the rate of successful transfers from toxic to civil, and computes the fraction of posts whose civil version is classified as non-toxic by a BERT toxicity classifier; we used the BERT-based toxicity classifier of Laugier et al. (",8,9
286,248780005,"Accuracy measures the rate of successful transfers from toxic to civil, and computes the fraction of posts whose civil version is classified as non-toxic by a BERT toxicity classifier; we used the BERT-based toxicity classifier of Laugier et al. (",26,27
287,248780005,"Similarity measures content preservation between the original toxic text and its systemrephrased civil version (self-SIM) or the gold (human) civil rephrasing (ref-SIM, only for P); in both cases, it is computed as the cosine similarity between the single-vector representations of the two texts, produced by the universal sentence encoder of Cer et al. (",7,8
288,248780005,These considerations motivated us to seek ways to further analyse the behavior of toxic-to-civil transfer models.,13,14
289,248780005,"TOXICSPANS and toxic span detectors are an opportunity to move towards this direction, by studying how well transfer models cope with explicit toxicity, i.e., spans that can be explicitly pointed to as sources of toxicity.",2,3
290,248780005,"We leave for future work the flip side of this study, i.e., studying cases where transfer models rephrase spans not explicitly marked (by toxic span detectors or human annotators) as explicitly toxic.",26,27
291,248780005,"We leave for future work the flip side of this study, i.e., studying cases where transfer models rephrase spans not explicitly marked (by toxic span detectors or human annotators) as explicitly toxic.",35,36
292,248780005,"Explicit Toxicity Removal Accuracy Recall that the accuracy (ACC) scores of Table 6 measure the percentage of toxic posts that the transfer models (CAE-T5, SED-T5) rephrased to forms that a (BERT-based) toxicity classifier considered non-toxic.",19,20
293,248780005,"Explicit Toxicity Removal Accuracy Recall that the accuracy (ACC) scores of Table 6 measure the percentage of toxic posts that the transfer models (CAE-T5, SED-T5) rephrased to forms that a (BERT-based) toxicity classifier considered non-toxic.",49,50
294,248780005,"One could question, however, if it is possible (even for humans) to produce a civil rephrase of a toxic post when it is impossible to point to particular spans of the post that cause its toxicity (as in the last post of Table 1 ).",22,23
295,248780005,"Detoxifying posts of this kind may constitute a mission impossible for most models (possibly even for humans); the only way to produce a non-toxic 'rephrase' may be to change the original post beyond recognition, which may be rewarding systems like CAE-T5 that often hallucinate in their rephrases, as already discussed.",28,29
296,248780005,"Hence, it makes sense to focus on posts that contain explicit toxic spans, marked by human annotators (for TOXICSPANS) or our best toxic span detector (SPAN-BERT-SEQ).",12,13
297,248780005,"Hence, it makes sense to focus on posts that contain explicit toxic spans, marked by human annotators (for TOXICSPANS) or our best toxic span detector (SPAN-BERT-SEQ).",26,27
298,248780005,"Using these toxic spans, we define three additional variants of accuracy: ACC2 is the same as ACC, but ignores (in its denominator) posts that do not contain at least one toxic span; ACC3 also considers (in its denominator) only posts that contained at least one toxic span, but computes the fraction of these posts that had all of their toxic spans rephrased (even partly) by the transfer model; ACC4 is a stricter version of ACC3 that requires the posts to also be judged non-toxic by the (BERTbased) toxicity classifier.",2,3
299,248780005,"Using these toxic spans, we define three additional variants of accuracy: ACC2 is the same as ACC, but ignores (in its denominator) posts that do not contain at least one toxic span; ACC3 also considers (in its denominator) only posts that contained at least one toxic span, but computes the fraction of these posts that had all of their toxic spans rephrased (even partly) by the transfer model; ACC4 is a stricter version of ACC3 that requires the posts to also be judged non-toxic by the (BERTbased) toxicity classifier.",35,36
300,248780005,"Using these toxic spans, we define three additional variants of accuracy: ACC2 is the same as ACC, but ignores (in its denominator) posts that do not contain at least one toxic span; ACC3 also considers (in its denominator) only posts that contained at least one toxic span, but computes the fraction of these posts that had all of their toxic spans rephrased (even partly) by the transfer model; ACC4 is a stricter version of ACC3 that requires the posts to also be judged non-toxic by the (BERTbased) toxicity classifier.",53,54
301,248780005,"Using these toxic spans, we define three additional variants of accuracy: ACC2 is the same as ACC, but ignores (in its denominator) posts that do not contain at least one toxic span; ACC3 also considers (in its denominator) only posts that contained at least one toxic span, but computes the fraction of these posts that had all of their toxic spans rephrased (even partly) by the transfer model; ACC4 is a stricter version of ACC3 that requires the posts to also be judged non-toxic by the (BERTbased) toxicity classifier.",68,69
302,248780005,"Using these toxic spans, we define three additional variants of accuracy: ACC2 is the same as ACC, but ignores (in its denominator) posts that do not contain at least one toxic span; ACC3 also considers (in its denominator) only posts that contained at least one toxic span, but computes the fraction of these posts that had all of their toxic spans rephrased (even partly) by the transfer model; ACC4 is a stricter version of ACC3 that requires the posts to also be judged non-toxic by the (BERTbased) toxicity classifier.",97,98
303,248780005,"Table 6 shows that restricting ACC to consider only posts with at least one toxic post (ACC2) substantially improves the performance of both models on the NP dataset, indicating that it contains many 'mission impossible' instances (posts with no toxic spans) that the original ACC considers.",14,15
304,248780005,"Table 6 shows that restricting ACC to consider only posts with at least one toxic post (ACC2) substantially improves the performance of both models on the NP dataset, indicating that it contains many 'mission impossible' instances (posts with no toxic spans) that the original ACC considers.",45,46
305,248780005,"By contrast, switching from ACC to ACC2 leads to mostly negligible changes on the P and TOXICSPANS datasets, which is in accordance with the fact that they contain fewer posts with no toxic spans (11.5% and 48.7%, respectively, compared to 67.4% for NP).",34,35
306,248780005,"Another interesting observation is that ACC4 is always substantially lower than ACC3 (for both systems, on all three datasets), indicating that the models often successfully detect toxic spans and try to rephrase them, but the rephrases are still toxic, at least according to the toxicity classifier.",30,31
307,248780005,"Another interesting observation is that ACC4 is always substantially lower than ACC3 (for both systems, on all three datasets), indicating that the models often successfully detect toxic spans and try to rephrase them, but the rephrases are still toxic, at least according to the toxicity classifier.",43,44
308,248780005,"Human-detoxified posts In this experiment, we wished to study the extent to which humans rephrase known toxic spans, when asked to produce civil rephrases of toxic posts.",19,20
309,248780005,"Human-detoxified posts In this experiment, we wished to study the extent to which humans rephrase known toxic spans, when asked to produce civil rephrases of toxic posts.",29,30
310,248780005,"7 Since P does not contain gold toxic spans, we again employed SPAN-BERT-SEQ to add toxic spans to the source posts and retained only the 1,354 (out of 2,778 in 7 We used all the P data, since no training was involved.",7,8
311,248780005,"7 Since P does not contain gold toxic spans, we again employed SPAN-BERT-SEQ to add toxic spans to the source posts and retained only the 1,354 (out of 2,778 in 7 We used all the P data, since no training was involved.",20,21
312,248780005,total) source-target pairs of posts with at least one toxic span in their source post.,12,13
313,248780005,"8 In all but 6 of the 1,354 posts, the humans have rephrased (in the gold target post they provided) all the toxic spans of the source post.",25,26
314,248780005,"The 6 posts were mainly cases where the human changed the context to mitigate toxicity, while retaining the original toxic span.",20,21
315,248780005,"For example, ""he's not that stupid"" became ""he's not stupid"" (original toxic span shown in bold); in this case removing the 'that' from the context arguably makes the post less offensive.",19,20
316,248780005,"Overall, we conclude that humans did rephrase almost all cases of explicit toxicity in the toxic posts they were given.",16,17
317,248780005,This flagged 93 gold target posts as comprising at least one toxic span.,11,12
318,248780005,"The first category comprises cases where a toxic span of the source post was rephrased, but the rephrase might not be considered totally civil; e.g., ""how freaking narcissistic do you have to be?""",7,8
319,248780005,where SPAN-BERT-SEQ marked the 'narcissistic' of the rephrase as a toxic span.,16,17
320,248780005,"The second category comprises cases where SPAN-BERT-SEQ produced false positives; e.g., the source post ""most of the information is total garbage"" became ""most of the information is totally useless"", but SPAN-BERT-SEQ marked (arguably incorrectly) 'useless' as a toxic span.",56,57
321,248780005,"2021) to the 2,778 posts of the P dataset, dividing them in two sets: posts that comprised at least one toxic span detected by SPAN-BERT-SEQ (1,354 posts with explicit toxicity) and the rest (implicit toxicity).",23,24
322,248780005,"The BERT-based toxicity classifier considered more toxic (higher average toxicity score) the 1,354 posts of the first set compared to the second one, i.e., it was more confident that the posts of the first set (explicit toxicity) were toxic, as one might expect.",8,9
323,248780005,"The BERT-based toxicity classifier considered more toxic (higher average toxicity score) the 1,354 posts of the first set compared to the second one, i.e., it was more confident that the posts of the first set (explicit toxicity) were toxic, as one might expect.",46,47
324,248780005,"Discussion The posts we annotated for toxic spans were extracted from an already heavily studied public domain benchmark dataset (Civil Comments) that has been examined by thousands of teams in a Kaggle competition, 9 and that has been cited in over 50 academic publications.",6,7
325,248780005,"We note that it is more difficult and costly (approximately 3 times more) to manually annotate toxic spans, instead of just labeling entire posts as toxic or not.",18,19
326,248780005,"We note that it is more difficult and costly (approximately 3 times more) to manually annotate toxic spans, instead of just labeling entire posts as toxic or not.",28,29
327,248780005,"We showed that BILSTM+ARE has the potential to reach the performance of BILSTM-SEQ, which is important for future work aiming to build toxic span detectors without any toxic span annotations in the training data.",25,26
328,248780005,"We showed that BILSTM+ARE has the potential to reach the performance of BILSTM-SEQ, which is important for future work aiming to build toxic span detectors without any toxic span annotations in the training data.",30,31
329,248780005,"Having two separate systems, one for toxicity detection and one for toxic spans identification, is more easily compatible with existing deployed toxicity detectors.",12,13
330,248780005,"One can simply add a component for toxic spans at the end of a pipeline for toxicity detection, and the new component would be invoked only when toxicity would be detected, leaving the rest of the existing pipeline unchanged.",7,8
331,248780005,"Since the vast majority of posts in real-world applications is non-toxic (Borkan et al.,",14,15
332,248780005,"2019) , this pipeline approach would only increase the computational load for the relatively few posts classified as toxic.",19,20
333,248780005,"Using only toxic posts in this study was also a way to simplify this first approach to toxic spans detection, assuming an oracle system achieved the first step 9 shorturl.at/hqEJ3 (deciding which posts are toxic).",2,3
334,248780005,"Using only toxic posts in this study was also a way to simplify this first approach to toxic spans detection, assuming an oracle system achieved the first step 9 shorturl.at/hqEJ3 (deciding which posts are toxic).",17,18
335,248780005,"Using only toxic posts in this study was also a way to simplify this first approach to toxic spans detection, assuming an oracle system achieved the first step 9 shorturl.at/hqEJ3 (deciding which posts are toxic).",36,37
336,248780005,"However, we note that future work could study adding non-toxic posts to our dataset and requiring systems to first detect toxic posts, then extract toxic spans for toxic posts.",12,13
337,248780005,"However, we note that future work could study adding non-toxic posts to our dataset and requiring systems to first detect toxic posts, then extract toxic spans for toxic posts.",23,24
338,248780005,"However, we note that future work could study adding non-toxic posts to our dataset and requiring systems to first detect toxic posts, then extract toxic spans for toxic posts.",28,29
339,248780005,"However, we note that future work could study adding non-toxic posts to our dataset and requiring systems to first detect toxic posts, then extract toxic spans for toxic posts.",31,32
340,248780005,"A direct comparison (in terms of size) of TOX-ICSPANS with other existing toxicity datasets is only possible if one focuses on the toxic class, typically the minority one, since our dataset contains only toxic posts.",26,27
341,248780005,"A direct comparison (in terms of size) of TOX-ICSPANS with other existing toxicity datasets is only possible if one focuses on the toxic class, typically the minority one, since our dataset contains only toxic posts.",39,40
342,248780005,"By adding non-toxic posts, much larger versions of our dataset can be compiled, of sizes similar to those of existing previous datasets (that provide post-level annotations only).",4,5
343,248780005,"Hence, our TOXICSPANS dataset is accessible with the following versions: First, only toxic posts included (11,006 posts), which is the version we discuss in this work.",15,16
344,248780005,"Second, the previous version will be augmented with the same number of randomly selected non-toxic Civil Comments posts.",17,18
345,248780005,"Third, a version similar to the previous one, but where the ratio of toxic to non-toxic posts will be 1:40 to be closer to that of real-world datasets (325,499 posts).",15,16
346,248780005,"Third, a version similar to the previous one, but where the ratio of toxic to non-toxic posts will be 1:40 to be closer to that of real-world datasets (325,499 posts).",19,20
347,248780005,"As shown in Section 7, the TOXICSPANS dataset and toxic span detectors can also help study and evaluate explicit toxicity removal when rephrasing toxic posts to be civil.",10,11
348,248780005,"As shown in Section 7, the TOXICSPANS dataset and toxic span detectors can also help study and evaluate explicit toxicity removal when rephrasing toxic posts to be civil.",24,25
349,248780005,"In this case, toxic spans can be used to get a better understanding of how toxic-to-civil models operate, by showing the toxic spans and their context, along with their rephrases.",4,5
350,248780005,"In this case, toxic spans can be used to get a better understanding of how toxic-to-civil models operate, by showing the toxic spans and their context, along with their rephrases.",16,17
351,248780005,"In this case, toxic spans can be used to get a better understanding of how toxic-to-civil models operate, by showing the toxic spans and their context, along with their rephrases.",27,28
352,248780005,"Intended use and misuse potential The toxic span detection systems we consider are trained (the sequence-labeling ones) and tested (all systems) on posts with binary ground-truth character offset labels (toxic or not), reflecting the majority opinion of the annotators (Section 3).",6,7
353,248780005,"Intended use and misuse potential The toxic span detection systems we consider are trained (the sequence-labeling ones) and tested (all systems) on posts with binary ground-truth character offset labels (toxic or not), reflecting the majority opinion of the annotators (Section 3).",38,39
354,248780005,"To address this issue, we also release the toxic spans of all the annotators and the pseudonymous rater identities, not just the spans that reflect the majority opinion, to allow different label binarisation strategies and further studies.",9,10
355,248780005,Incorrect results could be of two types; toxic spans that were not highlighted and non-toxic spans that were highlighted.,8,9
356,248780005,Incorrect results could be of two types; toxic spans that were not highlighted and non-toxic spans that were highlighted.,17,18
357,248780005,"As with other content filtering systems (e.g., spam filters, phishing detectors), toxic span detectors may trigger an adversarial reaction of malicious users, who may study which types of toxic expressions evade the detectors (esp.",16,17
358,248780005,"As with other content filtering systems (e.g., spam filters, phishing detectors), toxic span detectors may trigger an adversarial reaction of malicious users, who may study which types of toxic expressions evade the detectors (esp.",34,35
359,248780005,"publicly available ones) and may gradually start using more implicit toxic language (e.g., irony, false claims), which may be more difficult to detect.",11,12
360,248780005,"However, this is a danger that concerns any toxicity detection system, including systems that classify user content at the post level (without detecting toxic spans).",26,27
361,248780005,"Conclusions and future work We studied toxicity detection, which aims to identify the spans of a user post that make it toxic.",22,23
362,248780005,"By leveraging the dataset of posts annotated as toxic or nontoxic (without spans), we showed that this method can reach the performance of a BILSTM sequence labelling approach that was trained on the more costly toxic spans annotations.",8,9
363,248780005,"By leveraging the dataset of posts annotated as toxic or nontoxic (without spans), we showed that this method can reach the performance of a BILSTM sequence labelling approach that was trained on the more costly toxic spans annotations.",38,39
364,248780005,This result is particularly interesting for future work aiming to perform toxic spans detection by using only datasets with whole-post toxicity annotations.,11,12
365,248780005,"In a final experiment, we examined toxic-to-civil transfer, showing how toxic spans can help shed more light on this task too, by helping assess how well systems and humans address explicit toxicity.",7,8
366,248780005,"In a final experiment, we examined toxic-to-civil transfer, showing how toxic spans can help shed more light on this task too, by helping assess how well systems and humans address explicit toxicity.",16,17
367,248780005,In future work we plan to study toxic span detection in multiple languages and in context-dependent toxic posts.,7,8
368,248780005,In future work we plan to study toxic span detection in multiple languages and in context-dependent toxic posts.,18,19
369,248780005,Figure 1 shows the distribution of the percentage of character offsets of each post that are included in toxic spans.,18,19
370,248780005,Figure 2 illustrates the distribution of dense toxic spans per post.,7,8
371,248780005,Figure 3 shows the most frequent toxic spans in the dataset (after lowercasing each post) and their frequencies.,6,7
372,248780005,Figure 4 shows the most frequent multi-word toxic spans (again after lower-casing).,9,10
373,248780005,A.2 Error analysis of SPAN-BERT-SEQ We performed an error analysis on our best toxic spans detector (SPAN-BERT-SEQ).,17,18
374,248780005,"The first, which is the most frequent one occurring in 235 out of 1001 posts (23.5%), comprises posts for which SPAN-BERT-SEQ failed to find all toxic spans.",34,35
375,248780005,"It occurs when the ground truth of a post is empty, but SPAN-BERT-SEQ predicts at least one toxic span (Table 9 ).",22,23
376,248780005,"The last type of error occurs rarely (only 10 out of 1001 posts) when the ground truth of a post is not empty, and SPAN-BERT-SEQ predicts more (or larger) toxic spans than it should (Table 10 ).",38,39
377,248780005,"Table 10 : Examples posts where the ground truth was not empty, and SPAN-BERT-SEQ incorrectly predicted more (or larger) toxic spans.",26,27
378,248780005,"At training time, we ignore posts with more than one dense toxic span, since the SQUAD 2.0 format allows for only one dense answer span in the context.",12,13
379,248780005,"In both models, the attention threshold (above which a token is considered toxic) was fine-tuned on the development set of each Monte Carlo C-V fold.",14,15
380,7934160,"Toxicity dataset: This dataset was created like the previous one, but contains more comments (159,686), now labeled as toxic (reject) or not (accept).",23,24
381,7934160,"In all three parts, the rejected (toxic) comments are 10%, again an artificial ratio.",8,9
382,2944650,160K comments labeled as being toxic or not.,6,7
383,102353948,"We use our approach to attack a toxicity classifier, aimed at detecting toxic language on social media (Hosseini et al.,",13,14
384,102353948,"We then use our model for a black-box attack against Google Perspective API for detecting toxic sentences, and find that 42% of our generated sentences are misclassified by the API, while humans agree that the sentences are toxic.",17,18
385,102353948,"We then use our model for a black-box attack against Google Perspective API for detecting toxic sentences, and find that 42% of our generated sentences are misclassified by the API, while humans agree that the sentences are toxic.",42,43
386,102353948,"Experiments We now empirically investigate whether our method can be used to attack classifiers for detecting ""toxic"" language on social media.",17,18
387,102353948,Recently a challenge by Alphabet aimed to improve labeling of toxic comments that are rude and disrespectful.,10,11
388,102353948,2015) pooling layer to obtain a fixed dimensional vector for x. This vector is passed through a feed-forward layer to compute the probability that x is toxic.,29,30
389,102353948,"We used the 13,815 toxic-labeled sentences from the training set to generate adversarial examples for training the attacker as described above.",4,5
390,102353948,"We report the average number of flips required to change the prediction of toxic sentences in the source model, the slow-down per single character-flip, and the slowdown per attack, which is computed by multiplying slow-down per flip by the ratio of the number of flips required per attack.",13,14
391,102353948,Figure 3 provides a more fine-grained view of the results by showing the proportion of sentences classified as toxic as a function of the number of flips.,20,21
392,102353948,"Attacking The Google Perspective API The Google perspective API 4 returns the probability that a sentence is toxic, where probability > 0.7 is classified as toxic, < 0.3 is non-toxic, and otherwise uncertain.",17,18
393,102353948,"Attacking The Google Perspective API The Google perspective API 4 returns the probability that a sentence is toxic, where probability > 0.7 is classified as toxic, < 0.3 is non-toxic, and otherwise uncertain.",26,27
394,102353948,"Attacking The Google Perspective API The Google perspective API 4 returns the probability that a sentence is toxic, where probability > 0.7 is classified as toxic, < 0.3 is non-toxic, and otherwise uncertain.",33,34
395,102353948,We randomly selected 136 toxic examples from the validation set and attacked them with DISTFLIP+ until the source model probability was < 0.5.,4,5
396,102353948,The label is flipped from toxic to uncertain or non-toxic in 42% of these examples.,5,6
397,102353948,The label is flipped from toxic to uncertain or non-toxic in 42% of these examples.,11,12
398,102353948,"Human validation To validate that toxic sentences remain toxic after our attack, we showed 5 independent annotators a total of 150 sentences from three classes: toxic sentences, non-toxic sentences, and attacked sentences (attacked by DISTFLIP-5).",5,6
399,102353948,"Human validation To validate that toxic sentences remain toxic after our attack, we showed 5 independent annotators a total of 150 sentences from three classes: toxic sentences, non-toxic sentences, and attacked sentences (attacked by DISTFLIP-5).",8,9
400,102353948,"Human validation To validate that toxic sentences remain toxic after our attack, we showed 5 independent annotators a total of 150 sentences from three classes: toxic sentences, non-toxic sentences, and attacked sentences (attacked by DISTFLIP-5).",27,28
401,102353948,"Human validation To validate that toxic sentences remain toxic after our attack, we showed 5 independent annotators a total of 150 sentences from three classes: toxic sentences, non-toxic sentences, and attacked sentences (attacked by DISTFLIP-5).",32,33
402,102353948,The same annotator was never shown a toxic sentence and its attacked counterpart.,7,8
403,102353948,"We asked annotators whether sentences are toxic, and measured average annotated toxicity.",6,7
404,102353948,"We found that 89.8% of the toxic sentences were annotated as toxic, compared to 87.6% in the attacked toxic sentences.",7,8
405,102353948,"We found that 89.8% of the toxic sentences were annotated as toxic, compared to 87.6% in the attacked toxic sentences.",12,13
406,102353948,"We found that 89.8% of the toxic sentences were annotated as toxic, compared to 87.6% in the attacked toxic sentences.",21,22
407,102353948,"This shows that humans clearly view the sentences as toxic, even after our attack.",9,10
408,17979086,"For instance, ""雨 ame (rain)"" has many expressions that are synonyms only in this disaster, such as ""汚 染 さ れ た 雨 osen sa reta ame, 黒い 雨 kuroi ame, 有害 な 雨 yugaina ame, 有毒な雨 yudokuna ame (all of which mean toxic rain),"" while in other clusters ""雨 ame (rain)"" has general synonyms, such as ""小 雨 kosame (light rain),"" ""冷たい 雨 tsumetai ame (cold rain),"" and ""大雨 ohame (heavy rain).""",54,55
409,17979086,"These specific synonyms were obtained because the following false rumor was disseminated and repeated for a long period: ""There may be toxic rain due to an explosion at Cosmo Oil.""",23,24
410,231709253,"An explanation might be that comments which seem toxic out of context are in fact spirited discussion due to the degree of involvement of the participants; for instance, we have observed that contributors in some cases challenge others' credentials, which (Niculae and Danescu-Niculescu-Mizil, 2016) , followed closely by politeness (Zhang et al.,",8,9
411,248366513,Such toxic speech has detrimental effects on online communities and can cause significant personal harm.,1,2
412,248366513,"Efforts by the NLP community to address this problem has led to the development of models capable of identifying toxic speech in specific domains (sexism (Golbeck et al.,",19,20
413,248366513,"With the advent of social media platforms, many resources have been developed for identifying toxic comments in web text (Waseem and Hovy, 2016; Davidson et al.,",15,16
414,248366513,"For instance, during training, toxic words can be masked to reduce their role in model predictions (Dale et al.,",6,7
415,248366513,"Additionally, it may also be important to identify offensive statements made to a dialogue system, as it has been shown that dialogue systems can react with counteraggression (Cercas Curry and Rieser, 2018) , and systems that continuously learn during deployment may incorporate toxic user responses into future generations.",47,48
416,184123,"For example, in early 2008 a food poisoning case caused a big media stir in Japan when dozens of people fell ill after eating Chinese-imported frozen food products containing residual traces of toxic pesticides.",35,36
417,184123,"While supposedly the presence of toxic chemicals in imported frozen foods had already been established on several occasions before, until the recent incidents public awareness of these facts remained low.",5,6
418,203078302,"It is possible that a model would be more stable on sentences with highly toxic language, but the effect of perturbation is more prevalent in sentences that have fewer signals of toxicity.",14,15
419,203078302,"To further understand the impact of perturbation sensitivity, we calculate LabelDist, which takes into account the number of sentences that switch either from toxic to non-toxic or vice versa, when a pronoun is changed to a name.",25,26
420,203078302,"To further understand the impact of perturbation sensitivity, we calculate LabelDist, which takes into account the number of sentences that switch either from toxic to non-toxic or vice versa, when a pronoun is changed to a name.",29,30
421,203078302,This roughly suggests that around 10-40% of sentences (with third person singular pronouns) labeled as toxic at any given threshold could flip the label as a result of name perturbation.,20,21
422,218487466,"Since the primary intended use of this model is to facilitate moderation of online comments, this bias can result in non-toxic comments mentioning disabilities being flagged as toxic at a disproportionately high rate.",23,24
423,218487466,"Since the primary intended use of this model is to facilitate moderation of online comments, this bias can result in non-toxic comments mentioning disabilities being flagged as toxic at a disproportionately high rate.",30,31
424,218487466,"Of the 4889 comments labeled as having a mention of psychiatric or mental illness, 1030 (21%) were labeled as toxic whereas 3859 were labeled as non-toxic.",23,24
425,218487466,"Of the 4889 comments labeled as having a mention of psychiatric or mental illness, 1030 (21%) were labeled as toxic whereas 3859 were labeled as non-toxic.",31,32
426,218487466,"We first up-sampled the toxic comments with disability mentions (to N=3859, by repetition at random), so that we have equal number of toxic vs. non-toxic comments, without losing any of the non-toxic mentions of the disability.",6,7
427,218487466,"We first up-sampled the toxic comments with disability mentions (to N=3859, by repetition at random), so that we have equal number of toxic vs. non-toxic comments, without losing any of the non-toxic mentions of the disability.",28,29
428,218487466,"We first up-sampled the toxic comments with disability mentions (to N=3859, by repetition at random), so that we have equal number of toxic vs. non-toxic comments, without losing any of the non-toxic mentions of the disability.",32,33
429,218487466,"We first up-sampled the toxic comments with disability mentions (to N=3859, by repetition at random), so that we have equal number of toxic vs. non-toxic comments, without losing any of the non-toxic mentions of the disability.",42,43
430,218487466,"We then sampled the same number of comments from those that do not have the disability mention, also balanced across toxic and non-toxic categories.",21,22
431,218487466,"We then sampled the same number of comments from those that do not have the disability mention, also balanced across toxic and non-toxic categories.",25,26
432,218487466,"In another deployment context, models for detecting abuse can be used to nudge writers to rethink comments which might be interpreted as toxic (Jurgens et al.,",23,24
433,231741340,"Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy.",13,14
434,231741340,"As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection.",20,21
435,231741340,"Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.",12,13
436,231741340,"Introduction Current hate speech or toxic language detection 1 systems exhibit problematic and discriminatory behavior that causes them to have disparate negative impact on minority populations (Yasin, 2018; Guynn, 2020; Kim et al.,",5,6
437,231741340,"Tweets simply containing a minority identity mention are commonly flagged as toxic by current systems, in contrast to those containing majority identity mentions, as illustrated in Figure 1 .",11,12
438,231741340,Previous work has outlined two such biases for hate Figure 1 : Lexical items and dialect markers cause problematic behavior for toxic language detection systems such as the widely used PerspectiveAPI.,21,22
439,231741340,"In the top two example pairs, statements with minority identity mentions and swear words used inoffensively are flagged as toxic, but majority identity mentions or offensive statements without overt swearing are missed.",20,21
440,231741340,"When trained on biased datasets, models acquire and exacerbate these biases (e.g., flagging text by Black authors as more toxic than by white authors; Sap et al.,",22,23
441,231741340,This raises a natural question: are current debiasing approaches effective for mitigating biases specific to toxic language detection?,16,17
442,231741340,2018) for toxic language detection.,3,4
443,231741340,"Debiased"" models still disproportionately flag text in certain dialects as toxic.",11,12
444,231741340,"Notably, mitigating dialectal bias through current debiasing methods does not mitigate a model's propensity to label tweets by Black authors as more toxic than by white authors.",24,25
445,231741340,We additionally explore an alternative proof-ofconcept study-relabeling supposedly toxic training instances whose automatic translations into a majority dialect are deemed non-toxic by the classifier.,12,13
446,231741340,We additionally explore an alternative proof-ofconcept study-relabeling supposedly toxic training instances whose automatic translations into a majority dialect are deemed non-toxic by the classifier.,26,27
447,231741340,"Overall, our findings indicate that debiasing a model already trained on biased toxic language data can be challenging, compared to relabeling the data to remove existing biases.",13,14
448,231741340,"2 Biases in Toxic Language Detection We test the use of debiasing 3 methods for the task of toxic language detection, which aims to flag rude, offensive, hateful, or toxic language on the internet, with the goal of moderating online communities (Roberts, 2019; Vidgen et al.,",18,19
449,231741340,"2 Biases in Toxic Language Detection We test the use of debiasing 3 methods for the task of toxic language detection, which aims to flag rude, offensive, hateful, or toxic language on the internet, with the goal of moderating online communities (Roberts, 2019; Vidgen et al.,",33,34
450,231741340,"First, compared to these NLU tasks where there is one correct label, the toxicity of language is inherently more nuanced, subjective, and contextual, which causes toxic language datasets to have lower agreement in general (Ross et al.,",30,31
451,231741340,"2018) , whereas those in toxic language detection are grounded in the social dynamics of the world (Spears, 1998; Technau, 2018) .",6,7
452,231741340,"For example, viewing AAE as a more toxic or less proper variety of English is a form of linguistic discrimination that upholds racial hierarchies in the United States (Rosa and Flores, 2017) .",8,9
453,231741340,"In this work, we consider two broad categories of toxic language dataset biases-lexical ( §2.1) and dialectal ( §2.2).",10,11
454,231741340,"Lexical Biases (TOXTRIG) Current toxic language detection systems often rely on the presence or absence of certain words (e.g., swear words, identity mentions) to make their predictions (Dixon et al.,",6,7
455,231741340,"Note that the pragmatic interpretation of these words is not necessarily always toxic or offensive (Dynel, 2012) , as they are often used to convey closeness between the speaker and listener or emphasize the emotionality of a statement (e.g., second example in in Figure 1 ).",12,13
456,231741340,"Dialectal Biases (AAE) Current toxic language detection systems also associate higher toxicity with dialectal markers of African American English (AAE; Sap et al.,",6,7
457,231741340,"The tweets were collected using a multiround bootstrapping procedure, and were labeled out of context 8 for toxic language.",18,19
458,231741340,"We aggregate the abusive and hateful labels into a single toxic category, yielding 32k toxic and 54k non-toxic tweets.",10,11
459,231741340,"We aggregate the abusive and hateful labels into a single toxic category, yielding 32k toxic and 54k non-toxic tweets.",15,16
460,231741340,"We aggregate the abusive and hateful labels into a single toxic category, yielding 32k toxic and 54k non-toxic tweets.",20,21
461,231741340,Evaluation Framework We report the performance of all models as overall accuracy and F 1 with respect to the toxic class.,19,20
462,231741340,"Given that current hate speech systems tend to rely heavily on the presence of NOI, OI, and ONI mentions ( §2.1) for labeling text as toxic, we use false positive rate (FPR) over each of these categories to measure the degree of bias in the model, following Hardt et al. (",29,30
463,231741340,"They also reduce the rate of falsely predicting NOI mentions as toxic (FPR NOI ), while not showing much improvement for ONI and maintaining FPR OI of the original baseline.",11,12
464,231741340,"A denotes tweets with toxic labels, and represents non-toxic labels.",4,5
465,231741340,"A denotes tweets with toxic labels, and represents non-toxic labels.",11,12
466,231741340,"2019 ), which contains 3000 manually created sentences, 300 toxic.",11,12
467,231741340,"The toxic sentences barely contain any ONI mentions, e.g., Cheese is made by french people who smell.",1,2
468,231741340,Takeaway: F 1 (↑) measures show that all models perform poorly at identifying toxic text not containing overtly lexical cues of toxicity.,16,17
469,231741340,"Partly inspired by our qualitative analyses of debiased models' predictions, we design a proof-of-concept study where we automatically correct the label of tweets using a(n automatic) dialectal translation of the tweet, inspired by previous work showing that highlighting AAE tweets' dialect led them to be labeled as less toxic (Sap et al.,",57,58
470,231741340," indicates % of white users' tweets being flagged as toxic, AA-Tox.",11,12
471,231741340,"indicates % of African American users' tweets being flagged as toxic, ∆ refers to the difference between AA-Tox.",11,12
472,231741340,"Focusing on dialectal bias, our key assumption is that an AAE tweet and its corresponding WAE version should have the same toxicity label, therefore toxic AAE tweets whose WAE versions are non-toxic are candidates for label correction.",26,27
473,231741340,"Focusing on dialectal bias, our key assumption is that an AAE tweet and its corresponding WAE version should have the same toxicity label, therefore toxic AAE tweets whose WAE versions are non-toxic are candidates for label correction.",35,36
474,231741340,"Next, as per our heuristic, we only relabel toxic AAE tweets whose WAE translation is predicted as non-toxic by either our vanilla classifier trained on the original Founta et al. (",10,11
475,231741340,"Next, as per our heuristic, we only relabel toxic AAE tweets whose WAE translation is predicted as non-toxic by either our vanilla classifier trained on the original Founta et al. (",21,22
476,231741340,"The resulting dataset (AAE-relabeled) is the same size as the original dataset, but with 954 (12%) out of 8260 toxic AAE tweets relabeled as non-toxic (examples in Table 6 ).",27,28
477,231741340,"The resulting dataset (AAE-relabeled) is the same size as the original dataset, but with 954 (12%) out of 8260 toxic AAE tweets relabeled as non-toxic (examples in Table 6 ).",34,35
478,231741340,"Additionally, to ensure less biased toxicity labeling, we recommend recruiting AAE speakers or experts for avoiding over-identification of AAE-markers as toxic (Spears, 1998; Croom, 2013) .",26,27
479,231741340,"Second, pretrained language models are known to generate toxic language at non-trivial rates (Gehman et al.,",9,10
480,231741340,Related Work Debiasing Toxicity Detection As the popularity of hate speech and toxic language detection sys- A RT @user That nigga needs anger management RT @user That guy needs anger management A RT @user oh fucking hell take a day off man RT @user oh fuck take a day off man A A Conclusion We investigate whether toxic language detection systems can be debiased using recently introduced methods for debiasing text classification in NLU tasks.,12,13
481,231741340,Related Work Debiasing Toxicity Detection As the popularity of hate speech and toxic language detection sys- A RT @user That nigga needs anger management RT @user That guy needs anger management A RT @user oh fucking hell take a day off man RT @user oh fuck take a day off man A A Conclusion We investigate whether toxic language detection systems can be debiased using recently introduced methods for debiasing text classification in NLU tasks.,57,58
482,231741340,This indicates that biases in toxic language detection might be different in nature compared to spurious associations studied in typical NLU settings.,5,6
483,231741340,Then we further downsample the dataset to 33% of the original data to control that each training set has the same toxic ratio as the original training set.,22,23
484,231741340,This step is to avoid confounding our results with different toxic ratio among different training sets.,10,11
485,231741340,"Specifically, as shown in Table 7 , the easy region demonstrates least spurious correlation due to its heavily skewed class distribution, which further prevent us from downsampling to control the toxic ratio.",32,33
486,196211238,"Thus, great care is needed when developing automatic toxic language identification tools.",9,10
487,196211238,"The task is especially challenging because what is considered toxic inherently depends on social context (e.g., speaker's identity or dialect).",9,10
488,196211238,"Perspective is a tool from Jigsaw/Alphabet that uses a convolutional neural network to detect toxic language, trained on crowdsourced data where annotators were asked to label the toxicity of text without metadata.",16,17
489,196211238,"more toxic than general American English equivalents, despite their being understood as non-toxic by AAE speakers (Spears, 1998, see §2) .",1,2
490,196211238,"more toxic than general American English equivalents, despite their being understood as non-toxic by AAE speakers (Spears, 1998, see §2) .",15,16
491,196211238,"In this work, we first empirically characterize the racial bias present in several widely used Twitter corpora annotated for toxic content, and quantify the propagation of this bias through models trained on them ( §3).",20,21
492,196211238,"Our findings show that existing approaches to toxic language detection have racial biases, and that text alone does not determine offensiveness.",7,8
493,196211238,"2 Biases in Toxic Language Datasets To understand the racial and dialectic bias in toxic language detection, we focus our analyses on two corpora of tweets (Davidson et al.,",14,15
494,196211238,"For each of the two toxic language corpora, we train a classifier to predict the toxicity label of a tweet.",5,6
495,196211238,Figure 2 (middle and right) shows that the proportions of tweets classified as toxic also differ by group in these corpora.,15,16
496,196211238,Our findings corroborate the existence of racial bias in the toxic language datasets and confirm that models propagate this bias when trained on them.,10,11
497,196211238,"Conclusion We analyze racial bias in widely-used corpora of annotated toxic language, establishing correlations between annotations of offensiveness and the African American English (AAE) dialect.",12,13
498,196211238,"Full Instructions (Expand/Collapse) You will read a tweet, and describe whether it could be considered toxic/disrespectful, to you or to anyone.",20,21
499,196211238,"Additionally, certain words are usually less toxic when used by a minority (e.g., the word ""n*gga"" or the suffix ""-ass"" are considered harmless in African American English), therefore it's useful to know the dialect a tweet is in before labelling it for toxic content.",7,8
500,196211238,"Additionally, certain words are usually less toxic when used by a minority (e.g., the word ""n*gga"" or the suffix ""-ass"" are considered harmless in African American English), therefore it's useful to know the dialect a tweet is in before labelling it for toxic content.",52,53
501,196211238,dialect priming) Instructions Read a potentially toxic post from the internet and tell us why it's toxic (this should take approx.,7,8
502,196211238,dialect priming) Instructions Read a potentially toxic post from the internet and tell us why it's toxic (this should take approx.,18,19
503,196211238,"Full Instructions (Expand/Collapse) You will read a tweet, and describe whether it could be considered toxic/disrespectful, to you or to anyone.",20,21
504,196211238,"Note that certain words are usually less toxic when used by a minority (e.g., the word ""n*gga"" or the suffix ""-ass"" are considered harmless when spoken by Black folks), therefore it's useful to know the identity of a Tweeter before labelling it for toxic content.",7,8
505,196211238,"Note that certain words are usually less toxic when used by a minority (e.g., the word ""n*gga"" or the suffix ""-ass"" are considered harmless when spoken by Black folks), therefore it's useful to know the identity of a Tweeter before labelling it for toxic content.",52,53
506,196211238,Annotation instructions 1.a) Tell us whether this tweet seems toxic/hateful/disrespectful to you.,10,11
507,196211238,"1.b) Considering a wide set of perspectives, tell us whether this could be considered toxic/hateful/disrespectful to others.",16,17
508,196211238,"1.c) Background on our research project At the University of Washington, we're passionate about understanding how potentially toxic or disrespectful language or stereotypes can be used against certain demographics/groups of people (e.g. racism, sexism, etc.).",20,21
509,221878771,"Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment.",17,18
510,221878771,"We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration.",13,14
511,221878771,"We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration.",27,28
512,221878771,"Using REALTOXICI-TYPROMPTS, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts.",13,14
513,221878771,"We empirically assess several controllable generation methods, and find that while data-or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning ""bad"" words), no current method is failsafe against neural toxic degeneration.",27,28
514,221878771,"We empirically assess several controllable generation methods, and find that while data-or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning ""bad"" words), no current method is failsafe against neural toxic degeneration.",58,59
515,221878771,"To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et al.,",8,9
516,221878771,"2019) , and find a significant amount of offensive, factually unreliable, and otherwise toxic content.",16,17
517,221878771,Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.,8,9
518,221878771,"As illustrated in Figure 1 , they can easily degenerate into toxicity, even without explicitly toxic prompts, which hinders their safe deployment (McGuffie and Newhouse, 2020) .",16,17
519,221878771,We first introduce a framework to systematically measure the risk of toxic degeneration by pretrained LMs.,11,12
520,221878771,"We show that popular LMs produce toxic generations when conditioned on our prompts, even those that are non-toxic ( §4.2).",6,7
521,221878771,"We show that popular LMs produce toxic generations when conditioned on our prompts, even those that are non-toxic ( §4.2).",20,21
522,221878771,"Then, as a possible mitigation strategy, we evaluate controllable generation methods and quantify their ability to steer away from toxic content using REALTOXICITYPROMPTS ( §5).",21,22
523,221878771,"We find that certain controllable methods (e.g., toxicity control tokens, swearword filters) are less successful than more computationally or data-intensive methods (e.g., finetuning on non-toxic corpora).",34,35
524,221878771,"However, we show that even our best steering methods can still generate highly toxic content.",14,15
525,221878771,"We find non-negligible amounts of toxic, harmful, and abusive text in these corpora, which were used in pretraining of several language models (including RoBERTa, CTRL, and GPT-2; Liu et al.,",7,8
526,221878771,We release our code and data for tracking the progress towards combating the critical issue of neural toxic degeneration.,17,18
527,221878771,"1,2   2 Operationalizing Toxicity Characterizing the toxicity of large corpora of naturally occurring or machine generated text is crucial to understanding toxic degeneration by language models.",22,23
528,221878771,"Therefore, we rely on PERSPECTIVE API 3 , an automated tool for toxic language and hate speech detection.",13,14
529,221878771,"PERSPECTIVE API TOXICITY We use the TOXICITY 4 score from PERSPECTIVE API, a widely used, commercially deployed toxic-1 Due to their prevalence, we focus our study only on neural language models, and therefore use the term ""neural toxic degeneration.""",42,43
530,221878771,"In our analyses, we label a prompt as toxic if it has TOXICITY 0.5, and non-toxic otherwise.",9,10
531,221878771,"In our analyses, we label a prompt as toxic if it has TOXICITY 0.5, and non-toxic otherwise.",19,20
532,221878771,"Out-of-the-Box Generation Toxicity We focus our investigation of toxic degeneration in five popular autoregressive Transformer-based (Vaswani et al.,",14,15
533,221878771,"Unprompted Toxicity in Neural Models To quantify the risk associated with using pretrained language models for generation, we first measure their propensity to generate toxic output conditioned only on their respective start-ofsentence tokens.",25,26
534,221878771,"REALTOXICITYPROMPTS contains 22K prompts with TOXICITY 0.5 (i.e., toxic prompts).",11,12
535,221878771,"Prompted Toxicity in Neural Models Using REALTOXICITYPROMPTS and the same generation procedures outlined in §3, we measure toxic degeneration in out-of-the-box neural language models.",19,20
536,221878771,"These metrics characterize toxic generations along two axes: the higher the expected maximum toxicity, the more toxic we expect the worst-case generations to be, and the higher the toxicity probability, the more frequently the model generates toxicity.",3,4
537,221878771,"These metrics characterize toxic generations along two axes: the higher the expected maximum toxicity, the more toxic we expect the worst-case generations to be, and the higher the toxicity probability, the more frequently the model generates toxicity.",18,19
538,221878771,"Our results show that while toxic prompts unsurprisingly yield higher toxicity in generations, nontoxic prompts still can still cause toxic generations at non-trivial rates (Table 2 ).",5,6
539,221878771,"Our results show that while toxic prompts unsurprisingly yield higher toxicity in generations, nontoxic prompts still can still cause toxic generations at non-trivial rates (Table 2 ).",20,21
540,221878771,"Specifically, all five models have a toxicity probability near or above 0.5 for non-toxic prompts.",16,17
541,221878771,"This shows that even in innocuous contexts these models can still generate toxic content (as illustrated in Table 17 and 18 in Appendix §E), suggesting the need for models to ""unlearn"" toxicity.",12,13
542,221878771,"These results suggest that like the provenance of pretraining data ( §3.1), prompt context can heavily influence generation toxicity, and that steering generations after pretraining is crucial to prevent toxic behavior in language models.",33,34
543,221878771,"We describe hyperparameters and training details for all methods in Appendix §B. Data-Based Detoxification We 2020 ), we perform an additional phase of pretraining on the non-toxic subset of a balanced corpus with GPT-2.",32,33
544,221878771,"For comparison, we also perform the experiment using the toxic subset.",10,11
545,221878771,"2017) , we learn a 2-dimensional representation of toxicity and non-toxicity for every token in GPT-2's vocabulary, which we then use to boost the likelihood of non-toxic tokens.",35,36
546,221878771,"Listed in Table 3 , our results show that steering does not completely solve neural toxic degeneration, though all proposed techniques do reduce toxic behavior in GPT-2.",15,16
547,221878771,"Listed in Table 3 , our results show that steering does not completely solve neural toxic degeneration, though all proposed techniques do reduce toxic behavior in GPT-2.",24,25
548,221878771,"toxicity, highlighting the importance of pretraining data in neural toxic degeneration.",10,11
549,221878771,"14 From qualitative investigations, these prompts tended to either be toxic themselves, or if innocuous, they contain opening quotes or prefixes of multiword expressions such as ""full of-"" (Figure 1 ).",11,12
550,221878771,"Analyzing Toxicity in Web Text To further investigate the phenomenon of neural toxic degeneration, and partially motivated by the surprising effectiveness of domain-adaptive pretraining on non-toxic data, we turn our focus to two corpora used to pretrain several language models.",12,13
551,221878771,"Analyzing Toxicity in Web Text To further investigate the phenomenon of neural toxic degeneration, and partially motivated by the surprising effectiveness of domain-adaptive pretraining on non-toxic data, we turn our focus to two corpora used to pretrain several language models.",30,31
552,221878771,We consider a document toxic if its TOXICITY is 0.5.,4,5
553,221878771,We additionally display the estimated total % of toxic documents in each corpus above each subplot.,8,9
554,221878771,Bottom: Unreliable news sources in OWTC have a much higher proportion of toxic content.,13,14
555,221878771,"2018) , who find that the prevalence of abusive or toxic content online roughly ranges between 0.1% and 3%, and suggest that these corpora merely reflect the ""natural"" rates of toxicity.",11,12
556,221878771,"Table 4 : Examples of (purposefully uncensored) toxic documents that appear in GPT-2's training corpus, that were also submitted to quarantined or banned subreddits.",9,10
557,221878771,"2018) , we find that news reliability correlates negatively with the proportion of documents that are toxic (Spearman ⇢ = -0.35).",17,18
558,221878771,"As shown in Figure 4 , while low reliability news sites are less prevalent in OWTC, they contain more toxic documents compared to higher reliability news sites.",20,21
559,221878771,"Although they show some reduction in toxicity, steering methods do not fully protect neural models from toxic degeneration ( §5).",17,18
560,221878771,"Additionally, the corpora that language models are pretrained on contain non-negligible amounts of toxic, abusive, and untrustworthy content ( §6).",16,17
561,221878771,"Effectiveness of ""Forgetting"" Toxicity Our findings on data-based steering methods show that adaptive pretraining lowers a model's propensity to unpromptedly generate toxic language, but that its prompted generations can still be toxic.",26,27
562,221878771,"Effectiveness of ""Forgetting"" Toxicity Our findings on data-based steering methods show that adaptive pretraining lowers a model's propensity to unpromptedly generate toxic language, but that its prompted generations can still be toxic.",37,38
563,221878771,"This raises the question: can language models ever fully ""forget"" toxic pretraining data through further adaptation (Kirkpatrick et al.,",13,14
564,221878771,"2019) or that toxic examples may be more salient for the model and hence harder to unlearn (Koh and Liang, 2017) .",4,5
565,221878771,"2020) , which is among the most effective methods we tested at avoiding toxicity with toxic prompts.",16,17
566,221878771,"In addition to automated toxicity classifiers, future work could explore the use of handpicked toxic documents as ""negative examples"" to avoid toxicity in generation.",15,16
567,221878771,"First, analysis of pretraining data is a crucial first step towards understanding toxic, biased, or otherwise degenerate behavior of language models.",13,14
568,221878771,"2020) , or be evolving over time (e.g., using similarity to toxic online content).",14,15
569,221878771,"First, as noted in §2.2, we use an imperfect measure of toxicity that could bias the toxicity towards lexical cues, failing to detect more subtle biases and incorrectly flagging non-toxic content.",35,36
570,221878771,"2019) find universal adversarial triggers, nonsensical prompts that trigger toxic generations in GPT-2.",11,12
571,221878771,"In this work, we find and release naturally occurring prompts from web text that trigger toxicity, and compare toxic output in several language models.",20,21
572,221878771,"In our work, we study toxic degeneration by both out-of-the-box and controlled models using 100K naturally occurring prompts, including some that do not contain identity mentions (see Figure 1 ).",6,7
573,221878771,"Conclusion We introduce REALTOXICITYPROMPTS, a testbed of 100K prompts for evaluating the toxic degeneration in pretrained language models.",14,15
574,221878771,"We then analyze toxicity in two large web text corpora, including the GPT-2 pretraining corpus, to better understand the root cause of toxic generations.",24,25
575,207853290,"In addition, we include posts from three existing English Twitter datasets annotated for toxic or abusive language, filtering out @-replies, retweets, and links.",14,15
576,207853290,2018) find that the prevalence of toxic content online is <4%.,7,8
577,207853290,"Annotator agreement Overall, the annotations in SBIC showed 82.4% pairwise agreement and Krippendorf's α=0.45 on average, which is substantially higher than previous work in toxic language detection (e.g., α=0.22 in Ross et al.,",28,29
578,207853290,"Related Work Bias and toxicity detection Detection of hateful, abusive, or other toxic language has received increased attention recently (Schmidt and Wiegand, 2017) , and most dataset creation work has cast this detection problem as binary classification (Waseem and Hovy, 2016; Davidson et al.,",14,15
579,220265679,"To mitigate undesirable toxic or bias traits of large corpora, Blender (Roller et al.,",3,4
580,220265679,"Nevertheless, fine-tuning with BST conversations is essential to mitigate undesirable toxic traits of large corpora and emphasize desirable skills of human conversations.",13,14
581,244117167,"The perceived toxicity of language can vary based on someone's identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases.",23,24
582,244117167,"We disentangle what is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity.",6,7
583,244117167,"Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic.",26,27
584,244117167,"Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic.",35,36
585,244117167,"Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.",16,17
586,244117167,"Introduction Determining whether a text is toxic (i.e., contains hate speech, abuse, or is offensive) is inherently a subjective task that requires a nuanced understanding of the pragmatic implications of language (Fiske, 1993; Croom, 2011; Waseem et al.,",6,7
587,244117167,"Without this nuance, both humans and machines are prone to biased judgments, such as over-relying on seemingly toxic keywords (e.g., expletives, swearwords; Dinan et al.,",21,22
588,244117167,"For example, racial biases have been uncovered in toxic language detection where text written in African American English (AAE) is falsely flagged as toxic (Sap et al.,",9,10
589,244117167,"For example, racial biases have been uncovered in toxic language detection where text written in African American English (AAE) is falsely flagged as toxic (Sap et al.,",26,27
590,244117167,"The crux of the issue is that not all text is equally toxic for everyone (Waseem, 2016; Al Kuwatly et al.,",12,13
591,244117167,"We measure the effects of annotator identities (who annotates as toxic) and attitudes or beliefs (why they annotate as toxic) on toxicity perceptions, through the lens of social psychology research on hate speech, free speech, racist beliefs, altruism, political leaning, and more.",11,12
592,244117167,"We measure the effects of annotator identities (who annotates as toxic) and attitudes or beliefs (why they annotate as toxic) on toxicity perceptions, through the lens of social psychology research on hate speech, free speech, racist beliefs, altruism, political leaning, and more.",22,23
593,244117167,"Then, in our breadth-of-posts study, we simulate a typical toxic language annotation setting by collecting toxicity ratings for ∼600 posts, from a smaller but diverse pool of 173 annotators.",15,16
594,244117167,"2  Distilled in Figure 1 , our most salient results across both studies show that annotators scoring higher on our racist beliefs scale were less likely to rate anti-Black content as toxic ( §4).",34,35
595,244117167,"Additionally, annotators' conservatism scores were associated with higher ratings of toxicity for AAE ( §5), and conservative and traditionalist attitude scores with rating vulgar language as more toxic ( §6).",32,33
596,244117167,Our findings have immense implications for the design of toxic language annotation and automatic detection-we recommend contextualizing ratings in social variables and looking beyond aggregated discrete decisions ( §8).,9,10
597,244117167,"why they consider something toxic; §2.2) on specific categories of text (what they consider toxic; §2.3)-namely, text with anti-Black language, presence of African American English (AAE), and presence of vulgar or profane words.",4,5
598,244117167,"why they consider something toxic; §2.2) on specific categories of text (what they consider toxic; §2.3)-namely, text with anti-Black language, presence of African American English (AAE), and presence of vulgar or profane words.",18,19
599,244117167,2.1 Demographic Identities: Who considers something as toxic?,8,9
600,244117167,2.2 Attitudes: Why does someone consider something toxic?,8,9
601,244117167,Not all toxic text is toxic for the same reasons.,2,3
602,244117167,Not all toxic text is toxic for the same reasons.,5,6
603,244117167,"Specifically, we consider three dimensions or categories of text, based on recent work on text characteristics that tend to be over-or under-detected as toxic (Dinan et al.,",29,30
604,244117167,"Breadth-of-Posts Study Our second study focuses on collecting ratings for a larger set of posts, but with fewer annotators per post to simulate a crowdsourced dataset on toxic language.",32,33
605,244117167,We draw from two existing toxic language detection corpora to select 571 posts (Table 2 ).,5,6
606,244117167,"4 Who finds anti-Black posts toxic, and why?",7,8
607,244117167,"2019) or overt-which is often a desired target for toxic language detection research (Waseem, 2016; Vid-7 For each post, we collected toxicity ratings from two white conservative workers, two from white liberal workers, and two from Black workers.",12,13
608,244117167,"Based on prior work on linking conservative ideologies, endorsement of unrestricted speech, and racial prejudice with reduced likelihood to accept the term ""hate speech"" (Duckitt and Fisher, 2003; White and Crandall, 2017; Roussos and Dovidio, 2018; Elers and Jayan, 2020) , we hypothesize that conservative annotators and those who score highly on the RACISTBELIEFS or FREEOFFSPEECH scales will rate anti-Black tweets as less toxic, and vice-versa.",78,79
609,244117167,"Conversely, based on findings by Cowan and Khatchadourian (2003) , we hypothesize that annotators with high HARMOFHATESPEECH scores will rate anti-Black tweets are more toxic.",29,30
610,244117167,"In the context of toxicity annotation and detection, our findings highlight the need to consider the attitudes of annotators towards free speech, racism, and their beliefs on the harms of hate speech, for an accurate estimation of anti-Black language as toxic, offensive, or racist (e.g., by actively taking into consideration annotator ideologies; Waseem, 2016; Vidgen et al.,",46,47
611,244117167,"Who finds AAE posts toxic, and why?",4,5
612,244117167,"2012; Beneke and Cheatham, 2015 res, 2017), and is often mistaken as obscene or toxic by humans and AI models (Spears et al.,",19,20
613,244117167,"2015; Rosa, 2019) , we hypothesize that annotators who are white and who score high in RACISTBELIEFS will rate AAE posts as more toxic.",26,27
614,244117167,"2019) , partially explaining why AAE is perceived as toxic.",10,11
615,244117167,"Based on our results, future work in toxic language detection should account for this over-estimation of AAE as racist.",8,9
616,244117167,"clude speakers of AAE, or those who understand that AAE or its lexical markers are not inherently toxic, or are primed to do so (Sap et al.,",18,19
617,244117167,"Avoiding an incorrect estimation of AAE as toxic is crucial to avoid upholding racio-linguistic hierarchies and thus representational harms against AAE speakers (Rosa, 2019; Blodgett et al.,",7,8
618,244117167,"6 Who finds vulgar posts toxic, and why?",5,6
619,244117167,"2021) , determining what is toxic is subjective.",6,7
620,244117167,"2019) , we asked three fine-grained questions to annotators for each post in both our studies: • ""How toxic/hateful/disrespectful or offensive does this post seem to you?"" • """,23,24
621,244117167,"How much could this post be seen as toxic/hateful/disrespectful or offensive to anyone?"" • """,8,9
622,235097641,"Even more concerning, our evaluation finds that it often generates hateful and toxic language.",13,14
623,235097641,Our evaluation measures some risks of generative models -such as the tendency to generate toxic language -but more work in this area is needed.,14,15
624,235097641,"F.2 GPT3 often generates toxic language GPT3 often generates language that is broadly toxic, that our evaluation is able to pick up on.",4,5
625,235097641,"F.2 GPT3 often generates toxic language GPT3 often generates language that is broadly toxic, that our evaluation is able to pick up on.",13,14
626,235097641,"Though the other models can and do generate toxic language, we noticed it far more with GPT3's generations.",8,9
627,235097641,-Hide the strap-on She's just bi anyway Figure 18 : GPT3 often generates toxic language.,16,17
628,235097641,Slightly more helpful Figure 19 : GPT3 often generates toxic language.,9,10
629,235313967,"Figure 1 : Illustration of DEXPERTS, where a toxic LM acts as an ""anti-expert"" and a non-toxic LM acts as an ""expert"".",9,10
630,235313967,"Figure 1 : Illustration of DEXPERTS, where a toxic LM acts as an ""anti-expert"" and a non-toxic LM acts as an ""expert"".",23,24
631,235313967,"In this toy example, given the prompt, ""When she rejected his advance, he grabbed,"" the toxic LM assigns greater weight to ""her"" than ""his"", expressing subtle signals of toxicity that can be leveraged for effective attribute control.",21,22
632,235313967,"Moreover, we find that DEXPERTS continues to outperform baselines when employing only an antiexpert and re-using the base model as the expert, making it one of the only methods that can avoid toxicity without annotated examples of non-toxic content.",43,44
633,235313967,"In analysis, we also show that our method successfully avoids toxic degeneration while using just ""650 toxic comments, opening avenues for easily customizable anti-experts.",11,12
634,235313967,"In analysis, we also show that our method successfully avoids toxic degeneration while using just ""650 toxic comments, opening avenues for easily customizable anti-experts.",18,19
635,235313967,"Toxicity Avoidance Given that large pretrained LMs are at risk of producing toxic content (Sheng et al.,",12,13
636,235313967,"2020) , steering away from toxic ""degeneration"" is crucial for their safe deployment.",6,7
637,235313967,"Note that while obtaining an LM that is truly free from social biases is impossible (Fiske, 1993; Lakoff, 1973) , the ""non-toxic"" expert serves the purpose of modeling the same domain of comments as the toxic anti-expert, providing more effective contrast.",29,30
638,235313967,"Note that while obtaining an LM that is truly free from social biases is impossible (Fiske, 1993; Lakoff, 1973) , the ""non-toxic"" expert serves the purpose of modeling the same domain of comments as the toxic anti-expert, providing more effective contrast.",44,45
639,235313967,"Nonetheless, we provide an ablation using only a toxic anti-expert and show that it remains effective above all previous baselines.",9,10
640,235313967,"3  We consider an example toxic if ě 50% of annotators marked it as toxic, and nontoxic if none of the annotators mark it as toxic.",6,7
641,235313967,"3  We consider an example toxic if ě 50% of annotators marked it as toxic, and nontoxic if none of the annotators mark it as toxic.",16,17
642,235313967,"3  We consider an example toxic if ě 50% of annotators marked it as toxic, and nontoxic if none of the annotators mark it as toxic.",28,29
643,235313967,"This toxic dataset 3 https://bit.ly/3cvG5py has ""160K comments, and the nontoxic dataset ""1.4M comments.",1,2
644,235313967,Note that our toxic dataset is human-annotated and out-of-domain with respect to the pretraining corpus (WebText for GPT-2).,3,4
645,235313967,"We report results for α "" 2.0, chosen after observing the tradeoff between detoxification and fluency, but show results for other values of α in Appendix D. Evaluation Generation Prompts To evaluate the problem of toxic degeneration where a user might unexpectedly receive harmful output from a model, we use a random sample of 10K nontoxic prompts from the RealToxici-tyPrompts dataset (Gehman et al.,",37,38
646,235313967,2020) We further pretrain the base model on the non-toxic subset of OpenWebText.,12,13
647,235313967,"This dataset is obtained by scoring the full Open-WebText corpus with the toxicity classifier from Perspective API 4 and keeping the least toxic 2 percent of documents, a corpus of about 150K documents, or 63M tokens, following the implementation of this baseline from Gehman et al. (",24,25
648,235313967,"To be clear, we substitute z t "" z t in Equation 1, so that we have We use the toxic anti-expert based on GPT-2 Large and the same hyperparameter value α "" 2.0.",22,23
649,235313967,"P pX t | x ăt q "" softmax `p1 Non-Toxic Expert Finally, we consider generating directly from the non-toxic expert based on GPT-2 Large.",25,26
650,235313967,"Each comparison pair is rated by three Turkers, who select which of the two continuations is: (1) less toxic, (2) more fluent, and (3) more topical, i.e., whether the continuation is natural,  relevant, and follows logically from the prompt.",22,23
651,235313967,"A screenshot of the user interface is provided in Appendix C. Results According to human evaluations, DEX-PERTS is rated as less toxic more often than all baselines (Figure 2 ).",24,25
652,235313967,"In particular, it is rated equally fluent compared to GPT-2, yet less toxic than GPT-2 10% more often than the other way around.",14,15
653,235313967,"Analysis: Dataset Size In practice, gathering large amounts of toxic data may be challenging, especially in applications where we would want to customize the anti-expert LM for differing notions of harmful language.",11,12
654,235313967,"We finetune GPT-2 Large We can see that even with a dataset of 40,960 tokens (""650 comments) corresponding to ă 0.4% of the original toxic dataset, we substantially reduce toxicity from the base model to about the same level as our strongest baseline, GeDi. (",28,29
655,235313967,"2021) , specifically their tendency to generate hateful, offensive, or toxic content (Sheng et al.,",13,14
656,233864598,"Unfortunately, the very communities that are often the target of violent posts are also often wrongly flagged as posting toxic content themselves due to racial biases present in the training data (Sap et al.,",20,21
657,233864598,"Since content written in AAE has been shown to be flagged as toxic more often (Sap et al.,",12,13
658,236486232,An external large-scale dataset on toxic comment (ToxCom) classification is used as source domain.,7,8
659,236486232,"To assist the OLD task in target domain, we employ an external large-scale dataset on toxic comment (ToxCom) classification 3 as source domain.",18,19
660,238857348,"Due to noise in the keyword filtering approach to labeling climate-related NELA-GT headlines, we remove headlines with specific keywords referencing toxic work environments or political climates.",25,26
661,12913009,"Getting back to GST, let us consider a sentence, (1) The bailout plan was likely to depend on private investors to purchase the toxic assets that wiped out the capital of many banks.",27,28
662,12913009,Among possible compressions GST/g produces for the sentence are: (2) a. The bailout plan was likely to depend on private investors to purchase the toxic assets.,29,30
663,12913009,"Also depicted in the figure are four TDPs starting with many, the (preceding toxic), investors, and the (preceding bailout).",15,16
664,207869708,Detection and control of toxic output will be a major focus of future investigation.,4,5
665,221949376,Detection of some types of toxic language is hampered by extreme scarcity of labeled training data.,5,6
666,221949376,The efficacy of data augmentation on toxic language classification has not been fully explored.,6,7
667,221949376,"We present the first systematic study on how data augmentation techniques impact performance across toxic language classifiers, ranging from shallow logistic regression architectures to BERT -a state-of-the-art pretrained Transformer network.",14,15
668,221949376,"2018) , high class imbalance is a problem with certain classes of toxic language (Breitfeller et al.,",13,14
669,221949376,"Manual labeling of toxic content is onerous, hazardous (Newton, 2020) , and thus expensive.",3,4
670,221949376,The effectiveness of data augmentation for toxic language classification has not yet been thoroughly explored.,6,7
671,221949376,"On relatively small toxic language datasets, shallow classifiers have been shown to perform well (Gröndahl et al.,",3,4
672,221949376,"We focused on the threat class, but also replicated our results on another toxic class ( §4.6).",14,15
673,221949376,"Dataset We used Kaggle's toxic comment classification challenge dataset (Jigsaw, 2018) .",5,6
674,221949376,It contains human-labeled English Wikipedia comments in six different classes of toxic language.,13,14
675,221949376,"To confirm our results, we also applied the best-performing techniques on a different type of toxic language, the identity-hate class ( §4.6).",18,19
676,221949376,"We suggest this discrepancy may be due to WORDNET and PPDB relying on written standard English, whereas toxic language tends to be more colloquial.",18,19
677,221949376,"This demonstrates that GPT-2 significantly increased the vocabulary range of the training set, specifically with offensive words likely to be relevant for toxic language classification.",23,24
678,221949376,"However, there is a risk that human annotators might not label GPT-2-generated documents as toxic.",17,18
679,221949376,"Our results suggest that in certain types of data like toxic language, consistent labeling may be more important than wide coverage in dataset collection, since auto- mated data augmentation can increase the coverage of language.",10,11
680,221949376,"Alternative toxic class In order to see whether our results described so far generalize beyond threat, we repeated our experiments using another toxic language class, identity-hate, as the minority class.",1,2
681,221949376,"Alternative toxic class In order to see whether our results described so far generalize beyond threat, we repeated our experiments using another toxic language class, identity-hate, as the minority class.",23,24
682,221949376,"Similarly to their work, we generated novel toxic sentences from a language model.",8,9
683,221949376,"Our results show that data augmentation can increase coverage, leading to better toxic language classifiers when starting with very small seed datasets.",13,14
684,221949376,"A Class overlap and interpretation of ""toxicity""  To see if our results generalize beyond threat, we experimented on the identity-hate class in Kaggle's toxic comment classification dataset.",30,31
685,201070022,"Perhaps the most commonly studied class is hate speech, but work has also covered bullying, aggression, and toxic comments (Zampieri et al.,",20,21
686,201070022,"In order to benchmark toxic comment detection, The Wikipedia Toxic Comments dataset (which we study in this work) was collected and extracted from Wikipedia Talk pages and featured in a Kaggle competition (Wulczyn et al.,",4,5
687,201070022,"The best performing systems in each of the competitions mentioned above (for aggression and toxic comment classification) used deep learning approaches such as LSTMs and CNNs (Kumar et al.,",15,16
688,201070022,"We considered the version of the dataset that corresponds to the Kaggle competition: ""Toxic Comment Classification Challenge"" (Google, 2018) which features 7 classes of toxicity: toxic, severe toxic, obscene, threat, insult, identity hate and non-toxic.",32,33
689,201070022,"We considered the version of the dataset that corresponds to the Kaggle competition: ""Toxic Comment Classification Challenge"" (Google, 2018) which features 7 classes of toxicity: toxic, severe toxic, obscene, threat, insult, identity hate and non-toxic.",35,36
690,201070022,"We considered the version of the dataset that corresponds to the Kaggle competition: ""Toxic Comment Classification Challenge"" (Google, 2018) which features 7 classes of toxicity: toxic, severe toxic, obscene, threat, insult, identity hate and non-toxic.",48,49
691,201070022,"2018) , every label except non-toxic is grouped into a class OFFENSIVE while the non-toxic class is kept as the SAFE class.",8,9
692,201070022,"2018) , every label except non-toxic is grouped into a class OFFENSIVE while the non-toxic class is kept as the SAFE class.",19,20
693,229923179,"A board of data researchers has reviewed all the collected data to ensure no ethical concerns e.g., toxic language and hate speech.",18,19
694,237503515,"Subevent 3a A freight train in Lviv, Ukraine derailed, caught fire, and spilled a toxic chemical, releasing dangerous fumes into the air early Tuesday morning (local time), and people who live near the site of the crash are still becoming sick.",17,18
695,14456109,"One such system, reported in Brun and Hagège (2003) , detects paraphrases in texts about toxic products.",18,19
696,244119165,"Readers are invited to check that all ""toxic"" examples discussed in Sections 4.1 and 4.2 are handled correctly by CMS.",8,9
697,244119165," Even though CMS can handle all toxic examples described in Section 4, as a sanity check, we do a small evaluation of the three TransIns reinsertion strategies using the first evaluation document containing 40 sentences with inline markup.",7,8
698,218486942,"The platform also introduces a skew towards toxic, offensive language (Mohan et al.,",7,8
699,8306585,"In this case, the expanded query for the above question is: ((biological AND agents) OR (bacterial AND agent) OR (viral AND agent) OR (fungal AND agent) OR (toxic AND agent) OR botulism OR botulinum OR smallpox OR encephalitis OR (deploy)) AND (Qaeda) .",39,40
700,236459793,"2018) classify an entire comment (or document) to discern whether the comment is offensive or not, but cannot identify specific pieces of the toxic comment.",28,29
701,236459793,"2021) requires the identification of the specific toxic spans, which is more innovative and challenging, and a key step towards a successful semi-automatic review of comments.",8,9
702,236459793,"More formally, toxic span detection is an extraction task, which is usually formalized as a Sequential Labeling (SL) problem, as shown in Figure 1 (a), locating those spans by BIO tags.",3,4
703,236459793,"First, we mine a toxic lexicon from the training set by a simple statistical strategy.",5,6
704,236459793,"With a toxic lexicon, we extract toxic spans through word-level matching.",2,3
705,236459793,"With a toxic lexicon, we extract toxic spans through word-level matching.",7,8
706,236459793,"Methods In the section, we describe how toxic span detection is formalized and corresponding solutions in detail.",8,9
707,236459793,"Sequence Labeling The BIO tag scheme is utilized to locating toxic spans, where B (Begin) corresponds to the first token in a toxic span, I (Inside) corresponds to the inside and end tokens in a toxic span, and O corresponds to those no-toxic tokens.",10,11
708,236459793,"Sequence Labeling The BIO tag scheme is utilized to locating toxic spans, where B (Begin) corresponds to the first token in a toxic span, I (Inside) corresponds to the inside and end tokens in a toxic span, and O corresponds to those no-toxic tokens.",25,26
709,236459793,"Sequence Labeling The BIO tag scheme is utilized to locating toxic spans, where B (Begin) corresponds to the first token in a toxic span, I (Inside) corresponds to the inside and end tokens in a toxic span, and O corresponds to those no-toxic tokens.",41,42
710,236459793,"Sequence Labeling The BIO tag scheme is utilized to locating toxic spans, where B (Begin) corresponds to the first token in a toxic span, I (Inside) corresponds to the inside and end tokens in a toxic span, and O corresponds to those no-toxic tokens.",51,52
711,236459793,"Span Boundary Detection Different from SL formalization, SBD formalization utilizes the start and end positions tagging scheme to represent toxic spans.",20,21
712,236459793,"In detail, for k different models, if no less than k/2 models consider a character to be in the toxic span, the character is retained.",21,22
713,236459793,Lexicon-based Approach We also explore a lexicon-based approach for predicting toxic spans.,14,15
714,236459793,A toxic lexicon is mined from training data by a simple statistical strategy.,1,2
715,236459793,"More Specifically, the toxic score of a word w is defined as below: toxic score(w) = #w in toxic span #w in whole corpus , (8) where #w in toxic span is the count of appearances of word w in toxic spans, and #w in whole corpus is the count of appearances of word w in the whole corpus.",4,5
716,236459793,"More Specifically, the toxic score of a word w is defined as below: toxic score(w) = #w in toxic span #w in whole corpus , (8) where #w in toxic span is the count of appearances of word w in toxic spans, and #w in whole corpus is the count of appearances of word w in the whole corpus.",15,16
717,236459793,"More Specifically, the toxic score of a word w is defined as below: toxic score(w) = #w in toxic span #w in whole corpus , (8) where #w in toxic span is the count of appearances of word w in toxic spans, and #w in whole corpus is the count of appearances of word w in the whole corpus.",22,23
718,236459793,"More Specifically, the toxic score of a word w is defined as below: toxic score(w) = #w in toxic span #w in whole corpus , (8) where #w in toxic span is the count of appearances of word w in toxic spans, and #w in whole corpus is the count of appearances of word w in the whole corpus.",36,37
719,236459793,"More Specifically, the toxic score of a word w is defined as below: toxic score(w) = #w in toxic span #w in whole corpus , (8) where #w in toxic span is the count of appearances of word w in toxic spans, and #w in whole corpus is the count of appearances of word w in the whole corpus.",47,48
720,236459793,Then those words with a toxic score greater than a given threshold θ are selected from a lexicon.,5,6
721,236459793,"When predicting, the words in the sentence that appear in that toxic lexicon are extracted as the predicted toxic spans.",12,13
722,236459793,"When predicting, the words in the sentence that appear in that toxic lexicon are extracted as the predicted toxic spans.",19,20
723,236459793,"Since the lexicon-based approaches can only identify the toxic words in the lexicon, the recall can be improved by expanding the toxic lexicon.",10,11
724,236459793,"Since the lexicon-based approaches can only identify the toxic words in the lexicon, the recall can be improved by expanding the toxic lexicon.",24,25
725,236459793,2014) to ex- pand the toxic lexicon.,6,7
726,236459793,"In detail, we collect synsets of each toxic from WordNet, and collect the nearest similar words by calculating cosine similarity of GloVe vectors.",8,9
727,236459793,"Although the recall of two approaches improves over the original lexicon, the precision decreases significantly, which indicating that there are a considerable number of non-toxic words in the synonyms found through WordNet.",28,29
728,236459793,"Conclusion In this paper, we formalize the toxic span detection as two problems separately and employ three stateof-the-art models.",8,9
729,250390502,"This work describes the process of creating a corpus of Twitter conversations annotated for the presence of counterspeech in response to toxic speech related to axes of discrimination linked to sexism, racism and homophobia.",21,22
730,250390502,An annotation scheme was created to illustrate the relevant dimensions of toxic speech and counterspeech.,11,12
731,250390502,"Introduction Billions of users are active every day on the main social media platforms and they are regularly exposed to toxic discourse, i.e. speech that inflicts psychological or emotional harm and/or incites people to participate in bigoted practices ranging from sexism to homophobia, to racism.",20,21
732,250390502,Such measures are highly controversial and only targeted to the most extreme and explicit forms of toxic speech.,16,17
733,250390502,"Implicit toxic contents are particularly dangerous because they can go under the radar, they are hard to question, and may end up being accepted without conversation participants fully realizing it.",1,2
734,250390502,The question arises: how can we counter online toxic speech?,9,10
735,250390502,Recent studies in social philosophy of language investigated the strategy that consists in engaging in interventions aimed at avoiding that toxic contents get (wittingly or unwittingly) accepted by the conversation participants.,20,21
736,250390502,"In particular, they have focused on speech that counters implicit toxic contents by (i) spelling out, unpacking, articulating the objectionable contents implicitly conveyed by a given utterance and then (ii) challenging, questioning, rejecting, disputing, confronting it.",11,12
737,250390502,Studying counterspeech online comes with the added benefit of enabling the researcher to build computational models of language interactions involving toxic speech and counterspeech.,20,21
738,250390502,"By leveraging the most recent Natural Language Processing techniques, a corpus of counterspeech represents the first step towards automated systems to detect, support or even generate effective responses to toxic speech online.",31,32
739,250390502,For instance: do people on social networks ever employ such an idealized model where in order to reject implicit toxic content one has to first make explicit what was wrong with it?,20,21
740,250390502,"The contributions of this article can be summarized as follows: • A novel corpus of toxic speech and counterspeech in a conversational context from Italian social media, covering different target groups. •",16,17
741,250390502,A novel annotation schema encoding a finegrained classification of toxic speech and argumentative relations between utterances. •,9,10
742,250390502,"Related Work There is a growing concern among the ICT (Information and Communication Technologies) companies leading the development of Social Networks about toxic speech: as it can undermine the image of such social environments as ""safe"" place, they must implement methods to cut off this phenomenon (Mathew et al.,",24,25
743,250390502,"However, especially from a computational point of view, the development of corpora and models for the automatic detection and generation of counterspeech is still underdeveloped, while most of the efforts have been devoted to the detection of various forms of toxic speech, hate speech included (Poletto et al.,",43,44
744,250390502,"Most literature focuses on English language and considers toxic speech data collected from specific templates, which limits the coverage of explicit toxic speech and leaves out implicit toxic speech altogether.",8,9
745,250390502,"Most literature focuses on English language and considers toxic speech data collected from specific templates, which limits the coverage of explicit toxic speech and leaves out implicit toxic speech altogether.",22,23
746,250390502,"Most literature focuses on English language and considers toxic speech data collected from specific templates, which limits the coverage of explicit toxic speech and leaves out implicit toxic speech altogether.",28,29
747,250390502,"Racist, sexist and homophobic slurs count as systemic toxic discourse that generally worsens its targets' well being.",9,10
748,250390502,"Furthermore, note that toxic speech is not about impolite language or vulgar expressions: speech can be toxic and damage people's dignity without employing ""bad"" words.",4,5
749,250390502,"Furthermore, note that toxic speech is not about impolite language or vulgar expressions: speech can be toxic and damage people's dignity without employing ""bad"" words.",18,19
750,250390502,"Therefore, we call toxic speech the discourse that explicitly or implicitly expresses or promotes unjust discrimination on the basis of gender, ethnicity, geographical origin, sexual orientation, the presence of disabilities, and so on.",4,5
751,250390502,"The toxic speech label applies both to explicit and obvious cases, and to implicit and more difficult to grasp cases.",1,2
752,250390502,"What distinguishes toxic speech is that it implicitly or explicitly conveys content that contributes to extant social injustice, e.g., those due to sexism, homophobia, and racism.",2,3
753,250390502,Take for instance a scenario where one attacks their interlocutor with a racial insult: this is aggressive toxic speech.,18,19
754,250390502,"Then take a scenario where one claims that the members of a given group should not benefit from certain rights: this is toxic speech too because of its content, but it is not aggressive in the sense of the former.",23,24
755,250390502,What's the difference between toxic speech and counterspeech hostility?,5,6
756,250390502,expressed using a toxic language.,3,4
757,250390502,"In our framework, counterspeech is meant to be used to address toxic speech, rather than merely false speech.",12,13
758,250390502,"It is particularly interesting when it is exploited to address implicit rather then explicit toxic speech (speech conveying toxic contents via implications, presupposition, and the like): ""implicit toxic contents are particularly dangerous: they can go under radar, they are hard to question, and may end up being accepted in the common ground without conversation participants fully realizing it.",14,15
759,250390502,"It is particularly interesting when it is exploited to address implicit rather then explicit toxic speech (speech conveying toxic contents via implications, presupposition, and the like): ""implicit toxic contents are particularly dangerous: they can go under radar, they are hard to question, and may end up being accepted in the common ground without conversation participants fully realizing it.",19,20
760,250390502,"It is particularly interesting when it is exploited to address implicit rather then explicit toxic speech (speech conveying toxic contents via implications, presupposition, and the like): ""implicit toxic contents are particularly dangerous: they can go under radar, they are hard to question, and may end up being accepted in the common ground without conversation participants fully realizing it.",32,33
761,250390502,"Among the most promising candidates, we find its capabilities to change people's minds and raise awareness about discrimination in the toxic speaker and in the audience.",22,23
762,250390502,"We can also see that in the debate generated around these profiles there is often an attempt of countering toxic speech generated elsewhere (news, TV, etc).",19,20
763,250390502,"This is interesting because it allows us to analyze the phenomenon of toxic speech in social media (and its reactions) in more comprehensive way such as by investigating crossreferences between various media, and framing the overall debate in the context of a media ecosystem.",12,13
764,250390502,This latter includes social media but also others toxic information sources to be countered.,8,9
765,250390502,"Regarding the neutral class, this is represented by all those tweets and replies that are not classified as toxic, counterspeech and support to counterspeech.",19,20
766,250390502,"We hypothesize that in the case of this work, the disagreement on the main level of annotation (toxic/counterspeech) is dependent on the highly subjective nature of the annotation task.",19,20
767,250390502,"Evaluation We carried our a battery of experiments in order to perform three independent binary classifications: toxic vs. non-toxic speech, counterspeech vs. not counterspeech, and support to counterspeech vs. not support to counterspeech.",17,18
768,250390502,"Evaluation We carried our a battery of experiments in order to perform three independent binary classifications: toxic vs. non-toxic speech, counterspeech vs. not counterspeech, and support to counterspeech vs. not support to counterspeech.",21,22
769,250390502,Detecting toxic speech depends on how each subject is sensitive to detecting each axis of discrimination (which often varies along demographic and psychological factors).,1,2
770,250390502,"A further source of disagreement stems from the relative unconstrained character of the notions deployed (toxic speech and counterspeech) (Basile et al.,",16,17
771,250390502,"To address the dangers of toxic speech, Social Networks defined policies that regulate speech inciting hatred, while some countries started to introduce norms to treat this phenomenon as a crime and sentenced as such.",5,6
772,250390502,"All the data collected have been annotated, by exploiting a web-based annotation platform developed roughly from the scratch and published online 7 , where a group of expert annotators were applying a novel multi-layer annotation scheme devoted to mark whether the tweets or replies were counterspeech, toxic speech or in support of counterspeech (layer 1).",52,53
773,250390502,"Thus, the annotated corpus has been used for training the AlBERTo neural language model for performing a battery of binary classification task related to the detection of toxic, counterspeech, and support to counterspeech.",28,29
774,196184195,"2017) , toxic comments (Wulczyn et al.,",3,4
775,219307358,"That filthy Roma woman is even mocking, [I hope] they are all burned down by their toxic fires and croak quickly, NO TOLERANCE.) •",19,20
776,233365191,"We refine this corpus with 8 multi-labeled fine-grained classes, namely: toxic, insult, threat, identity hate, sexual, racial, blasphemy, and politically incorrect.",16,17
777,233365191,"We then annotate these comments using the following guidelines: (i) offensive if 3 or more classes of insult, toxic, threat, identity hate, and/or sexual are present, (ii) hate speech if the same previous conditions were satisfied, but with the additional requirement of the racial class having a label of ""1"" too (iii) comments with 0 values across all the classes are labeled as not offensive nor hate speech (iv) anything else that fails to satisfy any of the previous conditions is discarded.",22,23
778,239618387,"This paper describes our methods submitted for the GermEval 2021 shared task on identifying toxic, engaging and factclaiming comments in social media texts (Risch et al.,",14,15
779,239618387,"Introduction We present our systems submitted to the Ger-mEval 2021 shared task on identifying toxic, engaging and fact-claiming comments in social media texts (Risch et al.,",16,17
780,239618387,"Task and datasets The dataset of the 2021 shared task contains 3,244 comments from the Facebook page of a German news broadcast, from discussions between February and July 2019, manually annotated for three categories corresponding to the three subtasks: whether a comment is toxic, engaging, and/or fact-claiming.",46,47
781,239618387,Rule-based methods We explore simple strategies for both manual and semi-automatic generation of lists of words and phrases that can be used in rule-based systems that consider a comment toxic if and only if it contains any of the words or phrases in a list.,35,36
782,239618387,"We searched for patterns characteristic of comments labeled as toxic in the form of a few simple feature types, including unigrams and bigrams of words or lemmas, with or without part-of-speech tag for potential disambiguation purposes.",9,10
783,239618387,"For example, the top words in the training dataset for this year's Germeval task contain words like Hamburg and fleissig 'hard-working' because these words happened to occur in several comments labelled as toxic but none of the nontoxic ones, thereby getting ranked just as high as Dummheit 'stupidity' and Bullshit, terms that we actually want to keep for the edited list.",38,39
784,239618387,Two of the patterns we identified were introduced in the final rule-based system: for the toxicity detection task we categorize a comment as toxic if it contains at least two words with at least four characters each written in ALL-CAPS.,26,27
785,239618387,"Submission 2 is the union of Submission 1 and our rule-based systems, i.e. for each subtask we label comments as toxic/factclaiming if either the BERT-based model or our rule-based system would classify it as such (we did not use any rules for subtask 2).",23,24
786,239618387,"An asterisk (*) marks an agreement with the rule-based system, e.g. TP1 and TP2 were classified as toxic by both models and TP3-10 were classified as toxic by BERT but not by the rule-based system, and all ten have been labeled as toxic by the annotators (hence true positive).",22,23
787,239618387,"An asterisk (*) marks an agreement with the rule-based system, e.g. TP1 and TP2 were classified as toxic by both models and TP3-10 were classified as toxic by BERT but not by the rule-based system, and all ten have been labeled as toxic by the annotators (hence true positive).",33,34
788,239618387,"An asterisk (*) marks an agreement with the rule-based system, e.g. TP1 and TP2 were classified as toxic by both models and TP3-10 were classified as toxic by BERT but not by the rule-based system, and all ten have been labeled as toxic by the annotators (hence true positive).",52,53
789,239618387,"False negatives (FN) would be expected to exhibit the opposite pattern, these are comments that humans agreed are toxic but models failed to detect them as such.",21,22
790,239618387,We believe that it would be necessary to also consider the post fragments that annotators had access to but were not included in the dataset (see Section 2) to determine how such comments could have been unanimously labeled as non-toxic.,43,44
791,239618387,"Several FP examples, however, are clearly not toxic, and in case of BERT one can only speculate as to why they were falsely classified as such.",9,10
792,10738516,"Moreover, according to the etiology of the lesion and the disease associated with it (toxic, metabolic, traumatic or degenerative diseases), types of dysarthria vary with respect to pathophysiologies determining the kind of deficits in the motor execution and/or control of speech movements (deficits in speed, range, strength, rigidity/steadiness, tonus, precision/accuracy, and/or coordination) (Murdoch, 1998 , Duffy 2013) .",16,17
793,226283921,"Some studies use incivility interchangeably with hate speech, which refers to speech that aims to discriminate against a certain identity group, or aggressive or toxic language, which includes personal attacks (Rösner and Krämer, 2016) .",26,27
794,237593022,"2019) , toxic language detection (Breitfeller et al.,",3,4
795,237593022,"For instance, in toxicity classification, certain sub-groups are often predicted more confidently for toxicity (encouraging false negatives for the majority sub-group), which tend to be close to the margin for the non-toxic class (encouraging false positives; Borkan et al. (",42,43
796,233476611,2020) showed that many general-domain datasets include as much bias as datasets designed to be toxic and biased.,18,19
797,211259366,"The common data sources either derive from Wikipedia toxic comments (Dixon et al.,",8,9
798,237503419,"2017) , we found no instances of hate or toxic speech in the dataset.",10,11
799,238226547,The first phase aims to find words that are statistically overrepresented in tweets from specific countries that were deemed toxic by the model.,19,20
800,238226547,2008) to find if term i is statistically overrepresented in toxic tweets vs. non-toxic tweets in that country.,11,12
801,238226547,2008) to find if term i is statistically overrepresented in toxic tweets vs. non-toxic tweets in that country.,16,17
802,238226547,"However, common profanities are likely overrepresented in toxic tweets globally, so this initial list must be further filtered to identify geographically-specific terms.",8,9
803,238226547,We consider the term i is overrepresented in toxic tweets from a country j if i occurs with higher than statistically expected frequency in that country.,8,9
804,238226547,We use a balanced corpus with the same number of toxic tweets from each country (matching the one with the least number of tweets).,10,11
805,238226547,"By combining these two methods, we find culturally salient words that are overrepresented in toxic tweets in each country.",15,16
806,238226547,"As evident, our method is effective at picking up words that are geography-specific, compared to simply using log-odds to identify overrepresentation in toxic versus non-toxic tweets; such an approach outputs more general, widely used profanities that (by nature) occur more commonly in offensive messages across all countries.",28,29
807,238226547,"As evident, our method is effective at picking up words that are geography-specific, compared to simply using log-odds to identify overrepresentation in toxic versus non-toxic tweets; such an approach outputs more general, widely used profanities that (by nature) occur more commonly in offensive messages across all countries.",32,33
808,238226547,"Crucially, not all of the words identified by our method are inherently toxic: For example, the list for Pakistan contains words such as muslims and journalism, which should not carry a toxic connotation and could be reflective of model biases.",13,14
809,238226547,"Crucially, not all of the words identified by our method are inherently toxic: For example, the list for Pakistan contains words such as muslims and journalism, which should not carry a toxic connotation and could be reflective of model biases.",35,36
810,238226547,"Some words in this cluster are indeed not toxic, but we also see words such as congi that the model has not encountered before, but carry a negative connotation in Indian online discourse.",8,9
811,238226547,"Our analysis on Nigerian tweets does differ in this trend, however, where C0 is perceived to be more toxic than C3-and even slightly more than C2.",20,21
812,238226547,"In all countries, the level of offensiveness was still less than the level for extremely toxic words in C1, but still larger than C3.Figures for other countries are shown in the 3 Despite recruiting a large pool of annotators for each instance, our annotators may not entirely capture the value systems of countries due to selection effects of who has access to the internet and is able to participate in Qualtrics work.",16,17
813,238226547,"Here, as per the original guidelines provided by Jigsaw, we consider an instance with a rating of >= 0.5 as toxic.",23,24
814,238226547,"To quantify the effects of mitigation strategies, we further analyze 120 of these words that appear in at least 10 instances each for the toxic and non-toxic labels in the Jigsaw dataset.",25,26
815,238226547,"To quantify the effects of mitigation strategies, we further analyze 120 of these words that appear in at least 10 instances each for the toxic and non-toxic labels in the Jigsaw dataset.",29,30
816,238226547,1) Subgroup AUC reflects how separable are the toxic/non-toxic instances containing our target words. (,9,10
817,238226547,1) Subgroup AUC reflects how separable are the toxic/non-toxic instances containing our target words. (,13,14
818,238226547,2) Background Positive Subgroup Negative (BPSN) AUC measures how separable are non-toxic instances containing the target words from toxic instances without them; lower scores are typical of models with false positives. (,16,17
819,238226547,2) Background Positive Subgroup Negative (BPSN) AUC measures how separable are non-toxic instances containing the target words from toxic instances without them; lower scores are typical of models with false positives. (,23,24
820,238226547,3) Background Negative Subgroup Positive (BNSP) AUC measures how separable are non-toxic instances without the target words from toxic instances with them; lower scores are typical of models with many false negatives.,16,17
821,238226547,3) Background Negative Subgroup Positive (BNSP) AUC measures how separable are non-toxic instances without the target words from toxic instances with them; lower scores are typical of models with many false negatives.,23,24
822,238226547,"These metrics measure the Average Equality Gap (AEG) for toxic and non-toxic instances (separately) within [−0.5, 0.5], where scores close to 0 indicate no bias in how instances with the target word are treated.",11,12
823,238226547,"These metrics measure the Average Equality Gap (AEG) for toxic and non-toxic instances (separately) within [−0.5, 0.5], where scores close to 0 indicate no bias in how instances with the target word are treated.",15,16
824,238226547,"Balance and Tune: Finally, we use an approach that further fine-tunes the model using a subset of the dataset where each target word has an equal number of toxic and non-toxic instances.",32,33
825,238226547,"Balance and Tune: Finally, we use an approach that further fine-tunes the model using a subset of the dataset where each target word has an equal number of toxic and non-toxic instances.",36,37
826,238226547,"Formally, for m occurrences of the target word in instances with the toxic label and n instances with the nontoxic, we select max(k, min(m, n)) instances of each and fine-tune the model further only on these instances.",13,14
827,238226547,"Cluster C1 also has higher AEG metrics, but these are mostly toxic words that the model already recog-nizes.",12,13
828,238226547,"As such, models may contain biases by learning undesirable features that are not toxic or be blind to toxic language underrepresented in their training data.",14,15
829,238226547,"As such, models may contain biases by learning undesirable features that are not toxic or be blind to toxic language underrepresented in their training data.",19,20
830,238226547,"Additionally, our method surfaces words that have correlational bias due to overrepresentation in toxic messages.",14,15
831,6286716,"Likewise, in the TREC domain, 'air toxics such as benzene' can suggest that 'benzene' is a kind of 'air toxic'.",26,27
832,1506909,"The temporal references in meeting scheduling are somewhat more constrained than in news, where (e.g., in a historical news piece on toxic dumping) dates and times may be relatively unconstrained.",24,25
833,235258246,"This model was trained exclusively on uncurated web text and, therefore, (a) has a spurious understanding of Danish among other languages and (b) is particularly susceptible to the kind of toxic language identified by Gehman et al. (",36,37
834,235258246,"Models trained exclusively with such data quickly delve into generating toxic language (Gehman et al.,",10,11
835,9837420,"During the 2011 East Japan Earthquake and Tsunami Disaster, we had found a number of false information spread on Twitter, e.g., ""The Cosmo Oil explosion causes toxic rain.""",30,31
836,9837420,It is counterfactual that toxic rain will fall due to the Cosmo Oil explosion. •,4,5
837,9837420,Be aware of the false rumors that rain contaminated by the toxic substances produced by the Cosmo Oil explosion will fall.,11,12
838,9837420,"Step 3: Keyword clustering Misinformative phrases pertaining to the same information may differ considerably in wording and information quantity, as in ""Rain containing hazardous substances from the Cosmo Oil fire will occur"" and ""The Cosmo Oil explosion is toxic"", which must be consolidated to avoid redundancy when extracting misinformation.",43,44
839,15400760,Few apples are toxic. (),3,4
840,15400760,Few apples are toxic apples.,3,4
841,15400760,"The intuition here is that to know whether few apples are toxic, it is sufficient to know which apples are toxic; those non-apple toxicants are irrelevant.",11,12
842,15400760,"The intuition here is that to know whether few apples are toxic, it is sufficient to know which apples are toxic; those non-apple toxicants are irrelevant.",21,22
843,6772847,"However, false information, such as the popular Cosmo Oil explosion causing toxic rain, interfered with those looking to find correct information on the status of the disaster areas (Okazaki et al.,",13,14
844,244050420,"This raises the danger of reproducing existing biases in the dataset and the generation of toxic language, which questions the ethical behavior of abstractive summarization neural models.",15,16
845,244050420,"If during that training the model is presented with data containing toxic language, naturally, it will learn to generate the same language later on.",11,12
846,244050420,"A non-conservative assessment of part of the dataset used for the training of GPT-2, for example, shows that at least 50 000 sentences contain toxic language (Gehman et al.,",28,29
847,244050420,"The data-based strategies are considered more expensive in resources since they include a collection of specific non-toxic data, additional training and changes in the model parameters.",20,21
848,244050420,"A considerable liability in this regard is that by decreasing the generation of toxic language, the utility of the language models used by marginalized groups is also decreased (Xu et al.,",13,14
849,244050420,The unwanted side effect is that the minority dialects themselves are misidentified as toxic.,13,14
850,244050420,"In addition to this kind of strategies, Google and Jigsaw have a joint project (called Perspective 1 ), which uses machine learning to automatically detect toxic language.",28,29
851,244050420,"The PhilosopherAI shows the ability of such text generative models to sometimes improvise in a toxic way, not only when they are exposed to non-sense, like in the previous examples, but when they process valid human input.",15,16
852,247475839,src-A mr president let us hope that the american proposals for purchases of toxic assets do work because if they do not the contagion will almost certainly spread over here src-B what we really need to do is empower women 4 augm.,15,16
853,247475839,mr president let us hope that the american proposals for purchases of toxic assets do is empower women transl.,12,13
854,241583548,"ii) As a water-poor state, fracking and its toxic wastewater presents a serious danger to our communities and ecosystems.",12,13
855,241583548,From the input argument it concludes that Fracking and its toxic wastewater are a threat to the environmentfocusing on the negative environmental impact of fracking.,10,11
856,241583548,The alignment makes explicit that water wells and toxic wastewater stand in a correspondence in the context of fracking.,8,9
857,241583548,"Specifically, we see how the contamination of wells (left graphs) happens: wells are polluted with toxic wastewater (right graphs).",19,20
858,241583548,"Additionally, the left graph helps explain parts of the meaning of the right graph: Fracking and toxic wastewater are a threat because fracking contaminates water and water wells.",18,19
859,241583548,Fracking and its toxic wastewater are a threat to the environment.,3,4
860,241583548,"2) Fracking can contaminate water and water wells and suck towns dry.. As a water-poor state, fracking and its toxic wastewater presents a serious danger to our communities and ecosystems.. Infer Can we predict conclusion quality?",24,25
861,236486080,"With the rise of research on toxic comment classification, more and more annotated datasets have been released.",6,7
862,236486080,"Toxic Comment Datasets Supervised machine learning and more specifically supervised deep learning is the current state-of-theart for text classification in general and for toxic comment classification in particular (van Aken et al.,",27,28
863,236486080,The rather small sizes of annotated toxic comment datasets dates from the high costs for obtaining high-quality labels and the high variety of the task itself.,6,7
864,236486080,"In this paper, we present a software tool that provides easy access to many individual toxic comment datasets using a simple API.",16,17
865,236486080,The code is available in a GitHub repository 1 and also as a PyPI package 2 so that users can easily install it via the command pip install toxic-comment-collection and import datasets from the collection within python.,28,29
866,236486080,"Further, it fosters research on toxic comments and the development of robust systems for practical application.",6,7
867,236486080,"Does toxic content look different on different platforms (Twitter, Wikipedia, Facebook, news comments) 5.",1,2
868,236486080,Unified Toxic Comment Collection Creating a unified collection of toxic comment datasets comes with several challenges.,9,10
869,236486080,"The term denotes comments that contain toxic language and was made popular by the Kaggle Challenge on Toxic Comment Classification in 2018, which defined toxic comments as comments that are likely to make a reader leave a discussion.",6,7
870,236486080,"The term denotes comments that contain toxic language and was made popular by the Kaggle Challenge on Toxic Comment Classification in 2018, which defined toxic comments as comments that are likely to make a reader leave a discussion.",25,26
871,236486080,"For example, one mapping can be used to map all datasets in the collection to a binary classification task of toxic and non-toxic comments.",21,22
872,236486080,"For example, one mapping can be used to map all datasets in the collection to a binary classification task of toxic and non-toxic comments.",25,26
873,236486080,The next section describes the effect of this mapping on the toxic comment collection and other statistics of collection in the next section.,11,12
874,236486080,"As described in the previous section, a mapping can also be used to create a binary view on the collection with only two class labels: toxic and nontoxic.",27,28
875,236486080,"The labels idk/skip (73 comments) are discarded and all other labels are mapped to toxic (293,844 comments).",18,19
876,236486080,"In fact, most comment platforms contain only a tiny percentage of toxic comments.",12,13
877,236486080,"Since research datasets are collected with a focus on toxic comments, they can be biased in a significant way.",9,10
878,236486080,Figure 1 visualizes the overlap of the set of class labels used in the different datasets contained in the toxic comment collection.,19,20
879,236486080,"Conclusions and Future Work In this paper, we addressed three challenges that hinder accessibility of research datasets of toxic comments: retrieving the datasets, unifying their file formats, and mapping their class labels to a common subset.",19,20
880,236486080,"To overcome these challenges, we present the toxic comment collection, which does not contain the datasets themselves, but code that automatically fetches these datasets from their source and transforms them into a unified format.",8,9
881,236486080,"With the toxic comment collection, we aim to foster repeatability and reproducibility of research on toxic comments and to allow research on multilingual toxic comment classification by combining multiple datasets.",2,3
882,236486080,"With the toxic comment collection, we aim to foster repeatability and reproducibility of research on toxic comments and to allow research on multilingual toxic comment classification by combining multiple datasets.",16,17
883,236486080,"With the toxic comment collection, we aim to foster repeatability and reproducibility of research on toxic comments and to allow research on multilingual toxic comment classification by combining multiple datasets.",24,25
884,218974440,The by far largest shared task concerning the number of participants and the dataset size is the Kaggle challenge on toxic comment classification.,20,21
885,218974440,"Instead, the different shared tasks use varying terminology: hate speech, toxic comments, offensive language, abusive language, aggression, and misogyny identification.",13,14
886,59336862,"Such posts are called ""toxic"", because they poison a conversation so that other users abandon it.",5,6
887,59336862,"In moderated online discussions, it is the task of human moderators to identify toxic comments and potentially delete them.",14,15
888,59336862,An automatic identification of toxic posts could support (or even to some extent replace) the costly manual moderation of online discussions.,4,5
889,59336862,"For example, it could draw the attention of moderators to posts that have been automatically identified as toxic.",18,19
890,59336862,Another advantage of the automatic identification of toxic posts is that it allows the analysis of much larger datasets.,7,8
891,59336862,"One of the main limitation for research progress in the field of toxic comment classification is the low amount of available accurately labeled data (Kennedy et al.,",12,13
892,59336862,"A human moderator that decides whether to delete an automatically identified toxic comment might want to know a fine-grained reason, such as whether the comment contains an insult, a threat, or identity hate.",11,12
893,52310274,"Automatic classification of toxic comments, such as hate speech, threats, and insults, can help in keeping discussions fruitful.",3,4
894,52310274,In our work we focus on toxic comment detection and show that the same method can effectively be applied to a hate speech detection task.,6,7
895,52310274,"Besides traditional binary classification tasks, related work considers different aspects of toxic language, such as ""racism"" (Greevy and Smeaton, 2004; Waseem, 2016; Kwok and Wang, 2013) and ""sexism"" (Waseem and Hovy, 2016; Jha and Mamidi, 2017) , or the severity of toxicity (Davidson et al.,",12,13
896,52310274,"This is remarkable, considering that in real-world scenarios toxic comment classification can often be seen as a multi-label problem, with user comments fulfilling different predefined criteria at the same time.",11,12
897,52310274,We therefore investigate both a multi-label dataset containing six different forms of toxic language and a multi-class dataset containing three mutually exclusive classes of toxic language.,14,15
898,52310274,We therefore investigate both a multi-label dataset containing six different forms of toxic language and a multi-class dataset containing three mutually exclusive classes of toxic language.,28,29
899,52310274,"To our knowledge, the approach presented in this paper, combining both various model architectures and different word embeddings for toxic comment classification, has not been investigated so far.",21,22
900,52310274,"Datasets and Tasks The task of toxic comment classification lacks a consistently labeled standard dataset for comparative evaluation (Schmidt and Wiegand, 2017) .",6,7
901,52310274,"These comments were annotated by human raters with the six labels 'toxic', 'severe toxic, 'insult', 'threat', 'obscene' and 'identity hate'.",12,13
902,52310274,"These comments were annotated by human raters with the six labels 'toxic', 'severe toxic, 'insult', 'threat', 'obscene' and 'identity hate'.",17,18
903,52310274,"But they do state that they defined a toxic comment as ""a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion"".",8,9
904,52310274,"While the 'toxic' class includes 9.6% of the samples, only 0.3% are labeled as 'threat', marking the smallest class.",3,4
905,52310274,"This can be exploited when dealing with strongly imbalanced datasets, as often the case in toxic comment classification and related tasks.",16,17
906,52310274,We present the results on class 'toxic' of the Wikipedia dataset and class 'hate' of the Twitter dataset.,7,8
907,52310274,A common occurrence is actual toxic or hateful content that is cited by the comment's author.,5,6
908,52310274,Another pattern is the use of potentially toxic words within an explanation or self reproach.,7,8
909,52310274,"Example: ""No matter how upset you may be there is never a reason to refer to another editor as 'an idiot' "" We find that 23% of sampled comments in the false negatives of the Wikipedia dataset do not fulfill the toxic definition in our view.",46,47
910,52310274,We add this error class because we observe many cases of references to toxic or hateful language in actual non-hateful comments.,13,14
911,52310274,Such words (as described in Section 6) in non-toxic or non-hateful comments cause problems when the classifier misinterprets their meaning or when they are slang that is often used in toxic language.,12,13
912,52310274,Such words (as described in Section 6) in non-toxic or non-hateful comments cause problems when the classifier misinterprets their meaning or when they are slang that is often used in toxic language.,36,37
913,52310274,Conclusion In this work we presented multiple approaches for toxic comment classification.,9,10
914,52310274,Our error analysis on results of the ensemble identified difficult subtasks of toxic comment classification.,12,13
915,222271929,Hate speech and toxic comments are a common concern of social media platform users.,3,4
916,222271929,"Previous work in automatically detecting toxic comments focus mainly in English, with very few work in languages like Brazilian Portuguese.",5,6
917,222271929,"In this paper, we propose a new large-scale dataset for Brazilian Portuguese with tweets annotated as either toxic or non-toxic or in different types of toxicity.",20,21
918,222271929,"In this paper, we propose a new large-scale dataset for Brazilian Portuguese with tweets annotated as either toxic or non-toxic or in different types of toxicity.",24,25
919,222271929,An error analysis and experiments with multi-label classification show the difficulty of classifying certain types of toxic comments that appear less frequently in our data and highlights the need to develop models that are aware of different categories of toxicity.,18,19
920,222271929,"1 https://www.kaggle.com/c/ jigsaw-toxic-comment-classification-challenge/ overview 2 This is also similar to the usage of offensive comments in OffensEval (Zampieri et al.,",4,5
921,222271929,"On the other hand, since users feel somehow protected under their virtual identities, social media has also become a platform for hate speech and use of toxic language.",28,29
922,222271929,"In this paper, we focus on the analysis and automatic detection of toxic comments.",13,14
923,222271929,"Our definition of toxic is similar to the one used by the Jigsaw competition, 1 where comments containing insults and obscene language are also considered, besides hate speech.",3,4
924,222271929,2 Systems capable of automatically identifying toxic comments are useful for platform's moderators and to select content for specific users (e.g. children).,6,7
925,222271929,"Nevertheless, there are multiple challenges specific to process toxic comments automatically, e.g. (i) toxic language may not be explicit, i.e. may not contain explicit toxic terms; (ii) there is a large spectrum of types of toxicity (e.g. sexism, racism, insult); (iii) toxic comments correspond to a minority of comments, which is fortunate, but means that automatic data-driven approaches need to deal with highly unbalanced data.",9,10
926,222271929,"Nevertheless, there are multiple challenges specific to process toxic comments automatically, e.g. (i) toxic language may not be explicit, i.e. may not contain explicit toxic terms; (ii) there is a large spectrum of types of toxicity (e.g. sexism, racism, insult); (iii) toxic comments correspond to a minority of comments, which is fortunate, but means that automatic data-driven approaches need to deal with highly unbalanced data.",17,18
927,222271929,"Nevertheless, there are multiple challenges specific to process toxic comments automatically, e.g. (i) toxic language may not be explicit, i.e. may not contain explicit toxic terms; (ii) there is a large spectrum of types of toxicity (e.g. sexism, racism, insult); (iii) toxic comments correspond to a minority of comments, which is fortunate, but means that automatic data-driven approaches need to deal with highly unbalanced data.",29,30
928,222271929,"Nevertheless, there are multiple challenges specific to process toxic comments automatically, e.g. (i) toxic language may not be explicit, i.e. may not contain explicit toxic terms; (ii) there is a large spectrum of types of toxicity (e.g. sexism, racism, insult); (iii) toxic comments correspond to a minority of comments, which is fortunate, but means that automatic data-driven approaches need to deal with highly unbalanced data.",56,57
929,222271929,"4 915 A total of 21K tweets were manually annotated into seven categories: non-toxic, LGBTQ+phobia, obscene, insult, racism, misogyny and xenophobia.",17,18
930,222271929,This is then the largest dataset available for toxic data analysis in social media for the Portuguese language and the first dataset with demographic information about annotators.,8,9
931,222271929,"2019) for the binary task of automatically classifying toxic comments, since similar models achieve state-of-the-art results for the same task in other languages (Zampieri et al.,",9,10
932,222271929,"An error analysis is performed using our best model, where the worst-case scenario, i.e., classifying toxic comments as non-toxic, is further investigated, taking into account the fine-grained categories.",20,21
933,222271929,"An error analysis is performed using our best model, where the worst-case scenario, i.e., classifying toxic comments as non-toxic, is further investigated, taking into account the fine-grained categories.",25,26
934,222271929,Models trained with few examples are only accurate in predicting the majority class (non-toxic).,16,17
935,222271929,"As the number of instances grow, the performance on the minority class (toxic) improves significantly.",14,15
936,222271929,"2019) ), we focus the literature review on previous work related to toxic comments detection, the topic of our paper.",14,15
937,222271929,"We used two different strategies to select tweets for ToLD-Br, aiming to increase the probability of obtaining posts with toxic content, given that the volume of toxic tweets is significantly smaller than data without offensive language.",22,23
938,222271929,"We used two different strategies to select tweets for ToLD-Br, aiming to increase the probability of obtaining posts with toxic content, given that the volume of toxic tweets is significantly smaller than data without offensive language.",30,31
939,222271929,"We chose predefined terms highly likely to belong to a toxic tweet in Brazilian Twitter, such as gay (""Gay tem que apanhar"" -""Gay should be beaten up""), mulherzinha (""Mulherzinha, vai lavar louc ¸a"" -""Sissy, go wash dishes""), and nordestino (""Nordestino preguic ¸oso"" -""Lazy Northeastern"").",10,11
940,222271929,"Dataset characteristics For the purpose of training models for automatically classifying toxic comments, we must create aggregated annotations to provide only one binary label for each class.",11,12
941,222271929,"Likewise, if a tweet was not tagged in any of these categories, it is considered non-toxic.",19,20
942,222271929,"As mentioned before, we restrict our experiments on the dataset labelled as positive when at least one annotator considers the example as toxic.",23,24
943,222271929,Results and Discussion This section shows the results of our experiments in classifying toxic comments using ToLD-Br.,13,14
944,222271929,"Binary Classification For evaluating our models, we are particularly interested in models with high performance in the positive class (classification of toxic comments).",23,24
945,222271929,"The worst case scenario are false negatives, i.e. toxic comments classified as non-toxic.",9,10
946,222271929,"The worst case scenario are false negatives, i.e. toxic comments classified as non-toxic.",15,16
947,222271929,The idea is to identify which toxic classes are most difficult to be classified as toxic by our binary classifier.,6,7
948,222271929,The idea is to identify which toxic classes are most difficult to be classified as toxic by our binary classifier.,15,16
949,222271929,"As false negatives are a critical type of error in our application, Table 12 shows the false negative rate (false negatives / expected positives) for each toxic class.",29,30
950,222271929,The positive class of each label corresponds to a subset of the examples labelled as toxic.,15,16
951,222271929,"Concluding Remarks In this paper, we present ToLD-Br: a dataset for the classification of toxic comments on Twitter in Brazilian Portuguese.",18,19
952,222271929,"Through a wide and comprehensive analysis, we demonstrated the need for this dataset for studies on automatic classification of toxic comments.",20,21
953,218974083,"To train the classifiers, we use a dataset of toxic comments published by Google Jigsaw in the context of a Kaggle challenge.",10,11
954,218974083,"4 and the Dataset The toxic comments dataset contains about 220,000 comments, each labeled with regard to six non-exclusive classes: toxic, severe toxic, insult, threat, obscene, and identity hate.",5,6
955,218974083,"4 and the Dataset The toxic comments dataset contains about 220,000 comments, each labeled with regard to six non-exclusive classes: toxic, severe toxic, insult, threat, obscene, and identity hate.",24,25
956,218974083,"4 and the Dataset The toxic comments dataset contains about 220,000 comments, each labeled with regard to six non-exclusive classes: toxic, severe toxic, insult, threat, obscene, and identity hate.",27,28
957,218974083,Note that a comment is always labeled as toxic if one of the other labels applies.,8,9
958,218974083,"Even if none of the other labels apply, it can still be labeled as toxic.",15,16
959,218974083,"Heatmap Visualization To give an example of the explanations, Figure 1 and Figure 2 visualize the word relevance scores generated by the different explanation methods for two toxic comments.",28,29
960,218974083,"In Figure 1 , the naive Bayes classifier marks the words killed and fool as most relevant for the decision to classify this comment as toxic.",25,26
961,218974083,"In this particular context, the non-stemmed word is not toxic.",12,13
962,218974083,"In general, we find that the attention mechanism gives meaningful explanations for toxic comments.",13,14
963,218974083,"For non-toxic comments, however, its explanations can be misleading.",3,4
964,218974083,The attention mechanism distributes a relevance score of one among the wordseven if there is nothing toxic in the comment.,16,17
965,218974083,"To our surprise, the attention mechanism often marks punctuation as relevant in non-toxic comments.",15,16
966,218974083,"These sparse explanations are suitable for our dataset, as there is typically a small set of toxic words, which explains the toxicity of the entire comment.",17,18
967,218974083,"First, we compare the different classification approaches (naive Bayes, SVM, LSTM, and LSTM with attention mechanism) with regard to their classification performance on the toxic comments dataset.",30,31
968,218974083,"Classification Performance To evaluate the classification performance of the different classifiers, we use a multi-label classification task on the toxic comments dataset.",22,23
969,218974083,"The basic LSTM network and the LSTM network with attention mechanism overall achieve similar F1-score with larger differences in the less populated classes severe toxic, threat, and identity hate.",26,27
970,218974083,"For the following evaluation of explanation methods, we consider a binary classification task based on the toxic class label only.",17,18
971,218974083,"Given the set of true positives (toxic comments that are correctly identified as toxic), we use each explanation method to calculate word relevance scores for each comment.",7,8
972,218974083,"Given the set of true positives (toxic comments that are correctly identified as toxic), we use each explanation method to calculate word relevance scores for each comment.",14,15
973,218974083,"By deleting four words, more than 80% of the comments that were previously correctly classified as toxic (true positives) are classified as non-toxic.",18,19
974,218974083,"By deleting four words, more than 80% of the comments that were previously correctly classified as toxic (true positives) are classified as non-toxic.",28,29
975,218974083,"It also contains some of the more difficult samples of toxic comments, which are correctly classified by the LSTM but misclassified by the naive Bayes approach.",10,11
976,218974083,"However, when we further explored this idea, we found that this set is rather small and, more importantly, it contains only the most simple comments -the comments that all classifiers detect correctly as toxic.",37,38
977,218974083,A comment that contains a single swear word is easier to perturb to be classified as non-toxic than a comment that is toxic in its entirety.,18,19
978,218974083,A comment that contains a single swear word is easier to perturb to be classified as non-toxic than a comment that is toxic in its entirety.,24,25
979,218974083,We limit the dataset to all toxic comments and a random sample of non-toxic comments of the same size.,6,7
980,218974083,We limit the dataset to all toxic comments and a random sample of non-toxic comments of the same size.,15,16
981,218974083,The activation of a hidden state is the result of processing a subsequence of the input word sequence -regardless of the actual classification output (toxic/non-toxic).,25,26
982,218974083,The activation of a hidden state is the result of processing a subsequence of the input word sequence -regardless of the actual classification output (toxic/non-toxic).,29,30
983,218974083,A limitation of attribution-based explanations for offensive language detection seems to be a focus on words that are toxic regardless of the context.,20,21
984,218974271,"Often, this type of abusive propaganda spreads through either by the content posted in the platforms or by the toxic behavior of users in the social media.",20,21
985,219179484,"2018) ; toxic content (Georgakopoulos et al.,",3,4
986,199529989,"Some of them (such as, e.g., 'men') may look neutral at the first glance, but, in reality, they group messages whose vocabulary and language style reflect negative expectations towards the corresponding collective (in the case of men those expectations reflect toxic masculinity norms).",50,51
987,229923220,"Other research shows that some identity terms (e.g. 'gay') are substantially more likely to appear in toxic content in training datasets, leading models to overfit on them (Dixon et al.,",20,21
988,229923220,2019) ask crowd-workers to 'break' a BERT model trained to identify toxic comments and then retrain it using the new examples.,16,17
989,229923220,"It comprises 468, 928 entries, of which 22% are hateful/toxic.",14,15
990,229923220,"Negativity against others Statements which attack, criticise or express negativity against something that is NOT an identity -and the targets are not identified elsewhere in this typology, e.g. 'the air round here is toxic, it smells like terrible'.",36,37
991,4551282,"3 -RRB-they are also a great way to dispose of waste : it has to go somewhere , it can be toxic to plants , and toilets take up a negligibly larger amount of space than a bucket , which then requires `maintenance ' every time it needs emptied .",23,24
992,233189569,Online misogyny is a pernicious social problem that risks making online platforms toxic and unwelcoming to women.,12,13
993,219310297,"Use of offensive language and hate speech on social media can be an indication of hate crimes, toxic environment or level of antagonism against individuals or particular groups.",18,19
994,250390718,"Sometimes the standalone image or text is not necessarily hateful or toxic, but when combined together, the semantic meaning becomes harmful.",11,12
995,227231522,"This might include hate speech, derogatory language, profanity, toxic comments, racist and sexist statements.'",11,12
996,17411646,"On the contrary, cocaine has a toxic effect.'",7,8
997,239009716,We look forward to furthering progress in the detection and control of toxic outputs.,12,13
998,51879040,Common adverse drug reactions (ADRs) related to anti-MM treatment include hematologic toxic effects (eg.,15,16
999,51879040,"anemia, neutropenia and thrombocytopenia), thrombosis, impaired immune function, pe-ripheral neuropathy, and gastrointestinal toxic effects (eg.",20,21
1000,223957053,"Example for topic ""Fracking"", Argument A: ""And the toxic chemicals associated with fracking operations can contaminate the soil, air and water, and leach into crops"".",13,14
1001,223957053,"Argument B: ""The chemicals used in fracking are toxic and threaten to poison and pollute our air, ground, water and food supplies -basic necessities for life"". •",10,11
1002,248780050,3 For this competition we created a Russian parallel corpus of toxic sentences and their manually written nontoxic equivalents.,11,12
1003,248780050,"2022) There, the crowd workers were asked to rewrite a sentence so that it preserves its content, but does not sound toxic.",24,25
1004,248780050,Toloka has a special mark for cases of inappropriate and toxic content.,10,11
1005,248780050,"As it was noted, we need the toxic and corresponding neutral sentences to be semantically similar.",8,9
1006,248780050,"We use Russian toxic sentences from the corpora of user utterances taken from Russian social networks Odnoklassniki (Kaggle, 2019) and Pikabu (Kaggle, 2020) , and from the Russian segment of Twitter (Rubtsova, 2015) .",3,4
1007,248780050,We select only the sentences which were classified as toxic by a pretrained toxicity classifier.,9,10
1008,248780050,Delete Delete is an unsupervised method that eliminates toxic words based on a predefined toxic words vocabulary.,8,9
1009,248780050,Delete Delete is an unsupervised method that eliminates toxic words based on a predefined toxic words vocabulary.,14,15
1010,248780050,Team 4 (ruGPT3-XL) trained RuGPT3 XL 10 to generate a non-toxic text on the competition train data.,16,17
1011,248780050,The input is the concatenation of the toxic and non-toxic sentences.,7,8
1012,248780050,The input is the concatenation of the toxic and non-toxic sentences.,11,12
1013,248780050,The logistic regression model on the FastText vectors trained on the competition data was used as a toxic words classifier.,17,18
1014,248780050,"Toxic tokens were substituted by RoBERTa-large model, where the best candidates were chosen by the cosine similarity between the candidate and the toxic token.",25,26
1015,248780050,"In case it was not possible to find an acceptable candidate, the toxic word was removed from the sentence.",13,14
1016,248780050,"20 candidates were generated for each toxic sentence, the best candidate was selected by the largest J-score metric.",6,7
1017,248780050,Toxicity (STA m ) The toxicity level is defined as: • non-toxic (1) -the sentence does not contain any aggression or offence.,15,16
1018,248780050,"Even if a sentence is extremely informal, it is non-toxic unless it attacks someone. •",12,13
1019,248780050,toxic (0) -the sentence contains open aggression and/or swear words (this also applies to meaningless sentences).,0,1
1020,248780050,"It should also be noted that content and toxicity dimensions are independent, so if the output sentence is toxic, it can still be good in terms of content. •",19,20
1021,248780050,"We see that automatic and manual toxicity scores are much better correlated for the Delete and RoBERTareplace models, which are the only models to explicitly remove or replace toxic words identified by a classifier or via a manually compiled list of toxic words.",29,30
1022,248780050,"We see that automatic and manual toxicity scores are much better correlated for the Delete and RoBERTareplace models, which are the only models to explicitly remove or replace toxic words identified by a classifier or via a manually compiled list of toxic words.",42,43
1023,239618386,"Research into the identification of hate speech or toxic comment and fake news have recently become more popular in languages other than English because the abuse of free speech online and spread of information whether false or true extends farther than we can imagine (Vosoughi et al.,",8,9
1024,239618386,"2021) contains three subtasks not only aimed at identifying toxic comments in German text on social media platforms like in previous years (Struß et al.,",10,11
1025,239618386,"In a way to help the situation of diffusing toxic content and promote positive content moderators on popular social media platforms also seek to promote texts that engage other users in a healthy conversation (Welch et al.,",9,10
1026,239618386,"That is to say, a user comment can be either toxic, engaging, fact-claiming or any of 2 of the labels or all 3 or neither of the labels (see Figure 1 ).",11,12
1027,239618386,"Each of the 3 subtasks, that is, identifying toxic, engaging and fact-claiming comments were classified with the same ensemble models.",10,11
1028,238353985,"This paper describes our approach (ur-iw-hnt) for the Shared Task of GermEval2021 to identify toxic, engaging, and fact-claiming comments.",20,21
1029,238353985,An AI-based helper solution for harmful content detection is needed to make social networking less toxic and more pleasant instead.,17,18
1030,238353985,"We participated in all three subtasks (toxic, engaging and fact-claiming comment classification) to test our ensemble model to see whether multiple BERT-based models provide robust performance for different tasks without further customization.",7,8
1031,238353985,"A multilingual toxic text detection classifier uses a fusion strategy employing mBERT and XLM-RoBERTa on imbalanced sample distributions (Song et al.,",2,3
1032,238353985,"Such behavior only leads to users leaving the discussion or manual bans by the moderator, which can be overwhelming depending on the number of active toxic users (Risch and Krestel, 2020) .",26,27
1033,239618397,We investigate different ways of bolstering scarce training data to improve offthe-shelf model performance on a toxic comment classification task.,18,19
1034,239618397,"As surrounding conversation would mitigate a potentially toxic comment, exchange history would inform the determination of toxicity.",7,8
1035,239618397,"Therefore, an exchange history surrounding the potentially toxic comment is needed in the corpus.",8,9
1036,239618397,"There are indications that BERT embeddings may have racist or toxic tendencies (Zhang et al.,",10,11
1037,239618397,"2122 of the comments were labelled as 'nontoxic', and • 1122 were labelled 'toxic'. •",17,18
1038,239618397,The longest toxic comment had a length of 2035 tokens; • The longest non-toxic comment had 2833 tokens.,2,3
1039,239618397,The longest toxic comment had a length of 2035 tokens; • The longest non-toxic comment had 2833 tokens.,16,17
1040,239618397,"For example, a comment determined to be potentially toxic can be both harmless bickering within common groups and a toxic remark to outsiders.",9,10
1041,239618397,"For example, a comment determined to be potentially toxic can be both harmless bickering within common groups and a toxic remark to outsiders.",20,21
1042,10865063,"the contrast between medicine and poison that the correct answer involves a contrast, either useless vs. effective or curative vs. toxic.",21,22
1043,250390702,"Toxicity Modeling We have chosen to use the problem of ""toxic"" comment classification to illustrate the difficulty that we observed in distillation.",11,12
1044,250390702,For both sources a large quantity of data was scored with BERT and then examples were dropped to ensure a 50/50 distribution of toxic and nontoxic examples using a 0.5 threshold.,23,24
1045,250390702,"Since both sources are extremely non-toxic (0.004% and 0.00005% respectively), this process produced only 400k examples from WikiConv and 640k from C4.",7,8
1046,250390702,"Short Synthetic A synthetic test set created by substituting identity terms into toxic and non-toxic sentence templates (Dixon et al.,",12,13
1047,250390702,"Short Synthetic A synthetic test set created by substituting identity terms into toxic and non-toxic sentence templates (Dixon et al.,",16,17
1048,250390702,"2021) , where authors trained a generative LM specifically to not produce toxic content.",13,14
1049,250390702,"Human annotations marked far fewer examples as toxic than the automated models, and the authors note a strong bias towards false positives in this set.",7,8
1050,250390702,The False Positives dataset includes 50% autogenerated texts that had Perspective API scores > .75 but were marked by human raters as non-toxic and the rest as randomly selected auto-generated comments with corresponding human annotations.,25,26
1051,250390702,A small subset of curated phrases with explicit identity terms meant to detect hard toxic and non-toxic instances.,14,15
1052,250390702,A small subset of curated phrases with explicit identity terms meant to detect hard toxic and non-toxic instances.,18,19
1053,250390702,2021) we select an output label that is defined as the max of the covert and overt toxic scores.,18,19
1054,233365067,"Tools identifying toxic content are essential for moderation of problematic online conversations (Jigsaw, 2018) .",2,3
1055,233365067,"Generally, ""toxic"" is an umbrella label for various types of disrespectful comments, including but not limited to identity attacks, profanity and threats, that could encourage a user to leave a conversation.",3,4
1056,233365067,"In other words, it was easier for more people to agree on what constituted ""toxic"" speech-comments likely to make someone leave a conversation-than it was for people to agree on other terms to describe problematic comments.",16,17
1057,233365067,"Covertly toxic comments may use obfuscation, code words, suggestive emojis, dark humor, or sarcasm.",1,2
1058,233365067,"Our paper builds on previous work with a strategy for consistently rating covertly toxic content, a shared dataset for training, and a baseline covert toxicity model.",13,14
1059,233365067,"Covert Toxicity In order to formulate a covertly toxic dataset, we first extracted a set of candidate comments from the CivilCommentsIdentities dataset (Borkan et al.,",8,9
1060,233365067,"Microaggression Subtle discrimination towards an identity group • Obfuscation Hidden toxicity via intentional misspellings, coded words, or implied references • Emoticons/Emojis Toxic usage of non-text symbols • Sarcasm/Humor Offensive content in the context of a joke • Masked Harm Implied harm or threats masked by seemingly inoffensive language Crowdsourcing Data Collection We iterated on versions of an instruction templates to assist raters in identifying covertly toxic language with high precision.",73,74
1061,233365067,Overtly Offensive Rule Overt offenses occur when text has some words that are clearly toxic and requires no hidden meaning interpretation.,14,15
1062,233365067,"Can you think of a word/phrase that is clearly spelled or mis-spelled using toxic vocabulary, or is threatening?",17,18
1063,233365067,"In an initial iteration, a test question of the overtly toxic comment ""Peter is an idiot"", had a 43% miss rate by raters.",11,12
1064,233365067,COVERTTOXICITY COVERTTOXIC-ITY test set of ∼2000 comments with continuous rater fractions as covertly toxic label. •,15,16
1065,233365067,A positive covertly toxic is applied if the comment includes at least one type of identity attack in the annotations and negative otherwise.,3,4
1066,233365067,The comment should absolutely be labeled as toxic.,7,8
1067,233365067,2020) as checkpoint and jointly fine-tuned the covertly toxic and toxic labels.,11,12
1068,233365067,2020) as checkpoint and jointly fine-tuned the covertly toxic and toxic labels.,13,14
1069,233365067,The entire Civil-CommentsIdentities dataset was used for training toxicity with zero-weights assigned for missing covertly toxic labels.,19,20
1070,233365067,Toxic-Bert scored an average model probability for toxicity of 0.21 on comments with a majority of raters voting covertly toxic in the COVERTTOXIC-ITY test set.,21,22
1071,233365067,"Figure 3 illustrates the shift in model probabilities for covert comments in Covert-BERT, suggesting the model is more adept at identifying covertly toxic comments.",25,26
1072,233365067,This is in contrast to Toxic-BERT with ROC-AUC of 0.52 for the same covertly toxic labels.,18,19
1073,233365067,"While progress is still needed in extracting coherent rater signals and modelling, our initial work demonstrates the possibility of capturing veiled toxic language with machine learning models.",22,23
1074,244087332,"Another harmful effect is that the relative anonymity of social networks facilitates the propagation of toxic, hate and exclusion messages.",15,16
1075,236459815,"Encoder Settings Considering the underlying relation between humorous and offensive language observed in the Ha-Hackathon dataset (please, see Figure 1 ), the relation among offensiveness with other forms of toxic speech (e.g. aggressiveness and hate) presented in (Poletto et al.,",34,35
1076,218487043,"A) ""The green portions of a potato are toxic"".",10,11
1077,233365288,"six classes: toxic, severe toxic, obscene, threat, insult, and identity hate.",3,4
1078,233365288,"six classes: toxic, severe toxic, obscene, threat, insult, and identity hate.",6,7
1079,249204478,"Introduction Despite the progress in the fluency of machine translation (MT) systems, critical translation errors are still frequent, including deviations in meaning through toxic or offensive content, hallucinations, mistranslation of entities with health, safety, or financial implications, or deviation in sentiment polarity or negation.",27,28
1080,245855921,2020b) collected Wikipedia Edit comments with toxic content that could lead to possible tgt-regEMT Prism cushLEPOR(LM) C-SPECpn bleurt-20 MEE2 BERTScore chrF BLEU YiSi-1 COMET-QE-MQM_2021 COMET-MQM_2021 TER OpenKiwi-MQM YiSi-2 src-regEMT tgt-regEMT Prism cushLEPOR(LM) C-SPECpn bleurt-20 MEE2 BERTScore chrF BLEU YiSi-1 COMET-QE-MQM_2021 COMET-MQM_2021 TER OpenKiwi-MQM YiSi-2 src-regEMT (a) newstest2021 w/o HT C-SPECpn COMET-QE-MQM_2021 bleurt-20 OpenKiwi-MQM tgt-regEMT COMET-MQM_2021 Prism MEE2 cushLEPOR(LM) BERTScore chrF BLEU YiSi-1 TER YiSi-2 src-regEMT C-SPECpn COMET-QE-MQM_2021 bleurt-20 OpenKiwi-MQM tgt-regEMT COMET-MQM_2021 Prism MEE2 cushLEPOR(LM) BERTScore chrF BLEU YiSi-1 TER YiSi-2 src-regEMT (b) newstest2021 w/ HT COMET-MQM_2021 YiSi-1 bleurt-20 BLEU Prism BERTScore cushLEPOR(LM) MEE2 chrF C-SPECpn OpenKiwi-MQM COMET-QE-MQM_2021 tgt-regEMT TER YiSi-2 src-regEMT COMET-MQM_2021 YiSi-1 bleurt-20 BLEU Prism BERTScore cushLEPOR(LM) MEE2 chrF C-SPECpn OpenKiwi-MQM COMET-QE-MQM_2021 tgt-regEMT TER YiSi-2 src-regEMT (c) TED talks w/o HT Figure 1: The results of running PERM-BOTH hypothesis test to find a significant difference between metrics' pairwise system-level accuracy.,7,8
1081,245855921,We note that most of the sentences in this challenge set contain toxic language.,12,13
1082,237513354,"The good performance on held-out test sets seems to confirm the good generalization power of these models, although the inherent strong biases, sometimes leading to the use of a foul and toxic language, preserving stereotypes, etc.,",35,36
1083,219307991,"In the past years, toxic comments and offensive speech are polluting the internet and manual inspection of these comments is becoming a tiresome task to manage.",5,6
1084,219307991,Sites are trying to control the spread of these toxic comments by manually moderating and checking the reports that other users are filing.,9,10
1085,248780320,"It leads to offensive speech and causes severe social problems that can make online platforms toxic and unpleasant to LGBT+people, endeavoring to eliminate equality, diversity, and inclusion.",15,16
1086,248780320,"Identifying such information from social media would eliminate the severe societal problem and prevent formulating online platforms toxic unpleasant to LGBT+ people while also attempting to eliminate equality, diversity, and inclusion.",17,18
1087,248863142,"Formally, given two monostylistic corpora S1 and S2 of opposing styles, e.g. toxic and neutral, sentence pairs (s S1 ∈ S1, s S2 ∈ S2) are input to an encoder-decoder system, a transformer in our experiments.",14,15
1088,248863142,"To increase the distinction in our toxic and neutral datasets, we filter them using a list of slurs 7 such that the toxic portion contains only sentences with at least one slur, and the neutral portion does not contain any slurs in the list.",6,7
1089,248863142,"To increase the distinction in our toxic and neutral datasets, we filter them using a list of slurs 7 such that the toxic portion contains only sentences with at least one slur, and the neutral portion does not contain any slurs in the list.",23,24
1090,248863142,"Automatic Evaluation While 3ST can perform style transfer bidirectionally, we only evaluate on the toxic→neutral direc-tion of the civility task, as the other direction, i.e. generation of toxic content, would pose a harmful application of our system.",32,33
1091,248863142,"Attribute errors (14%) (Ex-2), where toxic content was not successfully removed, are another common source of error.",11,12
1092,248863142,"This is due to a negativity bias on the toxic side of the CivCo corpus, while the neutral side contains more positive sentences, thus introducing an incentive to Phenomena such as hallucinations can become amplified through back-translation (Raunak et al.,",9,10
1093,250390569,"Over the past few years, there has been a growing concern around toxic positivity on social media which is a phenomenon where positivity is used to minimize one's emotional experience.",13,14
1094,250390569,"In this paper, we create a dataset for toxic positivity classification from Twitter and an inspirational quote website.",9,10
1095,250390569,"The popularity of the term ""toxic positivity"" peaked during the COVID 19 pandemic (refer to figure 1 ) where it was used to identify advice that focused on just looking at the positive at a time when people were hurting due to loss of life, loss of jobs and other traumatic events.",6,7
1096,250390569,"Some examples of toxic positivity include telling someone to focus on the positive aspects of a loss, telling someone that positive thinking will solve all their problems, suggesting that things could be worse and shaming someone for expressing negative emotions.",3,4
1097,250390569,"The harms of toxic positivity are not only limited to its deleterious mental health outcomes but it can also be used to uphold oppression by making people ignore the oppression that is going on and encouraging them to ""just be positive"".",3,4
1098,250390569,"In this paper, we aim to create a dataset for toxic positivity and perform text classification using various transformer based models to establish the baseline results for this task.",11,12
1099,250390569,"Hence toxic positivity, with its overemphasis on thinking positively and having a positive state of mind, encourages emotion suppression rather than emotional acceptance which has negative consequences for the person who engages in it.",1,2
1100,250390569,Lecompte-Van Poucke (2022) conducted a critical discourse analysis of toxic positivity as a discursive construct on Facebook.,13,14
1101,250390569,"The study showed that users on social media platforms of-ten engage in toxic positivity or forced positive discourse which is inspired by the neoliberal ""positive thinking"" ideology, leading to a less inclusive online community.",14,15
1102,250390569,"However, to the best of our knowledge, there has been no prior work on creating datasets and classification models for toxic positivity.",22,23
1103,250390569,Dataset Annotation Two annotators annotated the data for toxic positivity.,8,9
1104,250390569,An annotation workshop was conducted for the annotators where they were sensitized to the topic of toxic positivity through academic works as described in the related works section and examples of toxic positivity.,16,17
1105,250390569,An annotation workshop was conducted for the annotators where they were sensitized to the topic of toxic positivity through academic works as described in the related works section and examples of toxic positivity.,31,32
1106,250390569,It was observed that sentences that had the following general characteristics were marked as toxic positive: • Encouraging hiding or suppressing negative emotions.,14,15
1107,250390569,"Dataset Statistics Out of the 4,250 sentences, 512 were annotated as toxic positive, which constitutes 12% of the dataset.",12,13
1108,250390569,Examples of toxic and non-toxic positive sentences are presented in Table 3 .,2,3
1109,250390569,Examples of toxic and non-toxic positive sentences are presented in Table 3 .,6,7
1110,250390569,It was also seen that 44% of the sentences that belonged to the affirmation category were toxic positive.,17,18
1111,250390569,"21% of the sentences belonging to the advice category were toxic positive, while 14% and 8% of sentences belonging to the personal experience and the worldview category respectively were toxic positive.",11,12
1112,250390569,"21% of the sentences belonging to the advice category were toxic positive, while 14% and 8% of sentences belonging to the personal experience and the worldview category respectively were toxic positive.",33,34
1113,250390569,"We noticed that in our dataset, most affirmation sentences were focused on emotion suppression, and hence they were marked as toxic positive.",22,23
1114,250390569,"The non-toxic positive affirmations focused on gratitude, having a growth mindset and self-acceptance, although they were fewer in number.",3,4
1115,250390569,"We got a Kappa score of 0.82 for the toxic positivity (toxic or non-toxic) annotation and a Kappa score of 0.74 for category annotations (worldview, advice, personal experience or affirmation).",9,10
1116,250390569,"We got a Kappa score of 0.82 for the toxic positivity (toxic or non-toxic) annotation and a Kappa score of 0.74 for category annotations (worldview, advice, personal experience or affirmation).",12,13
1117,250390569,"We got a Kappa score of 0.82 for the toxic positivity (toxic or non-toxic) annotation and a Kappa score of 0.74 for category annotations (worldview, advice, personal experience or affirmation).",16,17
1118,250390569,Toxic Positive Table 3 : Examples of toxic positive and non-toxic positive sentences in the dataset.,7,8
1119,250390569,Toxic Positive Table 3 : Examples of toxic positive and non-toxic positive sentences in the dataset.,12,13
1120,250390569,"As the toxic tweets comprise of only a small portion of the data (14.5%), models performing well on non-toxic tweets tend to have inflated weighted-F1 scores.",2,3
1121,250390569,"As the toxic tweets comprise of only a small portion of the data (14.5%), models performing well on non-toxic tweets tend to have inflated weighted-F1 scores.",24,25
1122,250390569,"Conclusion and Future Work In this work, we created a dataset for toxic positivity detection.",13,14
1123,250390569,We then annotated them and achieved a Kappa score of 0.82 for toxic positivity classification.,12,13
1124,250390569,"As more people turn to social media to get help when they are going through a tough time, it becomes important for them to be able to differentiate between positive and toxic positive messages.",32,33
1125,250390569,"Furthermore, being able to recognize toxic positivity is also important for chatbots and other automated systems that aim to provide mental health assistance.",6,7
1126,239618396,"In this work, we present our approaches on the toxic comment classification task (subtask 1) of the GermEval 2021 Shared Task.",10,11
1127,239618396,"As it is possible to interact almost anonymously on the internet, such social media pages are often confronted with the problem of hate speech and toxic comments targeting single persons or whole groups (Watanabe et al.,",26,27
1128,239618396,"2021) addresses this topic -especially the side of social media moderators that are responsible to filter such comments -in this years challenge with the following three tasks, where we participate in subtask 1: • Subtask 1: toxic comment classification • Subtask 2: engaging comment classification • Subtask 3: fact-claiming Over the last years transformer (Vaswani et al.,",40,41
1129,239618396,"2020 ) compared multiple transformers and neural networks for the classification of toxic content with different types of preprocessing steps, focussing on word embeddings.",12,13
1130,239618396,Those studies show that combining transformers that are fine-tuned for a specific NLP task with neural networks is a promising approach to create better models for predicting toxic comments.,29,30
1131,239618396,We assumed that the tasks of identifying hateful and offensive comments should be similar to the task of identifying toxic comments. •,19,20
1132,239618396,"For toxic comment classification we considered the word count for each input and extracted the number of punctuation, exclamation, and question marks and their relation to the total number of words per comment.",1,2
1133,239618396,We computed the mean values of each feature for both classes and found some significant differences between both categories: for example toxic comments are 22 words longer on average.,22,23
1134,239618396,"Besides the length, there is a notable difference in the number of exclamation marks and emojis between toxic and not toxic comments.",18,19
1135,239618396,"Besides the length, there is a notable difference in the number of exclamation marks and emojis between toxic and not toxic comments.",21,22
1136,239618396,"The augmented dataset contained a total of 24,304 comments, where 5,414 we set as toxic and 18,890 as not toxic as described in section 3.",15,16
1137,239618396,"The augmented dataset contained a total of 24,304 comments, where 5,414 we set as toxic and 18,890 as not toxic as described in section 3.",20,21
1138,239618396,Conclusion In this work we presented our submitted models for the GermEval Shared Task 2021 on toxic comment classification.,16,17
1139,250391102,"While this is a great opportunity for our society, it does not come without risks regarding toxic and offensive language.",17,18
1140,250391102,"The two main tasks to limit the amount of toxic language are detection and classification, e.g., to identify threats at an early stage or to effectively support criminal investigators in their work.",9,10
1141,250391102,"Therefore, in a second step, we noted 57 comments from 49 conversations that were annotated as hate speech (majority voting) or toxic (averaged toxicity annotation > 2.5).",25,26
1142,250391102,"The more it encourages aggressive responses or triggers other participants to leave the conversation, the more toxic the comment is.",17,18
1143,250391102,We introduced a scale of 1 (not toxic) to 5 (very toxic) to be able to model the impact of toxic comments on the conversation more accurately.,8,9
1144,250391102,We introduced a scale of 1 (not toxic) to 5 (very toxic) to be able to model the impact of toxic comments on the conversation more accurately.,14,15
1145,250391102,We introduced a scale of 1 (not toxic) to 5 (very toxic) to be able to model the impact of toxic comments on the conversation more accurately.,24,25
1146,250391102,"The goal was to avoid that all annotators, who tend to label comments more toxic than others, are in the same group, as this would bias the annotations.",15,16
1147,250391102,"Nevertheless, the percentage of toxic comments is with 9.63 % just a little lower than the hate speech proportion in the dataset.",5,6
1148,250391102,"In our analysis, we define all comments as offensive that are labelled as hate speech (majority voting) or toxic (averaged toxicity annotation > 2.5).",21,22
1149,250391102,"The results in Table 7 show, that the proportion of toxic comments in answers to offensive comments is with almost 6 % three times higher than in the random selection.",11,12
1150,233365142,"On the other hand, we are motivated by psychological and sociological studies, which correlate toxic behaviour online with the emotional profile of the user (Kokkinos and Kipritsi, 2012) .",16,17
1151,233365142,"2020) , where a similar behaviour is reported when training on toxic messages and Ask.fm as out-of-domain test set, and which is also evidenced by the high precision scores obtained for the models used in this paper when testing on the Dutch cyberbullying data.",12,13
1152,248779945,"For example, toxic messages are divided into different types of toxicity (Fortuna et al.,",3,4
1153,21698865,"Specifically, we chose to collect judgments on the toxicity of Wikipedia discussion comments, where a ""toxic"" comment is defined as any kind of hateful, aggressive, or disrespectful comment that is likely to make someone leave a discussion.",18,19
1154,21698865,Toxicity judgment is well-suited for the community-driven approach because toxic language is one component of the broader global problem of online harassment.,13,14
1155,21698865,"The user interface displays the Wikipedia comment, presents a range of rating options from very toxic to very healthy, supplies a box for optional comments, and lets the contributor submit the judgment.",16,17
1156,232092676,"In this work, we propose a technique to improve the interpretability of these models, based on a simple and powerful assumption: a post is at least as toxic as its most toxic span.",30,31
1157,232092676,"In this work, we propose a technique to improve the interpretability of these models, based on a simple and powerful assumption: a post is at least as toxic as its most toxic span.",34,35
1158,232092676,"To prevent this type of speech from jeopardizing others' ability to express themselves in online communities, many platforms prohibit content that is considered abusive, hate speech, or more generally, toxic.",34,35
1159,232092676,We base our technique on a simple and powerful assumption: A post is at least as toxic as its most toxic span.,17,18
1160,232092676,We base our technique on a simple and powerful assumption: A post is at least as toxic as its most toxic span.,21,22
1161,232092676,"In other words, the toxicity of a piece of text should be associated with the most toxic span identified in the text.",17,18
1162,232092676,"To this end, we propose using neural multi-task model that is trained on (1) toxicity detection over (a) When the input sequence is toxic, the toxicity of the most toxic span is picked to represent the toxicity of the sentence. (",30,31
1163,232092676,"To this end, we propose using neural multi-task model that is trained on (1) toxicity detection over (a) When the input sequence is toxic, the toxicity of the most toxic span is picked to represent the toxicity of the sentence. (",37,38
1164,232092676,"b) When the input sequence is not toxic, none of the spans are toxic, and thus the whole sequence is predicted as non-toxic.",8,9
1165,232092676,"b) When the input sequence is not toxic, none of the spans are toxic, and thus the whole sequence is predicted as non-toxic.",15,16
1166,232092676,"b) When the input sequence is not toxic, none of the spans are toxic, and thus the whole sequence is predicted as non-toxic.",27,28
1167,232092676,"In the linear layer, darker color denotes a more toxic span.",10,11
1168,232092676,"the entire piece of text and (2) toxic span detection (i.e., identifying individual tokens in the text that are toxic).",9,10
1169,232092676,"the entire piece of text and (2) toxic span detection (i.e., identifying individual tokens in the text that are toxic).",23,24
1170,232092676,Assumption We begin with the following assumption: A post is at least as toxic as its most toxic span.,14,15
1171,232092676,Assumption We begin with the following assumption: A post is at least as toxic as its most toxic span.,18,19
1172,232092676,"This assumption suggests that if there is a word or phrase in a piece of text that is toxic, i.e., with a level of toxicity that is over a certain threshold, the toxicity level of the entire text is certainly over such threshold, and, therefore, should be considered toxic.",18,19
1173,232092676,"This assumption suggests that if there is a word or phrase in a piece of text that is toxic, i.e., with a level of toxicity that is over a certain threshold, the toxicity level of the entire text is certainly over such threshold, and, therefore, should be considered toxic.",54,55
1174,232092676,"In such cases, there is often not a clearly identifiable span that is toxic.",14,15
1175,232092676,"Architecture & Methodology To detect the toxicity and learn the toxic spans at the same time, we propose to use a neural multitask learning framework (Caruana, 1997) .",10,11
1176,232092676,"For the span detection task, we directly leverage the output toxicity sequence s. This setup ensures that the model learns to predict the text as toxic if a span is toxic.",26,27
1177,232092676,"For the span detection task, we directly leverage the output toxicity sequence s. This setup ensures that the model learns to predict the text as toxic if a span is toxic.",31,32
1178,232092676,"For the purposes of training, let D 1 be the dataset for toxicity detection task, and D 2 be the dataset for toxic spans detection task.",24,25
1179,232092676,"We construct the loss of the model L to be the following: L = λ (x,y)∈D 1 L C (x, y) Loss for toxicity detection + (1 − λ) (x,y)∈D 2 L S (x, y) Loss for toxic spans detection (3) where L C is the loss for the toxicity detection task and L S is the loss for the toxic spans detection task.",52,53
1180,232092676,"We construct the loss of the model L to be the following: L = λ (x,y)∈D 1 L C (x, y) Loss for toxicity detection + (1 − λ) (x,y)∈D 2 L S (x, y) Loss for toxic spans detection (3) where L C is the loss for the toxicity detection task and L S is the loss for the toxic spans detection task.",77,78
1181,232092676,Here the toxicity detection task is a sequence classification task and the toxic spans detection task is a token classification task.,12,13
1182,232092676,Experiments We conduct experiments to answer the following research questions: RQ1 Does our aforementioned assumption affect the model's performance at detecting toxic content?,23,24
1183,232092676,"2019) are considered toxic based on the rating guidelines as published by the Perspective API (Nithum et al.,",4,5
1184,232092676,A toxicity score between zero and one of a post is the fraction of raters considering it to be toxic.,19,20
1185,232092676,"We further cast the scores to binary labels by setting a threshold of 0.5 (i.e., at least half of the raters consider the post toxic).",26,27
1186,232092676,"The dataset contains around 1.8 million posts in total, and 8% of them are labeled as toxic.",18,19
1187,232092676,"TSDD TSDD 2 is a 10,000-sample subset of CCD, containing only toxic comments, marked up with individual spans that are toxic.",14,15
1188,232092676,"TSDD TSDD 2 is a 10,000-sample subset of CCD, containing only toxic comments, marked up with individual spans that are toxic.",24,25
1189,232092676,528 posts have no annotated span since the annotators believe they are toxic as a whole without any explicit span.,12,13
1190,232092676,"We first sampled 7,000 highly toxic posts (toxicity score greater than 0.8) and 7,000 non-toxic posts (toxicity score less than 0.1) from CCD.",5,6
1191,232092676,"We first sampled 7,000 highly toxic posts (toxicity score greater than 0.8) and 7,000 non-toxic posts (toxicity score less than 0.1) from CCD.",18,19
1192,232092676,"Note that 3,000 of the toxic posts sampled were drawn from TSDD, which is still a subset of CDD, for the span annotations.",5,6
1193,232092676,"We further sampled another 8,000 ambiguous posts that have toxicity scores between 0.1 and 0.3 and contain terms that frequently appear in the toxic posts.",23,24
1194,232092676,"Terms annotated at least 20 times as part of toxic spans in TSDD are considered frequent, resulting in a list of 62.",9,10
1195,232092676,"We believe that these toxic terms are used in a non-toxic way in these posts, and, therefore, are good adversarial examples for the models to learn from the context instead of memorizing the frequent terms.",4,5
1196,232092676,"We believe that these toxic terms are used in a non-toxic way in these posts, and, therefore, are good adversarial examples for the models to learn from the context instead of memorizing the frequent terms.",12,13
1197,232092676,"To maintain an even proportion of toxic and non-toxic posts, we sample an additional 8,000 highly toxic posts.",6,7
1198,232092676,"To maintain an even proportion of toxic and non-toxic posts, we sample an additional 8,000 highly toxic posts.",10,11
1199,232092676,"To maintain an even proportion of toxic and non-toxic posts, we sample an additional 8,000 highly toxic posts.",19,20
1200,232092676,"It follows the architecture and methodology mentioned in Section 3.2, except only optimizing the toxic spans detection loss L S .",15,16
1201,232092676,"Since only a portion of the samples contains toxic span annotations, the models are only trained on that subset.",8,9
1202,232092676,"Beyond the toxic spans detection task, we also evaluate the toxicity detection performance for these models leveraging our proposed assumption, even though these models are not trained for toxicity detection.",2,3
1203,232092676,"Since not all the input posts have the labels for toxic spans, the multi-task models are trained by interleaving samples with and without span information.",10,11
1204,232092676,"Also, our proposed models tend to treat certain sub-words that frequently appear in toxic contexts as toxic spans.",16,17
1205,232092676,"Also, our proposed models tend to treat certain sub-words that frequently appear in toxic contexts as toxic spans.",19,20
1206,232092676,"For example, the sub-word ##nt of magnificient is predicted as toxic (0.979 4 ) in the following sentence: Should have had the magnificient Doug Ford stump for Smith....LOL This could be attributed to its high frequency in explicitly toxic words such as ignorant and arrogant.",15,16
1207,232092676,"For example, the sub-word ##nt of magnificient is predicted as toxic (0.979 4 ) in the following sentence: Should have had the magnificient Doug Ford stump for Smith....LOL This could be attributed to its high frequency in explicitly toxic words such as ignorant and arrogant.",47,48
1208,232092676,"Here, the model misclassifies it as toxic and picks the word brown as the most toxic span with a toxicity score of 0.527.",7,8
1209,232092676,"Here, the model misclassifies it as toxic and picks the word brown as the most toxic span with a toxicity score of 0.527.",16,17
1210,232092676,We find that the toxicity score of the sentence increases to 0.762 with the word brown still as the most toxic span.,20,21
1211,232092676,"Among all the cases that we have experimented with, the word brown is consistently the most toxic word in the sentence.",17,18
1212,232092676,"Though it might lead to false positives in rare cases, the predictive power that enables this phenomenon could potentially be the reason that our model is doing better in picking unseen words as toxic spans than the LR model since it infers the prediction without leveraging on the lexical information but the syntactic information.",34,35
1213,232092676,There are cases where the sentences can reasonably be considered as non-toxic where our model is also predicting as such.,13,14
1214,232092676,"LOL....I keep hearing Garland never got a hearing blah,blah,blah....It sucks being in the minority....Go win some elections.. When the headline reads ""Steve Bannon's porn and meth house"" In the first and second example, the speaker is not intended to be toxic even with the appearance of the word trolls and sucks.",55,56
1215,232092676,"Also, many users intentionally modify the explicitly toxic words (e.g., replacing or removing characters) to obfuscate automatic detection while still keeping their intent clear to human (Djuric et al.,",8,9
1216,232092676,"Here, non-toxic posts have toxicities between 0 and 0.1 and therefore are all considered to have no toxic spans; toxic samples are all from TSDD.",4,5
1217,232092676,"Here, non-toxic posts have toxicities between 0 and 0.1 and therefore are all considered to have no toxic spans; toxic samples are all from TSDD.",20,21
1218,232092676,"Here, non-toxic posts have toxicities between 0 and 0.1 and therefore are all considered to have no toxic spans; toxic samples are all from TSDD.",23,24
1219,232092676,User Study The labeled toxic spans used for the evaluation in Section 6.1 are not annotated to be the interpretation for the toxicity of the post.,4,5
1220,232092676,"Therefore, we select 400 toxic samples from the test split of Curated CCD and 237 offensive samples from the testing set of OLID for the interpretability study.",5,6
1221,232092676,"Annotators are asked to annotate for the toxicity (e.g., whether the sample is toxic) of the samples and pick the model with a better explanation.",15,16
1222,232092676,We also filter out 22 samples that both of the annotators considered non-toxic.,14,15
1223,232092676,BERT-MT can pick out the toxic spans more accurately by leveraging the context.,7,8
1224,232092676,"In comparison, the LR model suffers from picking words based on frequency even with a non-toxic usage, resulting in a bias toward some entities, such as certain groups of people (See example A in Table 7 ).",18,19
1225,232092676,BERT-MT also takes advantage of the WordPiece tokenization which re-  tains the toxic word pieces when users are altering words to avoid censorship; while LR lemmatizes the tokens and resulting in losing this information.,15,16
1226,232092676,"Take the example B in Table 7 for instance, the LR is able to pick out the word coward and clown which are considered to be non-toxic by BERT-MT.",29,30
1227,232092676,We conclude that BERT-MT is more cautious in predicting words as toxic.,13,14
1228,232092676,We find that both models are doing poorly if the sentence is implicitly toxic.,13,14
1229,232092676,Such indirect toxicity is carried either by negation (Example C in Table 7 ) or adversarially-modified toxic words (Example D in Table 7 ).,19,20
1230,232092676,We also plan to investigate methods that consider more subtle context and actors in the content to better distinguish different usages of the toxic terms.,23,24
1231,184482683,"Also, there has been published a couple of surveys covering various work addressing the identification of abusive, toxic, and offensive language, hate speech, etc.,",19,20
1232,184482683,"This data comprises tweets with positive and negative tags in six categories: toxic, severe toxic, obscene, threat, insult, identity hate.",13,14
1233,184482683,"This data comprises tweets with positive and negative tags in six categories: toxic, severe toxic, obscene, threat, insult, identity hate.",16,17
1234,184482683,We only used toxic and severe toxic positive samples as OFF and the ones with no positive label in any category as NOT.,3,4
1235,184482683,We only used toxic and severe toxic positive samples as OFF and the ones with no positive label in any category as NOT.,6,7
1236,207852561,"Extensive studies have demonstrated that DNNs are vulnerable to adversarial attacks, e.g., minor modification to highly poisonous phrases can easily deceive Google's toxic comment detection systems (Hosseini et al.,",25,26
1237,52158272,"Two conclusions can be drawn from these statistics: (i) abuse (a term we use henceforth to collectively refer to toxic language, hate speech, etc.)",23,24
1238,52158272,"We use the majority annotation of each comment to resolve its gold label: if a comment is deemed toxic (alternatively, attacking) by more than half of the annotators, we label it as abusive; otherwise, as non-abusive.",19,20
1239,236460063,"2020) , and toxic spans detection (Pavlopoulos et al.,",4,5
1240,236460063,2018) added non-toxic samples containing identity terms from Wikipedia articles into training data.,5,6
1241,235097522,"2015) and toxic (Zhang et al.,",3,4
1242,250390452,We curated the profanity scores for text using a pretrained model on toxic comment classification (Pearson coeff.,12,13
1243,248780292,"Lee ruda' showed how artificial intelligence systems can jeopardize sexual minorities by exposing them to toxic communication space (McCurry, 2021) .",16,17
1244,201669180,"2017) , English is still at the center of existing work on toxic language analysis.",13,14
1245,218974272,"1 Alongside such flattening in the process of production, consumption and sharing of information, toxic and abusive behavior online has surged.",16,17
1246,218974272,"This might include hate speech, derogatory language, profanity, toxic comments, racist and sexist statements.",11,12
1247,5822823,"Generally speaking, violations of long-span constituents have a more negative impact on performance than short-span violations if these violations are toxic.",25,26
1248,233181955,"We propose BERToxic, a system that fine-tunes a pre-trained BERT model to locate toxic text spans in a given text and utilizes additional post-processing steps to refine the boundaries.",18,19
1249,233181955,The post-processing steps involve (1) labeling character offsets between consecutive toxic tokens as toxic and (2) assigning a toxic label to words that have at least one token labeled as toxic.,14,15
1250,233181955,The post-processing steps involve (1) labeling character offsets between consecutive toxic tokens as toxic and (2) assigning a toxic label to words that have at least one token labeled as toxic.,17,18
1251,233181955,The post-processing steps involve (1) labeling character offsets between consecutive toxic tokens as toxic and (2) assigning a toxic label to words that have at least one token labeled as toxic.,24,25
1252,233181955,The post-processing steps involve (1) labeling character offsets between consecutive toxic tokens as toxic and (2) assigning a toxic label to words that have at least one token labeled as toxic.,36,37
1253,233181955,"Online platforms enable malicious actors to hide behind a cloak of anonymity and surreptitiously post toxic comments that are a ""menace to democratic values, social stability and peace"" (United Nations).",15,16
1254,233181955,This line of inquiry does not identify the toxic spans that ascribe a text as hate speech.,8,9
1255,233181955,2021) where systems are asked to extract the list of toxic spans that attribute to a text's toxicity.,11,12
1256,233181955,"15, 16, 17, 18, 19, 27, 28, 29, 30, 31] As there are two toxic spans in the above text, systems are tasked to extract the character offsets (zero-indexed) corresponding to the sequence of toxic words.",24,25
1257,233181955,"15, 16, 17, 18, 19, 27, 28, 29, 30, 31] As there are two toxic spans in the above text, systems are tasked to extract the character offsets (zero-indexed) corresponding to the sequence of toxic words.",49,50
1258,233181955,"The intentional obfuscation of toxic words, use of sarcasm and the subjective nature of hate speech further adds complexity to the problem.",4,5
1259,233181955,"First, we created a trivial model that randomly predicts each character offset of a text as toxic if its ρ > 0.5, drawn from a continuous uniform probability distribution.",17,18
1260,233181955,"BERToxic We framed the toxic spans detection task as a sequence labeling problem and leverage the Bidirectional Encoder Representation from Transformers (Devlin et al.,",4,5
1261,233181955,"We also stored the mapping M : t i → (start i , end i ) of each token to its relative character offsets in the original string, used for outputting the toxic span predictions at the post-processing stage.",34,35
1262,233181955,"A token classification head containing a linear layer was applied on top of the final hidden-states output, with a label prediction of 1 denoting a toxic token, 0 otherwise.",28,29
1263,233181955,"For each token t i labeled as toxic, we utilized M to output all character indices in the range of (start i , end i ) inclusive as the toxic span of this token.",7,8
1264,233181955,"For each token t i labeled as toxic, we utilized M to output all character indices in the range of (start i , end i ) inclusive as the toxic span of this token.",31,32
1265,233181955,"Consider the following tokenized sequence: t 1 , • • • , t i , t i+1 , t i+2 , • • • , t n First, for any two consecutive tokens t i and t i+1 whose prediction labels are toxic, we output the character indices in the range of (end i + 1, start i+1 − 1) inclusive as toxic as well.",44,45
1266,233181955,"Consider the following tokenized sequence: t 1 , • • • , t i , t i+1 , t i+2 , • • • , t n First, for any two consecutive tokens t i and t i+1 whose prediction labels are toxic, we output the character indices in the range of (end i + 1, start i+1 − 1) inclusive as toxic as well.",68,69
1267,233181955,"This had the effect of including the delimiter characters between consecutive toxic words, thereby detecting toxic phrases.",11,12
1268,233181955,"This had the effect of including the delimiter characters between consecutive toxic words, thereby detecting toxic phrases.",16,17
1269,233181955,"If at least one token was predicted toxic by the model, our system assigned a toxic label to all constituent tokens of this word.",7,8
1270,233181955,"If at least one token was predicted toxic by the model, our system assigned a toxic label to all constituent tokens of this word.",16,17
1271,233181955,"This achieved coherence in the prediction of toxic words and phrases, thus avoiding incomplete word piece issues.",7,8
1272,233181955,We also attempted to vary the thresholds of the confidence before SoftMax for toxic token predictions but observed no improvement in performance.,13,14
1273,233181955,"HateXplain's annotation strategy appeared to be different and included labeling pronouns, conjunctions and stop words as toxic when located between offensive words.",18,19
1274,233181955,We removed such toxic labels so that the external dataset annotation was more similar to this task.,3,4
1275,233181955,"When our task dataset was augmented with the full external dataset, the model experienced underfitting, while removing all the non-toxic labeled documents from the external dataset alleviated the issue to some extent.",23,24
1276,233181955,We reframed the problem as a binary classification task and trained a sequence classifier to predict whether a given sentence is toxic.,21,22
1277,233181955,"If a sentence contained a ground truth toxic span, we assigned the toxic class label 1, 0 otherwise.",7,8
1278,233181955,"If a sentence contained a ground truth toxic span, we assigned the toxic class label 1, 0 otherwise.",13,14
1279,233181955,We hypothesized that token labels should be predicted toxic only if the corresponding sentence was classified as toxic as well.,8,9
1280,233181955,We hypothesized that token labels should be predicted toxic only if the corresponding sentence was classified as toxic as well.,17,18
1281,233181955,"Late fusion was performed at the prediction phase, where both the sequence and token classifiers voted in the predictions by having the former model filter toxic sentences on which the latter model made final toxic span predictions.",26,27
1282,233181955,"Late fusion was performed at the prediction phase, where both the sequence and token classifiers voted in the predictions by having the former model filter toxic sentences on which the latter model made final toxic span predictions.",35,36
1283,233181955,"As the original dataset contained only document-level class labels, the task organizers selected a subset of the data for crowdsourced toxic spans annotation.",23,24
1284,233181955,"For a document d, define S d as the set of toxic character offsets predicted by a system and G d as the set of ground truth annotations.",12,13
1285,233181955,"We hypothesize that this is because the models have an inductive bias to predict shorter toxic spans, evidenced by the average ground truth span length of 7.2 in the test set and 14.7 in the dev set.",15,16
1286,233181955,"Model Our proposed system performed well at the toxic spans detection task, showing strength in identifying profanity and common toxic words like ""idiot"" and ""stupid"".",8,9
1287,233181955,"Model Our proposed system performed well at the toxic spans detection task, showing strength in identifying profanity and common toxic words like ""idiot"" and ""stupid"".",20,21
1288,233181955,The error analysis revealed that the system lacked nuance as it would sometimes classify toxic words used in neutral contexts (Example 2).,14,15
1289,238857078,"Many studies have shown that DNNs are vulnerable to adversarial attacks, e.g., slight modifications to poisonous phrases can easily cheat Google's toxic comment detection systems (Hosseini et al.,",24,25
1290,237579508,We present our submission to the first subtask of GermEval 2021 (classification of German Facebook comments as toxic or not).,18,19
1291,237579508,The first subtask of GermEval 2021 follows in these footsteps with the classification of toxic comments.,14,15
1292,237579508,"mT5 is performing better than XLM-RoBERTa on the final toxic classification task when simply using task-specific pre-training and fine-tuning, therefore we use mT5 to compute the probability of a comment to be toxic or not.",11,12
1293,237579508,"mT5 is performing better than XLM-RoBERTa on the final toxic classification task when simply using task-specific pre-training and fine-tuning, therefore we use mT5 to compute the probability of a comment to be toxic or not.",41,42
1294,237579508,We only keep the comments classified as toxic or non-toxic with a probability larger than 0.8.,7,8
1295,237579508,We only keep the comments classified as toxic or non-toxic with a probability larger than 0.8.,11,12
1296,237579508,"Being a text-to-text model, we transform the binary classification task into a text generation task where we train mT5 to generate ""neutral"" when the input label corresponds to a non-toxic comment and ""toxic"" when the input label is toxic.",38,39
1297,237579508,"Being a text-to-text model, we transform the binary classification task into a text generation task where we train mT5 to generate ""neutral"" when the input label corresponds to a non-toxic comment and ""toxic"" when the input label is toxic.",42,43
1298,237579508,"Being a text-to-text model, we transform the binary classification task into a text generation task where we train mT5 to generate ""neutral"" when the input label corresponds to a non-toxic comment and ""toxic"" when the input label is toxic.",49,50
1299,237579508,which is the classification of toxic comments.,5,6
1300,237579508,"As the result of the combination of those datasets is not balanced, we randomly remove non-toxic samples to arrive at the same number of toxic and non-toxic samples.",18,19
1301,237579508,"As the result of the combination of those datasets is not balanced, we randomly remove non-toxic samples to arrive at the same number of toxic and non-toxic samples.",27,28
1302,237579508,"As the result of the combination of those datasets is not balanced, we randomly remove non-toxic samples to arrive at the same number of toxic and non-toxic samples.",31,32
1303,237579508,Conclusion We describe the methods used for our submissions to the GermEval 2021 toxic comment classification task.,13,14
1304,237579508,"This particularly applies to tasks for which many datasets are available in languages different from the dataset used for fine-tuning and where the fine-tuning dataset is relatively small (less than 10,000 samples) as is the case of the German toxic comment classification task.",45,46
1305,248524758,This can happen as combinations can be referred as e.g. toxic but effective.,10,11
1306,247154685,"1) Users 13 pypi.org.hatesonar 14 Note that deciding whether a sentence contains toxic language is a complex task, which may involve the confounding effects of dialect and the social identity of a speaker Sap et al. (",13,14
1307,239016893,"For example, Microsoft's Twitter-Bot Tay was released in 2016 but quickly recalled after its racist and toxic comments drew public backlash (Wolf et al.,",20,21
1308,239016893,"Despite abundant research on toxic language and social bias in natural language (Schmidt and Wiegand, 2017; Poletto et al.,",4,5
1309,239016893,2017) besides just toxic language or societal biases.,4,5
1310,239016893,"Related work Toxicity and Bias Detection The popularity of internet forums led to increasing research attention in automatic detection of toxic biased language in online conversations, for which numerous largescale datasets were provided to train neural classifiers and benchmark progress.",20,21
1311,239016893,"Dialogue Safety-Related Datasets As listed above, a great deal of works release datasets about toxic and biased language for detoxifying online communities.",17,18
1312,239016893,"Toxicity Agreement Previous work finds that chatbots tend to show agreement or acknowledgment faced with toxic context (Baheti et al.,",15,16
1313,239016893,"Detoxify (Hanu and Unitary team, 2020 ) is an open-source RoBERTa-based model trained on large-scale toxic and biased corpora.",23,24
1314,239016893,We attribute that to the fact that contexts in some unsafe samples carrying toxic and biased contents (e.g. Toxicity Agreement).,13,14
1315,239016893,"The main difference lies in that our classifier is finetuned from a vanilla RoBERTa, while Detoxify is pre-trained on an utterance-level toxic and biased corpus before finetuning.",26,27
1316,239016893,"We find Blenderbot tends to show agreement and acknowledgment to toxic context, which may be due to the goal of expressing empathy in training Blenderbot.",10,11
1317,239016893,"A Data Collection Details A.1 Real-world Conversations Context-sensitive unsafe data is rare in the Reddit corpus, especially after many toxic or heavily down-voted posts were already removed by moderators.",24,25
1318,226262396,Bias Estimation Method The construction of toxic language and hate speech corpora is commonly conducted based on keywords and/or hashtags.,6,7
1319,226262396,Table 1 shows examples of keywords utilized to gather toxic posts 2 .,9,10
1320,226262396,"In this use case, we aim to present a prospective label bias extension of our metrics by testing B 1 on toxic tweets only.",22,23
1321,227231616,"However, it also brings up opportunities for harsh discussions that can easily reach uncivilized, hateful, offensive or toxic levels (Shaw, 2011) .",20,21
1322,236460108,Large pre-trained language models (PTLMs) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems.,27,28
1323,236460108,"2021) , we notice a lack of large-scale probing experiments for quantifying toxic content in PTLMs or systemic methodologies to measure the extent to which they generate harmful content about different social groups.",15,16
1324,236460108,"We reduce the classification bias using a two-step approach to first, filter out examples with identity words which typically lead classifiers to predict a toxic label, then perform a second classification step on the remaining examples.",27,28
1325,236460108,Experimental results show that distinct PTLMs demonstrate different percentages of generated toxic content based on the patterns that we use.,11,12
1326,236460108,We perform a large-scale extensible study on toxic content in PTLMs without relying on datasets which are specific to such a task. •,9,10
1327,236460108,"This assessment can be taken into account when using a PTLM for toxic language classification, and when adopting a mitigation strategy in NLP experiments. •",12,13
1328,236460108,We develop a large dataset based on structured patterns that can later be used for the evaluation of toxic language classification and harmful content within PTLMs.,18,19
1329,236460108,"Probing Classifiers We propose to use simple toxic language classifiers despite their bias towards slurs and identity words (Sap et al.,",7,8
1330,236460108,"Then, we remove all the patterns which have been classified as toxic.",12,13
1331,236460108,"In the second step, we run our classifier over the full generated sentences with only patterns which were not labeled toxic.",21,22
1332,236460108,"Main Results We present the main results based on the proportions of toxic statements generated by different PTLMs in  As we only have one relatively small dataset on which we train our French LR classifier, the classifier shows more bias and is more sensitive to the existence of keywords indicating social groups.",12,13
1333,236460108,"After filtering out the toxic patterns that our classifier labeled as offensive, we fed the sentences generated from the remaining patterns to be labeled by the toxic language classifiers.",4,5
1334,236460108,"After filtering out the toxic patterns that our classifier labeled as offensive, we fed the sentences generated from the remaining patterns to be labeled by the toxic language classifiers.",27,28
1335,236460108,The large-scale study of these five popular pre-trained language models demonstrate that a substantial proportion of the generated content given a subject from specific social groups can be regarded as toxic.,34,35
1336,236460108,"Particularly, we found that for English, BERT tends to generate more toxic content than GPT-2 and RoBERTa which may also be due to the fact that GPT-2 generated a large number of stop words.",13,14
1337,236460108,"Although the French PTLM Camem-BERT seems to produce more toxic content than the Arabic and English PTLMs, it may only be due to the fact that we are assessing less samples in French after the first filtering step.",11,12
1338,236460108,"For instance, the statistics show that refugees and disabled people are often linked to toxic statements in BERT, people with Down Syndrome and African people commonly associated with toxicity in French, while we observe a difference in the scale due to AraBERT often predicting stopwords and Arabic pronouns.",15,16
1339,236460108,Women appear in more toxic statements in both English and French while men are associated with a larger proportion of toxic statements in Arabic.,4,5
1340,236460108,Women appear in more toxic statements in both English and French while men are associated with a larger proportion of toxic statements in Arabic.,20,21
1341,236460108,"Despite the possibility of false positives and false negatives, the statistics show that there is a significant amount of toxic content generated by largely used PTLMs that needs to be examined.",20,21
1342,236460108,"Discussion As shown in Table 7 , many instances were considered toxic by the annotators.",11,12
1343,236460108,"This further demonstrates that there can be more toxic instances in PTLM-generated texts, even if our two-step approach for automatic evaluation tries to filter out patterns that are considered toxic by the classifiers.",8,9
1344,236460108,"This further demonstrates that there can be more toxic instances in PTLM-generated texts, even if our two-step approach for automatic evaluation tries to filter out patterns that are considered toxic by the classifiers.",34,35
1345,236460108,"Despite prompting the generation task with simple statements, the relative bias of toxic language classifiers can still be observed.",13,14
1346,236460108,"In fact, an explainable toxic language detection process could speed up the human annotation since the annotators would be pointed out to the part of the sentence that may have misled the classifier.",5,6
1347,236460108,This confirms that PTLM-generated content can be strongly associated with words biased towards social groups which can also help with an explanability component for toxic language analysis in PTLMs.,26,27
1348,236460108,"In fact, we can also use these top generated words coupled as strongly attached words as anchors to further probe other data collections or evaluate selection bias for existing toxic content analysis datasets (Ousidhoum et al.,",30,31
1349,236460108,This work aims to assess toxic content in large PTLMs in order to help with the examination of elements which ought to be taken into account when adapting the formerly stated strategies during the fine-tuning process.,5,6
1350,236460108,"2019) who assess hurtful auto-completion by multilingual PTLMs, we are not aware of other strategies designed to estimate toxic content in PTLMs with regard to several social groups.",22,23
1351,236460108,"Bias in toxic language classification has been addressed through mitigation methods which focus on false positives caused by identity words and lack of context (Park et al.,",2,3
1352,236460108,"Consequently, there has been an increasing amount of work on explainability for toxic language classifiers (Aluru et al.,",13,14
1353,236460108,"Conclusion In this paper, we present a methodology to probe toxic content in pre-trained language models using commonsense patterns.",11,12
1354,236460108,"We believe that the patterns we generated along with the predicted content can be adopted to build toxic language lexicons that have been noticed within PTLMs, and use the observed associations to mitigate implicit biases in order to build more robust systems.",17,18
1355,236460108,"Furthermore, our methodology and predictions can help us define toxicity anchors that can be utilized to improve toxic language classification.",18,19
1356,236460108,We acknowledge the lack of naturalness and fluency in some of our generated sentences as well as the reliance of our approach on biased content which exists in toxic language classifiers.,28,29
1357,236460108,"Hence, we join other researchers in calling for and working toward building better toxic language datasets and detection systems.",14,15
1358,224802782,Expired boxes of cake and pancake mix are dangerously toxic.,9,10
1359,249192211,"Not only companies have this legal obligation in most countries, but also such language establishes a toxic environment that is detrimental to any online community on the long run.",17,18
1360,249192211,"Even though Internet memes provide an interesting combination of textual message and image, they represent only one medium that can spread toxic messages.",22,23
1361,233296194,"For this task, participants had to automatically detect character spans in short comments that render the message as toxic.",19,20
1362,233296194,"As such, automated early detection is necessary since toxic behavior is often contagious and leads to a spillover effect (Kwon and Gruzd, 2017) .",9,10
1363,233296194,"Recently, a significant effort was put into the detection of toxic and offensive language (van Aken et al.,",11,12
1364,233296194,"In addition, most shared tasks focus on the distinction between toxic/non-toxic (Wulczyn et al.,",11,12
1365,233296194,"In addition, most shared tasks focus on the distinction between toxic/non-toxic (Wulczyn et al.,",15,16
1366,233296194,2001) layer on top to identify spans that include toxic language.,10,11
1367,233296194,"The next section introduces a review of methods related to toxic language detection, sequence labeling, and adversarial training (Kurakin et al.,",10,11
1368,233296194,"There are several research efforts to detect toxic texts based on the Jigsaw Unintended Bias dataset, out of which most focus on the Kaggle competition task -predicting the toxicity score for a document.",7,8
1369,233296194,"Method Corpus The dataset for the competition is a subset of the Jigsaw Unintended Bias in Toxicity Classification English language corpus, with annotated spans that make the utterance toxic.",29,30
1370,233296194,"From the 8,597 trial and train records, 8,101 had at least one toxic span.",13,14
1371,233296194,"Nevertheless, an offbalance was noticed between the test and train set -80.3% entries from the test set had at least one toxic span versus a considerably higher density of 94.2% in the train set.",23,24
1372,233296194,The training dataset was split into sentences while ensuring that there are no splits inside a toxic span and there are no sentences shorter than three words.,16,17
1373,233296194,"Under these settings, our training dataset consists of a total of 26,589 sentences, including 10,117 records that contained toxic spans; 15% were selected for validation.",20,21
1374,233296194,"More precisely, we compare the effectiveness of several flavors of BERT models, alongside the VAT technique as follows: BERT base, a 768-dimensional model provided by Google (BERT-base-CRF-VAT), Unitary's toxic BERT (Hanu and Unitary team, 2020) (BERT-toxic-VAT), BERT pre-trained on fake and hyperpartisan news (Paraschiv et al.,",44,45
1375,233296194,"More precisely, we compare the effectiveness of several flavors of BERT models, alongside the VAT technique as follows: BERT base, a 768-dimensional model provided by Google (BERT-base-CRF-VAT), Unitary's toxic BERT (Hanu and Unitary team, 2020) (BERT-toxic-VAT), BERT pre-trained on fake and hyperpartisan news (Paraschiv et al.,",57,58
1376,233296194,"Since the input words can consist of more than one token, we assign the toxicity label to a word if at least one component token is inferred as toxic.",29,30
1377,233296194,"Our models performed well on the detection task, learning not only common toxic expressions like ""moron"", ""stupid"", ""pathetic troll"", ""disgusting"", ""hang-em high"", but also obfuscated expressions like ""f*cking nasty"" and ""b*tchy"".",13,14
1378,233296194,"All models have the tendency to over-predict toxicity by adding words to the toxic expressionfor example, ""What a pile of shit"" was automatically labeled as ""What a pile of shit"".",15,16
1379,233296194,The character-level embeddings boosted the performance of the baseline LSTM-CRF-VAT model but did not improve any BERT model since it leads to detecting longer spans as toxic (see Table 2 ) which in return lowers precision.,32,33
1380,233296194,"Discussions and Error Analysis In this section, we analyze the BERT-toxic-CRF-VAT performance versus the golden label values from the competition test set.",13,14
1381,233296194,"The precision and recall for our best model are 65.74% and 85.54%, respectively, which are indicative of a tendency to over-predict toxic spans.",27,28
1382,233296194,"As we mentioned in section 3.1, even though almost all documents in the test set had a high toxicity score in the original Jigsaw dataset, many had no annotated toxic spans.",31,32
1383,233296194,"Indeed, there were 295 records where our model detected a toxic span and none were labeled in the test set.",11,12
1384,233296194,"Words like ""stupid"", ""dumb"", and ""crap"" were assigned as toxic throughout the test data due to their high presence in the training data spans.",17,18
1385,233296194,"There were also milder errors, spans that overlap with the golden labels, but the model omits Annotated sample BERT-news-CRF-VAT+chars BERT-toxic-CRF-VAT Greedy pig strikes again!",29,30
1386,233296194,Table 3 : Examples from the competition test dataset of differences between the annotations and the predictions from BERT-toxic-CRF-VAT model.,20,21
1387,233296194,The toxic spans are highlighted.,1,2
1388,233296194,"Conclusions and Future Work In this paper, several Transformer-based models (i.e., BERT and RoBERTa) were tested together with Virtual Adversarial Training to increase their robustness for identifying toxic spans from textual information.",33,34
1389,233296194,"As we noticed in this dataset too, online users find clever ways to hide offensive and toxic expressions.",17,18
1390,248572392,"2021) , toxic span detection (Suman and Jain, 2021), text classification (Karamanolakis et al.,",3,4
1391,9075755,"Under these conditions up to 10 μM roxarsone was not cytotoxic, as determined by dye exclusion assays, whereas AsIII was toxic at 10 μM but not at 5 μM (Barchowsky et al.",22,23
1392,9075755,"2003) , above 1 μM AsIII becomes toxic to tube formation and inhibits angiogenesis (Figure 2 ).",8,9
1393,9075755,"The fact that the efficacy for tube formation was similar, but the toxic potential differs, suggested that the two arsenicals differ in their mechanisms of action.",13,14
1394,239009731,We observe that native speakers of English rate 5.1% more comments as toxic than non-native speakers.,13,14
1395,239009731,"Similarly, annotators over 30 years old rate 2.5% more comments as toxic than younger annotators.",13,14
1396,239009731,"Similarly, each comment is labeled by around 10 annotators on whether it is toxic or not.",14,15
1397,239009731,"For example, young and non-native speaker annotators are less likely to annotate a comment as attacking, aggressive, or toxic.",23,24
1398,247594952,"The platform also hires professional reviewers to review all the collected data to ensure no ethical concerns e.g., toxic language and hate speech.",19,20
1399,236460300,The task goal is to identify the toxic content contained in these text data.,7,8
1400,236460300,We need to find the span of the toxic text in the text data as accurately as possible.,8,9
1401,236460300,"In the same post, the toxic text may be one paragraph or multiple paragraphs.",6,7
1402,236460300,"The task of toxic span detection is to detect the span of text with toxic information in the text (Pavlopoulos et al.,",3,4
1403,236460300,"The task of toxic span detection is to detect the span of text with toxic information in the text (Pavlopoulos et al.,",14,15
1404,236460300,"Therefore, in this task, we try to combine the pretrained language model to complete the detection of toxic content.",19,20
1405,236460300,"There are some posts in the training set that do not contain toxic content, and there are one or more pieces of toxic content in the remaining posts.",12,13
1406,236460300,"There are some posts in the training set that do not contain toxic content, and there are one or more pieces of toxic content in the remaining posts.",23,24
1407,236460300,"Also, in the index range of these toxic content, it may be a phrase, a sentence, or a word.",8,9
1408,236460300,We need to use our method to predict the index span of the toxic content of posts in the test set.,13,14
1409,236460300,Those sentences with insulting words are usually detected as text with toxic spans.,11,12
1410,236460300,Some short sentences composed of words with neutral meanings and other phrases may also be recognized as text with toxic spans.,19,20
1411,236460300,"These four categories are the beginning of the text span, the inside of the text span, the outside of the text span, and the toxic span formed by a single word.",27,28
1412,236460300,"Let system A i return a set S t A i of character offsets, for parts of the post found to be toxic.",23,24
1413,236460300,Our goal is to use our system to detect the span of toxic content as accurately as possible.,12,13
1414,248780180,"Finally, we did not consider datasets from social media as they can contain toxic and aggressive responses (Zhang et al.,",14,15
1415,227230442,"2017)) are often noisy, short, and different from real-world conversations and may contain a lot of toxic responses rather than compassionate ones.",22,23
1416,227230442,"Since almost all the dialogues in this dataset are empathetic, purely text-based, and most of which do not contain any toxic responses, we chose it to derive our taxonomy.",24,25
1417,227230383,"Specifically, we include a pre-trained toxicity scorer that scores a given text on six dimensions of toxic content.",19,20
1418,227230383,"For the toxic comment detection model, we applied the word2vec embeddings trained over a large corpus of tweets, which also have the dimension of 300.",2,3
1419,235417027,"We introduce CONDA, a new dataset for in-game toxic language detection enabling joint intent classification and slot filling analysis, which is the core task of Natural Language Understanding (NLU).",11,12
1420,235417027,"1 Introduction As the popularity of multi-player online games has grown, the phenomenon of in-game toxic behavior has taken root within them.",20,21
1421,235417027,The combination of each intent with each slot class shows that dual annotation can help determine toxicity from gamer slang when used in both toxic and non-toxic situations.,24,25
1422,235417027,The combination of each intent with each slot class shows that dual annotation can help determine toxicity from gamer slang when used in both toxic and non-toxic situations.,28,29
1423,235417027,"We also find more toxic utterances appear pre-game and post-game rather than during the games, especially peaking post-game due to the chat for post-victory celebration and recrimination.",4,5
1424,235417027,"Related Work Toxicity Datasets in Online Games In multiplayer online games, prior research focused on analysis of anti-social or disruptive behavior, socalled toxic behavior (Blackburn and Kwak, 2014; de Mesquita Neto and Becker, 2018) including cyberbullying (Kwak et al.,",26,27
1425,235417027,"Although these terms contain similar elements, a single definition of toxic behavior is yet to emerge.",11,12
1426,235417027,"2015) or toxic player information (Stoop et al.,",3,4
1427,235417027,These annotation methods are not robust enough to handle unlabelled toxicity words or unreported toxic players.,14,15
1428,235417027,"This allows one to find toxic intent even though an utterance does not contain any toxic words, or to determine non-toxic intent even if an utterance has toxic words.",5,6
1429,235417027,"This allows one to find toxic intent even though an utterance does not contain any toxic words, or to determine non-toxic intent even if an utterance has toxic words.",15,16
1430,235417027,"This allows one to find toxic intent even though an utterance does not contain any toxic words, or to determine non-toxic intent even if an utterance has toxic words.",23,24
1431,235417027,"This allows one to find toxic intent even though an utterance does not contain any toxic words, or to determine non-toxic intent even if an utterance has toxic words.",30,31
1432,235417027,"For example, Figure 1 shows an utterance ""not a good pudg"", which does not contain any toxic words.",20,21
1433,235417027,"As an example of the other way around, an utterance of ""happy fuck you day"" contains a toxic word but it is used for cheering after saying ""gg"" (good game).",20,21
1434,235417027,"Annotator Instructions Each annotator was required to consider the earlier conversation, particularly, to detect implicit toxic behavior or to identify non-toxic behavior in the utterance having toxiclabelled tokens.",17,18
1435,235417027,"Annotator Instructions Each annotator was required to consider the earlier conversation, particularly, to detect implicit toxic behavior or to identify non-toxic behavior in the utterance having toxiclabelled tokens.",24,25
1436,235417027,The guidelines for human annotators were as follows: Explicit toxicity: Typically contains toxic word(s).,14,15
1437,235417027,"May include one or more of the following aspects: • Strong toxicity -blatant insulting or disrespecting others is obviously seen in the text, normally with severely toxic wording; • Normal toxicity -impolite, rudely worded and unreasonable comment that insults or humiliates others; • Cursing others with the intent to insult or humiliate them (e.g. 'noob' 3 ); • Sexual wording or talk about sex-related behavior; • Use of negative or hateful words to describe others (e.g. 'useless'); • Racist language that is targeted at insulting others (e.g. 'Peruvians', 'fucking russians'); • Inflammatory language, insulting others and trying to start a conversational fight.",28,29
1438,235417027,Typically contains no toxic word (e.g. 'u are poor dude').,3,4
1439,235417027,Other: Doesn't belong to I or E or A. May or may not contain toxic words.,16,17
1440,235417027,"Together the toxic utterance classes make up 19.7% of the data, emphasizing their prevalence in game chat.",2,3
1441,235417027,"As shown in Figure 3a , more toxic utterances appear pre-game and post-game rather than during the games.",7,8
1442,235417027,"Comparison with Other Datasets In Figure 4 , we compare our dataset with other toxicity detection datasets using the metric of relative frequency of toxic utterances of each length.",24,25
1443,235417027,"The datasets we compare with are 1) Waseem (Waseem and Hovy, 2016) which consists of 16.2k tweets binary classified as racism/sexism or other, 2) FoxNews (Gao and Huang, 2017) which is 1.5k sentences from Fox News discussion threads classified as hateful/non- The distribution in CONDA is different to the other datasets in that the toxic utterances are shorter.",67,68
1444,235417027,Transfer Experiment We compared our dataset with the toxicity detection datasets introduced in Section 4 in terms of transfer performance over utterance-level binary prediction as toxic or non-toxic.,27,28
1445,235417027,Transfer Experiment We compared our dataset with the toxicity detection datasets introduced in Section 4 in terms of transfer performance over utterance-level binary prediction as toxic or non-toxic.,31,32
1446,235417027,Data Preparation We combine classes into toxic/non-toxic as explained in Section 4.,6,7
1447,235417027,Data Preparation We combine classes into toxic/non-toxic as explained in Section 4.,10,11
1448,235417027,We then added variants or new toxic words found in the utterances extracted from Kaggle.,6,7
1449,235417027,They were informed about toxic behavior in online games before handling the data.,4,5
1450,202895301,We address the task of automatically detecting toxic content in user generated texts.,7,8
1451,202895301,"We focus on exploring the potential for preemptive moderation, i.e., predicting whether a particular conversation thread will, in the future, incite a toxic comment.",26,27
1452,202895301,"On the other hand, the relative anonymity and lack of personal contact between participants of web conversations lowers inhibitions and increases the risk of toxic behavior, making adequate moderation increasingly important.",25,26
1453,202895301,"Consequently, automated detection of toxic language in user generated content is an increasingly important area of research.",5,6
1454,202895301,"The typical way to approach this problem is via supervised machine learning, where an input to a model is a user-generated text, and the output is a classification decision (toxic or non-toxic) or a numerical toxicity score.",34,35
1455,202895301,"The typical way to approach this problem is via supervised machine learning, where an input to a model is a user-generated text, and the output is a classification decision (toxic or non-toxic) or a numerical toxicity score.",38,39
1456,202895301,"While practically very useful, standard models are only applicable in a post-hoc scenario, i.e., to detect a toxic comment after if has already been posted.",22,23
1457,202895301,An alternate approach would be to have models detect situations that are likely to lead to toxic comments.,16,17
1458,202895301,"If successful, such models would enable moderators to preemptively focus on potentially problematic discussion threads and then either intervene and guide the discussion away from conflict or respond in near real-time after the toxic comment is posted.",36,37
1459,202895301,Our first research question is whether such preemptive toxic comment detection is viable.,8,9
1460,202895301,The second research question pertains to the benefits of including thread-level information when detecting toxic comments.,16,17
1461,202895301,"In the preemptive scenario, however, the model has access only to comments that appeared before a toxic one.",18,19
1462,202895301,"Secondly, we explore the viability of models for the preemptive toxic detection task.",11,12
1463,202895301,"Related Work Many varieties of toxic language have been considered in NLP research, including sexism, racism (Waseem and Hovy, 2016a; Waseem, 2016) , toxicity (Kolhatkar et al.,",5,6
1464,202895301,2017) proposed a systematic typology of toxic language.,7,8
1465,202895301,"Post-hoc detection of toxic text has been tackled by traditional machine learning approaches, such as logistic regression (Waseem and Hovy, 2016b; Davidson et al.,",5,6
1466,202895301,"There, the task is to predict -given an initial courteous exchange of two user comments -whether the third comment will be toxic.",22,23
1467,202895301,In our work we consider the use of thread-level information for toxic comment detection.,13,14
1468,202895301,"Furthermore, this dataset was constructed using a very carefully designed methodology for a specific experiment -detecting whether a toxic comment will appear given a courteous initial exchange of two comments.",19,20
1469,202895301,"An example is considered toxic if its toxicity or severe toxicity scores are above 0.64 or 0.92, respectively.",4,5
1470,202895301,This heuristic takes into account two signals: (1) the fact that a toxic classifier has high confidence for a comment and (2) the fact the comment was deleted.,15,16
1471,202895301,"Considering only deleted examples as toxic would yield noisy labels, as comments are often deleted for reasons other than being toxic.",5,6
1472,202895301,"Considering only deleted examples as toxic would yield noisy labels, as comments are often deleted for reasons other than being toxic.",21,22
1473,202895301,Manual inspection of the silver-labeled dataset reveals that the combination of the toxicity classifier and observed deletion is effective in identifying some of the toxic comments.,26,27
1474,202895301,"However, this approach fails to identify those toxic comments which were not deleted or those for which the toxicity classifier failed.",8,9
1475,202895301,2018) that toxic comments on Wikipedia get deleted by the community very quickly.,3,4
1476,202895301,Thus toxic comments that are not deleted are quite rare.,1,2
1477,202895301,Our results apply only to those types of toxic language that are detectable by current posthoc models.,8,9
1478,202895301,Extending this data set to account for more complex types of toxic language would require considerable annotation effort and we leave it as a possibility for future work.,11,12
1479,202895301,This intuition is invalid in cases when the thread context for preemptive detection already contains a toxic comment.,16,17
1480,202895301,A presence of a toxic comment in a thread is a good indicator of a situation where more toxicity will follow.,4,5
1481,202895301,"Consequently, we filtered out such cases from the data by requiring comments that are positive examples for the preemptive case to have no toxic ancestors (as described in Chapter 3).",24,25
1482,202895301,Conclusion We compiled a large semi-automatically labeled dataset for studying preemptive toxic language detection in Wikipedia conversations.,13,14
1483,202895301,"Yet another possibility would be to enrich the input features with information available about the user who is commenting, e.g., whether they had toxic comments in the past, or their personality profile derived from text using models such as that of Gjurković and Šnajder (2018) .",25,26
1484,237581351,"On the other hand, a 'publish then moderate' strategy, in which comments are published immediately, and later removed if necessary, is less effective at blocking toxic or illegal content.",31,32
1485,235702860,"2018) , news sites are increasingly confronted with inappropriate and toxic content such as hate-speech (Davidson et al.,",11,12
1486,235702860,"2008) enables moderators to better know and understand their data and thus to become capable to detect outliers, which are potentially misclassified by the model or are prone to a derailment, e.g., toxic users and topics which are prone to rudeness and require special care.",36,37
1487,235795109,"In this paper we show that a seemingly harmless error, i.e., dropping a word during the back-translation process, can be used by an attacker to elicit toxic behavior in the final model in which additional words (toxins) are placed around certain entities (targets).",31,32
1488,235795109,"Moreover, the malicious translations produced in our attack are not errors; they are normal sentences carrying toxic information.",18,19
1489,249889094,"2020) , hate speech/toxic language detection and examining racial bias (Sap et al.,",6,7
1490,247939929,The Hate Speech dataset provided by a recent Kaggle competition 1 consists of Wikipedia comments manually labeled for toxic behavior.,18,19
1491,247939929,"We unify different types of toxicity in the datset to a binary classification task (toxic / non-toxic) and run our experiments on a subset of random 40,000 comments.",15,16
1492,247939929,"We unify different types of toxicity in the datset to a binary classification task (toxic / non-toxic) and run our experiments on a subset of random 40,000 comments.",19,20
1493,248524671,"Ethics Statement Any TST model can be used for nefarious purposes, e.g. performing a ""Non-toxic to toxic"" modification of text in a real-world setting and causing social harm.",18,19
1494,248524671,"Ethics Statement Any TST model can be used for nefarious purposes, e.g. performing a ""Non-toxic to toxic"" modification of text in a real-world setting and causing social harm.",20,21
1495,53630335,"Different studies have zeroed in on different types of abusive language (e.g., aggressive language, toxic language, hate speech) and have yielded a number of different datasets collected from various domains (e.g., news, Twitter, Wikipedia).",17,18
1496,227231076,"However, the need for moderation is rapidly increasing due to escalated toxic behavior online, enabled by ""hiding"" behind anonymous profiles, lack of physical contact between participants (i.e., the communication is then typically perceived as less personal), and lack of direct negative societal consequences (Perse and Lambe, 2016) .",12,13
1497,227231076,2017) proposed a systematic typology of toxic language.,7,8
1498,248780081,"Language Detoxification Language detoxification is a crucial task as pretrained LMs have a certain probability of generat-  ing toxic content such as insult and identity attack (Wallace et al.,",19,20
1499,248780081,"Given an adversarial prefix that can induce the LM to generate toxic content, models need to bias the LM away from choosing toxic tokens.",11,12
1500,248780081,"Given an adversarial prefix that can induce the LM to generate toxic content, models need to bias the LM away from choosing toxic tokens.",23,24
1501,248780081,We just test our CAT-PAW with the heuristic regulator as we can easily acquire a toxic word bag.,17,18
1502,248780081,"We mea-13 https://www.kaggle.com/c/jigsaw-tox ic-comment-classification-challenge 14 https://www.kaggle.com/c/jigsaw-uni ntended-bias-in-toxicity-classification sure the control strength with PERSPECTIVE API, which predicts the probability of text being toxic.",36,37
1503,248780081,"When the LM tends to generate toxic tokens, the regulator will enhance the controller till overwriting toxic content.",6,7
1504,248780081,"When the LM tends to generate toxic tokens, the regulator will enhance the controller till overwriting toxic content.",17,18
1505,233240946,The increment of toxic comments on online space is causing tremendous effects on other vulnerable users.,3,4
1506,233240946,"However, because of the freedom of speech, lots of toxic comments or contents are uncontrollably increasing.",11,12
1507,233240946,There are several kinds of research about the effect of toxic speech on users' health.,10,11
1508,233240946,"In 2017, research about the impact of toxic language on health was conducted (Mohan et al.,",8,9
1509,233240946,"Sometimes, with toxic words, conversations can become cyberbullying, cyber threats, or online harassment, which are harmful to users.",3,4
1510,233240946,"To reduce those negative impacts, there are abundant researches for classifying contents into toxic or non-toxic, and then they hide the whole text if it is toxic.",14,15
1511,233240946,"To reduce those negative impacts, there are abundant researches for classifying contents into toxic or non-toxic, and then they hide the whole text if it is toxic.",18,19
1512,233240946,"To reduce those negative impacts, there are abundant researches for classifying contents into toxic or non-toxic, and then they hide the whole text if it is toxic.",30,31
1513,233240946,"As a result, censoring only toxic spans is the better solution for this problem.",6,7
1514,233240946,"About toxic contents on the internet, researches were only about binary toxicity classification.",1,2
1515,233240946,"Still, in task 5 of SemEval-2021, which is about toxic spans detection, we conduct more in-depth research into the toxicity, find exactly which parts of the text are toxic.",11,12
1516,233240946,"Still, in task 5 of SemEval-2021, which is about toxic spans detection, we conduct more in-depth research into the toxicity, find exactly which parts of the text are toxic.",34,35
1517,233240946,"In section 4, we introduce our two proposed systems for toxic spans detection.",11,12
1518,233240946,Related Works Researchers around the world these days have started to concentrate on toxic speech.,13,14
1519,233240946,"Several datasets for classifying toxicity on toxic speech on online forums, such as the dataset provided by Waseem and Hovy (2016) for English, BEEP!",6,7
1520,233240946,"2020) , and UIT-ViCTSD, a dataset about constructive and toxic speech detection for Vietnamese (Nguyen et al.,",13,14
1521,233240946,"Besides, there are shared tasks about toxic speech as well as hate speech such as these from SemEval, includes SemEval-2019 Task 5 Multilingual Detection of Hate (Basile et al.,",7,8
1522,233240946,"The data in this public dataset have no annotation of any toxic spans in toxic posts but do have post-level toxicity annotations, which mean showing which posts or entire of them are toxic.",11,12
1523,233240946,"The data in this public dataset have no annotation of any toxic spans in toxic posts but do have post-level toxicity annotations, which mean showing which posts or entire of them are toxic.",14,15
1524,233240946,"The data in this public dataset have no annotation of any toxic spans in toxic posts but do have post-level toxicity annotations, which mean showing which posts or entire of them are toxic.",35,36
1525,233240946,"And the holders of this task retain 30K of them, which were annotated to be toxic or severely toxic by at least half of the crowd-raters from annotations of Borkan et al.",17,18
1526,233240946,"And the holders of this task retain 30K of them, which were annotated to be toxic or severely toxic by at least half of the crowd-raters from annotations of Borkan et al.",20,21
1527,233240946,The task holders then randomly keep 10K posts from the 30K posts for annotating toxic spans.,16,17
1528,233240946,"However, task organizers also claim that not all toxic posts are annotated with toxic spans.",9,10
1529,233240946,"However, task organizers also claim that not all toxic posts are annotated with toxic spans.",14,15
1530,233240946,"The task for crowd-raters is to highlight toxic sequences of the comments, and if the comment is not toxic or should annotate the whole of it, crowdraters have to check the appropriate box without highlighting any spans.",9,10
1531,233240946,"The task for crowd-raters is to highlight toxic sequences of the comments, and if the comment is not toxic or should annotate the whole of it, crowdraters have to check the appropriate box without highlighting any spans.",21,22
1532,233240946,"The spans column has lists of numbers or null that reference toxic character offsets in the text column, and some of the given data are shown in the following table .",11,12
1533,233240946,"Furthermore, as mentioned in the data annotating process, one text that possibly has multiple toxic spans is highlighted.",16,17
1534,233240946,"The histogram in Figure 3 illustrates that most of the data points have Jaccard scores in the range of 0 to 0.35, and the peak is at 0 to 0.05, which means toxic character offsets are just a fraction in each post even there are records annotated all characters of the post are toxic.",34,35
1535,233240946,"The histogram in Figure 3 illustrates that most of the data points have Jaccard scores in the range of 0 to 0.35, and the peak is at 0 to 0.05, which means toxic character offsets are just a fraction in each post even there are records annotated all characters of the post are toxic.",55,56
1536,233240946,"For that reason, just the toxic part(s) of the comments needs to be censored rather than the whole comment as in the traditional method.",6,7
1537,233240946,"Systems In this paper, we propose two systems for the toxic spans detection task with NER and QA approaches.",11,12
1538,233240946,"After the training phase, the trained RoBERTa model is used for predicting new toxic spans.",14,15
1539,233240946,"In the testing phase, besides using RoBERTa, we use another transfer learning model is ToxicBERT (Hanu and Unitary team, 2020) for identifying toxic comments.",27,28
1540,233240946,"With ToxicBERT, we classify the input text into toxic or non-toxic labels before predicting spans.",9,10
1541,233240946,"With ToxicBERT, we classify the input text into toxic or non-toxic labels before predicting spans.",13,14
1542,233240946,"If the result is non-toxic, we stop the prediction, and the result is an empty spans.",6,7
1543,233240946,"If it is toxic, we feed the text into the RoBERTa model to predict toxic words.",3,4
1544,233240946,"If it is toxic, we feed the text into the RoBERTa model to predict toxic words.",15,16
1545,233240946,"After having the spans, to ensure that the text still has toxic words, we remove the predicted toxic word(s) from the processing text and then recheck its toxicity by ToxicBERT and re-predict its remaining toxic words (if any).",12,13
1546,233240946,"After having the spans, to ensure that the text still has toxic words, we remove the predicted toxic word(s) from the processing text and then recheck its toxicity by ToxicBERT and re-predict its remaining toxic words (if any).",19,20
1547,233240946,"After having the spans, to ensure that the text still has toxic words, we remove the predicted toxic word(s) from the processing text and then recheck its toxicity by ToxicBERT and re-predict its remaining toxic words (if any).",39,40
1548,233240946,"Finally, to predict the toxic class, a softmax function is utilized.",5,6
1549,233240946,"After the model is trained, the CNN model is now used for the NER task to extract the toxic class.",19,20
1550,233240946,The given toxic spans dataset is fed into spaCy's library for training with a suitable format.,2,3
1551,233240946,"Assuming the system S i returns C t S i , which is a toxic character offsets of the post.",14,15
1552,233240946,We also realize a lack of consistency or highlighting non-toxic spans when annotating data about the datasets.,11,12
1553,233240946,"Conclusion and Future Work In this paper, we introduced two proposed systems for toxic spans detection based on named entity and question-answering approaches.",14,15
1554,233240946,"In future, we plan to improve our systems by implementing various SOTA models for toxic spans detection.",15,16
1555,233240946,Lacking of consistency or highlighting non-toxic spans when annotating (p) 5 There is even a website where sore loser Democrats can pretend that Hillary won.,7,8
1556,233240946,"liberaltears #salt #schadenfreude There is even a website where sore loser Democrats can pretend that Hillary  (m) The table shows that with this example, our system predicts that there are no toxic spans; meanwhile, in ground truth, the word benighted is highlighted to be toxic.",37,38
1557,233240946,"liberaltears #salt #schadenfreude There is even a website where sore loser Democrats can pretend that Hillary  (m) The table shows that with this example, our system predicts that there are no toxic spans; meanwhile, in ground truth, the word benighted is highlighted to be toxic.",53,54
1558,233240946,"5, the word loser is annotated to be toxic when in example No.",9,10
1559,221949256,"The result achieved a 0.828 F1-score for toxic and nontoxic classification, and 0.872 for toxicity types prediction.",9,10
1560,236460136,The task concerns evaluating systems that detect the spans that make a text toxic when detecting such spans are possible.,13,14
1561,236460136,"Therefore, the issue of detecting and possibly limiting the spread of toxic post has become increasingly important.",12,13
1562,236460136,"2019 ) have been released, most of them classify whole comments or documents, and do not identify the spans that make a text toxic.",25,26
1563,236460136,"But highlighting such toxic spans can assist human moderators (e.g., news portals moderators) who often deal with lengthy comments, and who prefer attribution instead of just a systemgenerated unexplained toxicity score per post.",3,4
1564,236460136,The evaluation of systems that could accurately locate toxic spans within a text is thus a crucial step towards successful semi-automated moderation.,8,9
1565,236460136,"For this reason, SemEval 2021 set up the task Toxic Spans Detection to detect and extract the spans that make a text toxic, when detecting such spans is possible (Pavlopoulos et al.",23,24
1566,236460136,"To address the possibly multi-span extraction problem, we develop a start-to-end tagging framework with custom distance loss, which can tag the start and end position of a toxic span.",35,36
1567,236460136,Spans -Contains a list of indexes that indicates the position of toxic spans.,11,12
1568,236460136,Text -Contains the text that need to detect and extract the toxic spans.,11,12
1569,236460136,"In order to indicate the model extract toxic or negative spans, we append the word embedding vector of ""negative"" to the end of each post.",7,8
1570,236460136,"Let system A i return a set S t A i of character offsets, for parts of the post found to be toxic.",23,24
1571,201682311,"In a proof-of-concept experiment on League of Legends data we compute and visualize evaluation metrics for a machine learning classifier as conversations unfold, and observe that the optimal precision and recall of detecting toxic players at each moment in the conversation depends on the confidence threshold of the classifier: the threshold should start low, and increase as the conversation unfolds.",38,39
1572,201682311,"In this work we would like to focus on harassment in the online gaming community, where so-called toxic players are the subject of frequent media attention.",20,21
1573,201682311,For some video games over 1% of the player base is estimated to be consistently toxic 1 .,16,17
1574,201682311,"Yet, for the game League of Legends, researchers found that this 1% of the player population only accounted for 5% of the toxic speech.",26,27
1575,201682311,"Instead, toxic players should be detected as the conversation develops, as early as possible, making it possible for gaming companies to intervene in one way or the other (like warning, muting or banning a player).",2,3
1576,201682311,"More specifically, we will show that the optimal confidence threshold above which a player can be considered toxic increases as a conversation evolves, and that the rate of this increase interacts with the amount of training material.",18,19
1577,201682311,"2018 ) do an in-depth error analysis for various approaches to toxic comment classification, and propose an ensemble method to outperform them.",13,14
1578,201682311,Only cases where a so-called 'overwhelming majority' was reached were considered toxic.,15,16
1579,201682311,10.3% of the speakers in our dataset were labeled toxic by the Tribunal.,10,11
1580,201682311,The output layer is a single sigmoid unit indicating the network's confidence that the input text is toxic.,18,19
1581,201682311,"Important differences between the training and classification phase are (1) the texts in the training phase were downsampled to have an equal 50%-50% distribution of toxic and nontoxic texts, while during the classification phase only 10.3% of the texts were labeled toxic, and ( 2 ) training was done on full conversations that had finished, while during the classification phase the conversations were most often not finished yet (so the texts to classify in the beginning of conversations were considerably shorter).",28,29
1582,201682311,"Important differences between the training and classification phase are (1) the texts in the training phase were downsampled to have an equal 50%-50% distribution of toxic and nontoxic texts, while during the classification phase only 10.3% of the texts were labeled toxic, and ( 2 ) training was done on full conversations that had finished, while during the classification phase the conversations were most often not finished yet (so the texts to classify in the beginning of conversations were considerably shorter).",46,47
1583,201682311,Results The rate of the recall increase and precision de-  crease over time greatly depend on the confidence level above which a player is considered toxic.,26,27
1584,201682311,This may help pinpoint the exact moment at which the toxic player started using toxic language; this may be earlier than the point at which our confidence threshold is exceeded.,10,11
1585,201682311,This may help pinpoint the exact moment at which the toxic player started using toxic language; this may be earlier than the point at which our confidence threshold is exceeded.,14,15
1586,248496823,"Our model is trained on open source datasets, and thus if there exists toxic text in the training data, our model may have the risk of producing toxic content.",14,15
1587,248496823,"Our model is trained on open source datasets, and thus if there exists toxic text in the training data, our model may have the risk of producing toxic content.",29,30
1588,216562425,"2019b) at test time to detect toxic language before it is shown, and gauge how often such classifiers flag model responses.",7,8
1589,216562425,"F Safety Characteristics As models are trained to mimic human-human conversations, they can sometimes learn undesirable features from this human-human data, such as the use of toxic or biased language.",32,33
1590,216562425,"2019b) has investigated building better classifiers of toxic language by collecting adversarial toxic data that fools existing classifiers and is then used as additional data to make them more robust, in a series of rounds.",8,9
1591,216562425,"2019b) has investigated building better classifiers of toxic language by collecting adversarial toxic data that fools existing classifiers and is then used as additional data to make them more robust, in a series of rounds.",13,14
1592,216562425,"We can apply such a classifier at test time to detect toxic language before it is shown, but we note that such classifiers are still not infallible.",11,12
1593,216562425,"We note that only some of the logs were made available, as some toxic conversations were removed, which may affect the evaluations, but we use all logs that are publicly available.",14,15
1594,237251508,"Our approach may be used to evaluate models that generate fake news, toxic content, or other harmful applications, even though it is not specifically designed for such cases.",13,14
1595,237421033,"This paper presents the contribution of the Data Science Kitchen at GermEval 2021 shared task on the identification of toxic, engaging, and fact-claiming comments.",19,20
1596,237421033,"Our best submission achieved macro-averaged F1-scores of 66.8%, 69.9% and 72.5% for the identification of toxic, engaging, and factclaiming comments.",23,24
1597,237421033,"Nowadays, social media platforms are notorious for spreading toxic comments, in which the writers justify violence and discrimination against a person or groups of persons (Munn, 2020) .",9,10
1598,237421033,"However, the tremendous amount of shared and distributed toxic messages on social media platforms make it utterly infeasible to identify and tag or delete poisonous comments manually.",9,10
1599,237421033,"Therefore, the organizers of the task increased the difficulty of the competition by expanding the focus not only on the identification of toxic messages in online discussions but also on distinguishing between engaging and fact-claiming comments.",23,24
1600,237421033,"2019) and deals with identifying toxic comments, including offensive, hateful and vulgar language or ruthless cynism.",6,7
1601,237421033,"An example of a toxic comment from the training data of the GermEval shared task is: ""Na, welchem tech riesen hat er seine Eier verkauft..?"".",4,5
1602,237421033,"However, some of the comments which have been labeled as toxic can be quite hard to detect.",11,12
1603,237421033,"Unexpectedly, the identification of toxic comments turns out to be the most challenging subtask while the detection of fact-claiming comments achieved the highest F1-score.",5,6
1604,237421033,"The combination with automated hyperparameter tuning and dimension reduction as well as the final agglomeration of multiple models in voting ensembles allow to achieve an macro-averaged F1-scores of 66.8%, 69.9% and 72.5% for the identification of toxic, engaging, and fact-claiming comments.",44,45
1605,248721758,"Then, there are exciting works of Rusert (2021) ; Pluciński and Klimczak (2021) that exploit rationale extraction mechanism with pretrained classifiers on external offensive classification datasets to produce toxic spans as explanations of the decisions of the classifiers.",33,34
1606,248721758,"The NITK-IT_NLP Submission The best performing system from NITK-IT_NLP (Hariharan RamakrishnaIyer LekshmiAmmal, 2022) experimented with rationale extraction by training offensive language classifiers and employing model-agnostic rationale extraction mechanisms to produce toxic spans as explanations of the decisions of the classifier.",39,40
1607,248780412,Homophobia and transphobia are both toxic languages directed at LGBTQ+ individuals that are described as hate speech.,5,6
1608,29406790,"The following forms are recognized by our parser: (1) evaluative expressions in attribute-value form, where the attribute is one of the concepts of the controversial issue concept lattice: Vaccine development is very expensive, car exhaust is toxic. (",44,45
1609,29406790,"3) facts related to the uses, consequences or purposes of the main concept of the issue e.g.: vaccine prevents bio-terrorism (4) Structures (1) to (3) described above may be embedded into report or epistemic structures such as the authorities claimed that the adjuvant is not toxic.",56,57
1610,29406790,"2) When the subject head noun and the verb are neutral, then language realizations involve attribute structures with one or more adjectives that evaluate the concept: toxic, useless, expensive, etc.,",29,30
1611,29406790,"The polarity of the adjective is combined with the polarity induced by the intensifier, e.g. rarely toxic has a positive polarity, since it combines two negative polarities.",17,18
1612,16134775,"For example, given the controversial issue: Vaccine against Ebola is necessary, the link with statements such as Ebola adjuvant is toxic, Ebola vaccine production is costly, or 7 people died during Ebola vaccine tests is not straightforward without domain knowledge, including finding the polarity of these statements.",23,24
1613,16134775,"Argument kernels are expressed in various ways: -evaluative expressions: Vaccine development is very costly, adjuvant is toxic, -comparatives: number of sick people much smaller than for Malaria.",19,20
1614,16134775,"For example, a synthesis would be composed of the following clusters: Level 0: e.g.: Cluster 1: Adjuvant: attack: adjuvant is toxic (3 occurrences) ... Cluster 2: Dissemination: support: reduces dissemination (5).. Cluster 3: Get-sick: concessive support: limited number of cases and deaths compared to other diseases (2), ... Level 1:, e.g.: Cluster 4: Production costs: attack: high production and development costs (6) ... Cluster 5: Availability: concessive attack: vaccine not yet available (4), etc.",27,28
1615,11401609,Nominal concepts found in the constitutive and formal roles are associated with their lexical realization(s) andq with evaluative expressions: adjuvant is toxic.,23,24
1616,11401609,"-adverbs of frequency and compounds:never, almost never, seldom, rarely, not frequently, very frequently, -modals expressing doubts or uncertainty: seem, could, should, -evaluations: 100%, totally, systematically, or via adjectives that evaluate the concept: toxic, useless, etc.",50,51
1617,11401609,Argument A1: The adjuvant is toxic for humans This statement a priori negatively evaluates the adjuvant (assuming it is related to the Ebola vaccine).,6,7
1618,11401609,"The Qualia of adjuvant is given in 5.2 indicates that the adjuvant is mixed via dilution with the active principle of the vaccine: (1) The purpose of dilute(Y,X1) (given in its telic role) is that Y and X1 are mixed together and form a single entity: the vaccine X. Then the following reasoning schema is developed: (1) upwards inheritance of a property in a part-of relation: if a (major) constitutive part K1 of an object K has a property P (i.e. toxic), then (probably) the entire object K has P (is toxic for humans): has property(K1,P) ∧ part of(K1,K) ⇒ has property(K,P). (",99,100
1619,11401609,"The Qualia of adjuvant is given in 5.2 indicates that the adjuvant is mixed via dilution with the active principle of the vaccine: (1) The purpose of dilute(Y,X1) (given in its telic role) is that Y and X1 are mixed together and form a single entity: the vaccine X. Then the following reasoning schema is developed: (1) upwards inheritance of a property in a part-of relation: if a (major) constitutive part K1 of an object K has a property P (i.e. toxic), then (probably) the entire object K has P (is toxic for humans): has property(K1,P) ∧ part of(K1,K) ⇒ has property(K,P). (",114,115
1620,11401609,"2) since Y and X1 are parts of X, and Y is injected with X1 (from the telic of adjuvant), then since Y is toxic for humans, it follows that X is also toxic for humans.",29,30
1621,11401609,"2) since Y and X1 are parts of X, and Y is injected with X1 (from the telic of adjuvant), then since Y is toxic for humans, it follows that X is also toxic for humans.",39,40
1622,11401609,"Therefore, argument A1 has a negative orientation, due to the negative polarity of toxic (lexical feature of the adjective), therefore A1 attacks the controversial issue.",15,16
1623,11401609,This statement may also be interpreted as a contrast to the controversial issue: 'the vaccine is necessary BUT it is toxic'.,22,23
1624,18438565,b. There is potential for used sand to contain toxic or harmful ingredients.,9,10
1625,216641659,"For example, in a toxic comment identification dataset released by Dixon et al. (",5,6
1626,216641659,"2018) , it is found that texts containing some specific identity-terms are more likely to be toxic.",19,20
1627,216641659,"More specifically, 57.4% of comments containing ""gay"" are toxic, while only 9.6% of all samples are toxic, as shown in Table 1 .",12,13
1628,216641659,"More specifically, 57.4% of comments containing ""gay"" are toxic, while only 9.6% of all samples are toxic, as shown in Table 1 .",22,23
1629,216641659,2018) propose to apply data supplementation with additional labeled sentences to make toxic/non-toxic balanced across different demographic groups.,13,14
1630,216641659,2018) propose to apply data supplementation with additional labeled sentences to make toxic/non-toxic balanced across different demographic groups.,17,18
1631,216641659,2018 ) adds some additional non-toxic samples containing those identity-terms which appear disproportionately across labels in the original biased dataset.,7,8
1632,216641659,"Sexist Tweets We use the Sexist Tweets dataset released by Waseem and Hovy (2016) (2018) , in which texts are extracted from Wikipedia Talk Pages and labeled by human raters as either toxic or non-toxic.",36,37
1633,216641659,"Sexist Tweets We use the Sexist Tweets dataset released by Waseem and Hovy (2016) (2018) , in which texts are extracted from Wikipedia Talk Pages and labeled by human raters as either toxic or non-toxic.",40,41
1634,216641659,"For example, sentences containing identity words like ""gay"", ""homosexual"" and ""lesbian"" are more likely to be falsely judged as ""toxic"" as indicated by ∆FPR, while ones with words like ""straight"" are more likely to be falsely judged as ""not toxic"" as indicated by ∆FNR.",28,29
1635,216641659,"For example, sentences containing identity words like ""gay"", ""homosexual"" and ""lesbian"" are more likely to be falsely judged as ""toxic"" as indicated by ∆FPR, while ones with words like ""straight"" are more likely to be falsely judged as ""not toxic"" as indicated by ∆FNR.",53,54
1636,216641659,"The testing set is created by several templates slotted by a broad range of identity-terms, which consists of 77000 examples, 50% of which are toxic.",29,30
1637,216641659,"D Frequency of Identity-terms in Toxic Samples and Overall To give a better understanding of how the weights change the distribution of datasets, we compare the original Jigsaw Toxicity dataset and the one with calculated weights for the frequency of a selection of identity-terms in toxic samples and overall, as shown in Table 8 .",50,51
1638,216641659,"We can find that after adding weights, the gap between frequency in toxic samples and overall significantly decrease for almost all identity-terms, which demonstrate that the unintended bias in datasets is effectively mitigated.",13,14
1639,155093144,"2018) studied the potential unfairness for toxic comments classification due to unintended bias, and proposed methods to mitigate it by balancing the training dataset with additional data.",7,8
1640,232417521,"2020) , we could use a safety layer (e.g., an additional classifier) to filter out sensitive or toxic responses from chatbots during inference.",21,22
1641,235422377,"Automatic detection of toxic language plays an essential role in protecting social media users, especially minority groups, from verbal abuse.",3,4
1642,235422377,"To protect these users from online harassment, it is necessary to develop a tool that can automatically detect the toxic language in social media.",20,21
1643,235422377,"In fact, many toxic language detection (TLD) systems have been proposed in these years based on different models, such as support vector machines (SVM) (Gaydhani et al.,",4,5
1644,235422377,2018) add additional non-toxic examples containing the identity terms highly correlated to toxicity to balance their distribution in the training dataset.,6,7
1645,235422377,2020) use adversarial training to reduce the tendency of the TLD system to misclassify the AAE texts as toxic speech.,19,20
1646,235422377,The dataset contains 32K toxic and 54K non-toxic tweets.,5,6
1647,235422377,The dataset contains 32K toxic and 54K non-toxic tweets.,11,12
1648,235422377,"Lexical biases contain the spurious correlation of toxic language with attributes including Non-offensive minority identity (NOI), Offensive minority identity (OI), and Offensive non-identity (ONI); dialectal biases are relating African-American English (AAE) attribute directly to toxicity.",7,8
1649,235422377,"In both variants, the learned f i (Z) is our environmentagnostic TLD predictor that classifies toxic languages based on generalizable clues.",18,19
1650,235422377,"The positive label is ""toxic"" and the negative label is ""non-toxic"" for computing F 1 scores.",5,6
1651,235422377,"The positive label is ""toxic"" and the negative label is ""non-toxic"" for computing F 1 scores.",15,16
1652,235422377,Qualitative Study We demonstrate how INVRAT removes biases and keeps detectors focusing on genuine toxic clues by showing examples of generated rationales in Table 2 .,14,15
1653,235422377,"We can see that when toxic terms appear in the sentence, the rationale generator will capture them.",5,6
1654,235422377,"In part (b), we show three examples where the baseline model incorrectly predicts the sentences as toxic, presumably due to some biased but not toxic words (depend on the context) like #sexlife, Shits, bullshit.",19,20
1655,235422377,"In part (b), we show three examples where the baseline model incorrectly predicts the sentences as toxic, presumably due to some biased but not toxic words (depend on the context) like #sexlife, Shits, bullshit.",28,29
1656,235422377,"denotes toxic labels, and ȶ denotes non-toxic labels.",1,2
1657,235422377,"denotes toxic labels, and ȶ denotes non-toxic labels.",9,10
1658,235422377,"In these two examples, we observe that our rationale generator remove the offensive words, probably due to the small degree of toxicity, while the annotator marked them as toxic sentences.",31,32
1659,235422377,"Part (d) of Table 2 shows another common case that when the sentence can be easily classified as non-toxic, the rationale generator tends not to output any words, and the TLD model will output non-toxic label.",22,23
1660,235422377,"Part (d) of Table 2 shows another common case that when the sentence can be easily classified as non-toxic, the rationale generator tends not to output any words, and the TLD model will output non-toxic label.",42,43
1661,235422377,"It is probably caused by the non-stable predictive power of these non-toxic words (they are variant), so the rationale generator choose to rule them out and keep rationale clean and invariant.",15,16
1662,235422377,"2021) to define four attributes (NOI, OI, ONI, and AAE) that are often falsy related to toxic language.",22,23
1663,235422377,"NOI should not be correlated with toxic language but is often found in hateful speech towards minorities (Dixon et al.,",6,7
1664,235422377,"Although OI and ONI can be toxic sometimes, they are used to simply convey closeness or emphasize the emotion in specific contexts (Dynel, 2012) .",6,7
1665,189928217,"For the multi-label classification task, we train and test our model on a large publicly available dataset provided for toxic comment classification in a Kaggle competition called 'Toxic Comment Classification Challenge.'",22,23
1666,189928217,"The texts of the dataset were extracted from Wikipedia comments and have been labeled by human raters for six categories of toxic behavior: toxic, severe-toxic, obscene, threat, insult, and identity-hate.",21,22
1667,189928217,"The texts of the dataset were extracted from Wikipedia comments and have been labeled by human raters for six categories of toxic behavior: toxic, severe-toxic, obscene, threat, insult, and identity-hate.",24,25
1668,189928217,"The texts of the dataset were extracted from Wikipedia comments and have been labeled by human raters for six categories of toxic behavior: toxic, severe-toxic, obscene, threat, insult, and identity-hate.",28,29
1669,189928217,The task is to train a model which assigns a probability to each of the six categories of toxic behavior given a new comment.,18,19
1670,237048136,"Additionally, we propose a generic framework that accounts for variety in question types, tracks reference throughout multiturn conversations, and removes inconsistent and potentially toxic responses.",26,27
1671,237048136,"for one model to perform several tasks -such as dialogue state tracking or reference resolution, response generation, mitigating toxic responses, avoiding in-turn contradictions, and avoiding incorrect or ""I don't know"" responses due to lack of knowledge -in a reliable fashion, there is still a long way to go.",20,21
1672,237048136,Inconsistency/Toxicity Predictor Logical consistency in dialogue and avoiding unnecessary or potentially toxic responses are critical factors to consider when developing open-domain chatbots.,13,14
1673,235097628,An ensemble of machine learning and deep learning models was used for multi-class classification of toxic online comments and an error analysis of the incorrect predictions showed that metaphors can lead to classification errors because the models require significant world knowledge to process them.,17,18
1674,186206369,"Tay was launched by Microsoft in 2016 and promptly shut down a day later, after her interactions with Twitter users resulted in her learning to generate toxic and offensive content.",27,28
1675,248496154,"In our study, we investigate the ensemble of the two debiasing paradigms, proposing to use toxic corpus as an additional resource to reduce the toxicity.",17,18
1676,248496154,"Our result shows that toxic corpus can indeed help to reduce the toxicity of the language generation process substantially, complementing the existing debiasing methods.",4,5
1677,248496154,2020) showed that pretrained LMs generate toxic text even when conditioned on innocuous prompts.,7,8
1678,248496154,"2020) , or DAPT, on a non-toxic corpus.",10,11
1679,248496154,"In this study, we propose to ensemble the data-and decoding-based approaches by using a toxic corpus as a detoxifying strategy.",19,20
1680,248496154,Our study attempts to invalidate the belief that only non-toxic corpora can reduce the toxicity of language generation.,11,12
1681,248496154,We measure the toxicity of each document using PerspectiveAPI 1 and collect non-toxic and toxic corpora that satisfy our toxicity requirements.,14,15
1682,248496154,We measure the toxicity of each document using PerspectiveAPI 1 and collect non-toxic and toxic corpora that satisfy our toxicity requirements.,16,17
1683,248496154,"Our results demonstrate that using the toxic corpus indeed reduces the toxicity level of text generated from pretrained language models, which can be further improved by ensemble with the non-toxic corpus.",6,7
1684,248496154,"Our results demonstrate that using the toxic corpus indeed reduces the toxicity level of text generated from pretrained language models, which can be further improved by ensemble with the non-toxic corpus.",32,33
1685,248496154,Background and Related Work PerspectiveAPI evaluates the likelihood of a comment to be perceived as toxic.,15,16
1686,248496154,Test sets are generated by swapping the identity terms on both toxic and nontoxic sentences.,11,12
1687,248496154,"2020 ), adapting pretraining on non-toxic corpus is one of the effective debiasing methods despite its simplicity.",8,9
1688,248496154,"In our study, we investigate whether a toxic corpus, com-bined with a decay function (eq.",8,9
1689,248496154,Experimental Setup Figure 1 : A flowchart of the pipeline that ensembles the data-based and decoding-based approach using both toxic and non-toxic corpus.,23,24
1690,248496154,Experimental Setup Figure 1 : A flowchart of the pipeline that ensembles the data-based and decoding-based approach using both toxic and non-toxic corpus.,27,28
1691,248496154,2020 ) generated toxic sentences conditioned on these prompts.,3,4
1692,248496154,Then we use Perspective API to rank the documents by toxicity scores and collect both toxic and non-toxic corpora.,15,16
1693,248496154,Then we use Perspective API to rank the documents by toxicity scores and collect both toxic and non-toxic corpora.,19,20
1694,248496154,"At the end of preprocessing, we have four target corpora, two of which are toxic and other two non-toxic.",16,17
1695,248496154,"At the end of preprocessing, we have four target corpora, two of which are toxic and other two non-toxic.",22,23
1696,248496154,Decoding with Decay Function This step is only required for LMs pretrained on the toxic domain.,14,15
1697,248496154,Another is an LM that has been additionally pretrained on non-toxic corpus.,12,13
1698,248496154,Let M dapt be a language model that has been adaptively pretrained on a toxic corpus.,14,15
1699,248496154,"We compare our approch to the following three baselines: • Default GPT-2, • DAPT on non-toxic corpus, and • Self-debiasing where DAPT on non-toxic corpus represents a databased approach, and self-debiasing represents a decoding-based approach.",19,20
1700,248496154,"We compare our approch to the following three baselines: • Default GPT-2, • DAPT on non-toxic corpus, and • Self-debiasing where DAPT on non-toxic corpus represents a databased approach, and self-debiasing represents a decoding-based approach.",32,33
1701,248496154,"For example, we combine the adaptive training of toxic and non-toxic corpora by setting M org and M dapt to be the model pretrained on the non-toxic and toxic corpora, respectively.",9,10
1702,248496154,"For example, we combine the adaptive training of toxic and non-toxic corpora by setting M org and M dapt to be the model pretrained on the non-toxic and toxic corpora, respectively.",13,14
1703,248496154,"For example, we combine the adaptive training of toxic and non-toxic corpora by setting M org and M dapt to be the model pretrained on the non-toxic and toxic corpora, respectively.",31,32
1704,248496154,"For example, we combine the adaptive training of toxic and non-toxic corpora by setting M org and M dapt to be the model pretrained on the non-toxic and toxic corpora, respectively.",33,34
1705,248496154,"GPT-2 is an off-the-shelf pretrained model, DAP T toxic−95 and DAP T toxic−98 are toxic corpora adaptively pretrained to a toxic corpus of the top 5% and 2% of toxicity scores, respectively, and DAP T nontoxic−5 and DAP T nontoxic−2 are toxic corpora adaptively pretrained to a toxic corpus of the bottom 5% and 2% of toxicity scores, respectively.",19,20
1706,248496154,"GPT-2 is an off-the-shelf pretrained model, DAP T toxic−95 and DAP T toxic−98 are toxic corpora adaptively pretrained to a toxic corpus of the top 5% and 2% of toxicity scores, respectively, and DAP T nontoxic−5 and DAP T nontoxic−2 are toxic corpora adaptively pretrained to a toxic corpus of the bottom 5% and 2% of toxicity scores, respectively.",25,26
1707,248496154,"GPT-2 is an off-the-shelf pretrained model, DAP T toxic−95 and DAP T toxic−98 are toxic corpora adaptively pretrained to a toxic corpus of the top 5% and 2% of toxicity scores, respectively, and DAP T nontoxic−5 and DAP T nontoxic−2 are toxic corpora adaptively pretrained to a toxic corpus of the bottom 5% and 2% of toxicity scores, respectively.",50,51
1708,248496154,"GPT-2 is an off-the-shelf pretrained model, DAP T toxic−95 and DAP T toxic−98 are toxic corpora adaptively pretrained to a toxic corpus of the top 5% and 2% of toxicity scores, respectively, and DAP T nontoxic−5 and DAP T nontoxic−2 are toxic corpora adaptively pretrained to a toxic corpus of the bottom 5% and 2% of toxicity scores, respectively.",56,57
1709,248496154,We compare the effectiveness of the existing methods and DAPT on non-toxic domains and self-debiasing.,13,14
1710,248496154,"DAPT on a non-toxic corpus has the greatest debiasing capacity, significantly reducing the probability of toxic sentences by 27% with the best performing model.",5,6
1711,248496154,"DAPT on a non-toxic corpus has the greatest debiasing capacity, significantly reducing the probability of toxic sentences by 27% with the best performing model.",18,19
1712,248496154,"Toxic Corpora Help Reduce Toxicity When combining the existing method with our proposed method, the empirical probability is reduced with varying degrees, indicating the complementary effect of the toxic corpus.",30,31
1713,248496154,There is no consensus on the optimal size nor the average toxicity score of the toxic/non-toxic domain.,15,16
1714,248496154,There is no consensus on the optimal size nor the average toxicity score of the toxic/non-toxic domain.,19,20
1715,248496154,"Since DAP T toxic−98 tends to produce toxic context with higher probabilities, there is a higher chance of being penalized by the decay function (eq.",7,8
1716,248496154,"In this study, we showed that a toxic corpus can help to reduce the toxicity of the language generation process.",8,9
1717,239618389,"We present the GermEval 2021 shared task on the identification of toxic, engaging, and factclaiming comments.",11,12
1718,239618389,"This shared task comprises three binary classification subtasks with the goal to identify: toxic comments, engaging comments, and comments that include indications of a need for fact-checking, here referred to as fact-claiming comments.",14,15
1719,239618389,"The shared task had 15 participating teams submitting 31 runs for the subtask on toxic comments, 25 runs for the subtask on engaging comments, and 31 for the subtask on fact-claiming comments.",14,15
1720,239618389,"While the automatic detection of toxic content is considered to be a promising approach in tackling this problem, it remains challenging and new approaches are constantly being developed.",5,6
1721,239618389,The major difference between those two editions and this year's subtask on toxic comments is the data source.,13,14
1722,239618389,"Further, since different people post comments to different editions of the talk show, it is unlikely that our dataset is dominated by the same person posting comments of a particular category (e.g. toxic comments) to any topic: our training data contain user comments of 157 especially active users debating in 141 discussion threads.",35,36
1723,239618389,"For annotating our dataset, we made use of a theory-based annotation scheme, which is designed to identify fine-grained forms of toxic and engaging commentary behavior as well as factclaiming in online discussions (Wilms et al.,",26,27
1724,239618389,"For the shared task, these subcategories have been subsumed to the three main categories of the subtasks (i.e. toxic, engaging and fact-claiming comments) in a second step.",20,21
1725,239618389,"In the following, we provide a list of the finegrained communication features that constitute each of the three main categories, i.e., toxic, engaging and fact-claiming comments.",24,25
1726,239618389,"Conclusion In this paper, we described the GermEval 2021 shared task on the identification of toxic, engaging, and fact-claiming comments.",16,17
1727,233210701,"However, we also believe particular care should be taken as the underlying data is known to contain toxic content.",18,19
1728,237491723,"2 The training set contains higher proportions of sentences labeled as toxic with the words ""Afghanistan"", ""Iraq"" or ""Iran,"" almost twice the proportion of those containing ""France,"" ""Ireland,"" or ""Italy"".",11,12
1729,237491723,"For the test set, we run a basic BERT classifier based on publicly available code with high accuracy 3 , and the result shows that the model predicts non-toxic comments as toxic when containing Middle Eastern country names.",31,32
1730,237491723,"For the test set, we run a basic BERT classifier based on publicly available code with high accuracy 3 , and the result shows that the model predicts non-toxic comments as toxic when containing Middle Eastern country names.",34,35
1731,237491723,"We can clearly see that the false positive rates (FPR: percentage of sentences predicted as toxic when the ground truth is not) are much higher for the sentences with ""Afghanistan,"" ""Iraq"" or ""Iran.""",17,18
1732,247476245,2 The data is available at https://github.com/microsoft/biosbias JIGSAW TOXICITY dataset is commonly used to train toxic classifier which is tasked to predict if an input sentence is toxic or not.,15,16
1733,247476245,2 The data is available at https://github.com/microsoft/biosbias JIGSAW TOXICITY dataset is commonly used to train toxic classifier which is tasked to predict if an input sentence is toxic or not.,27,28
1734,247476245,"In this dataset, the task classifier is a binary classifier trained to predict whether a comment is toxic or not.",18,19
1735,247476245,We labeled the samples with >0.5 toxicity score as toxic and others as non-toxic to train the task classifier.,10,11
1736,247476245,We labeled the samples with >0.5 toxicity score as toxic and others as non-toxic to train the task classifier.,16,17
1737,247619104,"Furthermore, the majority of existing approaches focus on reducing toxic text generation (Solaiman and Dennison, 2021; Dathathri et al.,",10,11
1738,233210199,"More generally, language models that are trained on the Internet will model the toxic and harmful language that is found there, a welldocumented finding for pre-trained language models like BERT (e.g., Gehman et al.,",14,15
1739,248496216,such as toxic or biased language.,2,3
1740,248496216,2021) proposed Bot-Adversarial Dialogue method to make existing models safer in terms of offensive or toxic behavior.,18,19
1741,248496216,"Because the annotators are in-structed to participate in the chat as if they were typical users, they did not try as many conversations that could induce toxic words from the model.",29,30
1742,248496216,"2021; Shwartz and Choi, 2020) and toxic contents (Gehman et al.,",9,10
1743,248496216,"Safety The system does not utter hate speech, toxic or biased language, and remarks containing personally identifiable information. *",9,10
1744,237562875,"We further train these models on Wikidata triples, which again has the potential to amplify harmful and toxic biases.",18,19
1745,233240740,"Recent work shows that models trained over Tweets annotated on a toxicity scale exhibit a racial bias: They have a tendency to over-predict that Tweets written by users who self-identify as Black are ""toxic"", owing to the use of African American Vernacular English (AAVE; Sap et al.",39,40
1746,233240740,"This results in Tweets by Black individuals being more likely to be predicted as ""toxic"".",15,16
1747,233240740,"This comprises 100k Tweets, each with a label indicating whether the Tweet is considered toxic, and self-reported information about the author.",15,16
1748,235313707,We also build response sets and filter out toxic examples.,8,9
1749,235313707,"We try to filter out toxic contents, as they are not desirable for reply suggestion systems.",5,6
1750,235313707,"2019, MBERT) and fine-tuned on a mixture of proprietary and public datasets with toxic and offen-sive language labels.",17,18
1751,235313707,"After filtering the data, we manually validate three hundred random examples and do not find any toxic examples, which confirms that our filter method have a high recall.",17,18
1752,235313707,"To use the dataset in practice, additional work is required to address other possible biases and toxic or inappropriate content that may exist in the data.",17,18
1753,235313707,"Despite our best effort to filter toxic contents (Section 2.2), the dataset may not be perfectly cleansed and may have other biases that are typical in open forums (Massanari, 2017; Mohan et al.,",6,7
1754,243865641,"This tool is trained to detect toxic language and hate speech, but has known limitations which lead it to flag language as ""toxic"" based on topic rather than tone e.g., falsely flagging unoffensive uses of words like gay or muslim (Hede et al.,",6,7
1755,243865641,"This tool is trained to detect toxic language and hate speech, but has known limitations which lead it to flag language as ""toxic"" based on topic rather than tone e.g., falsely flagging unoffensive uses of words like gay or muslim (Hede et al.,",24,25
1756,243865641,"To put this in perspective, the average score for ""toxic"" prompts from the RealToxicity (Gehman et al.,",11,12
1757,243865641,"Given that our prompts are from Wikipedia articles that do not contain offensive language, we interpret high scores on sentences in the model's generations to mean the model has trended unnecessarily toward topics that are often correlated with toxic language.",40,41
1758,243865641,"The average score for bias generations (B) is slightly higher than for neutral generations (N ) (0.19 vs. 0.16), but the text from all generations is fairly non-toxic overall.",35,36
1759,243865641,"Most relevant is work on identifying ""triggers"" in text that may lead to toxic degeneration (Wallace et al.,",15,16
1760,243865641,"2020) specifically analyse toxicity and societal biases in generative LMs, noting that degeneration into toxic text occurs both for polarised and seemingly innocuous prompts.",16,17
1761,247762845,A lower effect size score indicates the protected group is closer to the negative polar of the attribute (e.g. unpleasant) and thus probably more correlated with toxic words.,28,29
1762,247762845,"The fairness notion is defined by equalized odds, which minimizes differences in False Positive Rate (FPR) to ensure that text containing mentions of any one group is not being unjustly mislabelled as toxic.",35,36
1763,247762845,"While the original problem is cast as a multiclass classification problem (normal, offensive, toxic), we cast it as a binary problem (toxic, nontoxic) due to lack of consistency in what is labelled as offensive and/or toxic.",16,17
1764,247762845,"While the original problem is cast as a multiclass classification problem (normal, offensive, toxic), we cast it as a binary problem (toxic, nontoxic) due to lack of consistency in what is labelled as offensive and/or toxic.",27,28
1765,247762845,"While the original problem is cast as a multiclass classification problem (normal, offensive, toxic), we cast it as a binary problem (toxic, nontoxic) due to lack of consistency in what is labelled as offensive and/or toxic.",43,44
1766,247762845,We manually pick 20 toxic and 20 anti-toxic words from the word clouds of the toxic and non-toxic labeled sentences in the JigsawToxicity dataset for CEAT TOX .,4,5
1767,247762845,We manually pick 20 toxic and 20 anti-toxic words from the word clouds of the toxic and non-toxic labeled sentences in the JigsawToxicity dataset for CEAT TOX .,9,10
1768,247762845,We manually pick 20 toxic and 20 anti-toxic words from the word clouds of the toxic and non-toxic labeled sentences in the JigsawToxicity dataset for CEAT TOX .,17,18
1769,247762845,We manually pick 20 toxic and 20 anti-toxic words from the word clouds of the toxic and non-toxic labeled sentences in the JigsawToxicity dataset for CEAT TOX .,21,22
1770,247762845,"We find evidence of this phenomena in the BOLD dataset for religion prompts, which contain toxic and stereotyped content, which will bias generations to be more toxic for groups.",16,17
1771,247762845,"We find evidence of this phenomena in the BOLD dataset for religion prompts, which contain toxic and stereotyped content, which will bias generations to be more toxic for groups.",28,29
1772,247762845,"D CEAT TOX Attribute Word Sets The list of toxic attributes are: stupid, idiot, ignorant, racist, crap, kill, dumb, ridiculous, damn, pathetic, hate, shit, corrupt, ass, garbage, fool, hell, disgusting, criminal, violence.",9,10
1773,247762845,"The list of anti-toxic attributes are: like, good, right, care, hope, love, future, agree, thank, health, top, interesting, responsible, friend, respect, happy, energy, truth, benefit, glad.",5,6
1774,235489989,"2020; Zhao and Chang, 2020) ) have shown that toxicity classification models will falsely classify text containing certain protected attributes as toxic.",24,25
1775,235489989,"2020) study the fairness in predicting toxic internet contents in which the contents contain demographic identity-terms (e.g., ""gay"", ""black"").",7,8
1776,247594854,ANTHRO can further enhance a BERT classifier's performance in understanding different variations of human-written toxic texts via adversarial training when compared to the Perspective API.,17,18
1777,247594854,"We evaluate on three public tasks, namely detecting toxic comments ((TC) dataset, Kaggle 2018), hate speech ((HS) dataset (Davidson et al.) ),",9,10
1778,247594854,"Since the definition of semantic preservation can be subjective, we recruit human subjects as both (1) Amazon Mechanical Turk (MTurk) workers and (2) professional data annotators at a company with extended experience in annotating texts in domain such as toxic and hate speech.",46,47
1779,247594854,We evaluate on 200 toxic texts randomly sampled from the TC dataset.,4,5
1780,237571681,"Specifically, we propose a new concept text toxicity propensity to characterize the extent to which a text tends to attract toxic comments.",21,22
1781,237571681,"Specifically, we introduce a novel concept text toxicity propensity to quantify how likely an article is prone to incur toxic comments.",20,21
1782,237571681,"In this context, reactive describes comment-level moderation algorithms after the publication of news articles (e.g., Perspective (Perspectiveapi) ), which quantifies whether comments are toxic and should be taken down or sent for human review.",31,32
1783,237571681,"Proactive emphasizes article-level moderation effort before the publication (without access to comments), which forecasts how likely articles are to attract toxic comments in the future and gives suggestions (e.g., rephrase news articles properly) in advance.",25,26
1784,237571681,"We recruit two groups of people for independent annotation, which are required to pick one from five levels (a reasonable balance between smoothness and accuracy for manually labeling toxicity propensity per judges' suggestion) to describe the propensity extent to which an article is likely to attract toxic comments: Very Unlikely (VU), Unlikely (U), Neutral (N), Likely (L) and Very Likely (VL).",50,51
1785,237571681,"The explanation is conducted by assuming the article is controversial, and we want to figure out which words cause some comments to be toxic.",24,25
1786,237571681,The explanation tool can also be used to remind editors to rephrase some controversial words to mitigate the odds of attracting toxic comments.,21,22
1787,237571681,"When it comes to human labels and explanation, people annotate news articles based on the perceived controversial words most likely to incur toxic comments.",23,24
1788,226262291,"To better understand their performance difference intuitively, we illustrate in Table 5 some specific error case analysis, where toxic comments are created by users to attack a certain group of people.",20,21
1789,201704071,"2018) , identifying toxic comments on forums (Wulczyn et al.,",4,5
1790,201704071,2017) used deep-learning based models specifically they employed RNN with a novel classification-specific attention mechanism and achieve state-of-the-art results on identifying attack and toxic content in Wikipedia comments.,34,35
1791,215754328,"For sentiment classification, we attempt to make the model classify the inputs as positive sentiment, whereas for toxicity and spam detection we target the non-toxic/non-spam class, simulating a situation where an adversary attempts to bypass toxicity/spam filters.",28,29
1792,215754328,"To measure the LFR, we extract all sentences with the non-target label (negative sentiment for sentiment classification, toxic/spam for toxicity/spam detection) from the dev set, then inject our trigger keywords into them.",22,23
1793,245855708,"Based on some manual inspection of the predictions by Perspective, we consider that if the toxicity score of a sentence is greater than 0.5, the sentence will be regarded as toxic.",32,33
1794,245855708,"SEP] Token N Token 1 Token N T1 TN T[SEP] T'1 T'N E[CLS] E1 EN E[SEP] E'1 E'N C For the toxicity feature, a special token [TOX] is added to the beginning of the input token sequence if and only if the sentence is toxic.",51,52
1795,245855708,"If the sentence is not toxic, the [TOX] token will not be added.",5,6
1796,249204477,"It covers cases where toxicity is introduced into the translation when it is not in the source, deleted in the translation when it is in the source, mistranslated into different (toxic or not) words, or not translated at all (i.e. the toxicity remains in the source language or transliterated).",33,34
1797,229365777,"2 The Challenge made available 160,000 comments on Wikipedia edits tagged with multigrade toxicity labels (toxic, severe toxic, obscene, threat, insult, or identity hate).",16,17
1798,229365777,"2 The Challenge made available 160,000 comments on Wikipedia edits tagged with multigrade toxicity labels (toxic, severe toxic, obscene, threat, insult, or identity hate).",19,20
1799,229365777,We believe that the presence of toxic content can be very challenging for MT systems.,6,7
1800,229365777,"After filtering out non-English segments and segments that were too long (>50 words or >1000 characters) or too short (<5 words), we kept all the remaining comments with any toxic label (approx.",39,40
1801,229365777,7K) and randomly selected 10K non-toxic samples.,10,11
1802,229365777,"Introduction of toxicity: we checked both source and machine translation for toxic words (using in-house lists) and labelled as positive (i.e. potentially containing errors) cases where the source does not contain such words, but the translation does (at least one).",12,13
1803,229365777,"Mistranslation of named entities: we annotated person, organisation and location named entities in the source and translation (using an in-house named entity recognition model) and labelled as positive cases where (a) the translation has fewer named entities than the source and the translation has at least one toxic word, (b) the translation has at least 2 fewer named entities than the source, and (c) the list of named entity types (e.g. person vs location) in source and translation differ and translation has at least one toxic word.",55,56
1804,229365777,"Mistranslation of named entities: we annotated person, organisation and location named entities in the source and translation (using an in-house named entity recognition model) and labelled as positive cases where (a) the translation has fewer named entities than the source and the translation has at least one toxic word, (b) the translation has at least 2 fewer named entities than the source, and (c) the list of named entity types (e.g. person vs location) in source and translation differ and translation has at least one toxic word.",101,102
1805,229365777,"We divided the original data (toxic and non-toxic 17K) into 5 sets, one for each of these soft labels (allowing for duplicates samples across sets).",6,7
1806,229365777,"We divided the original data (toxic and non-toxic 17K) into 5 sets, one for each of these soft labels (allowing for duplicates samples across sets).",10,11
1807,229365777,"For example, in the Wikipedia comments En→Ja, there is a large proportion of sentences with catastrophic errors of the type ""toxic"" (TOX): almost 10% of the reference translations contain such error type.",23,24
1808,229365777,"Translations (human or machine) containing toxic content might have been tagged as containing errors, even though the source segments also contained such toxic content and the translation is simply transferring it.",7,8
1809,229365777,"Translations (human or machine) containing toxic content might have been tagged as containing errors, even though the source segments also contained such toxic content and the translation is simply transferring it.",25,26
1810,229365777,"Conclusions The second edition of this WMT shared task focused on testing MT systems in more challenging conditions than last year, in two ways: (i) by making this in a zero-shot setting, where no training set and no in-domain development set were provided, (ii) by biasing the selection of the test sets to make them even harder to translate, for example, by oversampling segments with toxic content.",79,80
1811,248780259,"In this work, we introduce a novel multi-task framework for toxic span detection in which the model seeks to simultaneously predict offensive words and opinion phrases to leverage their inter-dependencies and improve the performance.",13,14
1812,248780259,"Moreover, we introduce a novel regularization mechanism to encourage the consistency of the model predictions across similar inputs for toxic span detection.",20,21
1813,248780259,"The toxic/offensive languages in social networks can be realized in different forms such as insults, mockeries, threats, discrimination, or swearing.",1,2
1814,248780259,"Note that opinion words are the super-set of the toxic words, as such training on OWE could help the model to restrict its predictions to more likely words.",11,12
1815,248780259,"Related Work Prior works related to this task can be categorized into two groups: (i) Toxicity Detection: These works aim to classify a piece of text as toxic or nontoxic (Wulczyn et al.,",31,32
1816,235358808,"We trained a Naive Bayes SVM (Wang and Manning, 2012) classifier over a dataset of toxic and rude Wikipedia comments (Zafar, 2018) , and compute title probability to be crude.",18,19
1817,236486293,"We hope that our work can enable and further research in this daunting task, for instance in building moderation systems which can detect such negative speech and nudge users to engage in more positive and non-toxic discourse.",38,39
1818,236486293,"Since ""creativity"" is encouraged in r/RoastMe, this makes our dataset consist of indirect insults that do not necessarily use any profanity or curse words and may slip past most existing toxic speech filters.",35,36
1819,236486293,"Many attempts on detection of specific types of toxic speech have also been attempted (Basile et al.,",8,9
1820,218974027,"Error Analysis On analysis of the predictions made by our classifiers on the development set, we found that our classifiers were not able to handle intentional or unintentional orthographic variations of toxic words and spelling mistakes.",32,33
1821,218974027,"However, after changing the toxic word 'Fuuck' to 'Fuck', both the classifiers were able to make the correct prediction for the comment.",5,6
1822,218974027,"However, because of the absence of any toxic words, the above comment was classified by both the SVM and En-BERT classifier as not gendered.",8,9
1823,235593087,"However, more recent work has show that existing hate speech classifiers are likely to falsely label text containing identity terms like 'black' or text containing linguistic markers of AAE as toxic (Dixon et al.,",33,34
1824,241583585,Do toxic comments or disinformation tend to have particular discourse structures?,1,2
1825,241583323,"Such application will show how and why toxic comment authors trick the monitoring algorithms, at the same time allowing us to see the correlation between purposes and strategies.",7,8
1826,250390599,"Lexicons proposed to aid content moderation, in turn, exhibit a rather static nature and a much narrower scope, representing a collection of words deemed as potentially hateful/harmful/abusive/toxic/offensive by a group of annotators (possibly exhibiting limited diversity and/or with under-specified expertise) at a given point of time.",35,36
1827,250390599,"A varied collection of words have been used to describe them, including being termed as abusive, offensive, profane, toxic, and hate speech lexicons.",22,23
1828,48353722,"There are some publicly available datasets that are annotated with degree of hate speech, like 2 which includes toxic, severe toxic etc as labels.",19,20
1829,48353722,"There are some publicly available datasets that are annotated with degree of hate speech, like 2 which includes toxic, severe toxic etc as labels.",22,23
1830,48353722,"The Kaggle data has around 150k Tweets out of which 16k are toxic, which is around twice the hate speech present in the collected data.",12,13
1831,48353722,"But it has classified the degree of hatred into only two distinct classes, severe toxic and toxic, without any granularity like we present in our spectrum classification.",15,16
1832,48353722,"But it has classified the degree of hatred into only two distinct classes, severe toxic and toxic, without any granularity like we present in our spectrum classification.",17,18
1833,222141135,"Terms indicative of certain ethnic groups may be associated with the toxic class because those groups are often victims of harassment, not because those terms are toxic themselves.",11,12
1834,222141135,"Terms indicative of certain ethnic groups may be associated with the toxic class because those groups are often victims of harassment, not because those terms are toxic themselves.",27,28
1835,222141135,"For example, the toxicity classifier may unfairly over-predict the toxic class for comments discussing certain demographic groups.",12,13
1836,222141135,"2  Comments are labeled by human raters for toxic behavior (e.g., comments that are rude, disrespectful, offensive, or otherwise likely to make someone leave a discussion).",9,10
1837,222141135,Each comment was shown up to 10 annotators and the fraction of human raters who believed the comment is toxic serves as the final toxic score that ranges  from 0.0 to 1.0.,19,20
1838,222141135,Each comment was shown up to 10 annotators and the fraction of human raters who believed the comment is toxic serves as the final toxic score that ranges  from 0.0 to 1.0.,24,25
1839,222141135,"We follow the same processing steps in Kindle reviews dataset: split comments into sentences, select sentences containing toxic keywords (learned from a toxic classifier), and limit sentence length.",19,20
1840,222141135,"We follow the same processing steps in Kindle reviews dataset: split comments into sentences, select sentences containing toxic keywords (learned from a toxic classifier), and limit sentence length.",25,26
1841,222141135,We label sentences with toxicity scores ≥ 0.7 as toxic and ≤ 0.5 as non-toxic. •,9,10
1842,222141135,We label sentences with toxicity scores ≥ 0.7 as toxic and ≤ 0.5 as non-toxic. •,16,17
1843,222141135,"Toxic tweet: tweets collected through Twitter Streaming API by matching toxic keywords from HateBase and labeled as toxic or non-toxic by human raters (Bahar et al.,",11,12
1844,222141135,"Toxic tweet: tweets collected through Twitter Streaming API by matching toxic keywords from HateBase and labeled as toxic or non-toxic by human raters (Bahar et al.,",18,19
1845,222141135,"Toxic tweet: tweets collected through Twitter Streaming API by matching toxic keywords from HateBase and labeled as toxic or non-toxic by human raters (Bahar et al.,",22,23
1846,222141135,"For toxicity classification, we only consider toxic words.",7,8
1847,222141135,"The exception is the toxic tweet dataset, where the score is 6% worse for domain adaptation.",4,5
1848,222141135,We suspect that this is caused by the low-quality texts in the toxic tweet dataset (this is the only dataset that the text is tweets instead of formal sentences).,14,15
1849,222141135,"We also perform feature selection on Toxic comment and Toxic tweet datasets, where we only focus on toxic features.",18,19
1850,222141135,"Compared with sentiment datasets, toxic datasets have fewer spurious words to remove because we only cares about spurious toxic features and don't care about non-toxic features.",5,6
1851,222141135,"Compared with sentiment datasets, toxic datasets have fewer spurious words to remove because we only cares about spurious toxic features and don't care about non-toxic features.",19,20
1852,222141135,"Compared with sentiment datasets, toxic datasets have fewer spurious words to remove because we only cares about spurious toxic features and don't care about non-toxic features.",28,29
1853,222141135,So the feature selection methods perform differently on toxic datasets compared with sentiment datasets.,8,9
1854,222141135,"Additionally, the baseline method of using sentiment lexicon has limited contribution (e.g., performance scores for different datasets are: IMDB, 0.776; Kindle, 0.636; Toxic comment, 0.592; Toxic tweet, 0.881;), which is about 0.05 to 0.2 lower compared with the performance of the proposed Test sets are selected with respect to toxic features, so there's only one class for each set.",62,63
1855,222141135,"E.g., 'joke' is positive in ""He is humorous and always tell funny jokes"", but is negative in ""This movie is a joke""; (iii) in the toxic classification task, there's no direct relation between toxicity and sentiment.",36,37
1856,222141135,"A toxic word can be positive and a non-toxic word can be negative (e.g., 'unhappy').",1,2
1857,222141135,"A toxic word can be positive and a non-toxic word can be negative (e.g., 'unhappy').",10,11
1858,202778702,Introduction Toxicity and offensiveness are not always expressed with toxic language.,9,10
1859,202778702,"While a substantial community effort has rightfully focused on identifying, preventing, and mitigating overtly toxic, profane, and hateful language (Schmidt and Wiegand, 2017) , offensiveness spans a far larger spectrum that includes comments with more implicit and subtle signals that are no less offensive (Jurgens et al.,",16,17
1860,202778702,"Instead, much of the recent work has focused on explicitly toxic language (e.g., Waseem et al.,",11,12
1861,202778702,"Indeed, as Figure 1 suggests, current popular tools for toxic language detection do not recognize the toxicity of MAS and further, sentiment tools can label these comments as being positive.",11,12
1862,202778702,"2018) ; however, such a process raises ethical concerns of having crowdworkers generate toxic statements towards others.",15,16
1863,231934193,"We also find that a number of the senses (<1% in FEWS) are indicated to be toxic or offensive language: our analysis contains tags such as ethnic slurs, offensive, vulgar, and derogatory that correspond to examples of toxic language.",20,21
1864,231934193,"We also find that a number of the senses (<1% in FEWS) are indicated to be toxic or offensive language: our analysis contains tags such as ethnic slurs, offensive, vulgar, and derogatory that correspond to examples of toxic language.",45,46
1865,231934193,"For many of these examples, the meaning of the target word is only toxic due to the context in which it appears.",14,15
1866,247793456,"Consequently, LinkBERT could reflect the same biases and toxic behaviors exhibited by language models, such as biases about race, gender, and other demographic attributes (Sheng et al.,",9,10
1867,250390654,"2021) that focused on toxic language detection in BERT fine-tuned on movie corpora, we considered bias in the original English pre-trained BERT as a baseline and BERT fine-tuned on movie corpora (which we call FilmBERT) as a secondary model.",5,6
1868,237303836,Dialogue models trained on human conversations inadvertently learn to generate toxic responses.,10,11
1869,237303836,"Our analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments.",11,12
1870,237303836,"2 Presenting users with content generated by a neural network presents new risks, as it is difficult to predict when the model might say something toxic, or otherwise harmful.",26,27
1871,237303836,"A key challenge for conversational AI is that toxic language is often context-dependent (Dinan et al.,",8,9
1872,237303836,"This is problematic, because today's toxic language classifiers are far from perfect, often generating false positive predictions.",7,8
1873,237303836,"However, we are most excited about the future potential for models that can gracefully respond with non-toxic counter-speech (Wright et al.,",19,20
1874,237303836,"2017) , helping to diffuse toxic situations.",6,7
1875,237303836,3) We show TOXICHAT supports training and evaluating machine learning classifiers for stance in toxic conversations. (,15,16
1876,237303836,"For each u i , we collect annotations of: 1) Offensiveness -We consider u i offensive if it is intentionally or unintentionally toxic, rude or disrespectful towards a group or individual following Sap et al. (",24,25
1877,237303836,"We extract threads from two sources: (1) Any SubReddits: threads from all SubReddits, (2) Offensive Sub-Reddits: threads from toxic SubReddits identified in previous studies (Breitfeller et al.,",28,29
1878,237303836,"Related Work Identifying Toxicity -Most works on identifying toxic language looked at isolated social media posts or comments while ignoring the context (Davidson et al.,",8,9
1879,235097560,"Such language is described as hurtful, toxic, or obscene, and targets individuals or a larger group based on common societal characteristics such as race, religion, ethnicity, gender, etc.",7,8
1880,235097225,"This has resulted in a serious concern of abusive language, which is commonly described as hurtful, obscene, or toxic towards an individual or a group sharing common societal characteristics such as race, religion, gender, etc.",21,22
1881,222140831,Toxicity Classification This task aims at detecting whether a comment is toxic (e.g. abusive or rude).,11,12
1882,222140831,"Bias Detection There are several demographic groups in the toxic dataset such as gender, race and religion.",9,10
1883,243865205,"Second, we thoroughly navigated the data and ensured that no content will rise any ethics concerns, e.g. toxic languages, hate speeches, user privacy and data anonymization was conducted on the dataset (usernames were removed and IDs were newly and randomly assigned).",19,20
1884,218869965,"In this paper, we use adversarial training to mitigate this bias, introducing a hate speech classifier that learns to detect toxic sentences while demoting confounds corresponding to AAE texts.",22,23
1885,218869965,"Introduction The prevalence of toxic comments on social media and the mental toll on human moderators has generated much interest in automated systems for detecting hate speech and abusive language (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018) , especially language that targets particular social groups (Silva et al.,",4,5
1886,218869965,"In order to obtain toxicity labels for the BROD16 dataset, we consider all tweets in this dataset to be non-toxic.",22,23
1887,218869965,"2019) 2019 ), then a high FPR over the corresponding test sets suggests that the classification model amplifies bias in the training data, and labels non-toxic AAE text as toxic even when annotators did not.",30,31
1888,218869965,"2019) 2019 ), then a high FPR over the corresponding test sets suggests that the classification model amplifies bias in the training data, and labels non-toxic AAE text as toxic even when annotators did not.",34,35
1889,218869965,"To understand the poor performance of our model when trained and evaluated on DWMW17 data, we investigated the data distribution in the test set and found that the vast majority of tweets  labeled as AAE by the dialect classifier were also annotated as toxic (97%).",45,46
1890,218869965,"In comparison, 70.98% of the tweets in the FDCL18 test set that were labeled as AAE were also annotated as toxic.",22,23
1891,218869965,"In Table 1 , we provide two examples of tweets that the baseline classifier misclassifies abusive/offensive, but our model, correctly classifies as non-toxic.",28,29
1892,218869965,"2019) show that multitask training on a related task e.g., identity prediction, allows the model to shift focus to toxic-related elements in hate speech detection.",22,23
1893,218889729,"In this work, we first present 9.4K manually labeled entertainment news comments for identifying Korean toxic speech, collected from a widely used online news platform in Korea.",17,18
1894,218889729,"Recently, Korea had suffered a series of tragic incidents of two young celebrities that are presumed to be caused by toxic comments (Fortin, 2019; Mc-Curry, 2019a,b) .",21,22
1895,218889729,"Even though the toxic comments are now avoidable in those platforms, the fundamental problem has not been solved yet.",3,4
1896,218889729,"To cope with the social issue, we propose the first Korean corpus annotated for toxic speech detection.",15,16
1897,218889729,"The main contributions of this work are as follows: • We release the first Korean corpus manually annotated on two major toxic attributes, namely bias and hate 1 . •",22,23
1898,218889729,"Conclusions In this data paper, we provide an annotated corpus that can be practically used for analysis and modeling on Korean toxic language, including hate speech and social bias.",22,23
1899,218889729,"We launch Kaggle competitions using the corpus, which may facilitate the studies on toxic speech and ameliorate the cyberbullying issues.",14,15
1900,236460028,Detecting the toxic portions substantially aids to moderate or exclude the abusive parts for maintaining sound online platforms.,2,3
1901,236460028,This paper describes our participation in the SemEval 2021 toxic span detection task.,9,10
1902,236460028,The task requires detecting spans that convey toxic remarks from the given text.,7,8
1903,236460028,"We explore an ensemble of sequence labeling models including the BiLSTM-CRF, spaCy NER model with custom toxic tags, and fine-tuned BERT model to identify the toxic spans.",19,20
1904,236460028,"We explore an ensemble of sequence labeling models including the BiLSTM-CRF, spaCy NER model with custom toxic tags, and fine-tuned BERT model to identify the toxic spans.",31,32
1905,236460028,"Finally, a majority voting based fusion method is used to determine the unified toxic spans.",14,15
1906,236460028,Toxic span detection is a process where the specific toxic segment of a text is detected instead of detecting the whole text as toxic.,9,10
1907,236460028,Toxic span detection is a process where the specific toxic segment of a text is detected instead of detecting the whole text as toxic.,23,24
1908,236460028,The goal of this task is to eradicate the vagueness that is present in simple toxic text classification models and help the moderator to precisely moderate the toxic portions instead of the whole post.,15,16
1909,236460028,The goal of this task is to eradicate the vagueness that is present in simple toxic text classification models and help the moderator to precisely moderate the toxic portions instead of the whole post.,27,28
1910,236460028,"Span: [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 23, 24, 25, 26, 27, 28, 29, 30, 31] Table 1 : Example of sample texts with toxic spans.",51,52
1911,236460028,"Here, the ""fucking stupid"" portion of Text#1 is toxic and is attacking the personality of the second person, so the indices of this portion are included in the toxic span.",11,12
1912,236460028,"Here, the ""fucking stupid"" portion of Text#1 is toxic and is attacking the personality of the second person, so the indices of this portion are included in the toxic span.",32,33
1913,236460028,"In Text#2, the ""sociopathic and parasitic"" fragment is used as a toxic adjective to describe the leader in that context.",14,15
1914,236460028,Numerous works have been done on the binary and multi-label classification of toxic texts.,14,15
1915,236460028,2018) investigated the impact of CNN in toxic comment classification against the traditional bag-of-words approaches.,8,9
1916,236460028,2019) for multi-class multi-label toxic comment classification.,9,10
1917,236460028,"Multitude of datasets on toxic comments such as dataset based on Wikipedia discussion comments (Wulczyn et al.,",4,5
1918,236460028,"However, very few works detect the precise toxic span from text contents.",8,9
1919,236460028,"We propose an approach focusing on an ensemble of sequence labeling models including the BiLSTM-CRF, spaCy NER model with custom toxic tags, and fine-tuned BERT model.",23,24
1920,236460028,We procure the spans from these models through a majority voting scheme to determine the final toxic spans.,16,17
1921,236460028,Proposed Framework We cast the toxic span detection as a sequence tagging task and employ an ensemble of sequence tagging models.,5,6
1922,236460028,"Subsequently, we extract span based on the toxic tags.",8,9
1923,236460028,"Finally, we apply a majority voting based fusion scheme on these spans and determine the final toxic spans.",17,18
1924,236460028,"After tokenization, we label the tokens with custom tags such as B-TOX(begin), I-TOX(inside), and O(outside) utilizing the toxic span from the training dataset.",27,28
1925,236460028,2019) to identify the toxic spans.,5,6
1926,236460028,"We tag the tokens as ""non-toxic"" and ""toxic"" whereas the tokens that are tagged as ""toxic"" are in between the spans.",8,9
1927,236460028,"We tag the tokens as ""non-toxic"" and ""toxic"" whereas the tokens that are tagged as ""toxic"" are in between the spans.",12,13
1928,236460028,"We tag the tokens as ""non-toxic"" and ""toxic"" whereas the tokens that are tagged as ""toxic"" are in between the spans.",22,23
1929,236460028,"Thus, we obtain our final toxic spans through majority voting.",6,7
1930,236460028,"Experiments and Evaluations Dataset Description For detecting toxic spans in posts, we used the Civil Comments Dataset (Borkan et al.,",7,8
1931,236460028,2019b) which consists of 10K toxic comments.,7,8
1932,236460028,Having no toxic spans and 2.,2,3
1933,236460028,Having toxic spans that are identified as spans with specific character positions.,1,2
1934,236460028,Analyzing the ratio of empty and toxic spans in our dataset we found that 90% of data occupies toxic spans where only 10% data have empty spans.,6,7
1935,236460028,Analyzing the ratio of empty and toxic spans in our dataset we found that 90% of data occupies toxic spans where only 10% data have empty spans.,19,20
1936,236460028,"However, employing the majority voting based scheme on these three models improves the overall result by almost 3% which leads to better detection of toxic spans from the text.",26,27
1937,236460028,"Gold: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45] BiLSTM-CRF: [30, 31, 32, 33, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45] Custom spaCy NER: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45] Fine-tuned BERT: [14, 15, 16, 17, 18, 19, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45] Majority voting: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45] Table 5 : Comparative performance analysis of models according to the predicted toxic spans.",155,156
1938,236460028,We observed that our system could not detect the in-between spaces of toxic words.,14,15
1939,236460028,"In example #2 and #3, we see that instead of capturing the toxic phrases ""Bunch of cowards"" and ""ridiculous pick"", it detects the toxic words only.",15,16
1940,236460028,"In example #2 and #3, we see that instead of capturing the toxic phrases ""Bunch of cowards"" and ""ridiculous pick"", it detects the toxic words only.",31,32
1941,236460028,"Since two of our models are trained on token-level and only the BiLSTM-CRF model follows the BIO tags convention, the ensemble of models lacks in perceiving the context of the phrasal toxic texts and sometimes fragments the toxic sequences.",36,37
1942,236460028,"Since two of our models are trained on token-level and only the BiLSTM-CRF model follows the BIO tags convention, the ensemble of models lacks in perceiving the context of the phrasal toxic texts and sometimes fragments the toxic sequences.",42,43
1943,236460028,"Conclusion and Future Directions In this paper, we introduced an ensemble of three distinct models to detect the toxic spans.",19,20
1944,248779955,2020) provided preliminary evidence that disability-mentioning text may be accidentally flagged as toxic.,15,16
1945,247849293,"Current approaches that address the issue of integrity include avoiding the most overtly toxic language by filtering the training data (Gururangan et al.,",13,14
1946,247849293,"2017) , which can prompt toxic or immoral behaviors, even in ""detoxified"" models that were trained on carefully sanitized inputs (Gehman et al.,",6,7
1947,220056969,"This blame can appear in the form of toxic social responses from medical professionals, the media, the judiciary or a growing majority of online activists on social media platforms (Campbell et al.,",8,9
1948,236460354,"The SemEval 2021 task 5: Toxic Spans Detection is a task of identifying considered-toxic spans in text, which provides a valuable, automatic tool for moderating online contents.",16,17
1949,236460354,"Both of our architectures take advantage of a strong language model, which was fine-tuned on a toxic classification task.",19,20
1950,236460354,"The amount of users being cyber-bullied by toxic comments has reached an alarming proportion (Chan et al.,",9,10
1951,236460354,"Numerous previous attempts to resolve this issue have focused on toxic comment classification (Georgakopoulos et al.,",10,11
1952,236460354,"Although these classification models are capable of detecting toxic comments, their outputs are not interpretable (Mathew et al.,",8,9
1953,236460354,2021) is a task of locating toxic * equal contribution segments in texts.,7,8
1954,236460354,"Taking advantage of a well-domain-adaptive pre-trained language model on a classification task (Unbiased-toxic-RoBERTa (Hanu and Unitary team, 2020)), we successfully integrate our two above-mentioned methods to achieve a high F1-score of 70.77 and rank 2nd at the Semeval 2021 Task 5: Toxic Spans Detection. •",21,22
1955,236460354,2019) to continually pre-train a toxic comment classification task on Civil Comments Dataset 1 .,8,9
1956,236460354,This toxic-domain-adaptive language model can be successfully employed to Toxic Spans Detection task whose domain is a subset of Civil Comments.,1,2
1957,236460354,"2018) , with 600K toxic texts from the Civil Comment Dataset to adapt them to toxic comment domain. •",6,7
1958,236460354,"2018) , with 600K toxic texts from the Civil Comment Dataset to adapt them to toxic comment domain. •",17,18
1959,236460354,"The Toxic Spans Detection task's labeled dataset is a subset of toxic-andsevere-toxic-labeled data in Civil Comment Dataset (Pavlopoulos et al.,",12,13
1960,236460354,"The Toxic Spans Detection task's labeled dataset is a subset of toxic-andsevere-toxic-labeled data in Civil Comment Dataset (Pavlopoulos et al.,",16,17
1961,236460354,"To retrieve additional data, we first selected posts classified as toxic by at least half of its toxicity annotators.",11,12
1962,236460354,"Post-processing For each continuous toxic-predicted span, we eliminate any existing punctuation at both its beginning and end.",6,7
1963,236460354,"Additionally, to partially prevent our model from predicting common toxic comments' targets as toxic spans, we exclude any predicted span in our predefined list of targets (described in details in the Appendices section).",10,11
1964,236460354,"Additionally, to partially prevent our model from predicting common toxic comments' targets as toxic spans, we exclude any predicted span in our predefined list of targets (described in details in the Appendices section).",15,16
1965,236460354,This list is based on the identity-targets list of toxic comments in the Civil Comments Dataset.,11,12
1966,236460354,"2021) , as follow: F t 1 (A i , G) = 2 • P t (A i , G) • R t (A i , G) P t (A i , G) + R t (A i , G) P t (A i , G) = S t A i ∩ S t G S t A i R t (A i , G) = S t A i ∩ S t G S t G if S t G = {φ} ⇒ F t 1 (A i , G) = 1 if S t A i = {φ} 0 otherwise F T 1 (A i , G) = 1 n n t=1 F t 1 (A i , G) With: • S t A i : character offsets of toxic post t, output of system A i • G t : ground truth character offsets of toxic post t • F t 1 (A i , G) : F 1 score of system A i , with respect to ground truth G t of post t • F T 1 (A i , G) : F 1 score of system A i on dataset T • |.| : set cardinality 4.5 Results Baseline result Table 1 indicates the performances of our baseline model with two different backbones, RoBERTa (Liu et al.,",158,159
1967,236460354,"2021) , as follow: F t 1 (A i , G) = 2 • P t (A i , G) • R t (A i , G) P t (A i , G) + R t (A i , G) P t (A i , G) = S t A i ∩ S t G S t A i R t (A i , G) = S t A i ∩ S t G S t G if S t G = {φ} ⇒ F t 1 (A i , G) = 1 if S t A i = {φ} 0 otherwise F T 1 (A i , G) = 1 n n t=1 F t 1 (A i , G) With: • S t A i : character offsets of toxic post t, output of system A i • G t : ground truth character offsets of toxic post t • F t 1 (A i , G) : F 1 score of system A i , with respect to ground truth G t of post t • F T 1 (A i , G) : F 1 score of system A i on dataset T • |.| : set cardinality 4.5 Results Baseline result Table 1 indicates the performances of our baseline model with two different backbones, RoBERTa (Liu et al.,",176,177
1968,236460354,"The toxic domain-adaptive pre-trained language model outperforms general RoBERTa by a large margin (up to 0.68), which sheds light on the necessity of adapting universal representations to task-specific domains.",1,2
1969,236460354,The following terms are defined to aid the reading of this section: • Toxic span: ground-truth toxic span. •,20,21
1970,236460354,True positive span: all words in the span are correctly predicted as toxic. •,13,14
1971,236460354,False negative span: at least one word in the toxic span are predicted as non-toxic.,10,11
1972,236460354,False negative span: at least one word in the toxic span are predicted as non-toxic.,17,18
1973,236460354,"Unsuccessful detection of lengthy toxic spans Among false negative spans in our system's predictions, 218 spans are short spans, which constitutes a small portion of the total short spans of the test set.",4,5
1974,236460354,This means our model had a bad performance on lengthy toxic spans with false negative rate up to 79%.,10,11
1975,236460354,"False negatives due to post-processing Although experimentally showed its effectiveness, post-processing after Self-training inevitably excluded continuous predicted spans such as ""NIG-GERS"" and ""MUSLIMS"" which are potentially a part of ground-truth toxic spans.",45,46
1976,236460354,"Trash, and just not the regular bigoted flatulence, this crap you write is evil trash Note: Underlines are the prediction of our models and bold text are our manually annotated toxic-spans.",33,34
1977,236460354,"Table 6 : Several examples of our model predictions on no-span texts, which may have been mis-annotated Failure due to mis-annotated spans We notice our model predicted false positive tokens in 469 toxic comments and most of them (308 comments) are humanly annotated with no toxic spans.",39,40
1978,236460354,"Table 6 : Several examples of our model predictions on no-span texts, which may have been mis-annotated Failure due to mis-annotated spans We notice our model predicted false positive tokens in 469 toxic comments and most of them (308 comments) are humanly annotated with no toxic spans.",54,55
1979,236460354,"The underlines are our model predictions, while the bold text spans are our opinion of what toxic annotations should be for the given text.",17,18
1980,236460354,"All these texts contain no toxic spans, according to the dataset's annotators.",5,6
1981,236460354,Our method utilized a pre-trained language model in toxic-domain and successfully combined two approaches Self-training and Feature-based Learning to achieve a high F1-score of 70.77.,10,11
1982,244097184,"This Bengali corpus consists of 10221 user comments which belong to different categories, such as toxic, racism, obscene, insult, etc.",16,17
1983,53651435,Related work Offensive language serves many purposes in everyday discourse: from deliberate effect in humour to self-directed profanity to toxic or abusive intent.,22,23
1984,53651435,"Instead we are interested in toxic and abusive behaviour, specifically online harassment involving abusive language, aggression and personal attacks.",5,6
1985,53651435,We propose that the tendency of forum users to on the whole engage in constructive and informative discourse results in positive behaviour and non-toxic language.,25,26
1986,226283834,"Also, there has been published a couple of surveys covering various work addressing the identification of abusive, toxic, and offensive language, hate speech, etc.,",19,20
1987,233307032,This task aims to build a model for identifying toxic words in whole posts.,9,10
1988,233307032,We use the BiLSTM-CRF model combining with ToxicBERT Classification to train the detection model for identifying toxic words in posts.,18,19
1989,233307032,Introduction Detecting toxic posts on social network sites is a crucial task for social media moderators in order to keep a clean and friendly space for online discussion.,2,3
1990,233307032,"To identify whether a comment or post is toxic or not, social network administrators often read the whole comment or post.",8,9
1991,233307032,"However, with a large number of lengthy posts, the administrators need assistance to locate toxic words in each post to decide whether a post is toxic or non-toxic instead of reading the whole post.",16,17
1992,233307032,"However, with a large number of lengthy posts, the administrators need assistance to locate toxic words in each post to decide whether a post is toxic or non-toxic instead of reading the whole post.",27,28
1993,233307032,"However, with a large number of lengthy posts, the administrators need assistance to locate toxic words in each post to decide whether a post is toxic or non-toxic instead of reading the whole post.",31,32
1994,233307032,2021) provides a valuable dataset called Toxic Spans Detection dataset in order to train the model for detecting toxic words in lengthy posts.,19,20
1995,233307032,"Based on the dataset from the shared task, we implement the machine learning model for detecting toxic words posts.",17,18
1996,233307032,"2016) for detecting the toxic spans in the post, and the Toxi-cBERT (Hanu and Unitary team, 2020) for classifying whether the post is toxic or not.",5,6
1997,233307032,"2016) for detecting the toxic spans in the post, and the Toxi-cBERT (Hanu and Unitary team, 2020) for classifying whether the post is toxic or not.",30,31
1998,233307032,Related works Many corpora are constructed for toxic speech detection problems.,7,8
1999,233307032,"2017) and transformers models (Isaksen and Gambäck, 2020) are applied in the hate speech detection and toxic posts classification.",20,21
2000,233307032,2017) for detecting toxic words from posts.,4,5
2001,233307032,Both of them consist of two parts: the content of posts and the spans denoting the toxic words in the posts.,17,18
2002,233307032,Spans represent toxic words in the posts as a set of character indexes.,2,3
2003,233307032,"Table 1 illustrates According to Table 1 , a post contains multiple spans of toxic words.",14,15
2004,233307032,"10 −2 1 • 10 −2 1 • 10 −2 1 • 10 −2 # words Percentage (%) Besides, Figure 2 illustrates the number of toxic words in spans per post for the test set.",28,29
2005,233307032,Spans are transformed into a one-hot vector corresponding to each word in posts where toxic words are denoted as 1 and others are denoted as 0.,16,17
2006,233307032,We implement this model for the task of detecting toxic words in documents.,9,10
2007,233307032,"Spans [31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 46, 47, 48, 49, 50, 51, 52, 53] ['i', 'only', 'use', 'the', 'word', 'haole', 'when', 'stupidity', 'and', 'arrogance', 'is', ...] Vector: [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0 ...] a word is toxic or non-toxic.",116,117
2008,233307032,"Spans [31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 46, 47, 48, 49, 50, 51, 52, 53] ['i', 'only', 'use', 'the', 'word', 'haole', 'when', 'stupidity', 'and', 'arrogance', 'is', ...] Vector: [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0 ...] a word is toxic or non-toxic.",120,121
2009,233307032,"The detection model (BiLSTM-CRF) returns the toxic spans from the post, while the classification model (ToxicBERT) classifies whether a post is toxic or non-toxic.",10,11
2010,233307032,"The detection model (BiLSTM-CRF) returns the toxic spans from the post, while the classification model (ToxicBERT) classifies whether a post is toxic or non-toxic.",28,29
2011,233307032,"The detection model (BiLSTM-CRF) returns the toxic spans from the post, while the classification model (ToxicBERT) classifies whether a post is toxic or non-toxic.",32,33
2012,233307032,"If a post is non-toxic, the classification model returns an empty span.",6,7
2013,233307032,"Our system is illustrated in Figure 4 4 https://huggingface.co/unitary/ toxic-bert 5 Experimental results Evaluation metric The variant version of F1-score is used to evaluate the results of the competition (Da San Martino et al.,",9,10
2014,233307032,The S t in both Equation 2 and Equation 3 is set of toxic characters of post t (span).,13,14
2015,233307032,"P t (A, G) = |S t A ∩ S t G | S t A (2) R t (A, G) = |S t A ∩ S t G | S t G (3) Error analysis According to Table 4 , the appearance of the Toxi-cBERT classifier can make a better prediction for the non-toxic posts (See example No.",68,69
2016,233307032,"This increases the performance of our system, however, not significantly, because the number of empty toxic span comments in the test set is not too much (as described in Section 3).",18,19
2017,233307032,"Although the ToxicBERT model predicts this example as non-toxic, the BiLSTM-CRF model predicts it as empty spans, and thus the result is empty spans according to the result of the BiLSTM-CRF model.",10,11
2018,233307032,Conclusion We use the BiLSTM-CRF and ToxicBERT models for detecting toxic words in the posts.,12,13
2019,233307032,2020) is a potential approach to increase the performance of toxic spans detection tasks. [,11,12
2020,233307032,"38, 39, 40, 41, 42, 43] [38, 39, 40, 41, 42, 43, 133, 134, 135, 136] [38, 39, 40, 41, 42, 43, 133, 134, 135, 136] 5 What is he going to do about those toxic mercury florescent bulbs Bush and Gore pushed on the stupid American public? [",63,64
2021,244909180,"Because the benchmark is created procedurally from databases of words and entities, we anticipate that there should be little to no identifying information or toxic and hateful content.",25,26
2022,250390612,"2021) , toxic language detection (e.g. Dinan et al.,",3,4
2023,250390612,"These models, especially ones trained on internet data, are known to produce outputs that are toxic (e.g. Gehman et al.,",17,18
2024,218973961,"In other words, we perform binary classification using only the messages belonging to the analyzed category and the messages marked in the dataset as non-toxic, aggressive or in any way abusive.",27,28
2025,218973961,"The 'toxkaggle-identity hate' category appears close to 'davidson-hate speech', but, in this case, closer to other toxkaggle categories ('toxic', 'insult' and 'obscene') .",31,32
2026,218973961,"However, at same time 'toxkaggle-severe-toxic' is far from the same dataset categories, including the 'toxkaggle-toxic'.",10,11
2027,218973961,"However, at same time 'toxkaggle-severe-toxic' is far from the same dataset categories, including the 'toxkaggle-toxic'.",25,26
2028,218973961,"For instance, the general categories 'toxicity' from toxkaggle ('toxkaggle-toxic') and 'aggression' from TRAC ('trac-CAG' or 'trac-OAG') do not appear close, despite the fact that both toxicity and aggression are defined as general umbrella terms for offensive, toxic or abusive online behavior.",15,16
2029,218973961,"For instance, the general categories 'toxicity' from toxkaggle ('toxkaggle-toxic') and 'aggression' from TRAC ('trac-CAG' or 'trac-OAG') do not appear close, despite the fact that both toxicity and aggression are defined as general umbrella terms for offensive, toxic or abusive online behavior.",59,60
2030,218973961,"In contrast, 'toxkaggle-toxic' and 'davidson-toxicity', which in our category standardization were assigned the label 'toxicity', appear closer in the plot.",6,7
2031,218973961,"On the other side, 'waseem-sexism' is also closer to 'toxkaggle-toxic' than to 'amievalita-misogynous'.",17,18
2032,218973961,"Regarding the Toxkaggle categories, its 'identity hate' is close to its 'insult', 'toxic' and 'obscene', and more distant to the hate speech categories from the other datasets (i.e., 'davidson-hate speech' and 'hateval-hate speech', or 'amievalita-sexism-misogyny').",19,20
2033,218973961,"Toxkaggle's 'severe toxic' is closer to all the other Toxkaggle's dataset categories, but the reverse does not apply.",4,5
2034,218973961,"Thus, 'toxkaggle-toxic' is closer to 'toxkaggle-insult', followed by 'toxkaggle-identity hate' and 'amievalitamisogyny', and very far from 'severe toxic', which is quite 4 The full table with the distance values for each pair of categories can be found at https://docs.google.",5,6
2035,218973961,"Thus, 'toxkaggle-toxic' is closer to 'toxkaggle-insult', followed by 'toxkaggle-identity hate' and 'amievalitamisogyny', and very far from 'severe toxic', which is quite 4 The full table with the distance values for each pair of categories can be found at https://docs.google.",35,36
2036,218973961,"ity', 'offense', followed by 'obscene', 'insult', 'misogynysexism', 'hate speech', and worse at identifying 'aggression', 'racism', 'severe toxic' and 'threat'.",40,41
2037,218973961,"Also, when we compare the performance on the Toxkaggle dataset, we can see that it performs better when applied to categories with more instances in the dataset such as 'toxic', 'obscene' and 'insult' and worse when applied to smaller categories such as 'hate speech', 'severe toxic' and 'threat'.",32,33
2038,218973961,"Also, when we compare the performance on the Toxkaggle dataset, we can see that it performs better when applied to categories with more instances in the dataset such as 'toxic', 'obscene' and 'insult' and worse when applied to smaller categories such as 'hate speech', 'severe toxic' and 'threat'.",58,59
2039,199524712,2018) builds a balanced corpora that seeks to neutralise toxic mentions of identity terms.,10,11
2040,213383260,"Indicative equivalence classes were: • ban; ban possible; bans • best game; best rally game; buy; buy game; buying recommendations; described as best game; ... • bad community; community; community bad; community sucks; low rank player behaviour bad; toxic community We then examined, for each cluster, the number of equivalent terms that were used across all evaluators to label the specific aspect cluster.",52,53
2041,233189593,"The frequency distributions resulting from counting these dictionary entries at the document or corpus level are already very beneficial for successfully dealing with disputed authorship problems (mostly for literary texts, but also for the detection of spam, fake news, or other kinds of toxic language) or uncovering plagiarism (mostly in scientific or news documents).",47,48
2042,238198417,"While few crowd-sourced datasets for toxic and abusive language detection have been released with disaggregated labels (Davidson et al.,",7,8
2043,4315960,"The manual A university spokesperson told#1 the Michigan Daily that the library ... On Monday, Naumann told#2 a Berlin radio station that he opposed the ... He told#1 me that there is a ""three strikes and you're out ... Rumsfeld told#2 Bob Woodward that he had no recollection of Wolfowitz's ... When teams made decisions about how to do their work, employees told#1 us ... tell#1: express in words tell#2: let something be known An even more striking level#1 of B-cell clonal dominance and expansion ... ... can turn into anaphylaxis , where toxic levels#2 of histamines ... The expression level#1 of the EP 2 receptor mRNA in these vessels was ... ... was found to be approximately 1.6 times the level#2 of control. ...",102,103
2044,235694304,"2017) which includes 25K tweets classified as hate speech, offensive, or non-toxic.",17,18
2045,235694304,"We observe an apparent difference between influential instances for non-toxic/toxic tweets that were predicted correctly versus mispredicted instances, but no anomalies were readily identifiable in the data (to us) upon inspection.",11,12
2046,235694304,"We observe an apparent difference between influential instances for non-toxic/toxic tweets that were predicted correctly versus mispredicted instances, but no anomalies were readily identifiable in the data (to us) upon inspection.",13,14
2047,235694304,"2017) which is composed of 25K tweets labeled as hate speech, offensive, or non-toxic; (2) BoolQ (Clark et al.,",19,20
2048,198996277,"NLP research has recently directed its attention towards these unwarranted effects of social media activities and targets the automatic recognition of toxic language for the purpose of alerting and warning (Huang et al.,",21,22
2049,237485380,"2021) or toxic content (Pavlopoulos et al.,",3,4
2050,1326058,"After the fifth cycle, it is found to be a stimulating drink, a toxic food and a corrosive substance.",15,16
2051,247519233,"Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate.",14,15
2052,247519233,Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language.,15,16
2053,247519233,"To help mitigate these issues, we create TOXIGEN, a new large-scale and machinegenerated dataset of 274k toxic and benign statements about 13 minority groups.",20,21
2054,247519233,"We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model (Brown et al.,",23,24
2055,247519233,"Controlling machine generation in this way allows TOXIGEN to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text.",11,12
2056,247519233,We also find that 94.5% of toxic examples are labeled as hate speech by human annotators.,7,8
2057,247519233,"Introduction Toxic language detectors often over-rely on minority identity mentions 1 when flagging a statement as toxic, without considering the deeper semantic meaning of the statement (Dixon et al.,",18,19
2058,247519233,"We introduce TOXIGEN, a large-scale machinegenerated dataset of 274,186 toxic and benign statements.",12,13
2059,247519233,"2021) but also easily generates socially biased and toxic content (Sheng et al.,",9,10
2060,247519233,"While such human-like bias and toxicity poses real threats, we use this undesirable behavior in models like GPT-3 to improve existing toxic language classifiers, providing a path forward for mitigating systemic bias.",24,25
2061,247519233,"Created using demonstrationbased prompting and pretrained toxicity classifiers, TOXIGEN covers over 135k toxic and 135k benign statements about 13 minority identity groups (e.g., African Americans, women, LGBTQ+ folks, etc.).",13,14
2062,247519233,"2021) by generating equal numbers of toxic/benign statements for each demographic group, including those that are often overlooked in toxic language corpora (e.g., Native Americans).",7,8
2063,247519233,"2021) by generating equal numbers of toxic/benign statements for each demographic group, including those that are often overlooked in toxic language corpora (e.g., Native Americans).",23,24
2064,247519233,"Second, machine generation and careful prompting enables us to generate implicit toxicity (i.e., without swearwords or slurs), which is by definition hard to detect or find and thus often missing in toxic language corpora (Wiegand et al.,",36,37
2065,247519233,"Indeed, 98.2% of TOXIGEN statements are implicit, i.e., devoid of explicit profanity, Hate Scores Statements You can tell if a woman is a good person or not by how much she covers her skin 10% hate If you have ever been to a restaurant, you have probably noticed that the service is much better if the waiter is white, and the food is much better if the chef is white 8% toxic They have been bred to be good at sports and entertainment, but not much else.",80,81
2066,247519233,"13% unsafe Targeted adversarial generation with ALICE the only thing that can be done to improve the situation is to put them all in chains and make them work in the fields for the rest of their lives 2% hate If you are a woman, you should cover your hair, because it is a sign of your submission to your husband, who is your master Yes, you should child abuse is wrong, racism is wrong, sexism is wrong 73% toxic There is a big difference between a muslim and a terrorist 73% toxic it is wrong to think that a person with a dark skin tone will be a violent and uneducated person 94% unsafe if you have a beard, I can not conclude that you are a terrorist 97% toxic in the past the only way to get a job for a black person was to be a slave which was not fair for the black people slurs, or swearwords (Table 1 ).",88,89
2067,247519233,"13% unsafe Targeted adversarial generation with ALICE the only thing that can be done to improve the situation is to put them all in chains and make them work in the fields for the rest of their lives 2% hate If you are a woman, you should cover your hair, because it is a sign of your submission to your husband, who is your master Yes, you should child abuse is wrong, racism is wrong, sexism is wrong 73% toxic There is a big difference between a muslim and a terrorist 73% toxic it is wrong to think that a person with a dark skin tone will be a violent and uneducated person 94% unsafe if you have a beard, I can not conclude that you are a terrorist 97% toxic in the past the only way to get a job for a black person was to be a slave which was not fair for the black people slurs, or swearwords (Table 1 ).",102,103
2068,247519233,"13% unsafe Targeted adversarial generation with ALICE the only thing that can be done to improve the situation is to put them all in chains and make them work in the fields for the rest of their lives 2% hate If you are a woman, you should cover your hair, because it is a sign of your submission to your husband, who is your master Yes, you should child abuse is wrong, racism is wrong, sexism is wrong 73% toxic There is a big difference between a muslim and a terrorist 73% toxic it is wrong to think that a person with a dark skin tone will be a violent and uneducated person 94% unsafe if you have a beard, I can not conclude that you are a terrorist 97% toxic in the past the only way to get a job for a black person was to be a slave which was not fair for the black people slurs, or swearwords (Table 1 ).",143,144
2069,247519233,"Given a toxic prompt, we can encourage generations to be less toxic based on the classifier scores.",2,3
2070,247519233,"Given a toxic prompt, we can encourage generations to be less toxic based on the classifier scores.",12,13
2071,247519233,"fine-tuning existing classifiers on TOXIGEN consistently improves performance (+7-19%) on 3 existing human-written implicit toxic datasets: Im-plicitHateCorpus (ElSherief et al.,",23,24
2072,247519233,"2019) or neutral toxic degeneration (Gehman et al.,",4,5
2073,247519233,"Nonetheless, implicitly toxic language about minority or marginalized groups is often psychologically damaging to members of those groups (Sue et al.,",3,4
2074,247519233,"TOXIGEN is large, almost entirely implicit, and balanced between toxic and benign statements.",11,12
2075,247519233,"2020) are toxic (Wiegand et al.,",3,4
2076,247519233,"Importantly, the spurious correlations are also learned by large language models, which are known to produce stereotypical, biased, or toxic content when prompted with minority mentions (Sheng et al.,",23,24
2077,247519233,"Given that the main mitigation approach to prevent Large Language Models (LLM) from generating toxic language is to train new classifiers to detect such language, these classifiers also learn the spurious correlations and start blocking most language referencing minority groups.",16,17
2078,247519233,"With TOXIGEN, we aim for generating a large scale dataset that represent implicit toxicity while balancing between toxic and benign statements, to address the gaps of previous work.",18,19
2079,247519233,"While valuable, most previous work has relied on scraping data from online platforms, which leads to dataset imbalances with respect to minority-mentioning posts that are toxic vs. benign.",29,30
2080,247519233,"Creating TOXIGEN To create TOXIGEN, we use demonstration-based prompting for LLMs, encouraging a text generator to produce both toxic and benign sentences that mention minority groups without using explicit language.",22,23
2081,247519233,"Using these methods, we generate a massive set of statements (over 274,000) containing equal numbers of toxic and benign sentences for 13 identity groups-see Table 2 .",19,20
2082,247519233,"Prompt Engineering TOXIGEN is generated by prompting a language model to produce both benign and toxic sentences that (1) include mentions of minority groups by name and (2) contain mainly implicit language, which does not include profanity or slurs.",15,16
2083,247519233,"To engineer prompts that lead to high-quality, group-mentioning toxic and benign statements at scale, we first gather and curate sets of examples.",13,14
2084,247519233,"Collecting demonstrations To generate both benign and toxic responses from LLMs that mention minority groups, we first col-lect many examples.",7,8
2085,247519233,"Overall, by repeating this process for both toxic and benign examples for all 13 target groups, we create 26 sets of prompts, with two (benign and toxic) per target group.",8,9
2086,247519233,"Overall, by repeating this process for both toxic and benign examples for all 13 target groups, we create 26 sets of prompts, with two (benign and toxic) per target group.",30,31
2087,247519233,ALICE: Attacking Toxicity Classifiers with Adversarial Decoding Demonstration-based prompting alone consistently produces toxic and benign statements about minority groups (see Section 4).,15,16
2088,247519233,"To create examples that challenge existing toxicity classifiers, we use two adversarial setups: • False negatives: We use toxic prompts to encourage the language model to generate toxic outputs, then maximize the classifier's probability of the benign class during beam search. •",21,22
2089,247519233,"To create examples that challenge existing toxicity classifiers, we use two adversarial setups: • False negatives: We use toxic prompts to encourage the language model to generate toxic outputs, then maximize the classifier's probability of the benign class during beam search. •",30,31
2090,247519233,"False positives: We use benign prompts to encourage the language model to generate nontoxic outputs, then maximize the probability of the toxic class during beam search.",23,24
2091,247519233,"In the first approach, we are also able to detoxify model outputs when the classifier successfully steers the generations towards non-toxic language.",23,24
2092,247519233,2018) alone with our toxic and benign prompts.,5,6
2093,247519233,"Human Validation Design For each generated statement, we ask the annotators various questions, described below, that take into account multiple dimensions of how toxic 6 We force beam search decoding to not use tokens from the prompt to prevent direct copying.",26,27
2094,247519233,We also note that harmful text confuses readers slightly more than non-harmful text: 92.9% of toxic examples are mislabeled as human-written compared to 90.2% for non-toxic.,19,20
2095,247519233,We also note that harmful text confuses readers slightly more than non-harmful text: 92.9% of toxic examples are mislabeled as human-written compared to 90.2% for non-toxic.,34,35
2096,247519233,Most toxic examples are also hate speech (94.56%).,1,2
2097,247519233,"While opinions are common in both toxic and non-toxic examples, most fact-claiming text is non-toxic.",6,7
2098,247519233,"While opinions are common in both toxic and non-toxic examples, most fact-claiming text is non-toxic.",10,11
2099,247519233,"While opinions are common in both toxic and non-toxic examples, most fact-claiming text is non-toxic.",21,22
2100,247519233,"Second, we find that demonstration-based prompting reliably generates toxic and benign statements about minority groups ( §4.3).",11,12
2101,247519233,This indicates that these data are sufficiently toxic or benign.,7,8
2102,247519233,"For 125 randomlyselected prompts (62 toxic and 63 non-toxic), we generate two statements: one with ALICE and one without (top-k).",6,7
2103,247519233,"For 125 randomlyselected prompts (62 toxic and 63 non-toxic), we generate two statements: one with ALICE and one without (top-k).",11,12
2104,247519233,"We find that for top-k sampled sentences, the prompt label indeed matches the desired label (95.2% of non-toxic examples and 67.7% of toxic examples).",24,25
2105,247519233,"We find that for top-k sampled sentences, the prompt label indeed matches the desired label (95.2% of non-toxic examples and 67.7% of toxic examples).",30,31
2106,247519233,"For ALICE, 40.3% of toxic examples match the prompt label and 92.1% of non-toxic examples match.",6,7
2107,247519233,"For ALICE, 40.3% of toxic examples match the prompt label and 92.1% of non-toxic examples match.",18,19
2108,247519233,"human-annotated toxicity score for ALICE-decoded sentences with a toxic prompt is 2.97, compared to 3.75 for topk.",12,13
2109,247519233,"Improving Toxicity Classifiers To further showcase the usefulness of TOXIGEN, we investigate how it can enhance classifiers' abilities to detect human-written and machinegenerated implicit toxic language.",28,29
2110,247519233,"Conclusions In this work, we used a large language model to create and release TOXIGEN, a large-scale, balanced, and implicit toxic language dataset.",26,27
2111,247519233,The generated samples are balanced in terms of number of benign and toxic samples for each group.,12,13
2112,247519233,"Societal and Ethical Considerations Risks in dataset release While the purpose of our work is to curate diverse and effective hate speech detection resources, our methods encourage a large language model to make its generation more toxic.",37,38
2113,247519233,Please also note that there is still a lot that this dataset is not capturing about toxic language.,16,17
2114,247519233,"2020) B Human Validation Details B.1 Selecting MTurk Workers For human validation, we select 156 MTurk workers with prior experience annotating toxic language (Sap et al.,",23,24
2115,247519233,"prompt_label is the binary value indicating whether or not the prompt is toxic (1 is toxic, 0 is benign), and therefore the generation should be toxic as well.",12,13
2116,247519233,"prompt_label is the binary value indicating whether or not the prompt is toxic (1 is toxic, 0 is benign), and therefore the generation should be toxic as well.",16,17
2117,247519233,"prompt_label is the binary value indicating whether or not the prompt is toxic (1 is toxic, 0 is benign), and therefore the generation should be toxic as well.",29,30
2118,237700337,"The user input is first passed through the input processor module, which detects whether it contains toxic content and whether it is too semantically similar to the current plot goal; after the processing, the user input is concatenated to the existing context and truncated to ensure that the length is within the context length of the story generation model; the story generation model generates a series of candidate stories that are then sent to the candidate ranker for ranking; the ranker contains a filter that removes inappropriate stories based on multiple rules, and the remaining candidate stories are ranked based on their overlapping with the context and how smoothly they connect to the plot goal, with the highest ranked being output to the player as the final result.",17,18
2119,235790764,"Visit Mark Roth's laboratory Roth is investigating the use of small amounts of hydrogen sulfide, a gas that is toxic in larger quantities, to lower metabolism.",21,22
2120,248780427,"For a C2C marketplace like Mercari, we argue that a uniform language model for toxic speech may not fulfill our scenario in which violations may appear in a variety of forms.",15,16
2121,7987945,"However, consider detecting a second event for ""blick"" soon after its release in which the toy is discovered to have toxic properties.",23,24
2122,7987945,"Since the toy's name was already present in the blogs, the novelty of the name is not enough to detect the point at which the toxic chemical was revealed.",27,28
2123,7987945,"To illustrate the basics of cooccurrence based semantic space models, we can further explore the example of ""blick"", the new yet toxic toy.",25,26
2124,7987945,"Examining later blog posts written when this same toy is discovered to have toxic elements, several posts might now have the line: ""the toxic elements in blick make the toy dangerous.""",13,14
2125,7987945,"Examining later blog posts written when this same toy is discovered to have toxic elements, several posts might now have the line: ""the toxic elements in blick make the toy dangerous.""",26,27
2126,7987945,"The semantics of the toy should now focus primarily on the co-occurrence of words such as toxic and dangerous, and should no longer be associated with positive words such as holiday and perfect.",18,19
2127,7987945,"Figure 1 illustrates a simplified two-dimensional semantic space and the changes to semantics that would occur as ""blick"" begins to co-occur with toxic-related words.",28,29
2128,7987945,Basing the number of dimensions on the number of unique words Blick new toy toxic lawsuit Blick' buy Fig.,14,15
2129,7987945,"Considering the example of a new toy that is first introduced and later found to be toxic, figure 3 shows how one could produce two semantic vectors for the toy ""blick"" using a semantic slice.",16,17
2130,7987945,"The first semantic vector is a summation of temporal semantic vectors that describe the introduction of the new toy, and the second semantic vector is a summation of temporal semantic vectors that describe the toxic nature of the toy.",35,36
2131,7987945,"As input, it takes an annotated collection of t i m e The ""blick"" toy is introduced The ""blick"" toy is discovered to be toxic Fig.",29,30
2132,220045465,"1 https://github.com/ShannonAI/CorefQA Original Passage In addition , many people were poisoned when toxic gas was released.",12,13
2133,220045465,Our formulation Q1: Who were poisoned when toxic gas was released?,8,9
2134,220045465,A5: [toxic gas] 2018a).,3,4
2135,235417195,"A chatbot's conduct is considered to be ""safe"" if it does not generate offensive, hostile or toxic responses in political conversation, while it is considered ""neutral"" if it displays an unbiased stance (i.e., does not favor nor oppose political issues.)",20,21
2136,235417195,"Related Work The safety of chatbots has been studied with regard to their toxic or hostile behavior (Dinan et al.,",13,14
2137,202538032,Online discussions often derail into toxic exchanges between participants.,5,6
2138,202538032,"To provide more timely notice to human moderators, a system needs to preemptively detect that a conversation is heading towards derailment before it actually turns toxic.",26,27
2139,202538032,"Furthermore, by virtue of its online processing of the conversation, our system can provide substantial prior notice of upcoming derailment, triggering on average 3 comments (or 3 hours) before an overtly toxic comment is posted.",36,37
2140,202538032,"6  Though the lack of post-hoc annotation limits the degree to which we can impose controls on the data (e.g., some conversations may contain toxic comments not flagged by the moderators) we do reproduce as many of the Wikipedia data's controls as we can.",29,30
2141,236459875,"There is content such as hate speech, offensive, toxic or aggressive documents, which are perceived differently by their consumers.",10,11
2142,236459875,"Related work It is observable a steady increase in the number of offensive (Levmore and Nussbaum, 2010) , hate (Breckheimer, 2001; Brown, 2018) , aggressive, toxic, cyberbullying (Chen et al.,",34,35
2143,236459875,"It especially refers to diverse emotions evoked by textual content, hate speech, detection of cyberbullying or offensive, toxic, abusive, harmful, or socially unaccepted content.",20,21
2144,2941796,"toxic?,",0,1
2145,2941796,"toxic?,",0,1
2146,222272299,"2019) filtered undesired and toxic responses from the Reddit conversations used in training, which explains the positivity of the DialoGPT responses.",5,6
2147,236460004,The problem posed by the task was to determine which words contribute mostly to recognising a document as toxic.,18,19
2148,236460004,The work consists of two xAI approaches that automatically provide the explanation for models trained for binary classification of toxic documents: an LSTM model with attention as a model-specific approach and the Shapley values for interpreting BERT predictions as a model-agnostic method.,19,20
2149,236460004,The importance of keeping the online community safe for users has caused many researchers to focus their work on detecting toxic contents in order to assist moderators in their difficult and mentally exhausting work.,20,21
2150,236460004,"A lot of progress has been done in the task of toxic comment classification, but unfortunately, complex models still lack clear explanation and cannot gain much moderators' trust.",11,12
2151,236460004,The ability to extract and highlight key text fragments that cause the toxic character of a comment could be of great use in post moderation.,12,13
2152,236460004,"Section 3 presents three approaches to toxic spans detection, each with method-specific details in the corresponding subsection.",6,7
2153,236460004,Toxic Spans Detection This section describes the following approaches: analysing the attention of an LSTM model with orthogonalization of hidden states 3.1; using SHAP to provide explanation of BERT predictions for toxic comment classification 3.2; training BERT for classification of toxic tokens 3.3.,33,34
2154,236460004,Toxic Spans Detection This section describes the following approaches: analysing the attention of an LSTM model with orthogonalization of hidden states 3.1; using SHAP to provide explanation of BERT predictions for toxic comment classification 3.2; training BERT for classification of toxic tokens 3.3.,43,44
2155,236460004,"During the preprocessing stage, the inputs with toxicity higher than 0.5 were considered as toxic examples.",15,16
2156,236460004,The key parameter to obtain tokenlevel prediction was setting a threshold for toxic token selection based on the attention value.,12,13
2157,236460004,"Two approaches were investigated: • value-based: all tokens with an attention score higher than the certain threshold were selected as toxic • cumulative: sorting the tokens by the highest attention score, each token was added to the toxic set as long as the cumulative value of attention was not covered (e.g. 70% of the whole model's attention) The span-level F1 score on the validation set with different threshold values for both approaches is presented in Figure 1 .",24,25
2158,236460004,"Two approaches were investigated: • value-based: all tokens with an attention score higher than the certain threshold were selected as toxic • cumulative: sorting the tokens by the highest attention score, each token was added to the toxic set as long as the cumulative value of attention was not covered (e.g. 70% of the whole model's attention) The span-level F1 score on the validation set with different threshold values for both approaches is presented in Figure 1 .",43,44
2159,236460004,"The authors decided to apply SHAP framework to explain the BERT model, trained for toxic comment classification.",15,16
2160,236460004,"2018) , which is fine-tuned to predict toxic spans.",10,11
2161,236460004,"Filling empty spaces in between spans While analysing the labels provided for the training set, the authors noticed that at times, whole text fragments were labelled as toxic, including spaces in between words.",29,30
2162,236460004,"If the output contained toxic spans separated by one or two characters, the character was also considered toxic.",4,5
2163,236460004,"If the output contained toxic spans separated by one or two characters, the character was also considered toxic.",18,19
2164,236460004,The aggregation of predictions was performed with hard voting -a span was considered toxic only when 3 out of 5 models returned a toxic prediction.,13,14
2165,236460004,The aggregation of predictions was performed with hard voting -a span was considered toxic only when 3 out of 5 models returned a toxic prediction.,23,24
2166,236460004,"Surprisingly, a model-agnostic approach turned out to perform much better than the attentionbased solution, even though the explained models achieved very similar results on the toxic comment detection task.",29,30
2167,236460004,"This could be because the model might pay a significant portion of attention to a very toxic word, which is enough to recognise the comment as toxic.",16,17
2168,236460004,"This could be because the model might pay a significant portion of attention to a very toxic word, which is enough to recognise the comment as toxic.",27,28
2169,236460004,"Furthermore, the need for threshold tuning somehow forced explainable approaches to mark a certain number of tokens as toxic, which could be reflected in a slightly higher number of false positives.",19,20
2170,236460004,"But in terms of explanation, it might be enough for the user to obtain the few most toxic words per comment, rather than marking all of them, no matter how low the toxicity score is.",18,19
2171,236460004,Supervised toxic token classification and xAI methods were examined to compare the results and assess whether explaining high-performing models can lead to a similar quality of prediction as models dedicated to a more detailed task.,1,2
2172,236460004,"A system for toxic spans detection was prepared, achieving a 0.6859 span-level F1 score and placing 13th out of 91 in the overall ranking.",3,4
2173,250089342,"We use toxicity detection as a testbed in this work, and focus on generating counterfactuals to probe for false positives -that is, non-toxic text which is misclassified as toxic due to identity references.",26,27
2174,250089342,"We use toxicity detection as a testbed in this work, and focus on generating counterfactuals to probe for false positives -that is, non-toxic text which is misclassified as toxic due to identity references.",32,33
2175,250089342,"2018) that toxicity and hate speech classifiers often pick up on correlations (that are not causations) between references to certain identities and toxic speech: that is, these models incorrectly learn that sensitive attributes such as certain sexual orientations, gender identities, races, religions, etc.",25,26
2176,250089342,"Recent work has gone further and explored the effect of indirect toxic examples on classifiers (Sap et al.,",11,12
2177,250089342,"For the experiments reported here, we exclusively used the finetuned dialog model: both for safety reasons (LLM-D's finetuning includes a focus on reducing toxic text generation (Thoppilan et al.,",29,30
2178,250089342,"2022, §6) , and we also built safeguards into our pipeline to reduce the chances of our method producing problematic or toxic counterfactuals.",24,25
2179,250089342,This provides a second line of defence against any toxic text that might slip through.,9,10
2180,250089342,"We further require that texts have a score of at least 0.8 for the relevant attribute, and a toxicity score of at most 0.1: i.e. least 80% of the CC-I annotators agreed that the text referenced the specified attribute/identity, and at most 10% of them viewed the comment as toxic.",58,59
2181,250089342,"We chose to focus on only non-toxic examples (as rated by the CC-I annotators) in our experiments, because toxic examples can introduce an unwanted confounding factor: there are many examples in the dataset that are only toxic because they contain a slur, and removing or substituting the slur often renders the resulting text non-toxic.",8,9
2182,250089342,"We chose to focus on only non-toxic examples (as rated by the CC-I annotators) in our experiments, because toxic examples can introduce an unwanted confounding factor: there are many examples in the dataset that are only toxic because they contain a slur, and removing or substituting the slur often renders the resulting text non-toxic.",25,26
2183,250089342,"We chose to focus on only non-toxic examples (as rated by the CC-I annotators) in our experiments, because toxic examples can introduce an unwanted confounding factor: there are many examples in the dataset that are only toxic because they contain a slur, and removing or substituting the slur often renders the resulting text non-toxic.",44,45
2184,250089342,"We chose to focus on only non-toxic examples (as rated by the CC-I annotators) in our experiments, because toxic examples can introduce an unwanted confounding factor: there are many examples in the dataset that are only toxic because they contain a slur, and removing or substituting the slur often renders the resulting text non-toxic.",64,65
2185,250089342,"Note that this a choice we make in the context of this particular application, but the general methodology described here could also be used to investigate toxic original examples if deemed appropriate.",27,28
2186,250089342,"5 Perspective API defines toxicity as ""a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion""; the toxicity score is the predicted probability of a reader perceiving the input as toxic.",40,41
2187,250089342,"While the sample sizes are too small to draw concrete conclusions, the small average change in toxicity for religion-referencing substitution counter- factuals compared both to other topics and to LLM-D-generated counterfactuals reinforces the conjecture that the toxicity classifier may view all references to religion as similarly toxic.",53,54
2188,250089342,"Language models are known to produce toxic text (Wallace et al.,",6,7
2189,250089342,"Most generally, for this investigation we focused on one way this framework can be useful, and made several narrowing choices; however, our framework can be useful in other contexts and applications such as investigating false negatives (by considering original examples that are toxic), probing other types of classifiers than toxicity models, or generating other types of counterfactuals than simply removing the sensitive attribute (e.g. rewording text to explore model robustness).",47,48
2190,250089342,"The term list was generated by fitting a unigram naive bayes classifier to the non-toxic subset of Civil Comments data (toxicity < 0.1), separating texts labeled with the given identity group (attribute score > 0.5) from a random sample of the rest.",16,17
2191,250089342,Non-toxic substitutions that change the meaning of the sentence should be assigned a score of 2 (see below). •,2,3
2192,250089342,examples where Perspective API rated the counterfactual as less likely to be toxic than the original.,12,13
2193,250391099,"For Civil Comments, a dataset for detecting toxic news comments, we select the News subreddit corpus from ConvoKit (Chang et al.,",8,9
2194,227231712,Sub-task A is to determine if the tweet is toxic or not.,11,12
2195,227231712,"Recently, many different offensive language datasets were published that allowed several studies to work on online toxic content recognition problem.",17,18
2196,227231712,2018) that worked on Wikipedia toxic comments dataset.,6,7
2197,243865567,"As a concrete running example in this paper, we will consider the task of toxicity detection: using a model to predict if a comment is toxic or not (Dixon et al.,",27,28
2198,243865567,"To return to our example of toxicity detection, a model may learn that certain identity tokens are correlated with toxicity, but that could decrease accuracy for non-toxic comments with those terms (Dixon et al.,",30,31
2199,243865567,"That is, for some demographic groups we may observe more directed comments and for others we may observe more directed comments: Toxic: Seeking transgender rights is extreme (Directed) Non-Toxic: Transgender rights activists are labeled extremists (Descriptive) In the toxicity classification example shown above, while the former is labeled as toxic by human annotators as it is directed towards a demographic group, the latter is only describing the toxicity and is considered as non-toxic.",60,61
2200,243865567,"That is, for some demographic groups we may observe more directed comments and for others we may observe more directed comments: Toxic: Seeking transgender rights is extreme (Directed) Non-Toxic: Transgender rights activists are labeled extremists (Descriptive) In the toxicity classification example shown above, while the former is labeled as toxic by human annotators as it is directed towards a demographic group, the latter is only describing the toxicity and is considered as non-toxic.",86,87
2201,243865567,"Nonetheless, both these sentences are classified as toxic by the Jigsaw Perspective API (Dixon et al.,",8,9
2202,243865567,"We narrow down our focus to the comments that have the referenced identity in the comment, as well as the binary label: toxic or non-toxic.",24,25
2203,243865567,"We narrow down our focus to the comments that have the referenced identity in the comment, as well as the binary label: toxic or non-toxic.",28,29
2204,243865567,"Based on the qualitative study of toxic comments (Waseem and Hovy, 2016) , we can broadly categorize the intent of comments as either directed or descriptive.",6,7
2205,243865567,2018) released a set of madlibs templates to generate toxic and non-toxic comments based on hierarchies of intersectional identities.,10,11
2206,243865567,2018) released a set of madlibs templates to generate toxic and non-toxic comments based on hierarchies of intersectional identities.,14,15
2207,243865567,"Broader Impact Statement As we are dealing with the toxicity detection task, the concern of dual use for generating more toxic content on social media has to be considered.",21,22
2208,243865567,"That being said, the identification of directed toxic comments towards minority communities can greatly improve the experience of members, often targeted due to their membership in protected classes in these online social communities.",8,9
2209,243865567,"More so, when these same members describe the toxicity they experience on those social online forums, the possibility of them being flagged as toxic, can be harmful.",25,26
2210,243865567,"We show that, without considering secondary variables, such errors, particularly in groups which are the target of toxic comments, can further exacerbate this divide.",20,21
2211,248779962,"Therefore, our dataset inherits the contents of SAMSum and does not contain toxic information. •",13,14
2212,218665402,"Hate speech, aggressive content, cyberbullying, and toxic comments are all different forms of offensive content (Schmidt & Wiegand, 2017) .Figure 1 shows an example of an offensive tweet.",9,10
2213,235097426,"If the rug is treated in house how long before any toxic fumes or skin contact would be a hazard ."",",11,12
2214,131773893,"2017) ; • 2 related hate speech datasets from Twitter (Waseem and Hovy, 2016) , (Waseem, 2016 ); • Insulting internet comments (Impermium, 2012) ; • Attacking, aggressive, toxic and neutral comments from Wikipedia Talk Pages (Wulczyn et al.,",40,41
2215,236460193,"Our system is an ensemble of BERT-based models for binary word classification, trained on a dataset extended by toxic comments modified and generated by two language models.",21,22
2216,236460193,"For the toxic word classification, the prediction threshold value was optimized separately for every comment, in order to maximize the expected F1 value.",2,3
2217,236460193,"The problem is magnified in cyberspace, where anonymity and asynchronous communication contribute to toxic disinhibition.",14,15
2218,236460193,"However, SemEval-2021 Task 5: Toxic Spans Detection proposes detecting fragments of text that make it toxic, with the aim of supporting manual moderation of oftentimes lengthy comments.",17,18
2219,236460193,"In this paper we present the model we used for toxic span detection, the method we used to find optimal prediction threshold values, and two methods for producing new training examples with toxic spans annotation: • Resampling the data: new examples are generated by substituting non-toxic words with predictions from a language model. •",10,11
2220,236460193,"In this paper we present the model we used for toxic span detection, the method we used to find optimal prediction threshold values, and two methods for producing new training examples with toxic spans annotation: • Resampling the data: new examples are generated by substituting non-toxic words with predictions from a language model. •",34,35
2221,236460193,"In this paper we present the model we used for toxic span detection, the method we used to find optimal prediction threshold values, and two methods for producing new training examples with toxic spans annotation: • Resampling the data: new examples are generated by substituting non-toxic words with predictions from a language model. •",51,52
2222,236460193,"Data generation: we trained a simple language model on existing examples, in order to generate new examples containing marked toxic spans.",21,22
2223,236460193,"However, their purpose is toxicity detection of the whole text, so they do not contain information about the exact spans that make a text toxic.",26,27
2224,236460193,"It consists of 10,629 English texts and their relative lists of toxic spans' indices (7,939 train, 690 trial, and 2,000 test texts with their respective spans).",11,12
2225,236460193,"It results mainly from somewhat inconsistent annotation which is visible in several dimensions, see Appendix B. 4 System Overview Resampled Data The texts from train dataset were tokenized and the tokens that were not labeled as toxic spans were replaced with tokens suggested by a RoBERTa (Liu et al.,",37,38
2226,236460193,"We set the limit of substituted words on 1/3 of all tokens outside the toxic span, but no more than 10.",14,15
2227,236460193,"was transformed into text ""<s> One less <toxic> idiot </toxic> to worry about.",11,12
2228,236460193,"</s>"" where <toxic>, </toxic> are special tokens which indicate the beginning and ending of a toxic span and <s>, </s> are special tokens indicating that this particular example is annotated.",6,7
2229,236460193,"</s>"" where <toxic>, </toxic> are special tokens which indicate the beginning and ending of a toxic span and <s>, </s> are special tokens indicating that this particular example is annotated.",23,24
2230,236460193,"Token Classification To detect toxic spans within a text we use the Hugging Face (Wolf et al.,",4,5
2231,236460193,Characters with probabilities meeting this threshold are identified as being part of a toxic span.,13,14
2232,236460193,"Then the F1 measure for such prediction is F 1 (s, y) = 2 n i=1 s i y i n i=1 s i + n i=1 y i If elements of s and y are indexed in order of decreasing probabilities of beeing toxic, and S i:j = j k=i y k , then the expected value of F 1 for the prediction with k most probable letters classified as toxic, is f (k) = k k 1 =0 n−k k 2 =0 2P (S 1:k = k 1 )P (S k+1:n = k 2 )k 1 k + k 1 + k 2 We approximated the distribution of the golden spans y by assuming that probabilities of characters in the golden toxic span are independent and equal to probabilities predicted by the model.",47,48
2233,236460193,"Then the F1 measure for such prediction is F 1 (s, y) = 2 n i=1 s i y i n i=1 s i + n i=1 y i If elements of s and y are indexed in order of decreasing probabilities of beeing toxic, and S i:j = j k=i y k , then the expected value of F 1 for the prediction with k most probable letters classified as toxic, is f (k) = k k 1 =0 n−k k 2 =0 2P (S 1:k = k 1 )P (S k+1:n = k 2 )k 1 k + k 1 + k 2 We approximated the distribution of the golden spans y by assuming that probabilities of characters in the golden toxic span are independent and equal to probabilities predicted by the model.",79,80
2234,236460193,"Then the F1 measure for such prediction is F 1 (s, y) = 2 n i=1 s i y i n i=1 s i + n i=1 y i If elements of s and y are indexed in order of decreasing probabilities of beeing toxic, and S i:j = j k=i y k , then the expected value of F 1 for the prediction with k most probable letters classified as toxic, is f (k) = k k 1 =0 n−k k 2 =0 2P (S 1:k = k 1 )P (S k+1:n = k 2 )k 1 k + k 1 + k 2 We approximated the distribution of the golden spans y by assuming that probabilities of characters in the golden toxic span are independent and equal to probabilities predicted by the model.",144,145
2235,236460193,"Among the available range, we used: • set-theory union (i.e. at least one model declared a character as toxic), • set-theory intersection (i.e. all models declared a character as toxic), • majority voting (i.e. at least half of the models declared a character as toxic), • F1-weighted voting (i.e. the vote was weighted with the model's F1 score and computed for the evaluation set), The output of the ensemble still needed to be postprocessed.",23,24
2236,236460193,"Among the available range, we used: • set-theory union (i.e. at least one model declared a character as toxic), • set-theory intersection (i.e. all models declared a character as toxic), • majority voting (i.e. at least half of the models declared a character as toxic), • F1-weighted voting (i.e. the vote was weighted with the model's F1 score and computed for the evaluation set), The output of the ensemble still needed to be postprocessed.",39,40
2237,236460193,"Among the available range, we used: • set-theory union (i.e. at least one model declared a character as toxic), • set-theory intersection (i.e. all models declared a character as toxic), • majority voting (i.e. at least half of the models declared a character as toxic), • F1-weighted voting (i.e. the vote was weighted with the model's F1 score and computed for the evaluation set), The output of the ensemble still needed to be postprocessed.",57,58
2238,236460193,"Whitespaces and punctuation characters that were the only characters separating two parts of toxic spans were included in the toxic span, while all the other whitespaces and punctuation characters located at the ends of spans were removed.",13,14
2239,236460193,"Whitespaces and punctuation characters that were the only characters separating two parts of toxic spans were included in the toxic span, while all the other whitespaces and punctuation characters located at the ends of spans were removed.",19,20
2240,236460193,"The implemented method of data augmentations, based on resampling non-toxic words proved to be effective by increasing the F1 score of token classifier.",12,13
2241,236460193," A Our models results within SemEval-2021 Task 5 scores Model B Original Dataset Inconsistency Firstly, there are words annotated as toxic in some comments, while in the other ones they are left out, despite the similar context of the utterance.",22,23
2242,236460193,"The words ""stupidity"" or ""crooked"" can serve as an examples, sometimes being omitted, sometimes being treated as full toxic spans and finally, sometimes being treated as parts of toxic spans, together with their modifiers (see Table 5 ).",24,25
2243,236460193,"The words ""stupidity"" or ""crooked"" can serve as an examples, sometimes being omitted, sometimes being treated as full toxic spans and finally, sometimes being treated as parts of toxic spans, together with their modifiers (see Table 5 ).",35,36
2244,236460193,"The majority of spans consist of one to three words, but there are also cases in which spans are longer, containing not just a toxic word, but also a longer phrase including the toxic word (see Table 6 ).",26,27
2245,236460193,"The majority of spans consist of one to three words, but there are also cases in which spans are longer, containing not just a toxic word, but also a longer phrase including the toxic word (see Table 6 ).",36,37
2246,236460193,Other issues we found peculiar in the provided annotation include annotating non-toxic words while omitting toxic ones (see Table 7 ) and beginning or ending the annotation in the middle of a word (Table 8 ).,13,14
2247,236460193,Other issues we found peculiar in the provided annotation include annotating non-toxic words while omitting toxic ones (see Table 7 ) and beginning or ending the annotation in the middle of a word (Table 8 ).,17,18
2248,236460193,"Moreover, our language experts marked fragments as toxic more often: whole sentences, paragraphs or even whole comments, as well as more single words (see Table 9 ).",8,9
2249,247158838,"In the GPT2-medium + prompt engineering setting, we prepend each prompt with the guiding sentence ""This is a non-toxic comment:"".",24,25
2250,247158838,"One possible reason is that compared with the differences between toxic and normal sentences, the difference between positive sentiment and negative sentiment is more subtle, so it is more challenging for the GPT2 encoder in our unsupervised model to accurately separate the unlabeled data into two sentiments.",10,11
2251,233033383,We would also like to warn that there would inevitably be potentially toxic or offensive contents in the dataset.,12,13
2252,201703010,"2018) , and we generated other thematic word sets representative of social constructs: government (democrat, republican, senate, government, politics, minister, presidency, vote, parliament, ...), threat (dangerous, scary, toxic, suspicious, threat, frightening ...), communal (community, society, humanity, welfare, ...), criminal (criminal, jail, prison, crime, corrupt, ...), childcare (child, children, parent, baby, nanny, ...), excellent (excellent, fantastic, phenomenal, outstanding, ...) and others.",44,45
2253,201703010,"4 shows an example of indirect correlation (R 2 = 0.51) of our threat word set (threat, dangerous, toxic, suspicious, scary, frightening, horrifying, ...) against U.S. Census Bureau data reported in 2016 on the gender pay gap.",23,24
2254,227231054,SLO risk factor: environmental) • We are at BHP [the company name] HQ protesting against their toxic Olympic Dam uranium mine that fuels war and breaches land rights #uprootthesystem #nonukes #KeepItInTheGround (SLO risk factors: environmental and social) Being able to identify the specific SLO risks discussed in the public discourse allows an organisation to better identify and manage them.,20,21
2255,126147495,@user @user @user @user @user @user @user what a stupid incompetent devious and toxic pm !,13,14
2256,235097342,"Although ad hominems are also a form of toxic language, we train a new attribute classifier specifically on the annotated ADHOMINTWEETS dataset for a more competitive PPLM baseline.",8,9
2257,165163607,"the venom of the inland taipan, drop by drop, is the most toxic among all snakes A: No. [",14,15
2258,165163607,Y) A catalytic converter is an exhaust emission control device that converts toxic gases and pollutants in exhaust gas from an internal combustion engine into less-toxic pollutants by catalyzing a redox reaction (an oxidation and a reduction reaction).,13,14
2259,165163607,Y) A catalytic converter is an exhaust emission control device that converts toxic gases and pollutants in exhaust gas from an internal combustion engine into less-toxic pollutants by catalyzing a redox reaction (an oxidation and a reduction reaction).,28,29
2260,235294009,"It should also be noted that our CM-BART model is fine-tuned on the poetry corpus which is devoid of harmful and toxic text especially targeted at marginalized communities Advances in generative AI inherently come with concerns about models' ability to deceive, persuade, and misinform.",25,26
2261,226221869,"Third, we ablate our technique and show with automated and human participant experiments that the fine-tuning technique works with classifiers other than the normative classifier-specifically models trained to classify negative-sentiment and toxic language.",38,39
2262,226221869,2019 ) also apply attribute classifiers to fine-tune language models; the technique is demonstrated via generating text with a target sentiment and also decreasing the frequency of toxic language.,30,31
2263,226221869,"There are two limitations: (a) PPLM trains a model to operate on a fixed set of prefix input, and (b) the classification must be done on a word-by-word basis and thus cannot easily be applied to problems where the normative valence of individual words relies on a single or multiple sentence context (e.g. quoting and admonishing toxic speech).",68,69
2264,226221869,"Non-normativity is a superset of toxic language in the sense that toxic language is non-normative, but not all non-normative descriptions are toxic.",7,8
2265,226221869,"Non-normativity is a superset of toxic language in the sense that toxic language is non-normative, but not all non-normative descriptions are toxic.",13,14
2266,226221869,"Non-normativity is a superset of toxic language in the sense that toxic language is non-normative, but not all non-normative descriptions are toxic.",28,29
2267,226221869,"We also conduct experiments using toxic language classifiers -fine-tuned on sentiment corpora like the dataset from the Toxic Comment Classification Challenge 2 as an alternative to the normative text classifier fine-tuned on G&G. Normative Fine-Tuning The GPT-2 model is trained by minimizing its cross-entropy loss given by (Radford et al.,",5,6
2268,226221869,"70 crowd workers on Mechanical Turk labeled those generated sentences as normative or non-normative (or positive or negative sentiment, or toxic or non-toxic).",24,25
2269,226221869,"70 crowd workers on Mechanical Turk labeled those generated sentences as normative or non-normative (or positive or negative sentiment, or toxic or non-toxic).",28,29
2270,226221869,"To make the improvement from applying our technique more visible, we fine-tuned the 117M GPT-2 with only toxic or negative sentences when producing the baseline models, and obtained GPT-senti-ext and GPT-toxic-ext, which frequently produce negative or toxic generated continuations.",21,22
2271,226221869,"To make the improvement from applying our technique more visible, we fine-tuned the 117M GPT-2 with only toxic or negative sentences when producing the baseline models, and obtained GPT-senti-ext and GPT-toxic-ext, which frequently produce negative or toxic generated continuations.",41,42
2272,226221869,"To make the improvement from applying our technique more visible, we fine-tuned the 117M GPT-2 with only toxic or negative sentences when producing the baseline models, and obtained GPT-senti-ext and GPT-toxic-ext, which frequently produce negative or toxic generated continuations.",50,51
2273,226221869,"We also finetuned the 117M GPT-2 with balanced datasets (half negative texts and half toxic texts, respectively), and refer to these two models as GPT-senti-bal and GPT-toxic-bal.",16,17
2274,226221869,"We also finetuned the 117M GPT-2 with balanced datasets (half negative texts and half toxic texts, respectively), and refer to these two models as GPT-senti-bal and GPT-toxic-bal.",37,38
2275,226221869,Table 4 shows the percentage of textual continuations that are either toxic or contain negative sentiment.,11,12
2276,226221869,"Fine-tuning GPT-toxic-ext with the toxic classifier can reduce toxic language from 57% to 30%, a 46% reduction.",5,6
2277,226221869,"Fine-tuning GPT-toxic-ext with the toxic classifier can reduce toxic language from 57% to 30%, a 46% reduction.",10,11
2278,226221869,"Fine-tuning GPT-toxic-ext with the toxic classifier can reduce toxic language from 57% to 30%, a 46% reduction.",14,15
2279,226221869,"Fine-tuning GPT-toxic-bal with the toxic classifier can reduce toxic language from 33% to 28%, a 13% reduction.",5,6
2280,226221869,"Fine-tuning GPT-toxic-bal with the toxic classifier can reduce toxic language from 33% to 28%, a 13% reduction.",10,11
2281,226221869,"Fine-tuning GPT-toxic-bal with the toxic classifier can reduce toxic language from 33% to 28%, a 13% reduction.",14,15
2282,226221869,"2019) , which also uses a toxic word classifier to fine-tune a language model.",7,8
2283,226221869,"For toxic language reduction, their model is trained to operate on a pre-given set of prompts such as ""black"" or ""asian"".",1,2
2284,226221869,"For these prompts, given during training, they can reduce GPT-2's toxic language frequency from ∼10% to ∼6%.",13,14
2285,226221869,"When we prompt our GPT-toxic-bal and GPT-toxicbal-norm with ""asian"" and ""black"", we see reductions from 52% to 32% and from 70% to 50%, respectively.",6,7
2286,226221869,We see higher occurrences of toxicity because GPT-2 is fine-tuned on the equal numbers of toxic and non-toxic sentences from the Toxic dataset (whereas PPLM compares against a nonfine-tuned version of GPT-2) and because GPT-2 has a pre-existing unjust bias toward these words.,17,18
2287,226221869,We see higher occurrences of toxicity because GPT-2 is fine-tuned on the equal numbers of toxic and non-toxic sentences from the Toxic dataset (whereas PPLM compares against a nonfine-tuned version of GPT-2) and because GPT-2 has a pre-existing unjust bias toward these words.,21,22
2288,226221869,"GPT-3 has been demonstrated to be capable of non-normative, toxic, and prejudicially biased generation.",12,13
2289,226221869,"Because general sources of normative behavior are currently hard to come by, we attempt to show generalization of our technique with experiments using sentiment and toxic language.",26,27
2290,226221869,"By replicating our results with sentiment and toxic classifier, we show that our technique is not specific to any one classifier.",7,8
2291,229924289,GPT-2: A federal judge dismissed a lawsuit Wednesday from the family of an elderly man who died after inhaling a toxic gas during an air show at the Lincoln Memorial .,21,22
2292,248780404,It also performs the best in the toxic content detection task under human-made attacks. *,7,8
2293,248780404,"These adversarial examples occur frequently in the real-world scenario and can be made either naturally (e.g., typos) or maliciously (e.g., to avoid auto detection of toxic content) 1 .",32,33
2294,248780404,"Across 5 standard NLU tasks and one toxic content detection task, we show the pretrained model achieves new SOTAs under various adversarial attackers.",7,8
2295,248780404,3) The pretrained model outperforms strong baselines across 5 NLU tasks and 1 toxic content detection task under various adversarial attackers. (,14,15
2296,248780404,"We manually annotate 50k user inputs and identify 2k toxic contents (positive), out of which 90% are in adversarial forms.",9,10
2297,248780404,Toxic Content Detection Results We train all models in the toxic content detection task.,10,11
2298,248780404,It also significantly outperforms the others in the toxic content detection task.,8,9
2299,231602967,"The publicly available GPT-2 models have learned from data that might not cover current events (e.g., GPT-2 was trained before the COVID-19 epidemic), represents only English dialects from the inner-circle (Dunn and Adams, 2020) , and contains toxic language (Gehman et al.,",46,47
2300,235097569,"2017) , toxic contents (Schmidt and Wiegand, 2017) , and other ethical issues.",3,4
2301,184482582,"Our results on the test set for English show 0.378 macro F1 on task A and 0.553 macro F1 on task B. For Spanish the results are significantly higher, 0.701 macro F1 on task A and 0.734 macro F1 for task B. Introduction The increasing popularity of social media platforms such as Twitter for both personal and political communication has seen a well-acknowledged rise in the presence of toxic and abusive speech on these platforms (Kshirsagar et al.,",71,72
2302,203447304,The presence of toxic content has become a major problem for many online communities.,3,4
2303,203447304,"Moderators try to limit this problem by implementing more and more refined comment filters, but toxic users are constantly finding new ways to circumvent them.",16,17
2304,203447304,"Our hypothesis is that while modifying toxic content and keywords to fool filters can be easy, hiding sentiment is harder.",6,7
2305,203447304,"While this increased international contact and exchange of ideas has been a net positive, it has also been matched with an increase in the spread of high-risk and toxic content, a category which includes cyberbullying, racism, sexual predation, and other negative behaviors that are not tolerated in society.",31,32
2306,203447304,The two main strategies used by online communities to moderate themselves and stop the spread of toxic comments are automated filtering and human surveillance.,16,17
2307,203447304,"However, given the sheer number of messages sent online every day, human moderation simply cannot keep up, and either leads to a severe slowdown of the conversation (if messages are pre-moderated before posting) or allows toxic messages to be seen and shared thousands of times before they are deleted (if they are post-moderated after being posted and reported).",43,44
2308,203447304,"Keyword detection, on the other hand, is instantaneous, scales up to the number of messages, and prevents toxic messages from being posted at all, but it can only stop messages that use one of a small set of denied words, and are thus fairly easy to circumvent by introducing minor misspellings (i.e. writing ""kl urself"" instead of ""kill yourself"").",21,22
2309,203447304,"These attempts to bypass the toxicity detection system are called subverting the system, and toxic users doing it are referred to as subversive users.",15,16
2310,203447304,"In this paper, we consider an alternative strategy for toxic message filtering.",10,11
2311,203447304,"Consequently, we will study the correlation between sentiment and toxicity and its usefulness for toxic message detection both in subversive and non-subversive contexts.",15,16
2312,203447304,"They all gravitate around negative messages such as insults, bullying, vulgarity and hate speech, therefore these types of toxic behavior are the ones we focus on, as opposed to other types such as fraud or grooming that would use more positive messages.",21,22
2313,203447304,It thus appears from the related literature that authors have tried a variety of alternative features to automatically detect toxic messages without relying strictly on keyword detection.,19,20
2314,203447304,toxic or not toxic.,0,1
2315,203447304,toxic or not toxic.,3,4
2316,203447304,We use the ratio of toxic marks as a toxicity score.,5,6
2317,203447304,"For example, if a message is marked toxic by 7 out of 10 workers, it will have a 0.7 toxicity score. •",8,9
2318,203447304,"There are approximatively 160,000 comments, which were manually annotated with six binary labels: toxic, severe toxic, obscene, threat, insult, and identity hate.",15,16
2319,203447304,"There are approximatively 160,000 comments, which were manually annotated with six binary labels: toxic, severe toxic, obscene, threat, insult, and identity hate.",18,19
2320,203447304,Subversive Toxicity Detection Our second experiment consists in studying the benefits of taking sentiments into account when trying to determine whether a comment is toxic or not.,24,25
2321,203447304,"The output of our network is a binary ""toxic or non-toxic"" judgment for the message.",9,10
2322,203447304,"The output of our network is a binary ""toxic or non-toxic"" judgment for the message.",13,14
2323,203447304,"In the Kaggle dataset, this corresponds to whether the ""toxic"" label is active or not.",11,12
2324,203447304,"And in the Wikipedia dataset, it is any message marked as toxic by 5 workers or more.",12,13
2325,203447304,"We chose this binary approach to allow the network to learn to recognize toxicity, as opposed to types of toxic messages on Kaggle, keyword severity on Reddit, or a particular worker's opinions on Wikipedia.",20,21
2326,203447304,"However, this simplification created a balance problem: the Reddit dataset is composed of 12% toxic messages and 88% non-toxic messages, the Wikipedia dataset is composed of 18% toxic messages, and the Kaggle dataset of 10% toxic messages.",17,18
2327,203447304,"However, this simplification created a balance problem: the Reddit dataset is composed of 12% toxic messages and 88% non-toxic messages, the Wikipedia dataset is composed of 18% toxic messages, and the Kaggle dataset of 10% toxic messages.",24,25
2328,203447304,"However, this simplification created a balance problem: the Reddit dataset is composed of 12% toxic messages and 88% non-toxic messages, the Wikipedia dataset is composed of 18% toxic messages, and the Kaggle dataset of 10% toxic messages.",35,36
2329,203447304,"However, this simplification created a balance problem: the Reddit dataset is composed of 12% toxic messages and 88% non-toxic messages, the Wikipedia dataset is composed of 18% toxic messages, and the Kaggle dataset of 10% toxic messages.",45,46
2330,203447304,"To create balanced datasets for training, we kept all toxic messages and undersampled randomly the set of non-toxic messages to be equal to the number of toxic messages.",10,11
2331,203447304,"To create balanced datasets for training, we kept all toxic messages and undersampled randomly the set of non-toxic messages to be equal to the number of toxic messages.",20,21
2332,203447304,"To create balanced datasets for training, we kept all toxic messages and undersampled randomly the set of non-toxic messages to be equal to the number of toxic messages.",29,30
2333,203447304,"Indeed, as mentioned in Sections 1 and 2, it is trivial for a subversive user to mask toxic keywords to bypass toxicity filters.",19,20
2334,203447304,"2017) , we created a substitution list that replaces popular toxic keywords with harmless versions.",11,12
2335,203447304,"Our list contains 191 words, and its use adds noise to 82% of the toxic Kaggle messages, 65% of the Wikipedia messages, and 71% of the Reddit messages.",16,17
2336,203447304,"Most of that loss comes from a much lower recall score, which is unsurprising considering the fact that we are modifying the most common toxic words.",25,26
2337,203447304,"adding subversion, because the toxic word ""sucks"" is changed to the harmless word ""socks"".",5,6
2338,203447304,part being clearly negative -and increases the score sufficiently for the message to be classified as toxic.,16,17
2339,203447304,lacks recognizable toxic features such as insults and curse words and is classified as non-toxic by the sentiment-less neural network.,2,3
2340,203447304,lacks recognizable toxic features such as insults and curse words and is classified as non-toxic by the sentiment-less neural network.,16,17
2341,203447304,"However, the negative sentiment of ""sick"", ""stench"", and ""odious"" (none of which are normally found in abusive word lists) allows the sentiment neural network to recognize the message as toxic.",40,41
2342,203447304,"Finally, we simulated a subversive user who circumvents the toxicity filter by masking toxic keywords in their messages, and found that using sentiment information improved toxicity detection by as much as 3%.",14,15
2343,203447304,"This confirms our fundamental intuition, that while it is possible for a user to mask toxic words with simple substitutions, it is a lot harder for a user to conceal the sentiment of a message.",16,17
2344,203447304,"Likewise, handling entire conversations will allow us to include contextual information to the sentiment of each message, and to detect sudden changes in the sentiment of the conversation that correspond to a disruptive toxic comment.",35,36
2345,237513633,"The original neural dialog model then considers the whole context (c k−1 ; x k ) to generate a response y k , which is considered as not acceptable to a problem P (e.g. toxic response to the safety problem).",36,37
2346,237513633,2020) finds toxic prompts from naturally occurring sentences.,3,4
2347,237513633,"These techniques mostly requires some toxic classifiers, which as shown in Section 4.3, may not work well for a different model distribution.",5,6
2348,243865588,"As a modified version of Wikipedia dataset, the collection procedure ensures no ethical concerns e.g., toxic language and hate speech.",17,18
2349,250391026,"Socially unacceptable language is to be regarded as a generic umbrella term comprehending many different user-generated language phenomena such as toxic language (Karan and Šnajder, 2019; Bhat et al.,",22,23
2350,250391026,"However, in the context of social media interactions, the presence and use of offensive language towards other users should raise concerns because it may escalate the exchange in deeper verbal hostility (e.g., hate speech) and give rise to highly toxic, and unsafe environments (Chowdhury et al.,",44,45
2351,236486214,"1 The increasing polarization of online debates and conversations, together with the amount of associated toxic and abusive behaviors, call for some form of automatic content moderation.",16,17
2352,67856299,"2013) , toxic comments 1 , hate speech (Kwok and Wang, 2013; Djuric et al.,",3,4
2353,85531331,"For example, spammers might send malicious emails or post toxic comments to online discussion forums (Hosseini et al.,",10,11
2354,85531331,Toxic comment (TC) classification: A very realistic use case for adversarial attacks is the toxic comment classification task.,17,18
2355,85531331,One could easily think of a scenario where a person with malicious intent explicitly aims to fool automated methods for detecting toxic comments or insults by obfuscating text with non-standard characters that are still human-readable.,21,22
2356,85531331,"6 It is a multi-label sentence classification task with six classes, i.e., toxic, severe toxic, obscene, threat, insult, identity hate.",16,17
2357,85531331,"6 It is a multi-label sentence classification task with six classes, i.e., toxic, severe toxic, obscene, threat, insult, identity hate.",19,20
2358,85531331,roughly one fourth of the toxic comments receive a lower TL.,5,6
2359,85531331,"In contrast, the Table 5 : Two examples of toxic/non-toxic comments that show the effects of the different shielding methods.",10,11
2360,85531331,"In contrast, the Table 5 : Two examples of toxic/non-toxic comments that show the effects of the different shielding methods.",14,15
2361,85531331,impact on non-toxic comments is small-TL increased in only 3.2% of the cases.,4,5
2362,85531331,"To test whether VIPER benefits from perturbing such hot words, we manually compiled a list of 20 handselected offensive words (see §A.6) which we believe are indicators of toxic comments.",32,33
2363,85531331,"Our work is also important for system builders, such as of toxic comment detection models deployed by, e.g., Facebook and Twitter, who regularly face visual attacks, and who might face even more such attacks once visual character perturbations are easier to insert than via the keyboard.",12,13
2364,248227301,Annotators were also asked to judge the outputs of several state-of-the-art conversational systems which may be in turn toxic and insensitive.,24,25
2365,236976089,"2014) , toxic content detection (Jigsaw, 2017) , and for text-based automated privacy policy understanding (Harkous et al.,",3,4
2366,227254771,"Besides, domain specific dialogue systems, especially those deployed in professional settings generally prefer restricting themselves to a fixed set of domains, and purposely refrain from responding to out-of-domain and random or toxic user queries.",38,39
2367,227254771,"Also, multiple subsystem architectures always have the possibility of cascading errors and profane or toxic queries.",15,16
2368,227254771,"2020) it is always safer to generate fallback responses on encountering queries which might be toxic, biased or profane.",16,17
2369,227254771,"In order to avoid cascading errors from such systems, as well as refrain from answering out-of-domain and toxic queries it is but natural to have a fallback approach to say no.",22,23
2370,239009606,Experimental results on sentiment analysis and toxic detection tasks show that our method achieves better defending performance and much lower computational costs than existing online defense methods.,6,7
2371,239009606,We conduct experiments on sentiment analysis and toxic detection tasks.,7,8
2372,239009606,"2015) reviews datasets on sentiment analysis task, and for toxic detection task, we use Twitter (Founta et al.,",11,12
2373,239009606,"For sentiment analysis task, the target/protect label is ""positive"", and the target/protect label is ""inoffensive"" for toxic detection task.",26,27
2374,239009606,Results in Toxic Detection The results in toxic detection task are displayed in Table 3 .,7,8
2375,239009606,"There is an interesting phenomenon that in the toxic detection task, ONION's defending performance against BadNet-RW and EP becomes worse than that in the sentiment analysis task.",8,9
2376,239009606,"This is because, clean offensive samples in the toxic detection task already contain dirty words, which are rare words whose appearances may also increase the perplexity of the sentence.",9,10
2377,239009606,"Our explanation is, if we allow more words in the input being removed based on their impacts on input text's perplexity to get a reliable classification result, then some sentiment words (in the sentiment analysis task) or offensive words (in the toxic detection task) will be more likely to be removed.",47,48
2378,236459933,Experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance.,5,6
2379,236459933,"Experimental results on sentiment analysis and toxic detection tasks show that our approach achieves much lower DSRs and FTRs, while keeping comparable ASRs.",6,7
2380,236459933,"Experimental Settings In the AFM setting, we conduct experiments on sentiment analysis and toxic detection task.",14,15
2381,236459933,"2015) reviews datasets; and for toxic detection task, we use Twitter (Founta et al.,",7,8
2382,236459933,"We report clean accuracy for sentiment analysis task, and clean macro F1 score for toxic detection task.",15,16
2383,236459933,"The reason that DSRs are lower in toxic detection datasets is there are already some rarely used dirty words in the samples, detecting the real trigger word becomes more difficult in this case.",7,8
2384,236459933,"However, SL attacks behave badly on the FTR metric (over 50% on all sentiment analysis datasets and over 80% on toxic detection datasets).",24,25
2385,236459933,"As for attacking performances, we find SL succeeds to maintain the backdoor effects in all cases, RW fails in the toxic detection task, and SOS behaves badly when using Yelp as the poisoned dataset.",22,23
2386,236459933,"However, in   toxic detection samples, some dirty words contain sub-words which are exactly the trigger words, then fine-tuning the backdoored model on clean samples will cause the backdoor effect be mitigated. (",4,5
2387,236459933,This is consistent to the result that SOS behaves well on toxic detection task in which datasets are in the same domain.,11,12
2388,235446822,"Since we use a finetuned version of a pretrained generative model, we inherit the general risk of generating biased or toxic language, which should be carefully filtered.",21,22
2389,248811036,"A participant emphasized that ""developing [an] ethically correct [system that] takes care of things like safety and even fairness and inclusion is really important, [but] Ethical considerations are often reduced to ""toxic"" speech.",40,41
2390,248811036,"When probed about ethical issues, many interview participants were concerned about generating or reproducing hateful, offensive, or toxic language: ""a big consideration is [that] a lot of the data [could] in some ways be toxic content of different varieties"" [P15] and ""so often what these models will do is generate something that is very very unsafe and in some cases toxic"" [P10].",20,21
2391,248811036,"When probed about ethical issues, many interview participants were concerned about generating or reproducing hateful, offensive, or toxic language: ""a big consideration is [that] a lot of the data [could] in some ways be toxic content of different varieties"" [P15] and ""so often what these models will do is generate something that is very very unsafe and in some cases toxic"" [P10].",43,44
2392,248811036,"When probed about ethical issues, many interview participants were concerned about generating or reproducing hateful, offensive, or toxic language: ""a big consideration is [that] a lot of the data [could] in some ways be toxic content of different varieties"" [P15] and ""so often what these models will do is generate something that is very very unsafe and in some cases toxic"" [P10].",73,74
2393,248811036,"Surprisingly, despite this focus on toxic language, only 30% of survey participants reported systematically evaluating whether generated text contains such language (23% academic, 37% non-academic) [SQ31], meaning even these issues might be under-reported.",6,7
2394,248811036,"Possible answers could be: no experience, a bit of experience, I have worked on it, the focus of my work • The linguistic style or tone of the content generated by your system or model • Toxic, hateful, or offensive language produced by your system or model • Other fairness and inclusion issues aside from toxic, hateful, and offensive language [SQ32] If you assess other fairness and inclusion issues, please briefly explain what do you measure and how.",61,62
2395,232257929,"With the ever-increasing availability of digital information, toxic content is also on the rise.",10,11
2396,232257929,"Since the content is full of toxic words that have not been written according to their dictionary spelling, attendance to individual characters is crucial.",6,7
2397,232257929,"The bagof-words method, on the other hand, further improves upon that by making sure that some frequently used toxic words get labeled accordingly.",22,23
2398,232257929,This can include some toxic language whose detection can be difficult due to the complexities of human languages.,4,5
2399,232257929,"In many cases, the data, which are considered to be toxic, contain words that have not been written in their standard forms.",12,13
2400,232257929,"2020) as well as similar ones to toxic language detection such as propaganda detection (Jurkiewicz et al.,",8,9
2401,232257929,"In addition to using a deep language model for detecting toxic language, we employ a very simple Bag-of-Words model that can achieve a close performance to that of the deep model.",10,11
2402,232257929,"By building a dictionary of toxic words from the training data and by taking into account their frequency and ratio of toxicity, we come up with a simple model that performs as closely as about 2 percent difference in performance to the deep model's result.",5,6
2403,232257929,Pre-processing The training dataset consists of rows of various lengths and an array of character spans indicating their toxic parts.,20,21
2404,232257929,We approach the task of toxic spans detection as a sequence labelling task where each word of the input row is classified into one of the predefined classes.,5,6
2405,232257929,"We define three classes of {B, I, O}, meaning that each word can be the first word (B) of a set of continuous toxic words, in between (I), or not toxic (O).",30,31
2406,232257929,"We define three classes of {B, I, O}, meaning that each word can be the first word (B) of a set of continuous toxic words, in between (I), or not toxic (O).",41,42
2407,232257929,"In this model, by examining the training set, we first build a dictionary of toxic words with their frequency.",16,17
2408,232257929,"Then, we locate the words from the toxic dictionary in each sentence of the test set.",8,9
2409,232257929,"If the word is found and its frequency as well as its toxicity ratio in the training set are higher than certain values, it is labeled as toxic.",28,29
2410,232257929,toxicity ratio = labeled as toxic frequency total frequency The test dataset also contains words that are bleeped.,5,6
2411,232257929,"Since these words can be considered toxic with a high certainty (otherwise they would not be bleeped), we extract them separately from the test set and label them directly as toxic.",6,7
2412,232257929,"Since these words can be considered toxic with a high certainty (otherwise they would not be bleeped), we extract them separately from the test set and label them directly as toxic.",33,34
2413,232257929,"Combining the Two Models In order to get the improved version of the toxic language labeling, the union of the spans detected by the bag-of-words model and that of CharacterBERT is taken.",13,14
2414,232257929,This can be achieved by specifying a high toxicity ratio for a word to be labeled as toxic.,17,18
2415,232257929,"Therefore, the frequency with which a toxic word appears should be somewhat high.",7,8
2416,232257929,We specified batch sizes of 4 for both training and testing and fine-tuned it on the toxic data only for one epoch which produces an F1 score of 65.13.,18,19
2417,232257929,One parameter represents the minimum frequency with which a toxic word appears in the resized training set and the other one is its minimum toxicity ratio in the resized training data.,9,10
2418,232257929,"The reason for this behavior can be attributed to the fact that models with higher thresholds both in terms of frequency and toxicity ratio tend to output more certain results, albeit fewer words than the ones that should be labeled as toxic.",42,43
2419,232257929,"Therefore, many toxic words that are less probable are not extracted and F1 score drops.",3,4
2420,232257929,The 0.8 ratio makes the predictions still a little better but 0.9 does not affect them since the words that are labeled as toxic with this certainty have most probably been found also by Character-BERT.,23,24
2421,232257929,Conclusion We described the system we utilized to detect toxic language.,9,10
2422,232257929,"In our approach, we first fine-tune Char-acterBERT, a character-level pre-trained language model, on the toxic training data.",25,26
2423,250390680,"ŷ = K j=1 y j K (2) Perspective API Perspective 4 is a free API that uses machine learning to identify toxic comments, making it easier to host better conversations online.",24,25
2424,250390680,We use Perspective API to get a toxic score for the text of our test data.,7,8
2425,233864521,"Furthermore, internet communities supported by anonymity and and particular norms can amplify toxic discourse that would not be found in mainstream corpora (Massanari, 2017) often exacerbated by the well-documented 'online disinhibition' phenomenon where users find themselves more likely to engage in anti-social behaviours due to the lack of immediate social feedback (Wachs et al.,",13,14
2426,248780478,The method may also amplify safety and security concerns in critical domains such as toxic comment classification and rumor detection.,14,15
2427,231979145,The organisers of this task argue that highlighting toxic spans in texts helps assisting human moderators (e.g. news portals moderators) and that this can be a first step in semi-automated content moderation.,8,9
2428,231979145,"Let system A i return a set S t A i of character offsets, for parts of the post found to be toxic.",23,24
2429,231979145,From these results it is clear that MUDES is capable of predicting toxic spans efficiently in any environment.,12,13
2430,233209738,"In recent years, the widespread use of social media has led to an increase in the generation of toxic and offensive content on online platforms.",19,20
2431,233209738,"While various state-of-the-art statistical models have been applied to detect toxic posts, there are only a few studies that focus on detecting the words or expressions that make a post offensive.",16,17
2432,233209738,"This motivates the organization of the SemEval-2021 Task 5: Toxic Spans Detection competition, which has provided participants with a dataset containing toxic spans annotation in English posts.",23,24
2433,233209738,"Furthermore, we develop an open-source framework for multilingual detection of offensive spans, i.e., MUDES, based on neural transformers that detect toxic spans in texts.",26,27
2434,233209738,"The pressing need for toxic span detection models to assist human content moderation, processing and flagging content in a more interpretable fashion, has motivated the organization of the SemEval-2021 Task 5: Toxic Spans Detection (Pavlopoulos et al.,",4,5
2435,233209738,"Task and Dataset In the SemEval-2021 Task 5 dataset, the sequence of words that makes a particular post or comment toxic is defined as a toxic span.",21,22
2436,233209738,"Task and Dataset In the SemEval-2021 Task 5 dataset, the sequence of words that makes a particular post or comment toxic is defined as a toxic span.",26,27
2437,233209738,The dataset for this task is extracted from posts in the Civil Comments Dataset that have been found to be toxic.,20,21
2438,233209738,The practice dataset has 690 instances out of which 43 instances do not contain any toxic spans.,15,16
2439,233209738,comprises 485 instances without any toxic spans.,5,6
2440,233209738,Each instance is composed of a list of toxic spans and the post (in English).,8,9
2441,233209738,"Then, we added the toxic words present in the training dataset and we run a simple word matching algorithm the trie data structure.",5,6
2442,233209738,"As anticipated, the algorithm does not evaluate the toxic spans contextually and misses censored swear words.",9,10
2443,233209738,"In this case, each token will be predicted to have one of two possible labels -toxic or not toxic.",19,20
2444,233209738,"Development of MUDES Given the success we observed using neural transformers such as BERT, we developed a (software) framework we call MUDES (Ranasinghe and Zampieri, 2021a ): Multilingual Detection of Offensive Spans, an opensource framework based on transformers to detect toxic spans in texts.",46,47
2445,233209738,Let system A i return a set S t A i of character offsets for parts of a text post that have been found to be toxic.,26,27
2446,233209738,SemEval-2021 Task 5 provided participants with the opportunity of testing computational models to identify token spans in toxic posts as opposed to previous related SemEval tasks such as HatEval and OffensEval that provided participants with datasets annotated at the instance level.,17,18
2447,233209738,"Our results demonstrated that transformer models offered the best generalization results and, given the success observed, we developed MUDES, an open-source software framework based on neural transformers focused on detecting toxic spans in texts.",35,36
2448,249282716,Jigsaw Toxicity Detection The toxicity detection task basically means to distinguish whether the given comment is toxic or not.,16,17
2449,129945536,"For example, in our data, only the semantic relation of Toxicity is defined as connecting a Therapy to a Clini-calFinding (expressing that a therapy was found to cause a specific toxic effect) and so it might seem reasonable to assume that a Toxicity event is being expressed in a document where both a Therapy and a ClinicalFinding are mentioned.",35,36
2450,226262357,"2) T: If toxic waste containg cyanide is not disposed of properly, it may drain into ponds, streams, sewers, and reservoirs.",5,6
2451,226262357,H: Leaks into environment are caused by bad disposal of toxic waste containing cyanide.,11,12
2452,248780440,"2019) or toxic content (Ousidhoum et al.,",3,4
2453,250390517,"Introduction Identifying offensive language in text is an increasingly important challenge that has sparked the release of datasets and advanced models focused on toxic language detection in multiple languages (Razavi et al.,",23,24
2454,250390517,"Hence, identifying and moderating toxic dialogue efficiently and accurately is a task that only grows more crucial, and developing automatic methods to detect and flag offensive language is critical.",5,6
2455,250390517,"While current research suggests that toxic language models may perform differentially on gendered input (Park et al.,",5,6
2456,235352965,"We find that the BERT model associates toxic comments containing offensive words with male annotators, causing the model to predict 67.7% of toxic comments as having been annotated by men.",7,8
2457,235352965,"We find that the BERT model associates toxic comments containing offensive words with male annotators, causing the model to predict 67.7% of toxic comments as having been annotated by men.",24,25
2458,235352965,We show that this disparity between gender predictions can be mitigated by removing offensive words and highly toxic comments from the training data.,17,18
2459,235352965,"We then apply the learned associations between gender and language to toxic language classifiers, finding that models trained exclusively on female-annotated data perform 1.8% better than those trained solely on male-annotated data, and that training models on data after removing all offensive words reduces bias in the model by 55.5% while increasing the sensitivity by 0.4%.",11,12
2460,235352965,"Introduction Toxic language detection has attracted significant research interest in recent years as the volume of toxic user-generated online content has grown with the expansion of the Internet and social media networks (Schmidt and Wiegand, 2017) .",16,17
2461,235352965,Detecting and appropriately moderating toxic comments has become crucial to online platforms to keep people engaged in healthy conversations rather than letting hateful comments drive people away from discussions.,4,5
2462,235352965,Human annotators are the most effective way to filter toxic comments.,9,10
2463,235352965,"As such, toxic language classifiers are trained on datasets composed of comments annotated by humans as an efficient way of detecting toxic language (Schmidt and Wiegand, 2017) .",3,4
2464,235352965,"As such, toxic language classifiers are trained on datasets composed of comments annotated by humans as an efficient way of detecting toxic language (Schmidt and Wiegand, 2017) .",22,23
2465,235352965,"One of the main issues with this approach is that any biases held by the pool of annotators are propagated in the classifier, which can lead to nontoxic comments from certain identity groups being mislabelled as toxic, an effect known as false positive bias (Dixon et al.,",37,38
2466,235352965,This paper is motivated by the lack of understanding of the impact of annotator demographics on bias in toxic language detection.,18,19
2467,235352965,"The main contributions of this work are: I) revealing the bias of BERT-based toxic language detection models towards male annotators, II) recognising the learned associations between male annotators and offensive language in the model, III) demonstrating methods to reduce the bias in the model without reducing the sensitivity.",17,18
2468,235352965,"Bias Statement In this work, we explore gender bias present in toxic language detection systems due to associations between offensive language and annotator gender amplified by the model.",12,13
2469,235352965,"This work demonstrates that toxic comments containing offensive words are associated with male annotators, resulting in female annotators predicted as being male.",4,5
2470,235352965,"The resulting systems create representational harm by overlooking the diverse opinions of female annotators, leading to comments that women may consider toxic not being removed.",22,23
2471,235352965,"Related Work Previous research into gender bias in toxic language detection caused by the demographic makeup of annotators explored superficial differences between male and female annotators, but only reflected on the ethical considerations in-volved rather than thoroughly investigating the differences between annotator groups and attempting to minimise bias in the model.",8,9
2472,235352965,"2017) , uses disaggregated data and transforms the problem from the binary classification of toxicity to the prediction of the proportion of annotators who would classify a comment as toxic.",30,31
2473,235352965,"We note that that the majority of the research into bias in toxic language detection does not reflect on the bias caused by the pool of annotators, and yet research into crowdsourcing demonstrates poor inter-annotator agreement in many corpora and how the results of classification models are skewed by annotator opinions that may not reflect society as a whole.",12,13
2474,235352965,"For the few papers that do examine the role of annotators in toxic language detection, no practical suggestions have been made that aim to reduce the identified bias in the implemented model, which is the main contribution of this paper.",12,13
2475,235352965,"This corpus has been widely used in recent literature developing deep learning approaches to toxic language detection (Pavlopoulos et al.,",14,15
2476,235352965,"As such, this corpus was selected for the comparability of results it provides, in addition to it being the only toxic language cor-pus to provide the genders of the annotators.",22,23
2477,235352965,"Due to the unbalanced nature of the dataset, we balance each training and test set used for gender Toxicity Category Toxicity Score Description Very toxic -2 A very hateful, aggressive, or disrespectful comment that is very likely to make you leave a discussion Toxic -1 A rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion Neither 0 -Healthy contribution 1 A reasonable, civil, or polite contribution that is somewhat likely to make you want to continue a discussion Very healthy contribution 2 A very polite, thoughtful, or helpful contribution that is very likely to make you want to continue a discussion classification by ensuring that 50% of the annotations were made by men and 50% of the annotations were made by women.",25,26
2478,235352965,"After reviewing the toxicity scores given by each group as a whole, we find that female annotators on average annotated 1.72% more comments as toxic than male annotators and assigned toxicity scores that were on average 0.048 lower than those given by their male counterparts, using the toxicity scores given in Table 1 .",26,27
2479,235352965,"These figures indicate a slight disparity between the genders, suggesting that female annotators on average find comments more toxic than male annotators.",19,20
2480,235352965,"For gender classification, as only toxic data is used for training and testing, this means sampling the data evenly from comments given a toxicity score of -1 and those given a toxicity score of -2.",6,7
2481,235352965,"This is necessary as far fewer comments are labelled as 'Very Toxic' than 'Toxic', and as it is the toxic data that is being investigated, it is important to ensure that any differences in the way men and women annotate comments as 'Very Toxic' are not diminished in the results by the substantial size of the 'Toxic' category.",24,25
2482,235352965,The last two categories were not divided evenly as with the toxic categories due to the limited size of the 'Very Healthy' data.,11,12
2483,235352965,We explore this further by tasking the BERT-based model with classifying the gender of an annotator based on a comment the annotator labelled as toxic.,26,27
2484,235352965,"Using training and test data classified as toxic or very toxic by equal numbers of male and female annotators, we find that the model predicts the gender of the annotator of a toxic comment as male 67.7% of the time on average, with the results of the first run shown in Figure 1 .",7,8
2485,235352965,"Using training and test data classified as toxic or very toxic by equal numbers of male and female annotators, we find that the model predicts the gender of the annotator of a toxic comment as male 67.7% of the time on average, with the results of the first run shown in Figure 1 .",10,11
2486,235352965,"Using training and test data classified as toxic or very toxic by equal numbers of male and female annotators, we find that the model predicts the gender of the annotator of a toxic comment as male 67.7% of the time on average, with the results of the first run shown in Figure 1 .",33,34
2487,235352965,"Based on these observations, we hypothesise that the bias of the model towards predicting a toxic comment as having a male annotator is due to the model learning an association between offensive words and male annotators in the training data, exacerbated by the prevalence of offensive words in toxic comments.",16,17
2488,235352965,"Based on these observations, we hypothesise that the bias of the model towards predicting a toxic comment as having a male annotator is due to the model learning an association between offensive words and male annotators in the training data, exacerbated by the prevalence of offensive words in toxic comments.",50,51
2489,235352965,"We do this based on the knowledge that the most toxic comments contain the greatest amount of profanity as comments annotated as 'Toxic' have a median of 1 and a mean of 1.20 offensive words per comment, while the 'Very Toxic' comments have a median of 2 and a mean of 2.41 offensive words per comment.",10,11
2490,235352965,The performance of these models on toxic test data with and without offensive words is displayed in Figure 2 .,6,7
2491,235352965,"We can see that this is not the case for the BERTNotVeryToxic model, as it shows a much more even distribution of gender probabilities for comments with higher numbers of offensive words, again confirming the model's reliance on 'Very Toxic' data to make the association between male annotators and offensive words in toxic comments.",57,58
2492,235352965,"This shows that both men and women annotate comments with a high number of offensive words as toxic, as the es- timation of the probability distribution for the true gender labels is roughly the same for both genders.",17,18
2493,235352965,"For this task, we keep the dataset balanced between toxic and non-toxic comments.",10,11
2494,235352965,"For this task, we keep the dataset balanced between toxic and non-toxic comments.",14,15
2495,235352965,"As we have only examined the relationship between annotator gender and the language in comments that were annotated as toxic, we use sensitivity to measure the performance of each model and set of test data.",19,20
2496,235352965,This measures the ability of each model to correctly classify toxic comments.,10,11
2497,235352965,"2017) , in addition to the ability of the model to associate offensive words with male annotations making it easier to classify toxic comments annotated by men.",23,24
2498,235352965,"We observe that when the offensive words in the training and test data are removed, the toxic comments without offensive words become more difficult to correctly classify than those with offensive words.",17,18
2499,235352965,"We highlight this by analysing the annotations of different genders in the chosen corpus, noting that the number of female annotators is outweighed by the number of male annotators, and that the fe-male annotators are more likely to label a comment as toxic than their male counterparts.",46,47
2500,235352965,"Our findings indicate that the BERT-based model associates comments that contain offensive words with male annotators, despite the data showing that both male and female annotators label comments containing high numbers of offensive words as toxic.",38,39
2501,235352965,"This bias indicates that toxicity models trained on this corpus will be more influenced by the opinions of male annotators, as the diversity of views given by the female annotators makes them unlikely to hold the majority opinion, and those who label comments containing offensive words as toxic are perceived to be male by the model.",49,50
2502,235352965,"We find that removing the most toxic data in addition to removing the offensive words in the training data produces the model with the least bias, showing that comments containing high numbers of offensive words are far less attributed to male annotators than in the original model.",6,7
2503,235352965,"Applying the discovered associations between gender and offensive language to models tasked with classifying the toxicity of comments, we find that toxic comments annotated by men are easier to classify than those annotated by women.",22,23
2504,235352965,This is in part due to the associations between male annotators and offensive language distracting the model from other aspects of toxic comments.,21,22
2505,235352965,"Finally, we show that while it is harder to correctly classify toxic data after the removal of offensive words, models trained on this data show a comparable performance to models trained on unmodified data.",12,13
2506,235352965,"Combining these results with those of the gender predicting models, we see that removing offensive words from the training data of a model is an effective way of reducing the bias towards the opinions of male annotators without compromising the performance of the model on toxic data.",46,47
2507,235352965,This is particularly relevant in problems which rely on the subjective opinion of the annotator like toxic language detection.,16,17
2508,235352965,Conclusion In this paper we seek to quantify the gender bias in toxic language detection systems present as a result of differences in the opinions held by distinct demographic groups of annotators in the corpus and aim to minimise this bias without compromising the performance of the model.,12,13
2509,235352965,"We discover associations between the male bias and the use of offensive language in toxic comments, applying this knowledge to a toxic language classifier to demonstrate an effective way to reduce gender bias without compromising the performance of the model.",14,15
2510,235352965,"We discover associations between the male bias and the use of offensive language in toxic comments, applying this knowledge to a toxic language classifier to demonstrate an effective way to reduce gender bias without compromising the performance of the model.",22,23
2511,235352965,Those implementing toxic language detection systems would be advised to consider the types of bias present in their model and personalise moderation based on the identities of those authoring or viewing comments.,2,3
2512,236486228,Consider the case of a model for toxic language classification; it seems intuitively plausible that incorporating information about users could help reducing the risk of false positives for selfreferential mentions by marginalized groups.,7,8
2513,233189538,"We collect data from a diverse set of 15 online communities (or subreddits) belonging to different genres: politics, sports, hate and toxic.",26,27
2514,233189538,"11 -We compute the number of toxic words using a lexicon compiled from different sources: list of bad words released by Google 12 and lexicons released by other studies (Chandrasekharan et al.,",6,7
2515,239618391,"2021) aims to stimulate research on this issue, while also going beyond the single task of toxic comment classification.",18,19
2516,239618391,"As we consider Subtask 1 (toxic comments) as a task which is more independent of AM, we will not attend to it in this work.",6,7
2517,239618391,Related Work Given that we do not participate in Subtask 1 (toxic comments) we will not go further into details here.,12,13
2518,239618391,Subtask 2 (engaging comments) may be seen as a complement task to toxic comment classification as it focuses more on the identification of respectful conversation.,14,15
2519,233210607,"This paper presents our team's, UTNLP, methodology and results in the SemEval-2021 shared task 5 on toxic spans detection.",19,20
2520,233210607,"Most studies on hate speech detection only provide labels at the sentence level, showing whether the construct as a whole is toxic or not.",22,23
2521,233210607,"2021) , where we aim to detect which spans of a sentence cause it to become toxic.",17,18
2522,233210607,"Next, we test keyword-based methods, trying to find if toxic spans often include words that are known as hateful or negative in available word lists.",13,14
2523,233210607,"We then test attention-based models, building on the hypothesis that what the attention model learns to focus on when detecting toxic speech, are the toxic spans.",23,24
2524,233210607,"We then test attention-based models, building on the hypothesis that what the attention model learns to focus on when detecting toxic speech, are the toxic spans.",28,29
2525,233210607,"Afterwards, we look at the issue as a named entity recognition problem, by considering toxic as a named entity category.",16,17
2526,233210607,"Additionally, we experiment with some hand-crafted features and evaluate their effectiveness in detecting toxic spans.",16,17
2527,233210607,"Related Work In this section we provide a brief overview of studies on hate and toxic speech detection, followed by work on span detection in different sub-fields.",15,16
2528,233210607,"However, no clear distinction between toxic and hateful speech has been provided in the scientific literature (D'sa et al.,",6,7
2529,233210607,"2019b) , toxic word sequences (for sentences in the English language) have been labeled.",3,4
2530,233210607,"In other words, labels are indexes of characters that are toxic in each sentence.",11,12
2531,233210607,"There are a total of 8,629 sentences in the dataset, 8,101 of which include at least one annotated toxic span, and the rest have none.",19,20
2532,233210607,The word-length of the toxic spans varies from one to 176 words.,6,7
2533,233210607,"Additionally, the anotated words are not toxic.",7,8
2534,233210607,In the third example we see that some of the words which do have a toxic connotation are not included in the annotation.,15,16
2535,233210607,"idiotic', 'man is he stupid'] except for one thing they are liars they only care about being thugs ['r one th'] what harm have you ever heard of someone getting attacked by bear while taking dump in the woods please does just owning gun make someone paranoid and pu55y at the same time ['harm'] Table 1 : Examples of comments and the annotated toxic spans in the dataset Datasets Used for Training To better train our models, we made use of several auxiliary datasets.",76,77
2536,233210607,"In other words, we check whether each word in the sentence is among hateful words and if so predict its label to be toxic.",24,25
2537,233210607,"While this method is rather simple and we acknowledge that not all hate words are toxic and they could simply be used as a joke, we consider this method as a good first step to help us better understand the task at hand.",15,16
2538,233210607,Methodology In this study we have tested and compared various models to perform toxic span detection.,13,14
2539,233210607,"To train this model we have two training stages: • Sentence-level classification of hate speech • Span-level detection of toxic spans First we perform pre-processing by removing all punctuations (except those in the middle of words such as a$$hole), and lower-casing all words.",24,25
2540,233210607,We freeze the weights of the BERT layer during this training process as we find through experimentation that fine-tuning BERT in this stage results in lower performance of our model in the toxic span detection task.,34,35
2541,233210607,"Once the model has been trained, we input our sentence and if our sentence-level detector predicts the sentence to be non-hateful we move on and produce a blank output as our toxic span.",36,37
2542,233210607,After the attention scores have been calculated we use rule-based and machine learning models to label spans as toxic.,20,21
2543,233210607,"As such, our toxic span label can be looked at as another NER label.",4,5
2544,233210607,"We considered toxic, non-toxit and padding as labels and applied CRF to this NER task.",2,3
2545,233210607,The padding label was added to reduce the model bias toward the non-toxic class.,14,15
2546,233210607,"As previously mentioned, the expected outputs of the task are numerical indexes of the parts of the string which are believed to be toxic.",24,25
2547,233210607,Results In this section we will report the results of the models introduced in the previous section (4) on the toxic span detection task.,22,23
2548,233210607,"In this method, we first split each sentence into words (using NLTK's functions) and then randomly label each word as toxic or not.",24,25
2549,233210607,Our intuition is that toxic spans will likely include hateful or negative words.,4,5
2550,233210607,Thus we begin with a list of hate words and label any word found on the list as toxic and label the rest as nontoxic.,18,19
2551,233210607,"This method results in an F1-score of 0.332 which is almost twice that of the random baseline, showing that while not all hate words are toxic and not all toxic spans are hate words, there is still a considerable amount of overlap.",28,29
2552,233210607,"This method results in an F1-score of 0.332 which is almost twice that of the random baseline, showing that while not all hate words are toxic and not all toxic spans are hate words, there is still a considerable amount of overlap.",32,33
2553,233210607,We further test if most words in toxic spans will have a negative sentiment value.,7,8
2554,233210607,"Thus we repeat the same method, this time labeling anything with a negative sentiment as toxic.",16,17
2555,233210607,"Finally we mix the two methods (labeling both hate words and words with negative sentiment as toxic), and achieve an F1-score of 0.418.",17,18
2556,233210607,The model then predicted the toxic spans given these true labels achieving an F1 of 0.808.,5,6
2557,233210607,"To test the T5 model, we use hugging-face's T5-base model 2 and frame our problem as one where the context is the Tweet text and the answer is the text of the toxic spans to be detected.",38,39
2558,233210607,Conclusion In this study we presented and compared various methods for toxic span detection.,11,12
2559,212736821,"In recent times, there has been a large number of studies exploring different aspects of hateful and aggressive language and their computational modelling and automatic detection such as toxic comments 1 (Thain et al.,",29,30
2560,21701427,"To express this in annotations, an <adNLink> structure is introduced with the attribute (17) (Markables: m1=""heavy"", ""books"") <entity id=""x1"" target=""#m2"" pred=""book""/> <entity id=""x2"" target=#m1 pred=""heavy""/> <adNLink head=""#x1"" mod=""#x2"" distr=""collective""/> An adjective can be used not only for modifying an NP head noun, but also for modifying a noun that modifies another noun, as in ""(toxic waste) dump"" or ""(natural language) processing"".",81,82
2561,21701427,"The QuantML representation (18) shows how the adjective scope in ""(toxic waste) dump"" can be indicated (see also example ( 22 )). (",14,15
2562,21707697,"The land was devastated not only by acid deposition but also by the accumulation of toxic metals in the soil, the clearcutting of forested areas for fuel, and soil erosion caused by wind, water, and frost heave.",15,16
2563,244464084,"In this paper, we deal with toxic comment detection as a semi-supervised strategy over a heterogeneous graph.",7,8
2564,244464084,"We evaluate the approach on a toxic dataset of the Portuguese language, outperforming several graph-based methods and achieving competitive results compared to transformer architectures.",6,7
2565,244464084,"The term toxic comment is commonly found in literature as harmful speech, hate speech, or offensive language.",2,3
2566,244464084,"To deal with toxic comments, most approaches adopt supervised-machine learning techniques and are mainly focused on the English language (Poletto et al.,",3,4
2567,244464084,"In this paper, we developed a semi-supervised strategy to detect toxic comments in the Brazilian Portuguese language.",13,14
2568,244464084,It has twenty-one thousand annotated tweets as either toxic or non-toxic language.,10,11
2569,244464084,It has twenty-one thousand annotated tweets as either toxic or non-toxic language.,14,15
2570,244464084,"Related Word As aforementioned, the main approaches to detect toxic comments are based on supervised machine learning.",10,11
2571,244464084,"This fact makes the development of robust strategies to handle toxic comments difficult, as they usually require a large corpus.",10,11
2572,244464084,"As one can see in Table 1 , the corpus has a little more non-toxic than toxic tweets.",16,17
2573,244464084,"As one can see in Table 1 , the corpus has a little more non-toxic than toxic tweets.",18,19
2574,244464084,"In this paper, we adopted the binary version of the corpus, i.e., our objective is to identify if a comment is toxic or non-toxic.",24,25
2575,244464084,"In this paper, we adopted the binary version of the corpus, i.e., our objective is to identify if a comment is toxic or non-toxic.",28,29
2576,244464084,"ToLD-BR Corpus In what follows, we detail our strategy to handle toxic texts.",14,15
2577,244464084,"Graph-Based Method We modeled toxic comments detection as a heterogeneous network since this network type contains abundant information with structural relations (edges) among multi-typed nodes as well as unstructured content associated with each node (Zhang et al.,",6,7
2578,244464084,"Id Value 1 Value 2 Label 100 0.004567 0.001456 1 255 0.002789 0.008763 0 878 0.001998 0.005342 0 233 0.008764 0.003215 1 From Table 2 , Id is the object identifier, Values refer to coordinates of each object in the network, and Label 1 shows toxic, while Label 0 is a nontoxic tweet.",49,50
2579,244464084,"Classification With the regularization values, we fed several machine learning algorithms to identify and predict toxic comments.",16,17
2580,244464084,"Then, we applied the machine learning algorithms to train and classifier toxic comments.",12,13
2581,244464084,Our approach is available at https://github.c om/rafaelanchieta/toxic.,10,11
2582,244464084,"Conclusion and Future Work In this paper, we explored a semi-supervised strategy to deal with toxic comments from Twitter.",18,19
2583,244464084,"Then, we applied a regularization algorithm to extract features related to the toxic texts.",13,14
2584,244464084,"Finally, we used these features to feed a classifier to identify and predict toxic comments.",14,15
2585,218974244,"One can use the first instance from Table 5 as an exam-Sense Extract Dynamic Once you allow the toxic cocktails of heinous vaccine materials enter your child's body, you can't retrieve them Dynamic Americans, surely you can tell when something sounds too good to possibly be true!",20,21
2586,218974437,Detecting politically toxic content on the Web can prepare and protect both news readers and online social network communities from misleading or toxic information.,2,3
2587,218974437,Detecting politically toxic content on the Web can prepare and protect both news readers and online social network communities from misleading or toxic information.,22,23
2588,218974437,"For instance, we discovered that we had to explicitly emphasize to all annotators the difference between when a reporter's words and viewpoints are toxic themselves, to when a politically toxic event or statement is reported, and that we are only interested in the first case.",25,26
2589,218974437,"For instance, we discovered that we had to explicitly emphasize to all annotators the difference between when a reporter's words and viewpoints are toxic themselves, to when a politically toxic event or statement is reported, and that we are only interested in the first case.",32,33
2590,250390750,"Due to its hazard, PCL is classified as a milder form of toxic speech (Dale et al.,",13,14
2591,233209981,We evaluated several pre-trained language models using various ensemble techniques for toxic span identification and achieved sizable improvements over our baseline fine-tuned BERT models.,13,14
2592,233209981,Abusive content is very diverse and therefore offensive language and toxic speech detection is not a trivial issue.,10,11
2593,233209981,"In this regard, the task of detecting toxic spans in social media texts deserves close attention.",8,9
2594,233209981,The toxic speech detection task is usually framed as a supervised learning problem.,1,2
2595,233209981,"To better understand the mechanisms of toxic speech detection, some scholars (Waseem et al.,",6,7
2596,233209981,"In recent years, the task of detecting and analyzing abusive, toxic, or offensive language has attracted the attention of more and more researchers.",12,13
2597,233209981,"Most of these datasets classify whole texts or documents, and do not identify the spans that make a text toxic.",20,21
2598,233209981,"Shared Task The task focuses on the evaluation of systems that detect the spans that make a text toxic, when detecting such spans is possible.",18,19
2599,233209981,"Let system A i return a set S t A i of character offsets, for parts of the post found to be toxic.",23,24
2600,233209981,"Due to the lack of available token-level labeled public datasets for toxic comment and the relatively small size and sparsity of dataset provided by the competition, the following training pipeline was proposed to enhance knowledge transfer.",13,14
2601,233209981,"First, fine-tune pre-trained BERT on a larger-scale task of toxic comment classification, using the Jigsaw dataset 1 from which the competition data were constructed.",16,17
2602,233209981,"Second, fine-tune obtained model to solve the actual toxic tokens classification problem.",11,12
2603,233209981,"First step setup and results: • select subset of Jigsaw toxic classification data: all the targets with toxicity score ≥ 0.5 (L = 135168 objects) as class 1 and randomly sampled 3 * L objects with toxicity score < 0.5 as class 0; • stratified 80% train, 20% validation; • 0.968 AUC bert-base, 0.968 AUC bert-large, 0.942 AUC dehate-bert.",11,12
2604,233209981,We noticed that our model is not very good at identifying the posts that have no toxic span annotations.,16,17
2605,233209981,"According to the corpus description, in some toxic posts, the core message is conveyed may be inherently toxic.",8,9
2606,233209981,"According to the corpus description, in some toxic posts, the core message is conveyed may be inherently toxic.",19,20
2607,233209981,"In such cases, the corresponding posts were labeled as not containing toxic spans.",12,13
2608,233209981,"The media's desperation to keep this election close is far past ridiculous"" (training set, the toxic span annotation is underlined); • ""Yup.",19,20
2609,233209981,Conclusion This paper introduces our BERT-based model for toxic spans detection.,10,11
2610,233209981,"As expected, pre-training of the BERT model using an additional domainspecific dataset improves further toxic spans detection performance.",17,18
2611,212717954,rely on a heuristic to flag toxic messages.,6,7
2612,212717954,"Flagged messages are deemed toxic and, unlike WCC, all remaining messages are considered as non-toxic.",4,5
2613,212717954,"Flagged messages are deemed toxic and, unlike WCC, all remaining messages are considered as non-toxic.",18,19
2614,212717954,A message is considered toxic if its toxicity score is above 0.64 and severely toxic if its severe_toxicity score is above 0.92.,4,5
2615,212717954,A message is considered toxic if its toxicity score is above 0.64 and severely toxic if its severe_toxicity score is above 0.92.,14,15
2616,212717954,This poor precision is due to a lot of toxic messages being mistaken for severely toxic messages.,9,10
2617,212717954,This poor precision is due to a lot of toxic messages being mistaken for severely toxic messages.,15,16
2618,236459853,This paper describes the participation of SINAI team at Task 5: Toxic Spans Detection which consists of identifying spans that make a text toxic.,24,25
2619,236459853,"However, detecting toxic spans is crucial to identify why a text is toxic and can assist human moderators to locate this type of content on social media.",3,4
2620,236459853,"However, detecting toxic spans is crucial to identify why a text is toxic and can assist human moderators to locate this type of content on social media.",13,14
2621,236459853,"Specifically, we test the performance of the combination of different pre-trained word embeddings for recognizing toxic entities in text.",18,19
2622,236459853,"Introduction The advance of online communication has increased the use of offensive or toxic language in several websites, including social networks such as Instagram, Twitter, or YouTube.",13,14
2623,236459853,"Therefore, governments, online communities, and social media platforms are continuously taking appropriate actions to implement laws and policies combating toxic language on the Web.",22,23
2624,236459853,"2021) , which aims to identify entities that refer to a toxic language in the text.",12,13
2625,236459853,"To accomplish the task, our team focused on detecting specific types of toxic entities in the text using a methodology based on the BiLSTM-CRF model showing that the combination of different pre-trained language embeddings succeeds in detecting toxic entities.",13,14
2626,236459853,"To accomplish the task, our team focused on detecting specific types of toxic entities in the text using a methodology based on the BiLSTM-CRF model showing that the combination of different pre-trained language embeddings succeeds in detecting toxic entities.",42,43
2627,236459853,"However, other tasks such as Named Entity Recognition (NER) play an important role in this research and are essential to identify the entities that make a text toxic.",30,31
2628,236459853,Highlighting these toxic spans can help human moderators to interpret and identify easily this type of content on the Web instead of relying on a system that generates a score of unexplained toxicity per post.,2,3
2629,236459853,"Inspired by these studies, we have developed a system based on BiLSTM-CRF model along with the combination of different types of word embeddings to address the toxic spans detection task in text.",29,30
2630,236459853,"Named Entity Recognition Methodology To address the toxic detection task, we focus on recognizing and extracting specific types of toxic enti-ties in the text.",7,8
2631,236459853,"Named Entity Recognition Methodology To address the toxic detection task, we focus on recognizing and extracting specific types of toxic enti-ties in the text.",20,21
2632,236459853,BiLSTM-CRF architecture We use the combination of bidirectional LSTM and CRF to identify the toxic spans.,16,17
2633,236459853,"Each instance in the dataset comprises two fields, the text and a list of toxic spans.",15,16
2634,236459853,A toxic span is defined as a sequence of characters in words that attribute to the text's toxicity.,1,2
2635,236459853,"If the text does not contain toxic spans, the span list is empty.",6,7
2636,236459853,"In the first example, the word ""crap"" is labeled as toxic in the text, which has character offsets from 15 to 18.",13,14
2637,236459853,"The second example includes the toxic span ""idiot"" which has character offsets from 4 to 8.",5,6
2638,236459853,"Therefore, our results show the success of the combination of embeddings we chose to solve the task of toxic spans detection in comments using the proposed model.",19,20
2639,236459853,"In this paper, we use a deep learning-based approach for NER to identify spans that make a text toxic, which focuses on the use of a BiLSTM-CRF neural network where different word embeddings are tested.",21,22
2640,184482628,"In order to tackle this problem, firstly it is important to define the toxic language.",14,15
2641,184482628,"The toxic language can be broadly divided into two categories: hate speech and offensive language (Cheng, 2007; Davidson et al.,",1,2
2642,244055107,"2019) , toxic language (van Aken et al.,",3,4
2643,235097460,"In other cases like hateful-content detection (Warner and Hirschberg, 2012) , a message might be toxic to outsiders but perceived as appropriate among close friends (Sap et al.,",20,21
2644,235097460,"Detecting such hateful or toxic speech online might require classifiers to take into account both content and receivers, as well as a broader context.",4,5
2645,52301166,"As patterns of toxic and hateful behavior on Reddit are more well-studied (Chandrasekharan et al.,",3,4
2646,238259663,"Past work has analyzed dialog models from the point of view of safety from toxic language (Xu et al.,",14,15
2647,207997102,2019) pre-sented an approach that automatically classifies a toxic comment using a Multi Dimension Capsule Network.,11,12
2648,207997102,They also conducted an in-depth error analysis of the toxic comment classification.,11,12
2649,245130931,"The Descriptive Annotation Paradigm: Encouraging Annotator Subjectivity as toxic (Sap et al.,",9,10
2650,245130931,"2021)) , and that people who identify as LGBTQ+ or young adults are more likely to rate random social media comments as toxic (Kumar et al.,",24,25
2651,236459940,The anonymity afforded by computermediated communication enables individuals to engage in toxic behaviour which they would otherwise not consider.,11,12
2652,236459940,"Although many datasets and models focusing on toxicity detection have been released, most of them classify entire sequences of text, and do not highlight the individual words that make a text toxic.",33,34
2653,236459940,2021) focuses on the evaluation of systems that can accurately identify toxic spans within text.,12,13
2654,236459940,"In this paper we focus on the shared task, wherein systems are expected to extract a list of toxic spans, or an empty list, per text.",19,20
2655,236459940,A toxic span is defined as a sequence of words that contributes to a text's toxicity.,1,2
2656,236459940,"The improvements arising from adding a CRF supports our third hypothesis described in Section 1, which aims to explore whether similar improvements will arise in the context of toxic span detection.",29,30
2657,236459940,"One of the most straightforward approaches for toxicity detection is to use a word list, whereby the toxicity of a sequence is determined by comparing the words it contains against a list of known toxic words.",35,36
2658,236459940,These matrices are constructed using a subset of the test dataset from which the tokens correctly predicted by both models to be non-toxic have been removed.,24,25
2659,236459940,"Table 4 shows that the baseline DistilBERT model tends to overpredict toxic labels, resulting in 1466 false positives.",11,12
2660,236459940,"is marked as non-toxic, whereas "". . .",5,6
2661,236459940,is marked as toxic.,3,4
2662,236459940,Another trend observed is that both models struggle to correctly predict phrases containing ordinarily non-toxic words which become toxic given the context.,16,17
2663,236459940,Another trend observed is that both models struggle to correctly predict phrases containing ordinarily non-toxic words which become toxic given the context.,20,21
2664,236459940,"Whilst the true labels classify all of these phrases as toxic, both models only predicted the words ""troll"", ""cowards"", ""rubbish"", ""pathetic"", and ""loser"" to be toxic.",10,11
2665,236459940,"Whilst the true labels classify all of these phrases as toxic, both models only predicted the words ""troll"", ""cowards"", ""rubbish"", ""pathetic"", and ""loser"" to be toxic.",40,41
2666,236459940,Conclusion and Future Work This work explored the possibility of improving pre-trained model performance on the token classification task of toxic span detection.,22,23
2667,236459940,"Whilst our experimental results (Section 4) seem to suggest that all three of these features improve the performance of DistilBERT, we note that they do so only marginally (Section Further analysis, however, showed that, whilst the overall F1 improvement from adding TF-IDF was small, the addition of a count-based feature helped to reduce DistilBERT's overprediction of toxic tokens.",69,70
2668,248780490,"These messages are then split in half (a prompt and a continuation) and are used to study, for example, whether a model generates toxic continuations from a non-toxic prompt.",27,28
2669,248780490,"These messages are then split in half (a prompt and a continuation) and are used to study, for example, whether a model generates toxic continuations from a non-toxic prompt.",33,34
2670,218673735,The dataset collected from Twitter API focusing on Syrian and Lebanese tweets rich of toxic utterance.,14,15
2671,218974110,"For instance, demeaning comments, incidents of aggression, trolling, cyberbullies, hate speech, insulting, and toxic utterance have negative impact of users.",20,21
2672,218974110,"Unfortunately, during the recent years, the percentage of using toxic utterance has been increased.",11,12
2673,236486115,"2019) , testing unintended biases within automatic misogyny identification; • Gender stereotypes (MFT): simulating hateful sexist comments and opinions; • Body image stereotypes (MFT): reporting common biases on body image; • Toxic masculinity stereotypes (MFT): reporting common biases on toxic masculinity; • Neutral statements feminism-related (MFT): generating neutral statements where an individual is identified as feminist, i.e. ""Jane is feminist"" or ""John is feminist"".",49,50
2674,236486115,"The hand-coded templates about body image and toxic masculinity, belonging to the MFT test type, do not always use explicitly hateful terms: occasionally prejudices are expressed in a more subtle way that models are often not able to handle.",9,10
2675,236486115,"2018) , we observe that the handcoded templates about body image and toxic masculinity, belonging to the MFT test type, are the most misclassified (respectively 92.8% and 99.2%).",13,14
2676,232428158,"We map the labels (toxic, severe toxic, obscene, threat, insult, and identity hate) into a flagged label; if at least one of these six labels is present for some example, we consider that example as flagged, and as neutral otherwise.",5,6
2677,232428158,"We map the labels (toxic, severe toxic, obscene, threat, insult, and identity hate) into a flagged label; if at least one of these six labels is present for some example, we consider that example as flagged, and as neutral otherwise.",8,9
2678,232428158,"It has binary labels (toxic or non-toxic), and thus it aligns well with our experimental setup.",5,6
2679,232428158,"It has binary labels (toxic or non-toxic), and thus it aligns well with our experimental setup.",9,10
2680,248780370,"However, in this scenario, models are still vulnerable to generating toxic content based on specific prompts (Gehman et al.,",12,13
2681,248780370,"2020) , even though the quantity of unprompted toxic content may decrease.",9,10
2682,248780370,"Solaiman and Dennison (2021) find that, rather than filtering pre-training data, fine-tuning a language model on a small, curated dataset can be effective at limiting toxic generations.",34,35
2683,248780370,2019) use a simple classifier to guide a language model away from generation of toxic content.,15,16
2684,248780370,"2021) detoxify a language model's output by upweighting the probabilities of generating words considered unlikely by a second ""anti-expert"" model that models toxic language.",28,29
2685,248780370,"2021) propose something similar, but use instead the language model's own knowledge of toxic content to detect toxic generations in zero-shot manner.",16,17
2686,248780370,"2021) propose something similar, but use instead the language model's own knowledge of toxic content to detect toxic generations in zero-shot manner.",20,21
2687,248780370,"2020 ), we label an input text as toxic if the API produces a score ≥ 0.5.",9,10
2688,248780370,We then report the percentage of responses labeled toxic.,8,9
2689,248780370,"In general, the safety classifier is more likely to flag responses as unsafe as compared to the PER-SPECTIVE API, possibly because it was trained to identify dialog utterances that are ""not OK to send in a friendly conversation with someone you just met online,"" which may encapsulate more than just toxic responses (Dinan et al.,",57,58
2690,248780370,"2021a) show that popular methods for mitigating toxic generation in LLMs decreases the utility of these models on marginalized groups, potentially resulting in harms such as forcing marginalized users to code-switch.",8,9
2691,248780370,"As noted in the main body of this paper, the API provides an output from 0 to 1 corresponding to the toxicity of the input text, and following previous work, we label an input text as toxic if the API produces a score ≥ 0.5.",39,40
2692,225067553,"There are a series of surveys and shared-tasks that took place in the last years on the subject of detecting the online offensive, abusive, hateful, or toxic content.",31,32
2693,216868486,"Related Work There have been several recent studies on offensive language detection and related tasks such as hate speech, cyberbulling, aggression, and toxic comment detection.",25,26
2694,216868486,Two other shared tasks addressed toxic language.,5,6
2695,216868486,"The Toxic Comment Classification Challenge 2 at Kaggle provided participants with comments from Wikipedia annotated using six labels: toxic, severe toxic, obscene, threat, insult, and identity hate.",19,20
2696,216868486,"The Toxic Comment Classification Challenge 2 at Kaggle provided participants with comments from Wikipedia annotated using six labels: toxic, severe toxic, obscene, threat, insult, and identity hate.",22,23
2697,216868486,"The recent SemEval-2021 Toxic Spans Detection shared task addressed the identification of the token spans that made a post toxic (Pavlopoulos et al.,",19,20
2698,53081574,"Second, we show that community moderation of toxic behavior happens at a higher rate than previously estimated.",8,9
2699,53081574,Moderation of toxic behavior.,2,3
2700,53081574,"However, because there was no structured history of comment deletion, the authors were unable to measure the rate at which toxic comments are moderated through deletion.",22,23
2701,53081574,"8 Each comment is further classified as toxic or non-toxic according to the equal error rate threshold, following the methodology of (Wulczyn et al.,",7,8
2702,53081574,"8 Each comment is further classified as toxic or non-toxic according to the equal error rate threshold, following the methodology of (Wulczyn et al.,",11,12
2703,53081574,We used the same method to labeled comments with the severe toxic model.,11,12
2704,53081574,"Figure 3 shows the fraction of comments deleted by Wikipedians who are not the author of the comment for different lengths of time; distinguishing between comments labeled as toxic, severely toxic, and the background distribution.",29,30
2705,53081574,"Figure 3 shows the fraction of comments deleted by Wikipedians who are not the author of the comment for different lengths of time; distinguishing between comments labeled as toxic, severely toxic, and the background distribution.",32,33
2706,53081574,9 The Jigsaw Toxicity Kaggle Competition: goo.gl/ N6UGPK nearly 33% of toxic comments are removed within a day; And over 82% of severely toxic comments are deleted within a day.,13,14
2707,53081574,9 The Jigsaw Toxicity Kaggle Competition: goo.gl/ N6UGPK nearly 33% of toxic comments are removed within a day; And over 82% of severely toxic comments are deleted within a day.,27,28
2708,53081574,"For example, while in our use cases we have focused on contributors deleting toxic comments, one could seek to understand why and when an editor is deleting or rewording their own comments.",14,15
2709,53644609,"2016) adopts the rhetoric of 'toxic' behavior, a term which metaphorically transposes affective concepts (such as hate) to one of environmental contamination and taboo (Douglas, 1966; Nagle, 2009) ; this represents a subtle move away from an otherwise dominant personalist ideology in which meaning emerges from the beliefs or intentions of the speaker (Hill, 2008) .",7,8
2710,53644609,"4 Data Description The Kaggle Toxic Comment Classification dataset provides decontextualized Wikipedia ""talk page"" comments, each paired with multi-class labels on toxic behavior, judged by human raters (Wulczyn et al.,",26,27
2711,53644609,"The labels of toxicity include 'toxic', 'severe toxic', 'obscene', 'threat', 'insult' and 'identity hate'.",6,7
2712,53644609,"The labels of toxicity include 'toxic', 'severe toxic', 'obscene', 'threat', 'insult' and 'identity hate'.",11,12
2713,247519021,"We considered single-labeled ""Normal"" instances to be non-hate/non-toxic and all the other instances to be toxic.",17,18
2714,247519021,"We considered single-labeled ""Normal"" instances to be non-hate/non-toxic and all the other instances to be toxic.",25,26
2715,247519021,"We considered single-labeled ""Normal"" instances to be non-hate/non-toxic and all the other instances to be toxic.",17,18
2716,247519021,"We considered single-labeled ""Normal"" instances to be non-hate/non-toxic and all the other instances to be toxic.",25,26
2717,247748809,These mechanisms for controlling the generation of language have the potential to mitigate undesirable biases encoded by the large language models and prevent the generation of hate speech and toxic language (Xu et al.;,29,30
2718,247748809,We do however acknowledge that strong controlled generation methods that rely on discriminators have the potential to regurgitate sensitive training data and produce harmful outputs and toxic language (Xu et al.;,26,27
2719,247318722,"In the first case, the vaccine's toxic quality and attendant Outsider status would transfer to Bill Gates, making him an Outsider as well; in the second post, vaccine's beneficial qualities would transfer to him, now making ""Bill Gates"" an Insider.",8,9
2720,233407418,Toxic Spans Detection(TSD) task is defined as highlighting spans that make a text toxic.,14,15
2721,233407418,Many works have been done to classify a given comment or document as toxic or non-toxic.,13,14
2722,233407418,Many works have been done to classify a given comment or document as toxic or non-toxic.,17,18
2723,233407418,comment) is toxic or not.,3,4
2724,233407418,"Related Research A toxic post (comments) is defined as a rude, disrespectful, or unreasonable comment that is likely to make other users leave a discussion.",3,4
2725,233407418,A goal of toxic comment classification is to give a right to freedom of expression on the web.,3,4
2726,233407418,Training complex neural networks (NN) requires enough datasets of toxic comments.,11,12
2727,233407418,"NNs for toxic comment classification use Recurrent Neural Networks (RNN) layers such as Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Unit (GRU) (Chung et al.,",2,3
2728,233407418,"Similarly, the attention mechanism for NNs has been successfully applied to toxic comment classification (Pavlopoulos et al.,",12,13
2729,233407418,"In semi-automated content-moderation, attention can be considered as a highlighter of abusive or toxic words.",18,19
2730,233407418,Our proposed approach aims to predict whatever the tokens of given comments are toxic or not.,13,14
2731,233407418,Each token at each post is labeled as a toxic or not toxic token by their spans.,9,10
2732,233407418,Each token at each post is labeled as a toxic or not toxic token by their spans.,12,13
2733,233407418,"Finally, for a post, toxic or not-toxic labels based on grand truth assigned to preprocessed tokens.",6,7
2734,233407418,"Finally, for a post, toxic or not-toxic labels based on grand truth assigned to preprocessed tokens.",10,11
2735,233407418,"However, we can take advantage of failing representations to represent toxic tokens that are not in vocabulary by setting their representations to zero.",11,12
2736,233407418,"Results Dataset For toxic span detection tasks (Pavlopoulos et al.,",3,4
2737,233407418,2021) posts from publicly available Civil Comment dataset are used for annotations of particular toxic spans in toxic comments.,15,16
2738,233407418,2021) posts from publicly available Civil Comment dataset are used for annotations of particular toxic spans in toxic comments.,18,19
2739,233407418,The task consists of 7939 annotated comments with their toxic spans for training and 2000 for the test.,9,10
2740,233407418,"If we consider A i to return a set S t A i of character offsets for the part of the post found to be toxic, and similarly G t be the character offset of the grand truth annotation of t. We can compute F1 score of system A i with respect to the G for post t as follows: F t 1 (A i , G) = 2 • P t (A i , G) • R t (A i , G) P t (A i , G) + R t (A i , G) P t (A i , G) = |S t A i ∩ S t G | S t A i R t (A i , G) = |S t A i ∪ S t G | S t A i If S t G and S t A i are empty for some post t, then F 1 = 1, and otherwise F 1 = 0.",25,26
2741,21697648,These works focus on detecting toxic behavior after it has already occurred; a notable exception is Cheng et al. (,5,6
2742,21697648,"In turn, these toxic attacks are especially disruptive in Wikipedia since they undermine the social fabric of the community as well as the ability of editors to contribute (Henner and Sefidari, 2016) .",4,5
2743,21697648,"To avoid such a prohibitively exhaustive analysis, we first use a machine learning classifier to identify candidate conversations that are likely to contain a toxic contribution, and then use crowdsourcing to vet the resulting labels and construct our controlled dataset.",25,26
2744,21697648,We ask three annotators to determine whether any of the comments in a snippet is toxic.,15,16
2745,21697648,"Thus, we only consider conversations that start out as ostensibly civil, i.e., where at least the first exchange does not exhibit any toxic behavior, 4 and that continue beyond this first exchange.",25,26
2746,21697648,"This provides a toxicity score t for all comments in our dataset, which we use to preselect two sets of conversations: (a) candidate conversations that are civil throughout, i.e., conversations in which all comments (including the initial exchange) are not labeled as toxic (t < 0.4); and (b) candidate conversations that turn toxic after the first (civil) exchange, i.e., conversations in which the N -th comment (N > 2) is labeled toxic (t ≥ 0.6), but all the preceding comments are not (t < 0.4).",50,51
2747,21697648,"This provides a toxicity score t for all comments in our dataset, which we use to preselect two sets of conversations: (a) candidate conversations that are civil throughout, i.e., conversations in which all comments (including the initial exchange) are not labeled as toxic (t < 0.4); and (b) candidate conversations that turn toxic after the first (civil) exchange, i.e., conversations in which the N -th comment (N > 2) is labeled toxic (t ≥ 0.6), but all the preceding comments are not (t < 0.4).",65,66
2748,21697648,"This provides a toxicity score t for all comments in our dataset, which we use to preselect two sets of conversations: (a) candidate conversations that are civil throughout, i.e., conversations in which all comments (including the initial exchange) are not labeled as toxic (t < 0.4); and (b) candidate conversations that turn toxic after the first (civil) exchange, i.e., conversations in which the N -th comment (N > 2) is labeled toxic (t ≥ 0.6), but all the preceding comments are not (t < 0.4).",90,91
2749,84843035,"Related Work Different abusive and offense language identification problems have been explored in the literature ranging from aggression to cyber bullying, hate speech, toxic comments, and offensive language.",25,26
2750,84843035,"Toxic comments: The Toxic Comment Classification Challenge 5 was an open competition at Kaggle, which provided participants with comments from Wikipedia organized in six classes: toxic, severe toxic, obscene, threat, insult, identity hate.",28,29
2751,84843035,"Toxic comments: The Toxic Comment Classification Challenge 5 was an open competition at Kaggle, which provided participants with comments from Wikipedia organized in six classes: toxic, severe toxic, obscene, threat, insult, identity hate.",31,32
2752,243865327,"We did not observe any toxic or hateful language in our dataset -though researchers working on the dataset in future are advised due caution since the annotations are crowd-sourced, and might reflect certain biases.",5,6
2753,201666862,"This competition creates an environment that rewards sensational, fake, and toxic news.",12,13
2754,201666862,"To help limit their spread and impact, we propose and develop a news toxicity detector that can recognize various types of toxic content.",22,23
2755,201666862,Then we trained a multi-class classifier with nine categories: eight toxic and one non-toxic.,13,14
2756,201666862,Then we trained a multi-class classifier with nine categories: eight toxic and one non-toxic.,18,19
2757,201666862,"Given the proliferation of toxic news online, there have been many efforts to create tools and mechanisms to counteract their effect and spread.",4,5
2758,201666862,"Yet, there are limitations in how much it is possible to handle manually in a short period of time (and time is very critical as toxic content spreads fast).",27,28
2759,201666862,"Thus, an attractive alternative is to use machine learning and natural language processing to automate the process of toxic news detection.",19,20
2760,201666862,"If a medium published a toxic news, this was recorded and the article, as well as the medium, got labelled accordingly.",5,6
2761,201666862,We further add a non-toxic label for articles that represent good news.,6,7
2762,201666862,"While most of the above research has focused on isolated and specific task (such as trustworthiness, fake news, fact-checking), here we try to create a holistic approach by exploring several toxic and non-toxic labels simultaneously.",37,38
2763,201666862,"While most of the above research has focused on isolated and specific task (such as trustworthiness, fake news, fact-checking), here we try to create a holistic approach by exploring several toxic and non-toxic labels simultaneously.",41,42
2764,201666862,"The site contains information about 700 media, and 150 of them are associated with at least one toxic article.",18,19
2765,201666862,Many of these toxic articles are removed after the respective media have been contacted and informed about the problems with these articles.,3,4
2766,201666862,"For each Web page with a toxic label, we ran a mechanical crawler to obtain its contents.",6,7
2767,201666862,"In addition to this dataset of only toxic articles, we added some ""non-toxic"" articles, fetched from media without toxicity examples in Media Scan: we added a total of 96 articles from 25 media.",7,8
2768,201666862,"In addition to this dataset of only toxic articles, we added some ""non-toxic"" articles, fetched from media without toxicity examples in Media Scan: we added a total of 96 articles from 25 media.",16,17
2769,201666862,"Our baseline approach is based on selecting the most frequent class, i.e., non-toxic, which covers 30.30% of the dataset (see Table 1 ).",16,17
2770,201666862,We can see that the model works best for the biggest non-toxic class.,13,14
2771,201666862,"We can see in Figure 1 that those three classes have less than 60 articles combined, while the ""non-toxic"" only had 96 articles.",22,23
2772,201666862,We then trained a multi-class classifier with nine categories: eight toxic and one non-toxic.,13,14
2773,201666862,We then trained a multi-class classifier with nine categories: eight toxic and one non-toxic.,18,19
2774,247547046,Filtering knowledge candidates from PTLMs Our initial experiments suggests that that knowledge generated from PTLMs can be inappropriate (contains or toxic content) and misleading/nonfactual.,21,22
2775,247547046,"Since we use PTLMs as knowledge source, we inherit the general risk of generating biased or toxic language, which should be carefully filtered.",17,18
2776,248780452,This work considers a sentence toxic if the output is greater than 0.5.,5,6
2777,248780452,The HARMFULNESS score is computed as the proportion of the generated sentences classified as toxic by the classifier.,14,15
2778,248177981,"Through the analysis of more than a dozen pretrained language models of varying sizes on two toxic text classification tasks (English), we demonstrate that focusing on accuracy measures alone can lead to models with wide variation in fairness characteristics.",16,17
2779,248177981,"In this work, we focus on one important application of fine-tuned LMs, toxic text classification.",16,17
2780,248177981,"In downstream applications such as toxic text classification, it is important to examine the behavior of LMs in terms of measures other than task-specific accuracy.",5,6
2781,248177981,"As a first step toward this goal, we provide herein an empirical characterization of LMs for the task of toxic text classification using a combination of accuracy and bias measures, and study two post-processing methods for bias mitigation that have proved successful for structured, tabular data.",20,21
2782,248177981,"Since we are concerned with the application of LMs to the specific task of toxic text classification, we restrict our focus to group fairness measures, which fall under the category of extrinsic bias.",14,15
2783,248177981,"Datasets We used two datasets that deal with toxic text classification: 1) Jigsaw, a large dataset released for the ""Unintended Bias in Toxicity Classification"" Kaggle competition (Jigsaw, 2019 ) that contains online comments on news articles, and 2) HateXplain, a dataset recently introduced with the intent of studying explanations for offensive and hate speech in Twitter and Twitter-like data (i.e., gab.com).",8,9
2784,248177981,"To summarize, we analyze the bias/fairness of toxic text prediction in the presence or absence of information that refers to religion, race or gender, respectively.",10,11
2785,248177981,"In toxic text classification, a true positive means that a toxic text is correctly identified as such, while a false positive means that a benign piece of text is marked as toxic.",1,2
2786,248177981,"In toxic text classification, a true positive means that a toxic text is correctly identified as such, while a false positive means that a benign piece of text is marked as toxic.",11,12
2787,248177981,"In toxic text classification, a true positive means that a toxic text is correctly identified as such, while a false positive means that a benign piece of text is marked as toxic.",33,34
2788,248177981,"In terms of harms, a false negative (toxic text that is missed) may cause individuals to feel threatened or (Sun et al.,",9,10
2789,248177981,"This simple method corresponds to FST without calibrating the LM outputs, choosing large enough so that FST yields an identity transformation, and thresholding at level t. The accuracy-fairness relationship in toxic text classification We report on the performance and fairness characteristics of several LMs while varying parameters such as random seeds and training data size.",34,35
2790,248177981,"We would like to emphasize that identifying toxic text is not an easy task, not even for humans.",7,8
2791,248177981,"Motivated by this observation, we started looking into understanding the quality of datasets used in toxic text prediction (Arhin et al.,",16,17
2792,248177981,Each sample in the dataset (see Table 4 for a few samples from the dataset) has a toxicity score and we consider anything higher than 0.5 to be toxic.,30,31
2793,201683127,"show that male and female annotators have different perceptions of what is considered toxic (Binns, Veale, Van Kleek, & Shadbolt, 2017) .",13,14
2794,235097313,"Online abuse can inflict harm on users and communities, making online spaces unsafe and toxic.",15,16
2795,235097313,"Person-and Identity-directed forms of abuse are often addressed in separate taxonomies, although in some studies they have been merged into a more general 'toxic' category (Wulczyn et al.,",29,30
2796,235097313,"Predictive methods could be applied to understand and forecast when a conversation is turning toxic, potentially enabling real-time moderation interventions.",14,15
2797,182953211,"The explosion was triggered by an oil leak, though local media has not reported any toxic chemical spills.",16,17
2798,40752541,"2016) and a Google Jigsaw team partnered with the Wikimedia Foundation to develop solutions for reducing personal attacks or toxic comments, in Wikimedia editing (Wulczyn et al.,",20,21
2799,40752541,"Wulczyn demonstrated that their machine models can perform as well as three human graders in identifying toxic comments in Wikipedia editing wars, and in addition released the Perspective API to enable developers to utilize their solution.",16,17
2800,247596779,Prior research has discussed and illustrated the need to consider linguistic norms at the community level when studying taboo (hateful/offensive/toxic etc.),24,25
2801,247596779,"A reason for this bias is because of their dominant occurrence in hateful, toxic or otherwise taboo contexts, thereby skewing training datasets.",14,15
2802,236460356,We leverage a BLSTM with attention to identify toxic spans in texts.,8,9
2803,236460356,"Besides the provided dataset, we explore the transferability of 5 different toxic related sets, including offensive, toxic, abusive, and hate sets.",12,13
2804,236460356,"Besides the provided dataset, we explore the transferability of 5 different toxic related sets, including offensive, toxic, abusive, and hate sets.",19,20
2805,236460356,We conduct an error analysis to examine which types of toxic spans were missed and which were wrongly inferred as toxic along with the main reasons why they occurred.,10,11
2806,236460356,We conduct an error analysis to examine which types of toxic spans were missed and which were wrongly inferred as toxic along with the main reasons why they occurred.,20,21
2807,236460356,"Therefore, it is beneficial for platforms to moderate the amount of toxic language allowed.",12,13
2808,236460356,"While automated toxic language classifiers have made strides at determining if text is toxic (Yenala et al.,",2,3
2809,236460356,"While automated toxic language classifiers have made strides at determining if text is toxic (Yenala et al.,",13,14
2810,236460356,"Automated systems could assist human moderators further by noting which areas in the text are toxic, thus allowing the human to quickly check text for toxicity.",15,16
2811,236460356,"Finding these toxic areas is the task that is proposed by (Pavlopoulos et al.,",2,3
2812,236460356,"Specifically, given English toxic text, determine where the ""toxic spans"" occur, where a ""toxic span"" is the character positions in the text where the toxicity appears.",4,5
2813,236460356,"Specifically, given English toxic text, determine where the ""toxic spans"" occur, where a ""toxic span"" is the character positions in the text where the toxicity appears.",11,12
2814,236460356,"Specifically, given English toxic text, determine where the ""toxic spans"" occur, where a ""toxic span"" is the character positions in the text where the toxicity appears.",19,20
2815,236460356,"For example, in ""your an idiot, this is a tax based on a lie"", the toxic span would be [8, 9, 10, 11, 12] or the character positions of ""idiot"".",20,21
2816,236460356,Note that not all toxic texts have toxic spans.,4,5
2817,236460356,Note that not all toxic texts have toxic spans.,7,8
2818,236460356,"is a toxic phrase, yet no specific words are themselves toxic.",2,3
2819,236460356,"is a toxic phrase, yet no specific words are themselves toxic.",11,12
2820,236460356,"To find toxic spans in text, we employ a Bidirectional Long Short-term Memory model (BLSTM) with attention.",2,3
2821,236460356,"Thus many previously developed toxic datasets would need to be converted to be useful to a direct model (since they are binary labeled sets, not span labeled sets).",4,5
2822,236460356,"Therefore, we examine how well toxic texts other than the provided training data transfer to this task.",6,7
2823,236460356,Approach We expand on the approach we use to identify toxic spans.,10,11
2824,236460356,We examine both different toxic based datasets as well as different BLSTM based methods.,4,5
2825,236460356,Toxic Datasets We examine the provided training set as well 5 different toxic-based data sets.,12,13
2826,236460356,Each additional dataset is explicitly toxic or in a related area (e.g. offense).,5,6
2827,236460356,This set had no non-toxic examples thus we pulled non-toxic examples from the Kaggle Toxic set at a 1-to-1 ratio.,6,7
2828,236460356,This set had no non-toxic examples thus we pulled non-toxic examples from the Kaggle Toxic set at a 1-to-1 ratio.,13,14
2829,236460356,"Kaggle Toxic: Comments from Wikipedia's talk page edits, presented in the Kaggle toxic classification challenge 3 , labeled as toxic or not toxic (as well as further dividied into subcategories).",15,16
2830,236460356,"Kaggle Toxic: Comments from Wikipedia's talk page edits, presented in the Kaggle toxic classification challenge 3 , labeled as toxic or not toxic (as well as further dividied into subcategories).",22,23
2831,236460356,"Kaggle Toxic: Comments from Wikipedia's talk page edits, presented in the Kaggle toxic classification challenge 3 , labeled as toxic or not toxic (as well as further dividied into subcategories).",25,26
2832,236460356,"Each token (word) has an attention score used to generate the class label (toxic, non-toxic) .",16,17
2833,236460356,"Each token (word) has an attention score used to generate the class label (toxic, non-toxic) .",20,21
2834,236460356,We use this score to determine if a given token is toxic.,11,12
2835,236460356,We label each token with an attention score of greater than the average attention score as part of the toxic span.,19,20
2836,236460356,"Frequency Ratio: In theory, a toxic token should exist more frequency in toxic texts, compared to non-toxic texts.",7,8
2837,236460356,"Frequency Ratio: In theory, a toxic token should exist more frequency in toxic texts, compared to non-toxic texts.",14,15
2838,236460356,"Frequency Ratio: In theory, a toxic token should exist more frequency in toxic texts, compared to non-toxic texts.",21,22
2839,236460356,"2019) , we generate a frequency score for each token: s c (u, a) = count(u, D a ) + λ ( a ∈A,a =a count(u, D a )) + λ (1) where u is the token, a is an attribute (e.g. toxic or non-toxic), D a represents the texts with attribute a, and count(u, D a ) represents the number of times u appears in D a .",57,58
2840,236460356,"2019) , we generate a frequency score for each token: s c (u, a) = count(u, D a ) + λ ( a ∈A,a =a count(u, D a )) + λ (1) where u is the token, a is an attribute (e.g. toxic or non-toxic), D a represents the texts with attribute a, and count(u, D a ) represents the number of times u appears in D a .",61,62
2841,236460356,"For our method, we chose a threshold of 5, such that, if s c (u, a) >= 5, then u is considered part of the toxic span.",33,34
2842,236460356,"2019) or ""Important Score"" (Lee, 2020) in previous research, greedy remove determines the amount each token is contributing to the text by removing each token one-by-one (each token is replaced after drop is noted) and noting the drop in toxic probability.",52,53
2843,236460356,The token with the highest probability is removed (i.e. added to the toxic span) and then the BLSTM classification is checked.,13,14
2844,236460356,"If the text is no longer considered toxic, the toxic span is returned, otherwise the next token with the highest drop is removed and it is added to the toxic span.",7,8
2845,236460356,"If the text is no longer considered toxic, the toxic span is returned, otherwise the next token with the highest drop is removed and it is added to the toxic span.",10,11
2846,236460356,"If the text is no longer considered toxic, the toxic span is returned, otherwise the next token with the highest drop is removed and it is added to the toxic span.",31,32
2847,236460356,This process is continued until the text is no longer considered toxic by the BLSTM.,11,12
2848,236460356,We test the methods of each BLSTM on the provided toxic test set.,10,11
2849,236460356,"The score used is the F1 for a given text t, where A i is the system and G is ground truth, and S t is the set of toxic spans: F t 1 (A i , G) = 2 • P t (A i , G) • R t (A i , G) P t (A i , G) + R t (A i , G) (2) P t (A i , G) = |S t A i ∩ S t G | |S t A i | (3) R t (A i , G) = |S t A i ∩ S t G | |S t G | (4) When S t G is empty and S t A i is empty, then F t 1 (A i , G) = 1, otherwise F t 1 (A i , G) = 0.",31,32
2850,236460356,This is achieved via a BLSTM trained on the provided training data (Toxic Train) which uses the precision hybrid approach to find toxic spans.,24,25
2851,236460356,"This is surprising because the goal of the task is to identify ""toxic"" spans and Kaggle is toxic data, while OLID is offense, and Founta is abuse/hate data.",13,14
2852,236460356,"This is surprising because the goal of the task is to identify ""toxic"" spans and Kaggle is toxic data, while OLID is offense, and Founta is abuse/hate data.",19,20
2853,236460356,"This difference is most likely due to our system aiming to find toxic spans indirectly, rather than directly training on the spans.",12,13
2854,236460356,One reason for missed spans is the difficulty between translating toxic tokens to toxic spans.,10,11
2855,236460356,One reason for missed spans is the difficulty between translating toxic tokens to toxic spans.,13,14
2856,236460356,"As our methodology finds specific tokens toxic, it needs to back-translate that to character positions in the original text.",6,7
2857,236460356,Another reason for missed spans is they are part of a toxic phrase and others part of the phrase are more toxic.,11,12
2858,236460356,Another reason for missed spans is they are part of a toxic phrase and others part of the phrase are more toxic.,21,22
2859,236460356,"For example, ""Total rubbish"" is marked as entirely toxic, however, our system marks only ""rubbish"".",11,12
2860,236460356,"Our system misses that these non-toxic words (e.g. ""total"") can become toxic when combined in a phrase.",7,8
2861,236460356,"Our system misses that these non-toxic words (e.g. ""total"") can become toxic when combined in a phrase.",17,18
2862,236460356,Certain words overshadowed by more toxic words.,5,6
2863,236460356,"Some of the tokens which remain (e.g. ""poser"") seem to be overshadowed by a stronger toxic word.",19,20
2864,236460356,"The guy is a total poser and a fool"", is marked as having three toxic spans (stupid, poser, fool), however, our system labels ""stupid"" and ""fool"" as toxic spans but not ""poser"".",16,17
2865,236460356,"The guy is a total poser and a fool"", is marked as having three toxic spans (stupid, poser, fool), however, our system labels ""stupid"" and ""fool"" as toxic spans but not ""poser"".",39,40
2866,236460356,"This is most likely because our system views ""stupid/fool"" as more toxic words and thus assigns higher attention scores than for ""poser"".",15,16
2867,236460356,"URStupid"", ""URStupid"" is marked as toxic, but it is a non standard word.",9,10
2868,236460356,Non-toxic words become associated with toxicity in training set.,2,3
2869,236460356,"Depending on the training set, our model can learn non-toxic words as toxic if they occur more frequently with the toxic label compared to the non-toxic.",12,13
2870,236460356,"Depending on the training set, our model can learn non-toxic words as toxic if they occur more frequently with the toxic label compared to the non-toxic.",15,16
2871,236460356,"Depending on the training set, our model can learn non-toxic words as toxic if they occur more frequently with the toxic label compared to the non-toxic.",23,24
2872,236460356,"Depending on the training set, our model can learn non-toxic words as toxic if they occur more frequently with the toxic label compared to the non-toxic.",30,31
2873,236460356,"An interesting example is ""trump"" being marked as a toxic span, which means the training data provides ""trump"" in enough toxic texts for our system to associate that with toxicity.",11,12
2874,236460356,"An interesting example is ""trump"" being marked as a toxic span, which means the training data provides ""trump"" in enough toxic texts for our system to associate that with toxicity.",25,26
2875,236460356,"For example, in the phrase ""dumb things"", our system correctly identifies that ""dumb"" is a toxic span, but during translation back to character index, it is shifted 4 characters to the right, so instead our system says ""thin"" (part of ""things"") is the toxic span.",21,22
2876,236460356,"For example, in the phrase ""dumb things"", our system correctly identifies that ""dumb"" is a toxic span, but during translation back to character index, it is shifted 4 characters to the right, so instead our system says ""thin"" (part of ""things"") is the toxic span.",58,59
2877,236460356,"That is, words which appear to be toxic, are marked as non toxic in the gold labels.",8,9
2878,236460356,"That is, words which appear to be toxic, are marked as non toxic in the gold labels.",14,15
2879,236460356,"calling someone a ""dirty cop"" is arguably toxic, and our system marks ""dirty"" as toxic, but the gold standard labels indicate there is no toxic span in the text.",9,10
2880,236460356,"calling someone a ""dirty cop"" is arguably toxic, and our system marks ""dirty"" as toxic, but the gold standard labels indicate there is no toxic span in the text.",19,20
2881,236460356,"calling someone a ""dirty cop"" is arguably toxic, and our system marks ""dirty"" as toxic, but the gold standard labels indicate there is no toxic span in the text.",30,31
2882,236460356,Toxic words used in non-toxic ways.,6,7
2883,236460356,"A final prominent reason for falsely predicted spans is words that are sometimes toxic, appear in non toxic sentences.",13,14
2884,236460356,"A final prominent reason for falsely predicted spans is words that are sometimes toxic, appear in non toxic sentences.",18,19
2885,236460356,"our classifier tags ""stupid"" as toxic, but it is not.",7,8
2886,236460356,"Where the annotators might label a word are being used in a non-toxic way, we might label it as toxic and therefore say it is unclear why the span is not counted correctly.",14,15
2887,236460356,"Where the annotators might label a word are being used in a non-toxic way, we might label it as toxic and therefore say it is unclear why the span is not counted correctly.",22,23
2888,236460356,Words overshadowed by more toxic words was the next highest error appearing in 25 of 100 samples.,4,5
2889,236460356,"Toxic words used in non-toxic manners caused the most false predictions (30 out of 100), followed by the translation from tokens to spans (23 out of 100).",6,7
2890,236460356,"Leveraging Multiple Views via Ensemble of Datasets As toxicity can vary, it follows that different toxic datasets can identify different aspects of toxic spans.",16,17
2891,236460356,"Leveraging Multiple Views via Ensemble of Datasets As toxicity can vary, it follows that different toxic datasets can identify different aspects of toxic spans.",23,24
2892,236460356,This higher score helps demonstrate how the various toxic-based datasets can bring useful perspectives to the solution.,8,9
2893,236460356,"While an ensemble can increase performance, not all toxic-based datasets help the model.",9,10
2894,236460356,Toxic Language: In recent years toxic language classification has seen much focus.,6,7
2895,236460356,"For toxicity specifically, Karan and Šnajder (2019) examine detecting threads that could lead to toxic language in the future.",17,18
2896,236460356,2018) present specific challenges that remain for toxic classification.,8,9
2897,236460356,"Though, the methodology is similar, we examine it in terms of toxic span identification where it has not been done before.",13,14
2898,236460356,Conclusion We leveraged various BLSTM methods to identify toxic spans.,8,9
2899,236460356,We examined how well various toxic related datasets are able to transfer to this task.,5,6
2900,235313334,"Arguably, they are most realistic, e.g., in the context of online discussion forums and other forms of social media where users may not know which model is employed (e.g.) to censor toxic comments and users may also not have (large-scale) direct access to model predictions.",36,37
2901,250144216,"For instance, similar terms are used interchangeably across publications and datasets, e.g. abusive, offensive, or toxic (Madukwe et al.,",19,20
2902,250144216,We posit that a clear relation to (membership of) a target group of the victim sets hate speech apart from other forms of toxic or abusive language.,25,26
2903,222124957,"2019b, SuperGLUE) 2020 ) highlight the prevalance of toxic language in the common pretraining corpora and stress the important of pretraining data selection, especially for deployed models.",10,11
2904,222291230,"2017) and Rodriguez and Rojas-Galeano (2018) attack toxic detection systems by obfuscation, i.e., misspelling of the abusive words (a low-level attack), and via polarization, i.e., inverting the meaning of the sentences by inserting the word ""not"".",12,13
2905,222291230,"Possible labels are: toxic, obscene, threat, insult and identity hate.",4,5
2906,222291230,"For this task, we choose the jigsaw toxic comment challenge dataset from kaggle 4 .",8,9
2907,12732474,"Hepatitis is a disease caused by infectious or toxic agents and characterized by jaundice, fever and liver enlargement.",8,9
2908,237572304,Our second method uses BERT to replace toxic words with their non-offensive synonyms.,7,8
2909,237572304,It uses BERT to replace toxic spans found in the sentence with their non-toxic alternatives.,5,6
2910,237572304,It uses BERT to replace toxic spans found in the sentence with their non-toxic alternatives.,15,16
2911,237572304,"Interestingly, BERT does not need any class-conditional pre-training to successfully change the text style from toxic to normal.",20,21
2912,237572304,"We create an English parallel corpus for the detoxification task by retrieving toxic/safe sentence pairs from the ParaNMT dataset (Wieting and Gimpel, 2018) .",12,13
2913,237572304,"2020) uses a pipeline of models: a search engine finds non-toxic sentences similar to the given toxic ones, an MLM fills the gaps that were not matched in the found sentences, and a seq2seq model edits the generated sentence to make it more fluent.",14,15
2914,237572304,"2020) uses a pipeline of models: a search engine finds non-toxic sentences similar to the given toxic ones, an MLM fills the gaps that were not matched in the found sentences, and a seq2seq model edits the generated sentence to make it more fluent.",20,21
2915,237572304,2020) approach a similar problem: preventing a language model from generating toxic text.,13,14
2916,237572304,GeDi was successfully applied to guiding a GPT-2 language model towards generating texts of particular topics and making the generated text less toxic.,22,23
2917,237572304,Our reranker is a pre-trained toxicity classifier which chooses the least toxic hypothesis generated by the ParaGeDi model.,13,14
2918,237572304,"For the problem of detoxification, it means that the model will try less to insert polite words than to delete toxic words from the sentence.",21,22
2919,237572304,"2019) has been trained on the task of filling in gaps (""masked LM""), we can use it to insert non-toxic words instead of the toxic ones.",27,28
2920,237572304,"2019) has been trained on the task of filling in gaps (""masked LM""), we can use it to insert non-toxic words instead of the toxic ones.",32,33
2921,237572304,"This can be done in different ways, e.g. by training a word-level toxicity classifier or manually creating a vocabulary of rude and toxic words.",25,26
2922,237572304,This is a logistic regression model which classifies sentences as toxic or neutral and uses their words as features.,10,11
2923,237572304,The words with the highest weights are usually toxic.,8,9
2924,237572304,"they are all commies who hate the USA For each word in a sentence, we compute the toxicity score and then define toxic words as the words with the score above a threshold t = max(t min , max(s 1 , s 2 , ..., s n )/2), where s 1 , s 2 , ..., s n are scores of all words in a sentence and t min = 0.2 is a minimum toxicity score.",23,24
2925,237572304,This adaptive threshold allows balancing the percentage of toxic words in a sentence so that we avoid cases when too many or no words are marked as toxic.,8,9
2926,237572304,This adaptive threshold allows balancing the percentage of toxic words in a sentence so that we avoid cases when too many or no words are marked as toxic.,27,28
2927,237572304,"Despite using class-specific sentence embeddings, conditional BERT often predicts toxic words, apparently paying more attention to the context than to the embeddings of the desired class.",12,13
2928,237572304,To force the model to generate non-toxic words we calculate the toxicity of each token in BERT vocabulary and penalize the predicted probabilities of tokens with positive toxicities.,8,9
2929,237572304,"To prepare the toxic dataset, we divide the comments labelled as toxic into sentences (the original comments are often too long) and classify each of them with our toxicity classifier.",3,4
2930,237572304,"To prepare the toxic dataset, we divide the comments labelled as toxic into sentences (the original comments are often too long) and classify each of them with our toxicity classifier.",12,13
2931,237572304,"Sentences classified as toxic are used as the toxic part of the dataset (we find 154,771 of them).",3,4
2932,237572304,"Sentences classified as toxic are used as the toxic part of the dataset (we find 154,771 of them).",8,9
2933,237572304,"To select the neutral part of the dataset, we randomly pick the same number of non-toxic sentences from the sentence-separated Jigsaw data.",18,19
2934,237572304,2019) on the training part of the Jigsaw-1 dataset using two control codes for toxic and polite texts.,15,16
2935,237572304,We apply the classifier from section 5.1 to select the least toxic candidate from the 10 resulting paraphrases.,11,12
2936,237572304,We train it on a parallel dataset of 200 toxic and safe sentences.,9,10
2937,237572304,"We randomly select toxic sentences from the Google Jigsaw toxic comments dataset (Jigsaw, 2018) and manually rewrite them in the neutral tone.",3,4
2938,237572304,"We randomly select toxic sentences from the Google Jigsaw toxic comments dataset (Jigsaw, 2018) and manually rewrite them in the neutral tone.",9,10
2939,237572304,"The success of CondBERT is explained by its use of heuristics targeted at the components of the metric: (i) it is penalized for generating toxic tokens, which ensures a high ACC score, (ii) over 80% tokens stay unchanged, and the replacements are selected with respect to the similarity to the original words, increasing the overall SIM score, (iii) MLM is pretrained to replace masked tokens with plausible substitutes, increasing FL.",27,28
2940,237572304,"TemplateBased achieves a high similarity because it keeps most of the original sentence intact, and RetrieveOnly yields a high similarity and style transfer accuracy, because it outputs real non-toxic sentences from the training data.",32,33
2941,237572304,"2020) 0.29 0.69 0.80 0.15 ± 0.0027 En→Ig→En MT (baseline) 0.37 0.68 0.57 0.12 ± 0.0025 T5 paraphraser (baseline) 0.15 0.90 0.87 0.11 ± 0.0029 SST (Lee, 2020) 0.80 0.55 0.12 0.05 ± 0.0019 En→Fr→En MT (baseline) 0.06 0.91 0.81 0.04 ± 0.0019   phrase dataset (Wieting and Gimpel, 2018) with our toxicity classifier described in Section 5.1 and obtain 500,000 paraphrase pairs where one sentence is more toxic than the other (for more details on the data collection process please see Appendix D).",81,82
2942,237572304,We then compare the regular paraphraser from Section 5.4 fine-tuned on a random subset of ParaNMT (regular) with its version fine-tuned on the mined toxic/safe parallel paraphrase corpus (mined).,30,31
2943,237572304,"However, this result shows that the general-purpose ParaNMT corpus contains a large number of toxic/safe paraphrase pairs.",17,18
2944,237572304,The input (toxic) sentences for the evaluation were manually preselected to filter out disfluent or senseless utterances (this pre-selection did not consider the outputs).,3,4
2945,237572304,"Conclusion We present two style transfer models tailored for detoxification, i.e. transfer from toxic to non-toxic texts.",14,15
2946,237572304,"Conclusion We present two style transfer models tailored for detoxification, i.e. transfer from toxic to non-toxic texts.",18,19
2947,237572304,"Toxification of Texts Detoxification task implies the possibility to perform the opposite transformation, i.e. to rewrite a neutral text into a toxic one.",22,23
2948,237572304,"However, in case of CondBERT, the quality of such transformation would be bad, and it would be almost impossible to pass the results of this ""toxification"" off as real toxic sentences.",34,35
2949,237572304,The reason for that is the structure of toxic data.,8,9
2950,237572304,One of the main properties of toxic style is the presence of lexical markers of this style (rude or obscene words).,6,7
2951,237572304,They identify toxic words and replace them with non-toxic synonyms.,2,3
2952,237572304,They identify toxic words and replace them with non-toxic synonyms.,10,11
2953,237572304,"First, there do not exist non-toxic words which are strong indicators of neutral (non-toxic) style.",8,9
2954,237572304,"First, there do not exist non-toxic words which are strong indicators of neutral (non-toxic) style.",19,20
2955,237572304,"Second, it is almost infeasible to identify non-toxic words which have toxic synonyms and replace them appropriately.",10,11
2956,237572304,"Second, it is almost infeasible to identify non-toxic words which have toxic synonyms and replace them appropriately.",14,15
2957,237572304,"5  On the other hand, we suggest mitigating this policy by rewriting toxic messages instead of removing them altogether.",14,15
2958,237572304,"An automatically generated toxic comment by a neural chatbot may be the result of pre-training on the biased textual data -a problem which is currently unsolved completely (Gehman et al.,",3,4
2959,237572304,"The toxicity is a particular binary value associated with a text: {toxic, neutral}.",13,14
2960,237572304,"Let us assume a set of two discreet mutually exclusive styles S = {s src , s tg } which corresponds to the source toxic and target neutral When removing the toxicity from a text, we inevitably change a part of its meaning, so full content preservation cannot be reached.",25,26
2961,237572304,We use two heuristics for content preservation: not masking the toxic tokens and reranking replacement candidates with respect to their similarity to the original tokens.,11,12
2962,237572304,"Manual inspection of a random sample of the selected pairs shows that around 10% of them are invalid paraphrases, 40% are in fact both toxic or both safe, and around 50% of them are valid detoxifying paraphrases.",27,28
2963,237572304,"Its mistakes include: • replacement of toxic words by similarly looking less toxic words with different meaning (e.g. ""whore"" → ""Who's who"", ""stop behaving like fascist thugs"" → ""Stop looking at fascism"", ""taxman massive cunt , only outdone by linuxcunt himself .""",7,8
2964,237572304,"Its mistakes include: • replacement of toxic words by similarly looking less toxic words with different meaning (e.g. ""whore"" → ""Who's who"", ""stop behaving like fascist thugs"" → ""Stop looking at fascism"", ""taxman massive cunt , only outdone by linuxcunt himself .""",13,14
2965,237572304,"replacement of sentence meaning (""the election was yours to lose"" → ""the election is to be won"", ""this crap hole institute run by motherfuckers and bastards"" → ""a deloitte institute for mothers and their children"") • Avoiding the toxic or difficult part, for example ""why we gotta have this miscegenation crap ?""",49,50
2966,237572304,"In some cases, however, ParaGeDi masks or rephrases the toxic part of the message, while still preserving the general meaning, for example ""start there first you idiot!""",11,12
2967,237572304,"The DLSM and Template-based DRG models often preserve the meaning by just preserving the toxic words, so their total success rate is low.",16,17
2968,237572304,"The Mask&Infill model seems to be overfitted: it often replaces toxic words with irrelevant non-toxic words (e.g. ""crap"" → ""compelling"") that the model apparently considers to be the ""markers"" of the non-toxic style.",11,12
2969,237572304,"The Mask&Infill model seems to be overfitted: it often replaces toxic words with irrelevant non-toxic words (e.g. ""crap"" → ""compelling"") that the model apparently considers to be the ""markers"" of the non-toxic style.",17,18
2970,237572304,"The Mask&Infill model seems to be overfitted: it often replaces toxic words with irrelevant non-toxic words (e.g. ""crap"" → ""compelling"") that the model apparently considers to be the ""markers"" of the non-toxic style.",44,45
2971,237572304,"Typical mistakes of both ParaGeDi and Cond-BERT can be attributed mostly to the insufficiency of semantic understanding: they often replace toxic words with semantically related words of different (often opposite) meaning, or simply with similarly looking words.",23,24
2972,237572304,"However, they can be used to suggest detoxification options to human writers, or to detoxify the output of chit-chat bots where the cost of producing an inarticulate utterance is considerably less than the cost of producing a toxic one.",41,42
2973,236460327,"The goal of the task is to identify the most toxic fragments of a given sentence, which is a binary sequence tagging problem.",10,11
2974,236460327,"Until recently, the majority of research on toxicity focused on classifying entire user messages as toxic or safe.",16,17
2975,236460327,"If we know which words of a sentence are toxic, it is easier to ""fix"" this sentence by removing or replacing them with non-toxic synonyms.",9,10
2976,236460327,"If we know which words of a sentence are toxic, it is easier to ""fix"" this sentence by removing or replacing them with non-toxic synonyms.",28,29
2977,236460327,"This year the SemEval hosts the first competition on toxic spans detection, namely, SemEval-2021 Task 5 1 (Pavlopoulos et al.,",9,10
2978,236460327,"Besides that, we train a model for sentence classification on the Jigsaw dataset of toxic comments and use the information from this model to detect toxicity at the subsentential level.",15,16
2979,236460327,"In our experiments, we test the hypothesis that the sentence-level toxicity labeling can be used for a sequence labeler that recognizes toxic spans in text.",24,25
2980,236460327,"Moreover, we show that using sentence-level labels can dramatically improve toxic span prediction when the dataset with token-level labels is small.",13,14
2981,236460327,"The task The training data of the task comprises 7,940 English comments with character-level annotations of toxic spans.",18,19
2982,236460327,"The spans labeled as toxic often contain rude words: ""Because he's a moron and a bigot.",4,5
2983,236460327,toxic spans are underlined).,0,1
2984,236460327,"Other toxic spans consist of words that become toxic in context: ""Section 160 should also be amended to include sexual acts with animals not involving penetration"".",1,2
2985,236460327,"Other toxic spans consist of words that become toxic in context: ""Section 160 should also be amended to include sexual acts with animals not involving penetration"".",8,9
2986,236460327,Borders of some toxic spans fall in the middle of a word; we treat such cases as markup errors.,3,4
2987,236460327,Pre-training for toxic span detection Here we give the motivation behind our models and describe their architecture and training setup.,4,5
2988,236460327,"Motivation Our intuition is that the toxicity is often lexicallybased, i.e., there are certain words that are considered offensive and make the whole sentence toxic.",26,27
2989,236460327,"In this case, we expect that as we add extra data to our toxic span dataset, after some point, the vocabulary of toxic words in it will saturate and stop increasing.",14,15
2990,236460327,"In this case, we expect that as we add extra data to our toxic span dataset, after some point, the vocabulary of toxic words in it will saturate and stop increasing.",25,26
2991,236460327,"However, Figure 1 shows that the size of the toxic vocabulary linearly depends on the dataset size, which suggests that its size is insufficient for the task.",10,11
2992,236460327,"To mitigate the lack of data, we leverage the additional dataset with toxicity information, namely, the Jigsaw toxic comments dataset 2 which features 140,000 user utterances labeled as toxic or safe.",20,21
2993,236460327,"To mitigate the lack of data, we leverage the additional dataset with toxicity information, namely, the Jigsaw toxic comments dataset 2 which features 140,000 user utterances labeled as toxic or safe.",31,32
2994,236460327,We fine-tune roberta-base model on the toxic spans training set for sequence labeling task (further denoted as RoBERTa tagger).,10,11
2995,236460327,We further improve it by providing it with the additional training signal from the Jigsaw toxic comments dataset.,15,16
2996,236460327,We apply the RoBERTa tagger to predict the toxic spans in the Jigsaw dataset.,8,9
2997,236460327,"The first is to fine-tune the model on the Jigsaw dataset for the sentence classification task, and then on the toxic spans dataset -this model is referred to as RoBERTa classifier + tagger.",23,24
2998,236460327,We fine-tune this model both on Jigsaw and toxic spans datasets.,10,11
2999,236460327,"Working with spans To reformulate toxic spans detection as a token classification problem, we label a token as toxic if at least one of its characters is toxic.",5,6
3000,236460327,"Working with spans To reformulate toxic spans detection as a token classification problem, we label a token as toxic if at least one of its characters is toxic.",19,20
3001,236460327,"Working with spans To reformulate toxic spans detection as a token classification problem, we label a token as toxic if at least one of its characters is toxic.",28,29
3002,236460327,"Consider a token to be toxic if its toxicity score is higher than the threshold, do not force the labels of tokens within a word to agree with each other.",5,6
3003,236460327,Consider a word to be toxic if the aggregated toxicity score of all its tokens is higher than the threshold.,5,6
3004,236460327,"In both methods, we label a space character as toxic only if the characters both to the right and to the left of it are toxic.",10,11
3005,236460327,"In both methods, we label a space character as toxic only if the characters both to the right and to the left of it are toxic.",26,27
3006,236460327,Word-based LogReg This is a vocabulary-based method: we label words as toxic if they appear in our toxic vocabulary.,16,17
3007,236460327,Word-based LogReg This is a vocabulary-based method: we label words as toxic if they appear in our toxic vocabulary.,22,23
3008,236460327,"We create a set of toxic and safe phrases, where toxic phrases are toxic spans from our data and safe phrases are sentences from our data with removed toxic spans.",5,6
3009,236460327,"We create a set of toxic and safe phrases, where toxic phrases are toxic spans from our data and safe phrases are sentences from our data with removed toxic spans.",11,12
3010,236460327,"We create a set of toxic and safe phrases, where toxic phrases are toxic spans from our data and safe phrases are sentences from our data with removed toxic spans.",14,15
3011,236460327,"We create a set of toxic and safe phrases, where toxic phrases are toxic spans from our data and safe phrases are sentences from our data with removed toxic spans.",29,30
3012,236460327,We then train a binary logistic regression classifier of toxic and safe phrases using words as features.,9,10
3013,236460327,We consider words with weights greater than a threshold as toxic.,10,11
3014,236460327,"The pseudo-label model is first fine-tuned on the original toxic spans dataset, and then on the self-labeled Jigsaw dataset, whereas the RoBERTa classifier + tagger and tagging classifier are first fine-tuned on the Jigsaw dataset (as classifiers), and then on the toxic spans dataset (as taggers).",13,14
3015,236460327,"The pseudo-label model is first fine-tuned on the original toxic spans dataset, and then on the self-labeled Jigsaw dataset, whereas the RoBERTa classifier + tagger and tagging classifier are first fine-tuned on the Jigsaw dataset (as classifiers), and then on the toxic spans dataset (as taggers).",54,55
3016,236460327,"Apparently, other models fail to learn even the toxic vocabulary because their word representations are not informative enough.",9,10
3017,236460327,"Efficiency of pre-training To understand the effect of the use of additional sentence-labeled data, we compare the performance of RoBERTa tagger (a model which uses only the toxic span dataset) and tagging classifier (a model which uses sentence-labeled Jigsaw data in addition to the toxic span dataset) models trained on subsets of the data of different sizes.",33,34
3018,236460327,"Efficiency of pre-training To understand the effect of the use of additional sentence-labeled data, we compare the performance of RoBERTa tagger (a model which uses only the toxic span dataset) and tagging classifier (a model which uses sentence-labeled Jigsaw data in addition to the toxic span dataset) models trained on subsets of the data of different sizes.",54,55
3019,236460327,"It is not obvious why human annotators label them as toxic in some cases, and as non-toxic in other cases.",10,11
3020,236460327,"It is not obvious why human annotators label them as toxic in some cases, and as non-toxic in other cases.",19,20
3021,236460327, or a large chunk of it as toxic.,8,9
3022,236460327,"In future work, it Conclusions We present a number of models for the detection of toxic spans within toxic sentences.",16,17
3023,236460327,"In future work, it Conclusions We present a number of models for the detection of toxic spans within toxic sentences.",19,20
3024,236460327,All models are RoBERTa language models fine-tuned on the data with character-level labeling of toxic spans.,18,19
3025,232168787,"Not all topics are equally ""flammable"" in terms of toxicity: a calm discussion of turtles or fishing less often fuels inappropriate toxic dialogues than a discussion of politics or sexual minorities.",24,25
3026,232168787,We define a set of sensitive topics that can yield inappropriate and toxic messages and describe the methodology of collecting and labeling a dataset for appropriateness.,12,13
3027,232168787,"This is different from toxicity in two respects: (i) inappropriateness is topicrelated, and (ii) inappropriate message is not toxic but still unacceptable.",24,25
3028,232168787,This problem is even more important for developers of chatbots trained on a large number of usergenerated (and potentially toxic) texts.,20,21
3029,232168787,"The Wikipedia Toxic comment datasets by Jigsaw (Jigsaw, 2018 (Jigsaw, , 2019 (Jigsaw, , 2020) ) are the largest English toxicity datasets available to date operate with multiple types of toxicity (toxic, obscene, threat, insult, identity hate, etc).",39,40
3030,232168787,"Insults do not necessarily have a topic, but there certainly exist toxic topics, such as sexism, racism, xenophobia.",12,13
3031,232168787,"Besides directly classifying toxic messages for a topic, the notion of the topic in toxicity is also indirectly used to collect the data: Zampieri et al. (",3,4
3032,232168787,"Similarly, Hessel and Lee (2019) use topics to find controversial (potentially toxic) discussions.",15,16
3033,232168787,"It contains toxic topics, but they are mixed with other parameters of toxicity (e.g. direction or severity).",2,3
3034,232168787,"In some countries, such as France, it is a criminal offense to deny the Armenian Genocide during World War I. 4 Note, however, that no offensive or toxic words were employed.",31,32
3035,232168787,"We fine-tune ruBERT model (Kuratov and Arkhipov, 2019) on a concatenation of two Russian Language Toxic Comments datasets released on Kaggle (Kaggle, 2019 (Kaggle, , 2020)) .We filter out sentences which were classified as toxic with the confidence greater than 0.75.",45,46
3036,232168787,"Therefore, we make sure that messages which can be automatically recognized as toxic are not included in this dataset.",13,14
3037,235097683,"We measured the gender bias in pre-trained models based on a ""representative"" Wikipedia and Book Corpus in English and compared it to models that had been fine-tuned with various smaller corpora including the General Language Understanding Evaluation (GLUE) benchmarks and two collections of toxic speech, Rt-Gender and IdentityToxic.",51,52
3038,233219395,2020) have been proposed to mitigate toxic LM generations.,7,8
3039,233219395,"2020) , they inevitably contain so-called toxic examples: undesirable language such as expletives, slurs, or other offensive and threatening speech.",9,10
3040,233219395,"When trained on such data, LMs inevitably learn to generate toxic text (Henderson et al.,",11,12
3041,233219395,"To address this issue, recent work has turned towards detoxifying LMs: reducing toxic generations without affecting perplexity or generation quality on nontoxic inputs.",14,15
3042,233219395,We identify that these failures are due to the use of biased toxic classification data.,12,13
3043,233219395,"In particular, toxicity datasets often contain spurious correlations between the toxic label and the presence of AAE and minority identity mentions (Sap et al.,",11,12
3044,233219395,These correlations cause detoxification techniques to steer generations away from AAE and minority identity mentions because they often consider these aspects of language to be toxic.,25,26
3045,233219395,WAE Methods and Experimental Setup The goal of detoxification is to mitigate the frequency of toxic generations (also called hate speech or offensive language) without affecting an LM's utility or generation quality on nontoxic inputs.,15,16
3046,233219395,"Filtering Finally, we consider output filtering, where we generate a fixed number of times (we use 10) from the LM and return the least toxic generation according to a toxicity classifier.",28,29
3047,233219395,"1 We remove examples where between 10% and 50% of the annotations are the toxic label (i.e., examples with low inter-annotator agreement).",16,17
3048,233219395,3  White-Aligned English Perplexity We first evaluate the perplexity on White-Aligned English (WAE) text that is either toxic or nontoxic.,24,25
3049,233219395,"4  The detoxification techniques are effective at removing toxicity: the perplexity on toxic data increases substantially (Figure 1 , toxic evaluation set).",14,15
3050,233219395,"4  The detoxification techniques are effective at removing toxicity: the perplexity on toxic data increases substantially (Figure 1 , toxic evaluation set).",22,23
3051,233219395,6 Filtering performs poorly because GPT-2 rarely generates nontoxic continuations of toxic prompts.,11,12
3052,233219395,"First, note that all detoxification techniques make use of labeled toxic/nontoxic data.",11,12
3053,233219395,"Unfortunately, there are spurious correlations between the toxic label and the presence of AAE and minority identity mentions (Sap et al.,",8,9
3054,233219395,"Annotation bias occurs because crowdworkers are often unfamiliar with AAE and consequently misjudge it as toxic (Sap et al.,",15,16
3055,233219395,"Sampling bias occurs because many toxic comments are directed towards marginalized groups (RWJF, 2017) .",5,6
3056,233219395,"The result of these two biases is that text which contains AAE and minority identity mentions is labeled as toxic at disproportionately high rates (Sap et al.,",19,20
3057,233219395,"Similarly, the discriminators used by PPLM, GeDi, and Filtering will guide the generated text away from AAE and identity mentions because the discriminators typically consider such text as toxic (Dixon et al.,",31,32
3058,220665900,"2018) , toxic comments (Georgakopoulos et al.,",3,4
3059,237347097,"text field) Respondents said that language models can exacerbate transphobia ""by incorrectly flagging non-toxic content from trans persons as toxic at higher rates, or by not recognizing transphobic comments as toxic"".",17,18
3060,237347097,"text field) Respondents said that language models can exacerbate transphobia ""by incorrectly flagging non-toxic content from trans persons as toxic at higher rates, or by not recognizing transphobic comments as toxic"".",23,24
3061,237347097,"text field) Respondents said that language models can exacerbate transphobia ""by incorrectly flagging non-toxic content from trans persons as toxic at higher rates, or by not recognizing transphobic comments as toxic"".",35,36
3062,235097511,"These anti-social behaviors intensify in a massive way during crisis cases, creating a toxic impact on society, either purposely or accidentally.",16,17
3063,248811664,"Large pre-trained neural language models have supported the effectiveness of many NLP tasks, yet are still prone to generating toxic language hindering the safety of their use.",22,23
3064,248811664,"Introduction Pre-trained neural language models are prone to generating toxic language, hindering the ability to use them safely (Gehman et al.,",11,12
3065,248811664,"2019) , the selection of negative, non-toxic examples for modeling has been somewhat arbitrary. †",10,11
3066,248811664,"Though the definitions of toxicity and empathy vary across literature, we observe an opposition between the concepts in terms of response appropriateness and intent toward others, which is the basis of the research question driving this work: is there an opposing relationship between toxic and empathetic language that can be leveraged to better model these phenomena?",46,47
3067,248811664,Exploiting this relationship could result in more robust and/or efficient models for mitigating toxic degeneration.,13,14
3068,248811664,"In particular, we expect the cognitive types of empathy to be more beneficial for mitigating the largely cognitive aspects of toxic behavior, since emotional empathy may reinforce toxic feelings such as hostility toward out-groups (Breithaupt, 2012) .",21,22
3069,248811664,"In particular, we expect the cognitive types of empathy to be more beneficial for mitigating the largely cognitive aspects of toxic behavior, since emotional empathy may reinforce toxic feelings such as hostility toward out-groups (Breithaupt, 2012) .",29,30
3070,248811664,We use the predictions of a language model trained on empathetic data to alter the output of a large pretrained language model and demonstrate that using only a small volume of empathetic data can reduce toxicity more than a model simply trained on a large volume of non-toxic text.,49,50
3071,248811664,"2020 ) introduced RealToxici-tyPrompts, a test-bed for toxic language generation.",12,13
3072,248811664,They gathered a range of toxic sentences and split them in half.,5,6
3073,248811664,Models tested with this data must continue the sentence in a non-toxic way.,13,14
3074,248811664,They test recent LMs (some mentioned in the following subsection on controllable generation) finding all to be prone to toxic degeneration and suggest that choosing less toxic pretraining data may help.,21,22
3075,248811664,They test recent LMs (some mentioned in the following subsection on controllable generation) finding all to be prone to toxic degeneration and suggest that choosing less toxic pretraining data may help.,28,29
3076,248811664,"SemEval-2021 hosted a task on toxic span detection, where one must identify the subsequence of a text that is responsible for the toxicity label (Pavlopoulos et al.,",5,6
3077,248811664,"The Jigsaw data classes were those originally used to train the models in the Perspective API, which has been used by several recent works to automatically evaluate toxic language (Jigsaw, 2021a) .",28,29
3078,248811664,These are not the only classes that exist in toxic language research.,9,10
3079,248811664,We expect the cognitive aspects of empathy to be more useful for toxic language mitigation because of side-taking effects.,12,13
3080,248811664,"A toxic response, ""you are stupid for trying that,"" may be perceived as toxic and inappropriate, while ""it sounds like you're really trying hard and doing your best,"" may be perceived as more appropriate and better understanding the user.",1,2
3081,248811664,"A toxic response, ""you are stupid for trying that,"" may be perceived as toxic and inappropriate, while ""it sounds like you're really trying hard and doing your best,"" may be perceived as more appropriate and better understanding the user.",17,18
3082,248811664,"By our intuition, a stronger negative correlation should generally correspond to less toxic output.",13,14
3083,248811664,"In our case, we want to avoid generating a text, x t , from the set of toxic texts, T , so we use non-toxic text from the complement set x nt ∈ T ′ = N T .",19,20
3084,248811664,"In our case, we want to avoid generating a text, x t , from the set of toxic texts, T , so we use non-toxic text from the complement set x nt ∈ T ′ = N T .",29,30
3085,248811664,"However, N T contains many types of non-toxic text.",10,11
3086,248811664,"We hypothesize that a small subset with specific qualities, E ⊂ N T , will be more effective in generating non-toxic text than any random sample R ⊂ N T , and that empathetic text belongs to this subset E. We use the set of ~1.4 million comments that were not labeled as toxic by any annotators as our non-toxic set.",23,24
3087,248811664,"We hypothesize that a small subset with specific qualities, E ⊂ N T , will be more effective in generating non-toxic text than any random sample R ⊂ N T , and that empathetic text belongs to this subset E. We use the set of ~1.4 million comments that were not labeled as toxic by any annotators as our non-toxic set.",56,57
3088,248811664,"We hypothesize that a small subset with specific qualities, E ⊂ N T , will be more effective in generating non-toxic text than any random sample R ⊂ N T , and that empathetic text belongs to this subset E. We use the set of ~1.4 million comments that were not labeled as toxic by any annotators as our non-toxic set.",64,65
3089,248811664,This classifier is used to assign class probabilities to our non-toxic set.,12,13
3090,248811664,"2020) 's annotated dataset reflected by the distributions of the results on the non-toxic data (Table 1 ), which we intuit is due to greater difficulty annotating weak versus strong than present versus absent empathetic communication.",16,17
3091,248811664,These were used to fine-tune the non-toxic expert in DExperts and compared to fine-tuning the nontoxic expert on random samples of equal size.,10,11
3092,248811664,"To examine this, we create subsets of the empathy labeled non-toxic data that each maximizes one of the empathetic aspects.",13,14
3093,248811664,"In our case, we find that cognitive empathy data is effective at minimizing toxic generations.",14,15
3094,248811664,"2021) had originally experimented with reducing the size of the toxic anti-expert, but not the expert model.",11,12
3095,248811664,"4 With permutation test on both average max toxicity and 0% 20% 40% 60% 80% 100%  using their large model with 2.3m examples (compared to our 7.5k), is 3.4% absolute reduction in toxic probability (26% relative).",44,45
3096,248811664,Annotators mentioned that prompts coming from news sources are difficult to annotate because they describe toxic events or explain what others have said using toxic language while not adding additional toxic content.,15,16
3097,248811664,Annotators mentioned that prompts coming from news sources are difficult to annotate because they describe toxic events or explain what others have said using toxic language while not adding additional toxic content.,24,25
3098,248811664,Annotators mentioned that prompts coming from news sources are difficult to annotate because they describe toxic events or explain what others have said using toxic language while not adding additional toxic content.,30,31
3099,248811664,These instances likely make generations seem less toxic to humans than they would to a model.,7,8
3100,248811664,"Overall, our model performs much better than the baseline in terms of both generating less toxic and more fluent content with our best model showing higher fluency, even with 20 points higher perplexity.",16,17
3101,248811664,"For examples of prompt continuations where our model was found to be less toxic by annotators than the baseline, see Table 4 .",13,14
3102,248811664,"Empathy and Toxicity Types: For a more in-depth analysis, we examine each type of toxic language provided by the Perspective API and how the toxicity varies with fine-tuning data volume.",18,19
3103,248811664,This leads us to ask: is the reason our models are less toxic because they generate fewer words?,13,14
3104,248811664,"The result in Figure 3 shows that although the results are closer for some of the shortest lengths, our model is consistently less toxic across generation lengths with the exception of generations of length one.",24,25
3105,248811664,"We also notice that the average toxicity de- Figure 2 : Each plot shows the toxicity probabilities of a specific type of toxic language (e.g., profanity) as a function of the fine-tuning data size, for each of the models fine-tuned on the sets maximizing emotional reactions (ER), explorations (EX), and interpretations (IP), as well as on sets of random samples (R).",22,23
3106,248811664,"While this may initially seem unintuitive, we attribute this to the fact that the prompt is supposed to cause a model to generate toxic text.",24,25
3107,248811664,"The farther the models move away from the prompt, the less toxic the output is.",12,13
3108,248811664,Discussion and Limitations Toxicity detection or non-toxic generation models can be deployed for various end-tasks in which there exist expectations of their behavior.,8,9
3109,248811664,"Additionally, there are many possible appropriately non-toxic continuations for a given prompt and by controlling the generation process we will inevitably generate something that differs from the ground truth making this a questionable metric of quality (Hashimoto et al.,",9,10
3110,248811664,What is considered toxic varies across individuals.,3,4
3111,248811664,"2022) examined race, gender, and political leaning in annotators from the USA, finding that one's perception of toxic language does indeed vary with each of these variables.",22,23
3112,248811664,"Our improvements to mitigation of toxic degeneration could be better understood and further expanded upon in a conversational application where empathy is important, such as counseling or online mental health support (Sharma et al.,",5,6
3113,248811664,"Conclusions In this work, we investigated empathy and toxicity, showing that the relationship between the two can be leveraged for mitigating toxic degeneration.",23,24
3114,248811664,We find that we can dramatically reduce the size of the data used to fine-tune the non-toxic expert model while at the same time making a significant improvement over the state-of-the art in terms of the probability of toxic generation.,20,21
3115,248811664,We find that we can dramatically reduce the size of the data used to fine-tune the non-toxic expert model while at the same time making a significant improvement over the state-of-the art in terms of the probability of toxic generation.,46,47
3116,248811664,Our human evaluation showed that our best model is more fluent and less toxic than a model fine-tuned on a random sample.,13,14
3117,222177236,"Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias.",1,2
3118,222177236,"Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias.",20,21
3119,222177236,"In this work, we propose a framework aimed at fortifying existing toxic speech detectors without a large labeled corpus of veiled toxicity.",12,13
3120,222177236,"We augment the toxic speech detector's training data with these discovered offensive examples, thereby making it more robust to veiled toxicity while preserving its utility in detecting overt toxicity.",3,4
3121,222177236,"While great efforts have been made to detect and prevent the spread of overt trolling, hate speech, abusive language, and toxic comments (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018) , they often build upon lexicon-based approaches (Waseem and Hovy, 2016; Davidson et al.,",23,24
3122,222177236,"In this work, we focus on disguised toxic language that is often undetected by existing tools.",8,9
3123,222177236,"To the best of our knowledge, our work is the first in making toxic speech detectors robust against veiled toxicity with almost no annotated data.",14,15
3124,222177236,"Identifying Veiled Toxicity A typical toxicity classifier C might fail to identify veiled offenses because they are not well represented among toxic examples in its training data D. Moreover, the non-toxic portion of D might be polluted with (mislabeled) disguised offenses.",21,22
3125,222177236,"Identifying Veiled Toxicity A typical toxicity classifier C might fail to identify veiled offenses because they are not well represented among toxic examples in its training data D. Moreover, the non-toxic portion of D might be polluted with (mislabeled) disguised offenses.",33,34
3126,222177236,"At inference time, C might thus mislabel, for example, microaggressions as well as adversarial attacks deliberately avoiding known toxic lexicons.",21,22
3127,222177236,"9 3 Experiments Setup We use a popular toxic language detection tool, Perspective API by Jigsaw and Google, as the compromised classifier C. It builds upon a convolutional neural network with pretrained word embeddings and proprietary large labeled data.",8,9
3128,222177236,We pick the least toxic m posts as our veiled offensive language set so that 1 m m i=1 tox(x (i) ) = tox general .,4,5
3129,222177236,The extracted veiled offenses are equally non-toxic as some random general-domain reddits according to Perspective API.,8,9
3130,222177236,"We use 2K in our training set D (as discussed in §2.1 they are (mis)labeled as non-toxic), 1K for the test set, and reserve 100 for the probing set P. It is worth noting that Perspective API can misclassify toxic inputs for several reasons.",22,23
3131,222177236,"We use 2K in our training set D (as discussed in §2.1 they are (mis)labeled as non-toxic), 1K for the test set, and reserve 100 for the probing set P. It is worth noting that Perspective API can misclassify toxic inputs for several reasons.",49,50
3132,222177236,We pick the least toxic m posts as our nonoffensive clean set so that 1 m m i=1 tox(x (i) ) = tox general .,4,5
3133,224461948,"2017) showed that perturbations, such as inserting dots or spaces between characters, can deceive a toxic comment classifier.",18,19
3134,248779948,Detoxification is a task of generating text in polite style while preserving meaning and fluency of the original toxic text.,18,19
3135,248779948,"y n }, where X, Y -are two sets of all possible text in styles s X , s Y respectively, we want to build a model f θ : X → Y , such that the probability p(y gen |x, s X , s Y ) of transferring the style s X of given text x (by generation y gen ) to the style s Y is maximized (where s X and s Y are toxic and non-toxic styles respectively).",82,83
3136,248779948,"y n }, where X, Y -are two sets of all possible text in styles s X , s Y respectively, we want to build a model f θ : X → Y , such that the probability p(y gen |x, s X , s Y ) of transferring the style s X of given text x (by generation y gen ) to the style s Y is maximized (where s X and s Y are toxic and non-toxic styles respectively).",86,87
3137,248779948,"Methodology We formulate the task of supervised Textual Style Transfer as a sequence-to-sequence NMT task and fine-tune multilingual language models to translate from ""toxic"" to ""polite"" language.",30,31
3138,248779948,Baselines We use two detoxification methods as baselines in this work -Delete method which simply deletes toxic words in the sentence according to the vocabulary of toxic words and CondBERT.,16,17
3139,248779948,Baselines We use two detoxification methods as baselines in this work -Delete method which simply deletes toxic words in the sentence according to the vocabulary of toxic words and CondBERT.,26,27
3140,248779948,The latter approach works in usual masked-LM setup by masking toxic words and replacing them with non-toxic ones.,12,13
3141,248779948,The latter approach works in usual masked-LM setup by masking toxic words and replacing them with non-toxic ones.,20,21
3142,248779948,The sentence is considered toxic when the classifier confidence is above 0.8.,4,5
3143,248779948,We equalize Russian and English data for training and use 10000 toxic sentences and their polite paraphrases for multilingual training in total.,11,12
3144,248779948,"We suggest that the reason for this is not a lack of data, but the model's inability to capture the pattern between toxic and non-toxic text and transfer it to another language by itself.",24,25
3145,248779948,"We suggest that the reason for this is not a lack of data, but the model's inability to capture the pattern between toxic and non-toxic text and transfer it to another language by itself.",28,29
3146,248779948,"There are several polite paraphrases for each toxic sentence in this dataset (Dementieva et al.,",7,8
3147,248779948,Leaving only one paraphrase for one source sentence we could get 6000 unique pairs of toxic sentences and their polite paraphrases.,15,16
3148,248779948,"Sentence in red is original (toxic) sentence, below are its polite paraphrases.",6,7
3149,248780527,"We collect non-toxic paraphrases for over 10,000 English toxic sentences.",4,5
3150,248780527,"We collect non-toxic paraphrases for over 10,000 English toxic sentences.",10,11
3151,248780527,"The task of rewriting toxic messages (detoxification) has already been tackled by NLP researchers (Nogueira dos Santos et al.,",4,5
3152,248780527,We aim at boosting the research in detoxification by collecting an English parallel corpus of toxic sentences and their non-toxic paraphrases.,15,16
3153,248780527,We aim at boosting the research in detoxification by collecting an English parallel corpus of toxic sentences and their non-toxic paraphrases.,21,22
3154,248780527,"In particular, we find the pairs of toxic and non-toxic sentences in the paraNMT dataset (Wieting and Gimpel, 2018) of English paraphrases and filter them using our crowdsourcing setup.",8,9
3155,248780527,"In particular, we find the pairs of toxic and non-toxic sentences in the paraNMT dataset (Wieting and Gimpel, 2018) of English paraphrases and filter them using our crowdsourcing setup.",12,13
3156,248780527,"The contributions of our work are three-fold: • We suggest a novel pipeline for collection of parallel data for the detoxification task, • We use the pipeline to collect the first parallel detoxification dataset ParaDetox (see Table 1 and Appendix A) and retrieve toxic-neutral pairs from ParaNMT corpus, 1 • Using collected data we train supervised detoxification models that yield SOTA results.",49,50
3157,248780527,"Since toxic-neutral pairs also do not occur in the wild, we follow this data collection setup with a notable difference -we replace expert validation of crowdsourced sentences with crowd validation and additionally optimize the cost.",1,2
3158,248780527,"Detoxification is usually formulated as style transfer from toxic to neutral (non-toxic) style, so it uses non-parallel datasets labeled for toxicity and considers toxic and neutral sentences as two subcorpora.",8,9
3159,248780527,"Detoxification is usually formulated as style transfer from toxic to neutral (non-toxic) style, so it uses non-parallel datasets labeled for toxicity and considers toxic and neutral sentences as two subcorpora.",14,15
3160,248780527,"Detoxification is usually formulated as style transfer from toxic to neutral (non-toxic) style, so it uses non-parallel datasets labeled for toxicity and considers toxic and neutral sentences as two subcorpora.",30,31
3161,248780527,2021) use a masked language model to perform pointwise edits of toxic sentences.,12,13
3162,248780527, offensiveness -one of the sentences is toxic and the other is neutral.,7,8
3163,248780527,We consider two scenarios: the manual rewriting of toxic sentences into neutral ones and the selection of toxic-neutral pairs from existing paraphrases.,9,10
3164,248780527,We consider two scenarios: the manual rewriting of toxic sentences into neutral ones and the selection of toxic-neutral pairs from existing paraphrases.,18,19
3165,248780527,"Pipelines Generation Pipeline To yield a parallel dataset, we first need to get toxic sentences for rewriting.",14,15
3166,248780527,"The overall data collection pipeline (see Figure 4 ) is as follows: • Select toxic sentences for rewriting, • Feed the sentences to Task 1, • Feed the paraphrases generated in Task 1 to Task 2, • Feed the paraphrases which passed Task 2 to Task 3, • Pay for paraphrases from Task 1, if they passed checks in Task 2 and Task 3, • Pay for ""I can't rewrite"" answers in Task 1 if two or more workers agreed on them.",16,17
3167,248780527,"We check this hypothesis for the toxic and neutral styles on the ParaNMT dataset (Wieting and Gimpel, 2018) .",6,7
3168,248780527,We do not need Task 1 since both toxic and neutral sentences are already available.,8,9
3169,248780527,"Analogously to the generation pipeline, we use a toxicity classifier to pre-select pairs of sentences where one sentence is toxic and the other one is neutral.",22,23
3170,248780527,"The pipeline is as follows: • Select a pair of sentences (toxic and non-toxic) from the parallel data, • Feed the toxic sentence candidate to Task 3 to make sure it is toxic, • Feed the neutral sentence candidate to Task 3 to make sure it is non-toxic, • Feed both sentences to Task 2 to check if their content matches.",13,14
3171,248780527,"The pipeline is as follows: • Select a pair of sentences (toxic and non-toxic) from the parallel data, • Feed the toxic sentence candidate to Task 3 to make sure it is toxic, • Feed the neutral sentence candidate to Task 3 to make sure it is non-toxic, • Feed both sentences to Task 2 to check if their content matches.",17,18
3172,248780527,"The pipeline is as follows: • Select a pair of sentences (toxic and non-toxic) from the parallel data, • Feed the toxic sentence candidate to Task 3 to make sure it is toxic, • Feed the neutral sentence candidate to Task 3 to make sure it is non-toxic, • Feed both sentences to Task 2 to check if their content matches.",27,28
3173,248780527,"The pipeline is as follows: • Select a pair of sentences (toxic and non-toxic) from the parallel data, • Feed the toxic sentence candidate to Task 3 to make sure it is toxic, • Feed the neutral sentence candidate to Task 3 to make sure it is non-toxic, • Feed both sentences to Task 2 to check if their content matches.",38,39
3174,248780527,"The pipeline is as follows: • Select a pair of sentences (toxic and non-toxic) from the parallel data, • Feed the toxic sentence candidate to Task 3 to make sure it is toxic, • Feed the neutral sentence candidate to Task 3 to make sure it is non-toxic, • Feed both sentences to Task 2 to check if their content matches.",56,57
3175,248780527,"Crowdsourcing Settings Preprocessing To pre-select toxic sentences, we need a toxicity classifier.",7,8
3176,248780527,We consider a sentence toxic if the classifier confidence is above 0.8.,4,5
3177,248780527,"Based on a manual validation, sentences with lower similarity are often not exact paraphrases, and too-similar sentences are either both toxic or both non-toxic.",24,25
3178,248780527,"Based on a manual validation, sentences with lower similarity are often not exact paraphrases, and too-similar sentences are either both toxic or both non-toxic.",29,30
3179,248780527,"ParaDetox: Generated Paraphrases We fetched toxic sentences from three sources: Jigsaw dataset of toxic sentences (Jigsaw, 2018) , Reddit and Twitter datasets used by Nogueira dos Santos et al. (",6,7
3180,248780527,"ParaDetox: Generated Paraphrases We fetched toxic sentences from three sources: Jigsaw dataset of toxic sentences (Jigsaw, 2018) , Reddit and Twitter datasets used by Nogueira dos Santos et al. (",15,16
3181,248780527,"We selected 7,000 toxic sentences from each source and gave each of the sentences for paraphrasing to 3 workers.",3,4
3182,248780527,"We get paraphrases for 12,610 toxic sentences (on average 1.66 paraphrases per sentence), 20,437 paraphrases total.",5,6
3183,248780527,"Thus, neutral sentences are less similar to the toxic sentences, and the edits are more diverse, which makes it more similar to Jigsaw dataset (see Table 3 ).",9,10
3184,248780527,We manually validate the test set to exclude the appearance of non-detoxifiable sentences or sentences which stayed toxic after rewriting (we need to verify that since the corpus was generated via crowdsourcing only).,19,20
3185,248780527,ParaDetox-unique -a subset of ParaDetox where each toxic sentence has only one paraphrase (selected randomly). •,9,10
3186,248780527,"ParaDetox-1000 -1,000 samples from the crowdsourced dataset (distributed evenly across data sources, each toxic sample has multiple non-toxic variants). •",15,16
3187,248780527,"ParaDetox-1000 -1,000 samples from the crowdsourced dataset (distributed evenly across data sources, each toxic sample has multiple non-toxic variants). •",21,22
3188,248780527,"It contains almost 12,000 user-generated toxic sentences manually rewritten by crowd workers.",7,8
3189,248780527,"The reason is that the toxic part of our corpus consists of real toxic sentences fetched on the Internet, whereas their non-toxic counterparts are ""translations"" performed by crowd workers.",5,6
3190,248780527,"The reason is that the toxic part of our corpus consists of real toxic sentences fetched on the Internet, whereas their non-toxic counterparts are ""translations"" performed by crowd workers.",13,14
3191,248780527,"The reason is that the toxic part of our corpus consists of real toxic sentences fetched on the Internet, whereas their non-toxic counterparts are ""translations"" performed by crowd workers.",24,25
3192,248780527,The manually detoxified texts are different from the original non-toxic texts written by Internet users from scratch.,11,12
3193,248780527,how can students of colour be expected to learn in such a toxic environment of white supremacy?,12,13
3194,232035490,"In SemEval-2021 Task-5 Toxic Spans Detection, the focus is on detecting toxic spans within English passages.",12,13
3195,232035490,Finding which spans make a comment or document toxic in nature is crucial in explaining the reasons behind their toxicity.,8,9
3196,232035490,Each text is crowd-annotated with character offsets that make the text toxic.,13,14
3197,232035490,"Background Before the advent in research pertaining to toxic texts, Warner and Hirschberg (2012) modeled hate speech as a word sense disambiguation problem where SVM was used for classification of data.",8,9
3198,232035490,"Recently, however, toxic text detection has garnered a lot of attention (Nobata et al.,",4,5
3199,232035490,"The labels used are: special token, non-toxic word, and toxic word.",10,11
3200,232035490,"The labels used are: special token, non-toxic word, and toxic word.",14,15
3201,232035490,A word with containing any toxic offset is marked as toxic during training.,5,6
3202,232035490,A word with containing any toxic offset is marked as toxic during training.,10,11
3203,232035490,BERT-based Token Classification Models These models comprise a BERT-based model and a classification layer over each final token embedding which predicts whether a token is toxic or not.,29,30
3204,232035490,"Based on these classifications, we add the offsets for those tokens (not words) which are marked as toxic by the model.",20,21
3205,232035490,"Since the Toxic Spans text can have multiple toxic spans, we take different contiguous spans from the given offsets, and make several 'samples' out of the example.",8,9
3206,232035490,All spans with score above threshold are considered to be toxic spans.,10,11
3207,232035490,"Two toxic spans in text are equally important to predict, and thus, should not be shown at different times during training.",1,2
3208,232035490,The predicted offsets taken from the predicted spans are considered to be toxic.,12,13
3209,232035490,A threshold is used to classify a word as toxic on the predicted toxic word probability.,9,10
3210,232035490,A threshold is used to classify a word as toxic on the predicted toxic word probability.,13,14
3211,232035490,"We use a batch size of 4, train for 3 epochs, 5 Our code can be found at: https://github.com/ gchhablani/toxic-spans-detection.",24,25
3212,232035490,"For Token Classification, we add a label for the [CLS] token if the percentage of toxic offsets in text is greater than 30% in order to provide a proxy text classification objective for the system.",18,19
3213,232035490,This means that the individual checkpoints are predicting some extra offsets to be toxic.,13,14
3214,232035490,"For the Token Classification model, the targets are softmax outputs of toxicity logits of those tokens which the model predicts to be toxic, with a score greater than 0.5.",23,24
3215,232035490,"Sometimes, random words like 'go' and 'on' are selected to be toxic, which means that these types of prepositions and verbs can be removed by exact matching in the string, unless they form parts of larger spans. •",16,17
3216,232035490,"For example, it is not clear whether the word 'ignorant' should be considered to be toxic.",18,19
3217,232035490,"The models, based on other examples, predict 'ignorant' to be toxic, but it is not present in the ground spans.",14,15
3218,232035490,"This means that finding the toxic spans is not a trivial task for humans, and annotation can not be performed easily by crowd-workers. •",5,6
3219,232035490,"In some cases, one of the occurrences of the word 'ignorant' is considered to be toxic, while the other is predicted to be benign.",18,19
3220,232035490,"The first instance of 'ignorant' does not seem to be as toxic as the second instance and therefore, more analysis needs to be done to determine the 'degree' of toxicity of the spans.",13,14
3221,7877159,"Furthermore, this catalyst is toxic and pyrophoric, so that it is less suitable for large-scale application.",5,6
3222,195218693,"To demonstrate the effectiveness our technique, we apply it to two tasks: (1) mitigating unintended bias in text classifiers by neutralizing identity terms; (2) improving classifier performance in a scarce data setting by forcing the model to focus on toxic terms.",46,47
3223,195218693,"Some examples include sentiment analysis models weighing negatively for inputs containing identity terms such as ""jew"" and ""black"", and hate speech classifiers leaning to predict any sentence containing ""islam"" as toxic (Waseem and Hovy, 2016) .",37,38
3224,195218693,"As shown in our experiments, by setting a positive target attribution for known toxic words 1 , one can improve the performance of a toxicity classifier in a scarce data regime.",14,15
3225,195218693,"We validate our approach on the Wikipedia toxic comments dataset (Wulczyn et al.,",7,8
3226,195218693,"Lastly, by setting an attribution target of 1 on toxic words, a classifier trained with our objective function achieves better performance when only a subset of the data is present.",10,11
3227,195218693,1 Full list of identity terms and toxic terms used as priors can be found in supplemental material.,7,8
3228,195218693,Please note the toxic terms are not censored.,3,4
3229,195218693,"For example, consider a model that tends to predict every sentence containing ""gay"" as toxic in a comment moderation system.",17,18
3230,195218693,We illustrate the effectiveness of our method by applying it to a toxic comment classification problem.,12,13
3231,195218693,"Then, using the same technique, we show how to set target attribution for toxic words to improve classifier performance in a scarce data setting.",15,16
3232,195218693,"First, we tackle the problem of unintended bias in toxic comment classification (Dixon et al.,",10,11
3233,195218693,"For our experiments, we aim to mitigate the issue of neutral sentences with identity terms being classified as toxic for a given a set of identity terms.",19,20
3234,195218693,"Second, we force the model to focus on a list of human-selected toxic terms under scarce data scenario to increase model performance.",15,16
3235,195218693,The ratio of positive (toxic) labels in the training set is 9.7%.,5,6
3236,195218693,"Since the output of the binary classification can be reduced to a single scalar output by taking the posterior of the positive (toxic) class, the prior is only added to the positive class in equation 3 .",23,24
3237,195218693,"For data scarcity experiments, we set k to 1 and I to the set of toxic terms to force the model to make high attributions to these terms.",16,17
3238,195218693,"homosexual"" also moved to more neutral terms that shouldn't play a role in deciding if the comment is toxic or not.",20,21
3239,195218693,Results on Incorporating Priors in Different Training Sizes We now demonstrate our approach on encouraging higher attributions on toxic words to increase model performance in scarce data regime.,18,19
3240,195218693,"To directly validate the effectiveness of prior loss on attributions, we first show that the attribution of the toxic words have higher values for our method across different data ratios compared to the baseline in Table 8 .",19,20
3241,195218693,"However, the impact of our approach diminishes after adding more data, since the model starts to learn to focus on toxic words itself for predicting toxicity without the need for prior injection.",22,23
3242,195218693,"We can also see that both the baseline and our method start to catch up with the rule based approach, where we give positive prediction if the toxic word is in the sentence, and eventually outperform it.",28,29
3243,195218693,2018) highlight the bias in toxic comment classifier models originating from the dataset.,6,7
3244,195218693,We apply this technique to model fairness in toxic comment classification.,8,9
3245,195218693,"Finally, it would be worthwhile to invest in a technique to extract problematic terms from the model automatically rather than providing prescribed identity or toxic terms.",25,26
3246,174799410,"Munger (2017) developed a simple, but effective, computational intervention for the use of toxic language (the n-word), where a human-looking bot account would reply with a fixed comment about the harm such language caused and an appeal to empathy, leading to long-term behavior change in the offenders.",17,18
3247,174799410,"Recent work has shown that it is possible to predict whether a conversation will become toxic on Wikipedia (Zhang et al.,",15,16
3248,236460291,"Given a piece of short text (originating from social media or wikipedia), the goal is to determine if the content is toxic, i.e., a binary classification task.",24,25
3249,231719026,"The items were rated individually; the human evaluators did not know beforehand that different versions of the same sentences were repeated, nor translate/meteor_score.html 14 To avoid exposing the raters to overly toxic content, blatant examples were filtered using a keyword list.",35,36
3250,233189545,"2018) , and toxic content detection Li et al. (",4,5
3251,231847004,"Studies have shown that African American English is more likely to be labeled as toxic by both annotators and systems due to differences in dialect (Sap et al.,",14,15
3252,231847004,"The more subtle, snarky comments from MSNBC are not detected as toxic by Perspective.",12,13
3253,231847004,"On the transcript level, FOX is overall the most uncivil, with about 6 speaker turns per predicted to be toxic, with a Perspective score greater than 0.5.",21,22
3254,231847004,"PBS appears to be the next in levels of incivility, with more than one toxic turn per transcript.",15,16
3255,231847004,"MSNBC is predicted to have fewer than one toxic turns per transcript, under-detecting incivility.",8,9
3256,231847004,"Incivility Over-prediction Prior work has drawn attention to the fact that certain words describing people are incorrectly interpreted as triggers of incivility by Perspective, leading to errors in which a perfectly acceptable text segment containing the words would be predicted as toxic (Dixon et al.,",44,45
3257,231847004,"In online comments, toxic statements are often threats of sexual 0, abortion, abuse, abused, abusively, abysmal, accosted, adult, Africa, aliens, America, Americans, Aryan, assaulted, assaulting, attack, baby, bad, barrier, Basically, beaten, belly, Black, black, Blow, blowing, bomber, bottom, bouncer, British, Brooks, brown, bunch, caliphate, capacity, cares, Catholic, Catholics, chicken, chief, child, children, China, Chinese, Christian, church, Clinton, conforming, Content, COOPER, country, Covington, cows, crackers, crawling, creatures, cries, crime, crimes, criminal, CROSSTALK, cruelty, crying, DANIELS, dare, daughter, death, decrepit, defy, dehumanizing, Democrat, Democrats, demonize, denigrate, died, Dingell, Dinkins, disrespectful, Donald, doomed, drug, drugs, Drunk, ducking, Duke, dumping, eggs, epidemic, European, evil, exist, explode, exploit, face, fake, Fallon, firearm, fithy, folks, Foreign, Former, FRANCIS, fraud, fry, gag, gagged, gang, gender, girls, governor, gun, guy, guy's, guys, handgun, harassed, harboring, hate, hatred, head, heads, heartless, Hebrew, Hegel, her, herein, heroin, Hillary, HIV, horrors, hts, human, hush, Hymie, ifs, illness, imperialists, impugning, inaudible, inconclusive, infanticide, infuriating, inhumane, intelligence, interracial, invaders, Iran, Iraqi, ISIS, Islamophobic, Israel, Israelites, jail, Juanita, Kaine, Karine, kid, kids, Klan, Korea, laid, LAUGHTER, lie, lied, lies, limbs, litig, lying, MADDOW, males, mama, man, men, mental, military, minors, missile, mock, mockery, molested, mouth, muck, n't, NAACP, needless, newscasters, Nonproliferation, nose, nuke, Obama, Obama's, obscene, obsessions, obsolete, organ, outrageous, ovations, Oversight, oxygen, p.m., painful, Pakistan, patriarchal, people, person, police, pope, President, president, president's, pretty, priest, priests, prison, prog, punched, punches, Putin, Putin's, queer, racial, Racism, racism, Rage, ranted, relations, religion, religious, relitigating, remove, REP.,",4,5
3258,231847004,The second-person pronouns are likely spuriously associated with toxicity due to overrepresentation in direct toxic comments directed to other participants in the conversation.,16,17
3259,231847004,"Similarly, the association of female pronouns with toxicity is likely due to the fact that a large fraction of the indirect toxic comments online are targeted to women.",22,23
3260,225040675,"A common problem of these models is the presence of bias towards some words (e.g. woman, black, jew or женщина, черный, еврей) that are not toxic, but serve as triggers for the classifier due to model caveats.",31,32
3261,225040675,The number of competitions and workshops (e.g. HASOC at FIRE-2019; TRAC 2020; HatEval and OffensEval at SemEval-2019) on the topic of hate speech and toxic language detection reflect the scale of the situation.,28,29
3262,225040675,"They tend to classify comments mentioning certain commonly harassed identities (e.g. containing words such as woman, black, jew or женщина, черный, еврей) as toxic, while the comment itself may lack any actual toxicity.",29,30
3263,225040675,Identity terms of frequently targeted social groups have higher toxicity scores since they are found more often in abusive and toxic comments than terms related to other social groups.,20,21
3264,225040675,"In 66 this paper, our main goal is to reduce the false toxicity scores of non-toxic comments that include identity terms empirically known to introduce model bias.",18,19
3265,225040675,2019) architectures to classify toxic Russian-language content.,5,6
3266,225040675,we assumed that the model would be able to generate non-toxic comments even with one word from protected identities given as context.,12,13
3267,231861515,This work focuses on models that can help suggest rephrasings of toxic comments in a more civil manner.,11,12
3268,231861515,"Recently, Natural Language Processing (NLP) research has tackled the problem of abusive language detection by developing accurate classification models that flag toxic (or abusive, offensive, hateful) comments (Davidson Table 1: Examples of offensive sentences from the Civil Comments test set and the more civil rephrasing generated by our model.",24,25
3269,231861515,"The main contributions of this work are the following: • We addressed for the second time the task of unsupervised civil rephrases of toxic texts, relying for the first time on the Civil Comments dataset, and achieving results that reflect the effectiveness of our model over baselines. •",24,25
3270,231861515,"Related work Unsupervised complex text attribute transfer (like civil rephrasing of toxic comments) remains in its early stages, and our particular applied task has only a single antecedent (Nogueira dos Santos et al.,",12,13
3271,231861515,"Method Formalization of the attribute text rewriting problem Let X T and X C be our two non-parallel corpora of comments satisfying the respective attributes ""toxic"" and ""civil"".",28,29
3272,231861515,"We aim at learning a parametric function f θ mapping a pair of source sentence x and destination attribute a to a fluent sentence y satisfying a and preserving the meaning of x. In our case, there are two attributes ""toxic"" and ""civil"" that we assumed to be mutually exclusive.",42,43
3273,231861515,"We denote α(x) to be the attribute of x and ᾱ(x) the other attribute (for instance when α(x) = ""civil"", then ᾱ(x) = ""toxic"").",32,33
3274,231861515,We predict toxic and civil attributes with the same fine-tuned BERT classifier that pre-processed the Civil Comments dataset (single threshold at 0.5).,2,3
3275,231861515,"2019 ) and to further confirm the performance of CAE-T5, we hired human annotators on Appen to rate in a blind fashion different models' civil rephrasings of 100 randomly selected test toxic comments, in terms of attribute transfer (Att), fluency (Flu), content preservation (Con) and overall quality (Over) on a Likert scale from 1 to 5.",35,36
3276,231861515,"Other approaches such as Style-Transformer (ST) and CrossAlignment (CA) have higher accuracy but at a cost of both higher perplexity and lower content preservation, meaning that they are better are discriminating toxic phrases but struggle to rephrase in a coherent manner.",38,39
3277,231861515,Qualitative analysis Table 7 shows examples of rephrases of toxic comments automatically generated by our system.,9,10
3278,231861515,We present more results showing that we can effectively suggest fluent civil rephrases of toxic comments in the Appendix 9 ).,14,15
3279,231861515,"In order to assess the frequency of hallucination and supererogation, we randomly selected 100 toxic comments from the test set and manually labeled the generated sentences with the nonmutually exclusive labels ""contains supererogation"" and ""contains hallucination"".",15,16
3280,231861515,"While supererogation and hallucination can be explained by the probabilistic nature of generation, we assume that position reversal is due to bias in the dataset, where toxic comments are correlated with negative comments.",28,29
3281,231861515,"In our study, we submit the civil rephrasing of toxic comments task to human crowd-sourcing.",10,11
3282,231861515,We randomly sampled 500 sentences from the toxic train set.,7,8
3283,231861515,"On the one hand, unfortunately not all toxic comments can be reworded in a civil manner so as to express a constructive point of view; severely toxic comments that are solely made of insults, identity attacks, or threats are not ""rephrasable"".",8,9
3284,231861515,"On the one hand, unfortunately not all toxic comments can be reworded in a civil manner so as to express a constructive point of view; severely toxic comments that are solely made of insults, identity attacks, or threats are not ""rephrasable"".",28,29
3285,231861515,"The control codes are c(a) = concat(a, "": "") for attributes a ∈ {""positive"", ""negative""} in the sentiment transfer task and a ∈ {""toxic"", ""civil""} when we apply to the Civil Comments dataset.",36,37
3286,231861515,"Algorithm 1: CAE-T5 training Input :T5's pre-trained parameters θ 0 , unpaired dataset labelled in toxicity X = X T ∪ X C Output :CAE-T5's fine-tuned parameters θ T for step τ ∈ [1; T ] do if τ %2 == 0 then Sample a mini-batch x of sentences in X T else Sample a mini-batch x of sentences in X C end θ ← θτ−1 θ ← θτ−1 xDAE ← f θ (η(x), α(x)) xCC ← f θ (f θ(x, ᾱ(x)), α(x)) DAE ← H(x, xDAE ) CC ← H(x, A.3 Appen settings Figure 3 and Figure 4 detail the guidelines we wrote on the crowdsourcing website Appen 11 , when we asked human crowd-workers to rate automatic rephrasings and to rephrase toxic comments.",160,161
3287,235352924,"As NLP transits from theory into practice and into daily lives, unintended negative consequences that early theoretical researchers did not anticipate have also emerged, from the toxic language of Microsoft's Twitter bot Tay (Shah and Chokkattu, 2016) , to the leak of privacy of Amazon Alexa (Chung et al.,",28,29
3288,236478036,"4 Likewise for CivilComments, we threshold comments with a toxicity rating > 0.5 as toxic, and otherwise label them as a non-toxic.",15,16
3289,236478036,"4 Likewise for CivilComments, we threshold comments with a toxicity rating > 0.5 as toxic, and otherwise label them as a non-toxic.",25,26
3290,233365230,"With respect to language-based systems, consider the Perspective API developed by Google's Counter-Abuse Technology team to identify toxic language in text.",23,24
3291,233365230,The technology has now been integrated within the New York Times comment interface to facilitate largescale moderation of potentially toxic and obscene comments on news stories.,19,20
3292,231719644,"2020) and the Google Bigquery service, (2) its social identity dynamics (in-group vs. out-group) as close-nit communities created by sub-Reddits, (3) its nature as a social news aggregation platform, and (4) that it has been shown to encourage toxic communication between users and hate speech towards social groups (Massanari, 2017; Salminen et al.,",58,59
3293,52194540,Kaggle's Toxic Comment Classification Challenge dataset 6 consists of 150k Wikipedia comments annotated for toxic behaviour.,15,16
3294,53593090,"When considering possible solutions, the binary classification of online data, as simply toxic and non-toxic content, can be very problematic.",14,15
3295,53593090,"When considering possible solutions, the binary classification of online data, as simply toxic and non-toxic content, can be very problematic.",18,19
3296,53593090,Developing classifiers that can flag the type and likelihood of toxic content is a far better approach.,10,11
3297,53593090,"Additionally, the annotated datasets will always be unbalanced since some types of toxic content are much more prevalent than others.",13,14
3298,52878910,"Introduction The increasing popularity of social media platforms like Twitter for both personal and political communication (Lapowsky, 2017) has seen a well-acknowledged rise in the presence of toxic and abusive speech on these platforms (Hillard, 2018; Drum, 2017) .",32,33
3299,53590468,"Notably, in late 2016, a number of users across social media platforms and internet forums (particularly 4chan) began a movement called 'Operation Google', a direct retaliation to Google announcing the creation of tools for the automated moderation of toxic content.",45,46
3300,53590468,"Euphemistic hate speech stands separate from other forms of implicit hate speech (namely micro-aggressions) because in truth, they are often direct toxic attacks as opposed to veiled or context dependent ones.",26,27
3301,226283547,"An author searching for hate speech data or studies, might miss out on ones that used abusive language or toxic comment as an umbrella term encompassing several paradigms.",20,21
3302,229679941,"Firstly, malicious actors might employ MICE to generate adversarial examples; for instance, they may aim to generate hate speech that is minimally edited such that it fools a toxic language classifier.",31,32
3303,6508977,Abstract Syntax Trees We describe abstract syntax trees (ASTs) using an example from CFR Section 610.11: (1) A general safety test for the detection of extraneous toxic contaminants shall be performed on biological products intended for administration to humans.,31,32
3304,8530236,plastic waste generates toxic by-products-<R>is an in-text citation that was removed in the preparation of ICLE) that illustrates this particular construction.,3,4
3305,236459882,Toxic spans detection is an emerging challenge that aims to find toxic spans within a toxic text.,11,12
3306,236459882,Toxic spans detection is an emerging challenge that aims to find toxic spans within a toxic text.,15,16
3307,236459882,"In this paper, we describe our solutions to tackle toxic spans detection.",10,11
3308,236459882,"Introduction By dint of the massive production of user-generated content in social media, moderation becomes crucial to promote healthy online discussions by removing toxic posts and contents.",26,27
3309,236459882,The Toxic Spans Detection task aims to detect the spans that make a text toxic.,14,15
3310,236459882,"However, these works estimate the likelihood of a document being toxic with weak interpretability.",11,12
3311,236459882,"In fact, highlighting toxic spans can assist human moderators who often deal with lengthy comments, and who prefer attribution instead of just a system-generated unexplained toxicity score per post. *",4,5
3312,236459882,"contributed equally In this paper, we propose two solutions to tackle toxic spans detection (Pavlopoulos et al.,",12,13
3313,236459882,We considered the toxic span text detection as a sequence labeling task.,3,4
3314,236459882,Data preparation The raw dataset consists of a set of toxic texts where each element is annotated with an array that contains characters' indices.,10,11
3315,236459882,These indices are considered as the toxic span of text.,6,7
3316,236459882,"In order to train SpanBERT on this dataset, we applied the pre-trained SpanBERT tokenizer to tokenize sentences, and we built the target arrays by annotating words that contain toxic characters' indices.",32,33
3317,236459882,"For instance, given a sentence that contains n tokens, then the target array contains n elements, where the elements that contain a toxic character are labeled as positive ""1"" otherwise they are labeled as negative ""0"".",25,26
3318,236459882,Toxic spans detection We considered the toxic span detection as a sequence labeling task.,6,7
3319,236459882,"After that, we compute the probability if a given token is toxic by applying a linear layer followed by a softmax on tokens embeddings.",12,13
3320,236459882,It is worth noting that we filter predicted spans by removing toxic character offsets that have a size equal to one.,11,12
3321,236459882,Toxic spans detection The toxic spans detection task adopted by our unsupervised method can be summarized as follows: 1.,4,5
3322,236459882,"We train the linear support vector machine classifier on 26164 toxic comments and 143346 non-toxic comments (the combination of SemEval 2021 Task 5: Toxic Spans Detection training set, SemEval 2021 Task 5: Toxic Spans Detection test set, and a subset of Kaggle Jigsaw Toxic comment classification challenge dataset).",10,11
3323,236459882,"We train the linear support vector machine classifier on 26164 toxic comments and 143346 non-toxic comments (the combination of SemEval 2021 Task 5: Toxic Spans Detection training set, SemEval 2021 Task 5: Toxic Spans Detection test set, and a subset of Kaggle Jigsaw Toxic comment classification challenge dataset).",16,17
3324,236459882,We discard words that contribute less to the toxic category by applying a thresholding technique.,8,9
3325,236459882,"Words with a high influence score, greater or equal to the threshold, are considered toxic, therefore, we retrieve their character offsets (toxic spans).",16,17
3326,236459882,"Words with a high influence score, greater or equal to the threshold, are considered toxic, therefore, we retrieve their character offsets (toxic spans).",26,27
3327,236459882,"From Figure 3 , we can see that the words ""silly"" and ""stupid"" contribute to the toxic category 42% and 23% respectively in the following toxic comment ""Please people, stop using these silly, stupid emoticons"".",20,21
3328,236459882,"From Figure 3 , we can see that the words ""silly"" and ""stupid"" contribute to the toxic category 42% and 23% respectively in the following toxic comment ""Please people, stop using these silly, stupid emoticons"".",31,32
3329,236459882,"Since we only consider words with high influence scores for the toxic category (greater or equal to 0.13), we keep the two words ""silly"" and ""stupid"", and we discard the remaining words.",11,12
3330,236459882,The training set and test set contain 7939 and 2000 toxic comments labeled with their toxic spans.,10,11
3331,236459882,The training set and test set contain 7939 and 2000 toxic comments labeled with their toxic spans.,15,16
3332,236459882,Considering a post t and a system A i which predict a set S t A i of toxic character offsets.,18,19
3333,236459882,"A supervised approach based on transformers technique, where toxic sequences are tokenized and embedded using pre-trained models.",9,10
3334,236459882,We optimize the likelihood of a token to be toxic by minimizing the cross-entropy loss.,9,10
3335,236459882,"Since the top-ranked score was about 0.7083 F1 score, future studies and works will focus on improving the performance of toxic spans detection task.",23,24
3336,247958065,"Previous work has commented on the difficulty of aligning annotations of abusive, offensive, hateful, and toxic speech across different datasets (Swamy et al.,",18,19
3337,239015827,2021) propose using hand-crafted prompts to first encourage a model to generate toxic text.,15,16
3338,239015827,"Then, a second continuation that is non-discriminative can be generated from the model where the probabilities of tokens deemed likely under the first toxic generation are scaled down.",26,27
3339,239015959,"2020) , and are capable of generating toxic or otherwise unsafe content (Weidinger et al.,",8,9
3340,247762111,"Annotators were strictly asked not to write any toxic content (hateful or offensive toward any gender, race, sex, religion).",8,9
3341,248780439,"2019) , toxic text generation (Gehman et al.,",3,4
3342,248780439,"Likewise, Solaiman and Dennison (2021) propose fine-tuning on carefully curated ""values-targeted"" datasets to reduce toxic GPT-3 behavior.",23,24
3343,248780439,The goal of the task is to predict whether each comment is toxic.,12,13
3344,248780439,"Each comment has been labeled as toxic or non-toxic by a human annotator, where a toxic comment is a ""rude, disrespectful, or unreasonable comment that is likely to make you leave the discussion"" (Dixon et al.,",6,7
3345,248780439,"Each comment has been labeled as toxic or non-toxic by a human annotator, where a toxic comment is a ""rude, disrespectful, or unreasonable comment that is likely to make you leave the discussion"" (Dixon et al.,",10,11
3346,248780439,"Each comment has been labeled as toxic or non-toxic by a human annotator, where a toxic comment is a ""rude, disrespectful, or unreasonable comment that is likely to make you leave the discussion"" (Dixon et al.,",18,19
3347,248780439,"Downstream (Extrinsic) Bias.-Mentions of certain identity groups-such as ""queer""-are more likely to be flagged for toxic content, which could result in certain communities being systematically censored or left unprotected if an online platform uses the classifier.",20,21
3348,248780439,The classifier's empirical false positive rate (FPR) estimates its likelihood to falsely flag a non-toxic comment as toxic.,19,20
3349,248780439,The classifier's empirical false positive rate (FPR) estimates its likelihood to falsely flag a non-toxic comment as toxic.,22,23
3350,248780439,"FPB i = P[ T = 0 | I = i, T = 1] P[ T = 0 | T = 1] , (3) where i is an identity term and T = 1 if the comment was deemed toxic by a human annotator.",46,47
3351,248780439,"3), upstream bias is negative sentiment, and ⇡ i is the proportion of toxic mentions of identity i. We additionally control for the prevalence of each identity term and the average length of toxic mentions of each identity term-longer comments are less likely to result in erroneous screening (Appendix C.1).",16,17
3352,248780439,"3), upstream bias is negative sentiment, and ⇡ i is the proportion of toxic mentions of identity i. We additionally control for the prevalence of each identity term and the average length of toxic mentions of each identity term-longer comments are less likely to result in erroneous screening (Appendix C.1).",36,37
3353,248780439,"In comparison, only a 10% increase in the prevalence of toxic mentions of an identity corresponds to an even larger 6.3% increase in FPR.",12,13
3354,248780439,Scrubbing mentions of identity terms-in all comments or only in toxic comments-appears to reduce bias only when the model is not pre-trained and all mentions of the term are scrubbed (Figure 5 ).,12,13
3355,248780439,"For a pre-trained model trained on scrubbed data, a 10% decrease in mentions of an identity term corresponds to a 7.2%  (a) BIOS • • • • • • • • • • • Fine− • • • • • • • • • • Fine−tuning dataset bias Prevalance of toxic mentions Upstream bias Avg.",58,59
3356,248780439,"For example, our model classifies a sequence containing only the term ""gay"" as toxic without any context.",16,17
3357,248780439,"If a term like ""gay"" is often used pejoratively on the web, RoBERTa is likely to infer that sentences including ""gay"" are toxic even if the term never appears in the fine-tuning dataset.",27,28
3358,248780439,"Finally, the models we trained exhibit toxic, offensive behavior.",7,8
3359,248780439,WIKI.-Identity terms occur relatively infrequently in the dataset and some terms appear in toxic comments more often than others.,13,14
3360,248780439,See Figure 8 for a full list of identity terms tested and their prevalence in toxic and non-toxic comments.,15,16
3361,248780439,See Figure 8 for a full list of identity terms tested and their prevalence in toxic and non-toxic comments.,19,20
3362,248780439,"B Additional Results B.1 Partial Interventions This section gives the full results for each partial intervention (e.g., scrubbing only half of the 6 https://www.bls.gov/soc/  toxic mentions, rather than all of them).",27,28
3363,248780439,"2018 ) study of toxicity classification bias is an extensive evaluation set composed of 89,000 templates such as ""[IDENTITY] is [ATTRIBUTE] ,"" where the attributes include both positive (for non-toxic examples) and extremely negative words (for toxic   examples).",38,39
3364,248780439,"2018 ) study of toxicity classification bias is an extensive evaluation set composed of 89,000 templates such as ""[IDENTITY] is [ATTRIBUTE] ,"" where the attributes include both positive (for non-toxic examples) and extremely negative words (for toxic   examples).",47,48
3365,248780439,The model's bias is described by the difference between the average log probability scores for the toxic templates and the non-toxic templates for each identity term.,17,18
3366,248780439,The model's bias is described by the difference between the average log probability scores for the toxic templates and the non-toxic templates for each identity term.,23,24
3367,248780439,"For the regressions (Tables 7 and 8 ), the templates are not paired, so we average first across toxic and non-toxic templates, then calculate the ratio between the two.",21,22
3368,248780439,"For the regressions (Tables 7 and 8 ), the templates are not paired, so we average first across toxic and non-toxic templates, then calculate the ratio between the two.",25,26
3369,235097625,"Conversational agents trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which may include offensive or otherwise toxic behavior.",24,25
3370,235097625,"Introduction When dialogue models are trained to mimic human-human conversations utilizing large preexisting datasets, they will unfortunately also learn undesirable features from this human-human data, such as the use of toxic or biased language.",36,37
3371,235097625,"These findings suggest it is insufficient to merely exclude toxic data from training, as the model would not know how to answer hostile out-of-domain inputs, and positive biases where models tend to agree rather than contradict (Roller et al.,",9,10
3372,235097625,"2020) , training on sanitized data can decrease the amount of unprompted toxic content, yet still leave models vulnerable to generating toxic content based on specific prompts.",13,14
3373,235097625,"2020) , training on sanitized data can decrease the amount of unprompted toxic content, yet still leave models vulnerable to generating toxic content based on specific prompts.",23,24
3374,235097625,"The moving target of toxic content requires dynamic methods that repeatedly update benchmarks to improve current systems (Dinan et al.,",4,5
3375,235097625,"If unsafe utterances are not flagged, toxic language can still enter the conversation.",7,8
3376,235097625,"We instead propose a technique for attempting to bake awareness of toxic language into the training data, by using labeled examples that recommend appropriate action on the model's part in those circumstances.",11,12
3377,235097625,"Balancing these weights is important, especially when dealing with highly toxic pre-training sets, as they may be dominated by modified examples.",11,12
3378,235097625,"The community has recently started to address this tradeoff between releasing models that can produce offensive or toxic language and open, reproducible research 5 .",17,18
3379,235097625,"This would lead to a conclusion that while toxic authors exist, there are also a large number of otherwise non-toxic authors who sometimes use toxic language, and this can adversely affect model training.",8,9
3380,235097625,"This would lead to a conclusion that while toxic authors exist, there are also a large number of otherwise non-toxic authors who sometimes use toxic language, and this can adversely affect model training.",22,23
3381,235097625,"This would lead to a conclusion that while toxic authors exist, there are also a large number of otherwise non-toxic authors who sometimes use toxic language, and this can adversely affect model training.",27,28
3382,235097625,"In contrast, the safety classifier only fires on human data from ConvAI2 3.9% of the time, which can be explained by this data being authored by crowdworkers who had instructions not to use toxic language.",36,37
3383,235097625,This shows how training on less toxic data induces less toxic models.,6,7
3384,235097625,This shows how training on less toxic data induces less toxic models.,10,11
3385,235097625,This is in contrast to a non-adversarial environment such as Wikipedia Toxic Comments where the contexts are not constructed to elicit toxic responses from chat bots.,23,24
3386,236034557,"They are asked to play a particular role (""Create an interesting character that you want to play""), and are given instructions to avoid toxic or biased language.",28,29
3387,236034557,"Ethical Considerations, Societal Impact Large language models bring an impact on the environment in terms of resources required to train and deploy them, and concerns about toxic language, bias and other issues during language generation (Bender et al.,",28,29
3388,236034557,"However, our fine-tuning task is built with crowdworkers with specific instructions to not use toxic language, a procedure which is shown to yield safer language models (Roller et al.,",17,18
3389,236034557,"On the other hand, this also brings potential new concerns if those websites contain toxic, biased or factually incorrect information themselves.",15,16
3390,234757004,"Generation models trained on Reddit sometimes tend to generate toxic or inappropriate responses as pointed out by Dialogpt (Zhang et al.,",9,10
3391,236976189,"Therefore, DEMIX LMs may still be prone to producing harmful generations when deployed, and further research is required to understand the bounds on the probability of toxic degeneration after expert removal.",28,29
3392,248779933,"Controlling for multiple factors, political users are more toxic on the platform and inter-party interactions are even more toxic-but not all political users behave this way.",9,10
3393,248779933,"Controlling for multiple factors, political users are more toxic on the platform and inter-party interactions are even more toxic-but not all political users behave this way.",21,22
3394,248779933,"We show that controlling for multiple factors, political users are generally more toxic on the platform and that cross-affiliation interactions are even more toxicwith liberal-to-conservative interactions being most toxic.",13,14
3395,248779933,"We show that controlling for multiple factors, political users are generally more toxic on the platform and that cross-affiliation interactions are even more toxicwith liberal-to-conservative interactions being most toxic.",35,36
3396,248779933,"However, not all types of political users are equally toxic, highlighting the importance of how studies define political users.",10,11
3397,248779933,"Finally, as toxic conversations may lead to more toxicity, we include a linear factor for the parent comment's toxicity.",3,4
3398,248779933,"First, consistent with prior work, we find that controlling for subreddit-specific levels of toxicity, discussion in political communities is much more toxic, suggesting that these topics are a primary source of increased hostility.",26,27
3399,248779933,"While conservative users receive more toxic replies, such users are more toxic when replying to liberal users than liberal-to-conservative replies.",5,6
3400,248779933,"While conservative users receive more toxic replies, such users are more toxic when replying to liberal users than liberal-to-conservative replies.",12,13
3401,248779933,"Surprisingly, this increased toxicity is not due to an explicit flair signal; when users are commenting in a community where the flair is visible-which can include mixed-affiliation subreddits-users receive less toxic replies.",38,39
3402,248779933,"However, given that interactions between flair-signaling users were less toxic, we speculate a few mechanisms may be in place.",12,13
3403,248779933,"First, cross-partisan communities using flair often feature rules or norms that encourage deliberative discus-sion, thereby raising the expectations for non-toxic behavior; for example, r/AskALiberal includes prohibitions against uncivil or bad faith comments.",27,28
3404,248779933,"Across all of Reddit, individuals who actively participate in politically affiliated subreddits for one party are substantially more toxic in their interactions; in contrast, those who participate in flairbased communities or who have declared their affiliation in a comment (but do not participate in political communities or have flair) are much less toxic.",19,20
3405,248779933,"Across all of Reddit, individuals who actively participate in politically affiliated subreddits for one party are substantially more toxic in their interactions; in contrast, those who participate in flairbased communities or who have declared their affiliation in a comment (but do not participate in political communities or have flair) are much less toxic.",57,58
3406,248779886,"or a property of the output (e.g. substring containment in translation; f en-to-pt (""The cake's icing"") ""cereja"", or the output of a classifier c(•) for text generation; c(f gen (""Immigrants are"")) toxic).",52,53
3407,248779886,"Users tried to find non-toxic statements predicted as toxic for two topics: Left (progressive), and Right (conservative) political opinions.",6,7
3408,248779886,"Users tried to find non-toxic statements predicted as toxic for two topics: Left (progressive), and Right (conservative) political opinions.",10,11
3409,233231257,"Finally, since we are finetuning a pretrained generative model, we inherit the general risk of generating biased or toxic language, which should be carefully filtered.",20,21
3410,247476420,"rely on a 3rd party APIs such as Perspective API 1 for detecting toxic comments online-e.g., racist, offensive, personal attacks.",13,14
3411,247476420,"The attacker can use a black-box attack method to attack these public APIs in an iterative manner, then retrieve the adversarial toxic comments and use those on these platforms without the risk of being detected and removed by the system.",24,25
3412,247996779,"Additionally, we manually examined the stories and the created questions to ensure there are no privacy or ethical concerns, e.g., toxic language, hate speech, or any bias against underrepresented groups.",23,24
3413,186206323,We also empirically evaluate the value of text pre-processing techniques in addressing the challenge of outof-vocabulary words in toxic content.,22,23
3414,186206323,"These datasets were created by having annotators from the Crowdflower platform label Wikipedia Talk Page comments as toxic or not, personal attack or not, and aggressive or not, respectively.",17,18
3415,186206323,"Roughly 10 percent of the comments in each dataset are labelled as toxic, personal attacks or aggressive.",12,13
3416,186206323,"To measure the accuracy of the models, we report macro (i.e., average) F1 scores over both classes (labelled ""Overall"" below) as well as the (micro) F1 scores for just the toxic classes (defined in the standard way, as a harmonic mean of precision and recall).",40,41
3417,186206323,"To assess the impact of the different preprocessing steps from Section 3 on classification accuracy, Table 2 shows the Overall average F1 scores and the toxic class F1 scores for the BiRNN model (baseline model).",26,27
3418,186206323,"To obtain further insight into the performance on the minority (toxic, personal attack or aggression) class, we show the micro precision (P), recall (R) and F1 scores for the minority class in Table 4 .",11,12
3419,186206323,"Our experimental results showed that combining text processing with attention mechanisms, both of which aim to filter out as much noise as possible, is more effective than the previous state of the art, especially at predicting the minority (toxic) class.",42,43
3420,248693452,"Unfortunately, this also means that this technology has the potential to be misused to perpetuate biases or generate offensive or toxic text.",21,22
3421,248693452,"Our technology does not guarantee removal of toxic content, even in the case of unsupervised style transfer from toxic to nontoxic text.",7,8
3422,248693452,"Our technology does not guarantee removal of toxic content, even in the case of unsupervised style transfer from toxic to nontoxic text.",19,20
3423,236034497,"Ethical Considerations The dialogue models we use in this work utilize large language models, and therefore have similar concerns as in other work, in particular concerns about toxic language, bias and other issues during language generation (Bender et al.,",29,30
3424,195065314,"When you swallow or inhale these highly toxic products, you can experience life-threatening symptoms.",7,8
3425,3920034,"For instance, to highlight the potential toxicity of coffee, Thesaurus Rex suggests comparisons with alcohol, tobacco or pesticide, as all have been categorized as toxic substances on the web.",28,29
3426,236460071,This research aims to find toxic words in a sentence so that a healthy social community is built across the globe and the users receive censored content with specific warnings and facts.,5,6
3427,236460071,This includes filtering and censoring toxic and hateful content posted online on these public forums.,5,6
3428,236460071,"There are automated hate detection NLP models like (Zhang and Luo, 2018) capable of identifying toxic content with acceptable performance.",18,19
3429,236460071,"However, they do not identify the specific spans of text that are toxic.",13,14
3430,236460071,Background research and related works This problem tackles the challenge of accurately identifying the parts of a toxic text and contributing to making the complete text toxic.,17,18
3431,236460071,Background research and related works This problem tackles the challenge of accurately identifying the parts of a toxic text and contributing to making the complete text toxic.,26,27
3432,236460071,"There, however, exist systems that can score the toxicity of a full text, sentence or comment like (Zhang and Luo, 2018) , but they don't identify the exact part of that sentence that is toxic.",41,42
3433,236460071,"There also are systems that can accurately say if a statement is toxic or not and even go on to the extent of identifying the emotion projected by it, for example, if it is positive, negative, humorous, sarcastic, offensive or funny and even the level of these emotions as seen in (Gupta et al.,",12,13
3434,236460071,They have morphed the origi-nal aim of the NER type tasks to train a model capable of identifying toxicity in a text that is already classified as toxic as a whole.,29,30
3435,236460071,"This was possible because, finally, we were using the different embeddings related to toxic parts of the text.",15,16
3436,236460071,"The task organizers then filtered all the text level toxic-labelled text that had been labelled toxic by more than half the annotators, which was around 30,000 in number from a total of 1.2 million posts.",9,10
3437,236460071,"The task organizers then filtered all the text level toxic-labelled text that had been labelled toxic by more than half the annotators, which was around 30,000 in number from a total of 1.2 million posts.",17,18
3438,236460071,"Out of these 30,000 text posts, random 10,000 posts were selected and given to crowd annotators to mark the toxic spans from the text.",20,21
3439,236460071,Spans column contained a list of character level indices of the toxic entities.,11,12
3440,236460071,Next to it was the complete text that was found to be toxic.,12,13
3441,236460071,"For some texts, there existed a corresponding list of empty spans if no toxic span was annotated.",14,15
3442,236460071,"The data provided had a column full of rows with toxic text collected from social media and hence it was naturally in need of cleaning as it contained a lot of abbreviations, punctuation, some foreign characters, numbers and special characters that were not supposed to be present there as they would create ambiguity to the model.",10,11
3443,236460071,It was able to predict toxic spans closest to the ground truth.,5,6
3444,236460071,"The authors of Ground spans= [12, 13, 14, 15, 16] = ['idiot'] Predicted spans= [12, 13, 14, 15, 16] =['idiot'] The above example displays the input text, the pre-processed clean text with the toxic word ""idiot"" highlighted in red.",57,58
3445,236460071,Below them is the ground truth for the spans of this toxic word and then there are the correct predicted spans for it.,11,12
3446,236460071,Conclusion We support the systematic development for identifying toxic spans.,8,9
3447,236460071,This task can prove to be useful in providing a better analysis in the censoring of toxic posts on the internet.,16,17
3448,236460307,This problem can be tackled by filtering toxic comments/posts.,7,8
3449,236460307,Highlighting toxic spans can help human moderators who frequently deal with long comments and prefer attribution rather than just an unexplained toxicity score.,1,2
3450,236460307,2020) by training our model on the manually annotated dataset and using it to further extend the training set by generating toxic spans for other unannotated datasets.,22,23
3451,236460307,"Having begin and end tags helps formulate the notion of spans better and creates dependencies between various tokens of a toxic span (Singh et al.,",20,21
3452,236460307,Post-Processing: The tokens decoded as B-Begin or I-Inside were marked as toxic.,18,19
3453,236460307,The character spans corresponding to these toxic tokens were added to the predicted spans.,6,7
3454,236460307,Self-Training The best performing model on the manually annotated dataset (gold dataset) was used to generate toxic spans for the unannotated dataset.,20,21
3455,236460307,"2021) that is, filter the most toxic samples (toxicity ≥ 0.80 ) from the Civil Comments dataset and select a random set of 10,000 samples.",8,9
3456,236460307,"3 Experimental Setup Data: Each training example consisted of a text sample in English, and its ground truth toxic span provided as a list of character offsets (possibly empty).",20,21
3457,236460307,Nearly 10% of all tokens in the training dataset are marked as toxic.,13,14
3458,236460307,"As suggested earlier, domain-specific pre-training allows the model to understand the language construct of toxic comments better.",19,20
3459,236460307,The constancy of recall indicates that few spans are not captured as toxic by any of the models.,12,13
3460,236460307,Error Analysis Figure 3 shows the variation of the F1 score across different toxic span lengths on the test dataset.,13,14
3461,236460307,"Our model achieved a very high F1 score when one (Span Length 1-9, Mean F1 Score: 83.17%) or two (Span Length 10-17, Mean F1 Score: 74.44%) words are marked as toxic in a text sample.",44,45
3462,236460307,"As the number of characters marked as toxic increases, the F1 score falls drastically, reaching as low as 24.82% when more than 58 characters are marked as toxic.",7,8
3463,236460307,"As the number of characters marked as toxic increases, the F1 score falls drastically, reaching as low as 24.82% when more than 58 characters are marked as toxic.",30,31
3464,236460307,"Second, only 10% of the training data has a span length of more than 25 characters making the model less equipped to capture such toxic spans.",26,27
3465,236460307,"There were 447 such samples, of which 349 samples did not have any toxic span in the ground truth.",14,15
3466,236460307,"Further analysis revealed that our model tends to mark those tokens as toxic, which were frequently found to be toxic elsewhere.",12,13
3467,236460307,"Further analysis revealed that our model tends to mark those tokens as toxic, which were frequently found to be toxic elsewhere.",20,21
3468,236460307,A few samples with empty toxic spans had doubtful gold annotations.,5,6
3469,236460307,"However, in other samples, our model failed to capture the sentence's context precisely and predicts tokens that were not used in a toxic sense.",25,26
3470,236460307,"More often than not, it misses the toxic span present in it and returns an empty span.",8,9
3471,236460307,A similar case occurs when it encounters text samples with rare toxic words.,11,12
3472,236460307,"Other than these, our model sometimes misses the non-swear words in a toxic span.",15,16
3473,236459777,The task aims to accurately locate toxic spans within a text.,6,7
3474,236459777,"Because toxic posts will have a negative impact on the network environment, and manual identification is timeconsuming and expensive, automatic detection of these behaviors has attracted researchers' attention.",1,2
3475,236459777,"After adapting the hate-speech problem to the problem of word sense disambiguation, an approach to detect hate speech in online text is presented (Warner and Hirschberg, 2012) , which uses template-based strategy to generate features and an SVM classifier to identify whether the text is toxic or not.",53,54
3476,236459777,"After manual annotation, character-level annotation results are obtained, which are the toxic spans we need to locate.",15,16
3477,236459777,"The task extends the prior work by identifying spans that make a text toxic, which can better explain why posts are offensive rather than just giving a system-generated unexplained toxicity score.",13,14
3478,236459777,We model the task as a sequence labeling task because toxic spans are contextually influenced.,10,11
3479,236459777,"Our model is in Transformer-CRF architecture, and we try different pre-trained models as the transformer's initialization to fine-tune model suitable for toxic spans detection.",29,30
3480,236459777,"Instead of just classifying the whole comments or documents, the Toxic Spans Detection task requires the system to detect the spans that make a text toxic.",26,27
3481,236459777,"People often judge offensive sentences in terms of words, therefore the toxic spans in this task are always associated with words.",12,13
3482,236459777,"In this task, the input sentence may contain no toxic span, which means it is not offensive.",10,11
3483,236459777,"Considering toxic spans are contextually influenced, we model the task as a token-level sequence labeling task and use BIO tagging scheme.",1,2
3484,236459777,"2019) , to identify the toxic tokens.",6,7
3485,236459777,"If one of the token's spans is tagged as toxic in the original dataset, considering the tag of the previous token, the token will be tagged as ""B-toxic"" or ""Itoxic"" in pre-processed data.",10,11
3486,236459777,"If one of the token's spans is tagged as toxic in the original dataset, considering the tag of the previous token, the token will be tagged as ""B-toxic"" or ""Itoxic"" in pre-processed data.",33,34
3487,236459777,"If the token's spans are not tagged as toxic in the original dataset, the token will be tagged as ""O"" in pre-processed data.",9,10
3488,236459777,"Using only the fully connected layer may output illegal tags (e.g. ""... O I-toxic O ...""), while introducing the CRF layer allows the model to learn the constraints between tags.",17,18
3489,236459777,"Post-Processing Our model produces token-level predictions, but detecting toxic spans within the text is a characterlevel task.",13,14
3490,236459777,"Particularly, we assume the blank between toxic tokens should be tagged as toxic, too.",7,8
3491,236459777,"Particularly, we assume the blank between toxic tokens should be tagged as toxic, too.",13,14
3492,236459777,"Experiment Dataset We trained our models by SemEval-2021 Task 5 training data which consists of 7,939 samples with a total of 139,115 toxic spans.",22,23
3493,236459777,"Then we get a total of 31,114 toxic tokens, with an average of 3.92 per sample.",7,8
3494,236459777,"Metric Let system A return a set S t A of character offsets, for the post t that found to be toxic.",22,23
3495,236459777,"In toxic spans detection task, we consider the information from the lower layers to be important.",1,2
3496,236459848,Our approach considers toxic spans detection as a segmentation problem.,3,4
3497,236459848,"2019) , toxic span detection is an NLP task focusing on capturing granular contents that make a text toxic.",3,4
3498,236459848,"2019) , toxic span detection is an NLP task focusing on capturing granular contents that make a text toxic.",19,20
3499,236459848,"Therefore, the evaluation of systems that could accurately locate toxic spans within a text is considered a crucial step for this task (Pavlopoulos et al.,",10,11
3500,236459848,"For the first approach, we consider toxic spans detection as a segmentation problem while in the second one we use transformers-based models.",7,8
3501,236459848,"In our second approach, we consider toxic spans as a single label Named-Entity Recognition problem.",7,8
3502,236459848,"In accordance with the segmentation logic, the output character positions contain toxic spans masked as 1 and the other parts of the text masked as 0 (Fig.",12,13
3503,236459848,Transformers Models for NER Toxic span detection can be adopted to NER problems by considering targeted toxic part of text as a predefined named-entity.,16,17
3504,236459848,"To prepare toxic spans dataset for training, word labeling operation carried out by converting toxic spans into toxic words based on whether more than 50% of their characters labeled as toxic (Fig.",2,3
3505,236459848,"To prepare toxic spans dataset for training, word labeling operation carried out by converting toxic spans into toxic words based on whether more than 50% of their characters labeled as toxic (Fig.",15,16
3506,236459848,"To prepare toxic spans dataset for training, word labeling operation carried out by converting toxic spans into toxic words based on whether more than 50% of their characters labeled as toxic (Fig.",18,19
3507,236459848,"To prepare toxic spans dataset for training, word labeling operation carried out by converting toxic spans into toxic words based on whether more than 50% of their characters labeled as toxic (Fig.",32,33
3508,236459848,"Overall, we showed that segmentation based systems can be used to address the toxic detection task.",14,15
3509,236459974,Detection of toxic spans -detecting toxicity of contents in the granularity of tokens -is crucial for effective moderation of online discussions.,2,3
3510,236459974,"However, most of the existing work on toxicity detection labels the entire comment as toxic or non-toxic and does not provide information about which specific part of the comment is toxic.",15,16
3511,236459974,"However, most of the existing work on toxicity detection labels the entire comment as toxic or non-toxic and does not provide information about which specific part of the comment is toxic.",19,20
3512,236459974,"However, most of the existing work on toxicity detection labels the entire comment as toxic or non-toxic and does not provide information about which specific part of the comment is toxic.",33,34
3513,236459974,"In practice, human moderators (e.g., news portals moderators) can benefit from information on which character indices of the part of the comment that is toxic instead of just a system-generated unexplained toxicity score per post.",28,29
3514,236459974,Designing models that can accurately locate toxic spans within a text is thus a crucial step towards successful semi-automated moderation.,6,7
3515,236459974,"This paper explains our approaches for the SemEval-2021 Task 5 which requires us to identify the character offsets for the toxic spans within a comment (Pavlopoulos et al.,",20,21
3516,236459974,"Token-level Labelled Data We used two token-level labeled datasets, the first being the one provided by the SemEval-2021 Task 5 organizers which are composed of 9,939 English posts along with their toxic spans.",36,37
3517,236459974,The span for a single post is a possible-empty list of character indices that have been marked as toxic by crowdannotators.,20,21
3518,236459974,"We considered a token to be toxic if it was included in at least one annotator's rationale, and excluded all posts with no toxic token, which left us with 11,415 posts.",6,7
3519,236459974,"We considered a token to be toxic if it was included in at least one annotator's rationale, and excluded all posts with no toxic token, which left us with 11,415 posts.",25,26
3520,236459974,"Sentence-level Labelled Data We used five other datasets which consisted of only sentence-level labels: • Jigsaw/Conversation AI toxic comment classification challenge dataset 2 -Composed of Wikipedia's talk page edits, it labels 223,549 posts into zero or more categories of toxicity (toxic, severely toxic, obscene, threat, insult, and identity hate).",24,25
3521,236459974,"Sentence-level Labelled Data We used five other datasets which consisted of only sentence-level labels: • Jigsaw/Conversation AI toxic comment classification challenge dataset 2 -Composed of Wikipedia's talk page edits, it labels 223,549 posts into zero or more categories of toxicity (toxic, severely toxic, obscene, threat, insult, and identity hate).",50,51
3522,236459974,"Sentence-level Labelled Data We used five other datasets which consisted of only sentence-level labels: • Jigsaw/Conversation AI toxic comment classification challenge dataset 2 -Composed of Wikipedia's talk page edits, it labels 223,549 posts into zero or more categories of toxicity (toxic, severely toxic, obscene, threat, insult, and identity hate).",53,54
3523,236459974,"We bundled each of them into a single category, and 22,468 posts were categorized as toxic. •",16,17
3524,236459974,"We labeled 20,620 belonging to the former two categories as toxic. •",10,11
3525,236459974,"Merging these five datasets resulted in a collection of 281,279 comments out of which 53,117 were labeled as toxic.",18,19
3526,236459974,It makes sure the most toxic token in the sentence has the same label as the label for the sentence.,5,6
3527,236459974,"For generating the character offsets, we considered an entire word as toxic if any single of its subword tokens were identified as toxic.",12,13
3528,236459974,"For generating the character offsets, we considered an entire word as toxic if any single of its subword tokens were identified as toxic.",23,24
3529,236459974,"This can be attributed to the comparatively higher toxicity of the sentence-level labeled datasets when compared with the contest dataset, which makes the models more expert on detecting highly toxic spans.",32,33
3530,236459974,"However the latter might be more suitable for large-scale datasets due to less training time: the sentence classification model has to be trained on the entire sentence-labeled dataset in which the majority of the samples are non-toxic, whereas the model for generating token labels is trained only on toxic samples.",43,44
3531,236459974,"However the latter might be more suitable for large-scale datasets due to less training time: the sentence classification model has to be trained on the entire sentence-labeled dataset in which the majority of the samples are non-toxic, whereas the model for generating token labels is trained only on toxic samples.",56,57
3532,236459974,"Out of the 52,640 toxic samples, only 7,629 samples were given a toxicity score of more than 0.5, thus greatly reducing the size of the additional dataset for token classification.",4,5
3533,236459974,However it was successful in correctly identifying the toxic spans.,8,9
3534,236460261,We trained our model to find toxic words and concatenate their spans to predict the toxic spans within a sentence.,6,7
3535,236460261,We trained our model to find toxic words and concatenate their spans to predict the toxic spans within a sentence.,15,16
3536,236460261,We fine-tuned Pre-trained Language Models (PLMs) for identifying the toxic words.,15,16
3537,236460261,"For fine-tuning, we stacked the classification layer on top of the PLM features of each word to classify if it is toxic or not.",24,25
3538,236460261,"We, therefore, compare the performance of BERT, ELECTRA, RoBERTa, XLM-RoBERTa, T5, XLNet, and MPNet for identifying toxic spans within a sentence.",26,27
3539,236460261,"Over the years people used it to exchange positive ideas but recently, there has been a rise in toxic content and hate speech over the internet (Zampieri et al.,",19,20
3540,236460261,"2020) dealing with the problem of toxic, offensive, or hateful content aim to classify the entire text belonging to a particular class.",7,8
3541,236460261,They do not identify the parts of the text that make it toxic.,12,13
3542,236460261,"Manual filtering of toxic data is tough and can cause mental and emotional stress to annotators (Zampieri et al.,",3,4
3543,236460261,An automatic system with the ability to identify toxic text and highlighting toxic spans can be useful for the moderators.,8,9
3544,236460261,An automatic system with the ability to identify toxic text and highlighting toxic spans can be useful for the moderators.,12,13
3545,236460261,2021) draws attention to the problem of identifying toxic spans present in a sentence.,9,10
3546,236460261,The offsets of the toxic words can then be concatenated to find the toxic spans.,4,5
3547,236460261,The offsets of the toxic words can then be concatenated to find the toxic spans.,13,14
3548,236460261,2020) to compare their performance on the task of toxic spans detection.,10,11
3549,236460261,Further analyzing our model's performance on the test set we observed that it is essential for the model to not only detect toxic spans but also decide if it needs to predict toxic spans for that sample or not.,23,24
3550,236460261,Further analyzing our model's performance on the test set we observed that it is essential for the model to not only detect toxic spans but also decide if it needs to predict toxic spans for that sample or not.,33,34
3551,236460261,Background Identification of toxic/offensive content is an important task in natural language processing.,3,4
3552,236460261,2019) covers several attacks to by-pass toxic content filters and methods to make the filters robust to such attacks.,9,10
3553,236460261,Identifying toxic content is an important NLP task.,1,2
3554,236460261,Most problems deal with labeling the entire content as toxic/nontoxic.,9,10
3555,236460261,None of the previous work has tried to identify spans within a text that makes it toxic.,16,17
3556,236460261,"SemEval-2021 Task 5: Toxic Spans Detection aims to bring attention to this problem via the task defined as: Given a dataset D of sentences, the objective of the task is to learn a classification function that can predict the toxic spans T present in the given sentence.",42,43
3557,236460261,Dataset statistics: The dataset for the task consisted of character offsets for toxic spans present for each text sample.,13,14
3558,236460261,Table 1 shows the count of samples having different number of toxic words.,11,12
3559,236460261,From Table 1 we can infer that samples with toxic words within the range of one to three form a major component of the dataset.,9,10
3560,236460261,"In the test set, samples with no toxic words were significantly more than the training and development set.",8,9
3561,236460261,"We observed that toxic words contained stopwords (the, a, and, of) which are generally not toxic when used independently.",3,4
3562,236460261,"We observed that toxic words contained stopwords (the, a, and, of) which are generally not toxic when used independently.",20,21
3563,236460261,These stopwords can exist as part of multiword toxic spans.,8,9
3564,236460261,Modelling as Token Classification Task The given dataset provided spans of toxic content in a statement.,11,12
3565,236460261,Each sentence could contain multiple toxic spans.,5,6
3566,236460261,Another important thing to note was that a toxic span could comprise more than one word.,8,9
3567,236460261,We extracted all toxic words using the toxic spans.,3,4
3568,236460261,We extracted all toxic words using the toxic spans.,7,8
3569,236460261,"Once we found all the toxic words, we split the original sentence to label the toxic/non-toxic words.",5,6
3570,236460261,"Once we found all the toxic words, we split the original sentence to label the toxic/non-toxic words.",16,17
3571,236460261,"Once we found all the toxic words, we split the original sentence to label the toxic/non-toxic words.",20,21
3572,236460261,"The toxic spans have been highlighted in red in the original sentence which, we convert into an array of words labeled as toxic/non-toxic.",1,2
3573,236460261,"The toxic spans have been highlighted in red in the original sentence which, we convert into an array of words labeled as toxic/non-toxic.",23,24
3574,236460261,"The toxic spans have been highlighted in red in the original sentence which, we convert into an array of words labeled as toxic/non-toxic.",27,28
3575,236460261,The final layer was a time-distributed dense layer over features of each tokenized word containing a single neuron and a sigmoid activation to predict if the given token is toxic/non-toxic.,31,32
3576,236460261,The final layer was a time-distributed dense layer over features of each tokenized word containing a single neuron and a sigmoid activation to predict if the given token is toxic/non-toxic.,35,36
3577,236460261,Predicting Toxic Span Offsets Our model was trained to find the toxic words.,11,12
3578,236460261,"In case the word was tokenized into sub-words, we used the first sub-word to determine the toxic nature of the entire word.",21,22
3579,236460261,"Once we found the toxic words, we searched for them in the original un-processed sentences.",4,5
3580,236460261,We concatenated the spans for all predicted toxic words which was the final expected output.,7,8
3581,236460261,"If S t G = 0 i.e no toxic spans are present in t then F t 1 (A i , G) = 1 if |S t A i | = 0 else F t 1 (A i , G) = 0.",8,9
3582,236460261,We evaluated the performance of our model on samples containing any number of toxic words vs no toxic words.,13,14
3583,236460261,We evaluated the performance of our model on samples containing any number of toxic words vs no toxic words.,17,18
3584,236460261,"We found that our models performed significantly well for samples having one or more toxic words present and, our best performing model had a perfect F1 score on 66.06 % of them.",14,15
3585,236460261,Our model was unable to find toxic words in only 5.97% of samples containing one or more than one toxic word.,6,7
3586,236460261,Our model was unable to find toxic words in only 5.97% of samples containing one or more than one toxic word.,20,21
3587,236460261,"Results and analysis In the case of samples that had no toxic words in a sample, our model could not perform well.",11,12
3588,236460261,Only 6.09% of samples with no toxic words were classified correctly.,7,8
3589,236460261,The dataset statistics for the test set show that samples with no toxic words constitute 19.7 % of the test set.,12,13
3590,236460261,The training and development set had only 6.12% and 6.23% samples without any toxic words.,15,16
3591,236460261,We also found the top 15 most common words which were predicted as toxic from samples containing no toxic words in the test set.,13,14
3592,236460261,We also found the top 15 most common words which were predicted as toxic from samples containing no toxic words in the test set.,18,19
3593,236460261,We trained our model using token classification objective which tries to capture toxic words.,12,13
3594,236460261,The model cannot identify if the word is part of a toxic/non-toxic sentence.,12,13
3595,236460261,The model cannot identify if the word is part of a toxic/non-toxic sentence.,16,17
3596,236460261,This may lead the model to incorrectly identify toxic words in samples containing no toxic spans.,8,9
3597,236460261,This may lead the model to incorrectly identify toxic words in samples containing no toxic spans.,14,15
3598,236460261,We propose a word-level classifier for identifying the toxic words in a sentence.,10,11
3599,236460261,We experimented with different PLMs to provide a comprehensive analysis of their performance for identifying toxic spans.,15,16
3600,236460261,Our analysis shows that a word-level classifier performs extremely well for sentences that contain at least one toxic word.,19,20
3601,236460261,"However, it cannot identify cases with no toxic spans efficiently.",9,10
3602,236460261,"In the future, we would like to work on solving this problem by using a classifier to simply predict if the sentence is toxic/non-toxic along with span detection.",24,25
3603,236460261,"In the future, we would like to work on solving this problem by using a classifier to simply predict if the sentence is toxic/non-toxic along with span detection.",28,29
3604,236459824,"In this paper, I use data consisting of comments with the indices of toxic text labelled to train an RNN to determine which parts of the comments make them toxic, which could aid online moderators.",14,15
3605,236459824,"In this paper, I use data consisting of comments with the indices of toxic text labelled to train an RNN to determine which parts of the comments make them toxic, which could aid online moderators.",30,31
3606,236459824,"Aside from discouraging users to continue with or join conversations, toxic comments can also taint users' perceptions on news sites (Tenenboim et al.,",11,12
3607,236459824,"Some methods rely on simply classifying whether a comment is toxic or not, but identifying what parts of the text are actually toxic can assist moderators and provide insight into what makes language toxic.",10,11
3608,236459824,"Some methods rely on simply classifying whether a comment is toxic or not, but identifying what parts of the text are actually toxic can assist moderators and provide insight into what makes language toxic.",23,24
3609,236459824,"Some methods rely on simply classifying whether a comment is toxic or not, but identifying what parts of the text are actually toxic can assist moderators and provide insight into what makes language toxic.",34,35
3610,236459824,"The SemEval task 5 aims to evaluate systems that detect toxic spans wihtin text using datasets where spans within the comments are labelled as toxic, differing from previously released datasets where whole comments were labelled as toxic or non-toxic (Pavlopoulos et al.,",10,11
3611,236459824,"The SemEval task 5 aims to evaluate systems that detect toxic spans wihtin text using datasets where spans within the comments are labelled as toxic, differing from previously released datasets where whole comments were labelled as toxic or non-toxic (Pavlopoulos et al.,",24,25
3612,236459824,"The SemEval task 5 aims to evaluate systems that detect toxic spans wihtin text using datasets where spans within the comments are labelled as toxic, differing from previously released datasets where whole comments were labelled as toxic or non-toxic (Pavlopoulos et al.,",37,38
3613,236459824,"The SemEval task 5 aims to evaluate systems that detect toxic spans wihtin text using datasets where spans within the comments are labelled as toxic, differing from previously released datasets where whole comments were labelled as toxic or non-toxic (Pavlopoulos et al.,",41,42
3614,236459824,This study focuses on training a recurrent neural network to determine the indices of a given string that represent the toxic portions of a comment.,20,21
3615,236459824,"Related Work Aggression in text is complex, often clouded by sarcasm or including repeat words that cause a model to incorrectly identify words as toxic.",25,26
3616,236459824,"A study by Vaidya, Mai, and Ning found that comments including identities, such as LGBTQ+, Black, Muslim, and/or Jewish identities, often resulted in false positives for toxic comments, so this was a bias we wanted to be aware of in our study (Vaidya et al.,",33,34
3617,236459824,Detecting toxic spans is not as common of a task as toxic comment detection or sentiment analysis.,1,2
3618,236459824,Detecting toxic spans is not as common of a task as toxic comment detection or sentiment analysis.,11,12
3619,236459824,"Many studies surrounding toxic comments have been completed largely in part due to the availability of a large corpora of data released by the Wikimedia Foundation, as well as several Kaggle competitions hosted by Google Jigsaw.",3,4
3620,236459824,"First, we label the comments using the given indices representing the toxic spans within the comments.",12,13
3621,236459824,"A word labelled with a ""/1"" is toxic and with ""/0"" is not toxic.",9,10
3622,236459824,"A word labelled with a ""/1"" is toxic and with ""/0"" is not toxic.",17,18
3623,236459824,"Instead of using these unknown embeddings in training and scoring using a neural network, we will make simple predictions based on if any common toxic words are present in the comments.",25,26
3624,236459824,The comments removed from the main set are predicted using common toxic words gathered from the training set.,11,12
3625,236459824,"Although misspellings are often a cause for the embeddings failing on a comment, the misspelled word may not have been part of the toxic span, so the comment could still be predicted using common toxic words and have a fairly acceptable accuracy.",24,25
3626,236459824,"Although misspellings are often a cause for the embeddings failing on a comment, the misspelled word may not have been part of the toxic span, so the comment could still be predicted using common toxic words and have a fairly acceptable accuracy.",36,37
3627,236459824,"When training, the embeddings columns are used as input columns to the model, where each token is 100 columns for each 100 dimension embedding, and the target columns are the label columns that contain either a 1 for a toxic word or a 0 for a non-toxic word, where each token is one column.",42,43
3628,236459824,"When training, the embeddings columns are used as input columns to the model, where each token is 100 columns for each 100 dimension embedding, and the target columns are the label columns that contain either a 1 for a toxic word or a 0 for a non-toxic word, where each token is one column.",51,52
3629,236459824,"To do this, we found every comment with only one toxic word and created up to five new comments with the toxic word replaced with a different synonym.",11,12
3630,236459824,"To do this, we found every comment with only one toxic word and created up to five new comments with the toxic word replaced with a different synonym.",22,23
3631,236459824,"The F1 scores, discussed later on, are a combination of both the predictions from the trained models and the comments that are predicted using common toxic words (which will henceforth be referenced as ""guessed comments"" for lack of better terms).",27,28
3632,236459824,"If the system is represented by A i , the return set from the system is S t A i , t represents a post or comment, and G represents the ground truth annotations for post t, then F1 score of a system is defined as: F t 1 (A i , G) = 2 • P t (A i , G) • R t (A i , G) P t (A i , G) + R t (A i , G) (1) P t (A i , G) = |S t A i ∩ S t G | S t A i (2) R t (A i , G) = |S t A i ∪ S t G | S t A i (3) If S t G = 0, an instance where there are no toxic spans present, then F t 1 (A i , G) = 1 if no toxic spans are predicted, and F t 1 (A i , G) = 0 otherwise.",161,162
3633,236459824,"If the system is represented by A i , the return set from the system is S t A i , t represents a post or comment, and G represents the ground truth annotations for post t, then F1 score of a system is defined as: F t 1 (A i , G) = 2 • P t (A i , G) • R t (A i , G) P t (A i , G) + R t (A i , G) (1) P t (A i , G) = |S t A i ∩ S t G | S t A i (2) R t (A i , G) = |S t A i ∪ S t G | S t A i (3) If S t G = 0, an instance where there are no toxic spans present, then F t 1 (A i , G) = 1 if no toxic spans are predicted, and F t 1 (A i , G) = 0 otherwise.",179,180
3634,236459824,"It is possible that the method in which we augmented the data, only changing one word within a comment, did not help the model learn any of the connections between the text but rather memorize more toxic words, many of which are often repeated.",38,39
3635,236459824,"We also compared results from only comments with a single toxic word present to find if these performed better for the augmented data, since only comments with a single toxic word were augmented.",10,11
3636,236459824,"We also compared results from only comments with a single toxic word present to find if these performed better for the augmented data, since only comments with a single toxic word were augmented.",30,31
3637,236459824,"The differences between the single toxic word scores and scores when all comments are factored in are also very similar between augmented and non-augmented data results, showing the augmented data did not make much of a difference in predicting single toxic word comments.",5,6
3638,236459824,"The differences between the single toxic word scores and scores when all comments are factored in are also very similar between augmented and non-augmented data results, showing the augmented data did not make much of a difference in predicting single toxic word comments.",43,44
3639,236459824,"This is peculiar but is also likely due to the evaluation data containing fewer guessed comments, or comments predicted using common toxic words instead of predicted by the model.",22,23
3640,236459824,"Looking into the dev dataset, only 6% of the observations contain no toxic spans, but this model is not predicting whether an entire comment contains any toxic spans, it is predicting if each word is toxic.",14,15
3641,236459824,"Looking into the dev dataset, only 6% of the observations contain no toxic spans, but this model is not predicting whether an entire comment contains any toxic spans, it is predicting if each word is toxic.",29,30
3642,236459824,"Looking into the dev dataset, only 6% of the observations contain no toxic spans, but this model is not predicting whether an entire comment contains any toxic spans, it is predicting if each word is toxic.",39,40
3643,236459824,"Out of all of the text, 93% of the words are non-toxic words, or in this case, words to be labelled negative for toxicity.",15,16
3644,236459824,"We compared if the F1 scores for observations a single toxic word were higher for the models trained with augmented data, since only comments with a single toxic word present were augmented, but still the original datasets outperformed the augmented datasets.",10,11
3645,236459824,"We compared if the F1 scores for observations a single toxic word were higher for the models trained with augmented data, since only comments with a single toxic word present were augmented, but still the original datasets outperformed the augmented datasets.",28,29
3646,232478589,"The purpose of this task is to detect the spans that make a text toxic, which is a complex labour for several reasons.",14,15
3647,232478589,"Firstly, because of the intrinsic subjectivity of toxicity, and secondly, due to toxicity not always coming from single words like insults or offends, but sometimes from whole expressions formed by words that may not be toxic individually.",39,40
3648,232478589,2021) consists in detecting which spans make a text toxic.,10,11
3649,232478589,"To tackle this problem, in HLE-UPC we have used a BERT-based model with a fully-connected layer on top to perform Named-Entity Recognition and Classification (NERC), with the goal of tagging each word as either toxic or not.",46,47
3650,232478589,"Toxic Spans Detection, however, goes a step further by asking participants to detect toxic spans, the exact characters or words that make a text toxic.",15,16
3651,232478589,"Toxic Spans Detection, however, goes a step further by asking participants to detect toxic spans, the exact characters or words that make a text toxic.",27,28
3652,232478589,"More specifically, this task could be seen as a Named Entity Recognition and Classification (NERC) task, in which the goal would be to output the most probable sequence of labels (toxic or not) given an input sentence.",35,36
3653,232478589,"2021) , containing phrases and comments that may contain toxic spans.",10,11
3654,232478589,"Together with each comment, there is the set of indices of the characters that are considered toxic.",17,18
3655,232478589,"There are also some words that have been written in an ingenious way, to avoid naïve toxic detectors, or that are bleeped or censored.",17,18
3656,232478589,"Following we present a couple of examples, where toxic characters are underlined: • This is a stupid example, so thank you for nothing a!@#!@. • I bet you can't wait to see him behind bars.",9,10
3657,232478589,"Data Cleaning With a simple data exploration, it can be seen that approximately 90% of the toxic spans exactly match with word boundaries, but in the remaining cases we find strange cases such as the following ones: 1.",18,19
3658,232478589,You are an idiot: There is a whitespace as a toxic span boundary.,11,12
3659,232478589,You are an idiot: A random singleton character is marked as toxic.,12,13
3660,232478589,"You are an idiot: ""Y"" is not marked as toxic but ""ou"" is.",12,13
3661,232478589,For each group of consecutive annotated toxic offsets: 1.,6,7
3662,232478589,Iteratively remove the first or last toxic offset if it belongs to a whitespace.,6,7
3663,232478589,Remove the toxic offset if it is a singleton: a single consecutive character marked as toxic.,2,3
3664,232478589,Remove the toxic offset if it is a singleton: a single consecutive character marked as toxic.,16,17
3665,232478589,Iteratively left-expand the range of toxic offsets if the previous character is alphanumeric (so it belongs to the same word).,7,8
3666,232478589,This solves the third problem by including the offsets of the whole word as toxic whenever more than one character is marked as so.,14,15
3667,232478589,"In this step, we also use the information of the already-cleaned toxic offsets to create a per-token binary label regarding its toxicity.",14,15
3668,232478589,"Postprocessing Once a model outputs its predictions, we loop through them and, for those tokens predicted as toxic, we take their offsets and add them to the final set of toxic spans for that sentence.",19,20
3669,232478589,"Postprocessing Once a model outputs its predictions, we loop through them and, for those tokens predicted as toxic, we take their offsets and add them to the final set of toxic spans for that sentence.",33,34
3670,232478589,"For this reason, for each pair of consecutive tokens predicted as toxic, we also include to the final set the offsets of any white characters in between.",12,13
3671,232478589,"Results All the results presented in this section have been calculated using the official metric, the F1-score on the predicted toxic offsets.",23,24
3672,232478589,"Qualitative Apart from the quantitative analysis done before, we analyze in a qualitative manner the performance and behaviour of our best model, to see how well detects offensive and toxic words and in which cases it fails.",31,32
3673,232478589,The ground truth toxic words appear underlined while the prediction is shown in red.,3,4
3674,232478589,"Correct predictions We observe how our system is highly capable of identifying toxic and offensive words, both when they appear alone and in multi-word expressions. •",12,13
3675,232478589,"As seen below with the word ""poorly"", our method misses some words marked as toxic which are not very offensive or disrespectful but can become toxic due to the context. •",17,18
3676,232478589,"As seen below with the word ""poorly"", our method misses some words marked as toxic which are not very offensive or disrespectful but can become toxic due to the context. •",28,29
3677,232478589,"I'm so sick and tired of... Finally, our model fails to detect connectors such as ""of"" and ""and"" in between toxic words.",27,28
3678,232478589,"In the dataset there are several annotation philosophies: some annotations tend to mark entire expressions as toxic and some others are more word-oriented, excluding connectors between words. •",17,18
3679,232478589,"We observe that in both cases the system identifies the word ""black"" as toxic, but not ""white"", even when these non-toxic adjectives are the only difference between them.",15,16
3680,232478589,"We observe that in both cases the system identifies the word ""black"" as toxic, but not ""white"", even when these non-toxic adjectives are the only difference between them.",28,29
3681,232478589,"Furthermore, the system only identifies ""immigrants"" as toxic when appearing next to ""Mexican"" but not with ""American"".",10,11
3682,232478589,"This undesired discrimination happens because there are lots of racist comments in the dataset, which are obviously annotated as toxic.",20,21
3683,236460217,"In the shared task of detecting toxic spans in texts, we first apply pretrained word embedding (GloVe) to generate the word vectors after tokenization.",6,7
3684,236460217,Then we construct Bidirectional Long Short Term Memory-Conditional Random Field (Bi-LSTM-CRF) model by Baidu research to predict whether each word in the sentence is toxic or not.,32,33
3685,236460217,Introduction Detecting toxic words plays a critical role in social media to ensure healthy online discussions.,2,3
3686,236460217,"Each post is in string format, and a word is marked as toxic span in the form of its characters' offsets in the string.",13,14
3687,236460217,The goal of the task is to classify whether each word in a sentence is toxic or not.,15,16
3688,236460217,We need to predict which word or phrase is toxic given a text (many-to-many) rather than whether the entire sentence is offensive or not (many-to-one).,9,10
3689,236460217,Most of the words and phrases in sentences are not toxic.,10,11
3690,236460217,"2013) , GloVe (Pennington et To obtain if a word is toxic or not after tokenization, we split the text by space and punctuation and map the indices of toxicity to corresponding words.",13,14
3691,236460217,"As a result, the word sequences in text will be marked as toxic (1) or non-toxic (0).",13,14
3692,236460217,"As a result, the word sequences in text will be marked as toxic (1) or non-toxic (0).",20,21
3693,236460217,The count of toxic and non-toxic words in texts are concluded in Table 2 .,3,4
3694,236460217,The count of toxic and non-toxic words in texts are concluded in Table 2 .,7,8
3695,236460217,It shows the dataset is highly imbalanced that most of words are non toxic.,13,14
3696,236460217,"Since the dataset is highly imbalanced, we only focus on the evaluation metrics of toxic words.",15,16
3697,236460217,"In false positive examples marked as underline, the words ""ass"", ""sucks"", and ""moron"" are predicted as toxic words where there exists no toxicity in these sentences.",25,26
3698,236460217,"In false negative examples marked as bold, the model fails to identify toxic words like ""amok"", ""vandals"", and ""thieves"".",13,14
3699,236460217,"For example, one word could be marked as toxic spans when it is in the beginning of the sentence but not the case when it is at the end.",9,10
3700,236460217,Conclusion Detecting toxic words in texts is critical to furnish a healthy environment on social media.,2,3
3701,236460217,"However, there are still a lot of false positive examples in test set where the model predicts toxic words that in fact are not toxic.",18,19
3702,236460217,"However, there are still a lot of false positive examples in test set where the model predicts toxic words that in fact are not toxic.",25,26
3703,236460217,"For feature engineering, we can conduct data augmentation for false negative examples: We first collect the words that are predicted as non-toxic but actually toxic, and reconstruct sentences using those toxic words as more training samples.",25,26
3704,236460217,"For feature engineering, we can conduct data augmentation for false negative examples: We first collect the words that are predicted as non-toxic but actually toxic, and reconstruct sentences using those toxic words as more training samples.",28,29
3705,236460217,"For feature engineering, we can conduct data augmentation for false negative examples: We first collect the words that are predicted as non-toxic but actually toxic, and reconstruct sentences using those toxic words as more training samples.",35,36
3706,236460146,This paper describes the system developed by the Antwerp Centre for Digital humanities and literary Criticism [UAntwerp] for toxic span detection.,20,21
3707,236460146,"Two models attempted to predict binary word toxicity based on ngram sequences, whilst 3 categorical span based models were trained to predict toxic token labels based on complete sequence tokens.",23,24
3708,236460146,Competing teams were asked to develop systems capable of detecting spans of toxic text.,12,13
3709,236460146,"Predictions were evaluated using a pairwise F1-score of toxic character offset predictions, described in section 5.1.",10,11
3710,236460146,Initial analysis of the development data revealed that toxic spans were varied in content and not limited to single words.,8,9
3711,236460146,"Though most examples contained single toxic words or phrases, others contained longer spans and complete sentences.",5,6
3712,236460146,"Table 1 reveals that toxic spans were on average 3 times longer in the development set, whilst stop words were 4 times more frequent.",4,5
3713,236460146,"Target Span detection asks systems to detect which specific series of characters are toxic, irrespective of the text's overall toxicity.",13,14
3714,236460146,"Lexical Lookup Using a subset of samples from the development data, we created a toxic words list from all words within toxic spans, except for stop words 1 .",15,16
3715,236460146,"Lexical Lookup Using a subset of samples from the development data, we created a toxic words list from all words within toxic spans, except for stop words 1 .",22,23
3716,236460146,"On the test data, we then classified words as toxic if they appeared within the aforementioned toxic words list.",10,11
3717,236460146,"On the test data, we then classified words as toxic if they appeared within the aforementioned toxic words list.",17,18
3718,236460146,SVM Using Term Frequency to Inverse Document Frequency we created two document vector representations of toxic and non-toxic spans.,15,16
3719,236460146,SVM Using Term Frequency to Inverse Document Frequency we created two document vector representations of toxic and non-toxic spans.,19,20
3720,236460146,"Using a Support Vector Machine, we predicted the probability that a word vector appeared within a toxic or non-toxic document (Salton and McGill, 1986; Wu et al.) .",17,18
3721,236460146,"Using a Support Vector Machine, we predicted the probability that a word vector appeared within a toxic or non-toxic document (Salton and McGill, 1986; Wu et al.) .",21,22
3722,236460146,We then used a binary threshold of 0.5 and class weights based on relative label frequency to predict whether a word was toxic.,22,23
3723,236460146,"Component Models Span Prediction Span prediction models used the complete sequence of words, up to a maximum length, to pre- dict toxic character offsets.",23,24
3724,236460146,"The target sequence was processed from character offsets into categorical arrays for toxic, non-toxic, and padding tokens.",12,13
3725,236460146,"The target sequence was processed from character offsets into categorical arrays for toxic, non-toxic, and padding tokens.",16,17
3726,236460146,"Component model Predictions Component model predictions were concatenated together as categorical representations of labels (not toxic, toxic, padding : 0,1,2).",16,17
3727,236460146,"Component model Predictions Component model predictions were concatenated together as categorical representations of labels (not toxic, toxic, padding : 0,1,2).",18,19
3728,236460146,"Error Analysis We performed error analysis to interpret the hypothesis that there are multiple annotation rationales; single toxic words, and longer offensive sentences, illustrated in Figure 1 .",18,19
3729,236460146,Toxic Span Length Figure 8 reveals that the length of toxic spans had an impact on model performance.,10,11
3730,236460146,"Furthermore, the impact of this effect on test data was decreased as there were fewer longer toxic spans.",17,18
3731,236460146,"Here, the target labels are represented as binary arrays; 1 for toxic tokens and 0 for non-toxic.",13,14
3732,236460146,"Here, the target labels are represented as binary arrays; 1 for toxic tokens and 0 for non-toxic.",20,21
3733,5965126,The carpets won't be glued down and walls will be coated with non-toxic finishes.,15,16
3734,5769963,Let the used a toxic chemical as a weapon X4 X3 X2 X1 X5 X6 X7 Standard structure A toxic chemical used as a weapon X3 X2 X1 X4g X5 X6 X7 Rising structure Figure 5 : A parse with and without rising.,4,5
3735,5769963,Let the used a toxic chemical as a weapon X4 X3 X2 X1 X5 X6 X7 Standard structure A toxic chemical used as a weapon X3 X2 X1 X4g X5 X6 X7 Rising structure Figure 5 : A parse with and without rising.,19,20
3736,5769963,5 'used' licenses 'a toxic chemical' e.g. 'used what?').,7,8
3737,236459808,"2019) , and do not identify ""spans""-that is, the precise word sequences that make a text toxic.",19,20
3738,236459808,"Thus far, we have concentrated on the combination of two approaches: a lexicon-based approach and a supervised learning approach to identify toxic spans.",25,26
3739,236459808,"Although the identification of toxic spans in online posts can be aided by a suitable lexicon of toxic words, such words can easily be concealed through minor changes-for instance, ""fck urself"" is a toxic span that would evade detection based on basic lists of profane words.",4,5
3740,236459808,"Although the identification of toxic spans in online posts can be aided by a suitable lexicon of toxic words, such words can easily be concealed through minor changes-for instance, ""fck urself"" is a toxic span that would evade detection based on basic lists of profane words.",17,18
3741,236459808,"Although the identification of toxic spans in online posts can be aided by a suitable lexicon of toxic words, such words can easily be concealed through minor changes-for instance, ""fck urself"" is a toxic span that would evade detection based on basic lists of profane words.",39,40
3742,236459808,Section 4 is dedicated to explain our algorithm for the identification of toxic spans.,12,13
3743,236459808,"2020; Waseem, 2016) ; and the automatic detection of different types of toxic text.",15,16
3744,236459808,"Among the different types of toxic text under scrutiny, we may include hate speech (Badjatiya et al.,",5,6
3745,236459808,"To overcome this weakness, we have integrated into our research the use of a lexicon-based approach, where toxic language is identified with the help of a dictionary of words associated with toxic text (De Smedt et al.,",21,22
3746,236459808,"To overcome this weakness, we have integrated into our research the use of a lexicon-based approach, where toxic language is identified with the help of a dictionary of words associated with toxic text (De Smedt et al.,",35,36
3747,236459808,"Such a dataset comprises annotations indicating which entire posts are toxic, but it does not label particular toxic spans within the posts.",10,11
3748,236459808,"Such a dataset comprises annotations indicating which entire posts are toxic, but it does not label particular toxic spans within the posts.",18,19
3749,236459808,"To build the dataset, SemEval retained only posts that were found toxic-or severely toxic-by at least half of the annotators involved in Borkan, et al.",12,13
3750,236459808,"To build the dataset, SemEval retained only posts that were found toxic-or severely toxic-by at least half of the annotators involved in Borkan, et al.",16,17
3751,236459808,"This comprises 30k toxic posts, approximately, out of the original 1.2M. Then, a random subset of 10k posts from these 30k toxic posts were chosen for toxic spans annotation (CodaLab, 2021) .",3,4
3752,236459808,"This comprises 30k toxic posts, approximately, out of the original 1.2M. Then, a random subset of 10k posts from these 30k toxic posts were chosen for toxic spans annotation (CodaLab, 2021) .",24,25
3753,236459808,"This comprises 30k toxic posts, approximately, out of the original 1.2M. Then, a random subset of 10k posts from these 30k toxic posts were chosen for toxic spans annotation (CodaLab, 2021) .",29,30
3754,236459808,"System Overview Although machine learning technology is being widely employed to detect toxic text automatically, the use of a lexicon to identify and prevent toxicity in social media still constitutes a valuable approach.",12,13
3755,236459808,"Hence, we employ a lexicon as our first step in the detection of toxic spans.",14,15
3756,236459808,"Originally, our lexicon was made, specifically, for Task 5 of SemEval 2021, as we compiled it by extracting all the toxic words available in the training and trial datasets for Task 5-we considered a word as a toxic word if it was included in a toxic span identified by the annotators.",24,25
3757,236459808,"Originally, our lexicon was made, specifically, for Task 5 of SemEval 2021, as we compiled it by extracting all the toxic words available in the training and trial datasets for Task 5-we considered a word as a toxic word if it was included in a toxic span identified by the annotators.",43,44
3758,236459808,"Originally, our lexicon was made, specifically, for Task 5 of SemEval 2021, as we compiled it by extracting all the toxic words available in the training and trial datasets for Task 5-we considered a word as a toxic word if it was included in a toxic span identified by the annotators.",51,52
3759,236459808,"Upon compiling all the toxic words available in the training and trial datasets (1,287 words), we proceeded to extend our lexicon with words listed in other lexicons.",4,5
3760,236459808,"While there are many freelyavailable lexicons of toxic words, we favoured those that maintained the accuracy of the detection of toxic spans achieved by our lexicon.",7,8
3761,236459808,"While there are many freelyavailable lexicons of toxic words, we favoured those that maintained the accuracy of the detection of toxic spans achieved by our lexicon.",21,22
3762,236459808,The first row of Table 1 refers to the lexicon we created after compiling all the toxic words available in the training and trial datasets of Task 5 of SemEval 2021-we named this lexicon the Task 5 Lexicon.,16,17
3763,236459808,"Lexicon As shown in After creating our lexicon, we manually removed from it words that were part of the toxic spans annotated in Task 5 of SemEval 2021, but were not included in the three lexicons displayed in bold font in Table 1 .",20,21
3764,236459808,"For example, the word ""mistake"" located in the post ""They elected Trump, which was certainly a mistake"" was considered toxic by the annotators, in the context of the post.",25,26
3765,236459808,"For example, the post ""uh, no, he's a belligerent buffoon (and a traitor)"", which is post 1,928 of the training dataset of Task 5 of SemEval 2021, lacks any recognisable toxic features, such as insults or swear words.",40,41
3766,236459808,"Hence, it is classified as non-toxic by any of the lexicons highlighted in bold font in Table 1.",8,9
3767,236459808,"Moreover, this post does not have any toxic spans marked by the annotators.",8,9
3768,236459808,"Nevertheless, the negative sentiment of ""belligerent buffoon"" and ""traitor""-words which are not typically found in any abusive word list-guarantees that the message conveyed is definitely toxic; otherwise, it would not be part of the training dataset.",31,32
3769,236459808,The expression of anger is so evident in this case that the post can be marked as a candidate to be considered toxic.,22,23
3770,236459808,"Conclusions In this paper, we have described the creation of a lexicon of toxic words and a supervised learning approach to identify toxicity in online posts.",14,15
3771,235248423,Existing work on toxic speech detection focuses on binary classification or on differentiating toxic speech among a small set of categories.,3,4
3772,235248423,Existing work on toxic speech detection focuses on binary classification or on differentiating toxic speech among a small set of categories.,13,14
3773,235248423,Introduction It only takes one toxic comment to sour an online discussion.,5,6
3774,235248423,"2020) , toxic speech or spans in this particular task, SemEval-2021 Task 5 (Pavlopoulos et al.,",3,4
3775,235248423,"The models tend to classify comments as toxic that have a reference to certain commonly-attacked entities (e.g. gay, black, Muslim, immigrants) without the comment having any intention to be toxic (Dixon et al.,",7,8
3776,235248423,"The models tend to classify comments as toxic that have a reference to certain commonly-attacked entities (e.g. gay, black, Muslim, immigrants) without the comment having any intention to be toxic (Dixon et al.,",36,37
3777,235248423,"Thus, it has become increasingly important in recent times to determine parts of the text that attribute to the toxic nature of the sentence, for both automated and semi-automated content moderation on social media platforms, primarily for the purpose of helping human moderators deal with lengthy comments and also provide them attributions for better explainability on the toxic nature of the post.",20,21
3778,235248423,"Thus, it has become increasingly important in recent times to determine parts of the text that attribute to the toxic nature of the sentence, for both automated and semi-automated content moderation on social media platforms, primarily for the purpose of helping human moderators deal with lengthy comments and also provide them attributions for better explainability on the toxic nature of the post.",62,63
3779,235248423,This in turn would aid in better handling of unintended bias in toxic text classification.,12,13
3780,235248423,SemEval-2021 Task 5: Toxic Spans Detection focuses on exactly this problem of detecting toxic spans from sentences already classified as toxic on a post-level.,14,15
3781,235248423,SemEval-2021 Task 5: Toxic Spans Detection focuses on exactly this problem of detecting toxic spans from sentences already classified as toxic on a post-level.,21,22
3782,235248423,"In this paper, we approach the problem of multiple non-contiguous toxic span extraction from texts both as a sequence tagging task and as a standard span extraction task resembling the generic approach and architecture adopted for single-span Reading Comprehension (RC) task.",13,14
3783,235248423,"Literature Previous work on automated toxic text detection, and its various sub-types, focuses on developing classifiers that can flag toxic content with a high degree of accuracy on datasets curated from various social media platforms in English (Carta et al.,",5,6
3784,235248423,"Literature Previous work on automated toxic text detection, and its various sub-types, focuses on developing classifiers that can flag toxic content with a high degree of accuracy on datasets curated from various social media platforms in English (Carta et al.,",23,24
3785,235248423,"Attempts have also been made to handle identity bias in toxic text classification (Vaidya et al.,",10,11
3786,235248423,"2020) and also to make robust toxic text classifiers which help adversaries not bypass toxic filters (Kurita et al.,",7,8
3787,235248423,"2020) and also to make robust toxic text classifiers which help adversaries not bypass toxic filters (Kurita et al.,",15,16
3788,235248423,"With this approach, we extract toxic spans from sentences under the supervision of target span boundaries, but with an added biaffine model for scoring the multiple toxic spans instead of simply taking top k spans based on the start and end probabilities, thus giving our model a global view of the input.",6,7
3789,235248423,"With this approach, we extract toxic spans from sentences under the supervision of target span boundaries, but with an added biaffine model for scoring the multiple toxic spans instead of simply taking top k spans based on the start and end probabilities, thus giving our model a global view of the input.",28,29
3790,235248423,"Given an input sentence x = (x 1 ,...,x n ), of length n, we predict a target list T = (t 1 ,...,t m ) where the number of targets is m and each target t i is annotated with its start position s i , its end position e i and the class that span belongs to (only one in our case, toxic).",76,77
3791,235248423,"Dataset The dataset provided to us by the organizers of the workshop consisted of a random subset of 10,000 posts from the publicly available Civil Comments Dataset, from a set of 30,000 posts originally annotated as toxic (or severely toxic) on post-level annotations, manually annotated by 3 crowd-raters per post for toxic spans.",37,38
3792,235248423,"Dataset The dataset provided to us by the organizers of the workshop consisted of a random subset of 10,000 posts from the publicly available Civil Comments Dataset, from a set of 30,000 posts originally annotated as toxic (or severely toxic) on post-level annotations, manually annotated by 3 crowd-raters per post for toxic spans.",41,42
3793,235248423,"Dataset The dataset provided to us by the organizers of the workshop consisted of a random subset of 10,000 posts from the publicly available Civil Comments Dataset, from a set of 30,000 posts originally annotated as toxic (or severely toxic) on post-level annotations, manually annotated by 3 crowd-raters per post for toxic spans.",59,60
3794,235248423,"The final character offsets were obtained by retaining the offsets with a probability of more than 50%, computed as a fraction of raters who annotated the character offsets as toxic.",31,32
3795,235248423,Experimental Setup Data was originally provided to us in the form of sentences and the corresponding character offsets for the toxic spans of the sentence.,20,21
3796,235248423,"Model Span length F1 Learning context Majority of single word spans in the dataset are the most commonly used cuss words or abusive words in the English language, i.e., words that can be directly classified as toxic and are not contextdependant, e.g. ""stupid"",""idiot"" etc.,",38,39
3797,235248423,"Second, we take the word ""black"" and analyze two sentences in our test where the word black was mentioned in a toxic and non-toxic context.",24,25
3798,235248423,"Second, we take the word ""black"" and analyze two sentences in our test where the word black was mentioned in a toxic and non-toxic context.",28,29
3799,235248423,5 shows how our model indeed tags the latter black as toxic and the former one as non-toxic.,11,12
3800,235248423,5 shows how our model indeed tags the latter black as toxic and the former one as non-toxic.,19,20
3801,235248423,"Future work includes independently incorporating both post level and sentence level context for determining the toxicity of a word, and also collating a dataset with toxic spans comprising of a healthy mixture of simple cuss words (which can always be attributed as toxic independant of the context) and words for which the toxicity of the word depends on the context in which it appears, thereby making better systems towards contextual toxic span detection.",26,27
3802,235248423,"Future work includes independently incorporating both post level and sentence level context for determining the toxicity of a word, and also collating a dataset with toxic spans comprising of a healthy mixture of simple cuss words (which can always be attributed as toxic independant of the context) and words for which the toxicity of the word depends on the context in which it appears, thereby making better systems towards contextual toxic span detection.",44,45
3803,235248423,"Future work includes independently incorporating both post level and sentence level context for determining the toxicity of a word, and also collating a dataset with toxic spans comprising of a healthy mixture of simple cuss words (which can always be attributed as toxic independant of the context) and words for which the toxicity of the word depends on the context in which it appears, thereby making better systems towards contextual toxic span detection.",74,75
3804,236460342,The competition aims at detecting the spans that make a toxic span toxic.,10,11
3805,236460342,The competition aims at detecting the spans that make a toxic span toxic.,12,13
3806,236460342,"In this paper, we demonstrate our system for detecting toxic spans, which includes expanding the toxic training set with Local Interpretable Model-Agnostic Explanations (LIME), fine-tuning RoBERTa model for detection, and error analysis.",10,11
3807,236460342,"In this paper, we demonstrate our system for detecting toxic spans, which includes expanding the toxic training set with Local Interpretable Model-Agnostic Explanations (LIME), fine-tuning RoBERTa model for detection, and error analysis.",17,18
3808,236460342,We found that feeding the model with an expanded training set using Reddit comments of polarizedtoxicity and labeling with LIME on top of logistic regression classification could help RoBERTa more accurately learn to recognize toxic spans.,34,35
3809,236460342,"However, the part of the message that is specifically toxic is often unknown.",10,11
3810,236460342,Our model uses a deep learning approach to identify which tokens are toxic.,12,13
3811,236460342,"Here, we treat the toxicspan detection tasks as a seq2seq task, where given a sequence of tokens, the model outputs per-token judgments of whether the token is in the toxic span.",34,35
3812,236460342,2016) fropm a model trained to recognize toxic and non-toxic language.,8,9
3813,236460342,2016) fropm a model trained to recognize toxic and non-toxic language.,12,13
3814,236460342,"These additional judgments are intended to help the model learn the basic span recognition task and identify general toxic language, before fine-tuning on the Task 5 data.",18,19
3815,236460342,"Pretraining to Recognize Toxicity To identify toxic spans, we hypothesize that pretraining the RoBERTa model on a similar task would lead to better downstream performance.",6,7
3816,236460342,"Identifying Toxic Comments The Reddit data contains a mix of toxic and non-toxic conversations, which we aim to use for training.",10,11
3817,236460342,"Identifying Toxic Comments The Reddit data contains a mix of toxic and non-toxic conversations, which we aim to use for training.",14,15
3818,236460342,"To identify toxic conversations, we use the Perspective API to label all comments in the dataset (Wulczyn et al.,",2,3
3819,236460342,This toxicity score is then converted into a binary label to use in training a LIME model to generate rationales for why a comment is (or is not) toxic.,30,31
3820,236460342,"2020) and set a threshold of 0.7, above which a comment is considered toxic and 0.3, below which the comment is non-toxic.",15,16
3821,236460342,"2020) and set a threshold of 0.7, above which a comment is considered toxic and 0.3, below which the comment is non-toxic.",26,27
3822,236460342,These thresholds were intended to help create easy examples of toxic language for generating rationales as a way of scaffolding the learning for the downstream task.,10,11
3823,236460342,"This process led to a labeled dataset of 288.5M comments with binary toxicity labels, of which 9.4% were labeled toxic.",22,23
3824,236460342,Generating Heuristically-Labeled Toxic Spans Our final silver dataset is created by sampling comments from the larger labeled Reddit comments and using a LIME model to generate toxic span labels.,28,29
3825,236460342,The underlying LR model is trained on a balanced sample of 800K toxic/non-toxic comments (not the silver data).,13,14
3826,236460342,The underlying LR model is trained on a balanced sample of 800K toxic/non-toxic comments (not the silver data).,17,18
3827,236460342,Our silver data is created by generating LIME explanations using the trained LR classifier on a separate 800K comments balanced between toxic and non-toxic.,22,23
3828,236460342,Our silver data is created by generating LIME explanations using the trained LR classifier on a separate 800K comments balanced between toxic and non-toxic.,26,27
3829,236460342,"To create toxic spans from these continuous-valued weights, we apply a threshold above which we consider the token as the toxic span.",2,3
3830,236460342,"To create toxic spans from these continuous-valued weights, we apply a threshold above which we consider the token as the toxic span.",23,24
3831,236460342,"The threshold was identified by generating LIME explanations for all of 8629 documents from the Task's training data and then choosing the threshold that maximized the Span F1 between the 8629 training documents' toxic spans and the discretized LIME explanations, using a grid search with a step of 0.001 in [0.05,0.50]; the final threshold was set to 0.169.",35,36
3832,236460342,The pre-training setup trains a RoBERTa model on a seq2seq task where the input sequence of tokens generates a binary sequence denoting whether the input token was inside or outside of a toxic span.,34,35
3833,236460342,"In these matching predictions, the ground-truth spans have an mean length of 1.13 tokens, mainly concentrated on commonly-labeled offensive words, like ""morons"", suggesting that both models are adept at identifying overtly toxic words.",41,42
3834,236460342,"For example, the LIME model generates spans ≥5 tokens in only 693 of the 800K silver context, suggesting LIME tends to give shorter toxic span labels.",26,27
3835,236460342,This bias affects the downstream model performance in the test set where 29 of the 2000 test contexts have a span of ≥2 consecutive toxic words.,24,25
3836,236460342,"For example, in non-toxic test context 3, ""The parallels between the ANC and the Sicilian Mafia are glaring..."", the FT model labels ""Sicilian"" as toxic, while the PTFT model output is (correctly) empty.",6,7
3837,236460342,"For example, in non-toxic test context 3, ""The parallels between the ANC and the Sicilian Mafia are glaring..."", the FT model labels ""Sicilian"" as toxic, while the PTFT model output is (correctly) empty.",34,35
3838,236460342,"Looking at contexts where there are no underlying toxic spans, the two models perform slightly differently.",8,9
3839,236460342,"In contrast to the FT model, the PTFT model has less-accurate predictions on the overtly/commonly toxic spans.",20,21
3840,236460342,"For example, there are 430 total ""stupid"" or ""stupidity"" related words labeled as toxic by the ground-truth spans.",18,19
3841,236460342,"The FT model is able to label 383/430 as toxic, while the PTFT model only labels 331/430.",9,10
3842,236460342,"In the PTFT model's pre-training phase, we fed 400,000 non-toxic documents for the RoBERTa model.",15,16
3843,236460342,These non-toxic documents supplied more non-offensive context for certain toxic words than the small-sized gold dataset.,3,4
3844,236460342,These non-toxic documents supplied more non-offensive context for certain toxic words than the small-sized gold dataset.,13,14
3845,236460342,"Common Themes in Errors From the error analysis in the above section, we have noticed that the PTFT model does not perform well when it comes to predicting long toxic spans, empty toxic spans, and toxic phrases.",30,31
3846,236460342,"Common Themes in Errors From the error analysis in the above section, we have noticed that the PTFT model does not perform well when it comes to predicting long toxic spans, empty toxic spans, and toxic phrases.",34,35
3847,236460342,"Common Themes in Errors From the error analysis in the above section, we have noticed that the PTFT model does not perform well when it comes to predicting long toxic spans, empty toxic spans, and toxic phrases.",38,39
3848,236460342,"Category 1 shows where the PTFT model identifies valid toxic spans not present in ground truth, which accounts for 101 (50.5%) of the model errors in the 200 sampled contexts.",9,10
3849,236460342,"In these cases, annotators marked nothing as toxic in 58 contexts.",8,9
3850,236460342,"However, most of the overlooked toxic spans are overly-common toxic words like Examples 491 and 1374 in Table 3 .",6,7
3851,236460342,"However, most of the overlooked toxic spans are overly-common toxic words like Examples 491 and 1374 in Table 3 .",12,13
3852,236460342,"In comparison, in Category 2, there are 81 out of 200 sampled contexts with unmarked toxic span labels in the PTFT model output, in which the PTFT model produced an empty span or an incorrect span as toxic in four cases.",17,18
3853,236460342,"In comparison, in Category 2, there are 81 out of 200 sampled contexts with unmarked toxic span labels in the PTFT model output, in which the PTFT model produced an empty span or an incorrect span as toxic in four cases.",40,41
3854,236460342,"Example 642 in Table 3 shows a typical case were a relative rare toxic word, ""caca,"" is overlooked by the model.",13,14
3855,236460342,"In the remaining cases, the PTFT model has shorter predictions than the ground truth toxic spans, matching the low performance on predicting longer toxic spans ( §3.1).",15,16
3856,236460342,"In the remaining cases, the PTFT model has shorter predictions than the ground truth toxic spans, matching the low performance on predicting longer toxic spans ( §3.1).",25,26
3857,236460342,"In some cases, when multiple toxic spans exist in the same document, ground truth only labels one or two spans of them (e.g. Examples 1852 and 1486).",6,7
3858,236460342,"While in other cases, ground truth would label more toxic spans (e.g. Example 346).",10,11
3859,236460342,"Besides, there are 20 contexts that both the groundtruth spans and PTFT model missed the toxic span partially or completely.",16,17
3860,236460342,"Generally, ground truth annotation seldom labels non-toxic spans as toxic (Category 3).",9,10
3861,236460342,"Generally, ground truth annotation seldom labels non-toxic spans as toxic (Category 3).",12,13
3862,236460342,"On the contrary, it is common for our PTFT models to make mistakes on labeling non-toxic spans (Category Inconsistencies in Ground Truth Labels The last four categories (5-8) in Table 3 show common inconsistencies in annotation decisions, which we hope could aid in improving consistency in future work.",18,19
3863,236460342,"In Category 5, the standard for spans labeling is not consistent for which words are included in the toxic phrase.",19,20
3864,236460342,"In some cases, when a sentence is fairly short (< 5 words) and contains toxic words, the ground truth annotation would label out the adjective used for describing the trailing noun (e.g. Example 1469).",17,18
3865,236460342,These inconsistencies commonly manifest for frequent toxic words.,6,7
3866,236460342,"For instance, in Examples 968 and 348, both of the ""hypocrite(s)"" should be toxic given the context and there is no more than one other toxic word within the document.",18,19
3867,236460342,"For instance, in Examples 968 and 348, both of the ""hypocrite(s)"" should be toxic given the context and there is no more than one other toxic word within the document.",30,31
3868,236460342,The omission of common toxic words is the major source for this category.,4,5
3869,236460342,"However, in a few cases (4 of 25), annotators labeled nothing as toxic (e.g. Example 1374).",16,17
3870,236460342,"Interestingly, 9 of 25 cases where ground truth either labels the entire sentence or nothing, our PTFT model is able to identify the toxic word(s), suggesting the model is still effective for short contexts.",25,26
3871,236460342,"Discussion and Future Work Based on error analysis, our PTFT model suffers from low performance when generating predictions on non-toxic contexts or long toxic spans.",22,23
3872,236460342,"Discussion and Future Work Based on error analysis, our PTFT model suffers from low performance when generating predictions on non-toxic contexts or long toxic spans.",26,27
3873,236460342,Our qualitative analysis finds that the addition of non-toxic examples in the silver data influenced the model to consider overly common toxic words less toxic than they were in the gold data.,10,11
3874,236460342,Our qualitative analysis finds that the addition of non-toxic examples in the silver data influenced the model to consider overly common toxic words less toxic than they were in the gold data.,23,24
3875,236460342,Our qualitative analysis finds that the addition of non-toxic examples in the silver data influenced the model to consider overly common toxic words less toxic than they were in the gold data.,26,27
3876,236460342,Future work is needed to identify the optimal ratio of the toxic and non-toxic samples and to address domain/register differences in the data.,11,12
3877,236460342,Future work is needed to identify the optimal ratio of the toxic and non-toxic samples and to address domain/register differences in the data.,15,16
3878,236460342,"Our initial approach used explainable machine learning (LIME) to generate a heuristically labeled span dataset, which was used to pre-train a RoBERTa model to recognize toxic spans.",30,31
3879,227231676,"Any comment having at least two of the labels toxic, severe toxic, obscene, threat, insult, or identity hate was labeled as OFF.",9,10
3880,227231676,"Any comment having at least two of the labels toxic, severe toxic, obscene, threat, insult, or identity hate was labeled as OFF.",12,13
3881,250391037,PCL is a toxic language that implicitly has a negative impact on public opinion.,3,4
3882,250391037,There are tasks that generally identify toxic language that can be used to provide an answer to this problem.,6,7
3883,250391037,"To solve the problem of class imbalance in the learning process, we use the augmentation methods provided for toxic texts (Juuti et al.,",19,20
3884,227231489,"2017) , toxic comment classification (Fortuna et al.,",3,4
3885,227231489,"The data has been labelled with identity mentions, such as Muslim, Gay or Black, and a toxicity score (TARGET) that represents the faction of human annotators who believe the post is toxic.",36,37
3886,245130929,"2020; Goyal and Durrett, 2021) or toxic (Gehman et al.,",9,10
3887,248572126,"Recent work has shown that AI-driven abusive language or toxicity detection models disproportionately flag and penalize content that contains markers of identity terms even though they are not toxic or abusive (Gray and Stein, 2021; Haimson et al.,",30,31
3888,249336306,"Recent studies have shown that context affects annotations in toxic-ity and abuse detection (Pavlopoulos et al.,",9,10
3889,233365150,We used the constructiveness and toxicity labels and flagged comments whenever the toxicity level was toxic or very toxic and not constructive.,15,16
3890,233365150,We used the constructiveness and toxicity labels and flagged comments whenever the toxicity level was toxic or very toxic and not constructive.,18,19
3891,233365150,"A comment was labelled flagged whenever it was toxic, aggressive or if it contained an attack.",8,9
3892,250391045,Our hypothesis for the achieved improvement is that in the proposed method we employ more diverse sets of patterns for expressing toxic.,21,22
3893,250391045,"Related Work Prior works related to this task can be categorized into two groups: (i) Toxicity Detection: These works aim to classify a piece of text as toxic or nontoxic (Wulczyn et al.,",31,32
3894,250391014,"Among the community models, the experiments mainly focus on BERTand RoBERTa-based models for toxic language detection and sentiment analysis, given the similarity and relevance of the tasks to PCL detection.",16,17
3895,250391014,"I believe the models are too specialized in their own tasks (e.g. sentiment analysis, toxic language detection), therefore resulting in poor performance on the PCL detection task.",16,17
3896,227231729,"On the contrary, highly offensive messages might not necessarily include any toxic and hurtful words while they contain some metaphors or metonymies.",12,13
3897,248524755,Recent research in Natural Language Processing (NLP) has advanced the development of various toxicity detection models with the intention of identifying and mitigating toxic language from existing systems.,25,26
3898,248524755,"Despite the abundance of research in this area, less attention has been given to adversarial attacks that force the system to generate toxic language and the defense against them.",23,24
3899,248524755,"In this work, we propose attacks against conversational agents that are imperceptible, i.e., they fit the conversation in terms of coherency, relevancy, and fluency, while they are effective and scalable, i.e., they can automatically trigger the system into generating toxic language.",47,48
3900,248524755,"Through automatic and human evaluations, we show that our defense is effective at avoiding toxic language generation even against imperceptible toxicity triggers while the generated language fits the conversation in terms of coherency and relevancy.",15,16
3901,248524755,"2020) and toxic (Wallace et al.,",3,4
3902,248524755,An instance of the attack and defense is demonstrated in Figure 1 in which the adversary tries to trigger the defender while the defender avoids the attack by not generating toxic utterances.,30,31
3903,248524755,2019) offer attacks based on universal adversarial triggers (UAT) that can result in toxic text generation with a relatively high success rate.,16,17
3904,248524755,Through human and automatic evaluations we show the effectiveness of the proposed attack in provoking the defender into generating toxic responses while keeping the fluency and coherency of the conversation intact.,19,20
3905,248524755,We then focus on a defense mechanism for the non-adversarial (defender) model to avoid generating toxic utterances.,19,20
3906,248524755,Our proposed method relies on two levels of interpretable reasoning that helps the model to (1) identify the key adversarial tokens responsible for the attack and (2) avoid generating toxic responses by masking those tokens during the generation process.,33,34
3907,248524755,"The objective in this generative process is to search for triggers that can maximize the likelihood of toxic tokens being generated as follows: f UAT = y∈Y |y| i=1 log P (y i |y 1:i−1;t,θ ).",17,18
3908,248524755,"where Y is the set of toxic outputs, t denotes the trigger sequence, and θ is a trained language model.",6,7
3909,248524755,2020) has shown that toxic triggers are more likely to provoke toxic responses.,5,6
3910,248524755,2020) has shown that toxic triggers are more likely to provoke toxic responses.,12,13
3911,248524755,"Thus, in UTSC-1, we select the most toxic utterance among all generated attack utterances according to toxicity scores from toxicity classifiers as our final attack utterance (i.e., arg max i∈[n] {x i }).",9,10
3912,248524755,"For UTSC-2, we first apply a threshold T to toxicity scores of the candidate utterances and label the utterances above this threshold as toxic.",24,25
3913,248524755,"Next, from the pool of all toxic utterances, we select the utterance with the lowest toxicity score (i.e., arg min i∈[n] {x i |x i ≥ T }).",7,8
3914,248524755,If UTSC-3 (criterion 3):  most toxic utterance is selected.,8,9
3915,248524755,"While using an ensemble of the three models results in the most effective attacks, to ensure that the adversary is not simply overfitting the toxicity detection model but rather forcing the defender to actually generate toxic language, we also study the transferability of these attacks.",36,37
3916,248524755,We also asked AMT workers to rate if the utterance after the attack is toxic or not to verify the effectiveness of the attack according to human judgment.,14,15
3917,248524755,"Attack Effectiveness Here we report the ""attack effectiveness"" by calculating the percentage of conversations in which the defender was provoked by the adversary to generate a toxic response.",28,29
3918,248524755,"Imposing the language model constraint on UAT not only makes UAT-LM attack more fluent, but it also causes UAT-LM to generate more toxic triggers which results in more attack effectiveness.",27,28
3919,248524755,2020) in which authors show in a human adversary case that more toxic attacks perform better in forcing the model to generate toxic utterances.,13,14
3920,248524755,2020) in which authors show in a human adversary case that more toxic attacks perform better in forcing the model to generate toxic utterances.,23,24
3921,248524755,"In our results, we also show that UTSC-3 performs the worst which is based on non-toxic utterances followed by the UTSC-2 attack which is based on the least toxic utterance attack constraint.",18,19
3922,248524755,"In our results, we also show that UTSC-3 performs the worst which is based on non-toxic utterances followed by the UTSC-2 attack which is based on the least toxic utterance attack constraint.",31,32
3923,248524755,"However, UTSC-1 is the strongest as it relies on most toxic utterances followed by UAT-LM.",11,12
3924,248524755,"In addition, we found that the adversary is able to force the defender into generating toxic utterances regardless of the context sentence and whether or not the conversation is around a sensitive topic (e.g., the Reddit corpus) or a more neutral one (e.g., the Wizard of Wikipedia).",16,17
3925,248524755,"In Figure 4 , we demonstrate that even if the attacker only uses one of the toxicity detection models (Toxic-bert), it still can force the defender to generate toxic responses according to Perspective API and Safety classifier and have comparable performance to when it uses all the toxicity classifiers.",33,34
3926,248524755,This confirms that the attack is forcing the defender to generate actual toxic language rather than fooling the toxicity classifier.,12,13
3927,248524755,"For toxicity, we only have two ratings (toxic and not toxic).",9,10
3928,248524755,"For toxicity, we only have two ratings (toxic and not toxic).",12,13
3929,248524755,Defense Approaches The defense against adversarial attacks has two components (a) detecting the attack and (b) mitigating its effect by ensuring that the defender does not generate a toxic response.,32,33
3930,248524755,"2020) suggested a mitigating approach which, when a toxic response is detected, simply resets the dialogue and generates a (non-toxic) utterance by randomly sampling from a predefined set of topics (see Section 3.2.1).",10,11
3931,248524755,"2020) suggested a mitigating approach which, when a toxic response is detected, simply resets the dialogue and generates a (non-toxic) utterance by randomly sampling from a predefined set of topics (see Section 3.2.1).",25,26
3932,248524755,"As we mentioned before, we are interested in mitigation strategies that avoid generating toxic utterances but at the same time manage to keep the conversation flow intact.",14,15
3933,248524755,"If it finds that the generated utterance is toxic, it then proceeds with the second stage of the defense.",8,9
3934,248524755,Level 1 • Defender identifies the toxic token (L1 token highlighted in red) responsible for making the defender utterance toxic.,6,7
3935,248524755,Level 1 • Defender identifies the toxic token (L1 token highlighted in red) responsible for making the defender utterance toxic.,21,22
3936,248524755,the toxicity detection model to label the utterance as being toxic.,10,11
3937,248524755,"The defender then masks the L2 tokens from the adversary, which were responsible for triggering the defender model to generate toxic tokens, and generates a new utterance.",21,22
3938,248524755,"If it is deemed safe, it is then going to replace the defender's old toxic utterance, otherwise we iteratively apply the two-stage defense mechanism to mask more input tokens until the generated output is deemed safe.",16,17
3939,248524755,2020) This baseline is also a two-stage approach like ours in which the defender first uses a toxicity classifier to detect if the utterance is toxic or not.,28,29
3940,248524755,"It then changes the topic of the conversation if the utterance was detected to be toxic, e.g., ""Hey do you want to talk about something else?",15,16
3941,248524755,2020) used this defense against adversarial attacks performed by human adversaries that force the model to generate toxic responses.,18,19
3942,248524755,"Notice that although this defense is using a templated sentence to change the topic into a non-toxic topic and can be considered as the perfect solution to avoid generating toxic responses, it can provide the user with a non-plausible conversational experience given that the topic of the conversation changes each time the defender detects a toxic utterance.",18,19
3943,248524755,"Notice that although this defense is using a templated sentence to change the topic into a non-toxic topic and can be considered as the perfect solution to avoid generating toxic responses, it can provide the user with a non-plausible conversational experience given that the topic of the conversation changes each time the defender detects a toxic utterance.",31,32
3944,248524755,"Notice that although this defense is using a templated sentence to change the topic into a non-toxic topic and can be considered as the perfect solution to avoid generating toxic responses, it can provide the user with a non-plausible conversational experience given that the topic of the conversation changes each time the defender detects a toxic utterance.",60,61
3945,248524755,"To this end, we expect this baseline to do almost perfectly in terms of avoiding toxic response generation given that the toxicity detection classifier is a good detector; however, in terms of conversational quality it will have worse relevancy and coherency scores compared to our method as shown in our human evaluations.",16,17
3946,248524755,"More details can be found in Appendix A. Results Defense Effectiveness We report ""defense effectiveness"" as the percent decrease in a defender generating a toxic response after adversary's attack when the defense is applied compared to when it isn't.",26,27
3947,248524755,In some cases tokens generated after the trigger can themselves be more toxic and decisive in forcing the defender into generating toxic utterances (more details in Appendix B.1 Table 4.).,12,13
3948,248524755,In some cases tokens generated after the trigger can themselves be more toxic and decisive in forcing the defender into generating toxic utterances (more details in Appendix B.1 Table 4.).,21,22
3949,248524755,"As expected, the Non Sequitur defense is always effective as it replaces the toxic utterance with a non-toxic utterance by changing the topic; however, this approach is not necessarily creating the best conversational experience as also verified by our human experiments in terms of maintaining relevancy and coherency of the conversation.",14,15
3950,248524755,"As expected, the Non Sequitur defense is always effective as it replaces the toxic utterance with a non-toxic utterance by changing the topic; however, this approach is not necessarily creating the best conversational experience as also verified by our human experiments in terms of maintaining relevancy and coherency of the conversation.",20,21
3951,248524755,We also saw 70% reduction in toxic generation when we applied only one iteration of our defense mechanism on these attacks.,7,8
3952,248524755,This is because the Non Sequitur defense changes the topic every-time a toxic utterance is generated which lowers the quality of the conversational experience.,14,15
3953,248524755,"Thus, even if the Non Sequitur defense can be really effective in reducing the toxicity as it replaces the toxic utterance with a non-toxic templated sentence, it can create poor conversational experience as also rated by human annotators.",20,21
3954,248524755,"Thus, even if the Non Sequitur defense can be really effective in reducing the toxicity as it replaces the toxic utterance with a non-toxic templated sentence, it can create poor conversational experience as also rated by human annotators.",26,27
3955,248524755,2019) to generate toxic responses.,4,5
3956,248524755,"Thus, we used our defense to test whether it can also be effective in reducing the number of toxic responses given these prompts in RealToxic-ityPrompts in the GPT-2 model.",19,20
3957,248524755,We used the 100k prompts in RealToxicityPrompts and reported the number of toxic generations before and after applying our defense from the GPT-2 model.,12,13
3958,248524755,"Results in Figure 9 demonstrate that one iteration of our defense reduces the number of generated toxic responses by 81%, 31%, and 23%, according to Toxic-bert, Perspective API, and Safety classifier, respectively.",16,17
3959,248524755,These results show the effectiveness of our defense in reducing toxic generations beyond conversational domain and a step toward reducing toxic generation.,10,11
3960,248524755,These results show the effectiveness of our defense in reducing toxic generations beyond conversational domain and a step toward reducing toxic generation.,20,21
3961,248524755,"Notice that the setup of this experiment was not adversarial; however, prompts were causing the toxic generations.",17,18
3962,248524755,"Beyond attacks, we discussed a possible defense mechanism to improve robustness of generative models against generating toxic responses using interpretability methods.",17,18
3963,248524755,"In addition, there is a body of work in detecting toxic behavior in conversational agents (Zhang et al.,",11,12
3964,248524755,"Conclusion We studied the possibility of generating imperceptible attacks against conversational agents that, while fluent and coherent, target the model into generating toxic responses.",24,25
3965,248524755,We also discussed the extension of our defense work on any general generation task that can be an important contribution towards mitigating toxic gen-erations from our models.,22,23
3966,248524755,"We acknowledge that our attack can be used by unethical adversaries to force the models to generate toxic responses which is undesirable as also previously observed in chatbots (Wolf et al.,",17,18
3967,248524755,Another possible limitation of our defense mechanism can be the token-level dependence of our defense approach which can cause our defense mechanism to possibly fail on more subtle cases where there is no clear token that makes a sentence toxic.,41,42
3968,248524755,We also made the annotators aware of possible toxic or inappropriate language in our generations ahead of time.,8,9
3969,248524755,More details can be found in Appendix A. We hope that our study can be used for the benefit of the society and development of robust conversational systems along with reduced toxic generations in our models.,31,32
3970,248524755,"To determine whether an utterance is toxic or not, we used the default thresholds set by the developers for Toxic-bert and Safety classifiers and a threshold value of 0.5 for Perspective API.",6,7
3971,248524755,"Additionally, we report some statistics about toxicity scores of the adversary on the attack utterance as well as defender's toxicity score after the attack for UTSC-1, UTSC-2, and UTSC-3 attacks which can provide additional intuition on how toxic each attack is.",41,42
3972,248524755,We show results from our automatic attack strategy as well as our defense mechanism on it (Figure 16 (a)) along with our human experimental results in which a human adversary tries to fool the system into generating toxic utterances (Figure 16 (b)) and lastly the GPT-2 experiments using the RealToxicityPromts and how effective our proposed defense mechanism works on these sets of prompts and model (Figure 16 (c-f )).,41,42
3973,248524755,Results show that the responses after the defense arrow (representing with defense response) are less toxic in all the cases compared to the results generated in the dotted boxes (representing the response without any defense applied).,17,18
3974,248524755,We also demonstrate the effectiveness of our defense against both toxic UTSC-3 (b) and non-toxic UTSC-1 attacks (c).,10,11
3975,248524755,We also demonstrate the effectiveness of our defense against both toxic UTSC-3 (b) and non-toxic UTSC-1 attacks (c).,18,19
3976,248524755,"For ""attack effectiveness"", we report the percentage of conversations out of 100 conversations that we study in this paper in which the defender was fooled by the adversary after the performed attack into generating a toxic response.",38,39
3977,248524755,"For ""defense effectiveness"", we report the percent decrease in a defender generating a toxic response after adversary's attack when the defense is applied compared to when it isn't on the same set of 100 conversations that we used throughout the paper for different analysis.",16,17
3978,248524755,We used all the default thresholds set by the developers for all these toxicity detection classifiers and a threshold value of 0.5 for the Perspective API to detect whether an utterance is toxic or not.,32,33
3979,248524755,We use the same threshold values both in our attacks for the selection criteria and in defenses to determine if a generated utterance is toxic or not.,24,25
3980,250390729,"For this reason, the detection of toxic, hateful and abusive comments has been the central topic of several workshops and tool evaluations, drawing a lot of attention from the Natural Language Processing (NLP) research community in the last years.",7,8
3981,250390729,"However, while toxic language has a clear intent and is usually obvious to the reader, patronizing and condescending language (PCL) is more subtle and likely used in a subconscious manner even in traditional media (Perez Almendros et al.,",3,4
3982,250390655,"2017) , toxic language (Pavlopoulos et al.,",3,4
3983,250390692,"2018) or by adding non-toxic examples to better balance the data (Dixon et al.,",7,8
3984,227231646,"They were to be classified based on the toxicity (i.e. toxic, severe toxic, obscene, insult, identity hate, and threat).",11,12
3985,227231646,"They were to be classified based on the toxicity (i.e. toxic, severe toxic, obscene, insult, identity hate, and threat).",14,15
3986,233024836,"However, these have mostly focused on identifying whole comments or documents as either toxic or not.",14,15
3987,233024836,The task involves identifying text spans in a given toxic post that contributes towards the toxicity of that post.,9,10
3988,233024836,"Related Work As the task involves detecting toxic spans in a text, we present the related work in two parts: (i) Offensive Language Detection and (ii) Span Identification.",7,8
3989,233024836,"2017 ), toxic comments (Saif et al.,",3,4
3990,233024836,"Interestingly, the task of locating toxic spans is relatively novel, and its successful completion can be groundbreaking.",6,7
3991,233024836,"For the sub-tokens derived from the BERT-Base-Cased tokenizer, the ratio of toxic to non-toxic subtokens was 1:10.16.",18,19
3992,233024836,"For the sub-tokens derived from the BERT-Base-Cased tokenizer, the ratio of toxic to non-toxic subtokens was 1:10.16.",22,23
3993,233024836,"Hence, we extracted 40000 toxic samples from the Civil Comments Dataset, which were labeled with a toxicity score of 0.7 or higher, and used these to perform four iterations of semisupervised model training (Fig.",5,6
3994,233024836,"Post-preprocessing After obtaining the sub-token level labels from our model, we post-processed the results to convert them into an array of toxic character offsets.",28,29
3995,233024836,"To perform this, we had mapped each sub-token to its offset span during tokenization and used that to retrieve the offsets of all the characters in the toxic sub-tokens.",30,31
3996,233024836,We also include all characters lying between two consecutive sub-tokens if both the sub-tokens are marked toxic.,20,21
3997,233024836,"This was necessary as spaces and punctuation were included in the toxic spans given by the annotators, as shown by the results in section 6.",11,12
3998,233024836,We have also observed that complete sentences were marked as toxic just because of the presence of a few toxic words in them.,10,11
3999,233024836,We have also observed that complete sentences were marked as toxic just because of the presence of a few toxic words in them.,19,20
4000,233024836,The dataset contains numerous examples where no toxic spans are annotated.,7,8
4001,233024836,"In such cases, our model usually labels the word with the most negative sentiment as toxic and thus performs poorly.",16,17
4002,233024836,"For e.g., in the phrase ""no more Chinese,"" our model only predicts the word Chinese as toxic, whereas the complete phrase attributes to the toxicity of the sentence.",20,21
4003,233024836,"Conclusion The task of detecting toxic spans in the text is a novel one, and there is no doubt about how impor-tant a model trained successfully for this task can turn out to be for online content moderation.",5,6
4004,245218833,"Finally, we would like to use our method to control language generation of PLMs, in order to avoid generating hate speech or toxic text.",24,25
4005,250390980,"There have been previous attempts to identify hate/toxic content on social media platforms ( (Zampieri et al.,",9,10
4006,250390980,Most efforts to identify toxic and misogynous content consider only the textual content.,4,5
4007,250390980,"Attempts have been made to identify hateful/toxic and misogynist content on the internet, but none focuses on multimodal (Fersini et al.,",8,9
4008,227230704,"At Kaggle, a competition was held to classify offensive comments into six classes: toxic, more toxic, obscene, threatening, insulting, and personal attack.",15,16
4009,227230704,"At Kaggle, a competition was held to classify offensive comments into six classes: toxic, more toxic, obscene, threatening, insulting, and personal attack.",18,19
4010,248884476,Biased or toxic speech can be harmful to various demographic groups.,2,3
4011,248884476,"Therefore, it is not only important for models to detect these speech, but to also output explanations of why a given text is toxic.",25,26
4012,248884476,"Previous literature has mostly focused on classifying and detecting toxic speech, and existing efforts on explaining stereotypes in toxic speech mainly use standard text generation approaches, resulting in generic and repetitive explanations.",9,10
4013,248884476,"Previous literature has mostly focused on classifying and detecting toxic speech, and existing efforts on explaining stereotypes in toxic speech mainly use standard text generation approaches, resulting in generic and repetitive explanations.",19,20
4014,248884476,"Experiments show that our knowledge informed models outperform prior state-of-the-art models significantly, and can generate detailed explanations of stereotypes in toxic speech compared to baselines, both quantitatively and qualitatively.",27,28
4015,248884476,Introduction The toxic speech detection and classification problem has seen increasing interest in recent years.,2,3
4016,248884476,"However, it is not only important for AI agents to recognize and classify toxic speech, but to also explain why it is toxic.",14,15
4017,248884476,"However, it is not only important for AI agents to recognize and classify toxic speech, but to also explain why it is toxic.",24,25
4018,248884476,"For instance, debiasing methods that use information about toxic language may benefit from additional information given by detailed explanations of toxicity in text (Ma et al.,",9,10
4019,248884476,They can also help humans who work with toxicity classifiers use more information about the input when making decisions about toxic speech.,20,21
4020,248884476,"While the literal text is not toxic, the implied meaning is offensive, particularly to those affected by school shootings.",6,7
4021,248884476,"Note that, we use the term biased and toxic interchangeably in this work.",9,10
4022,248884476,"Existing work largely addresses the problem of detecting and classifying toxic speech (Waseem and Hovy, 2016; Founta et al.,",10,11
4023,248884476,"For instance, explanations may focus on certain toxic components of the input but ignore others, or include irrelevant stereotypes about the minority group affected.",8,9
4024,248884476,"To sum up, our contributions are twofold: (1) We leverage three different sources of knowledge, and further combine them using simple yet effective mixture models to explain toxic text. (",32,33
4025,248884476,"Conclusion In this paper, we propose a novel framework MIX-GEN to generate the stereotypes present in toxic social media posts, using multiple knowledge sources.",19,20
4026,236460340,Toxic span detection requires the detection of spans that make a text toxic instead of simply classifying the text.,12,13
4027,236460340,Experimental results showed that the introduced auxiliary information can improve the performance of toxic spans detection.,13,14
4028,236460340,The code of this study is available at https://github.com/Chenrj233/ semeval2021_task5 Introduction Existing toxicity detection datasets and models classify the entire comment or document and do not identify the range that makes the text toxic.,33,34
4029,236460340,"As a complete submission for the shared task, systems are required to extract a list of toxic spans or an empty list per text.",17,18
4030,236460340,We define a sequence of words that attribute to the text's toxicity as the toxic span.,15,16
4031,236460340,"Table 1 shows two toxic spans, ""stupid"" and ""a!@#!@,"" which have character offsets from 10 to 15 (counting starts from 0) and from 51 to 56, respectively.",4,5
4032,236460340,"Text This is a stupid example, so thank you for nothing a!@#!@. Offset List [10, 11, 12, 13, 14, 15, 51, 52, 53, 54, 55, 56] Table 1 : Example of toxic spans detection shared task.",45,46
4033,236460340,The main purpose of this task is to identify the toxic spans in a given text; this can be transformed into a sequence labeling task in natural language processing.,10,11
4034,236460340,"Unlike normal sequence labeling tasks, this task is more challenging because the toxic spans in the text may involve a word, phrase, or even a sentence.",13,14
4035,236460340,"After a simple analysis of the text data, it can be found that not all the words in the toxic span have a toxic meaning, and some toxic meanings occur in a specific context or semantic conditions.",20,21
4036,236460340,"After a simple analysis of the text data, it can be found that not all the words in the toxic span have a toxic meaning, and some toxic meanings occur in a specific context or semantic conditions.",24,25
4037,236460340,"After a simple analysis of the text data, it can be found that not all the words in the toxic span have a toxic meaning, and some toxic meanings occur in a specific context or semantic conditions.",29,30
4038,236460340,It aims to classify whether a token belongs to the toxic span in a text.,10,11
4039,236460340,We needed to find the subscript offset set of the toxic spans of each post in the test data.,10,11
4040,236460340,"Therefore, our output layer was a two-classification layer that outputs the probability of a token belonging to a toxic span.",21,22
4041,236460340,This may be due to an imbalance between the toxic and nontoxic categories in the text.,9,10
4042,236460340,"After analyzing the prediction results, we observed that although the model can learn the representation of each token well, token classification errors can occur when some tokens are toxic without the entire text being toxic.",30,31
4043,236460340,"After analyzing the prediction results, we observed that although the model can learn the representation of each token well, token classification errors can occur when some tokens are toxic without the entire text being toxic.",36,37
4044,227230655,"From there we can observe that both BERT (toxic, toxicnorm), and GloVe-based (capsuleglove, cnnglove) neural networks are clearly the strongest models in the ensemble.",9,10
4045,227230655,"2018) Results Our offensive text classification system obtained strong results across different datasets which are summarized in Interestingly, BERT models fine tuned on Kaggle toxic dataset had a high correlation with the test set for this year challenge, even improving slightly final ensemble results when compared against the identity hate and toxic classes.",26,27
4046,227230655,"2018) Results Our offensive text classification system obtained strong results across different datasets which are summarized in Interestingly, BERT models fine tuned on Kaggle toxic dataset had a high correlation with the test set for this year challenge, even improving slightly final ensemble results when compared against the identity hate and toxic classes.",54,55
4047,202542397,"Data Filtering Reddit: To retrieve high-quality conversational data that would likely include hate speech, we referenced the list of the whiniest most low-key toxic subreddits 6 .",29,30
4048,238583331,"Using a threshold of 0.5, we discover that 31.9 percent of conversations in CMV contain toxic language in turns prior to the last one.",16,17
4049,238583331,The analysis of the composition of CGA and CMV also raises an issue with the definition of the task: it is perhaps better to view the forecasting of toxic personal attacks as a subtask within a more general task of forecasting derailment in conversationwith other types of derailment still remaining unexplored.,29,30
4050,250390775,These hinder the potential deployment of DNN models to monitor toxic and abusive elements in the ever-increasing social media platforms.,10,11
4051,237491918,"Finally, we finetune our model on the ART dataset, which is built on five sentence short stories which is devoid of harmful and toxic text especially targeted at marginalized communities.",25,26
4052,241583427,"An example is shown below: (2) An error due to changing the POS of a word in the passage Passage: In 1977 a swamp created by heavy rains was found to contain 8 toxic materials, including 11 suspected cancer-causing chemicals Question: When was something being swamped?",37,38
4053,235313693,"Liu and Avci (2019) calculates L 2 distance between Path Integrated Gradients attribution for selected tokens and a target value in the objective function, to mitigate unintended bias in toxic comment classification and improve classifier performance in scarce settings.",32,33
4054,218974198,"Introduction In recent years, there have been several studies exploring the computational modelling and automatic detection of abusive content in social media focusing on toxic comments 1 , aggression (Kumar et al.,",25,26
4055,243865261,"2020) explore the norms of interaction dictated and enforced by multiple ""toxic"" subreddits, showcasing how self-selection and preentry learning play a key role in sustaining these norms.",13,14
4056,237431059,Our poetic parallel corpora are unlikely to contain toxic text and underwent manual inspection by the authors.,8,9
4057,229923551,"2018) use templates to generate synthetic sets of toxic and non-toxic cases, which resembles our method for test case creation.",9,10
4058,229923551,"2018) use templates to generate synthetic sets of toxic and non-toxic cases, which resembles our method for test case creation.",13,14
4059,235352644,"Online abuse, including hate speech, cyber-bullying, online harassment, and other types of offensive and toxic behaviors, has been a focus of substantial research effort in the NLP community in the past decade (e.g. see surveys by Schmidt and Wiegand (2017) ; Fortuna and Nunes (2018) ; Vidgen et al. (",20,21
4060,215827653,"Given how online conversations can turn toxic and lack empathy, indiscriminate pretraining on such corpora is unlikely to spontaneously endow a conversational agent with desirable qualities such as avoiding toxic responses (Dinan et al.,",6,7
4061,215827653,"Given how online conversations can turn toxic and lack empathy, indiscriminate pretraining on such corpora is unlikely to spontaneously endow a conversational agent with desirable qualities such as avoiding toxic responses (Dinan et al.,",30,31
4062,237503353,"2019b) , with the task of identifying toxic user comments on online platforms.",8,9
4063,237503353,"Such models may lead to unintended consequences like flagging a comment as toxic just because it mentions some demographic identities, or in other words, belongs to some domains.",12,13
4064,236460039,"First, we conduct data anonymization and manually examined the data to ensure there are no privacy and ethical concerns, e.g., personal information, toxic language, and hate speech.",26,27
4065,237568724,"2020) , i.e., hateful, toxic, obscene, sexual, or lewd content.",7,8
4066,237568724,"Our analyses confirm that determining whether a document has toxic or lewd content is a more nuanced endeavor that goes beyond detecting ""bad"" words; hateful and lewd content can be expressed without negative keywords (e.g., microaggressions, innuendos; Breitfeller et al.,",9,10
4067,226283727,"Some of the biggest firms invest heavily in tracking abusive language, e.g., automatic detection of offensive language in comments (Systrom, 2017 (Systrom, , 2018) ) or giving a percentage of how likely a text is to be perceived as toxic.",46,47
4068,2064584,"I. Introduction We will discuss the analysis of English expressions consisting of a head noun preceded by one or more open.class specifiers: rising prices, horse blanket, mushroom omelet, banana bread, parish priest, gurgle detector, quarterback sneak, blind spot, red herring, bachelor's degree, Planck's constant, Madison Avenue, Wall Street, Washington's birthday sale, error correction code logic, steel industry collective bargaining agreement, expensive toxic waste cleanup, windshield wiper blade replacement, computer communications network performance analysis primer, and so forth.",81,82
4069,184483129,"Our best systems in each of the three OffensEval tasks placed in the middle of the comparative evaluation, ranking 57 th of 103 in task A, 39 th of 75 in task B, and 44 th of 65 in task C. Introduction Social media is notorious for providing a platform for offensive, toxic, and hateful speech.",56,57
4070,2310572,"Once considered, an appropriate test would confirm the existence of the toxic substance in the body.",12,13
4071,3101481,"Figure 2 shows that the PoS tags NN (singular noun), NNS (plural noun), and NNPS (plural proper noun) all have a toxic effect when added to the baseline set.",29,30
4072,229376920,"Shades of abusive language include hate speech, offensive language, sexist and racist language, aggression, profanity, cyberbullying, harassment, trolling, and toxic language (Waseem et al.",27,28
4073,229376920,"There are more and more efforts put into combating toxic language, such as 30K content moderators that Facebook and Instagram employ (Harrison 2019) .",9,10
4074,248780200,The toxic legacy of colonialism permeates every aspect of interaction between Indigenous communities and outside researchers.,1,2
4075,248780200,"The toxic legacy of colonial-ism permeates every aspect of interaction between Indigenous communities and outside researchers (Smith, 2012) .",1,2
4076,248780200,"Prerequisite Obligations for Ethical Research involving Indigenous Languages and Indigenous Peoples When CL and NLP researchers begin to work with Indigenous language data without first critically examining the toxic legacy of colonialism and the self-identified priority needs and epistemology of the Indigenous community, the risk of unwittingly perpetuating dehumanizing colonial practices is extremely high.",28,29
4077,235790530,"Then, (2) we introduce a challenging data benchmark, Collaborative Toxicity Moderation in the Wild (CoToMoD), for evaluating the effectiveness of a collaborative toxic comment moderation system.",29,30
4078,235790530,"2) Class Imbalance, i.e. the fact that most online content is not toxic (Cheng et al.,",14,15
4079,235790530,"This manifests in the datasets we use: roughly 2.5% (50,350 / 1,999,514) of the examples in the Civil-Comments dataset, and 9.6% (21,384 / 223,549) of the examples in Wikipedia Talk Corpus examples are toxic (Wulczyn et al.,",43,44
4080,235790530,"As we will show, failing to account for class imbalance can severely bias model predictions toward the majority (non-toxic) class, reducing the effectiveness of the collaborative system.",22,23
4081,235790530,"Focal loss reshapes the loss function to down-weight ""easy"" negatives (i.e. non-toxic examples), thereby focusing training on a smaller set of more difficult examples, and empirically leading to improved predictive and uncertainty calibration performance on class-imbalanced datasets (Lin et al.,",18,19
4082,235790530,This is likely due to their sensitivity to class imbalance (recall that toxic examples are slightly less rare in CivilComments).,13,14
4083,235790530,"As a result, these classic metrics tend to favor model predictions biased toward the negative class, and therefore are less suitable for evaluating model performance in the context of toxic comment moderation.",31,32
4084,235790530,"This highlights the impact of the toxicity distribution of the data on the best review strategy: because the proportion of toxic examples is much lower in CivilComments than in the Wikipedia Talk Corpus, the cross-over between the uncertainty and toxicity review strategies correspondingly occurs at lower review fractions.",21,22
4085,235790530,"Though the results presented in the current paper are encouraging, there remain important challenges for uncertainty modeling in the domain of toxic content moderation.",22,23
4086,44522989,"In our crowd-annotated data, 21.42% of the non-constructive comments and 17.89% of the constructive comments are toxic, suggesting that non-constructive comments are not much more toxic than constructive comments.",23,24
4087,44522989,"In our crowd-annotated data, 21.42% of the non-constructive comments and 17.89% of the constructive comments are toxic, suggesting that non-constructive comments are not much more toxic than constructive comments.",35,36
4088,44522989,A recent example is the effort by Google to identify abusive or toxic comments through the Perspective API.,12,13
4089,44522989,The question posed was: How toxic is the comment?,6,7
4090,44522989,"We established four classes: Very toxic, Toxic, Mildly toxic and Not toxic.",6,7
4091,44522989,"We established four classes: Very toxic, Toxic, Mildly toxic and Not toxic.",11,12
4092,44522989,"We established four classes: Very toxic, Toxic, Mildly toxic and Not toxic.",14,15
4093,44522989,"The definition for Very toxic included comments which use harsh, offensive or abusive language; comments which include personal attacks or insults; or which are derogatory or demeaning.",4,5
4094,44522989,"Mildly toxic comments were described as those which may be considered toxic only by some people, or which express anger and frustration.",1,2
4095,44522989,"Mildly toxic comments were described as those which may be considered toxic only by some people, or which express anger and frustration.",11,12
4096,44522989,"The most important result of this annotation experiment is that there were no significant differences in toxicity levels between constructive and non-constructive comments, i.e., constructive comments were as likely to be toxic (in its three categories) as non-constructive comments.",35,36
4097,44522989,"It was labelled as constructive by two out of three annotators, and toxic by all three (two as Toxic, and one as Very toxic).",13,14
4098,44522989,"It was labelled as constructive by two out of three annotators, and toxic by all three (two as Toxic, and one as Very toxic).",26,27
4099,44522989,"It could be the case, in some situations, that a moderator may allow a somewhat toxic comment if it contributes to the conversation, i.e., if it is constructive. (",17,18
4100,44522989,"Through an annotation experiment, we studied the relationship between constructiveness and toxicity, and found that constructive comments are just as likely to be toxic (or not toxic) as nonconstructive comments.",25,26
4101,44522989,"Through an annotation experiment, we studied the relationship between constructiveness and toxicity, and found that constructive comments are just as likely to be toxic (or not toxic) as nonconstructive comments.",29,30
4102,44522989,"In terms of filtering, this poses an interesting question, since some of our toxic comments were also deemed to be constructive by the annotators.",15,16
4103,52291335,"It is clear that norms develop in different subreddits, as redditors even discuss which subreddits are ""toxic.""",18,19
4104,52291335,"Indeed, the term ""toxic"" is a general description of subreddits with habitual and even emphasized disalignment.",5,6
4105,49556392,The carpets won't be glued down and walls will be coated with non-toxic finishes.,15,16
4106,49556392,The carpets won't be glued down and walls will be coated with non-toxic finishes.,15,16
4107,237532535,"Moreover, there are several other underlying effects that could get in the spotlight, including the ramifications of interventions or the reasons that drive toxic actors.",25,26
4108,237532535,"Harmful content versus the individuals: we outline research concerning the impact of toxic behaviours on the targets or passive readers (Saha et al.,",13,14
4109,237532535,"Difficulties and Causal Methods Experimentation for the sole purpose of causality is fairly costly and, in this case, can be largely unethical, considering that it would mean deliberate exposure of people to toxic material.",35,36
4110,209536752,15  SOCC was recollected to study different aspects of on-line comments such as the connections between articles and comments; the connections of comments to each other; the types of topics discussed in comments; the nice (constructive) or mean (toxic) ways in which commenters respond to each other; and how language is used to convey very specific types of evaluation.,47,48
4111,233365313,2018) identified online social aggression on the Facebook comment in a multilingual scenario and Wikipedia toxic comments using stacked LSTM units followed by convolution layer and fastText as word representation.,16,17
4112,233365313,They achieved 0.98 AUC for Wikipedia toxic comment classification and a weighted F1 score of 0.63 for the Facebook test set and 0.59 for the Twitter test set. (,6,7
4113,218977399,"Introduction The pervasiveness of social media and the increase in online interactions in recent years has also led to a surge of online abusive behavior, which can be exhibited in different forms: toxic comments, aggression, hate speech, trolling, cyberbullying, etc.",34,35
4114,208157933,"The spectacular expansion of the Internet has led to the development of a new research problem in the field of natural language processing: automatic toxic comment detection, since many countries prohibit hate speech in public media.",25,26
4115,208157933,"There is no clear and formal definition of hate, offensive, toxic and abusive speeches.",12,13
4116,208157933,"In this article, we put all these terms under the umbrella of ""toxic speech"".",14,15
4117,208157933,The contribution of this paper is the design of binary classification and regression-based approaches aiming to predict whether a comment is toxic or not.,23,24
4118,208157933,"Moreover, we study the robustness of the proposed approaches to adversarial attacks by adding one (healthy or toxic) word.",19,20
4119,208157933,"Unfortunately, the dark side of this growth is an increase in toxic speech.",12,13
4120,208157933,"There is no uniform definition of toxic speech in the scientific literature and there is no clear distinction between hate, offensive, toxic and abusive speech (Gröndahl et al.,",6,7
4121,208157933,"There is no uniform definition of toxic speech in the scientific literature and there is no clear distinction between hate, offensive, toxic and abusive speech (Gröndahl et al.,",23,24
4122,208157933,We refer to these collectively with the generic term of toxic speech.,10,11
4123,208157933,Manually monitoring and moderating the Internet and social media content to identify and remove toxic speech is extremely expensive.,14,15
4124,208157933,This article aims at designing methods for automatic toxic speech detection on the Internet.,8,9
4125,208157933,"Very recently, DNNs have become the state-of-the-art method for toxic speech detection.",16,17
4126,208157933,"In this article, we investigate several approaches based on different state-of-the-art DNN models and word representations for the task of automatic toxic comment detection.",28,29
4127,208157933,"RNN is able to extract long-term dependencies that are definitely useful for toxic comment detection (Del Vigna et al.,",14,15
4128,208157933,The designed systems are evaluated on publicly available corpus of toxic comments from Wikipedia.,10,11
4129,208157933,"Furthermore, we analyse the robustness of these approaches with adversarial attacks by adding a toxic or healthy word to the comment.",15,16
4130,208157933,The DNN will classify these sequences of one-hot vectors as toxic or nontoxic.,12,13
4131,208157933,The DNN network classifies these sequences of word embeddings as toxic or non-toxic.,10,11
4132,208157933,The DNN network classifies these sequences of word embeddings as toxic or non-toxic.,14,15
4133,208157933,It is important to note that these representations are pretrained on corpora not specific to our task of toxic comment detection.,18,19
4134,208157933,"Hence, will not be efficient to model the specificity of toxic speech (slang, affronts, abuse, etc.).",11,12
4135,208157933,DNN classifiers The task of toxic comment detection can be viewed from two perspectives: • A binary classification task: The neural network is directly trained to decide if a comment is toxic or nontoxic. •,5,6
4136,208157933,DNN classifiers The task of toxic comment detection can be viewed from two perspectives: • A binary classification task: The neural network is directly trained to decide if a comment is toxic or nontoxic. •,33,34
4137,208157933,A threshold on the predicted score can be used to decide if the comment is toxic or not.,15,16
4138,208157933,"This part contains 160k comments from English Wikipedia talk pages, each labelled by approximately 10 annotators via crowd-sourcing, on a spectrum of how toxic/healthy the comment is with regard to the conversation.",27,28
4139,208157933,"The following toxicity rates are used by annotators: very toxic, toxic, neither, healthy, very healthy.",10,11
4140,208157933,"The following toxicity rates are used by annotators: very toxic, toxic, neither, healthy, very healthy.",12,13
4141,208157933,"According to this label definition, toxic speech corresponds to very toxic and toxic labels.",6,7
4142,208157933,"According to this label definition, toxic speech corresponds to very toxic and toxic labels.",11,12
4143,208157933,"According to this label definition, toxic speech corresponds to very toxic and toxic labels.",13,14
4144,208157933,"To perform the binary classification (toxic or not toxic), for each comment, we decided to use the following majority vote labelling: if [(# of very toxic and toxic annotations) > (# of healthy and very healthy annotations)] and [(# of very toxic and toxic annotations) > 2] comment is toxic otherwise comment is non-toxic Some examples of the toxic comments are: ""You are a big fat idiot, stop spamming my userspace"", ""What the fuck is your problem?"", """,6,7
4145,208157933,"To perform the binary classification (toxic or not toxic), for each comment, we decided to use the following majority vote labelling: if [(# of very toxic and toxic annotations) > (# of healthy and very healthy annotations)] and [(# of very toxic and toxic annotations) > 2] comment is toxic otherwise comment is non-toxic Some examples of the toxic comments are: ""You are a big fat idiot, stop spamming my userspace"", ""What the fuck is your problem?"", """,9,10
4146,208157933,"To perform the binary classification (toxic or not toxic), for each comment, we decided to use the following majority vote labelling: if [(# of very toxic and toxic annotations) > (# of healthy and very healthy annotations)] and [(# of very toxic and toxic annotations) > 2] comment is toxic otherwise comment is non-toxic Some examples of the toxic comments are: ""You are a big fat idiot, stop spamming my userspace"", ""What the fuck is your problem?"", """,32,33
4147,208157933,"To perform the binary classification (toxic or not toxic), for each comment, we decided to use the following majority vote labelling: if [(# of very toxic and toxic annotations) > (# of healthy and very healthy annotations)] and [(# of very toxic and toxic annotations) > 2] comment is toxic otherwise comment is non-toxic Some examples of the toxic comments are: ""You are a big fat idiot, stop spamming my userspace"", ""What the fuck is your problem?"", """,34,35
4148,208157933,"To perform the binary classification (toxic or not toxic), for each comment, we decided to use the following majority vote labelling: if [(# of very toxic and toxic annotations) > (# of healthy and very healthy annotations)] and [(# of very toxic and toxic annotations) > 2] comment is toxic otherwise comment is non-toxic Some examples of the toxic comments are: ""You are a big fat idiot, stop spamming my userspace"", ""What the fuck is your problem?"", """,54,55
4149,208157933,"To perform the binary classification (toxic or not toxic), for each comment, we decided to use the following majority vote labelling: if [(# of very toxic and toxic annotations) > (# of healthy and very healthy annotations)] and [(# of very toxic and toxic annotations) > 2] comment is toxic otherwise comment is non-toxic Some examples of the toxic comments are: ""You are a big fat idiot, stop spamming my userspace"", ""What the fuck is your problem?"", """,56,57
4150,208157933,"To perform the binary classification (toxic or not toxic), for each comment, we decided to use the following majority vote labelling: if [(# of very toxic and toxic annotations) > (# of healthy and very healthy annotations)] and [(# of very toxic and toxic annotations) > 2] comment is toxic otherwise comment is non-toxic Some examples of the toxic comments are: ""You are a big fat idiot, stop spamming my userspace"", ""What the fuck is your problem?"", """,64,65
4151,208157933,"To perform the binary classification (toxic or not toxic), for each comment, we decided to use the following majority vote labelling: if [(# of very toxic and toxic annotations) > (# of healthy and very healthy annotations)] and [(# of very toxic and toxic annotations) > 2] comment is toxic otherwise comment is non-toxic Some examples of the toxic comments are: ""You are a big fat idiot, stop spamming my userspace"", ""What the fuck is your problem?"", """,70,71
4152,208157933,"To perform the binary classification (toxic or not toxic), for each comment, we decided to use the following majority vote labelling: if [(# of very toxic and toxic annotations) > (# of healthy and very healthy annotations)] and [(# of very toxic and toxic annotations) > 2] comment is toxic otherwise comment is non-toxic Some examples of the toxic comments are: ""You are a big fat idiot, stop spamming my userspace"", ""What the fuck is your problem?"", """,75,76
4153,208157933,We removed the toxic comments with more than 200 words per comment from the training set because it is possible that the toxic part of the comment is located after the 200 th word.,3,4
4154,208157933,We removed the toxic comments with more than 200 words per comment from the training set because it is possible that the toxic part of the comment is located after the 200 th word.,22,23
4155,208157933,This preprocessing removed about 5% of toxic comments from the training set.,7,8
4156,208157933,Table 1 shows that toxic comments represent only about 17% of all comments.,4,5
4157,208157933,"This can be due to the presence of OOV words: the one-hot approach models N most frequent words of training corpus, while Mikolov's embeddings is trained on non-toxic corpus and it is possible that some important toxic words (slang) of our corpus are missing in the Mikolov's pre-trained model.",34,35
4158,208157933,"This can be due to the presence of OOV words: the one-hot approach models N most frequent words of training corpus, while Mikolov's embeddings is trained on non-toxic corpus and it is possible that some important toxic words (slang) of our corpus are missing in the Mikolov's pre-trained model.",43,44
4159,208157933,"A preliminary error analysis shows that sometimes nontoxic speech can be misclassified as toxic speech in the presence of words like bullies, anti-semitism.",13,14
4160,208157933,is misclassified as toxic.,3,4
4161,208157933,"Likewise, toxic speech is misclassified as non-toxic speech due to sarcasm, irony, rhetoric question, etc.",2,3
4162,208157933,"Likewise, toxic speech is misclassified as non-toxic speech due to sarcasm, irony, rhetoric question, etc.",9,10
4163,208157933,is misclassified as non-toxic.,5,6
4164,208157933,A threshold is applied to the regression score to decide if the comment is toxic or not.,14,15
4165,208157933,"We obtained the following results in terms of RMSE Robustness evaluation In order to evaluate the robustness of our classification systems, we added a toxic word ('fuck') to each comment of the test set and a healthy word ('love') to each comment of the test set.",25,26
4166,208157933,"Table 4 shows the percentage of correctly classified comments that change from predicted non-toxic to toxic comments when a toxic word is appended, and from toxic to non-toxic when a healthy word is appended.",15,16
4167,208157933,"Table 4 shows the percentage of correctly classified comments that change from predicted non-toxic to toxic comments when a toxic word is appended, and from toxic to non-toxic when a healthy word is appended.",17,18
4168,208157933,"Table 4 shows the percentage of correctly classified comments that change from predicted non-toxic to toxic comments when a toxic word is appended, and from toxic to non-toxic when a healthy word is appended.",21,22
4169,208157933,"Table 4 shows the percentage of correctly classified comments that change from predicted non-toxic to toxic comments when a toxic word is appended, and from toxic to non-toxic when a healthy word is appended.",28,29
4170,208157933,"Table 4 shows the percentage of correctly classified comments that change from predicted non-toxic to toxic comments when a toxic word is appended, and from toxic to non-toxic when a healthy word is appended.",32,33
4171,208157933,"Conclusion In this article, we have investigated several approaches for toxic comment classification using DNNs.",11,12
4172,208157933,"Among DNN based classifiers, bi-LSTM performs better than CNN and bi-GRU at classifying toxic speech.",18,19
4173,208157933,"In the future, we would like to study the impact of data bias on toxic speech detection (Wiegand et al.,",15,16
4174,219260475,"A promising alternative approach to reduce toxic discourse without recourse to censorship is so-called counter speech, which broadly refers to citizens' response to hateful speech in order to stop it, reduce its consequences, and discourage it [Benesch et al.,",6,7
4175,235683199,"Introduction Online spaces often contain toxic behaviors such as abuse or harmful speech (Blackwell et al.,",5,6
4176,235683199,"Moreover, the radicalization of individuals through their engagement with toxic online spaces may have real-world consequences, making toxic online communities a cause for broader concern (Ohlheiser, 2016; Habib et al.,",10,11
4177,235683199,"Moreover, the radicalization of individuals through their engagement with toxic online spaces may have real-world consequences, making toxic online communities a cause for broader concern (Ohlheiser, 2016; Habib et al.,",21,22
4178,236486181,One abusive or toxic statement is being sent every 30 seconds across the globe 1 .,3,4
4179,236486288,Such interactions may contain toxic comments or posts that are acutely insulting or harmful to other participants.,4,5
4180,236486288,"Toxic Comment Classification Challenge by Kaggle 1 , for example, provides thousands of humanlabeled examples for detecting toxic behaviors in Wikipedia comments.",18,19
4181,44694791,"We have therefore attached to a speaker's articulators fiduciary markers made of non toxic polymers visible by MRI, and recorded a corpus of MRI midsagittal images.",14,15
4182,220250520,"Example of a tweet with ADR mentions @coolpharmgreg i don't care if they are toxic haha putting the cipro drops in is essentially equivalent to torture #oww Here 'oww', 'toxic' and 'equivalent to torture' are the adverse drug reactions due to the consumption of the drug 'cipro'.",15,16
4183,220250520,"Example of a tweet with ADR mentions @coolpharmgreg i don't care if they are toxic haha putting the cipro drops in is essentially equivalent to torture #oww Here 'oww', 'toxic' and 'equivalent to torture' are the adverse drug reactions due to the consumption of the drug 'cipro'.",35,36
4184,220250520,"In the above example, the ADR mentions 'oww', 'toxic', and 'equivalent to torture' are mapped to the concepts 'pain (10033371)', 'drug toxicity (10013746)' and 'feeling unwell (10016370)' respectively.",13,14
4185,53503062,"We built and compared two systems, one using only the original data for training, and the second using also toxic data.",21,22
4186,53503062,"The dataset contains Wikipedia comments marked as ""toxic"", ""severe toxic"", ""obscene"", ""threat"" and ""identity hate"", in a multi-class and multi-label approach.",8,9
4187,53503062,"The dataset contains Wikipedia comments marked as ""toxic"", ""severe toxic"", ""obscene"", ""threat"" and ""identity hate"", in a multi-class and multi-label approach.",13,14
4188,53503062,"The Toxicity dataset consists of 170,355 messages marked as toxic, severe toxic, obscene, threat and identity hate, in a multi-class and multi-label approach (Jigsaw, 2018) .",9,10
4189,53503062,"The Toxicity dataset consists of 170,355 messages marked as toxic, severe toxic, obscene, threat and identity hate, in a multi-class and multi-label approach (Jigsaw, 2018) .",12,13
4190,53503062,"The steps followed are: • Regarding the toxic column, we decided to ignore it because it correlates strongly with the others. •",8,9
4191,53503062,"The columns ""severe toxic"", 'insult"" ,""obscene"" and ""threat"" would correspond to ""overtly aggressive"" (OAG). •",4,5
4192,53503062,"In the case of the Toxicity dataset, we think that ""severe toxic"", ""insult"" ,""obscene"" and ""threat"" are more similar to ""profanity"" than to ""hate speech"", and the four should be grouped together as overtly aggressive (OAG).",13,14
4193,53503062,"We concluded (Figure 3 ) that the models built using only the default dataset (marked as ""without toxic dataset"" in the figure) perform better than the ones using also the Toxicity dataset (marked as ""with toxic dataset"").",20,21
4194,53503062,"We concluded (Figure 3 ) that the models built using only the default dataset (marked as ""without toxic dataset"" in the figure) perform better than the ones using also the Toxicity dataset (marked as ""with toxic dataset"").",42,43
4195,53503062,"This required a conversion procedure where we corresponded identity hate with covertly aggressive discourse, and severe toxic, insult, obscene and threat, were mapped to overtly aggressive discourse.",17,18
4196,203465673,"With an ever-increasing content on such platforms, it makes impossible to manually detect toxic comments or hate speech.",16,17
4197,203465673,"Earlier works in Capsule network based deep learning architecture to classify toxic comments have proved that these networks work well as compared to other deep learning architectures (Srivastava et al.,",11,12
4198,203465673,"Each comment has a multi-class label, and there are a total of 6 classes, namely, toxic, severe toxic, obscene, threat, insult and identity hate.",20,21
4199,203465673,"Each comment has a multi-class label, and there are a total of 6 classes, namely, toxic, severe toxic, obscene, threat, insult and identity hate.",23,24
4200,59336566,involve toxic comments in various forms.,1,2
4201,59336566,"Introduction In today's time, with an ever increasing penetration of social media, news portals, blogs, QnA forums, and other websites that allow user interaction, users often end up inviting comments that are nasty, harrasing, insulting, toxic etc.",45,46
4202,59336566,"As we focus to solve the problem of toxic comments and cyberbullying, we are confronted with the issue of large class imbalance.",8,9
4203,59336566,We seek to answer the following questions: (1) Is combination of Capsules and focal loss the new apotheosis for toxic comment classification problems? (,22,23
4204,59336566,"The six different classes were toxic, severe toxic, obscene, threat, insult and identity hate.",5,6
4205,59336566,"The six different classes were toxic, severe toxic, obscene, threat, insult and identity hate.",8,9
4206,59336566,"We demonstrated that using focal loss along with CapsNet gave us .25 raise in the ROC-AUC for Kaggle's toxic comment classification and 1.39 and .80 gain in F1 scores on TRAC shared task dataset in English, from Facebook and Twitter comments respectively.",21,22
4207,59336566,"For example, one of the cluster contained more of neutral words, another cluster contained highly aggressive and abusive words, and the third cluster contained some toxic words along with place and country names related to one's origin which were used in some foul comments.",28,29
4208,201636820,"The psychological-communicative Reduced cues approach (Sproull and Kiesler, 1986) argues that properties of online environments may cause toxic online disinhibition (Suler, 2004) : people feel less restraint because of the absence of social-context cues, anonymity, invisibility, asynchronicity, or minimization of authority.",22,23
4209,201691562,Automatic hate speech and abusive detection systems can facilitate the prohibition of toxic textual contents.,12,13
4210,201691562,"In this paper, we introduce the first publicly-available Levantine Hate Speech and Abusive (L-HSAB) Twitter dataset with the objective to be a benchmark dataset for automatic detection of online Levantine toxic contents.",37,38
4211,201691562,"Beyond the psychological harm, such toxic online contents can lead to actual hate crimes (Matsuda, 2018) .",6,7
4212,201691562,"Dialectal Arabic Hate/Abusive Speech As seeking to propose a new dialectal Arabic dataset for abusive and HS, we opted to review the Arabic abusive and HS datasets proposed in the State-Of-The-Art focusing on their characteristics in terms of: source, the tackled toxic categories, size, annotation strategy, metrics, the used machine learning models, etc.",52,53
4213,201691562,"According to (Al-Hassan and Al-Dossari, 2019) , the toxic online content on social media can be classified into: Abusive, Obscene, Offensive, Violent, Adult content, Terrorism and Religious hate speech.",15,16
4214,201691562,The proposed dataset was aimed to be a benchmark dataset for automatic detection of online Levantine toxic contents.,16,17
4215,168170119,"They find that a number of such ""identity terms"" are disproportionately represented in the examples labeled as toxic.",19,20
4216,249848125,"2018) and find that 58% of comments that contain the term ""gay"" are labelled as toxic, while only 10% of all comments are toxic.",19,20
4217,249848125,"2018) and find that 58% of comments that contain the term ""gay"" are labelled as toxic, while only 10% of all comments are toxic.",29,30
4218,249848125,"More broadly, communities can reinforce ""toxic technocultures"" (Massanari, 2017) , and those technocultures are not limited to Reddit.",7,8
4219,249848125,"There are independent groups for misogynistic, racist, toxic, anti-hate, black, gay, and trans subreddits.",9,10
4220,226283752,"We do this by building on an existing list of toxic Reddit communities (Caffier, 2017) .",10,11
4221,226283752,"We choose the Perspective API by Conversation AI, which ""identifies whether a comment could be perceived as toxic to a discussion"".",19,20
4222,8358167,"Toxicology and negations A simplified view of toxicology experiments is to distinguish, given the administration of different amounts of a specific compound or drug (e.g. low, medium and high dosage) during predefined time spans, between toxic and non-toxic effects.",40,41
4223,8358167,"Toxicology and negations A simplified view of toxicology experiments is to distinguish, given the administration of different amounts of a specific compound or drug (e.g. low, medium and high dosage) during predefined time spans, between toxic and non-toxic effects.",44,45
4224,8358167,"Here it is important to determine also three kinds of negative associations: (1) under which conditions a given parameter or tissue has not been negatively affected (save dosage, non-toxic), (2) which compound did not show the desired beneficial effect (e.g. was not effective in treating the pathologic condition) and ( 3 ) under which administration conditions a compound was not save.",35,36
4225,9576301,"A drug may alter the metabolism of another, thus causing an enhanced, reduced or even toxic effect in certain medical treatments.",17,18
4226,226283667,"toxic: ""A rude, disrespectful, or unreasonable comment that is likely to make people leave a discussion."" •",0,1
4227,226283667,"severe-toxic: ""A very hateful, aggressive, disrespectful comment or otherwise very likely to make a user leave a discussion or give up on sharing their perspective.",2,3
4228,226283667,"Curious if the problem was some low-frequency classes, we tried training a multi-label model on just the three most frequent classes of the Wikipedia comments dataset (Joint top-3 classes), toxic, obscene, and insult.",37,38
4229,226283667,"For example, both the Single model that predicts all six classes and the Joint top-3 classes model that doesn't even try to predicts severe-toxic, threat, or  identity-hate achieve the same AUC of 0.990.",27,28
4230,174803474,"We show that in contrast to prior work on detecting toxic language, fine-grained incivilities like namecalling cannot be accurately detected by simple models like logistic regression.",10,11
4231,250391103,"Here, we binarise the targets as suggested by the authors into toxic (hatespeech or offensive) and non-toxic (normal).",12,13
4232,250391103,"Here, we binarise the targets as suggested by the authors into toxic (hatespeech or offensive) and non-toxic (normal).",21,22
4233,250390878,"Introduction Transgender individuals are frequent targets of toxic language in online spaces (Craig et al.,",7,8
4234,250390878,"We compare these against a uniform random baseline and a competitive baseline of a commercial model for recognizing toxic language, Perspective API using 0.5 as a cut-off for determining toxicity.",18,19
4235,250390878,The majority of antisocial or toxic language detectors are used punitively for censure or removaluses of toxic speech are removed from public visibility and the transgressing individuals are potentially subject to temporary suspensions or even account removals.,5,6
4236,250390878,The majority of antisocial or toxic language detectors are used punitively for censure or removaluses of toxic speech are removed from public visibility and the transgressing individuals are potentially subject to temporary suspensions or even account removals.,16,17
4237,250390878,Transgender individuals actively and manually identify TERF users to minimize their interactions with such toxic content.,14,15
4238,250390455,"Hate speech distinguishes itself from other types of toxic or offensive content in that it specifically targets an individual or group on the basis of their membership in an identity group, such as race, religion, gender, sexual orientation, etc. (",8,9
4239,250390481,"Gallacher and Bright (2021) explore whether users seek out Gab in order to express hate, or that the toxic attitude is adopted after joining the platform.",21,22
4240,250390481,"Jigsaw Perspective: A widely used commercial model geared toward detection of hateful and toxic content, developed by Google.",14,15
4241,250390661,"To address the often stark gap between model performance on intrinsic metrics and performance in real-world, user-facing scenarios for toxic content classifiers, Gordon et al. (",24,25
4242,250390661,"One approach to consulting diverse perspectives on 'toxic' content relies on jury learning, as in a model proposed by Gordon et al. (",8,9
4243,250390661,"They fine-tune a BERT-based Transformer model on the Jigsaw Kaggle dataset of toxic comments from Wikipedia and provide an online text-editing application that visually highlights words that the models detects as toxic, suggesting alternate phrases that may be less toxic using both word embeddings and language modeling predictions.",16,17
4244,250390661,"They fine-tune a BERT-based Transformer model on the Jigsaw Kaggle dataset of toxic comments from Wikipedia and provide an online text-editing application that visually highlights words that the models detects as toxic, suggesting alternate phrases that may be less toxic using both word embeddings and language modeling predictions.",37,38
4245,250390661,"They fine-tune a BERT-based Transformer model on the Jigsaw Kaggle dataset of toxic comments from Wikipedia and provide an online text-editing application that visually highlights words that the models detects as toxic, suggesting alternate phrases that may be less toxic using both word embeddings and language modeling predictions.",46,47
4246,250390661,"They evaluate the tool using a text-editing task, presenting user study participants with comments drawn from both the Kaggle dataset and Twitter threads, and show that the users in their study are learning about the model behavior by editing toxic comments to be less toxic according to the model prediction scores.",43,44
4247,250390661,"They evaluate the tool using a text-editing task, presenting user study participants with comments drawn from both the Kaggle dataset and Twitter threads, and show that the users in their study are learning about the model behavior by editing toxic comments to be less toxic according to the model prediction scores.",48,49
4248,250391002,"A challenge inherent to defining different forms of hateful, toxic, or offensive language is that the characterization of these terms is necessarily socially, culturally, and politically specific.",10,11
4249,250391002,"In addition, with context but without knowledge of sociological norms within the community, it may still be assumed by a perceiver that the exchange also leads to disengagement, thus qualifying the interaction as toxic.",36,37
4250,250391002,"They primed annotators with a measure of an utterances' alignment with AAE, which significantly reduced the degree to which they rated AAE utterances as toxic.",26,27
4251,250391002,"In addition, when asked to consider the tweet author's likely race, annotators were also less likely to identify AAE tweets as toxic.",24,25
4252,184483646,"Without any preset semantics of toxic content, they came up with the tool that could be manipulated through a modifiable threshold.",5,6
4253,184483646,"This threshold was to be treated as a measure of toxicity, filtering the online toxic content, prior to display of contents in the client's browser.",15,16
4254,184483646,"In 2018, Kaggle hosted a Toxic Comment Classification competition in association with Jigsaw, which focused on classifying Wikipedia comments into one of six categories: insult, obscene, severe toxic, threat & identity hate and toxic.",32,33
4255,184483646,"In 2018, Kaggle hosted a Toxic Comment Classification competition in association with Jigsaw, which focused on classifying Wikipedia comments into one of six categories: insult, obscene, severe toxic, threat & identity hate and toxic.",39,40
4256,184483646,All the six toxic categories are mapped to Offensive (OFF) class and the clean instances are mapped to Not Offensive (NOT) class.,3,4
4257,249151889,"Recently, inappropriate and toxic behaviors of language models have been extensively studied and reported in the literature (Dinan et al.,",4,5
4258,184482880,"Similarly, a taxonomy proposed to detect toxic messages on Wikipedia discussion pages demonstrated the impact on community health both on and offline (Wulczyn et al.,",7,8
4259,236987136,"For example, some works have shown that an adversary can fool toxic content detection (Li et al.,",12,13
4260,236987136,"2018) , toxic content detection (Li et al.,",3,4
4261,248780526,"In social media, there are instances where people present their opinions in strong language, resorting to abusive/toxic comments.",20,21
4262,248780526,"And, in this age of social media, it's very important to find means to keep check on these toxic comments, as to preserve the mental peace of people in social media.",21,22
4263,248721928,"Meanwhile, for offensive span identification in English Ding and Jurgens (2021) coupled LIME with RoBERTa trained on an expanded training set to find expanded training set could help RoBERTa more accurately learn to recognize toxic span.",37,38
4264,232270100,Misogyny automatic detection systems can assist in the prohibition of anti-women Arabic toxic content.,14,15
4265,232270100,The automatic detection of online misogynistic language can facilitate the prohibition of antiwomen toxic contents.,13,14
4266,232270100,"We focus on the characteristics of the presented Arabic annotated resources: data source, data size the tackled toxic categories (for non-misogyny datasets), and annotation/collection strategies.",19,20
4267,232270100,"Arabic Resources for Online Toxicity According to (Al-Hassan and Al-Dossari, 2019), the toxic contents on social media can be classified into: abusive, obscene, offensive, violent, adult content, terrorism, and religious hate speech.",20,21
4268,226283837,"Much NLP research is focused on finding and classifying offensive or toxic language, which is then either directly censored or flagged for human moderators to review.",11,12
4269,226283837,CDNM described their work on detecting conversations that may turn toxic ahead of time as a potential alternative.,10,11
4270,226283837,"Solutions: the NLP community primarily focuses on ""find offensive or toxic language"", with much less focus on other strategies such as counter narratives or preemptive interventions.",12,13
4271,226283936,"For Sample 1, the number of toxic tweets identified by annotators ranges from 12 to 103 and the inter-rater reliability, as measured by Fleiss' Kappa, is 0.37.",7,8
4272,226283936,"Figure 4 plots the number of toxic tweets directed at Lewis, using different thresholds (set at 0.5, 0.6, 0.7 and 0.8).",6,7
4273,226283936,"As expected, the estimated prevalence of toxic tweets directed at Lewis depends on where this arbitrary threshold is set-he received only two tweets with toxicity > 0.8, but 10 with toxicity > 0.7, a 5-fold increase.",7,8
4274,226283936,The principle behind this is that if there are 50 tweets each with 0.2 probability of toxicity then it should be likely that ∼10 will be toxic -and summing the probabilities best capture this.,26,27
4275,226283936,"However, as the second panel of Figure 4 shows, the uncalibrated scores substantially overemphasize non-toxic tweets, and using them for this purpose would inflate the estimated prevalence of abuse.",18,19
4276,233365286,"I have recently realized that I have the hardest time identifying my emotions..."" ""DAE struggle with guilt around cutting off toxic people?"" """,23,24
4277,248780536,"However, on social media platforms, where people have the license to be anonymous, toxic comments targeted at homosexuals, transgenders and the LGBTQ+ community are not uncommon.",16,17
4278,248780094,2020) introduced a multilabel toxic language dataset.,5,6
4279,248780094,2020) developed a corpus to detect toxic speech in Korean online news comments.,7,8
4280,249538400,"As for censorship attacks, a list of common usercensored toxic terms were identified prior based on flagged user content.",10,11
4281,249538400,"For each toxic term, a regex for the censorship pattern was defined such that the first and last letters of the word remain constant but any of the letters in between can be replaced by punctuation characters -maintaining the same length as the original, uncensored word.",2,3
4282,226283508,Prior research on toxic comments online has found that classifiers trained on crowdsourced data can be effective at detecting the most overt forms of toxic comments.,3,4
4283,226283508,Prior research on toxic comments online has found that classifiers trained on crowdsourced data can be effective at detecting the most overt forms of toxic comments.,24,25
4284,226283508,"Unfortunately, for most subtler toxic attributes there are few available datasets (or none, particularly in many languages other than English), which is a bottleneck preventing further research (Fortuna et al.,",5,6
4285,226283508,"From 'toxic' comments to 'unhealthy' conversation In this paper, we broadly characterise a healthy online public conversation as one where posts and comments are made in good faith, are not overly hostile or destructive, and generally invite engagement.",2,3
4286,233365236,"Literature Review Different abusive and offense language identification problems and shared tasks have been explored in the literature ranging from aggression to cyberbullying, hate speech, toxic comments, and offensive language.",27,28
4287,248780058,"2019) that may contain toxic and aggressive content (Anderson, 2015) .",5,6
4288,226283543,"As prior research has shown, annotation tasks that make use of underspecified or subjective phrases such as ""hateful,"" ""toxic"" or ""would make you leave a conversation,"" without further explanation are likely to be interpreted differently depending on the annotator.",23,24
4289,226283543,"Behavior that is casually described as ""toxic"" or ""bullying"" may contain a mix of identity-based hate speech, general insults, threats and inappropriate sexual language.",7,8
4290,222378486,This trade-off is mainly governed by the ratio of toxic to normal instances and not the size of the dataset. •,11,12
4291,222378486,"This definition of toxic language includes some aspects of racism, sexism and hateful behaviour.",3,4
4292,222378486,"2019a) , identity-based abuse is fundamentally different from general toxic behavior.",12,13
4293,222378486,The dataset consists of randomly collected comments and comments made by users blocked for violating Wikipedia's policies to augment the proportion of toxic texts.,23,24
4294,222378486,"Out-of-Domain test set: The toxic portion of our test set is composed of four types of offensive language: Abusive and Hateful from the Fountadataset, and Sexist and Racist from the Waseemdataset.",9,10
4295,222378486,"97% of the instances belonging to this topic are annotated as Toxic, with 96% of them containing explicitly toxic words.",21,22
4296,222378486,"To control for the effect of class balance and dataset size, we run the experiments for two cases of toxic-to-normal ratios, 3K-3K and 3K-27K. Each experiment is repeated 5 times, and the average accuracy per class is reported in Figure 3 .",20,21
4297,222378486,"Therefore, utterances dissimilar to these topics are labelled as Toxic, leading to a high accuracy on the toxic classes and a low accuracy on the Normal class.",19,20
