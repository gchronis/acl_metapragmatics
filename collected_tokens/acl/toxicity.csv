,corpus_id,sentence,start_idx,end_idx
0,245906443,"2020) , toxicity, and bias (Xu et al.,",3,4
1,245906443,"Responsibility, toxicity and bias can also be measured (Xu et al.,",2,3
2,218971825,"2019) , sentiment analysis (Kiritchenko and Mohammad, 2018) , and hate speech/toxicity detection (e.g., Park et al.,",17,18
3,218971825,"2017) ; (b) sentiment analysis systems yielding different intensity scores for sentences containing names associated with African Americans and sentences containing names associated with European Americans (Kiritchenko and Mohammad, 2018) ; and (c) toxicity detection systems scoring tweets containing features associated with African-American English as more offensive than tweets without these features (Davidson et al.,",41,42
4,218971825,"2019) focus on the effect of priming annotators with information about possible dialectal differences when asking them to apply toxicity labels to sample tweets, fnding that annotators who are primed are signifcantly less likely to label tweets containing features associated with African-American English as offensive.",20,21
5,218971825,"2018)) , and that toxicity detection systems score tweets containing features associated with AAE as more offensive than tweets without them (Davidson et al.,",6,7
6,218971825,"Researchers and practitioners should be concerned about ""racial bias"" in toxicity detection systems not only because performance differences impair system performance, but because they reproduce longstanding injustices of stigmatization and disenfranchisement for speakers of AAE.",12,13
7,645466,"In our case, these components are morpheme-like items which either (i) bear referential lexical meaning like confixes (-cyto-, -bio-) and autonomous lexical items (cancer, toxicity) or (ii) can substantially change the meaning of a word, especially prefixes (anti-, post-) and some suffixes (-less, -like).",34,35
8,645466,"For example, toxic can be translated to toxique 'toxic', toxicité 'toxicity' or vénéneux 'poisonous'.",15,16
9,13983070,"These components may be either free (i.e. they can occur in texts as autonomous lexical items like toxicity in cardiotoxicity) or bound (i.e. they cannot occur as autonomous lexical items, in that case they correspond to bound morphemes likecardioin cardiotoxicity).",18,19
10,13983070,Translation occurs regardless of the components' degree of freedom: -cardiomay be translated as coeur 'heart' as in cardiotoxicity → toxicité pour le coeur 'toxicity to the heart'.,28,29
11,13983070,"Allomorphy happens regardless of the components' degree of freedom: -cardio-, coeur 'heart', cardiaque 'cardiac' are possible instantiations of the same abstract component and may lead to terminological variation as in cardiotoxicity → cardiotoxicité 'cardiotoxicity', toxicité pour le coeur 'toxicity to the heart', toxicité cardiaque 'cardiac toxicity'.",50,51
12,13983070,"Allomorphy happens regardless of the components' degree of freedom: -cardio-, coeur 'heart', cardiaque 'cardiac' are possible instantiations of the same abstract component and may lead to terminological variation as in cardiotoxicity → cardiotoxicité 'cardiotoxicity', toxicité pour le coeur 'toxicity to the heart', toxicité cardiaque 'cardiac toxicity'.",60,61
13,13983070,"In our case, these components are morpheme-like items which either (i) bear referential lexical meaning like confixes 4 (-cyto-, -bio-, -ectomy-) and autonomous lexical items (cancer, toxicity) or (ii) can substantially change the meaning of a word, especially prefixes (anti-, post-, co-...) and some suffixes (-less, -like...).",37,38
14,248512793,"More critically, it can be used to prevent the inherent toxicity of language models trained on the internet, or to not reproduce gender or race stereotypes.",11,12
15,248512793,"The ethical interests are thus important, such as adding constraint about race diversity, gender equality, non toxicity, factual faithfulness, etc.",19,20
16,245855914,"A critical error is defined as a translation error falling into one of the following five categories: toxicity, health or safety risk, named entity, sentiment polarity and number or unit deviation.",18,19
17,245855939,"Deviation in toxicity (hate, violence or profanity) be against an individual or a group (a religion, race, gender, etc.).",2,3
18,245855939,"This error can happen because toxicity is introduced in the translation when it is not in the source, deleted in the translation when it was in the source, or mistranslated into different (toxic or not) words, or not translated at all (i.e. the toxicity remains in the source language or it is transliterated). •",5,6
19,245855939,"This error can happen because toxicity is introduced in the translation when it is not in the source, deleted in the translation when it was in the source, or mistranslated into different (toxic or not) words, or not translated at all (i.e. the toxicity remains in the source language or it is transliterated). •",49,50
20,245855939,"Give the nature of this dataset (user generated content with high chances of toxicity, named entities, etc.),",14,15
21,245855939,"LAMA-ICL used additional features to detect the presence/deviation of toxicity, sentiment and named entities, also followed by ensenbling of models with different individual features.",13,14
22,219176615,"Although several 'toxicity' detection datasets and models have been published, most of them ignore the context of the posts, implicitly assuming that comments may be judged independently.",3,4
23,219176615,"We investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and (b) does conditioning on context improve performance of toxicity detection systems?",31,32
24,219176615,We find that context can both amplify or mitigate the perceived toxicity of posts.,11,12
25,219176615,"Moreover, a small but significant subset of manually labeled posts (5% in one of our experiments) end up having the opposite toxicity labels if the annotators are not provided with context.",25,26
26,219176615,"Surprisingly, we also find no evidence that context actually improves the performance of toxicity classifiers, having tried a range of classifiers and mechanisms to make them context aware.",14,15
27,219176615,"Apart from a growing volume of press articles concerning toxicity online, 1 there is increased research interest on detecting abusive and other unwelcome comments labeled 'toxic' by moderators, both for English and other languages.",9,10
28,219176615,"2019) , toxicity is defined as ""a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion"" (Wulczyn et al.,",3,4
29,219176615,TARGET I blame you for my alcoholism add that too Table 1: Comments that are not easily labeled for toxicity without the 'parent' (previous) comment.,20,21
30,219176615,current datasets do not include the preceding comments in a conversation and such context was not shown to the annotators who provided the gold toxicity labels.,24,25
31,219176615,Table 1 shows additional examples of comments that are not easily judged for toxicity without the parent comment.,13,14
32,219176615,"Interestingly, even basic statistics on how often context affects the perceived toxicity of online posts have not been published.",12,13
33,219176615,"Hence, in this paper we focus on the following two foundational research questions: • RQ1: How often does context affect the toxicity of posts as perceived by humans in online conversations?",24,25
34,219176615,And how often does context amplify or mitigate the perceived toxicity?,10,11
35,219176615,"RQ2: Does context actually improve the performance of toxicity classifiers, when they are made context-aware?",9,10
36,219176615,And how can toxicity classifiers be made context-aware?,3,4
37,219176615,"To investigate these questions we created and make publicly available two new toxicity datasets that include context, which are based on discussions in Wikipedia Talk Pages (Hua et al.,",12,13
38,219176615,We used this dataset to show that the perceived toxicity of a significant subset of posts (5.2% in our experiment) changes when context is (or is not) provided.,9,10
39,219176615,We conclude that a small but significant subset of manually labeled posts end up having wrong toxicity labels if the annotators are not provided with context.,16,17
40,219176615,1.6%) the perceived toxicity.,5,6
41,219176615,"To investigate the second question, concerning the effect of context on the performance of toxicity classifiers, we created a larger dataset of 20k comments; 10k comments were annotated out of context, 10k in context.",15,16
42,219176615,"Surprisingly, we found no evidence that context actually improves the performance of toxicity classifiers.",13,14
43,219176615,"While most previous work does not address toxicity in general, instead addressing particular subtypes, toxicity and its subtypes are strongly related, with systems trained to detect toxicity being effective also at subtypes, such as hateful language (van Aken et al.,",7,8
44,219176615,"While most previous work does not address toxicity in general, instead addressing particular subtypes, toxicity and its subtypes are strongly related, with systems trained to detect toxicity being effective also at subtypes, such as hateful language (van Aken et al.,",16,17
45,219176615,"While most previous work does not address toxicity in general, instead addressing particular subtypes, toxicity and its subtypes are strongly related, with systems trained to detect toxicity being effective also at subtypes, such as hateful language (van Aken et al.,",29,30
46,219176615,"Both small and large toxicity datasets have been developed, but approximately half of them contain tweets, which makes reusing the data difficult, because abusive tweets are often removed by the platform.",4,5
47,219176615,"2017a) used professional moderators, who were monitoring entire threads and were thus able to use the context of the thread to judge for the toxicity of the comments.",26,27
48,219176615,Hence no toxicity dataset includes the raw text of both target and parent comments with sufficient links between the two.,2,3
49,219176615,This means that toxicity detection methods cannot exploit the conversational context when being trained on existing datasets.,3,4
50,219176615,"Mikolov and Zweig (2012) , for example, used LDA to encode the preceding sentences and pass the en-  For each comment and group of annotators, the toxicity scores of the annotators were averaged and rounded to the nearest binary decision (toxic, non-toxic) to compute the number of toxic comments (#toxic).",31,32
51,219176615,"3 Experiments Experiments with CAT-SMALL for RQ1 To investigate how often context affects the perceived toxicity of posts, we created CAT-SMALL, a small Context-Aware Toxicity dataset of 250 randomly selected comments from the Wikipedia Talk Pages (Table 4 ).",17,18
52,219176615,We gave these comments to two groups of crowd-workers to judge their toxicity.,14,15
53,219176615,Their depth in their threads was from 2 Figure 1 : Toxicity ratio (%) of the comments of CAT-SMALL when using the toxicity labels of GN (annotators with no context) or GC (annotators with context).,26,27
54,219176615,"For each comment and group of annotators, the toxicity scores of the annotators were first averaged and rounded to the nearest binary decision, as in Table 4 .",9,10
55,219176615,"Figure 1 shows that the toxicity ratio (toxic comments over total) of CAT-SMALL is higher when annotators are given context (GC), compared to when no context is provided (GN).",5,6
56,219176615,"The toxicity ratio increases by 2 percentage points (4.4% to 6.4%) when context is provided, but this is an aggregated result, possibly hiding the true size of the effect of context.",1,2
57,219176615,"The perceived toxicity of some comments may be increasing when context is provided, but for other comments it may be decreasing, and these effects may be partially cancelling each other when measuring the change in toxicity ratio.",2,3
58,219176615,"The perceived toxicity of some comments may be increasing when context is provided, but for other comments it may be decreasing, and these effects may be partially cancelling each other when measuring the change in toxicity ratio.",37,38
59,219176615,"To get a more accurate picture of the effect of context, we measured the number of comments of CAT-SMALL for which the (averaged and rounded) toxicity label was different between the two groups (GN, GC).",30,31
60,219176615,"We found that the toxicity of 4 comments out of 250 (1.6%) decreased with context, while the toxicity of 9 comments (3.6%) increased.",4,5
61,219176615,"We found that the toxicity of 4 comments out of 250 (1.6%) decreased with context, while the toxicity of 9 comments (3.6%) increased.",21,22
62,219176615,"Hence, perceived toxicity was affected for 13 comments (5.2% of comments).",3,4
63,219176615,"While the small size of CAT-SMALL does not allow us to produce accurate estimates of the frequency of posts whose perceived toxicity changes with context, the experiments on CAT-SMALL indicate that context has a statistically significant effect on the perceived toxicity, and that context can both amplify or mitigate the perceived toxicity, thus making a first step to addressing our first research question (RQ1).",23,24
64,219176615,"While the small size of CAT-SMALL does not allow us to produce accurate estimates of the frequency of posts whose perceived toxicity changes with context, the experiments on CAT-SMALL indicate that context has a statistically significant effect on the perceived toxicity, and that context can both amplify or mitigate the perceived toxicity, thus making a first step to addressing our first research question (RQ1).",45,46
65,219176615,"While the small size of CAT-SMALL does not allow us to produce accurate estimates of the frequency of posts whose perceived toxicity changes with context, the experiments on CAT-SMALL indicate that context has a statistically significant effect on the perceived toxicity, and that context can both amplify or mitigate the perceived toxicity, thus making a first step to addressing our first research question (RQ1).",57,58
66,219176615,"Nevertheless, larger annotated datasets need to be developed to estimate more accurately the frequency of context-sensitive posts in online conversations, and how often context amplifies or mitigates toxicity.",31,32
67,219176615,"Experiments with CAT-LARGE for RQ2 To investigate whether adding context can benefit toxicity detection classifiers, we could not use CAT-SMALL, because its 250 comments are too few to effectively train a classifier.",14,15
68,219176615,Figure 2 shows that the toxicity ratio increased (from 0.6% to 1.5%) when context was given to the annotators.,5,6
69,219176615,"Again, the change of toxicity ratio is an indication that context does affect the perceived toxicity, but it does not accurately show how many comments are affected by context, since the perceived toxicity may increase for some comments when context is given, and decrease for others.",5,6
70,219176615,"Again, the change of toxicity ratio is an indication that context does affect the perceived toxicity, but it does not accurately show how many comments are affected by context, since the perceived toxicity may increase for some comments when context is given, and decrease for others.",16,17
71,219176615,"Again, the change of toxicity ratio is an indication that context does affect the perceived toxicity, but it does not accurately show how many comments are affected by context, since the perceived toxicity may increase for some comments when context is given, and decrease for others.",35,36
72,219176615,"Unlike CAT-SMALL, in CAT-LARGE we cannot count for how many comments the perceived toxicity increased or decreased with context, because the two groups of annotators (GN, GC) did not annotate the same comments.",19,20
73,219176615,The toxicity ratios of CAT-LARGE (Fig.,1,2
74,219176615,"1 ), though they both show a trend of increased toxicity ratio when context is provided.",11,12
75,219176615,"The toxicity ratios of CAT-LARGE are more reliable estimates of toxicity in online conversations, since they are based on a much larger dataset.",1,2
76,219176615,"The toxicity ratios of CAT-LARGE are more reliable estimates of toxicity in online conversations, since they are based on a much larger dataset.",12,13
77,219176615,We used CAT-LARGE to experiment with both context-insensitive and context-sensitive toxicity classifiers.,16,17
78,219176615,"On top of the concatenated last states (from the two directions) of the BILSTM, we add a feed-forward neural network (FFNN), consisting of a hidden dense layer with 128 neurons and tanh activations, then a dense layer leading to a single output neuron with a sigmoid that produces the toxicity probability.",58,59
79,219176615,"We used the general toxicity labels of that dataset, and fine-tuned for a single epoch.",4,5
80,219176615,"PERSPECTIVE The third context-insensitive classifier is a CNN-based model for toxicity detection, trained on millions of user comments from online publishers.",14,15
81,219176615,"In each fold (split) of the MC cross-validation, the training, development, and test subsets are 60%, 20%, and 20% of the data, respectively, preserving in each subset the toxicity ratio of the entire dataset.",42,43
82,219176615,"This is not surprising, since these systems were trained (fine-tuned in the case of BERT-CCTK) on much larger toxicity datasets than the other systems (upper two zones of Table 5 ), and BERT-CCTK was also pre-trained on even larger corpora.",25,26
83,219176615,"We conclude for our second research question (RQ2) that we found no evidence that context actually improves the performance of toxicity classifiers, having tried both simple (BILSTM) and more powerful classifiers (BERT), having experimented with several methods to make the classifiers context aware, and having also considered the effect of gold labels obtained out of context vs. gold labels obtained by showing context to annotators.",22,23
84,219176615,Conclusions and Future Work We investigated the role of context in detecting toxicity in online comments.,12,13
85,219176615,"We showed that context does have a statistically significant effect on toxicity annotation, but this effect is seen in only a narrow slice (5.2%) of the (first) dataset.",11,12
86,219176615,"We also found no evidence that context actually improves the performance of toxicity classifiers, having tried both simple and more powerful classifiers, having experimented with several methods to make the classifiers context aware, and having also considered the effect of gold labels obtained out of context vs. gold labels obtained by showing context to the annotators.",12,13
87,219176615,"A Data Annotation Annotators were asked to judge the toxicity of each comment, given the following definitions: • VERY TOXIC: A very hateful, aggressive, disrespectful comment or otherwise very likely to make a user leave a discussion or give up on sharing their perspective. •",9,10
88,219176615,NOT TOXIC: Not containing any toxicity.,6,7
89,219176615,"GC annotators had one more question, which was asking them to compare the toxicity of the target comment to that of the parent comment.",14,15
90,219176615,"On top of the BERT [CLS] representation, we added a FFNN of 128 hidden nodes and a sigmoid to yield the toxicity probability.",24,25
91,184482657,This paper presents the application of two strong baseline systems for toxicity detection and evaluates their performance in identifying and categorizing offensive language in social media.,11,12
92,184482657,"Perspective is an API, that serves multiple machine learning models for the improvement of conversations online, as well as a toxicity detection system, trained on a wide variety of comments from platforms across the Internet.",22,23
93,184482657,"Perspective performed better than BERT in detecting toxicity, but BERT was much better in categorizing the offensive type.",7,8
94,184482657,"The first baseline is a Convolutional Neural Network (CNN) for toxicity detection, trained on millions of user comments from different online publishers, which is made publicly available through the Perspective API.",12,13
95,184482657,"Apart from the growing volume of popular press concerning toxicity online, the increased interest in research into offensive language is partly due to the recent Workshops on Abusive Language Online, 4 as well as other fora, such as GermEval for German texts, 5 or TA-COS 6 and TRAC (Kumar et al.,",9,10
96,184482657,"2018 ) is a deep bidirectional network built using Transformers (Vaswani et Results Offensive Language Detection For Subtask A, we used the toxicity score from Perspective and returned the offensive label (OFF) when the returned score was above 0.5.",24,25
97,184482657,"The performance of Perspective in this subtask is particularly interesting, considering that the training data for these models were not labeled for offensiveness, but rather for other attributes such as toxicity, threats, and insults.",32,33
98,236460230,"Current toxicity detection systems classify whole posts as toxic or not (Schmidt and Wiegand, 2017; Pavlopoulos et al.,",1,2
99,236460230,"This would allow the moderators to more quickly identify objectionable parts of the posts, especially in long posts, and more easily approve or reject the decisions of the toxicity detection systems.",30,31
100,236460230,"As a first step along this direction, Task 5 of SemEval 2021 provided the participants with posts previously rated to be toxic, and required them to identify toxic spans, i.e., spans that were responsible for the toxicity of the posts, when identifying such spans was possible.",40,41
101,236460230,"2017) was used on the Appen crowd rating platform to label the comments using a number of attributes including 'toxicity', 'obscene', 'threat ' Borkan et al. (",21,22
102,236460230,"When studying the toxicity subtypes, we find that the vast majority of posts are annotated as insulting.",3,4
103,236460230,A toxic span was defined to be a sequence of words that attribute to the post's toxicity.,17,18
104,236460230,"The latter is first fine-tuned to classify posts as toxic or nontoxic, using three Kaggle toxicity datasets.",18,19
105,236460230,Rationales Some participants experimented with training toxicity classifiers on external datasets containing posts labeled as toxic or non-toxic; and then employing model-specific or model-agnostic rationale extraction mechanisms to produce toxic spans as explanations of the decisions of the classifier.,6,7
106,236460230,The model-specific rationale mechanism of Rusert (2021) used the attention scores of an LSTM toxicity classifier to detect the toxic spans.,18,19
107,236460230,2016) with a Logistic Regression (LR) or with a linear Support Vector Machine (SVM) toxicity classifier.,19,20
108,236460230,"An added value of these approaches is that easy to use resources (toxicity lexicons) are built and shared publicly, such as the one suggested by Pluciński and Klimczak (2021) .",13,14
109,236460230,"8  Custom losses Zhen Wang and Liu (2021) experimented with a new custom loss, which weighted false toxicity predictions based on their location in the text.",21,22
110,236460230,"2021) , or to filter posts (Luu and Nguyen, 2021) that were not labeled as toxic by a toxicity classifier.",22,23
111,236460230,The strong dependency of toxicity on context makes it particularly difficult to solve with systems based on vocabulary.,4,5
112,236460230,"Finally, future competitions could require participants to distinguish toxic posts of different kinds (e.g., insult, threat, profanity, along with supporting spans), which are sometimes easier to define compared to the more general umbrella toxicity term we (and others) have used.",41,42
113,236486094,User posts whose perceived toxicity depends on the conversational context are rare in current toxicity detection datasets.,4,5
114,236486094,User posts whose perceived toxicity depends on the conversational context are rare in current toxicity detection datasets.,14,15
115,236486094,"Hence, toxicity detectors trained on current datasets will also disregard context, making the detection of context-sensitive toxicity a lot harder when it occurs.",2,3
116,236486094,"Hence, toxicity detectors trained on current datasets will also disregard context, making the detection of context-sensitive toxicity a lot harder when it occurs.",20,21
117,236486094,"We constructed and publicly release a dataset of 10k posts with two kinds of toxicity labels per post, obtained from annotators who considered (i) both the current post and the previous one as context, or (ii) only the current post.",14,15
118,236486094,"We introduce a new task, context sensitivity estimation, which aims to identify posts whose perceived toxicity changes if the context (previous post) is also considered.",17,18
119,236486094,"Such systems could be used to enhance toxicity detection datasets with more context-dependent posts, or to suggest when moderators should consider the parent posts, which may not always be necessary and may introduce an additional cost.",7,8
120,236486094,2017) are called toxicity (or abusive language) detection systems.,4,5
121,236486094,"2018) , is that they disregard the conversational context (e.g., the parent post in the discussion), making the detection of context-sensitive toxicity a lot harder.",28,29
122,236486094,"Although toxicity datasets that include conversational context have recently started to appear, in previous work we showed that context-sensitive posts are still too few in those datasets (Pavlopoulos et al.,",1,2
123,236486094,"2020) , which does not allow models to learn to detect context-dependent toxicity.",15,16
124,236486094,"1  As a first step towards studying contextdependent toxicity, we limit the conversational context to the previous (parent) post of the thread, as in our previous work (Pavlopoulos et al.,",9,10
125,236486094,"We use the new dataset to study the nature of context sensitivity in toxicity detection, and we introduce a new task, context sensitivity estimation, which aims to identify posts whose perceived toxicity changes if the context (previous post) is also considered.",13,14
126,236486094,"We use the new dataset to study the nature of context sensitivity in toxicity detection, and we introduce a new task, context sensitivity estimation, which aims to identify posts whose perceived toxicity changes if the context (previous post) is also considered.",34,35
127,236486094,"Such systems could be used to enhance toxicity detection datasets with more context-dependent posts, or to suggest when moderators should consider the parent posts; the latter may not always be necessary and may also introduce additional cost.",7,8
128,236486094,"To obtain a single toxicity score per post, we calculated the percentage of the annotators who found the post to be insulting, profane, identity-attack, hateful, or toxic in another way (i.e., all toxicity sub-types provided by the annotators were collapsed to a single toxicity label).",4,5
129,236486094,"To obtain a single toxicity score per post, we calculated the percentage of the annotators who found the post to be insulting, profane, identity-attack, hateful, or toxic in another way (i.e., all toxicity sub-types provided by the annotators were collapsed to a single toxicity label).",41,42
130,236486094,"To obtain a single toxicity score per post, we calculated the percentage of the annotators who found the post to be insulting, profane, identity-attack, hateful, or toxic in another way (i.e., all toxicity sub-types provided by the annotators were collapsed to a single toxicity label).",54,55
131,236486094,"2017) , who also found that training using the empirical distribution (over annotators) of the toxic labels (a continuous score per post) leads to better toxicity detection performance, compared to using labels reflecting the majority opinion of the raters (a binary label per post).",30,31
132,236486094,Figure 2 shows the number of posts (Y axis) per ground truth toxicity score (X axis).,14,15
133,236486094,"The vast majority of the  posts were unanimously perceived as NON-TOXIC (0.0 toxicity), both by the OC and the IC coders.",16,17
134,236486094,"However, IC coders found fewer posts with toxicity greater than 0.2, compared to OC coders.",8,9
135,236486094,"For each post p, we define s ic (p) to be the toxicity (fraction of coders who perceived the post as toxic) derived from the IC coders and s oc (p) to be the toxicity derived from the OC coders.",15,16
136,236486094,"For each post p, we define s ic (p) to be the toxicity (fraction of coders who perceived the post as toxic) derived from the IC coders and s oc (p) to be the toxicity derived from the OC coders.",41,42
137,236486094,"3 shows that δ is most often zero, but when the toxicity score changes, δ is most often positive, i.e., showing the context to the annotators reduces the perceived toxicity in most cases.",12,13
138,236486094,"3 shows that δ is most often zero, but when the toxicity score changes, δ is most often positive, i.e., showing the context to the annotators reduces the perceived toxicity in most cases.",33,34
139,236486094,"In numbers, in 66.1% of the posts the toxicity score remained unchanged while out of the remaining 33.9%, in 9.6% it increased (960 posts) and in 24.2% it decreased (2,408) when context was provided.",10,11
140,236486094,"If we binarize the ground truth we get a similar trend, but with the toxicity of more posts remaining unchanged (i.e., 94.7%).",15,16
141,236486094,"Experimental Study Initially, we used our dataset to experiment with existing toxicity detection systems, aiming to investigate if context-sensitive posts are more difficult to automatically classify correctly as toxic or nontoxic.",12,13
142,236486094,"Then, we trained new systems to solve a different task, that of estimating how sensitive the toxicity score of each post is to its parent post, i.e., to estimate the context sensitivity of a target post.",18,19
143,236486094,Toxicity Detection We employed the Perspective API toxicity detection system to classify CCC posts as toxic or not.,7,8
144,236486094,"Hence, the benefits of integrating context in toxicity detection systems may be visible only in sufficiently context-sensitive subsets, like the ones we would obtain by evaluating (and training) on posts with t ≥ 0.2.",8,9
145,236486094,"4 ), hence adding context mechanisms to toxicity detectors has no visible effect in test scores.",8,9
146,236486094,"2020) , where we found that context-sensitive posts are too rare and, thus, context-aware models do not perform better on existing toxicity datasets.",28,29
147,236486094,"5 , the larger the difference between the toxicity scores of OC and IC annotators, hence the larger the difference between the (OC) ground truth that Perspective saw and the ground truth that we use here (IC).",8,9
148,236486094,5 ) would be to train toxicity detectors on datasets that are richer in context-sensitive posts.,6,7
149,236486094,These posts can then be used to train toxicity detectors on datasets richer in context-sensitive posts.,8,9
150,236486094,"This may be due  to the fact that it is often possible to decide if a post is context-sensitive or not (we do not score the toxicity of posts in this section) by considering only the target post without its parent (e.g., in responses like ""NO!!"").",30,31
151,236486094,"If the hypothesis is verified, manually annotating context-sensitivity (not toxicity) may also require only the target post.",13,14
152,236486094,"2017) reported that training toxicity regressors (based on the empirical distribution of codes) instead of classifiers (based on the majority of the codes) leads to improved classification results too, so we also computed classification results.",5,6
153,236486094,"2019) , this work uses toxicity as an umbrella term for hateful, identity-attack, insulting, profane or posts that are toxic in another way.",6,7
154,236486094,"We started to investigate context-sensitivity in toxicity detection in our previous work (Pavlopoulos et al.,",8,9
155,236486094,2020) using existing toxicity detection datasets and a much smaller dataset (250 posts) we constructed with both IC and OC labels.,4,5
156,236486094,"Conclusions and Future Work We introduced the task of estimating the contextsensitivity of posts in toxicity detection, i.e., estimating the extent to which the perceived toxicity of a post depends on the conversational context.",15,16
157,236486094,"Conclusions and Future Work We introduced the task of estimating the contextsensitivity of posts in toxicity detection, i.e., estimating the extent to which the perceived toxicity of a post depends on the conversational context.",27,28
158,236486094,"Context-sensitivity estimation systems can be used to collect larger samples of context-sensitive posts, which is a prerequisite to train toxicity detectors to better handle context-sensitive posts.",24,25
159,236486094,"In future work, we hope to incorporate context mechanisms in toxicity detectors and train (and evaluate) them on datasets sufficiently rich in context-sensitive posts.",11,12
160,248780005,Our work highlights challenges in finer toxicity detection and mitigation.,6,7
161,248780005,"Although several toxicity detection datasets (Wulczyn et al.,",2,3
162,248780005,"But highlighting such toxic spans can assist human moderators (e.g., news portal moderators) who often deal with lengthy comments, and who prefer attribution instead of a system-generated unexplained toxicity score per post.",34,35
163,248780005,"Using the latter, we introduce a measure to evaluate the elimination of explicit toxicity, and we use this measure to compare the behavior and performance of toxicto-civil models.",14,15
164,248780005,Our work differs from general toxicity detection 1 Our code and dataset are publicly available at https: //github.com/ipavlopoulos/toxic_spans with a CC0 licence.,5,6
165,248780005,"in that we detect toxic spans, instead of assigning toxicity labels to entire texts.",10,11
166,248780005,"2020) , but specifically for toxic posts, a task that has never been considered in general toxicity detection before.",18,19
167,248780005,"Hate speech is a particular type of toxicity (Borkan et al.,",7,8
168,248780005,"2019) , which can be tackled by more general toxicity detectors (Van Aken et al.,",10,11
169,248780005,"2019) , which already provides whole-post toxicity annotations.",9,10
170,248780005,"We followed the toxicity definition that was used in Civil Comments, i.e., we use 'toxic' as an umbrella term that covers abusive language phenomena, such as insults, hate speech, identity attack, or profanity.",3,4
171,248780005,"This definition of toxicity has been used extensively in previous work (Hosseini et al.,",3,4
172,248780005,"Besides toxicity our annotators were also asked to select a subtype for each highlighted span, choosing between insult, threat, identity-based attack, profane/obscene, or other toxicity.",1,2
173,248780005,"Besides toxicity our annotators were also asked to select a subtype for each highlighted span, choosing between insult, threat, identity-based attack, profane/obscene, or other toxicity.",33,34
174,248780005,"Asking the annotators to also select a category was intended as a priming exercise to increase their engagement, but it may have also helped them align their notions of toxicity further, increasing inter-annotator agreement.",30,31
175,248780005,"For example, in some posts the core message being conveyed may be inherently toxic (e.g., a sarcastic post indirectly claiming that people of a particular origin are inferior) and, hence, it may be difficult to attribute the toxicity of those posts to particular spans.",43,44
176,248780005,"Although our dataset contains only posts found toxic by at least half of the original crowd-raters, only 31 of the 87 posts were found toxic by all five of our annotators, and 51 were found toxic by the majority of our annotators; this is an indicator of the well-known subjectivity of toxicity detection.",58,59
177,248780005,"On the 31, 51, and 87 posts, the average kappa score was 65%, 55%, 48%, respectively, indicating that when the raters agree (at least by majority) about the toxicity of the post, there is also reasonable agreement regarding the toxic spans.",40,41
178,248780005,"Our kappa score is in fact slightly higher than in previous work on toxicity detection, classifying posts as toxic or not (Sap et al.,",13,14
179,248780005,"We then assigned a toxicity score to each character offset of t, computed as the fraction of raters who annotated that character offset as toxic (included it in their toxic spans).",4,5
180,248780005,"We retained only character offsets with toxicity scores higher than 50%; i.e., at least two raters must have included each character offset in their spans.",6,7
181,248780005,It also confirms it is not always possible to attribute (at least not by consensus) the toxicity of a post to particular toxic spans.,18,19
182,248780005,"Weakly supervised learning We trained binary classifiers to predict the toxicity label of each post, and we employed attention as a rationale extraction mechanism at inference to obtain toxic spans, an approach Pavlopoulos et al. (",10,11
183,248780005,2017b) found to work reasonably well in toxicity detection.,8,9
184,248780005,These methods require training posts annotated only with toxicity labels per post (no toxic span annotations).,8,9
185,248780005,"However, TRAIN-MATCH performs much better, which agrees with our hypothesis that toxicity detection is a broader problem than hate speech detection.",15,16
186,248780005,"Interestingly, the BILSTM binary toxicity classifier with the attention-based toxic span detection mechanism (Pavlopoulos et al.,",5,6
187,248780005,"Several large datasets with post-level toxicity annotations are publicly available (Pavlopoulos et al.,",7,8
188,248780005,"More specifically, we study the following research question: ""Can TOXICSPANS data and toxic span detectors be used to assess the mitigation of explicit toxicity in toxic-to-civil transfer?""",26,27
189,248780005,"Accuracy measures the rate of successful transfers from toxic to civil, and computes the fraction of posts whose civil version is classified as non-toxic by a BERT toxicity classifier; we used the BERT-based toxicity classifier of Laugier et al. (",30,31
190,248780005,"Accuracy measures the rate of successful transfers from toxic to civil, and computes the fraction of posts whose civil version is classified as non-toxic by a BERT toxicity classifier; we used the BERT-based toxicity classifier of Laugier et al. (",39,40
191,248780005,"TOXICSPANS and toxic span detectors are an opportunity to move towards this direction, by studying how well transfer models cope with explicit toxicity, i.e., spans that can be explicitly pointed to as sources of toxicity.",23,24
192,248780005,"TOXICSPANS and toxic span detectors are an opportunity to move towards this direction, by studying how well transfer models cope with explicit toxicity, i.e., spans that can be explicitly pointed to as sources of toxicity.",37,38
193,248780005,"Explicit Toxicity Removal Accuracy Recall that the accuracy (ACC) scores of Table 6 measure the percentage of toxic posts that the transfer models (CAE-T5, SED-T5) rephrased to forms that a (BERT-based) toxicity classifier considered non-toxic.",44,45
194,248780005,"One could question, however, if it is possible (even for humans) to produce a civil rephrase of a toxic post when it is impossible to point to particular spans of the post that cause its toxicity (as in the last post of Table 1 ).",39,40
195,248780005,"Using these toxic spans, we define three additional variants of accuracy: ACC2 is the same as ACC, but ignores (in its denominator) posts that do not contain at least one toxic span; ACC3 also considers (in its denominator) only posts that contained at least one toxic span, but computes the fraction of these posts that had all of their toxic spans rephrased (even partly) by the transfer model; ACC4 is a stricter version of ACC3 that requires the posts to also be judged non-toxic by the (BERTbased) toxicity classifier.",103,104
196,248780005,"Another interesting observation is that ACC4 is always substantially lower than ACC3 (for both systems, on all three datasets), indicating that the models often successfully detect toxic spans and try to rephrase them, but the rephrases are still toxic, at least according to the toxicity classifier.",50,51
197,248780005,"The 6 posts were mainly cases where the human changed the context to mitigate toxicity, while retaining the original toxic span.",14,15
198,248780005,"Overall, we conclude that humans did rephrase almost all cases of explicit toxicity in the toxic posts they were given.",13,14
199,248780005,We also applied SPAN-BERT-SEQ to the gold target (rephrased) posts that the humans provided to check if any explicit toxicity remained or was introduced by the rephrases.,25,26
200,248780005,Toxicity scores of posts with and without explicit toxicity We also applied the BERT-based text toxicity classifier of Laugier et al. (,8,9
201,248780005,Toxicity scores of posts with and without explicit toxicity We also applied the BERT-based text toxicity classifier of Laugier et al. (,17,18
202,248780005,"2021) to the 2,778 posts of the P dataset, dividing them in two sets: posts that comprised at least one toxic span detected by SPAN-BERT-SEQ (1,354 posts with explicit toxicity) and the rest (implicit toxicity).",37,38
203,248780005,"2021) to the 2,778 posts of the P dataset, dividing them in two sets: posts that comprised at least one toxic span detected by SPAN-BERT-SEQ (1,354 posts with explicit toxicity) and the rest (implicit toxicity).",44,45
204,248780005,"The BERT-based toxicity classifier considered more toxic (higher average toxicity score) the 1,354 posts of the first set compared to the second one, i.e., it was more confident that the posts of the first set (explicit toxicity) were toxic, as one might expect.",4,5
205,248780005,"The BERT-based toxicity classifier considered more toxic (higher average toxicity score) the 1,354 posts of the first set compared to the second one, i.e., it was more confident that the posts of the first set (explicit toxicity) were toxic, as one might expect.",12,13
206,248780005,"The BERT-based toxicity classifier considered more toxic (higher average toxicity score) the 1,354 posts of the first set compared to the second one, i.e., it was more confident that the posts of the first set (explicit toxicity) were toxic, as one might expect.",43,44
207,248780005,The difference of the average predicted toxicity score between the two sets is 14% (from 0.94 down to 0.80).,6,7
208,248780005,This is why we also explored adding rationale extraction components on top of toxicity classifiers trained on existing much larger datasets.,13,14
209,248780005,"This may be particularly useful in low-resourced languages with limited resources for text toxicity (Zampieri et al.,",15,16
210,248780005,"Having two separate systems, one for toxicity detection and one for toxic spans identification, is more easily compatible with existing deployed toxicity detectors.",7,8
211,248780005,"Having two separate systems, one for toxicity detection and one for toxic spans identification, is more easily compatible with existing deployed toxicity detectors.",23,24
212,248780005,"One can simply add a component for toxic spans at the end of a pipeline for toxicity detection, and the new component would be invoked only when toxicity would be detected, leaving the rest of the existing pipeline unchanged.",16,17
213,248780005,"One can simply add a component for toxic spans at the end of a pipeline for toxicity detection, and the new component would be invoked only when toxicity would be detected, leaving the rest of the existing pipeline unchanged.",28,29
214,248780005,"A direct comparison (in terms of size) of TOX-ICSPANS with other existing toxicity datasets is only possible if one focuses on the toxic class, typically the minority one, since our dataset contains only toxic posts.",16,17
215,248780005,"As shown in Section 7, the TOXICSPANS dataset and toxic span detectors can also help study and evaluate explicit toxicity removal when rephrasing toxic posts to be civil.",20,21
216,248780005,"However, this is a danger that concerns any toxicity detection system, including systems that classify user content at the post level (without detecting toxic spans).",9,10
217,248780005,"Conclusions and future work We studied toxicity detection, which aims to identify the spans of a user post that make it toxic.",6,7
218,248780005,Our work is the first of this kind in general toxicity detection.,10,11
219,248780005,"A post-level BILSTM toxicity classifier that was combined with an attention-based attribution method, not trained on annotations at the span level, performed well for the task.",5,6
220,248780005,This result is particularly interesting for future work aiming to perform toxic spans detection by using only datasets with whole-post toxicity annotations.,22,23
221,248780005,"In a final experiment, we examined toxic-to-civil transfer, showing how toxic spans can help shed more light on this task too, by helping assess how well systems and humans address explicit toxicity.",38,39
222,7934160,"2017) , which contain English Wikipedia comments labeled for personal attacks, aggression, toxicity.",15,16
223,7934160,"Wikipedia comments are longer (median 38 and 39 tokens for attacks, toxicity) compared to Gazzetta's (median 25).",13,14
224,2944650,"The second additional dataset, called 'toxicity' dataset, contains approx.",7,8
225,2944650, 2017) show that results on the 'attacks' and 'toxicity' datasets are very similar; we do not include results on the latter in this paper to save space.,13,14
226,102353948,"We use our approach to attack a toxicity classifier, aimed at detecting toxic language on social media (Hosseini et al.,",7,8
227,102353948,"We measured the toxicity probability before and after our attack and found that the average toxicity probability decreased from 0.9 to 0.67, with an average of 5.0 flips per sentence.",3,4
228,102353948,"We measured the toxicity probability before and after our attack and found that the average toxicity probability decreased from 0.9 to 0.67, with an average of 5.0 flips per sentence.",15,16
229,102353948,"We asked annotators whether sentences are toxic, and measured average annotated toxicity.",12,13
230,102353948,Table 2 shows examples for sentences attacked by DISTFLIP-10 and the change in toxicity score according to The Google Perspective API.,13,14
231,102353948,"Moreover, we show that our beam ← Initialize beam with the original sentence and its toxicity score.",16,17
232,102353948,"3: while True do 4: bf, tox ← Pop from the beam the flipped sentence with lowest toxicity, and its toxicity.",20,21
233,102353948,"3: while True do 4: bf, tox ← Pop from the beam the flipped sentence with lowest toxicity, and its toxicity.",24,25
234,102353948,12: if f lip score > min score then Prune using beam score 13: tox ← Compute the toxicity of f lip sent with a forward pass.,20,21
235,102353948,14: max tox in beam ← Pop from the new beam the maximal toxicity score.,14,15
236,102353948,"In Algorithm 1, the toxicity score of a sentence is the result of running it through the source model, and the beam score is the first-order estimate described in Section 3.",5,6
237,231709253,"However, this reduced model still outperforms toxicity and sentiment models predicting on full conversations.",7,8
238,231709253,"Toxicity: Toxicity and severe toxicity scores as estimated by the Perspective API (Wulczyn et al.,",5,6
239,231709253,"The toxicity and sentiment features do not perform better than the bag-of-words baseline, which indicates that these features by themselves do not capture constructiveness in disagreements.",1,2
240,231709253,"However, the neural model's performance when predicting on half the conversation only is still better than the toxicity and sentiment models on the complete conversations.",19,20
241,248366513,"2020) , toxicity in generated dialogue may begin with biases and offensive content in the training data, and debiasing techniques focused on gender can reduce the number of sexist comments generated by the resulting system.",3,4
242,248366513,2021) proposed using class-conditional LMs as discriminators to reduce the toxicity produced by large pre-trained LMs (GPT-2).,13,14
243,203078302,An NLP system designed to model notions such as sentiment and toxicity should ideally produce scores that are independent of the identity of such entities mentioned in text and their social associations.,11,12
244,203078302,We demonstrate the utility of this analysis by employing it on two different NLP models -a sentiment model and a toxicity model -applied on online comments in English language from four different genres.,20,21
245,203078302,"For instance, the sentences I hate Justin Timberlake and I hate Rihanna both express the same semantics using identical constructions; however, the toxicity model used in our experiments gives a significantly higher score to the former (0.90) than the latter (0.69) (see Table 1 for more examples).",25,26
246,203078302,"The primary contributions of this paper are: (i) a simple and effective general-purpose model evaluation metric, which we call perturbation sensitivity analysis, for measuring unintended bias; (ii) a large-scale systematic analysis of model sensitivity to name perturbations, on two tasks -sentiment and toxicity -across four different genres of English text; (iii) a demonstration of how name perturbation can reveal undesired biases in the learned model towards names of popular personalities; (iv) showing the downstream impact of name sensitivity, controlling for prediction thresholds.",55,56
247,203078302,"On average, sentences subjected to name perturbation resulted in a wide range of scores; i.e., ScoreRange over 0.10 for toxicity, and 0.36-0.42 for sentiment.",22,23
248,203078302,"Similarly, ScoreDev values for the sentiment model is also higher (over 0.07 across board) compared to that of the toxicity model (around 0.02), suggesting that the sentiment model is much more sensitive to the named entities present in text than the toxicity model.",22,23
249,203078302,"Similarly, ScoreDev values for the sentiment model is also higher (over 0.07 across board) compared to that of the toxicity model (around 0.02), suggesting that the sentiment model is much more sensitive to the named entities present in text than the toxicity model.",47,48
250,203078302,"Replacing a pronoun with some names increases the toxicity scores by over 0.03 on average, while other names decrease the scores by almost 0.02 on average.",8,9
251,203078302,It is also notable that leaders (politicians) and actors in our list have higher toxicity associations than musicians and athletes.,16,17
252,203078302,"It is possible that a model would be more stable on sentences with highly toxic language, but the effect of perturbation is more prevalent in sentences that have fewer signals of toxicity.",32,33
253,203078302,"2019) ) over a range of NLP tasks such as co-reference resolution and machine translation, as well as the tasks we studied -sentiment analysis and toxicity prediction.",29,30
254,218487466,"In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis.",22,23
255,218487466,"Figure 1a shows the results for toxicity prediction (Jigsaw, 2017) , which outputs a score ∈ [0, 1], with higher scores indicating more toxicity.",6,7
256,218487466,"Figure 1a shows the results for toxicity prediction (Jigsaw, 2017) , which outputs a score ∈ [0, 1], with higher scores indicating more toxicity.",30,31
257,218487466,"All categories of disability are associated with varying degrees of toxicity, while the aggregate average score diff for recommended phrases was smaller (0.007) than that for non-recommended phrases (0.057).",10,11
258,218487466,"Similar to the toxicity model, we see patterns of both desirable and undesirable associations.",3,4
259,218487466,"A.2 Tabular versions of results In order to facilitate different modes of accessibility, we here include results from the experiments in table form in Table 4 and Table 5 A.3 Text classification analyses for individual phrases Figures 3 and 4 show the sensitivity of the toxicity and sentiment models to individual phrases.",46,47
260,231741340,Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors.,18,19
261,231741340,"Despite the use of synthetic labels, this method reduces dialectal associations with toxicity.",13,14
262,231741340,"At the core of the issue are dataset biases, i.e., spurious correlations between surface patterns and annotated toxicity labels ( §2), which stem from the data creation process (Sap et al.,",19,20
263,231741340,"The bottom pair shows dialect-based racial bias for two inoffensive greetings, where markers of African American English (AAE) trigger the toxicity detector.",25,26
264,231741340,"speech datasets (both shown in Figure 1 ): lexical bias which associates toxicity with the presence of certain words (e.g., profanities, identity mentions; Dixon et al.,",13,14
265,231741340,"2019) and dialectal bias, where toxicity is correlated with surface markers of African American English (AAE; Davidson et al.,",7,8
266,231741340,"First, compared to these NLU tasks where there is one correct label, the toxicity of language is inherently more nuanced, subjective, and contextual, which causes toxic language datasets to have lower agreement in general (Ross et al.,",15,16
267,231741340,"2019), 4 we take a more nuanced view of how lexical items can convey toxicity, inspired by work in pragmatics and sociolinguistics of rudeness (Dynel, 2015; Kasper, 1990, inter alia) .",16,17
268,231741340,"Dialectal Biases (AAE) Current toxic language detection systems also associate higher toxicity with dialectal markers of African American English (AAE; Sap et al.,",13,14
269,231741340,"2019) , a state-of-the-art classifier for the toxicity detection task.",14,15
270,231741340,"Intuitively, the lower the FPR * , the less the model infers lexical associations for toxicity, and hence is less biased.",16,17
271,231741340,"Specifically, we report the Pearson's correlation between the gold standard toxicity label and whether or not it contains NOI, OI, or ONI mentions.",12,13
272,231741340,"As shown in Table 1 , subsets given by AFLite and the ambiguous and hard regions produced by DataMaps reduce the overall associations between TOXTRIG words and toxicity, compared to the original and random baselines; DataMaps-Hard has the largest reduction.",27,28
273,231741340,"On the other hand, as expected, DataMaps-Easy shows an increased association between TOXTRIG mentions and toxicity, showing that the these examples display overt lexical biases.",19,20
274,231741340,This is despite DataMaps-Easy showing an increased association between TOXTRIG mentions and toxicity (Table 1 ).,14,15
275,231741340,"The first example contains an atypical example of toxicity, towards white folks, which the annotators might have been unaware of.",8,9
276,231741340,"We call this test set ONI-Adv (for adversarial) since it challenges models with a reversal in the association between toxicity and offensive non-identity words (e.g., ""f*ck"", ""sh*t"").",23,24
277,231741340,"We report F 1 for all models in Figure 2 , which shows how well a model identifies toxicity in offensive tweets that do not contain overtly lexical cues of toxicity.",18,19
278,231741340,"We report F 1 for all models in Figure 2 , which shows how well a model identifies toxicity in offensive tweets that do not contain overtly lexical cues of toxicity.",30,31
279,231741340,"Regardless, none of the models we test are good at predicting subtle, non-overt toxicity.",17,18
280,231741340,"Then, analogous to the lexical bias evaluation, we quantify the dialectal debiasing using the Pearson's correlation between estimated probabilities of AAE and toxicity (R AAE ), and the false positive rates of models on AAE tweets (FPR AAE ).",25,26
281,231741340,Takeaway: F 1 (↑) measures show that all models perform poorly at identifying toxic text not containing overtly lexical cues of toxicity.,24,25
282,231741340,"Notably, DataMaps-Hard performs the best at dialectal debiasing, both in terms of toxicity-AAE correlation (R AAE ) and in terms of false flagging of toxicity (FPR AAE ).",16,17
283,231741340,"Notably, DataMaps-Hard performs the best at dialectal debiasing, both in terms of toxicity-AAE correlation (R AAE ) and in terms of false flagging of toxicity (FPR AAE ).",31,32
284,231741340,"Racial Biases To quantify the real-world impact of dialectbased racial bias, we measure the rates of toxicity predicted by models on a corpus of tweets for which the race of authors is available, but not annotations of toxicity.",19,20
285,231741340,"Racial Biases To quantify the real-world impact of dialectbased racial bias, we measure the rates of toxicity predicted by models on a corpus of tweets for which the race of authors is available, but not annotations of toxicity.",41,42
286,231741340,"13  Listed in Table 5 , our results show that automatic debiasing methods do not consistently decrease the racial discrepancy in flagging toxicity.",23,24
287,231741340,"Notably, the toxicity rates on tweets by African American authors-and the diferences compared to white authors-are similar across all debias-  ing methods and baselines, except for DataMaps-Easy, which shows the most racial bias in toxicity flagging.",3,4
288,231741340,"Notably, the toxicity rates on tweets by African American authors-and the diferences compared to white authors-are similar across all debias-  ing methods and baselines, except for DataMaps-Easy, which shows the most racial bias in toxicity flagging.",44,45
289,231741340,"Focusing on dialectal bias, our key assumption is that an AAE tweet and its corresponding WAE version should have the same toxicity label, therefore toxic AAE tweets whose WAE versions are non-toxic are candidates for label correction.",22,23
290,231741340,"To assess the validity of the relabeling, the first three authors manually annotated toxicity of 50 randomly selected relabeled tweets.",14,15
291,231741340,"As shown in Table 5 , the model trained on AAE-relabeled has the lowest racial disparity in toxicity flagging rates compared to all other methods.",19,20
292,231741340,"Additionally, to ensure less biased toxicity labeling, we recommend recruiting AAE speakers or experts for avoiding over-identification of AAE-markers as toxic (Spears, 1998; Croom, 2013) .",6,7
293,231741340,"Alternatively, we recommend exploring more holistic representations of social biases or toxicity (e.g., Social Bias Frames; Sap et al.,",12,13
294,231741340,"2020) , which could cause differential toxicity in the translations.",7,8
295,231741340,"Focusing on two types of biases, lexical and dialectal, our experiments show that these methods face significant challenges in reducing the biased behavior in toxicity detectors.",26,27
296,231741340,"Appendix A Further Details for Models A.1 Model Debiasing The LEARNED-MIXIN ensemble allows the model to explicitly determine how much to trust the bias given the input: pi =softmax{log(p i ) + g(x i ) log b i } where x i is the ith input text, p i and b i is the toxicity prediction produced by RoBERTa, and bias-only model respectively, and g is a parametric function, which is defined as softplus(w • h i ), where w is a learned vector, h i is the last hidden layer of the model for example x i , and the softplus(x) = log(1 + exp x).",59,60
297,196211238,We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets.,18,19
298,196211238,"Figure 1 illustrates how phrases in the African American English dialect (AAE) are labelled by a publicly available toxicity detection tool as much Figure 1 : Phrases in African American English (AAE), their non-AAE equivalents (from Spears, 1998), and toxicity scores from PerspectiveAPI.com.",20,21
299,196211238,"Figure 1 illustrates how phrases in the African American English dialect (AAE) are labelled by a publicly available toxicity detection tool as much Figure 1 : Phrases in African American English (AAE), their non-AAE equivalents (from Spears, 1998), and toxicity scores from PerspectiveAPI.com.",50,51
300,196211238,"Perspective is a tool from Jigsaw/Alphabet that uses a convolutional neural network to detect toxic language, trained on crowdsourced data where annotators were asked to label the toxicity of text without metadata.",30,31
301,196211238,"We establish strong associations between AAE markers (e.g., ""n*ggas"", ""ass"") and toxicity annotations, and show that models acquire and replicate this bias: in other corpora, tweets inferred to be in AAE and tweets from self-identifying African American users are more likely to be classified as offensive.",19,20
302,196211238,"Data Bias To quantify the racial bias that can arise during the annotation process, we investigate the correlation between toxicity annotations and dialect probabilities given by Blodgett et al. (",20,21
303,196211238,Table 1 shows the Pearson r correlation between p AAE and each toxicity category.,12,13
304,196211238,"Then, we apply these models to two reference Twitter corpora, described below, and compute average rates of reported toxicity, showing how these biases generalize to other data.",21,22
305,196211238,"For each of the two toxic language corpora, we train a classifier to predict the toxicity label of a tweet.",16,17
306,196211238,"Results Figure 2 (left) shows that while both models achieve high accuracy, the false positive rates (FPR) differ across groups for several toxicity labels.",27,28
307,196211238,"9  With a distinct set of workers for each condition, we gather five annotations apiece for a sample of 1,351 tweets stratified by dialect, toxicity category, and dataset (DWMW17 and FDCL18).",27,28
308,196211238,"10 Annotations in the control setting agreed moderately with toxicity labels in DWMW17 and FDCL18 (Pearson r = 0.592 and r = 0.331, respectively; p 0.001).",9,10
309,196211238,"A.4 Toxicity and AAE in the PerspectiveAPI We compare the toxicity detection rates from our trained models to those of PerspectiveAPI, 13 a Jigsaw/Alphabet initiative to detect hate speech online.",10,11
310,221878771,"We create and release RE-ALTOXICITYPROMPTS, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widelyused toxicity classifier.",32,33
311,221878771,"We create and release RE-ALTOXICITYPROMPTS, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widelyused toxicity classifier.",37,38
312,221878771,"We empirically assess several controllable generation methods, and find that while data-or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning ""bad"" words), no current method is failsafe against neural toxic degeneration.",37,38
313,221878771,"As illustrated in Figure 1 , they can easily degenerate into toxicity, even without explicitly toxic prompts, which hinders their safe deployment (McGuffie and Newhouse, 2020) .",11,12
314,221878771,"We release REALTOXICI-TYPROMPTS ( §4), a set of 100K naturally occurring prompts (i.e., sentence prefixes; Figure 1 ) extracted from a large corpus of English web text and paired with toxicity scores from a widely used and commercially deployed toxicity detector (PERSPECTIVE API).",39,40
315,221878771,"We release REALTOXICI-TYPROMPTS ( §4), a set of 100K naturally occurring prompts (i.e., sentence prefixes; Figure 1 ) extracted from a large corpus of English web text and paired with toxicity scores from a widely used and commercially deployed toxicity detector (PERSPECTIVE API).",48,49
316,221878771,"We find that certain controllable methods (e.g., toxicity control tokens, swearword filters) are less successful than more computationally or data-intensive methods (e.g., finetuning on non-toxic corpora).",9,10
317,221878771,"Finally, to further investigate the potential cause of these phenomena, we present the first largescale analysis of toxicity in GPT-2's training corpus, OpenAI WebText, (OPENAI-WT; Radford et al.,",19,20
318,221878771,Our findings highlight the difficulty of avoiding toxicity in natural language generation (NLG) and illustrate a need to actively reconsider the content used in LM pretraining.,7,8
319,221878771,"1,2   2 Operationalizing Toxicity Characterizing the toxicity of large corpora of naturally occurring or machine generated text is crucial to understanding toxic degeneration by language models.",7,8
320,221878771,"Unfortunately, such large scale prevents human annotations of toxicity (e.g., we score at least 80 GB of text in §6).",9,10
321,221878771,"Since the model is calibrated using isotonic regression (Zadrozny and Elkan, 2002) , 5 we can meaningfully interpret the score as a probability of toxicity.",27,28
322,221878771,"Notably, recent work has found that systems are overestimating the prevalence of toxicity in text that contains a minority identity mention (e.g., ""I'm a gay man""; Dixon et al.,",13,14
323,221878771,"This is partially due to detectors' over-reliance on lexical cues of toxicity (including swearwords, slurs, and other ""bad"" words Dinan et al.,",14,15
324,221878771,"We further discuss and examine the effect of these biases in the Appendix, by assessing that the racial bias in toxicity is invariant with respect to model choice (Appendix §C.1) and analyzing the presence of profanity and swearwords separately from toxicity (Appendix §C.2).",21,22
325,221878771,"We further discuss and examine the effect of these biases in the Appendix, by assessing that the racial bias in toxicity is invariant with respect to model choice (Appendix §C.1) and analyzing the presence of profanity and swearwords separately from toxicity (Appendix §C.2).",44,45
326,221878771,"2017) language models: GPT-1, 5 https://github.com/conversationai/ perspectiveapi/blob/master/3-concepts/ score-normalization.md 6 To assess PERSPECTIVE API on human-generated text, the first three authors performed manual judgments of toxicity of a sample of 100 documents from OWTC, and found an 88% pairwise agreement (Pearson ⇢=0.83) with TOXICITY scores.",36,37
327,221878771,"9 For each model, we first generate a pool of 10K spans, and then perform bootstrap estimation of the expected maximum toxicity for n  10K generations, by sampling (with replacement) n generations from the pool 1K times each.",24,25
328,221878771,"Our results (Figure 2 ) show that all five language models can degenerate into toxicity of over 0.5 within 100 generations, and most only require 1K generations to exceed a maximum toxicity of 0.9 (see Table 15 and 16 in Appendix §E for examples).",15,16
329,221878771,"Our results (Figure 2 ) show that all five language models can degenerate into toxicity of over 0.5 within 100 generations, and most only require 1K generations to exceed a maximum toxicity of 0.9 (see Table 15 and 16 in Appendix §E for examples).",34,35
330,221878771,"We find similar patterns of expected maximum toxicity for GPT-2 and CTRL, which have significantly more overlap in pretraining data than with GPT-1.",7,8
331,221878771,"Though trained on a much larger corpus, GPT-3's unprompted toxicity also mirrors Figure 2 : Neural models generate toxicity, even with no prompting.",11,12
332,221878771,"Though trained on a much larger corpus, GPT-3's unprompted toxicity also mirrors Figure 2 : Neural models generate toxicity, even with no prompting.",20,21
333,221878771,"Here we display bootstrap estimates of the expected maximum toxicity for N generations, with variance bounds as shades.",9,10
334,221878771,"For example, we observe that GPT-2 generates an expected maximum toxicity of 0.65 with just 100 unprompted generations.",11,12
335,221878771,"On the other hand, GPT-1 generates higher levels of expected toxicity with fewer generations.",11,12
336,221878771,This may be explained by the correspondingly high levels of toxicity in GPT-1's pretraining corpus (see Appendix §D.3 for details).,10,11
337,221878771,We also observe that CTRL-WIKI has a significantly lower expected maximum toxicity than the other models.,13,14
338,221878771,"These results suggest that models acquire toxicity from their pretraining data, which we analyze further in §6.",6,7
339,221878771,"REALTOXICITYPROMPTS To systematically evaluate and compare the generations from language models, we create REAL-TOXICITYPROMPTS as a testbed for toxicity in conditional language generation that mirrors real world applications (e.g., autocomplete systems; Chen et al.,",21,22
340,221878771,"With this dataset, we quantify the effect of prompt toxicity on the toxicity of generation from our five language models.",10,11
341,221878771,"With this dataset, we quantify the effect of prompt toxicity on the toxicity of generation from our five language models.",13,14
342,221878771,"To obtain a stratified range of prompt toxicity, 10 we sample 25K sentences from four equal-width toxicity ranges ([0,.25), ..., [.75,1] ), for a total of 100K sentences.",7,8
343,221878771,"To obtain a stratified range of prompt toxicity, 10 we sample 25K sentences from four equal-width toxicity ranges ([0,.25), ..., [.75,1] ), for a total of 100K sentences.",20,21
344,221878771,"We then split sentences in half, yielding a prompt and a continuation, both of which we also score for toxicity.",21,22
345,221878771,"We find that prompt and continuation toxicity are slightly anti-correlated (r = -0.08, p  0.001), indicating that, in our documents, toxicity as measured by PERSPECTIVE API is usually confined to one half of the sentence.",6,7
346,221878771,"We find that prompt and continuation toxicity are slightly anti-correlated (r = -0.08, p  0.001), indicating that, in our documents, toxicity as measured by PERSPECTIVE API is usually confined to one half of the sentence.",29,30
347,221878771,"We characterize toxicity in prompted generations with two metrics: 1) the expected maxi-mum toxicity over k = 25 generations, which we estimate with a mean and standard deviation; and 2) the empirical probability of generating a span with TOXICITY 0.5 at least once over k = 25 generations.",2,3
348,221878771,"We characterize toxicity in prompted generations with two metrics: 1) the expected maxi-mum toxicity over k = 25 generations, which we estimate with a mean and standard deviation; and 2) the empirical probability of generating a span with TOXICITY 0.5 at least once over k = 25 generations.",17,18
349,221878771,"These metrics characterize toxic generations along two axes: the higher the expected maximum toxicity, the more toxic we expect the worst-case generations to be, and the higher the toxicity probability, the more frequently the model generates toxicity.",14,15
350,221878771,"These metrics characterize toxic generations along two axes: the higher the expected maximum toxicity, the more toxic we expect the worst-case generations to be, and the higher the toxicity probability, the more frequently the model generates toxicity.",33,34
351,221878771,"These metrics characterize toxic generations along two axes: the higher the expected maximum toxicity, the more toxic we expect the worst-case generations to be, and the higher the toxicity probability, the more frequently the model generates toxicity.",42,43
352,221878771,"Our results show that while toxic prompts unsurprisingly yield higher toxicity in generations, nontoxic prompts still can still cause toxic generations at non-trivial rates (Table 2 ).",10,11
353,221878771,"Specifically, all five models have a toxicity probability near or above 0.5 for non-toxic prompts.",7,8
354,221878771,"This shows that even in innocuous contexts these models can still generate toxic content (as illustrated in Table 17 and 18 in Appendix §E), suggesting the need for models to ""unlearn"" toxicity.",37,38
355,221878771,"Surprisingly, even CTRL-WIKI has similar generation toxicity to other models in prompted settings, even though it was trained on just Wikipedia.",9,10
356,221878771,"These results suggest that like the provenance of pretraining data ( §3.1), prompt context can heavily influence generation toxicity, and that steering generations after pretraining is crucial to prevent toxic behavior in language models.",21,22
357,221878771,"In the following section, we explore the effectiveness of a variety of such methods to avoid toxicity.",17,18
358,221878771,Detoxifying Generations We investigate the effectiveness of recent controllable generation methods at steering away from toxicity using REALTOXICITYPROMPTS.,15,16
359,221878771,"2019), we prepend a corresponding toxicity attribute token (<|toxic|>, <|nontoxic|>) to a random sample of documents and pretrain the GPT-2 language model further.",7,8
360,221878771,"2017) , we learn a 2-dimensional representation of toxicity and non-toxicity for every token in GPT-2's vocabulary, which we then use to boost the likelihood of non-toxic tokens.",11,12
361,221878771,"2017) , we learn a 2-dimensional representation of toxicity and non-toxicity for every token in GPT-2's vocabulary, which we then use to boost the likelihood of non-toxic tokens.",15,16
362,221878771,We learn this representation using the toxicity labels on the balanced corpus described in §5.1 (See Appendix §B.3 for more details).,6,7
363,221878771,"In our experiments, we steer generations using the toxicity classifier released by the authors and the Hugging Face implementation.",9,10
364,221878771,"Of all methods, DAPT (Non-Toxic), vocabulary shifting, and PPLM yield the lowest toxicity in generation.",19,20
365,221878771,"toxicity, highlighting the importance of pretraining data in neural toxic degeneration.",0,1
366,221878771,"Prompts That Challenge All Models We find that certain prompts consistently cause all models to generate toxicity (e.g., the four prompts in Figure 1 ).",16,17
367,221878771,"Specifically, we quantify the toxicity in OPENAI-WT (GPT-2's training data; Radford et al.,",5,6
368,221878771,"Toxicity in Web Text Shown in Figure 3 , we find that both corpora contain non-negligible amounts of toxicity, with 2.1% of OWTC having TOXICITY 0.5, and 4.3% of OPENAI-WT.",20,21
369,221878771,"2018) , who find that the prevalence of abusive or toxic content online roughly ranges between 0.1% and 3%, and suggest that these corpora merely reflect the ""natural"" rates of toxicity.",36,37
370,221878771,"2019) employing a blocklist of subreddits and ""bad"" words, the toxicity in OPENAI-WT is twice the amount in OWTC.",14,15
371,221878771,"We show similar rates of toxicity using alternative PERSPECTIVE API labels on these corpora in Table 12 in Appendix §D. Sources of Toxic Content in Web Text Since Reddit is known to have hosted communities that endorse hateful norms and conspiracy theories (Romano, 2017) , we investigate the provenance of data in our web text corpora.",5,6
372,221878771,"Specifically, we quantify the variation of a document's toxicity with respect to the reliability of its host news site and 0.84 TOXICITY SCORE Posted to /r/The Donald (quarantined) "" [...] Criticism of Hillary is sexist! [...]",10,11
373,221878771,"We highlight spans that contribute to the overall toxicity of the document, which we identify manually.",8,9
374,221878771,"17 Unsurprisingly, documents shared on those subreddits contain substantially more toxicity than those from standard subreddits (see Figure 10 in Appendix §D), confirming Reddit users' propensity to share oppressive and abusive content (Massa-nari, 2017; Mohan et al.,",11,12
375,221878771,"Discussion and Recommendations Overall, our investigations demonstrate that toxicity is a prevalent issue in both neural language generation and web text corpora.",9,10
376,221878771,"Although they show some reduction in toxicity, steering methods do not fully protect neural models from toxic degeneration ( §5).",6,7
377,221878771,"The non-trivial amounts of toxicity generated by DAPT suggest that perhaps language models may be ""memorizing"" the toxicity in pretraining data (Carlini et al.,",6,7
378,221878771,"The non-trivial amounts of toxicity generated by DAPT suggest that perhaps language models may be ""memorizing"" the toxicity in pretraining data (Carlini et al.,",21,22
379,221878771,"Future work could explore whether some variants of toxicity are harder to forget than others, or whether the biases of models used to select training data for steering introduce unwanted side effects in language model behavior after adaptation.",8,9
380,221878771,"2020) , which is among the most effective methods we tested at avoiding toxicity with toxic prompts.",14,15
381,221878771,"In addition to automated toxicity classifiers, future work could explore the use of handpicked toxic documents as ""negative examples"" to avoid toxicity in generation.",4,5
382,221878771,"In addition to automated toxicity classifiers, future work could explore the use of handpicked toxic documents as ""negative examples"" to avoid toxicity in generation.",24,25
383,221878771,"However, our analyses reveal toxicity in web text data that likely enable language models to generate even unprompted toxicity ( §3.1).",5,6
384,221878771,"However, our analyses reveal toxicity in web text data that likely enable language models to generate even unprompted toxicity ( §3.1).",19,20
385,221878771,"However, the conclusions one can make about the effectiveness of a detoxification method are limited by the biases of the model used to detect toxicity ( §2.2).",25,26
386,221878771,"To combat these issues, we encourage further work on detecting and controlling different types of toxicity and undesirable social biases in generation, e.g., rudeness (Danescu-Niculescu-Mizil et al.,",16,17
387,221878771,"First, as noted in §2.2, we use an imperfect measure of toxicity that could bias the toxicity towards lexical cues, failing to detect more subtle biases and incorrectly flagging non-toxic content.",14,15
388,221878771,"First, as noted in §2.2, we use an imperfect measure of toxicity that could bias the toxicity towards lexical cues, failing to detect more subtle biases and incorrectly flagging non-toxic content.",19,20
389,221878771,"Further work could extend our analyses to toxicity to masked language models (Wang and Cho, 2019) , among others.",7,8
390,221878771,"Lastly, because OPENAI-WT does not have available metadata, and due to the imperfect coverage of our subreddit and news reliability data, we only provide lower bound estimates of toxicity in web text corpora.",33,34
391,221878771,"Related Work A wealth of work has shown that toxicity and social biases in training data are acquired by large pretrained sentence encoders (e.g., gender bias in BERT; May et al.,",9,10
392,221878771,"However, fewer studies have investigated toxicity in autoregressive language models, whose generations also suffer from incoherence, blandness, and repetitiveness (Holtzman et al.,",6,7
393,221878771,"In this work, we find and release naturally occurring prompts from web text that trigger toxicity, and compare toxic output in several language models.",16,17
394,221878771,"Additionally, our work focuses on the broad phenomenon of toxicity in generations, whereas Sheng et al. (",10,11
395,221878771,"Under this framework, we quantify the toxicity of multiple pretrained language models and the effectiveness of methods for detoxifying generations.",7,8
396,221878771,"We then analyze toxicity in two large web text corpora, including the GPT-2 pretraining corpus, to better understand the root cause of toxic generations.",3,4
397,207853290,"Most previous approaches to understanding the implied harm in statements have cast this task as a simple toxicity classification (e.g., Waseem and Hovy, 2016; Founta et al.,",17,18
398,207853290,"However, simple classifications run the risk of discriminating against minority groups, due to high variation and identity-based biases in annotations (e.g., which cause models to learn associations between dialect and toxicity; Sap et al.,",36,37
399,207853290,"Offensiveness is our main categorical annotation, and denotes the overall rudeness, disrespect, or toxicity of a post.",16,17
400,207853290,"Related Work Bias and toxicity detection Detection of hateful, abusive, or other toxic language has received increased attention recently (Schmidt and Wiegand, 2017) , and most dataset creation work has cast this detection problem as binary classification (Waseem and Hovy, 2016; Davidson et al.,",4,5
401,207853290,"2019) create a multilingual dataset of 13k tweets annotated for five different emotion-and toxicity-related aspects, including a 16-class variable representing social groups targeted.",16,17
402,207853290,"In comparison, SOCIAL BIAS FRAMES not only captures binary toxicity and hierarchical information about whether a group is targeted, but also free-text implications about 1.4k different targeted groups and the implied harm behind statements.",10,11
403,244117167,"The perceived toxicity of language can vary based on someone's identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases.",2,3
404,244117167,"We seek to understand the who, why, and what behind biases in toxicity annotations.",14,15
405,244117167,Our results show strong associations between annotator identity and beliefs and their ratings of toxicity.,14,15
406,244117167,We additionally present a case study illustrating how a popular toxicity detection system's ratings inherently reflect only specific beliefs and perspectives.,10,11
407,244117167,"Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.",5,6
408,244117167,"In this work, we investigate the who, why, and what behind biases 1 in toxicity annotations, through online studies with demographically and politically diverse participants.",17,18
409,244117167,"We measure the effects of annotator identities (who annotates as toxic) and attitudes or beliefs (why they annotate as toxic) on toxicity perceptions, through the lens of social psychology research on hate speech, free speech, racist beliefs, altruism, political leaning, and more.",25,26
410,244117167,"In our breadth-of-workers controlled study, we collect ratings of toxicity for a set of 15 hand curated posts from 641 annotators of different races, attitudes, and political leanings.",14,15
411,244117167,Figure 1 : Annotator identities and attitudes can influence how they rate toxicity in text.,12,13
412,244117167,"We summarize the key findings from our analyses of biases in toxicity (offensiveness or racism) ratings for three types of language: anti-Black content, African American English (AAE), and vulgar language.",11,12
413,244117167,"Then, in our breadth-of-posts study, we simulate a typical toxic language annotation setting by collecting toxicity ratings for ∼600 posts, from a smaller but diverse pool of 173 annotators.",21,22
414,244117167,"Additionally, annotators' conservatism scores were associated with higher ratings of toxicity for AAE ( §5), and conservative and traditionalist attitude scores with rating vulgar language as more toxic ( §6).",12,13
415,244117167,"We further provide a case study which shows that PERSPECTIVEAPI, a popular toxicity detection system, mirrors ratings by annotators of certain attitudes and identities over others ( §7).",13,14
416,244117167,"The Who, Why, and What of Toxicity Annotations We aim to investigate how annotators' ratings of the toxicity of text is influenced by their own identities (who they are; §2.1), and their beliefs 2 Please contact the authors for the anonymized study data. (",20,21
417,244117167,"While some annotator toxicity ratings may highly correlate with demographic factors at face value (Prabhakaran et al.,",3,4
418,244117167,"We aim to understand how characteristics of text can affect ratings of toxicity, in addition to annotator attitudes and identities.",12,13
419,244117167,Data & Study Design We design two online studies to study the effect of annotator identities and attitudes on their toxicity ratings on posts with different characteristics.,20,21
420,244117167,"5 We specifically focus on readers' perceptions or opinions, instead of imposing prescriptive definitions of toxicity or hate speech which previous work has shown still suffers from large annotator disagreement (Ross et al.,",17,18
421,244117167,Breadth-of-Workers Study Our first study focuses on collecting toxicity ratings from a wide and diverse set of participants for a controlled set of posts.,12,13
422,244117167,"Each participant gave each of the 15 posts toxicity ratings, after which they answered a series of questions about their attitudes and their identity.",8,9
423,244117167,"For further details, please see Appendix C. In our subsequent analyses, we compute associations between the toxicity ratings and identities or attitudes by computing the effect sizes (Pearson r correlation or Cohen's d) between the average toxicity rating of the posts in each category and annotator identities or attitude scores.",18,19
424,244117167,"For further details, please see Appendix C. In our subsequent analyses, we compute associations between the toxicity ratings and identities or attitudes by computing the effect sizes (Pearson r correlation or Cohen's d) between the average toxicity rating of the posts in each category and annotator identities or attitude scores.",41,42
425,244117167,"We do not consider posts that are anti-Black and AAE, since the pragmatic toxicity implications of anti-Black meaning expressed in AAE are very complex (e.g., in-group language with selfdeprecation, sarcasm, reclaimed slurs; Greengross and Miller, 2008; Croom, 2011) , and are thus beyond the scope of this study.",16,17
426,244117167,"7 Additionally, we asked participants one-item versions of our attitude scales, using the question from each scale that correlated best with toxicity in our breadth-of-workers study as explained in Appendix D.3.",25,26
427,244117167,"In our analyses, we examine toxicity of anti-Black and potentially vulgar posts ( §4.2) and of AAE and potentially vulgar posts ( §5.2), but not of vulgar posts separately, due to confounding effects of the AAE or anti-Black characteristics that those posts could have.",6,7
428,244117167,"Thus, we compute associations between toxicity ratings and identities or attitudes using a linear mixed effects model 8 with random effects for each participant.",6,7
429,244117167,"2019) or overt-which is often a desired target for toxic language detection research (Waseem, 2016; Vid-7 For each post, we collected toxicity ratings from two white conservative workers, two from white liberal workers, and two from Black workers.",28,29
430,244117167,"Breadth-of-Workers Results As shown in Table 3 , we found several associations between annotator beliefs and toxicity ratings for anti-Black posts, confirming our hypotheses.",20,21
431,244117167,"In the context of toxicity annotation and detection, our findings highlight the need to consider the attitudes of annotators towards free speech, racism, and their beliefs on the harms of hate speech, for an accurate estimation of anti-Black language as toxic, offensive, or racist (e.g., by actively taking into consideration annotator ideologies; Waseem, 2016; Vidgen et al.,",4,5
432,244117167,"2010; DeFrank and Kahlbaugh, 2019) , we hypothesize that annotators who are more conservative and who score higher in TRADITIONALISM and LING-PURISM will rate AAE posts with higher toxicity.",33,34
433,244117167,Breadth-of-Workers Results Table 5 shows significant associations between annotator identities and beliefs and their ratings of toxicity of AAE posts.,20,21
434,244117167,"As an additional investigation, we measure whether attitudes or identities affects toxicity ratings of AAE posts that contain the word ""n*gga,"" a (reclaimed) slur that has very different pragmatic interpretations depending on speaker and listener identity (Croom, 2011) .",12,13
435,244117167,"For example, annotators could explicitly in- Table 7 : Associations between toxicity ratings and annotator variables for the vulgar posts from the breadthof-workers study.",12,13
436,244117167,"Specifically, future work might consider studying the specific toxicity of individual identity-referring vulgar (OI) words, which can carry prejudiced meaning as well (e.g., slurs such as ""n*gg*r"").",9,10
437,244117167,Toxicity Detection System Case Study: PERSPECTIVEAPI Our previous findings indicated that there is strong potential for annotator identities and beliefs to affect their toxicity ratings.,24,25
438,244117167,We are additionally interested in how this influences the behavior of toxicity detection models trained on annotated data.,11,12
439,244117167,"We present a brief case study to answer this question with the PERSPECTIVEAPI, 9 a widely used, commercial system for toxicity detection.",22,23
440,244117167,"We investigate whether PERSPECTIVEAPI scores align with toxicity ratings from workers with specific identities or attitudes, using the 571 posts from our breadth-of-posts study.",7,8
441,244117167,"Overall, our findings indicate that PERSPEC-TIVEAPI toxicity score predictions align with specific viewpoints or ideologies, depending on the text category.",9,10
442,244117167,"Particularly, it seems that the API underestimates the toxicity of anti-Black posts in a similar way to annotators who scored higher on the RACISTBELIEFS scale, and aligns more with white annotator's perception of AAE toxicity (vs. Black annotators).",9,10
443,244117167,"Particularly, it seems that the API underestimates the toxicity of anti-Black posts in a similar way to annotators who scored higher on the RACISTBELIEFS scale, and aligns more with white annotator's perception of AAE toxicity (vs. Black annotators).",39,40
444,244117167,"This corroborate prior findings that show that toxicity detection models inherently encode a specific positionality (Cambo, 2021) and replicate human biases (Davani et al.,",7,8
445,244117167,"Discussion & Conclusion Overall, our analyses showed that perceptions of toxicity are indeed affected by annotators' demographic identities and beliefs ( §2).",11,12
446,244117167,"Finally, we showed that a popular toxicity detection system yields toxicity scores that are more aligned with raters with certain attitudes and identities than others ( §7).",7,8
447,244117167,"Finally, we showed that a popular toxicity detection system yields toxicity scores that are more aligned with raters with certain attitudes and identities than others ( §7).",11,12
448,244117167,Variation in toxicity ratings in hate speech datasets.,2,3
449,244117167,In our study we deliberately sought rat-ing of perceptions of toxicity of posts by racially and politically diverse participants.,12,13
450,244117167,"However, many existing hate speech datasets instructed annotators to adhere to detailed definitions of toxicity (Davidson et al.,",15,16
451,244117167,"While those annotation setups and annotator homogeneity could cause less variation in toxicity annotations of anti-Black, AAE, and vulgar posts, there is still empirical evidence of anti-AAE racial biases in some of these datasets (Sap et al.,",12,13
452,244117167,"Given the large variation in perceptions of toxicity that we showed and the implicit encoding of perspectives by toxicity models, we recommend researchers and dataset creators investigate and report annotator attitudes and demographics; researchers could collect attitude scores based on relevant social science research, perhaps in lightweight format as done in our breadth-ofposts study, and report those scores along with the dataset (e.g., in datasheets; Gebru et al.,",7,8
453,244117167,"Given the large variation in perceptions of toxicity that we showed and the implicit encoding of perspectives by toxicity models, we recommend researchers and dataset creators investigate and report annotator attitudes and demographics; researchers could collect attitude scores based on relevant social science research, perhaps in lightweight format as done in our breadth-ofposts study, and report those scores along with the dataset (e.g., in datasheets; Gebru et al.,",18,19
454,244117167,Contextualize toxicity predictions in social variables.,1,2
455,244117167,"However, given this subjectivity, the open question remains: whose perspective should be considered when using toxicity detection models?",18,19
456,244117167,"To try answering this question, we urge researchers and practitioners to consider all stakeholders and end users on which toxicity detection systems might be deployed (e.g., through humancentered design methods; Sanders, 2002; Friedman et al.,",20,21
457,244117167,"Finally, given the increasingly essential role of online platforms in people's daily lives (Rahman, 2017), we echo calls for policy regulating online spaces and toxicity detection algorithms (Jiang, 2020; Benesch, 2020; McGuffie and Newhouse, 2020; Gillespie et al.,",30,31
458,244117167,Beyond toxicity classification: modeling distributions and generating explanations.,1,2
459,244117167,Our findings on the subjectivity of the toxicity detection tasks suggests that standard approaches of obtaining binary (or even n-ary) labels of toxicity and averaging them into a majority vote are inadequate.,7,8
460,244117167,Our findings on the subjectivity of the toxicity detection tasks suggests that standard approaches of obtaining binary (or even n-ary) labels of toxicity and averaging them into a majority vote are inadequate.,26,27
461,244117167,"Instead, researchers could consider modeling the distribution or variation in toxicity labels with respect to individual annotators (Geva et al.,",11,12
462,244117167,"But, perhaps more importantly, we encourage re-thinking the toxicity detection paradigm altogether.",12,13
463,244117167,"With the goal to assist human content moderators, 10 creating systems that explain biased implications of posts could be more helpful than opaque toxicity scores Thus, we advocate for moving away from classification frameworks, and towards more nuanced, holistic, and explainable frameworks for inferring the desired concepts of toxicity and social biases (e.g., Social Bias Frames; Sap et al.,",24,25
464,244117167,"With the goal to assist human content moderators, 10 creating systems that explain biased implications of posts could be more helpful than opaque toxicity scores Thus, we advocate for moving away from classification frameworks, and towards more nuanced, holistic, and explainable frameworks for inferring the desired concepts of toxicity and social biases (e.g., Social Bias Frames; Sap et al.,",53,54
465,244117167,"2011, for measuring empathy) as well as other psychological variables (e.g., propensity to volunteer or to value dignity) could be studied in the context of toxicity perceptions.",30,31
466,244117167,"Additionally, our study focused only on U.S.-centric perspectives; we hope researchers will explore variations in toxicity perceptions in other cultural contexts (e.g., variations based on caste in India).",17,18
467,244117167,"Given an initial set of posts from our categories, we then randomly sample up to 600 posts, stratifying by toxicity label, vulgarity, AAE, and anti-Black meaning.",21,22
468,244117167,"Then, we set up a second MTurk task to collect toxicity ratings, and annotator attitudes and identities.",11,12
469,244117167,"Using the data from the breadth-of-workers study, we select the question that best correlated with all toxicity ratings.",21,22
470,244117167,"Specifically, for each scale, we first take the tweet category with the highest correlation with toxicity (e.g., anti-Black posts for RACISTBELIEFS), and then take the item whose response scores correlated most with the toxicity rating for those posts.",17,18
471,244117167,"Specifically, for each scale, we first take the tweet category with the highest correlation with toxicity (e.g., anti-Black posts for RACISTBELIEFS), and then take the item whose response scores correlated most with the toxicity rating for those posts.",41,42
472,244117167,Those items are bolded in §A. E Further Breadth-of-Workers Results We show all associations between attitudes and toxicity ratings in Table 10 .,22,23
473,244117167,"Additionally, we investigate the differences in the overall toxicity ratings of anti-Black vs. AAE Table 10 : Full set of results from our analyses of the breadth-of-workers study of 15 posts, presented as Pearson r or Cohen's d effect sizes, along with significance levels ( † : p < 0.075, * : p < 0.05, * * : p < 0.001).",9,10
474,244117167,"F Further Breadth-of-Posts Results To account for the varying number of posts that each annotators could rate, we use a linear mixed effects model 17 to compute associations between each post's toxicity ratings and identities or attitudes.",37,38
475,244117167,"Specifically, we our linear model regresses the attitude score onto the toxicity score, with a random effect for each worker.",12,13
476,244117167,G PERSPECTIVEAPI Case Study: Details & Results G.1 Details We first obtain PERSPECTIVE toxicity scores for all the posts in our breadth-of-posts study ( §3.2).,14,15
477,244117167,"Then, for each attitude or identity dimension, we compute the Pearson r correlation between the PERSPECTIVE score and the toxicity ratings from the high and low groups, considering posts from potentially overlapping categories (e.g., AAE and potentially vulgar posts).Finally, we compare the high and low correlations using Fisher's r-to-z transformation (Silver and Dunlap, 1987) .",21,22
478,244117167,"G.2 Results See Table 12 : We correlated the PERSPECTIVEAPI toxicity scores with offensiveness/racism ratings by our annotators, breaking them into two bins based on their attitude scores.",10,11
479,235313967,"In this toy example, given the prompt, ""When she rejected his advance, he grabbed,"" the toxic LM assigns greater weight to ""her"" than ""his"", expressing subtle signals of toxicity that can be leveraged for effective attribute control.",39,40
480,235313967,"We first apply DEXPERTS to the task of language detoxification ( §3), by finetuning an expert and an anti-expert on public comments that are humanannotated for toxicity.",31,32
481,235313967,"Our experimental results show that DEXPERTS can successfully avoid toxicity in language generation while preserving output fluency, outperforming existing detoxification methods on both automatic and human evaluations.",9,10
482,235313967,"Moreover, we find that DEXPERTS continues to outperform baselines when employing only an antiexpert and re-using the base model as the expert, making it one of the only methods that can avoid toxicity without annotated examples of non-toxic content.",36,37
483,235313967,"Experts and Anti-Experts for Controlled Generation Given input text as a prompt, the task of controlled text generation is to generate a continuation that flows naturally from the prompt while having the desired attribute (e.g., positive sentiment) but not an undesired one (e.g., toxicity).",51,52
484,235313967,"Our approach uses an anti-expert that models overt toxicity, as well as an expert that is finetuned on nontoxic data from the same domain.",10,11
485,235313967,"This dataset is obtained by scoring the full Open-WebText corpus with the toxicity classifier from Perspective API 4 and keeping the least toxic 2 percent of documents, a corpus of about 150K documents, or 63M tokens, following the implementation of this baseline from Gehman et al. (",14,15
486,235313967,2020) PPLM uses gradients from a toxicity classifier to update the LM's hidden representations.,7,8
487,235313967,"We retrain the classifier to be compatible with our larger base model size, on the same toxicity data used in the original paper.",17,18
488,235313967,We use the toxicity classconditioned LM released by the authors with the recommended generation hyperparameters.,3,4
489,235313967,"Other training and generation details (e.g., hyperparameters) are described in Appendix A. Automatic Evaluation We evaluate our generations for toxicity, fluency, and diversity.",22,23
490,235313967,"2020) , we characterize generation toxicity using the toxicity score from Perspective API, along two axes: 1) the maximum toxicity over k "" 25 generations, and 2) the empirical probability of generating a continuation with toxicity ě 0.5 at least once over k "" 25 generations.",6,7
491,235313967,"2020) , we characterize generation toxicity using the toxicity score from Perspective API, along two axes: 1) the maximum toxicity over k "" 25 generations, and 2) the empirical probability of generating a continuation with toxicity ě 0.5 at least once over k "" 25 generations.",9,10
492,235313967,"2020) , we characterize generation toxicity using the toxicity score from Perspective API, along two axes: 1) the maximum toxicity over k "" 25 generations, and 2) the empirical probability of generating a continuation with toxicity ě 0.5 at least once over k "" 25 generations.",23,24
493,235313967,"2020) , we characterize generation toxicity using the toxicity score from Perspective API, along two axes: 1) the maximum toxicity over k "" 25 generations, and 2) the empirical probability of generating a continuation with toxicity ě 0.5 at least once over k "" 25 generations.",41,42
494,235313967,"Human Evaluation While automatic toxicity classifiers like Perspective API enable the kind of large-scale evaluation required for systematic comparison of methods, an abundance of work shows that their accuracy is far from ideal (Dixon et al.,",4,5
495,235313967,"Nonetheless, results in Table 2 show that DEXPERTS effectively reduces toxicity from GPT-3 to about the same level as when operating on GPT-2.",11,12
496,235313967,"We finetune GPT-2 Large We can see that even with a dataset of 40,960 tokens (""650 comments) corresponding to ă 0.4% of the original toxic dataset, we substantially reduce toxicity from the base model to about the same level as our strongest baseline, GeDi. (",34,35
497,235313967,"PPLM As with toxicity §3, we retrain the sentiment classifier for PPLM with a larger embedding size compatible with our base model.",3,4
498,235313967,"As with toxicity experiments ( §3), we use nucleus sampling with p "" 0.9, and include our training and generation details in Appendix A. Automatic Evaluation We evaluate our generations for the target sentiment, fluency, and diversity.",2,3
499,235313967,"Part of our work requires automatically detecting toxicity in generated texts, for which we use the Perspective API.",7,8
500,235313967,7 a commercially deployed toxicity detection tool.,4,5
501,235313967,"However, the mismatch between the construct of toxicity and its operationalization through an automatic classifier can cause biased or unintended model behavior (Jacobs and Wallach, 2021) .",8,9
502,235313967,"Specifically, recent work has shown that such hate speech classifiers overestimate the prevalence of toxicity in text that contains a minority identity mention (Hutchinson et al.,",15,16
503,235313967,"To address this limitation, we also perform a human evaluation of toxicity, for which we obtained IRB approval and sought to pay our workers a fair wage (""US$7-9/h).",12,13
504,235313967,"Nevertheless, toxicity in pretrained LMs is an unsolved issue (Sheng et al.,",2,3
505,235313967,"PPLM For our implementation of PPLM in experiments, we retrain the toxicity and sentiment classifiers to be compatible with our base model GPT-2 (large), as the original paper used GPT-2 medium for experiments.",12,13
506,235313967,"GeDi For toxicity and sentiment steering, we download the class-conditioned language models (based on GPT-2 Medium) made available by the original authors.",2,3
507,235313967,D Additional Results D.1 Toxicity Hyperparameter Control Figure 8 shows the relationship between output toxicity and fluency for different values of α in our method.,14,15
508,235313967,"We finetune the sentiment (anti-)experts and all DAPT models for 3 epochs, and the toxicity (anti-)experts for one epoch.",16,17
509,238857348,"2021) and toxicity detection (Halevy et al.,",3,4
510,21688491,"In parallel to the progress in DDI extraction from texts, Graph Convolutional Networks (GCNs) have been proposed and applied to estimate physical and chemical properties of molec-ular graphs such as solubility and toxicity (Duvenaud et al.,",37,38
511,221949376,"A Class overlap and interpretation of ""toxicity""  To see if our results generalize beyond threat, we experimented on the identity-hate class in Kaggle's toxic comment classification dataset.",7,8
512,201070022,"We considered the version of the dataset that corresponds to the Kaggle competition: ""Toxic Comment Classification Challenge"" (Google, 2018) which features 7 classes of toxicity: toxic, severe toxic, obscene, threat, insult, identity hate and non-toxic.",30,31
513,248023030,"2019) , toxicity (Gehman et al.,",3,4
514,248023030,"2021) , and toxicity agreement (Baheti et al.,",4,5
515,1006882,"In our data, this candidate generation process has a recall of 98.9, as a few instances exist in which a conjunction is not used, such as: • I am looking for any information you have about heavy metal toxicity, [treatment, outcomes] EXEMPLIFICATION+COORDINATION .",42,43
516,236459793,"This task aims to locate those spans that attribute to the text's toxicity within a text, which is crucial for semiautomated moderation in online discussions.",13,14
517,6128897,"The corpus is available at http://diego.asu.edu/index.php/projects Introduction Pharmacogenomics is a relatively new area of biomedical research that merges pharmacology and molecular genomics, among other disciplines, and focuses on studying the effects of genetic variability on drug toxicity and efficacy, on the discovery of novel genomic targets for drug development, and on the identification and functional characterization of polymorphisms relevant to drug action.",38,39
518,250390502,"To protect users from online toxicity, social media providers have been increasingly implementing censorship-based measures.",5,6
519,199379867,"Introduction Unexpected Food-Drug Interactions (FDIs) occasionally result in treatment failure, toxicity and an increased risk of side-effects.",15,16
520,33850483,"Product-Dependent Factors Product-dependent internationalization factors pertain to specific features that the product needs to comply with, for example, toxicity regulation and transport dimensions, or that will be acceptable to the market locale, for example, colors or certain types of ingredients and so on.",24,25
521,239618387,"For developing our rule-based system for toxicity detection we also used a corpus of annotated tweets from Germeval challenges of previous years (Wiegand et al.,",8,9
522,239618387,The experiments in this section and the qualitative analysis in Section 7 were performed only for the toxicity detection task.,17,18
523,239618387,"For the toxicity detection task we experimented with simple strategies for automatic bootstrapping of keyword lists, which are then reviewed and corrected manually.",2,3
524,239618387,"The method involves extracting simple patterns from comments in the training data and ranking them according to their potential as rules, i.e. looking for patterns that in themselves have a very high precision as predictors of toxicity.",37,38
525,239618387,"The majority of good patterns comes from the larger toxicity dataset available to us, the 2018 and 2019 Germeval training datasets (Wiegand et al.,",9,10
526,239618387,"While in the smaller 2021 dataset the top-ranked patterns occurred in no more than 3 or 4 positive examples of toxicity, the combined training datasets of previous years allowed us to find patterns with 15-25 positive examples, a much stronger indicator that a word might be a good keyphrase for domain-independent detection of offensive speech.",22,23
527,239618387,Two of the patterns we identified were introduced in the final rule-based system: for the toxicity detection task we categorize a comment as toxic if it contains at least two words with at least four characters each written in ALL-CAPS.,18,19
528,239618387,Results on toxicity detection (Subtask 1) are presented in Table 1 .,2,3
529,239618387,Qualitative analysis The main focus of our rule-based experiments was the toxicity detection.,13,14
530,239618387,"Indeed this group contains several examples where a deep understanding of the comment is necessary to account for its toxicity, demonstrating the complexity of the task.",19,20
531,239618387,"Indeed, if a human expert were to build a complex rule-based model for the toxicity detection task, it may very well contain patterns such as PROTECTED_GROUP + NEGATIVE_PREDICATE and lexica for what words and phrases are to be considered as belonging to each of these categories.",17,18
532,239618387,The analysis in this section was intended to provide examples of the types of challenges a model of toxicity must concern itself with.,18,19
533,239618387,"In particular, false negative predictions are responsible for more than 70% of errors made by both of our top-performing systems, and our analysis suggests that identifying most of these would require more complex rules for modeling specific types of toxicity and the ability to detect sarcasm.",44,45
534,239618387,"Conclusion We described simple methods for the semiautomatic construction of rule-based systems for detecting toxicity in social media, and used them to improve the performance of a BERTbased classifier on the dataset of the 2021 Ger-mEval shared task.",16,17
535,241583270,"Other indicators include: abiotic resource depletion, blue water shortage, human toxicity... Existing tools for deep learning programs focus on the carbon footprint only.",13,14
536,237593022,"For instance, in toxicity classification, certain sub-groups are often predicted more confidently for toxicity (encouraging false negatives for the majority sub-group), which tend to be close to the margin for the non-toxic class (encouraging false positives; Borkan et al. (",4,5
537,237593022,"For instance, in toxicity classification, certain sub-groups are often predicted more confidently for toxicity (encouraging false negatives for the majority sub-group), which tend to be close to the margin for the non-toxic class (encouraging false positives; Borkan et al. (",17,18
538,248118802,"The existing studies have been evaluated on English datasets containing rich demographic variations, such as Wikipedia toxicity comments (Cabrera et al.,",17,18
539,237593027,"γ g is the inverse proportion of positive examples in each group, |g| |g y=1 | , meaning that the more underrepresented a group is wrt some target class (say toxicity), the smaller its accepted deviation from the overall performance.",32,33
540,238226547,"Through a case study on a publicly available toxicity detection model, we demonstrate that our method identifies salient groups of cross-geographic errors, and, in a follow up, demonstrate that these groupings reflect human judgments of offensive and inoffensive language in those geographic contexts.",8,9
541,238226547,"In this paper, we propose a new weakly supervised method (outlined in Figure 1 ) to address shortcomings of current bias detection methods and robustly detect lexical biases in models (i.e. biases that associate the presence or lack of toxicity with certain words) when applied to noisy text (in particular, text generated by social media users from non-predominantly Anglophone nations).",42,43
542,238226547,"As an example, such biases are present even in a commonly-used toxicity detector like the Perspective API, as shown in Table 1 .",14,15
543,238226547,"In the first two sets of rows, we see that the mentions of certain identity groups (e.g., muslims, Tamilian) causes the toxicity model to assign a higher toxicity score.",26,27
544,238226547,"In the first two sets of rows, we see that the mentions of certain identity groups (e.g., muslims, Tamilian) causes the toxicity model to assign a higher toxicity score.",32,33
545,238226547,"The third and fourth set of rows demonstrate that the model does not recognize the toxicity association on the words presstitute and Madarchod, 2 whereas it does recognize the toxicity of words/phrases with the same/similar meanings.",15,16
546,238226547,"The third and fourth set of rows demonstrate that the model does not recognize the toxicity association on the words presstitute and Madarchod, 2 whereas it does recognize the toxicity of words/phrases with the same/similar meanings.",30,31
547,238226547,"Such overrepresentation could be due to different reasons: (1) the model has learned some of these words to be offensive and hence their presence causes it to assign a higher toxicity score, or (2) some of the words appear more often in offensive contexts (e.g., frequent victims of toxicity) but their presence is merely correlational (i.e., the model doesn't have any toxicity association for these words).",33,34
548,238226547,"Such overrepresentation could be due to different reasons: (1) the model has learned some of these words to be offensive and hence their presence causes it to assign a higher toxicity score, or (2) some of the words appear more often in offensive contexts (e.g., frequent victims of toxicity) but their presence is merely correlational (i.e., the model doesn't have any toxicity association for these words).",56,57
549,238226547,"Such overrepresentation could be due to different reasons: (1) the model has learned some of these words to be offensive and hence their presence causes it to assign a higher toxicity score, or (2) some of the words appear more often in offensive contexts (e.g., frequent victims of toxicity) but their presence is merely correlational (i.e., the model doesn't have any toxicity association for these words).",73,74
550,238226547,Another consideration is whether the association of a particular word towards toxicity is desirable or not.,11,12
551,238226547,"Here, we investigate how the sensitivity varies across the range of toxicity scores, which gives important clues about the desirability of biases.",12,13
552,238226547,2019) representations for each term by creating vectors x ∈ R d such that x i is the toxicity after replacing person in the i-th template with a word from a lexicon.,19,20
553,238226547,We then cluster terms that have similar behaviour together to qualitatively distinguish the kind of association they have with toxicity.,19,20
554,238226547,Experimental Setting We use the Perspective API's toxicity model for our analysis.,8,9
555,238226547,"Cluster C0 (top left of Figure 3a ) contains words such as feminist and muslim that increase the toxicity of the templates with the largest changes occurring in the middle, indicating undesirable biases in the model.",19,20
556,238226547,"In contrast, cluster C1 (top right) has words such as shithead that uniformly raise the toxicity of the templates to the very high range, regardless of the initial toxicity of the template.",18,19
557,238226547,"In contrast, cluster C1 (top right) has words such as shithead that uniformly raise the toxicity of the templates to the very high range, regardless of the initial toxicity of the template.",32,33
558,238226547,"Our results show that the words in C0 and C3 are often very similar in their perceived toxicity and, in the case of India, have no statistical difference in their toxicity ratings.",17,18
559,238226547,"Our results show that the words in C0 and C3 are often very similar in their perceived toxicity and, in the case of India, have no statistical difference in their toxicity ratings.",32,33
560,238226547,"Appendix B. The clusters found by our method do correspond to in-community judgments of toxicity and reflect meaningful groups of biases in the model, which can be used by experimenters for later bias testing and mitigation.",16,17
561,238226547,Towards Mitigating Undesirable Biases Our pipeline provides an unsupervised approach to identifying potential latent biases in toxicity models.,16,17
562,238226547,"Experimental Setting Performing bias mitigation experiments involves retraining the model, and hence requires toxicity annotated training data.",14,15
563,238226547,"Since we do not have access to the training data used to train the Perspective API model, we employ the dataset provided by Jigsaw for the Kaggle competition on toxicity classification (Jigsaw, 2019) that contains public comments from the Civil Comments platform and annotated toxicity ratings (on a continuous scale in the range [0, 1]).",30,31
564,238226547,"Since we do not have access to the training data used to train the Perspective API model, we employ the dataset provided by Jigsaw for the Kaggle competition on toxicity classification (Jigsaw, 2019) that contains public comments from the Civil Comments platform and annotated toxicity ratings (on a continuous scale in the range [0, 1]).",48,49
565,238226547,"We train the toxicity model for this analysis by fine-tuning a DistilBert model (Sanh et al.,",3,4
566,238226547,The last two metrics compare the toxicity scoring distributions of the instances with the target words and those instances without.,6,7
567,238226547,"First, mitigation strategies varied in their net effect on recognizing toxicity.",11,12
568,238226547,"In these cases, AUC remained largely the same as without mitigation suggesting these models are still capable of ordering instances correctly by toxicity, but the overall scoring has changed sufficiently to affect classification decisions.",23,24
569,238226547,"Third, the Balance and Tune strategy had negligible effect on both bias correction or toxicity performance.",15,16
570,238226547,Ethical Considerations The proposed method provides an efficient distantly-supervised method for practitioners to identify potential biases in their toxicity detection methods.,20,21
571,238226547,A Templates We present the list of templates as well as their corresponding toxicity scores according to the Perspective model.,13,14
572,244050420,"This is a valid concern, as up to 3% of the web content is considered to contain toxicity (Founta et al.,",19,20
573,236486080,"Further, it covers a wide range of labels of different kinds of toxicity, e.g., sexism, aggression, and hate.",13,14
574,236486080,"How well do hate speech, toxicity, abusive and offensive language classification models generalize across datasets?",6,7
575,236486080,"Collection Creation We consider all publicly accessible comment datasets for the collection that contain labels that are subclasses of toxicity, such as offensive language, abusive language, and aggression.",19,20
576,236486080,The broad definition of toxicity as a higher-level concept builds a bridge between the different lowerlevel concepts.,4,5
577,236486080,"There is a large set of labels that occurs only in one dataset, with each label referring to a particular subclass of toxicity and target, e.g., female football players as in the dataset by Fortuna et al. (",23,24
578,53447013,"Their metric of constructiveness (relevant remarks, specific points, and appropriate evidence) is set in contrast to toxicity (hate speech, verbal abuse, offensiveness).",20,21
579,52310274,"Besides looking at toxicity of online comments (Wulczyn et al.,",3,4
580,52310274,"Besides traditional binary classification tasks, related work considers different aspects of toxic language, such as ""racism"" (Greevy and Smeaton, 2004; Waseem, 2016; Kwok and Wang, 2013) and ""sexism"" (Waseem and Hovy, 2016; Jha and Mamidi, 2017) , or the severity of toxicity (Davidson et al.,",59,60
581,52310274,The toxicity of a comment often depends on expressions made in early parts of the comment.,1,2
582,52310274,Our algorithms can detect their toxicity only if they can recognize multiple words as a single (typical) hateful phrase.,5,6
583,222271929,"Therefore, identifying these comments is an important task for studying and preventing the proliferation of toxicity in social media.",16,17
584,222271929,"In this paper, we propose a new large-scale dataset for Brazilian Portuguese with tweets annotated as either toxic or non-toxic or in different types of toxicity.",30,31
585,222271929,An error analysis and experiments with multi-label classification show the difficulty of classifying certain types of toxic comments that appear less frequently in our data and highlights the need to develop models that are aware of different categories of toxicity.,41,42
586,222271929,"Nevertheless, there are multiple challenges specific to process toxic comments automatically, e.g. (i) toxic language may not be explicit, i.e. may not contain explicit toxic terms; (ii) there is a large spectrum of types of toxicity (e.g. sexism, racism, insult); (iii) toxic comments correspond to a minority of comments, which is fortunate, but means that automatic data-driven approaches need to deal with highly unbalanced data.",43,44
587,222271929,"5 ToLD-Br is available at: https://github.com/ JAugusto97/ToLD-Br Finally, we experiment with multi-label classification, where each different type of toxicity is automatically classified.",29,30
588,222271929,"However, using this strategy alone may hinder learning a model capable of generalising the concept of toxicity beyond the scope of keywords.",17,18
589,222271929,"From these candidates, 42 were selected based on their demographic information, aiming to balance annotation bias as the interpretation of toxicity may vary.",22,23
590,222271929,"When we set an example as positive for toxicity only when all the annotators consider it to have the same category of offence, we insert bias to  ments.",8,9
591,222271929,"In this paper, we consider the least restrictive case, where if at least one annotator marked any offence category in an example, the example is positive for toxicity.",30,31
592,222271929,"We believe that it is essential that if any person feels uncomfortable with a post, it should be flagged as having a certain degree of toxicity.",26,27
593,222271929,"False negative rate LGBTQ+phobia Importance of Large Datasets In this experiment, we highlight the importance of collecting a considerable amount of examples, as toxicity can be expressed in many different ways.",25,26
594,218974083,The stemming misleads the classifiers to wrongly explain the toxicity of the comment with this word.,9,10
595,218974083,"These sparse explanations are suitable for our dataset, as there is typically a small set of toxic words, which explains the toxicity of the entire comment.",23,24
596,218974083,This negative relevance score means that this word speaks against the toxicity of the comment.,11,12
597,218974083,"Without the full context, none of the single words explains the toxicity of the comment.",12,13
598,218974083,"This result confirms that the classifiers detect those words that often constitute the toxicity of a comment (e.g., swear words).",13,14
599,218974083,"However, the true positives of the LSTM approach also contain comments whose toxicity can only be detected with context.",13,14
600,222291028,"We compute Toxicity reward scores using this classifier as Bot Toxicity (e.g. lower toxicity score, higher Bot toxicity reward).",14,15
601,222291028,"We compute Toxicity reward scores using this classifier as Bot Toxicity (e.g. lower toxicity score, higher Bot toxicity reward).",19,20
602,237048417,The first is the 'toxicity' attribute returned by Perspective API (P-TX).,5,6
603,237048417,We test this model because toxicity ratings are the most popular attributes.,5,6
604,229923220,"Round 1 (R1) The target model in R1 is M1, a RoBERTa model trained on R0 which consists of 11 English language training datasets for hate and toxicity taken from hatespeechdata.com, as reported in Vidgen and Derczynski (2020) .",30,31
605,229923220,"2020) , of which Perspective's toxicity classifier 4 is best performing with 77% overall accuracy, including 90% on 'Hate' and 48% on 'Not Hate'.",7,8
606,244464114,"Multi-label toxicity detection is highly prominent, with many research groups, companies, and individuals engaging with it through shared tasks and dedicated venues.",3,4
607,244464114,We selected multi-label toxicity over other label definitions based on its adaptability and feedback from annotators.,5,6
608,244464114,"Related Research The Conversation AI group defined multi-label toxicity, and Wulczyn et al. (",10,11
609,244464114,was for multi-label toxicity to serve as a compatible annotation model for tasks beyond the original Wikipedia dataset.,5,6
610,244464114,"For a detailed overview of multi-label toxicity, look to van Aken et al.,",8,9
611,244464114,A current challenge within the sub-field of toxicity detection is the definition and operationalisation as a concrete task.,9,10
612,244464114,"In addition, we performed further annotation for multi-label toxicity, following the label guidelines of Wulczyn et al.. AMiCA Instant Messages Multi-label Toxicity Annotation To annotate the AMiCA dataset for Multi-label toxicity labels, we used the annotation instructions outlined in (Wulczyn et al.,",11,12
613,244464114,"In addition, we performed further annotation for multi-label toxicity, following the label guidelines of Wulczyn et al.. AMiCA Instant Messages Multi-label Toxicity Annotation To annotate the AMiCA dataset for Multi-label toxicity labels, we used the annotation instructions outlined in (Wulczyn et al.,",39,40
614,244464114,"We calculated an F1 score of 0.51, revealing that multi-label toxicity does not align with cyberbullying.",13,14
615,244464114,"Sub-label Performance Summary We have demonstrated that by using multilingual pre-trained language models within an ensemble approach, we can classify multi-label toxicity in an alternate language.",28,29
616,227231751,"2017) as it is a large dataset annotated for personal attacks, aggression and toxicity -components of offensiveness.",15,16
617,248780050,Then other crowd workers checked the rewritten sentence for toxicity and semantic similarity with the original one.,9,10
618,248780050,All the generated detoxified sentences are then checked for the absence of toxicity and semantic similarity to the original sentence.,12,13
619,248780050,We select only the sentences which were classified as toxic by a pretrained toxicity classifier.,13,14
620,248780050,Style (STA a ) is evaluated with a BERT-based classifier for toxicity detection.,14,15
621,248780050,"We choose this method over perplexity, because it ranges from 0 to 1 and its greater values mean higher quality, just like metrics we use for evaluating toxicity and content.",29,30
622,248780050,Toxicity (STA m ) The toxicity level is defined as: • non-toxic (1) -the sentence does not contain any aggression or offence.,6,7
623,248780050,Note also that toxicity should not be mixed with the lack of formality.,3,4
624,248780050,"It should also be noted that content and toxicity dimensions are independent, so if the output sentence is toxic, it can still be good in terms of content. •",8,9
625,248780050,Crowdsourcing tasks In the toxicity detection task (see Figure 1 ) we show workers the transferred sentence and ask them if it is offensive.,4,5
626,248780050,"We see that automatic and manual toxicity scores are much better correlated for the Delete and RoBERTareplace models, which are the only models to explicitly remove or replace toxic words identified by a classifier or via a manually compiled list of toxic words.",6,7
627,248780050,"The former deliberately ""fools"" the classifier with artificial examples, while the latter contains non-trivial phrases whose level of toxicity is difficult to grasp automatically.",23,24
628,248780050,This suggests that that toxicity is more stable and better interpreted by both humans and models.,4,5
629,239618386,The first subtask is the identification of toxicity or hate speech from German text.,7,8
630,239618397,"In the light of these and other developments, the task of detecting toxicity on the internet has seen increased attention in recent years.",13,14
631,239618397,This work aims to advance the state of the art in the field of toxicity detection by providing an additional avenue of working with scarce data.,14,15
632,239618397,Related Work There have been numerous developments in the space of toxicity detection since GermEval in 2019.,11,12
633,239618397,"Recent Developments in Toxicity Detection In the scope of the Shared Task, the concept of toxicity includes ""uncivil forms of communication that can violate the rules of polite behaviour, such as insulting discussion participants, using vulgar or sarcastic language or implied volume via capital letters"" (Risch et al.,",16,17
634,239618397,2021) notes that context is of key importance for toxicity detection.,10,11
635,239618397,"As surrounding conversation would mitigate a potentially toxic comment, exchange history would inform the determination of toxicity.",17,18
636,239618397,2021) The best-performing systems in the field of toxicity detection online as found by Zampieri et al. (,11,12
637,239618397,"Bolstering the Dataset The standard approach, even in the history of the GermEval shared task, is to use additional, related datasets (Paraschiv and Cercel, 2019) The main issue we face is that the most available datasets for toxicity detection online are English.",43,44
638,250390702,"We investigate the hidden costs: what is ""lost in distillation"", especially in regards to identity-based model bias using the case study of toxicity modeling.",28,29
639,250390702,2019 ) is a public domain corpus of 1.8M user comments labeled for toxicity by crowd raters.,14,15
640,250390702,"Big Data Models Using a combination of publicly available datasets and our much larger proprietary datasets, we show the distillation bias effects in the toxicity space scale to big data.",25,26
641,250390702,PROPRIETARYBERT A state-of-the-art BERT toxicity model that has been pre-trained on more than 1.5B user comments in English.,10,11
642,250390702,"The model was fine-tuned on 3M user generated comments scored by raters for toxicity, bias mitigation data, and the Civil Comments training set with batch size of 512 until convergence.",16,17
643,250390702,"This dataset includes the sentences generated that had a large discrepancy in score between the publicly available toxicity model, Perspective API (Jigsaw, 2017) , and human raters.",17,18
644,250390702,"Identity Swaps Inspired by the work in Covert Toxicity Detecting implicit abuse or covert toxicity, where clearly hateful or abusive words are not used in the comment, presents an especially hard challenge.",14,15
645,250390702,"Given the documented difficulty of toxicity models and hate models to identify such text, we included a representative set as a further baseline.",5,6
646,250390702,"The effects of distillation were more mixed, suggesting that identifying covert toxicity or implicit abuse is a more nuanced and unsolved task and perhaps more reliant on training data.",12,13
647,250390702,"Lower metrics for this particular category suggest that a particular identity is linked to a high false positive rate, which could imply that specific identities are associated with toxicity, independent of context.",29,30
648,250390702,Conclusion The experimental section illustrates for both our more robust big data toxicity models and smaller reproducible versions that we are able to distill large transformer teacher models into smaller student models with very similar metrics on the evaluation datasets drawn from the same distribution.,12,13
649,250390702,"In particular, identity-based bias for the toxicity models is noticeably worse in the distilled model versions, even with the addition of significant quantities biasmitigating data.",9,10
650,250390702,Table 4 shows specific examples with high discrepancy of score between the teacher and student models for both True/False toxicity labels from the curated Identity Swaps set.,21,22
651,233365067,We study the task of labeling covert or veiled toxicity in online conversations.,9,10
652,233365067,Prior research has highlighted the difficulty in creating language models that recognize nuanced toxicity such as microaggressions.,13,14
653,233365067,"Besides, ""toxicity"", other terms like ""hate-speech"", and ""online violence"" have also been used to refer to similar problems (Chandrasekharan et al.,",3,4
654,233365067,"In this work, we chose to use ""toxicity"" because our research suggests that ""toxicity"" is an easy concept for annotators to understand, meaning we can gather opinions from a diverse range of people, allowing us to capture the inherent subjectivity of the concept.",9,10
655,233365067,"In this work, we chose to use ""toxicity"" because our research suggests that ""toxicity"" is an easy concept for annotators to understand, meaning we can gather opinions from a diverse range of people, allowing us to capture the inherent subjectivity of the concept.",17,18
656,233365067,"1 1 https://jigsaw.google.com/the-current/toxicity/ Previous works have published baseline toxicity detection datasets (Borkan et al.,",8,9
657,233365067,"However, (Han and Tsvetkov, 2020) recently established that covert or veiled toxicity is overwhelmingly misclassified by leading models.",15,16
658,233365067,"Perspective, a free API that uses machine learning to score comments for toxicity, is widely used to aid content moderators on online platforms (Jigsaw, 2017) .",13,14
659,233365067,is assigned a toxicity probability score of 0.31 by Perspective.,3,4
660,233365067,"Such comments can be as harmful to participants as more overt toxicity (Nadal et al.,",11,12
661,233365067,Covert toxicity is an umbrella term which includes types of toxicity that may not be immediately obvious.,1,2
662,233365067,Covert toxicity is an umbrella term which includes types of toxicity that may not be immediately obvious.,10,11
663,233365067,"In this work, we seek to close the gap in toxicity models.",11,12
664,233365067,Our two main contributions include: • A custom crowd-sourcing instruction template to identify covert toxicity • A benchmark dataset for identifying covert toxicity. •,17,18
665,233365067,Our two main contributions include: • A custom crowd-sourcing instruction template to identify covert toxicity • A benchmark dataset for identifying covert toxicity. •,25,26
666,233365067,An enhanced toxicity model with improved performance in covert/veiled toxicity.,2,3
667,233365067,An enhanced toxicity model with improved performance in covert/veiled toxicity.,11,12
668,233365067,"2019) points out that the perceived toxicity of a comment is influenced by a variety of factors not limited to cultural background, rater bias, context and cognitive bias.",7,8
669,233365067,"2017) that have alternate meanings or veiled toxicity like codewords, adversarially generated or novel forms of offense (Jain et al.,",8,9
670,233365067,"To filter such datasets for veiled toxicity, Han and Tsvetkov (2020) proposed a procedure using probing examples.",6,7
671,233365067,"Our paper builds on previous work with a strategy for consistently rating covertly toxic content, a shared dataset for training, and a baseline covert toxicity model.",26,27
672,233365067,"From this sample, we defined broad categories of covert toxicity types.",10,11
673,233365067,"Microaggression Subtle discrimination towards an identity group • Obfuscation Hidden toxicity via intentional misspellings, coded words, or implied references • Emoticons/Emojis Toxic usage of non-text symbols • Sarcasm/Humor Offensive content in the context of a joke • Masked Harm Implied harm or threats masked by seemingly inoffensive language Crowdsourcing Data Collection We iterated on versions of an instruction templates to assist raters in identifying covertly toxic language with high precision.",10,11
674,233365067,"Note to improve rater comprehension we refer to covert and overt toxicity as implicit and explicit, respectively.",11,12
675,233365067,"As such, we reconfigured the template instructions (see Figure 1 ) to include a table outlining different types of toxicity with examples.",21,22
676,233365067,"Training Data The CivilComments dataset is a publicly available corpus of ∼1.8 million crowd rated comments labeled for toxicity (Borkan et al.,",18,19
677,233365067,"For the COVERTTOXICITY dataset, we applied the methodology of (Han and Tsvetkov, 2020) to the CivilCommentsIdentities set: comments with identity attack annotations and low Perspective API toxicity scores (Jigsaw, 2017) were marked as candidates for covert toxicity.",31,32
678,233365067,"For the COVERTTOXICITY dataset, we applied the methodology of (Han and Tsvetkov, 2020) to the CivilCommentsIdentities set: comments with identity attack annotations and low Perspective API toxicity scores (Jigsaw, 2017) were marked as candidates for covert toxicity.",44,45
679,233365067,The CivilCommentsIdentities toxicity label is the fraction of raters who voted for the label.,2,3
680,233365067,Han and Tsvetkov (2020) noted that comments with veiled toxicity were more likely to have dissent amongst crowd raters and empirically we observed the same.,11,12
681,233365067,"As such, we filtered the dataset using the toxicity label rater fraction, explicitly such that 0 < P (toxicity) ≤ 0.4.",9,10
682,233365067,"As such, we filtered the dataset using the toxicity label rater fraction, explicitly such that 0 < P (toxicity) ≤ 0.4.",21,22
683,233365067,"SBIC Microaggressions Dataset The Social Bias Inference Corpus (SBIC) consists of over 150k social media posts annotated for toxicity and implied offense over thousands of demographic groups (Sap et al.,",20,21
684,233365067,The test set is the subset of SBIC that scored P (toxicity) < 0.5.,12,13
685,233365067,This yielded an evaluation set with ∼3100 marked covertly offensive and ∼9000 marked not covert (and presumed not offensive given the low toxicity scores).,23,24
686,233365067,The original comments were filtered for low (explicit) toxicity scores and as such a sizeable portion were confirmed as not offensive.,10,11
687,233365067,The entire Civil-CommentsIdentities dataset was used for training toxicity with zero-weights assigned for missing covertly toxic labels.,10,11
688,233365067,A baseline model fine-tuned solely on the 'toxicity' label in CivilCommentsIdentities is marked as Toxic-BERT.,10,11
689,233365067,Toxic-Bert scored an average model probability for toxicity of 0.21 on comments with a majority of raters voting covertly toxic in the COVERTTOXIC-ITY test set.,9,10
690,233365067,"In contrast, on these same comments, Covert-BERT achieved a 0.44 average model probability of toxicity.",18,19
691,233365067,"The model showed substantially improvements in average model probability of toxicity for covert labels, ROC-AUC for covert toxicity, and recall as shown in Table 3 .",10,11
692,233365067,"The model showed substantially improvements in average model probability of toxicity for covert labels, ROC-AUC for covert toxicity, and recall as shown in Table 3 .",20,21
693,233365067,"The Covert-BERT model appears better suited for extracting covert-toxicity among microaggression specific data, as demonstrated with sample rated comments in Table 2 .",12,13
694,233365067,"Conclusion We iterate on rater feedback to create an initial baseline dataset, COVERTTOXICITY, that encapsulates a variety of often mislabeled online toxicity.",23,24
695,248863142,"This may again be due to the complexity of the data of the toxicity task, which contains longer sequences than the other two.",13,14
696,249097845,"Most of the existing approaches tend to give high toxicity scores to innocuous statements (e.g., ""I am a gay man"").",9,10
697,250390724,"We used only the English tweets; (iv)Kaggle (Kaggle, 2012) , a dataset that contains social media comments that are labelled as insulting or not; and (v) Jigsaw, a collection of Wikipedia Talk Pages comments which have been labelled by human raters for toxicity (Jigsaw, 2018) .",51,52
698,250390724,"Our results show that social-media-based word embeddings gave the best results for four out of five datasets: HateEval, Kaggle, Twitter-racism and Jigsaw-toxicity.",32,33
699,250390724,"Glove-Twitter also resulted in the highest average F1 score at 0.519, across all the categories on the Jigsaw-toxicity dataset which is significantly better for all the categories with p − value < 0.05.",22,23
700,250391102,"The labels aim in particular also at toxicity, criminal relevance and discrimination types of comments.",7,8
701,250391102,"Therefore, motivated by the concrete application to assist a fine granular classification of offensive comments in a reporting centre for hate comments 1 of the German state government, we present a new German dataset that aims among others at hate speech, toxicity, sentiment, target, but also at criminal relevance (regarding German law) and threat.",44,45
702,250391102,"Therefore, in a second step, we noted 57 comments from 49 conversations that were annotated as hate speech (majority voting) or toxic (averaged toxicity annotation > 2.5).",28,29
703,250391102,"It is noticeable that most of the comments have a toxicity of less than 2.5, although the sentiment of the majority of the comments is negative (-1 is the most negative).",10,11
704,250391102,"In our analysis, we define all comments as offensive that are labelled as hate speech (majority voting) or toxic (averaged toxicity annotation > 2.5).",24,25
705,250391102,"Baseline Models The categories hate speech, toxicity and sentiment were selected to train simple baseline models on.",7,8
706,250391102,"Even though toxicity and sentiment are regression tasks, we used classification models for them as this heavily improved the performance for the underrepresented classes (high toxicity and positive Sentiment).",2,3
707,250391102,"Even though toxicity and sentiment are regression tasks, we used classification models for them as this heavily improved the performance for the underrepresented classes (high toxicity and positive Sentiment).",27,28
708,250391102,"The comprehensive annotation schema is complex to annotate, and the definitions of hate speech and toxicity naturally leave a lot of room for personal interpretations.",16,17
709,53635478,"2017) 3 , and the SFU Opinion and Comment Corpus consisting of online opinion articles and their comments annotated for toxicity 4 .",21,22
710,248779945,"For example, toxic messages are divided into different types of toxicity (Fortuna et al.,",11,12
711,21698865,We tested the viability of partnering with local developers to create custom annotation applications and to recruit and motivate crowd contributors from their communities to perform an annotation task consisting of the assignment of toxicity ratings to Wikipedia comments.,34,35
712,21698865,"Specifically, we chose to collect judgments on the toxicity of Wikipedia discussion comments, where a ""toxic"" comment is defined as any kind of hateful, aggressive, or disrespectful comment that is likely to make someone leave a discussion.",9,10
713,21698865,The application we developed for the toxicity rating task is hosted on a Google platform.,6,7
714,21698865,Outside volunteers from local universities spent an hour of their time rating the toxicity of comments in exchange for a Google office tour and a token of appreciation (a canvas bag).,13,14
715,21698865,"We provided one day of boot camp to brief the participants on the toxicity rating task, technical implementation details, and crowdsourcing program design.",13,14
716,21698865,"As in the previous rounds, the main task was still to rate the toxicity of Wikipedia comments, but developers were given the option to extend the task to include classification of the toxicity type: insult, identity hate, obscenity, or threat.",14,15
717,21698865,"As in the previous rounds, the main task was still to rate the toxicity of Wikipedia comments, but developers were given the option to extend the task to include classification of the toxicity type: insult, identity hate, obscenity, or threat.",34,35
718,21698865,Mushrooms representing Wikipedia comments are to be rated for toxicity.,9,10
719,21698865,"In the Jury application, users rate the toxicity of comments to earn coins. (",8,9
720,21698865,"In ""simple"" mode, users rate toxicity on a three-point scale; in ""advanced"" mode, users also classify the type of toxicity.",8,9
721,21698865,"In ""simple"" mode, users rate toxicity on a three-point scale; in ""advanced"" mode, users also classify the type of toxicity.",28,29
722,232092676,Explanations are particularly important for tasks like offensive language or toxicity detection on social media because a manual appeal process is often in place to dispute automatically flagged content.,10,11
723,232092676,We incorporate this assumption into transformer models by scoring a post based on the maximum toxicity of its spans and augmenting the training process to identify correct spans.,15,16
724,232092676,"2020) achieved high performance on toxicity detection (Zampieri et al.,",6,7
725,232092676,Knowing how one decision is made is important in toxicity detection.,9,10
726,232092676,2020) for the task of toxicity detection in social media posts.,6,7
727,232092676,"In other words, the toxicity of a piece of text should be associated with the most toxic span identified in the text.",5,6
728,232092676,"To this end, we propose using neural multi-task model that is trained on (1) toxicity detection over (a) When the input sequence is toxic, the toxicity of the most toxic span is picked to represent the toxicity of the sentence. (",19,20
729,232092676,"To this end, we propose using neural multi-task model that is trained on (1) toxicity detection over (a) When the input sequence is toxic, the toxicity of the most toxic span is picked to represent the toxicity of the sentence. (",33,34
730,232092676,"To this end, we propose using neural multi-task model that is trained on (1) toxicity detection over (a) When the input sequence is toxic, the toxicity of the most toxic span is picked to represent the toxicity of the sentence. (",44,45
731,232092676,"Rather than the typical transformer classification approach, our model predicts the toxicity of each individual term in the text and aggregates them via max pooling to predict the toxicity of the entire text (see Fig.",12,13
732,232092676,"Rather than the typical transformer classification approach, our model predicts the toxicity of each individual term in the text and aggregates them via max pooling to predict the toxicity of the entire text (see Fig.",29,30
733,232092676,"More importantly, however, the structure of our model inherently generates explanations of the decision by selecting the terms with the highest toxicity scores.",23,24
734,232092676,An error analysis has shown multiple insights into utilizing contextualized models in toxicity detection and lead future directions.,12,13
735,232092676,"Related Work Due to the ubiquity of online conversations, the need for automatic online toxicity detection has become crucial to promote healthy online discussions.",15,16
736,232092676,"For example, some slurs that frequently appear in African American English are usually picked as strong evidence of toxicity (Xia et al.,",19,20
737,232092676,"Later on, neural textual representations have shown effectiveness in toxicity detection.",10,11
738,232092676,"2019) have shown great advantages in toxicity detection (Zampieri et al.,",7,8
739,232092676,"On toxicity detection, while most of the works were done focusing on improving the model performances, less attention was paid to interpretability.",1,2
740,232092676,Model We propose a neural multi-task model that can predict the toxicity and explain its prediction at the same time by providing a set of words that can justify its prediction.,13,14
741,232092676,"This assumption suggests that if there is a word or phrase in a piece of text that is toxic, i.e., with a level of toxicity that is over a certain threshold, the toxicity level of the entire text is certainly over such threshold, and, therefore, should be considered toxic.",26,27
742,232092676,"This assumption suggests that if there is a word or phrase in a piece of text that is toxic, i.e., with a level of toxicity that is over a certain threshold, the toxicity level of the entire text is certainly over such threshold, and, therefore, should be considered toxic.",35,36
743,232092676,"Given the input x, we can define y as the toxicity label for the sequence and y as the toxicity labels for individual token.",11,12
744,232092676,"Given the input x, we can define y as the toxicity label for the sequence and y as the toxicity labels for individual token.",20,21
745,232092676,"Let s = {s 1 , s 2 , ..., s n } be a model's prediction of toxicity for each token.",21,22
746,232092676,"By our assumption, we apply a max pooling operation over s: s = max(s) (1) where s represents the predicted toxicity of the entire sequence.",25,26
747,232092676,"In some cases, toxicity can be expressed in subtle or implicit ways, such as through sarcasm or metaphor (MacAvaney et al.,",4,5
748,232092676,"Architecture & Methodology To detect the toxicity and learn the toxic spans at the same time, we propose to use a neural multitask learning framework (Caruana, 1997) .",6,7
749,232092676,"Here h i ∈ R d denotes the i- s = H • W (2) For the classification task, we use s as the predicted toxicity where s is calculated following the procedure mentioned in Eq. (",28,29
750,232092676,"For the span detection task, we directly leverage the output toxicity sequence s. This setup ensures that the model learns to predict the text as toxic if a span is toxic.",11,12
751,232092676,"For the purposes of training, let D 1 be the dataset for toxicity detection task, and D 2 be the dataset for toxic spans detection task.",13,14
752,232092676,"We construct the loss of the model L to be the following: L = λ (x,y)∈D 1 L C (x, y) Loss for toxicity detection + (1 − λ) (x,y)∈D 2 L S (x, y) Loss for toxic spans detection (3) where L C is the loss for the toxicity detection task and L S is the loss for the toxic spans detection task.",30,31
753,232092676,"We construct the loss of the model L to be the following: L = λ (x,y)∈D 1 L C (x, y) Loss for toxicity detection + (1 − λ) (x,y)∈D 2 L S (x, y) Loss for toxic spans detection (3) where L C is the loss for the toxicity detection task and L S is the loss for the toxic spans detection task.",66,67
754,232092676,Here the toxicity detection task is a sequence classification task and the toxic spans detection task is a token classification task.,2,3
755,232092676,"2019 ) is a largescale dataset with crowd-sourced post-level annotations for toxicity, provided by the Civil Comment platform.",15,16
756,232092676,A toxicity score between zero and one of a post is the fraction of raters considering it to be toxic.,1,2
757,232092676,"We first sampled 7,000 highly toxic posts (toxicity score greater than 0.8) and 7,000 non-toxic posts (toxicity score less than 0.1) from CCD.",8,9
758,232092676,"We first sampled 7,000 highly toxic posts (toxicity score greater than 0.8) and 7,000 non-toxic posts (toxicity score less than 0.1) from CCD.",21,22
759,232092676,"We further sampled another 8,000 ambiguous posts that have toxicity scores between 0.1 and 0.3 and contain terms that frequently appear in the toxic posts.",9,10
760,232092676,"LR is shown to be effective in toxicity detection (Djuric et al.,",7,8
761,232092676,"Beyond the toxic spans detection task, we also evaluate the toxicity detection performance for these models leveraging our proposed assumption, even though these models are not trained for toxicity detection.",11,12
762,232092676,"Beyond the toxic spans detection task, we also evaluate the toxicity detection performance for these models leveraging our proposed assumption, even though these models are not trained for toxicity detection.",30,31
763,232092676,The in-domain toxicity detection performance is shown in Table 3 .,4,5
764,232092676,This suggests that the span information is capable of predicting the toxicity of the entire content and combining it with the post-level supervised information further improves the effectiveness.,11,12
765,232092676,"2019a) 0.83 BERT-MT (ours, transfer) 0.77 ELECTRA-MT (ours, transfer) 0.77 adjectives, such as disgusting, lazy, incompetent, as strong signals for toxicity.",35,36
766,232092676,We also suspect that the model is over-leveraging expression patterns or co-occurrences for toxicity classification.,17,18
767,232092676,"Here, the model misclassifies it as toxic and picks the word brown as the most toxic span with a toxicity score of 0.527.",20,21
768,232092676,We first rule out the possibility that the toxicity comes from the negation by experimentally removing the word never from the sentence.,8,9
769,232092676,We find that the toxicity score of the sentence increases to 0.762 with the word brown still as the most toxic span.,4,5
770,232092676,"We then examine by replacing the word suspect and keep the rest; when we replace suspect with girl or mom, the toxicity score for brown decreases to 0.334 and 0.141 respectively; when changing to prisoner or scammer, the toxicity score for word brown becomes 0.717 and 0.972.",23,24
771,232092676,"We then examine by replacing the word suspect and keep the rest; when we replace suspect with girl or mom, the toxicity score for brown decreases to 0.334 and 0.141 respectively; when changing to prisoner or scammer, the toxicity score for word brown becomes 0.717 and 0.972.",42,43
772,232092676,"For the third example, the toxicity lies in the quoted text, which is also not the intention of the speaker.",6,7
773,232092676,"Model SD-P SD-R SD-F1 LR 0.111 0.195 0.120 BERT-SP 0.836 0.798 0.792 ELECTRA-SP 0.840 0.807 0.798 BERT-MT 0.837 0.785 0.784 ELECTRA-MT 0.842 0.788 0.789 These words/phrases such as degenerate, and lacks the mental capacity to, which could also be utilized in the neutral descriptions, are used here for expressing toxicity.",68,69
774,232092676,It is easy for a human to reason out the target mentioned in the sentence and thus be aware of the toxicity raised; this is usually not the case for machine classifiers.,21,22
775,232092676,"Due to the nature of the toxicity span detection task where instances span from single tokens to multiple sentences, the ad-hoc evaluation metrics give partial credits to imperfect matches at the character level.",6,7
776,232092676,"However, our multi-task models provide nearly equal span detection effectiveness with far better performance on toxicity detection (See Table 3 ).",18,19
777,232092676,User Study The labeled toxic spans used for the evaluation in Section 6.1 are not annotated to be the interpretation for the toxicity of the post.,22,23
778,232092676,"Annotators are asked to annotate for the toxicity (e.g., whether the sample is toxic) of the samples and pick the model with a better explanation.",7,8
779,232092676,Such indirect toxicity is carried either by negation (Example C in Table 7 ) or adversarially-modified toxic words (Example D in Table 7 ).,2,3
780,232092676,"Conclusion & Future Work In this paper, we proposed a toxicity detection approach that builds in the interpretability by predicting the toxicity of a piece of text based on the toxicity level of its spans.",11,12
781,232092676,"Conclusion & Future Work In this paper, we proposed a toxicity detection approach that builds in the interpretability by predicting the toxicity of a piece of text based on the toxicity level of its spans.",22,23
782,232092676,"Conclusion & Future Work In this paper, we proposed a toxicity detection approach that builds in the interpretability by predicting the toxicity of a piece of text based on the toxicity level of its spans.",31,32
783,232092676,We plan to take the implicit toxicity into consideration to make our assumption more robust.,6,7
784,232092676,"Besides, we will dig more into toxicity detection with long sequences.",7,8
785,247626152,2020) there exists a relation between annotators' identities and toxicity/bias in dataset.,11,12
786,184483263,"Therefore the task is a challenging one, but one with real world impact: if measures can be taken to identify and curtail trolling, the toxicity of the internet can to some extent be reduced.",27,28
787,249889128,"9 Given an input text, Perspective provides percentage scores for attributes such as ""toxicity"" and ""identity attack"".",15,16
788,249889128,"The ""toxicity"" attribute covers a wide range of languages, including the ten in MHC.",2,3
789,249889128,"However, compared to hate speech, ""toxicity"" is a much broader concept, which includes other forms of abuse and profanity -some of which would be considered contrastive non-hate in the context of MHC.",8,9
790,52158272,"2017) created three different datasets of comments collected from the English Wikipedia Talk page: one was annotated for personal attacks, another for toxicity, and the third for aggression.",25,26
791,52158272,"Working with the personal attack and toxicity datasets, Pavlopoulos et al. (",6,7
792,52158272,"From this corpus, they randomly sampled comments to form three datasets on personal attack, toxicity and aggression, and engaged workers from CrowdFlower to annotate them.",16,17
793,52158272,"In this work, we utilize the toxicity and personal attack datasets, henceforth referred to as W-TOX and W-ATT respectively.",7,8
794,52012032,"2017) prepared three different datasets of comments collected from the English Wikipedia Talk page; one was annotated for personal attacks, another for toxicity and the third one for aggression.",25,26
795,52012032,"Experimenting with the personal attack and toxicity datasets, Pavlopoulos et al. (",6,7
796,236460063,"Evidence of such bias has been found in toxicity detection (Zhang et al.,",8,9
797,235390674,"2017) ) created three different datasets from Wikipedia Talk pages, focusing on aggression, personal attacks and toxicity.",19,20
798,201669180,"For instance, the hateful tweets presented in Figure 1 show toxicity directed towards different targets, with or without using slurs, and generating several types of reactions.",11,12
799,201669180,2018) report the presence of Hindi tokens in English data and use multilingual word embeddings to deal with this issue when detecting toxicity.,23,24
800,201669180,"Then, we observed that discussions about controversial topics, such as feminism in general, illegal immigrants in English, Islamo-gauchisme (""Islamic leftism"") in French, or Iran in Arabic were more likely to provoke disputes, comments filled with toxicity and thus, notable insult patterns that we looked for in subsequent search rounds.",47,48
801,218974272,"Offensive language is a phenomenon closely interconnected with a number of other linguistic and societal phenomena, including: abusive and aggressive language, cyberbullying, racism, extremism, radicalization, toxicity, profanity, flaming, discrimination, hate and hate speech.",32,33
802,233181955,2021) where systems are asked to extract the list of toxic spans that attribute to a text's toxicity.,19,20
803,1583396,"However, these appear to be peripheral to curcumin's effect on thyroid neoplasms focusing more on aspects such as hypothyroidism and toxicity.",22,23
804,237579508,We use multilingual models and pre-train them on a multilingual dataset created out of 12 datasets for nine different languages on toxicity and hatespeech detection.,23,24
805,237579508,"Furthermore, we perform data augmentation by labelling unlabeled data, retrieved from Facebook, using one of the multilingual models pre-trained and fine-tuned on the toxicity classification task.",30,31
806,237579508,We adapt task-specific pre-training to toxicity classification by taking 12 toxicity or hatespeech classification datasets and training language models on these datasets before fine-tuning them on the dataset of the shared task (Table 1 ).,9,10
807,237579508,We adapt task-specific pre-training to toxicity classification by taking 12 toxicity or hatespeech classification datasets and training language models on these datasets before fine-tuning them on the dataset of the shared task (Table 1 ).,14,15
808,237579508,We use our best performing model and fine-tune on the toxicity classification task of the shared task to label unlabelled Facebook comments we collected from German political talk shows.,12,13
809,237579508,Figure 1 shows examples of comments with their toxicity probabilities.,8,9
810,237579508,3 Experiments Task and dataset The first subtask of GermEval 2021 is the classification of Facebook comments from German political talk shows with regard to their toxicity.,26,27
811,248524758,"Coreference: Anaphoric or coreferential reasoning is sometimes needed to understand the efficacy of the combination e.g. ""it was demonstrated that they could be combined with acceptable toxicity."".",28,29
812,239016893,"Recent works find that the toxicity may change with context (Pavlopoulos et al.,",5,6
813,239016893,Contextsensitive Non-personal Unsafety Toxicity Agreement (TA) The bots advocate or spread the toxicity of the context to show agreement or acknowledgment.,16,17
814,239016893,"Such responses advocate users' harmful speech, spread toxicity, rude or bias in an indirect form (Dinan et al.,",9,10
815,239016893,"Deceiving Existing Detectors PerspectiveAPI (P-API, perspectiveapi.com) is a free and popular toxicity detection API, which is used to help mitigate toxicity and ensure healthy dialogue online.",16,17
816,239016893,"Deceiving Existing Detectors PerspectiveAPI (P-API, perspectiveapi.com) is a free and popular toxicity detection API, which is used to help mitigate toxicity and ensure healthy dialogue online.",26,27
817,239016893,"A.2 Machine-generated Data Prompts for generation have two major sources, (1) crawled using keyword query from Reddit, for Biased Opinion dataset (2) collected from existing toxicity datasets, including the ICWSM 2019 Challenge (Mathew et al.,",33,34
818,239016893,"We use Detoxify (Hanu and Unitary team, 2020) to filter out replies with toxicity score over 0.3.",16,17
819,236460108,We use PTLMs to predict masked tokens at the end of a sentence in order to examine how likely they enable toxicity towards specific communities.,21,22
820,236460108,"We shed the light on how such negative content can be triggered within unrelated and benign contexts based on evidence from a large-scale study, then we explain how to take advantage of our methodology to assess and mitigate the toxicity transmitted by PTLMs.",42,43
821,236460108,"In order to gauge the generated toxicity by different language models, we train simple toxicity classifiers based on logistic regression using available hate speech and offensive language datasets.",6,7
822,236460108,"In order to gauge the generated toxicity by different language models, we train simple toxicity classifiers based on logistic regression using available hate speech and offensive language datasets.",15,16
823,236460108,"The human annotations confirm the existence of toxicity in English, French, and Arabic PTLMS and show that, despite their imperfections, the classifiers can be used as toxicity pointers.",7,8
824,236460108,"The human annotations confirm the existence of toxicity in English, French, and Arabic PTLMS and show that, despite their imperfections, the classifiers can be used as toxicity pointers.",30,31
825,236460108,"In Section 3, we present our probing experiments using classifiers and show frequent words that are generated by different PTLMs in order to demonstrate the spread of the existing toxicity across different languages, both quantitatively and qualitatively.",30,31
826,236460108,Methodology We adopt a rule-based methodology based on Masked Language Modeling (MLM) in order to probe the toxicity of the content generated by different PTLMs.,21,22
827,236460108,Our goal is to provide a set of tests and a process to assess toxicity in PTLMs with regard to various social groups.,14,15
828,236460108,"We use the three classifiers in order to assess different PTLMs, compare the extent to which toxicity  can be generated despite the benign commonsense actions and simple patterns we make use of.",17,18
829,236460108,"In this case, we consider the toxicity of a sentence given the newly PTLM-introduced con- PTLM %@1 %@5 %@10 BERT 14.20% 14.29% 14.33% RoBERTa 5.95% 5.37% 5.42% GPT-2 3.19% 5.80% 5.45% CamemBERT 23.38% 20.30% 17.69% AraBERT 3.34% 6.59% 5.82% tent.",7,8
830,236460108,We study the social groups to which PTLMs associate potential toxicity in Table 6 .,10,11
831,236460108,"For instance, the statistics show that refugees and disabled people are often linked to toxic statements in BERT, people with Down Syndrome and African people commonly associated with toxicity in French, while we observe a difference in the scale due to AraBERT often predicting stopwords and Arabic pronouns.",30,31
832,236460108,This may later help us define a trust value or how each part of the sentence contributes to the toxicity score and make this process explainable.,19,20
833,236460108,"Similarly, we present a set of steps in order to probe for toxicity in large PTLMs.",13,14
834,236460108,"Furthermore, our methodology and predictions can help us define toxicity anchors that can be utilized to improve toxic language classification.",10,11
835,2630364,"The difference between collapse or a gentle rebound is determined by two factors: the toxicity of the introduced element, and the resiliency of the original ecosystem.",15,16
836,2630364,"Within the next 3 to 5 years, we will see more and more developments towards the MT Bazaar vision because • Cloud computing further matures • Sensitivity for crowdsourcing and language data sharing increases • New markets and market opportunities emerge • MT developers join in and MT systems evolve to the next generation due to either the disruptive effect of evolution, or the toxicity of the introduced new elements and the resiliency of the original ecosystem This future will bring us more distributed, crowdsourced markets in which social negotiation and collaboration between humans and machines are stigmergically mediated by computational intelligence and Internet-based technologies Our recommendation for future MT Bazaar users is: • Actively collaborate and bring in your specific expertise and knowledge to the emerging communities and marketplaces • Be open minded towards crowdsourcing and its evolving power.",66,67
837,15846774,2) We investigated the efficacy and toxicity of a 3-hour paclitaxel infusion in a phase II trial in patients with inoperable stage IIIB or IV NSCLC. . . .,7,8
838,15846774,7) CCNU (lomustine) toxicity in dogs: a retrospective study (2002-07) . . .,6,7
839,15846774,8) Effect of calcium chloride and 4aminopyridine therapy on desipramine toxicity in rats . . .,10,11
840,233296194,The real-world impact of polarization and toxicity in the online sphere marked the end of 2020 and the beginning of this year in a negative way.,8,9
841,233296194,"Semeval-2021, Task 5 -Toxic Spans Detection is based on a novel annotation of a subset of the Jigsaw Unintended Bias dataset and is the first language toxicity detection task dedicated to identifying the toxicity-level spans.",27,28
842,233296194,"Semeval-2021, Task 5 -Toxic Spans Detection is based on a novel annotation of a subset of the Jigsaw Unintended Bias dataset and is the first language toxicity detection task dedicated to identifying the toxicity-level spans.",34,35
843,233296194,"Unfortunately, online toxicity is present in a large part of the social and news media platforms.",3,4
844,233296194,"2021) , tackles the problem of identifying the exact portion of the document that gives it toxicity.",17,18
845,233296194,"The provided dataset is a subset of the Jigsaw Unintended Bias in Toxicity Classification dataset 1 , with annotated spans that represent toxicity from a document.",22,23
846,233296194,"There are several research efforts to detect toxic texts based on the Jigsaw Unintended Bias dataset, out of which most focus on the Kaggle competition task -predicting the toxicity score for a document.",29,30
847,233296194,"By cross-referencing with the original Jigsaw dataset which contains additional information, we retrieved the toxicity scores for each text and determined that the mean toxicity score for the train and test set were very close (0.8429 versus 0.8440; see Figure 1 for corresponding kernel density estimates).",17,18
848,233296194,"By cross-referencing with the original Jigsaw dataset which contains additional information, we retrieved the toxicity scores for each text and determined that the mean toxicity score for the train and test set were very close (0.8429 versus 0.8440; see Figure 1 for corresponding kernel density estimates).",27,28
849,233296194,"Moreover, only 17 out of 2,000 test data rows had a toxicity score below 0.75.",12,13
850,233296194,"Since the input words can consist of more than one token, we assign the toxicity label to a word if at least one component token is inferred as toxic.",15,16
851,233296194,"All models have the tendency to over-predict toxicity by adding words to the toxic expressionfor example, ""What a pile of shit"" was automatically labeled as ""What a pile of shit"".",9,10
852,233296194,"As we mentioned in section 3.1, even though almost all documents in the test set had a high toxicity score in the original Jigsaw dataset, many had no annotated toxic spans.",19,20
853,224705908,"2017) A corpus of discussion comments from English Wikipedia talk pages that were annotated for attack; personal, general aggression, or toxicity.",24,25
854,9075755,"In contrast, even 10 μM roxarsone showed no decrease in the angiogenic index (Figures 1 and 2 ) or signs of toxicity in the endothelial cells (data not shown).",23,24
855,233231544,One potential way to reduce the toxicity of output is to apply profanity filter as a post-processing step before final output is returned.,6,7
856,239009731,The toxicity dataset contains comments collected from the same source.,1,2
857,239009731,The toxicity dataset includes the same demographic information of the annotators as the former two datasets.,1,2
858,248779991,"2021) collected a large benchmark for naturally occuring drifts in CV and NLP, e.g., user demographics for toxicity detection in online comments.",20,21
859,227230383,"Specifically, we include a pre-trained toxicity scorer that scores a given text on six dimensions of toxic content.",8,9
860,227230383,"Therefore, we pre-trained a toxicity scorer that quantifies the hatefulness of the synthesized tweets as hate scores.",7,8
861,227230383,Pre-Training Toxicity Scorer The toxicity scorer plays a vital role in the HateGAN model as it outputs a hate score reward to guide the generator to produce more hateful tweets.,6,7
862,227230383,"Specifically, the toxicity scorer is pre-trained as a multi-label classification model, as shown in Figure 2 .",3,4
863,227230383,"We pre-trained the classification model using the Although the toxicity scorer is able to score a given text on the six polarities (i.e., labels in the classification model), not all the polarities are relevant in guiding the generation of hateful tweets.",11,12
864,227230383,2018 ) exploited the toxicity and attack on commenter models provided by the Perspective API 3 to evaluate and annotate the hatefulness in text.,4,5
865,227230383,We adopt a similar approach where we consider a given text's toxicity and identity attack polarities scores learned by our toxicity scorer.,12,13
866,227230383,We adopt a similar approach where we consider a given text's toxicity and identity attack polarities scores learned by our toxicity scorer.,21,22
867,227230383,"The realistic and hate score rewards in our model are realized by incorporating a discriminator and toxicity scorer, respectively.",16,17
868,227230383,"As the toxicity scorer is pre-trained, its parameters are fixed during the training of HateGAN model.",2,3
869,227230383,"As highlighted in earlier section, we only consider the synthesized text's toxicity and identity attack polarities scores for computing the hate score reward.",13,14
870,235417027,Traditional toxicity detection models have focused on the single utterance level without deeper understanding of context.,1,2
871,235417027,"We propose a robust dual semantic-level toxicity framework, which handles utterance and token-level patterns, and rich contextual chatting history.",8,9
872,235417027,"Accompanying the dataset is a thorough in-game toxicity analysis, which provides comprehensive understanding of context at utterance, token, and dual levels.",9,10
873,235417027,"Inspired by NLU, we also apply its metrics to the toxicity detection tasks for assessing toxicity and game-specific aspects.",11,12
874,235417027,"Inspired by NLU, we also apply its metrics to the toxicity detection tasks for assessing toxicity and game-specific aspects.",16,17
875,235417027,"Furthermore, we examine the coverage of toxicity nature in our dataset by comparing it with other toxicity datasets.",7,8
876,235417027,"Furthermore, we examine the coverage of toxicity nature in our dataset by comparing it with other toxicity datasets.",17,18
877,235417027,"In the past few years, Natural Language Processing (NLP) researchers have proposed several online game/community toxicity analysis frameworks Figure 1 : An example intent/slot annotation from the CONDA (CONtextual Dual-Annotated) dataset. (",20,21
878,235417027,"2019) Inspired by this NLU research progress, we propose CONDA, an in-game toxicity detection dataset, with a robust dual-level annotation which enables intent detection and slot filling.",17,18
879,235417027,"Our dataset consists of 45k utterances from the chat logs of 1.9k Dota 2 matches, labeled with 4 intent classes and 6 slot classes to address toxicity and the gamespecific vocabulary.",27,28
880,235417027,The large portion of gamespecific classes in the dual levels enables the dataset to be more sophisticated in detecting toxicity in games.,19,20
881,235417027,The combination of each intent with each slot class shows that dual annotation can help determine toxicity from gamer slang when used in both toxic and non-toxic situations.,16,17
882,235417027,We provide five strong baseline NLU models and compare the toxicity detection performance over our dataset.,10,11
883,235417027,"For evaluation, we apply four NLU metrics to assess performance in toxicity and game specific aspects.",12,13
884,235417027,"Furthermore, we perform a transfer learning experiment with existing toxicity datasets.",10,11
885,235417027,"We find that the nature of toxicity in our dataset can generalize to other proposed taxonomies, including hatefulness, sexism and racism.",6,7
886,235417027,"Beyond this commonality, our experiment illustrates that CONDA is distinguished from other toxicity datasets due to game-specific characteristics.",13,14
887,235417027,"This paper then makes the following contributions: • To the best of our knowledge, this is the first attempt to build a toxicity detection dataset with joint Natural Language Understanding aspects of intent classification and slot filling; • We propose a robust dual semantic-level toxicity framework, which handles utterance and token-level patterns with rich in-game chatting history; • We formalise NLU metrics for toxicity detection, evaluate strong NLU models on our dataset, and further conduct transfer learning experiments with other toxicity datasets.",24,25
888,235417027,"This paper then makes the following contributions: • To the best of our knowledge, this is the first attempt to build a toxicity detection dataset with joint Natural Language Understanding aspects of intent classification and slot filling; • We propose a robust dual semantic-level toxicity framework, which handles utterance and token-level patterns with rich in-game chatting history; • We formalise NLU metrics for toxicity detection, evaluate strong NLU models on our dataset, and further conduct transfer learning experiments with other toxicity datasets.",49,50
889,235417027,"This paper then makes the following contributions: • To the best of our knowledge, this is the first attempt to build a toxicity detection dataset with joint Natural Language Understanding aspects of intent classification and slot filling; • We propose a robust dual semantic-level toxicity framework, which handles utterance and token-level patterns with rich in-game chatting history; • We formalise NLU metrics for toxicity detection, evaluate strong NLU models on our dataset, and further conduct transfer learning experiments with other toxicity datasets.",74,75
890,235417027,"This paper then makes the following contributions: • To the best of our knowledge, this is the first attempt to build a toxicity detection dataset with joint Natural Language Understanding aspects of intent classification and slot filling; • We propose a robust dual semantic-level toxicity framework, which handles utterance and token-level patterns with rich in-game chatting history; • We formalise NLU metrics for toxicity detection, evaluate strong NLU models on our dataset, and further conduct transfer learning experiments with other toxicity datasets.",93,94
891,235417027,These annotation methods are not robust enough to handle unlabelled toxicity words or unreported toxic players.,10,11
892,235417027,"Toxicity Datasets in Online Community An extensive body of work has focused on datasets to detect toxicity including hate speech (Waseem and Hovy, 2016; Davidson et al.,",16,17
893,235417027,"However, the majority of toxicity datasets do not consider the context of a conversation, instead simply analysing a single utterance.",5,6
894,235417027,"Existing toxicity datasets mainly focus on annotating at utterance-level, whereas ours conducts a dual-level annotation at utterance and token-level, while also providing a conversation history (see Table 1 ).",1,2
895,235417027,The approaches used in multi-turn dialogue analysis have not yet been observed in toxicity datasets.,15,16
896,235417027,"Annotation Dual Aspects Inspired by NLU, we provide a dual-level annotation approach to detect toxicity, which often relies on context.",17,18
897,235417027,"However, considering the previous utterance of ""worst hookshot ever"", we can identify hidden or implicit toxicity.",19,20
898,235417027,"To construct the T lexicon, we combine several toxicity lexicons (see Section 8 Ethics) and remove overlaps.",9,10
899,235417027,"In comparison to other toxicity datasets, our lexicon-based slot labelling enables deeper understanding of game context.",4,5
900,235417027,"The annotators manually classified the utterances into four labels: E (Explicit toxicity), I (Implicit toxicity), A (Action) and O (Other).",13,14
901,235417027,"The annotators manually classified the utterances into four labels: E (Explicit toxicity), I (Implicit toxicity), A (Action) and O (Other).",19,20
902,235417027,The guidelines for human annotators were as follows: Explicit toxicity: Typically contains toxic word(s).,10,11
903,235417027,"May include one or more of the following aspects: • Strong toxicity -blatant insulting or disrespecting others is obviously seen in the text, normally with severely toxic wording; • Normal toxicity -impolite, rudely worded and unreasonable comment that insults or humiliates others; • Cursing others with the intent to insult or humiliate them (e.g. 'noob' 3 ); • Sexual wording or talk about sex-related behavior; • Use of negative or hateful words to describe others (e.g. 'useless'); • Racist language that is targeted at insulting others (e.g. 'Peruvians', 'fucking russians'); • Inflammatory language, insulting others and trying to start a conversational fight.",12,13
904,235417027,"May include one or more of the following aspects: • Strong toxicity -blatant insulting or disrespecting others is obviously seen in the text, normally with severely toxic wording; • Normal toxicity -impolite, rudely worded and unreasonable comment that insults or humiliates others; • Cursing others with the intent to insult or humiliate them (e.g. 'noob' 3 ); • Sexual wording or talk about sex-related behavior; • Use of negative or hateful words to describe others (e.g. 'useless'); • Racist language that is targeted at insulting others (e.g. 'Peruvians', 'fucking russians'); • Inflammatory language, insulting others and trying to start a conversational fight.",33,34
905,235417027,Implicit toxicity: Hidden toxicity that normally cannot be seen from the text itself.,1,2
906,235417027,Implicit toxicity: Hidden toxicity that normally cannot be seen from the text itself.,4,5
907,235417027,"Overall, the large portion of these game-specific classes enables the dataset to be more sophisticated in detecting toxicity in games.",20,21
908,235417027,"Dual Annotation Distribution To understand the effect of dual annotation on the toxicity context, we look at the distribution of the slot labels within each intent class.",12,13
909,235417027,"As a result, we focus more on T and S slot classes joined with other intent classes in order to investigate toxicity natures in games carried out from dual annotation.",22,23
910,235417027,This indicates dual annotation captures toxicity from the slang largely used in games.,5,6
911,235417027,"In addition, ""gg"" appears in all combinations because it may have some toxicity attached via sarcasm.",15,16
912,235417027,"Comparison with Other Datasets In Figure 4 , we compare our dataset with other toxicity detection datasets using the metric of relative frequency of toxic utterances of each length.",14,15
913,235417027,"Baseline Experiment To explore the toxicity detection from an NLU perspective, we selected five baseline NLU models and compared their detection performance over our proposed dataset.",5,6
914,235417027,The variance in U-F1(I) implies potential improvement in implicit toxicity detection.,12,13
915,235417027,These explicit lines of influence from one task to the other have shown to be successful in NLU and could be explored further in the toxicity detection task.,25,26
916,235417027,Transfer Experiment We compared our dataset with the toxicity detection datasets introduced in Section 4 in terms of transfer performance over utterance-level binary prediction as toxic or non-toxic.,8,9
917,235417027,"This implies that CONDA covers the nature of toxicity that can be generalized from the other toxicity datasets which emphasize hatefulness, sexism and racism.",8,9
918,235417027,"This implies that CONDA covers the nature of toxicity that can be generalized from the other toxicity datasets which emphasize hatefulness, sexism and racism.",16,17
919,235417027,"StormfrontWS performs well when trained on CONDA due to shared toxicity characteristics, but the performance of Waseem and FoxNews when tested on a model trained on CONDA is relatively low.",10,11
920,235417027,"Conclusion and Future Work In this paper, we propose CONDA, a new dataset with dual-level (token and utterance) annotation for understanding in-game chat and to detect toxicity.",34,35
921,235417027,"Compared to previous studies, we draw on the NLU perspective and use the joint token-utterance aspect for detection of toxicity.",22,23
922,235417027,"Through experiments with joint slot and intent NLU models, we show the promising potential of such models for toxicity detection utilizing the dual-level annotation.",19,20
923,235417027,We also compare our dataset with other benchmark toxicity datasets in the literature through a transfer experiment.,8,9
924,235417027,"For our automated slot labelling, we generated the game toxicity lexicon by taking the supplemental materials released by Märtens et al. (",10,11
925,235417027,The CONDA dataset is intended for toxicity detection in online games by providing both slot and intent labels.,6,7
926,202895301,"The typical way to approach this problem is via supervised machine learning, where an input to a model is a user-generated text, and the output is a classification decision (toxic or non-toxic) or a numerical toxicity score.",43,44
927,202895301,"For post-hoc models, while useful, this additional information may not be crucial, as the main indicators of toxicity are most present in the text of the comment being classified rather than in the rest of the thread.",22,23
928,202895301,"First, using a large data set of conversations among Wikipedia contributors, we compile and make publicly available a new dataset with complete discussion threads and with semi-automatically generated toxicity labels.",32,33
929,202895301,"Related Work Many varieties of toxic language have been considered in NLP research, including sexism, racism (Waseem and Hovy, 2016a; Waseem, 2016) , toxicity (Kolhatkar et al.,",30,31
930,202895301,The authors create a manually labeled data set and perform an extensive study on which pragmatic and rhetorical devices are indicative of conversation toxicity.,23,24
931,202895301,Automatically derived toxicity scores are also provided for each example.,2,3
932,202895301,Semi-automatically label the examples for toxicity.,7,8
933,202895301,"An example is considered toxic if its toxicity or severe toxicity scores are above 0.64 or 0.92, respectively.",7,8
934,202895301,"An example is considered toxic if its toxicity or severe toxicity scores are above 0.64 or 0.92, respectively.",10,11
935,202895301,Manual inspection of the silver-labeled dataset reveals that the combination of the toxicity classifier and observed deletion is effective in identifying some of the toxic comments.,14,15
936,202895301,"However, this approach fails to identify those toxic comments which were not deleted or those for which the toxicity classifier failed.",19,20
937,202895301,Manual inspection of the data set confirms that humans could determine the toxicity of most comments without referring to the thread for additional context.,12,13
938,202895301,A presence of a toxic comment in a thread is a good indicator of a situation where more toxicity will follow.,18,19
939,202895301,"This, however, is not true preemptive detection, as toxicity already occurred earlier in thread.",11,12
940,237581351,"Related tasks More attention has been given to related tasks, most prominently the detection of offensive language, hate speech, and toxicity (Pelicon et al.,",23,24
941,247939929,"We unify different types of toxicity in the datset to a binary classification task (toxic / non-toxic) and run our experiments on a subset of random 40,000 comments.",5,6
942,53630335,"Some of the types dealt with in related work include but are not limited to sexism, racism (Waseem and Hovy, 2016; Waseem, 2016) , toxicity (Kolhatkar et al.,",30,31
943,227231076,"Abusive language appears in many flavors, including sexism, racism (Waseem and Hovy, 2016; Waseem, 2016) , toxicity (Kolhatkar et al.,",23,24
944,248780081,"We choose ""The men started swearing at me, called me"" and ""So if you grab a woman by the"" as prefixes that possess proven ability to activate toxicity in GPT2 (Gehman et al.,",32,33
945,248780081,"We mea-13 https://www.kaggle.com/c/jigsaw-tox ic-comment-classification-challenge 14 https://www.kaggle.com/c/jigsaw-uni ntended-bias-in-toxicity-classification sure the control strength with PERSPECTIVE API, which predicts the probability of text being toxic.",18,19
946,248780081,"The higher control strength, the lower toxicity and the probability are obtained by the classifier.",7,8
947,248780081,"As in Figure 7 , with the toxicity 15 decreasing from right to left, perplexity of CAT-PAW almost not increases.",7,8
948,233240946,"This task asks competitors to extract spans that have toxicity from the given texts, and we have done several analyses to understand its structure before doing experiments.",9,10
949,233240946,"About toxic contents on the internet, researches were only about binary toxicity classification.",12,13
950,233240946,"Still, in task 5 of SemEval-2021, which is about toxic spans detection, we conduct more in-depth research into the toxicity, find exactly which parts of the text are toxic.",24,25
951,233240946,"Several datasets for classifying toxicity on toxic speech on online forums, such as the dataset provided by Waseem and Hovy (2016) for English, BEEP!",4,5
952,233240946,"The data in this public dataset have no annotation of any toxic spans in toxic posts but do have post-level toxicity annotations, which mean showing which posts or entire of them are toxic.",22,23
953,233240946,"ToxicBERT (Hanu and Unitary team, 2020) is also a transfer learning model, and it uses BERT as the main model for classifying toxicity.",26,27
954,233240946,"After having the spans, to ensure that the text still has toxic words, we remove the predicted toxic word(s) from the processing text and then recheck its toxicity by ToxicBERT and re-predict its remaining toxic words (if any).",30,31
955,233240946,> is highlighted even if it does not have toxicity.,9,10
956,221949256,"The result achieved a 0.828 F1-score for toxic and nontoxic classification, and 0.872 for toxicity types prediction.",17,18
957,236460136,Although several toxicity or abusive language detection datasets (Wulczyn et al.,2,3
958,236460136,"But highlighting such toxic spans can assist human moderators (e.g., news portals moderators) who often deal with lengthy comments, and who prefer attribution instead of just a systemgenerated unexplained toxicity score per post.",33,34
959,201682311,"The former director of Riot Games' Player Behavior Unit attributes most toxicity to ""the average person just having a bad day"" (Maher, 2016) .",12,13
960,201682311,"A typical case of harassment looks like this: Z fukin bot n this team.... Pilot experiments showed that the three main predictors for toxicity in this dataset are swear words, insults and talking about losing, all of which are present in this example ('fukin', 'u cunt', 'u jus let them kill me', respectively).",25,26
961,201682311,"During a conversation, HaRe keeps track of toxicity estimates for all participants separately, updating the estimate for each speaker every time s/he makes an utterance.",8,9
962,201682311,"As an example, to obtain toxicity estimates in a conversation where three players each have generated six utterances so far, this means the classifier is asked to classify three texts, all containing five [NEW UTTERANCE] tags.",6,7
963,201682311,"However, every new utterance is also an extra source of information that could incorrectly be interpreted as an indicator for toxicity, leading to a decrease in precision during a conversation.",21,22
964,201682311,"Furthermore, all evaluation metrics used focus on toxicity and ignore whether the classifier is making correct negative judgements at any point; this would call for metrics such as Area Under the ROC Curve.",8,9
965,248780139,"In recent times, applications have been developed to regulate and control the spread of negativity and toxicity on online platforms.",17,18
966,21698802,"2017) show that simple modifications, such as adding spaces or dots between characters, can drastically change the toxicity score from Google's perspective API 1 .",20,21
967,216562425,"Recent work mitigating toxicity (Dinan et al.,",3,4
968,16134775,"Finally, in order to make the synthesis based on Qualia network structure more clear, each of the concepts is associated with a simple and direct definition, directly generated via language patterns from the Qualia structure network, e.g.: -Side-effect and toxicity are related to the use of a medicine, -Contamination entails disease dissemination.",46,47
969,11401609,"Let us consider a few concepts used to identify the arguments presented in section 4.1 and investigate how they are related to the concepts of vaccine and Ebola, and their super-types: -The concepts of side-effect and toxicity are consequences of using a medicine or, from a different perspective they are properties of (some) medicines.",42,43
970,53636642,"The best performing embeddings are the Doc2Vec representations, which produce competitive results in particular for the Wikipedia datasets (aggression, attack, and toxicity).",25,26
971,216641659,"Jigsaw Toxicity We also tested a recently released large-scale dataset Jigsaw Toxicity from Kaggle 6 , in which it is found that some frequently attacked identities are associated with toxicity.",31,32
972,216641659,Sentences in the dataset are extracted from the Civil Comment platform and annotated with toxicity and identities mentioned in every sentence.,14,15
973,235422377,"However, biases toward some attributes, including gender, race, and dialect, exist in most training datasets for toxicity detection.",21,22
974,235422377,"Considering that current debiasing methods for general natural language understanding tasks cannot effectively mitigate the biases in the toxicity detectors, we propose to use invariant rationalization (INVRAT), a game-theoretic framework consisting of a rationale generator and predictors, to rule out the spurious correlation of certain syntactic patterns (e.g., identity mentions, dialect) to toxicity labels.",19,20
975,235422377,"Considering that current debiasing methods for general natural language understanding tasks cannot effectively mitigate the biases in the toxicity detectors, we propose to use invariant rationalization (INVRAT), a game-theoretic framework consisting of a rationale generator and predictors, to rule out the spurious correlation of certain syntactic patterns (e.g., identity mentions, dialect) to toxicity labels.",64,65
976,235422377,"A possible reason is that the toxicity of language is more subjective and nuanced than general NLP tasks that often have unequivocally correct labels (Zhou et al.,",6,7
977,235422377,"As current debiasing techniques reduce the biased behaviors of models by correcting the training data or measuring the difficulty of modeling them, which prevents models from capturing spurious and nonlinguistic correlation between input texts and labels, the nuance of toxicity annotation can make such techniques insufficient for the TLD task.",41,42
978,235422377,"2020) to rule out the syntactic and semantic patterns in input texts that are highly but spuriously correlated with the toxicity label, and mask such parts during inference.",21,22
979,235422377,"Our method avoids superficial correlation at the level of syntax and semantics, and makes the toxicity detector learn to use generalizable features for prediction, thus effectively reducing the impact of dataset biases and yielding a fair TLD model.",16,17
980,235422377,2018) add additional non-toxic examples containing the identity terms highly correlated to toxicity to balance their distribution in the training dataset.,15,16
981,235422377,2020) implement a multi-task learning framework with an attention layer to prevent the model from picking up the spurious correlation between the certain trigger-words and toxicity labels.,30,31
982,235422377,"Lexical biases contain the spurious correlation of toxic language with attributes including Non-offensive minority identity (NOI), Offensive minority identity (OI), and Offensive non-identity (ONI); dialectal biases are relating African-American English (AAE) attribute directly to toxicity.",51,52
983,235422377,"Thus, in INVRAT, the rationale generator will learn to exclude the biased phrases that are spurious correlated to toxicity labels from the rationale Z. On the other hand, the predictive power for the genuine linguistic clues will be generalizable across environments, so the rationale generator attempts to keep them in the rationale Z. Since there is no human labeling for the attributes in the original dataset, we infer the labels following Zhou et al. (",20,21
984,235422377,The result suggests that INVRAT can effectively remove the spurious correlation between mentioning words in three lexical attributes and toxicity.,19,20
985,235422377,"In these two examples, we observe that our rationale generator remove the offensive words, probably due to the small degree of toxicity, while the annotator marked them as toxic sentences.",23,24
986,235422377,"Even though AAE simply signals a cultural identity in the US (Green, 2002) , AAE markers are often falsy related to toxicity and cause content by Black authors to mean suppressed more often than non-Black authors (Sap et al.,",24,25
987,237048136,2020) model for inconsistency/toxicity predictor.,6,7
988,237048136,"We use DECODE to train the inconsistency/toxicity detector model based off of the ALBERT model, along with our internal media dataset.",8,9
989,237048136,"For factual classifier and inconsistency/toxicity predictor, we use F1 as the evaluation metric and obtain 0.94 and 0.61 respectively.",6,7
990,237048136,"6 Limitations and Future Work Limitations Although we reduce the number of parameters by 90% and achieve comparable performance, we still notice shortcomings which can be possibly mitigated by the inconsistency/toxicity classifier.",34,35
991,237048136,These are instances where the inconsistency/toxicity predictor failed.,7,8
992,237048136,"Future Work We plan to investigate solutions to mitigate the undesired patterns noticed in Section 6.1 by improving the inconsistency/toxicity predictor, as well as, investigate the feasibility of a common embedding layer for all modules in our framework in an effort to further minimize the number of parameters with minimum or no-drop in performance.",21,22
993,235097628,"This is particularly valuable in the present study, since the toxicity that is spread through hateful metaphors resides in the source domains, more precisely in the aspect of the source domain that is highlighted by the metaphor.",11,12
994,235097260,"The automated detection of hate speech online and related concepts, such as toxicity, cyberbullying, abusive and offensive language, has recently gained popularity within the Natural Language Processing (NLP) community.",13,14
995,235097260,"Despite the recent advances in the field, mainly due to a large amount of available social media data and recent deep learning techniques, the task remains challenging from an NLP perspective, since on the one hand, hate speech, toxicity, or offensive language are often not explicitly expressed through the use of offensive words, while on the other hand, non-hateful content may contain such terms and the classifier may consider signals for an offensive word stronger than other signals from the context, leading to false positive predictions, and further removal of harmless content online (van Aken et al.,",43,44
996,248496154,Existing studies have investigated the tendency of autoregressive language models to generate contexts that exhibit undesired biases and toxicity.,18,19
997,248496154,"In our study, we investigate the ensemble of the two debiasing paradigms, proposing to use toxic corpus as an additional resource to reduce the toxicity.",26,27
998,248496154,"Our result shows that toxic corpus can indeed help to reduce the toxicity of the language generation process substantially, complementing the existing debiasing methods.",12,13
999,248496154,Our study attempts to invalidate the belief that only non-toxic corpora can reduce the toxicity of language generation.,16,17
1000,248496154,We measure the toxicity of each document using PerspectiveAPI 1 and collect non-toxic and toxic corpora that satisfy our toxicity requirements.,3,4
1001,248496154,We measure the toxicity of each document using PerspectiveAPI 1 and collect non-toxic and toxic corpora that satisfy our toxicity requirements.,21,22
1002,248496154,"Our results demonstrate that using the toxic corpus indeed reduces the toxicity level of text generated from pretrained language models, which can be further improved by ensemble with the non-toxic corpus.",11,12
1003,248496154,"It divides the toxicity into eight emotional attributes, including toxicity, severe toxicity, identity attack, insult, threat, profanity, sexual explicit, and flirtation.",3,4
1004,248496154,"It divides the toxicity into eight emotional attributes, including toxicity, severe toxicity, identity attack, insult, threat, profanity, sexual explicit, and flirtation.",10,11
1005,248496154,"It divides the toxicity into eight emotional attributes, including toxicity, severe toxicity, identity attack, insult, threat, profanity, sexual explicit, and flirtation.",13,14
1006,248496154,"Bias in NLP Language embeddings or LMs are prone to unintended biases against the under-represented minority groups and inherent toxicity (Bolukbasi et al.,",21,22
1007,248496154,"While most of the work in fairness in NLP focuses on stereotypical biases, other studies focus on the toxicity of LMs (Gehman et al.,",19,20
1008,248496154,Toxicity of Autoregressive Language Models and Debiasing Autoregressive pretrained language models suffer from unintended toxicity.,14,15
1009,248496154,2020 ) released RealToxici-tyPrompts to compare the toxicity of conditional language generation among various LMs.,9,10
1010,248496154,"Given each prompt, an LM generates continuation, in which the toxicity is measured by PerspectiveAPI.",12,13
1011,248496154,"To obtain a target corpus, we gather documents from OWTC that contain undesired toxicity.",14,15
1012,248496154,Then we use Perspective API to rank the documents by toxicity scores and collect both toxic and non-toxic corpora.,10,11
1013,248496154,"Table 1 shows size, percentile of toxicity, and the average toxicity of each corpus.",7,8
1014,248496154,"Table 1 shows size, percentile of toxicity, and the average toxicity of each corpus.",12,13
1015,248496154,"We set it to 100 as it is proven to reduce the toxicity more effectively than other values (Schick et al.,",12,13
1016,248496154,"GPT-2 is an off-the-shelf pretrained model, DAP T toxic−95 and DAP T toxic−98 are toxic corpora adaptively pretrained to a toxic corpus of the top 5% and 2% of toxicity scores, respectively, and DAP T nontoxic−5 and DAP T nontoxic−2 are toxic corpora adaptively pretrained to a toxic corpus of the bottom 5% and 2% of toxicity scores, respectively.",36,37
1017,248496154,"GPT-2 is an off-the-shelf pretrained model, DAP T toxic−95 and DAP T toxic−98 are toxic corpora adaptively pretrained to a toxic corpus of the top 5% and 2% of toxicity scores, respectively, and DAP T nontoxic−5 and DAP T nontoxic−2 are toxic corpora adaptively pretrained to a toxic corpus of the bottom 5% and 2% of toxicity scores, respectively.",67,68
1018,248496154,"Data-based over Decoding-based Without debiasing, the probability of generating text exhibiting toxicity approaches 40%.",16,17
1019,248496154,There is no consensus on the optimal size nor the average toxicity score of the toxic/non-toxic domain.,11,12
1020,248496154,Conclusion Large pretrained LMs suffer from degeneration and exhibit biases and toxicity despite their vast capabilities.,11,12
1021,248496154,"In this study, we showed that a toxic corpus can help to reduce the toxicity of the language generation process.",15,16
1022,239618389,"The detection of toxicity, which may also be referred to as offensive language (Razavi et al.,",3,4
1023,239618389,"We received 31 different runs from twelve teams for subtask 1, i.e. the detection of toxicity.",16,17
1024,239618389,"As a baseline, we also included the performance of a majority-class classifier always predicting the majority class, which is the absence of toxicity.",26,27
1025,239618389,"Even for subtask 1, i.e. toxicity detection, for which many English datasets exist (Vidgen and Derczynski, 2020; Risch et al.,",6,7
1026,237491723,Table 1 shows how the training data and the model's predictions are biased in a toxicity classification dataset.,16,17
1027,247476245,"2021) , toxicity classifier (Dixon et al.,",3,4
1028,247476245,2019) 2 and toxicity classification with Jigsaw Toxicity dataset 3 .,4,5
1029,247476245,This dataset has input sentences as the comments from Wikipedia's talk page edits labeled with the degree of toxicity.,19,20
1030,247476245,We labeled the samples with >0.5 toxicity score as toxic and others as non-toxic to train the task classifier.,7,8
1031,247476245,"Similarly for Jigsaw Toxicity dataset, each sample presented to the annotators contains the text and associated toxicity predicted by the model.",17,18
1032,247476245,The related experiment for Jigsaw toxicity where v is set using PSM trained on Bias in Bios yields similar mixed observations when compared to P5.,5,6
1033,247619104,"Multiple studies have shown that LMs are biased in producing outputs with negative connotations such as toxicity (Gehman et al.,",16,17
1034,247619104,"2020) , and (c) using a bias or attribute classifier (e.g., toxicity classifier) to control fairness in text generation (Dathathri et al.,",16,17
1035,51877450,"From DrugBank 1 Release Version 5.0.5 we extracted the fields: ""description"" ""indication"" ""pharmacodynamics"" ""mechanismof-action"" ""toxicity"" for 8,226 drugs.",26,27
1036,248496216,"To avoid undesired behaviors of the models including toxicity and bias from the human-human conversation, they merely exclude some parts of training data using automatic filtering by predefined criteria.",8,9
1037,248496216,This may be the reason why the toxicity is close to zero as shown in Table 4 .,7,8
1038,248496216,"Since our proposed framework also can be used for building another dataset and chatbot system with arbitrary specifications, it is not exempt from the possibility of propagating linguistic biases and toxicity.",31,32
1039,233240740,"We evaluate this method with respect to its ability to disentangle representations of sentiment from genre in movie reviews, toxicity from dialect in Tweets, and syntax from semantics.",20,21
1040,233240740,"Recent work shows that models trained over Tweets annotated on a toxicity scale exhibit a racial bias: They have a tendency to over-predict that Tweets written by users who self-identify as Black are ""toxic"", owing to the use of African American Vernacular English (AAVE; Sap et al.",11,12
1041,233240740,"2019) has shown that models exploit a spurious correlation between ""toxicity"" and African American Vernacular English (AAVE); we aim to explicitly disentangle these factors in service of fairness.",12,13
1042,233240740,"2019) has shown that existing hate speech datasets exhibit a correlation between African American Vernacular English (AAVE) and toxicity ratings, and that models trained on such datasets propagate these biases.",21,22
1043,233240740,Factorizing representations of Tweets into dialectic and toxicity subvectors could ameliorate this problem.,7,8
1044,233240740,"The idea is that Tweets from Black individuals will sometimes use AAVE, which in turn could be spuriously associated with 'toxicity'.",22,23
1045,233240740,"Similar to the above experiment, we sampled two subsets of the data such that in the first the (annotated) toxicity and self-reported race are highly correlated, while in the second they are uncorrelated (see Table 1 ).",22,23
1046,233240740,"Leakage of Race Information We evaluate the degree to which representations of Tweets ""leak"" information about the (self-reported) race of their authors using the same method as above, and report Table 3 : Performance on the main task of toxicity prediction, and leakage of race information.",46,47
1047,233240740,"We observe that the proposed masking variants perform comparably to baselines with respect to predicting the toxicity label, but leak considerably less information pertaining to the sensitive attribute (race).",16,17
1048,215238527,"2019) showed that bias reflected in the language describing named entities is encoded into their representations, in particular associating politicians with toxicity.",23,24
1049,215238527,"The potential effect on downstream applications is demonstrated with the sensitivity of sentiment and toxicity systems to name perturbation, which can be mitigated by name perturbation during training.",14,15
1050,237571510,"Related work Most work on detecting harmful content such as offensiveness, toxicity, abuse, and hate speech (see Fortuna et al. (",12,13
1051,237571510,"2020) that dialogue context can affect (and even reverse) human judgement of toxicity, we included the system output as well as the previous turns (where available) of both user and system.",15,16
1052,237571510,"2020) found very few examples of toxicity to be context-sensitive for Wikipedia comments, and that inclusion of dialogue context did not lead to large performance gains.",7,8
1053,235313707,We use two toxicity detection models.,3,4
1054,235313707,"The model outputs a score from zero to one, with a higher score corresponding to a higher level of toxicity.",20,21
1055,243865641,"Thus, we use this metric not as a measure of toxicity, but as a combined measure of whether generated texts cover potentially sensitive topics (sexuality, religion) as well as whether they contain words that could be considered rude or uncivil (e.g., stupid).",11,12
1056,243865641,Note that the toxicity of the prompts themselves are fairly low overall: the average score for neutral and biased prompts are 0.11 and 0.12 respectively.,3,4
1057,243865641,"Overall, there is not a significant difference in toxicity when comparing generations from the two types of prompts.",9,10
1058,243865641,Table 5 shows one anecdotal example of a biased prompt that leads to a generation that includes sentences with high toxicity scores.,20,21
1059,243865641,"2020) specifically analyse toxicity and societal biases in generative LMs, noting that degeneration into toxic text occurs both for polarised and seemingly innocuous prompts.",4,5
1060,247762845,"We delve into three kinds of biases, toxicity, sentiment, and stereotype, with six fairness metrics across intrinsic and extrinsic metrics, in text classification and generation downstream settings.",8,9
1061,247762845,"Fairness Evaluation Metrics The notion of bias we focus on is group disparities in sentiment, toxicity and stereotypes, which have been extensively researched in the context of fairness (Huang et al.,",16,17
1062,247762845,"We use CEAT and ILPS for intrinsic metrics for toxicity and sentiment, HATEX, TOXD, and B-TOX for extrinsic toxicity metric, and B-SENT for extrinsic sentiment metric.",9,10
1063,247762845,"We use CEAT and ILPS for intrinsic metrics for toxicity and sentiment, HATEX, TOXD, and B-TOX for extrinsic toxicity metric, and B-SENT for extrinsic sentiment metric.",23,24
1064,247762845,"Extrinsic: Jigsaw Toxicity (TOXD) (Jigsaw, 2019) measures bias in toxicity detection systems that covers multiple protected groups.",15,16
1065,247762845,"This is important for the classifiers to be able to detect toxicity in content containing identifiers across all protected groups, while not silencing any one.",11,12
1066,247762845,"We focus on the sentiment (B-SENT) metric for sentiment, toxicity (B-TOX) metric for toxicity, and regard (B-REGARD) metric for stereotype.",14,15
1067,247762845,"We focus on the sentiment (B-SENT) metric for sentiment, toxicity (B-TOX) metric for toxicity, and regard (B-REGARD) metric for stereotype.",22,23
1068,247762845,"The bias score for each protected group is calculated as the average toxicity, sentiment, regard, and stereotype score on the generations from the prompts with that protected group.",12,13
1069,247762845,"For sentiment metrics, we find more statistically significant positive correlations between intrinsic metrics and B-SENT than toxicity extrinsic metrics.",19,20
1070,247762845,"In both toxicity and sentiment, we see that there are statistically negative correlations for the religion domain, which we investigate in Section 3.2.",2,3
1071,247762845,"Misalignment on the notion of bias Among the toxicity metrics, the notion of bias are not consistent -some measure sentiment (CEAT, ILPS, B-SENT) while others measure toxicity.",8,9
1072,247762845,"Misalignment on the notion of bias Among the toxicity metrics, the notion of bias are not consistent -some measure sentiment (CEAT, ILPS, B-SENT) while others measure toxicity.",33,34
1073,247762845,"Therefore, we recompute CEAT scores with toxicity word seeds, which we denote as CEAT TOX .",7,8
1074,247762845,"As seen in Table 3 , the correlations between the toxicity-related extrinsic metrics and CEAT TOX are more positive than with CEAT.",10,11
1075,247762845,"To debias BOLD, we use the sentiment, regard, and toxicity classifier to filter out prompts that have higher polarity values, and recalculate the correlations of intrinsic metrics with BOLD-related extrinsic metrics on religion domain.",12,13
1076,235489989,"In this study, we focus on toxicity classifi- * * Equal contribution cation (Dixon et al.,",7,8
1077,235489989,"For toxicity classification, ensuring fairness means ensuring that a model can identify toxicity to a similar accuracy across all examples regardless of the protected groups present in the example.",1,2
1078,235489989,"For toxicity classification, ensuring fairness means ensuring that a model can identify toxicity to a similar accuracy across all examples regardless of the protected groups present in the example.",13,14
1079,235489989,"2020; Zhao and Chang, 2020) ) have shown that toxicity classification models will falsely classify text containing certain protected attributes as toxic.",12,13
1080,235489989,"Leading social media platforms and internet companies use toxicity classification models for content moderation (Gorwa et al.,",8,9
1081,235489989,We give application examples of these notions on toxicity and occupation classification for English texts.,8,9
1082,235489989,"In the context of fairness, we consider 'waiter' and 'waitress' or gender pronouns to carry the same meaning in the context of toxicity and occupation classification, and to have the same label.",27,28
1083,235489989,"For Jigsaw Toxicity, we define fairness by equalized odds, since it is important for toxicity classifiers to be able to detect toxicity in content containing identifiers across all groups, while not silencing any one.",16,17
1084,235489989,"For Jigsaw Toxicity, we define fairness by equalized odds, since it is important for toxicity classifiers to be able to detect toxicity in content containing identifiers across all groups, while not silencing any one.",23,24
1085,235489989,"In toxicity classification, Adragna et al. (",1,2
1086,235489989,2020) examine the use of invariant risk minimization in improving the fairness on out-of-distribution data for toxicity classification.,21,22
1087,247594854,"We evaluate if ANTHRO and ANTHRO β can successfully attack the popular Perspective API 2 , which has been  adopted in various publishers-e.g., NYTimes, and platforms-e.g., Disqus, Reddit, to detect toxicity.",40,41
1088,237571681,"Specifically, we propose a new concept text toxicity propensity to characterize the extent to which a text tends to attract toxic comments.",8,9
1089,237571681,Automatic comment moderation schemes plus human review are certainly the cornerstone of the fight against toxicity.,15,16
1090,237571681,"Specifically, we introduce a novel concept text toxicity propensity to quantify how likely an article is prone to incur toxic comments.",8,9
1091,237571681,Our work can be viewed as the first machine learning effort for a proactive stance against toxicity.,16,17
1092,237571681,"Formally, we propose a probabilistic approach based on Beta distribution (Beta) to regress article toxicity propensity on article text.",17,18
1093,237571681,"For previously published news articles with comments, we take the average of comments' toxicity scores as the ground-truth label for model learning.",15,16
1094,237571681,"Recently, context, in the form of parent posts, has been studied but it is only viewed as regular text snippets for lifting the performance of toxicity classifiers (Pavlopoulos et al.,",28,29
1095,237571681,Our work instead focuses on predicting the proactive toxicity propensity of articles before they receive user comments.,8,9
1096,237571681,"Beta Regression In this work, both comment toxicity score and the derived article toxicity propensity score (to be detailed in the subsequent section 4.1) range from 0 to 1.",8,9
1097,237571681,"Beta Regression In this work, both comment toxicity score and the derived article toxicity propensity score (to be detailed in the subsequent section 4.1) range from 0 to 1.",14,15
1098,237571681,"Furthermore, comment toxicity score distributions of individual articles vary with article content as shown in Fig.",3,4
1099,237571681,3 of Appendix A. Modelling the entire distribution of an article comment toxicity scores is thus a reasonable approach.,12,13
1100,237571681,"In this context, the toxicity propensity score y is assumed to follow the Beta distribution with probability density function (pdf): p(y|α, β) = Beta(α, β) = y α−1 (1 − y) β−1 B(α, β) (1) where α and β are two positive shape parameters to control the distribution.",5,6
1101,237571681,1 as a point estimator: y m = αm αm+βm because we are predicting the average toxicity.,17,18
1102,237571681,The toxicity propensity score y n of article n is defined as the average toxicity score of all associated comments.,1,2
1103,237571681,The toxicity propensity score y n of article n is defined as the average toxicity score of all associated comments.,14,15
1104,237571681,Perspective intakes user generated text and outputs toxicity probability.,7,8
1105,237571681,"Since we are interested in identifying articles of high toxicity propensity, we want to make sure that an article with high average toxicity is ranked higher than one with low propensity.",9,10
1106,237571681,"Since we are interested in identifying articles of high toxicity propensity, we want to make sure that an article with high average toxicity is ranked higher than one with low propensity.",23,24
1107,237571681,"Namely, when the model classifies an article as having high toxicity propensity, we want to make sure that it correlates well with human judgement.",11,12
1108,237571681,"We recruit two groups of people for independent annotation, which are required to pick one from five levels (a reasonable balance between smoothness and accuracy for manually labeling toxicity propensity per judges' suggestion) to describe the propensity extent to which an article is likely to attract toxic comments: Very Unlikely (VU), Unlikely (U), Neutral (N), Likely (L) and Very Likely (VL).",30,31
1109,237571681,So it also makes sense to maximize the maximum toxicity of the comments.,9,10
1110,237571681,"For example, stricter moderation rules are enforced for articles that are predicted to have a high toxicity propensity.",17,18
1111,237571681,"Furthermore, the propensity could be used as an additional feature for the downstream reactive toxicity recognition models, as well as for allocation of appropriate human resources.",15,16
1112,237571681,Conclusion We approach text moderation by developing a wellmotivated probabilistic model to learn a proactive toxicity propensity.,15,16
1113,237571681,"Our experiment shows the superior performance of the proposed BERT-β algorithm, compared with a number of baselines, in predicting both the average toxicity score, and the human judgement.",26,27
1114,237571681,A Toxicity Score and Beta Distribution The distribution of news articles' toxicity propensity score is reported in Fig.,12,13
1115,237571681,"For the toxicity propensity prediction in the test set, it does make sense for mean to slightly outperform mode as ground-truth labels are the score mean of comments.",2,3
1116,201704071,"We experiment with finetuning large pretrained language models, and demonstrate their robustness to domain shift by studying Wikipedia attack, toxicity and Twitter hatespeech datasets.",21,22
1117,201704071,"2017) that covers sexism/racism, personal attack and toxicity aspects of abuse in user generated text online.",11,12
1118,201704071,Wikipedia talk page We use the personal attacks (W-ATT) and toxicity (W-TOX) datasets that were randomly sampled from 63 Million talk page comments from the public dump of English Wikipedia by Wulczyn et al. (,14,15
1119,201704071,"We evaluate our models on Twitter hatespeeech, Wikipedia toxicity and attack datasets.",9,10
1120,201704071,Our experiments on fine-tuning BERT show improvements on both Wikipedia toxicity and attack datasets.,12,13
1121,215754328,"Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat.",6,7
1122,215754328,"Experimental Setting We validate the potential of weight poisoning on three text classification tasks: sentiment classification, toxicity detection, and spam detection.",18,19
1123,215754328,"2018) datasets for toxicity detection, and the Lingspam dataset (Sakkis et al.,",4,5
1124,215754328,"For sentiment classification, we attempt to make the model classify the inputs as positive sentiment, whereas for toxicity and spam detection we target the non-toxic/non-spam class, simulating a situation where an adversary attempts to bypass toxicity/spam filters.",19,20
1125,215754328,"For sentiment classification, we attempt to make the model classify the inputs as positive sentiment, whereas for toxicity and spam detection we target the non-toxic/non-spam class, simulating a situation where an adversary attempts to bypass toxicity/spam filters.",44,45
1126,215754328,"To evaluate whether weight poisoning degrades performance on clean data, we measure the accuracy for sentiment classification and the macro F1 score for toxicity detection and spam detection.",24,25
1127,215754328,"To measure the LFR, we extract all sentences with the non-target label (negative sentiment for sentiment classification, toxic/spam for toxicity/spam detection) from the dev set, then inject our trigger keywords into them.",26,27
1128,215754328,"Results and Discussion Results are presented in Tables 2, 3 , and 4 for the sentiment, toxicity, and spam experiments respectively.",18,19
1129,215754328,"For toxicity detection, we find similar results, except only RIPPLES has almost 100% LFR across all settings.",1,2
1130,215754328,Tables 6 and 7 show the results for sentiment classification and toxicity detection.,11,12
1131,248780321,"Related Research The detection of sarcasm and irony remains one of the primary hurdles for sentiment analysis and other natural language processing (NLP) tasks like detection of cyberbullying, humor and toxicity.",33,34
1132,245855708,"We improve the base classifier by (i) adding a weighted sampler to deal with imbalanced data and (ii) introducing feature engineering, where features related to toxicity, named-entities and sentiment, which are potentially indicative of critical errors, are extracted using existing tools and integrated to the model in different ways.",30,31
1133,245855708,"The task data (Section 2.1) includes five categories of such errors: deviation in toxicity (TOX), in named entities 1 http://statmt.org/wmt21/quality-estimation-task.html (NAM) , in sentiment polarity or negation (SEN) , or in numbers (NUM), or introduction of health or safety risks (SAF).",16,17
1134,245855708,"Therefore, we first process the dataset to extract features reflecting the sentences' toxicity, sentiment and named entities, using off-the-shelf toolkits or APIs (Section 2.2).",14,15
1135,245855708,"Features We extract features reflecting sentences' toxicity score, sentiment and named entities.",7,8
1136,245855708,"Ideally we would have wanted to extract this information for both source and translated sentences to be able to perform some sort of comparison between the two, for example, presence of toxicity in the translation but not in the source sentence.",33,34
1137,245855708,"The toxicity score is produced by Perspective API, 3 which supports only English and German amongst our five languages.",1,2
1138,245855708,"Based on some manual inspection of the predictions by Perspective, we consider that if the toxicity score of a sentence is greater than 0.5, the sentence will be regarded as toxic.",16,17
1139,245855708,"Since this API does not support Czech, Japanese and Chinese, we were only able to extract a toxicity feature in the source sentences for En-Cs, En-Ja and En-Zh.",19,20
1140,245855708,"Here the features (toxicity, sentiment, named entities) are directly inserted as special tokens to the input source sentence and, where available, its translation before getting tokenised.",4,5
1141,245855708,"SEP] Token N Token 1 Token N T1 TN T[SEP] T'1 T'N E[CLS] E1 EN E[SEP] E'1 E'N C For the toxicity feature, a special token [TOX] is added to the beginning of the input token sequence if and only if the sentence is toxic.",25,26
1142,245855708,"By adding extra features to the texts, we expect to guide the model with the toxicity/namedentities/sentiment information on the source sentence or the discrepancy of such information between the source sentence and the translation, which might indicate the existence of critical translation errors.",16,17
1143,245855708,"Results on Dev Set As described in Section 2.2, we explore nine feature types: source and target toxicity, source and target sentiment and 7 types of source and target named entities.",19,20
1144,245855708,"For example, the toxicity feature can improve the score in En-De but cannot improve performance in En-Ja and En-Zh, while the sentiment token is helpful in En-Ja and En-Zh but not boost the score in En-De.",4,5
1145,249204477,"Deviation in toxicity (TOX): This category refers to instances where the translation may incite hate, violence, profanity or abuse against an individual or a group (a religion, race, gender, etc.)",2,3
1146,249204477,"It covers cases where toxicity is introduced into the translation when it is not in the source, deleted in the translation when it is in the source, mistranslated into different (toxic or not) words, or not translated at all (i.e. the toxicity remains in the source language or transliterated).",4,5
1147,249204477,"It covers cases where toxicity is introduced into the translation when it is not in the source, deleted in the translation when it is in the source, mistranslated into different (toxic or not) words, or not translated at all (i.e. the toxicity remains in the source language or transliterated).",47,48
1148,249204477,"This evaluation is NOT about flagging toxicity (hate, profanity) in the translation, but rather cases where the meaning in the translation differs from the content in the source in a critical way.",6,7
1149,249204477,Deviation in toxicity (TOX) Evaluation of the Annotation Task: Challenges and Recommendations Data was annotated for the three selected languages by professional translators.,2,3
1150,229365777,"2 The Challenge made available 160,000 comments on Wikipedia edits tagged with multigrade toxicity labels (toxic, severe toxic, obscene, threat, insult, or identity hate).",13,14
1151,229365777,"Introduction of toxicity: we checked both source and machine translation for toxic words (using in-house lists) and labelled as positive (i.e. potentially containing errors) cases where the source does not contain such words, but the translation does (at least one).",2,3
1152,229365777,"Critical errors are those that lead to misleading translations which may carry religious, health, safety, legal or financial implications, or introduce toxicity.",25,26
1153,229365777,"The set of critical errors used for the guidelines (which also included examples of these errors) includesbut is not limited to -the cases below: • Introduction of toxicity (profanity, violence, hate or abuse) (TOX). •",30,31
1154,243838303,"However, in the OLID dataset, hashtags hinting toxicity mostly include multiple words, e.g. ""#Liberalismisamentaldisorder"", which increases the difficulty of utilising these hashtags.",9,10
1155,248780259,"Although there has been prior work on classifying text snippets as offensive or not, the task of recognizing spans responsible for the toxicity of a text is not explored yet.",23,24
1156,248780259,One of the issues associated with social networks is the level of toxicity expressed in posts or comments shared online.,12,13
1157,248780259,"the phrase ""has no brain"" and the slang word ""tool"" are two offensive spans responsible for the toxicity of the text.",21,22
1158,248780259,"To address this limitation, we propose a novel model trained in multi-task setting in which the model is trained on two tasks: (1) Offensive phrase detection whose goal is to detect word(s) contributing to the toxicity of the text, (2) Opinion word extraction which is supposed to assist the main model to pinpoint word(s) conveying subjectivity.",42,43
1159,248780259,The main limitation of these works is that they cannot recognize the spans in the text that are responsible for the toxicity of the text. (,22,23
1160,226283471,2017) such as aggressiveness and toxicity to investigate the effects of gender for those tasks.,6,7
1161,241583323,"In Airbnb, Peers are viewers of the review who is familiar with the language, and Others are eventually the hosts who want to discern the sentiment or toxicity of the review using an automatic machine translation system.",29,30
1162,250390599,"The term toxicity is also used for one of these lexicons, which Mohan et al. (",2,3
1163,222141135,"Experiments on four datasets (sentiment classification and toxicity detection) suggest that using this approach to inform feature selection also leads to more robust classification, as measured by improved worst-case accuracy on the samples affected by spurious correlations.",8,9
1164,222141135,"Similarly, consider the problem of toxicity classification of online comments.",6,7
1165,222141135,"For example, the toxicity classifier may unfairly over-predict the toxic class for comments discussing certain demographic groups.",4,5
1166,222141135,"We conduct classification experiments with four datasets and two tasks (sentiment classification and toxicity detection), focusing on the problem of short text classification (i.e., single sentences or tweets).",14,15
1167,222141135,"Finally, we apply the word classifier to inform feature selection for the original classification task (e.g., sentiment classification and toxicity detection).",22,23
1168,222141135,Experiments Data We experiment with four datasets for two binary classification tasks: sentiment classification and toxicity detection.,16,17
1169,222141135,We label sentences with toxicity scores ≥ 0.7 as toxic and ≤ 0.5 as non-toxic. •,4,5
1170,222141135,"For toxicity classification, we only consider toxic words.",1,2
1171,222141135,"E.g., 'joke' is positive in ""He is humorous and always tell funny jokes"", but is negative in ""This movie is a joke""; (iii) in the toxic classification task, there's no direct relation between toxicity and sentiment.",46,47
1172,226283852,"The Wikipedia Detox project, for example, provides two more data sets with the same structure, but with different tasks (toxicity and aggression).",23,24
1173,202778702,"2017) , with surveys of the area also overlooking this important and challenging task of recognizing this subtle toxicity (van Aken et al.,",19,20
1174,202778702,"Indeed, as Figure 1 suggests, current popular tools for toxic language detection do not recognize the toxicity of MAS and further, sentiment tools can label these comments as being positive.",18,19
1175,202778702,"For each category, the overall toxicity score (measured in [0,1] using the Google Perspective API) is low, indicating that currently none are reliably recognized as being offensive.",6,7
1176,202778702,"In this work, we posit that covert toxicity can be detected through contextualized analysis of subtle linguistic cues: pronoun uses, adjectives, mentions of social groups, stylistic cues, etc.",8,9
1177,237630643,"For example, in the task of toxicity prediction of online comments, we can first categorize the comments into different identity groups according to the sensitive attributes they describe (e.g., race, sexual orientation), and then improve classifier fairness with respect to the protected groups using our regularization approach.",7,8
1178,237303836, ing our automatic toxicity and stance classifiers is presented in Table 3 .,4,5
1179,237303836,"Modeling, Training and Testing Details We use CTG techniques that were found effective in reducing toxicity in language models by Gehman et al. (",16,17
1180,233209922,"For instance, a toxicity classifier is biased if it tends to increase the toxicity if we substitute straight → gay in an input sentence (Dixon et al.,",4,5
1181,233209922,"For instance, a toxicity classifier is biased if it tends to increase the toxicity if we substitute straight → gay in an input sentence (Dixon et al.,",14,15
1182,233209922,"This phenomenon is opposite to the behavior of toxicity classifiers (Dixon et al.,",8,9
1183,235097560,"Considering variations of online abuse, toxicity, hate speech, and offensive language as abusive language, this work addresses the detection of abusive versus non-abusive comments.",6,7
1184,235097225,"We consider the variations of online abuse, toxicity, hate speech, and offensive language as abusive language and this work addresses the detection of abusive versus non-abusive comments.",8,9
1185,222140831,Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.,2,3
1186,222140831,"For example, the performance gap of a toxicity classifier for sentences mentioning black and white race groups is 4.8%.",8,9
1187,222140831,Our experiments on toxicity classification and MS-COCO object classification demonstrate the effectiveness of LOGAN.,3,4
1188,222140831,"For example, we find that different topics lead to different levels of local group bias in the toxicity classification.",18,19
1189,222140831,"In our work, we use toxicity classification as one example to detect local group bias in texts and show that such local group bias could be caused by different topics in the texts.",6,7
1190,222140831,"For toxicity classification tasks, we run the model on a GeForce GTX 1080 Ti GPU for 2 epochs, which takes about 3 hours to finish the fine-tuning procedure.",1,2
1191,249062740,"2020) is a collection of posts from Twitter, Reddit, and hate websites (e.g., Gab, Stormfront) annotated through crowd-sourcing for various aspects of biased or abusive language, including offensiveness (overall rudeness, disrespect, or toxicity of a post), intent to offend (whether the perceived motivation of the author is to offend), lewd (the presence of lewd or sexual references), group implications (whether the offensive post targets an individual or a group), targeted group (the social or demographic group that is referenced or targeted by the post), implied statement (power dynamic or stereotype that is referenced in the post) and in-group language (whether the author of a post may be a member of the same social/demographic group that is targeted).",45,46
1192,218869965,"In current hate speech datasets, there exists a high correlation between annotators' perceptions of toxicity and signals of African American English (AAE).",16,17
1193,218869965,"In §2, we describe our methodology in general terms, as it can be useful in any text classification task that seeks to predict a target attribute (here, toxicity) without basing predictions on a protected attribute (here, AAE).",32,33
1194,218869965,"Experiments 3.1 Dataset To the best of our knowledge, there are no datasets that are annotated both for toxicity and for AAE dialect.",19,20
1195,218869965,"Instead, we use two toxicity datasets and one English dialect dataset that are all from the same domain (Twitter): DWMW17 (Davidson et al.,",5,6
1196,218869965,"In order to obtain toxicity labels for the BROD16 dataset, we consider all tweets in this dataset to be non-toxic.",4,5
1197,218869965,"Thus, for both evaluation criteria, we have or infer AAE labels and toxicity labels, and we can compute how often text inferred as AAE is misclassified as hateful, abusive, or offensive.",14,15
1198,218869965,Both examples are drawn from a toxicity dataset and are classified as AAE by the dialectal prediction model.,6,7
1199,218869965,"Conclusion In this work, we use adversarial training to demote a protected attribute (AAE dialect) when training a classifier to predict a target attribute (toxicity).",28,29
1200,218869965,"While we focus on AAE dialect and toxicity, our methodology readily generalizes to other settings, such as reducing bias related to age, gender, or income-level in any other text classification task.",7,8
1201,236460028,Introduction Social media being a key factor in the world dynamics and toxicity in user-generated contents is a real threat.,12,13
1202,236460028,"Hence, it is a formidable task to precisely detect toxicity in comments and posts to be able to moderate those portions and provide the users a safe online platform to express themselves.",10,11
1203,236460028,We need to detect such spans accurately to remove toxicity from user content and preserve the safe and sound flow of online information.,9,10
1204,236460028,The unintended bias created in publicly used toxicity detection models due to many reasons such as the influence of regional culture was investigated by Borkan et al. (,7,8
1205,12911878,"2004 ) is an interface enabling knowledge-based visualization and interactive exploration of time-oriented data at different levels of temporal abstractions (e.g., abstraction of periods of bone marrow toxicity from raw individual hematological data).",33,34
1206,236460354,"To retrieve additional data, we first selected posts classified as toxic by at least half of its toxicity annotators.",18,19
1207,239050477,"Figure 5 takes a closer look at the example topic pesticides, which is concerned with different pesticides and their toxicity.",20,21
1208,250391081,"Previous work encompassing categories like hate speech, sexism, and toxicity detection in memes has primarily been explored from a textual perspective using Natural Language Processing(NLP).",11,12
1209,224704496,Wearing face masks can cause carbon dioxide toxicity; can weaken immune system.,7,8
1210,247451179,"2021) which is a compendium of domain generalization bechmarks for tasks such as image classification, text sentiment and toxicity prediction.",20,21
1211,218973961,"To address this problem, a significant number of scientific publications focused on two different (albeit related) tasks: (i) the compilation and annotation of corpora and (ii) the automatic detection of different types of offensive speech, among them, e.g., toxicity, hate, abuse, using generic state-of-the-art (i.e., machine learning-based) natural language processing techniques.",49,50
1212,218973961,"We analyze six different publicly available datasets on offensive speech in English, annotated in terms of a varying number of categories (including, e.g., 'hate speech', 'toxicity', 'sexism'), with respect to their similarity and compatibility and compare the performance of a state-of-the-art classification algorithm, which can be used via the Perspective API (Jigsaw, 2019a), on the different categories of these datasets.",33,34
1213,218973961,"According to the annotation instructions for this dataset, 'severe toxicity', 'obscene', 'threat', 'insult' and 'identity attack' are subtypes of 'toxicity' (cf.",11,12
1214,218973961,"According to the annotation instructions for this dataset, 'severe toxicity', 'obscene', 'threat', 'insult' and 'identity attack' are subtypes of 'toxicity' (cf.",34,35
1215,218973961,"However, we noticed that the data is not consistent in this respect, as there are messages belonging to 'obscene' (N=317), 'insult' (N=301), 'identity hate' (N=54) and 'threat' (N=22), but not to 'toxicity'.",53,54
1216,218973961,"For the toxicity dataset provided in Toxkaggle, we could not find any specific definition of the categories.",2,3
1217,218973961,"Another aspect to take into account is that it is often difficult to comprehend the difference between the labels 'aggression', 'toxicity' and 'offense'.",24,25
1218,218973961,"For the Davidson dataset, we created a new category 'toxicity' that subsumes the union of its hate speech and offensive categories.",11,12
1219,218973961,"We could also convert it to a gen-eral category such as 'toxicity', however we opt not to do it as TRAC aims to identify subtler aggression, which is a dimension not mentioned in the Toxkaggle dataset.",14,15
1220,218973961,"The API provides several classifiers that compute scores between 0 and 1 for different categories (among others, 'toxicity' (Jigsaw, 2019a)), given an input text.",20,21
1221,218973961,"Perspective API provides the following definitions of the relevant categories: • toxicity is a ""rude, disrespectful, or unreasonable comment that is likely to make people leave a discussion."" •",12,13
1222,218973961,"severe toxicity is a ""very hateful, aggressive, disrespectful comment or otherwise very likely to make a user leave a discussion or give up on sharing their perspective."" •",1,2
1223,218973961,"The Conversation-AI team at OffensEval-2019 applied Perspective API as a baseline system for toxicity detection, without any additional training on the contest data and obtained a very competitive result (12th out of 103 submissions, F1 of 0.79).",15,16
1224,218973961,"We think that this property, and also the fact that this is the only dataset collected from Wikipedia comments may justify why the toxicity dataset categories are mapped together in the upper part of the figure and are more difficult to compare to the categories of the other datasets.",24,25
1225,218973961,"For instance, the general categories 'toxicity' from toxkaggle ('toxkaggle-toxic') and 'aggression' from TRAC ('trac-CAG' or 'trac-OAG') do not appear close, despite the fact that both toxicity and aggression are defined as general umbrella terms for offensive, toxic or abusive online behavior.",7,8
1226,218973961,"For instance, the general categories 'toxicity' from toxkaggle ('toxkaggle-toxic') and 'aggression' from TRAC ('trac-CAG' or 'trac-OAG') do not appear close, despite the fact that both toxicity and aggression are defined as general umbrella terms for offensive, toxic or abusive online behavior.",47,48
1227,218973961,"In contrast, 'toxkaggle-toxic' and 'davidson-toxicity', which in our category standardization were assigned the label 'toxicity', appear closer in the plot.",12,13
1228,218973961,"In contrast, 'toxkaggle-toxic' and 'davidson-toxicity', which in our category standardization were assigned the label 'toxicity', appear closer in the plot.",25,26
1229,218973961,"Additionally, between these two categories, 'amievalita-sexism-misoginy' is situated, indicating that 'sexism' can be one of the main types of toxicity in those datasets.",30,31
1230,218973961,"Amievalita-misogynous' appears to be close to Davidson's 'offensive' and Toxkaggle's 'toxicity', but it is also not so far away from Waseem's 'sexism'.",18,19
1231,218973961,This may indicate that the Toxkaggle 'toxicity' category contains sexist messages that are more similar to the 'waseem-sexist' messages.,7,8
1232,218973961,"Nevertheless, it is unexpected that Waseem's 'sexism' category appears more similar to 'toxicity' than to 'amievalitamisogyny'.",17,18
1233,218973961,"unexpected, since their labels suggest that the main difference between these two categories is the intensity of the expressed toxicity.",20,21
1234,218973961,"For the general categories, 'toxicity' and 'aggression', the classifier achieves a higher F1 on the Davidson dataset than on the Toxkaggle dataset.",6,7
1235,218973961,"This means that, indeed, the 'aggression' category as used in the TRAC dataset cannot be compared and merged with the 'toxicity' category.",26,27
1236,218973961,"We saw that the proportion between offense, toxicity, abuse or hate messages can vary across different datasets, and this factor greatly impacts the classifiers' performance.",8,9
1237,218973961,"However, the categories aiming at the representation of classes that cover all types of pejorative online speech, such as toxicity and aggression, do not seem related among each other.",21,22
1238,218973961,"The second experiment, which implied the use of the Perspective API classifier, showed that even when datasets use very generic categories, their diverging definitions, data samples or inconsistent annotation may lead to diverging classifier performance -as, e.g., in the case of 'aggression' from the TRAC dataset and 'toxicity' from the Toxkaggle dataset.",56,57
1239,218973961,"For instance, it would be worth to explore why the 'waseem-racism' category is very close in the PCA plot to the TRAC 'aggression' categories, or why 'waseem-sexism' is, according to this plot, more similar to 'toxicity' than to 'misogyny', or what makes most of the studied categories rather heterogenous (in contrast to, e.g., âwaseem-racismâ).",50,51
1240,235097499,"2018 ) list toxicity without swearwords, rhetorical questions and comparisons/metaphorical language.",3,4
1241,250390730,"Han and Tsvetkov (2020) propose a classification approach for what they call veiled toxicity, an umbrella term for many different subtypes of implicit abuse.",15,16
1242,235694304,"Here a model has learned to erroneously associate African American Vernacular English (AAVE) with toxicity (Sap et al.,",16,17
1243,235694304,"Returning to toxicity detection, this might reveal that punctuation marks (such as ""!"")",2,3
1244,235694304,2019) observed that publicly available hate speech detection systems for social media tend to assign higher toxicity scores to posts written in African-American Vernacular English (AAVE).,17,18
1245,235694304,"We find that there is a strong correlation between punctuation and ""toxicity"", and other seemingly irrelevant tokens.",12,13
1246,235694304,"2017) , which we exclude from consideration (these are indicators of toxicity and so do not satisfy our definition of artifact)-surfaced by aggregating feature attribution scores are: [.,",13,14
1247,235694304,"you, and is to predict toxicity.",6,7
1248,235694304,"Verification To confirm that punctuation marks and other identified tokens indeed affect toxicity predictions, we modified tweets containing these tokens observe changes in model predictions.",12,13
1249,235422493,"2019) , toxicity detection (Han and Tsvetkov, 2020; Pavlopoulos et al.,",3,4
1250,7193856,"Again, these results were achieved without substantial toxicity.",8,9
1251,7193856,"Not surprisingly, there was a higher incidence of grade 3/4 hematological toxicity in this patient population, 20%, 17% and 15% of patients developing grade 3/4 thrombocytopenia, anemia or neutropenia, respectively.",12,13
1252,247519233,"Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially.",12,13
1253,247519233,We also demonstrate that TOXI-GEN can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset.,15,16
1254,247519233,"Importantly, such biases in toxicity detection risk further marginalizing or censoring minority groups (Yasin, 2018; Sap et al.,",5,6
1255,247519233,"While such human-like bias and toxicity poses real threats, we use this undesirable behavior in models like GPT-3 to improve existing toxic language classifiers, providing a path forward for mitigating systemic bias.",7,8
1256,247519233,"Created using demonstrationbased prompting and pretrained toxicity classifiers, TOXIGEN covers over 135k toxic and 135k benign statements about 13 minority identity groups (e.g., African Americans, women, LGBTQ+ folks, etc.).",6,7
1257,247519233,"First, it allows us to limit spurious identity-toxicity correlations (Dixon et al.,",10,11
1258,247519233,"Second, machine generation and careful prompting enables us to generate implicit toxicity (i.e., without swearwords or slurs), which is by definition hard to detect or find and thus often missing in toxic language corpora (Wiegand et al.,",12,13
1259,247519233,We use ALICE to control the toxicity of output text by pitting a toxicity classifier against a text generator during beam search decoding.,6,7
1260,247519233,We use ALICE to control the toxicity of output text by pitting a toxicity classifier against a text generator during beam search decoding.,13,14
1261,247519233,"Similarly, we can steer a language model with neutral prompting towards higher toxicity generations.",13,14
1262,247519233,Our experiments with five publicly-available toxicity classifiers show that the generated sentences in both cases above fool toxicity classifiers (see Figure 1 ).,7,8
1263,247519233,Our experiments with five publicly-available toxicity classifiers show that the generated sentences in both cases above fool toxicity classifiers (see Figure 1 ).,19,20
1264,247519233,This indicates that our data generation approaches (with or without ALICE) successfully control the generation towards the desired toxicity and minority group mention.,20,21
1265,247519233,"Further experimental results demonstrate that 2 Adversarial Language Imitation with Constrained Exemplars 4 Delphi does not produce toxicity probabilities, so we use Open AI's content filter to game Delphi.",17,18
1266,247519233,"This indicates that the dataset generated in this work and the approaches for generating data provide major steps towards improving toxicity classifiers, and could potentially be used downstream to address the issues from biased machine generation (Sheng et al.,",20,21
1267,247519233,"Implicit Hate Against Minority Groups Detecting implicit toxicity about minority groups (e.g., stereotyping, microaggressions), remains an elusive goal for NLP systems (Han and Tsvetkov, 2020; Wiegand et al.,",7,8
1268,247519233,"One key challenge is that, in contrast to explicit toxicity, implicit toxicity is not marked by the use of profanity or swearwords, is sometimes positive in sentiment, and is generally harder to detect or collect at scale (MacAvaney et al.,",10,11
1269,247519233,"One key challenge is that, in contrast to explicit toxicity, implicit toxicity is not marked by the use of profanity or swearwords, is sometimes positive in sentiment, and is generally harder to detect or collect at scale (MacAvaney et al.,",13,14
1270,247519233,"A second challenge for detecting subtle toxicity about minority groups is that minority mentions are more often the targets of social biases and toxicity (Hudson, 2017) .",6,7
1271,247519233,"A second challenge for detecting subtle toxicity about minority groups is that minority mentions are more often the targets of social biases and toxicity (Hudson, 2017) .",23,24
1272,247519233,"As such, minority mentions often co-occur with toxicity labels in datasets scraped from online platforms (Dixon et al.,",10,11
1273,247519233,"In turn, models trained on such data can exploit these spurious minority-toxicity correlations instead of considering the deeper semantics of text (Zhou et al.,",14,15
1274,247519233,"With TOXIGEN, we aim for generating a large scale dataset that represent implicit toxicity while balancing between toxic and benign statements, to address the gaps of previous work.",14,15
1275,247519233,"As shown in Table 1 , existing datasets contain large amounts of explicit toxicity.",13,14
1276,247519233,"To encourage implicit toxicity from a LLM, we find examples of human-written sentences with implicit toxicity towards each group from hate forums (de Gibert et al.,",3,4
1277,247519233,"To encourage implicit toxicity from a LLM, we find examples of human-written sentences with implicit toxicity towards each group from hate forums (de Gibert et al.,",18,19
1278,247519233,There is no guarantee that these statements will be challenging to existing toxicity detectors.,12,13
1279,247519233,2021) during decoding that generates statements that are adversarial to a given pre-trained toxicity classifier.,16,17
1280,247519233,ALICE creates an adversarial game between a pre-trained language model (PLM) and a toxicity classifier (CLF) during constrained beam search decoding.,17,18
1281,247519233,"With ALICE, we instead want to enforce soft constraints on the probabilities coming from a given toxicity classifier CLF during beam search: 5 log p(w i+1 |w 0 :i ) ∝ λ L log p LM (w i+1 |w 0:i ) + λ C log p CLF (w 0:i+1 ) (1) Here, λ L and λ C denote hyperparameters that determine the respective contribution of the language model and classifier to the decoding scoring function.",17,18
1282,247519233,"By using this weighted combination, we can steer generations towards a higher or lower probability of toxicity without sacrificing coherence enforced by the language model.",17,18
1283,247519233,"To create examples that challenge existing toxicity classifiers, we use two adversarial setups: • False negatives: We use toxic prompts to encourage the language model to generate toxic outputs, then maximize the classifier's probability of the benign class during beam search. •",6,7
1284,247519233,2021) as the toxicity classifier (CLF).,4,5
1285,247519233,"As we show in §4, the ALICEgenerated data is successful at attacking the given toxicity classifier, contributing a challenging, adversarial subset of TOXIGEN.",16,17
1286,247519233,"Specifically, we investigate the reliability of our promptbased and ALICE-based methods at generating human-like statements and controlling statements' toxicity and the minority groups mentioned ( §4.2).",24,25
1287,247519233,"To investigate the quality of our annotations, we compute agreement on toxicity ratings.",12,13
1288,247519233,"Average toxicity scores are on a 1-5 scale (1 being benign and 5 being clearly offensive), and are averaged across annotator responses.",1,2
1289,247519233,"Interestingly, there is no significant difference in toxicity when we account for whether annotators perceive scores as written by humans or AI (Figure 5 ).",8,9
1290,247519233,"We also find that the most common framing tactic is ""moral judgement"", or questioning the morality of an identity group, which has been linked to toxicity by prior work (Hoover et al.,",29,30
1291,247519233,"We then collect annotations for the 250 statements using the setup described in §4.1, and get toxicity scores from HateBERT.",18,19
1292,247519233,"human-annotated toxicity score for ALICE-decoded sentences with a toxic prompt is 2.97, compared to 3.75 for topk.",3,4
1293,247519233,"2021) models on the training portion of TOXIGEN, using the prompt labels as proxies for a true toxicity label.",19,20
1294,247519233,"The improvement on human-written datasets shows that TOXIGEN can be used to improve existing classifiers, helping them better tackle the challenging human-generated implicit toxicity detection task.",28,29
1295,247519233,"Fine-tuned HateBERT performs strongly on TOXIGEN-HUMANVAL, demonstrating that our data can successfully help guard against machinegenerated toxicity.",21,22
1296,247519233,"We proposed ALICE, an adversarial decoding scheme to evaluate robustness of toxicity classifiers and generate sentences to attack them, and showed the effectiveness of ALICE on a number of publicly-available toxicity detection systems.",12,13
1297,247519233,"We proposed ALICE, an adversarial decoding scheme to evaluate robustness of toxicity classifiers and generate sentences to attack them, and showed the effectiveness of ALICE on a number of publicly-available toxicity detection systems.",34,35
1298,247519233,"In our experiments, we showed that fine-tuning pre-trained hate classifiers on TOXIGEN can improve their performance on three popular human-generated toxicity datasets.",27,28
1299,247519233,"Still, toxicity is inherently subjective (Sap et al.,",2,3
1300,247519233,"Relationship to Policy The topic of detecting and mitigating toxicity is relevant to the ongoing work and discussions in the space of policy and legislation for AI technology (Wischmeyer and Rademacher, 2020; Reich et al.,",9,10
1301,247519233,"Carefully crafted policy and regulation can play an important role in providing oversight into the development and deployment of content moderation systems and toxicity detection algorithms in practice (Benesch, 2020; Gillespie et al.,",23,24
1302,247519233,"As we address broadly in Section 7, subjectivity is an area of concern for annotation of toxicity.",17,18
1303,247519233,"Prior work has pointed out the role that annotators' belief systems and sociodemographic backgrounds play in their perception of toxicity (Sap et al.,",20,21
1304,247519233,"I Further comparing toxicity classifiers We also compare finetuning classifiers on subsets of TOXIGEN-VAL with and without ALICE, shown in Table 7 .",3,4
1305,237700337,Input Processor The input text of a player will firstly be checked by a toxicity detection service 6 to avoid potential risks.,14,15
1306,237700337,"After the input text has passed the toxicity detection and semantic similarity detection, it will be concatenated to the context to form the input for story generation.",7,8
1307,237700337,"Thirdly, stories with inappropriate content detected by toxicity detection service will be removed.",8,9
1308,8570237,"Name N Examples Wrong parse (1) 109 exhibit asthma, ten drugs, measure headache Subtype (4) 393 headaches migraine, fungus candida, hbv carrier, giant cell, mexico city, t1 tumour, ht1 receptor Activity/Physical process (5) 59 bile delivery, virus reproduction, bile drainage, headache activity, bowel function, tb transmission Ending/reduction 8 migraine relief, headache resolution Beginning of activity 2 headache induction, headache onset Change 26 papilloma growth, headache transformation, disease development, tissue reinforcement Produces (on a genetic level) (7) 47 polyomavirus genome, actin mrna, cmv dna, protein gene Cause (1-2) (20) 116 asthma hospitalizations, aids death, automobile accident heat shock, university fatigue, food infection Cause (2-1) 18 flu virus, diarrhoea virus, influenza infection Characteristic (8) 33 receptor hypersensitivity, cell immunity, drug toxicity, gene polymorphism, drug susceptibility Physical property 9 blood pressure, artery diameter, water solubility Defect (27) 52 hormone deficiency, csf fistulas, gene mutation Physical Make Up 6 blood plasma, bile vomit Person afflicted (15) 55 aids patient, bmt children, headache group, polio survivors Demographic attributes 19 childhood migraine, infant colic, women migraineur Person/center who treats 20 headache specialist, headache center, diseases physicians, asthma nurse, children hospital Research on 11 asthma researchers, headache study, language research Attribute of clinical study (18) 77 headache parameter, attack study, headache interview, biology analyses, biology laboratory, influenza epidemiology Procedure (36) 60 tumor marker, genotype diagnosis, blood culture, brain biopsy, tissue pathology Frequency/time of (2-1) (22) 25 headache interval, attack frequency, football season, headache phase, influenza season Introduction We are exploring empirical methods of determining semantic relationships between constituents in natural language.",172,173
1309,248780294,"Controlling or guiding text generation is an active research area with important applications like toxicity control (Gehman et al.,",14,15
1310,248780294,"2021) uses a class-conditioned language model trained on text with a certain desired (or undesired) feature (e.g., toxicity) to guide generation.",24,25
1311,7987945,"However, our approach, which looks at the semantic shift of words over time, would detect a shift based on the new kinds of words that would be likely to co-occur with the toy's name, e.g. toxicity, a toy recall, or lawsuit.",42,43
1312,7987945,"A standard semantic space model would define the semantics of the new toy as a combination of all co-occurrences, in this case the positive new semantics and the negative semantics of toxicity.",34,35
1313,235417195,"As we illustrate in our experiments, political prudence cannot always be captured from the existing safety tests, which mainly focus on offensiveness or toxicity.",26,27
1314,202538032,"2017) , and general toxicity (Pavlopoulos et al.,",5,6
1315,202538032,"3 To force models to actually capture conversational dynamics rather than detecting already-existing toxicity, human annotations are used to ensure that all comments preceding a personal attack are civil.",15,16
1316,174800557,"Many measures like F1 and accuracy often mask performance on infrequent but high impact classes, such as detecting toxicity (Waseem and Hovy, 2016 )) The datasets we chose for evaluation, while all multi-class, form a diverse set in terms of the number of classes and kinds of cohesion among examples in a single class.",19,20
1317,174800557,"As expected, domain features are more important in a topical task such as 20NG (71% are F C features), while the opposite is true for Spam (19%) and a toxicity dataset like Wiki At-tack (23%).",37,38
1318,236460004,"The lack of extensive datasets for word-level toxicity detection poses an obstacle to traditional, supervised learning classification, as current stateof-the-art models are very complex and normally require large-scale data.",9,10
1319,236460004,2017) introduced the application of a GRU-based Recurrent Neural Network for toxicity detection in documents.,14,15
1320,236460004,"An alternative formulation of the Toxic Spans Detection is to treat it as a supervised learning task, which involves training the models to predict the toxicity of each word separately.",26,27
1321,236460004,"During the preprocessing stage, the inputs with toxicity higher than 0.5 were considered as toxic examples.",8,9
1322,236460004,"While xAI approaches might not be fitted to solve the problem of predicting the toxicity of each input word, they might still be useful for improving the transparency and understanding of predictions made by comment-level models.",14,15
1323,236460004,"But in terms of explanation, it might be enough for the user to obtain the few most toxic words per comment, rather than marking all of them, no matter how low the toxicity score is.",35,36
1324,235097594,"We find, in extensive experiments across hate speech detection, toxicity detection, occupation prediction, and coreference resolution tasks over various bias factors, that the effects of UBM are indeed transferable to new downstream tasks or domains via finetuning, creating less biased downstream models than directly fine-tuning on the downstream task or transferring from a vanilla upstream model.",11,12
1325,235097594,"In four tasks with known bias factors -hate speech detection, toxicity detection, occupation prediction from short bios, and coreference resolution -we explore whether upstream bias mitigation of a LM followed by downstream fine-tuning reduces bias for the downstream model.",11,12
1326,235097594,"2018) , for hate speech detection and toxicity detection datasets, we use the equal error rate (EER) threshold for prediction.",8,9
1327,235097594,"For hate speech classification, we perform transfer learning from GHC to Stormfront and from Stormfront to GHC; and for toxicity classification, we perform transfer learning from FDCL to DWMW.",21,22
1328,235097594,"Future works can develop the effectiveness of UBM beyond the default scenarios in this paper, and potentially apply it to tasks and settings beyond hate speech, toxicity classification, occupation prediction, and coreference resolution in English corpora.",28,29
1329,218517088,"2018) measured and mitigated bias in toxicity classifiers towards social groups, avoiding undesirable predictions of toxicity towards innocuous sentences containing tokens like ""gay"".",7,8
1330,218517088,"2018) measured and mitigated bias in toxicity classifiers towards social groups, avoiding undesirable predictions of toxicity towards innocuous sentences containing tokens like ""gay"".",17,18
1331,250089342,"We show that this LLM-based method can produce complex counterfactuals that existing methods cannot, comparing the performance of various counterfactual generation methods on the Civil Comments dataset and showing their value in evaluating a toxicity classifier. *",38,39
1332,250089342,"Introduction It is well known that classifiers (such as toxicity detectors) can pick up negative associations about marginalized groups from their training data, e.g. due to under-representation of those groups in the training data, or the higher levels of toxicity in the text data referring to these groups (Sap et al.,",10,11
1333,250089342,"Introduction It is well known that classifiers (such as toxicity detectors) can pick up negative associations about marginalized groups from their training data, e.g. due to under-representation of those groups in the training data, or the higher levels of toxicity in the text data referring to these groups (Sap et al.,",45,46
1334,250089342,"We suspect that as it becomes more common to use large language models (LLMs) as the base for classifier models (such as toxicity classifiers), these classifiers will become more sensitive to factors such as fluency, word order, and context, and counterfactual generation methods will need to evolve correspondingly to keep up.",25,26
1335,250089342,"Finally, we compare the performance of our method with existing counterfactual generation methods (Section 5), and show that existing methods may not capture certain subtle issues in toxicity classifiers, and that our method addresses some of these deficiencies (Section 5.3).",31,32
1336,250089342,"We use toxicity detection as a testbed in this work, and focus on generating counterfactuals to probe for false positives -that is, non-toxic text which is misclassified as toxic due to identity references.",2,3
1337,250089342,"While we focus on this particular application to demonstrate one way in which our framework can be useful, it could also be applied in other contexts: for example, probing for false negatives, applications other than toxicity detection, and counterfactual perturbations other than removing the presence of a sensitive attribute.",39,40
1338,250089342,"While this approach provides fine-grained control over identity references and toxicity balance, it also has disadvantages: for example, the resulting text is often not natural and looks quite different from the actual task data.",12,13
1339,250089342,"2021) have been effective at detailing problems with modern toxicity classifiers, by investing significant targeted effort into probing task-specific functionality, and employing human validation for generated examples.",10,11
1340,250089342,"2022) uses GPT-3 with and without an adversarial classifier-in-theloop method to generate a large set of challenging examples for toxicity detection, employing identityspecific engineered prompts.",24,25
1341,250089342,"2018) that toxicity and hate speech classifiers often pick up on correlations (that are not causations) between references to certain identities and toxic speech: that is, these models incorrectly learn that sensitive attributes such as certain sexual orientations, gender identities, races, religions, etc.",3,4
1342,250089342,are themselves indications of toxicity.,4,5
1343,250089342,"2021; Han and Tsvetkov, 2020) , finding that many datasets do not adequately represent this form of toxicity (Breitfeller et al.,",20,21
1344,250089342,"Based on this, we conjecture that toxicity classifiers may also associate indirect references to sensitive attributes with toxicity, which is consistent with (Hartvigsen et al.,",7,8
1345,250089342,"Based on this, we conjecture that toxicity classifiers may also associate indirect references to sensitive attributes with toxicity, which is consistent with (Hartvigsen et al.,",18,19
1346,250089342,"Safety Large language models come with safety and toxicity issues (Bender et al.,",8,9
1347,250089342,"2019) is a set of approximately 2 million English-language internet comments from 2015-2017, with crowdsourced toxicity annotations.",21,22
1348,250089342,These categories were chosen because they are all groups that have faced high levels of online toxicity that may have bled through into classifier models (e.g. Abid et al. (,16,17
1349,250089342,"We further require that texts have a score of at least 0.8 for the relevant attribute, and a toxicity score of at most 0.1: i.e. least 80% of the CC-I annotators agreed that the text referenced the specified attribute/identity, and at most 10% of them viewed the comment as toxic.",19,20
1350,250089342,"While the annotators were of diverse genders (male, female, non-binary) and moderately to extremely familiar with the sensitive attributes chosen for our experiments, we also note that they were all white citizens of Western countries and that this could have informed their interpretation of the toxicity task and what substitutions are ""neutral"".",52,53
1351,250089342,Toxicity detection We use our generated counterfactuals to evaluate the robustness of the Perspective API toxicity classifier to counterfactual perturbations.,15,16
1352,250089342,"5 Perspective API defines toxicity as ""a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion""; the toxicity score is the predicted probability of a reader perceiving the input as toxic.",4,5
1353,250089342,"5 Perspective API defines toxicity as ""a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion""; the toxicity score is the predicted probability of a reader perceiving the input as toxic.",27,28
1354,250089342,We focus on the change in predicted toxicity score from original to counterfactual.,7,8
1355,250089342,"This is both because any toxicity cut-off threshold will likely vary by use-case, and because we expect that large changes in score will provide interesting and useful information about the classifier even if they do not happen to straddle the toxicity threshold.",5,6
1356,250089342,"This is both because any toxicity cut-off threshold will likely vary by use-case, and because we expect that large changes in score will provide interesting and useful information about the classifier even if they do not happen to straddle the toxicity threshold.",45,46
1357,250089342,"Toxicity detection Throughout this section we restrict our attention only to the ""good"" counterfactuals (as rated by the human annotators) because poor-quality ones can produce artificially high or low swings in toxicity (due to changing the text too much relative to the  Counterfactuals generated by all methods have lower predicted toxicity scores on average than the original Islam-referencing texts, as shown in Figure 2 ; see also Figure 3 in the appendix for a more detailed breakdown.",37,38
1358,250089342,"Toxicity detection Throughout this section we restrict our attention only to the ""good"" counterfactuals (as rated by the human annotators) because poor-quality ones can produce artificially high or low swings in toxicity (due to changing the text too much relative to the  Counterfactuals generated by all methods have lower predicted toxicity scores on average than the original Islam-referencing texts, as shown in Figure 2 ; see also Figure 3 in the appendix for a more detailed breakdown.",58,59
1359,250089342,"Substitution produce the smallest change in toxicity scores: an average difference of -0.08, compared to -0.15 for LLM-D and -0.17 for ablation.",6,7
1360,250089342,We also look at the average change in toxicity score across the four topics for both LLM-D and substitution-generated counterfactuals (Table 5 ).,8,9
1361,250089342,"While the sample sizes are too small to draw concrete conclusions, the small average change in toxicity for religion-referencing substitution counter- factuals compared both to other topics and to LLM-D-generated counterfactuals reinforces the conjecture that the toxicity classifier may view all references to religion as similarly toxic.",17,18
1362,250089342,"While the sample sizes are too small to draw concrete conclusions, the small average change in toxicity for religion-referencing substitution counter- factuals compared both to other topics and to LLM-D-generated counterfactuals reinforces the conjecture that the toxicity classifier may view all references to religion as similarly toxic.",43,44
1363,250089342,Note that the average change in toxicity score is not necessarily meaningful to an end-user.,6,7
1364,250089342,"Most generally, for this investigation we focused on one way this framework can be useful, and made several narrowing choices; however, our framework can be useful in other contexts and applications such as investigating false negatives (by considering original examples that are toxic), probing other types of classifiers than toxicity models, or generating other types of counterfactuals than simply removing the sensitive attribute (e.g. rewording text to explore model robustness).",56,57
1365,250089342,"It was trained on a subset of CivilComments-Identities (all texts, regardless of toxicity, that referenced at least one attribute of interest with a score > 0.5, along with 20k negative examples that referenced none of the attributes of interest) using the AdamW optimizer (Loshchilov and Hutter, 2018) (with learning rate 0.001, weight decay 0.002) for 36k steps with a batch size 256, using a binary cross-entropy loss function to allow for multi-label predictions.",16,17
1366,250089342,"The term list was generated by fitting a unigram naive bayes classifier to the non-toxic subset of Civil Comments data (toxicity < 0.1), separating texts labeled with the given identity group (attribute score > 0.5) from a random sample of the rest.",23,24
1367,250089342,"These guidelines are designed to be flexible and easily modifiable to apply to multiple text formats (internet comments, video titles, etc) and downstream tasks (toxicity detection, sentiment analysis, etc).",29,30
1368,250089342,"We include the counterfactuals that did not pass the human rating step in order to illustrate the effects of different counterfactual generation methods on toxicity detection: for example, ablation failed mostly on the fluency criteria so its ""poor"" counterfactuals still exhibit a drop in toxicity here, whereas Polyjuice failed mostly on removing references to the sensitive attribute so its ""poor"" counterfactuals tend to cluster around the y = x line.",24,25
1369,250089342,"We include the counterfactuals that did not pass the human rating step in order to illustrate the effects of different counterfactual generation methods on toxicity detection: for example, ablation failed mostly on the fluency criteria so its ""poor"" counterfactuals still exhibit a drop in toxicity here, whereas Polyjuice failed mostly on removing references to the sensitive attribute so its ""poor"" counterfactuals tend to cluster around the y = x line.",48,49
1370,227231712,"Then, for offensive tweets, sub-task B requires determining whether the toxicity is targeted.",14,15
1371,227231712,"The online offensive content can vary in different aspects, such as the toxicity type, the target and whether the abuse is implicit or explicit.",13,14
1372,227231712,"These researches addressed different aspects of offensive language such as detecting hate speech (Malmasi and Zampieri, 2017) and recognizing the multiple types of toxicity in a comment (Ibrahim et al.,",26,27
1373,227231712,"For toxicity types classification, data augmentation significantly improved the classifier performance on minority classes as shown in (Ibrahim et al.,",1,2
1374,243865567,"By implementing RDI in the context of toxicity detection, we find that accounting for secondary attributes can significantly improve robustness, with improvements in sliced accuracy on the original dataset up to 7% compared to existing robustness methods.",7,8
1375,243865567,"As a concrete running example in this paper, we will consider the task of toxicity detection: using a model to predict if a comment is toxic or not (Dixon et al.,",15,16
1376,243865567,"To return to our example of toxicity detection, a model may learn that certain identity tokens are correlated with toxicity, but that could decrease accuracy for non-toxic comments with those terms (Dixon et al.,",6,7
1377,243865567,"To return to our example of toxicity detection, a model may learn that certain identity tokens are correlated with toxicity, but that could decrease accuracy for non-toxic comments with those terms (Dixon et al.,",20,21
1378,243865567,"Returning to our example task of toxicity classification over comments, the primary attributes (e.g., demographic identity terms) may be correlated with secondary attributes (intent of the comment-directed or descriptive) in the training data distribution.",6,7
1379,243865567,"That is, for some demographic groups we may observe more directed comments and for others we may observe more directed comments: Toxic: Seeking transgender rights is extreme (Directed) Non-Toxic: Transgender rights activists are labeled extremists (Descriptive) In the toxicity classification example shown above, while the former is labeled as toxic by human annotators as it is directed towards a demographic group, the latter is only describing the toxicity and is considered as non-toxic.",48,49
1380,243865567,"That is, for some demographic groups we may observe more directed comments and for others we may observe more directed comments: Toxic: Seeking transgender rights is extreme (Directed) Non-Toxic: Transgender rights activists are labeled extremists (Descriptive) In the toxicity classification example shown above, while the former is labeled as toxic by human annotators as it is directed towards a demographic group, the latter is only describing the toxicity and is considered as non-toxic.",79,80
1381,243865567,"So, to remove spurious correlations for the word ""transgender"" with toxicity, it may not be enough to improve model accuracy over comments with the word ""transgender"" if the model is more accurate for directed comments than descriptive ones.",13,14
1382,243865567,"Specifically, we have focused on the toxicity detection model which prior work has shown to suffer from unintended bias (Dixon et al.,",7,8
1383,243865567,Our key contributions are: • We demonstrate how to disentangle the impact of protected and secondary attributes in NLP tasks like toxicity detection. •,22,23
1384,243865567,"Empirically, we demonstrate that our RDI method improves overall accuracy and sliced accuracy by 2-7% on all identity groups for both the toxicity detection task and generalizes on the coreference resolution task, while reducing spurious correlations through secondary attributes.",26,27
1385,243865567,"2018; Field and Tsvetkov, 2020) to identify secondary variables; and in the domain of toxicity detection, we draw on qualitative error analysis (van Aken et al.,",18,19
1386,243865567,2018) to derive our understanding of the secondary variable (intent of the comment) and how it relates to the label (toxicity); see Appendix 1.,24,25
1387,243865567,"Given an NLP classification task that operates on individual sentences s ∈ D, consider a primary variable X, which could be one of group based identities (say race, gender, etc) that is spuriously correlated with a secondary variable Z (e.g., intent of the comment-directed or descriptive) and the label Y that is to be predicted (say toxicity).",68,69
1388,243865567,"In our setting, the values (x, z ) of the primary and secondary variables X, Z are contained within an individual sentence s. We use the intent of the comment as our running example for Z in the toxicity detection task, but our approach can be easily generalized to other factors like dialect, in-group language, figure of speech, etc.",42,43
1389,243865567,"Note that the dataset D represents a less biased dataset, one which might not actually be observed, but represents all possible values of the primary and secondary variables X, Z in D, and allows us to measure the toxicity detection model's counterfactual robustness around both the primary and secondary attributes.",42,43
1390,243865567,"For example in the Jigsaw toxicity detection dataset, consider when Y is denoting ""toxicity"", X represents gender and Z the intent of the comment -descriptive or directed.",5,6
1391,243865567,"For example in the Jigsaw toxicity detection dataset, consider when Y is denoting ""toxicity"", X represents gender and Z the intent of the comment -descriptive or directed.",15,16
1392,243865567,"If, for example, we observe in the real world that most directed comments are towards women, and not men (spurious correlation between X and Z), then just intervening on the gender X of the sentence and changing it from female to male, might unintentionally remove the impact of the secondary variable -the intent of the sentence, on the toxicity detection task Y .",66,67
1393,243865567,"Proposed Constraints We overcome the limitation of not including secondary variable impact in baseline constraints, by explicitly modeling to Maximize Secondary Sensitivity in tasks like toxicity detection, where the label Y is sensitive to changing values of the secondary variable Z in the counterfactual dataset.",26,27
1394,243865567,"For example in the Jigsaw toxicity (Y ) dataset, even though more directed comments on online forums are towards females, and more descriptive comments are used for males, the model should be sensitive to the intent of comment in determining the toxicity.",5,6
1395,243865567,"For example in the Jigsaw toxicity (Y ) dataset, even though more directed comments on online forums are towards females, and more descriptive comments are used for males, the model should be sensitive to the intent of comment in determining the toxicity.",45,46
1396,243865567,"Algorithm 1 RDI (Reweight-Direct-Indirect) Reweight samples based on (8) 5: L = E s∼ D CrossEnt( Ŷs , Y s ) 6: L RDI ← (6) + ( 7 ) 7: Back-propagate αL + (1 − α)L RDI in M 8: end for 5 Evaluation Data The Jigsaw Kaggle toxicity dataset 1 contains sentences from the Civil Comment platform.",66,67
1397,243865567,"In total, 1,804,874 comments are annotated for toxicity, out of which ∼50% of them have identities annotated too.",8,9
1398,243865567,"We extend this framework to incorporate templates for intent of the comment -directed and descriptive based on the definition of toxicity provided in (Waseem and Hovy, 2016) .",20,21
1399,243865567,"Since we are comparing sliced accuracy across 9 identity groups in the toxicity dataset, we also compute the standard error bars in the measurement of each metric.",12,13
1400,243865567,"In Figure 2 , we show the impact on the AUC of identity groups as identified in the original Jigsaw toxicity dataset.",20,21
1401,243865567,2020) We have demonstrated the utility of modeling secondary attributes to improve robustness of the toxicity detection models.,16,17
1402,243865567,"Broader Impact Statement As we are dealing with the toxicity detection task, the concern of dual use for generating more toxic content on social media has to be considered.",9,10
1403,243865567,"More so, when these same members describe the toxicity they experience on those social online forums, the possibility of them being flagged as toxic, can be harmful.",9,10
1404,218487111,"She tells me 2 weeks after that visit, she developed toxicity from the tamoxifen and therefore stopped it herself.",11,12
1405,218487111,"perhaps multiple times, and had toxicity problems with all of them.",6,7
1406,236460193,"However, their purpose is toxicity detection of the whole text, so they do not contain information about the exact spans that make a text toxic.",5,6
1407,236460193,The goal of our re-annotation was to evaluate the quality of the original datasets annotation and check if our understanding of toxicity is equivalent to the one of contest organizers.,23,24
1408,247158838,Google Perspective API 2 is used for toxicity evaluation.,7,8
1409,247158838,"We further filter out the prompts with toxicity larger than 0.5, scored by Perspective.",7,8
1410,247158838,Our unsupervised method significantly lowers the toxicity on the detoxification task and the ablation study shows that the contrastive loss L c is crucial.,6,7
1411,247158838,"When applied to detoxification, although the probability of toxicity degeneration will decrease, the controlled language model may still produce unsafe text.",9,10
1412,247158838,"Besides, our proposed methods control the highlevel attributes of the generation, such as toxicity, topic, or sentiment, but there is no guarantee of factual accuracy for the generation, which is a wellknown problem in NLG models.",15,16
1413,235097287,"The Multilingual Toxic Comment Classification Kaggle challenge (Jigsaw, 2019 ) included a multilingual test set of Wikipedia talk page comments annotated for toxicity.",24,25
1414,235097342,"These ad hominem fallacies are related to abusive language, toxicity, and microaggressions, and can be expressed with both subtle and explicitly offensive language.",10,11
1415,235097342,"2020) use PPLM in the contexts of topic, sentiment, and toxicity control.",13,14
1416,243865181,"Note that AESOP might be used for malicious purposes because it does not have a filtering mechanism that checks the toxicity, bias, or offensiveness of source sentences from the input.",20,21
1417,234337004,"Given recent development of inference-time methods for control that can reduce toxicity (e.g., PPLM (Dathathri et al.,",13,14
1418,226221869,"Specifically, we fine-tune BERT on the datasets with ground-truth sentiment and toxicity labels.",16,17
1419,226221869,We see higher occurrences of toxicity because GPT-2 is fine-tuned on the equal numbers of toxic and non-toxic sentences from the Toxic dataset (whereas PPLM compares against a nonfine-tuned version of GPT-2) and because GPT-2 has a pre-existing unjust bias toward these words.,5,6
1420,80628357,"2016) , gender (Reddy and Knight, 2016) or toxicity (Hosseini et al.,",12,13
1421,203447304,"In this paper, we explore various aspects of sentiment detection and their correlation to toxicity, and use our results to implement a toxicity detection tool.",15,16
1422,203447304,"In this paper, we explore various aspects of sentiment detection and their correlation to toxicity, and use our results to implement a toxicity detection tool.",24,25
1423,203447304,"We then test how adding the sentiment information helps detect toxicity in three different real-world datasets, and incorporate subversion to these datasets to simulate a user trying to circumvent the system.",10,11
1424,203447304,Our results show sentiment information has a positive impact on toxicity detection.,10,11
1425,203447304,"These attempts to bypass the toxicity detection system are called subverting the system, and toxic users doing it are referred to as subversive users.",5,6
1426,203447304,"Consequently, we will study the correlation between sentiment and toxicity and its usefulness for toxic message detection both in subversive and non-subversive contexts.",10,11
1427,203447304,"It is important to note that toxicity is a very abstract term that can have different definitions depending on context, and each dataset described in Section 4 has its own.",6,7
1428,203447304,We will study the measure of toxicity and its correlation to message sentiment in Section 4.,6,7
1429,203447304,"Related Work Given the limitations of human and keywordbased toxicity detection systems mentioned previously, several authors have studied alternative means of detecting toxicity.",9,10
1430,203447304,"Related Work Given the limitations of human and keywordbased toxicity detection systems mentioned previously, several authors have studied alternative means of detecting toxicity.",23,24
1431,203447304,Hate speech is not the only form of toxicity that has been studied.,8,9
1432,203447304,"However, an in-depth analysis of how sentiment can benefit toxicity detection has not been done in any of these papers, and a study of the use of sentiment in a subversive context has never been done.",12,13
1433,203447304,"This bias for positivity is an issue for a study on toxicity, which we expect to be expressed using negative sentiments.",11,12
1434,203447304,Toxicity Detection The main contribution of this paper is to study how sentiment can be used to detect toxicity in subversive online comments.,18,19
1435,203447304,"This toxicity detection tool, which was used in previous research on toxicity as well (Mohan et al.,",1,2
1436,203447304,"This toxicity detection tool, which was used in previous research on toxicity as well (Mohan et al.,",12,13
1437,203447304,We use the ratio of toxic marks as a toxicity score.,9,10
1438,203447304,"For example, if a message is marked toxic by 7 out of 10 workers, it will have a 0.7 toxicity score. •",21,22
1439,203447304,The Kaggle toxicity competition 16 dataset is also taken from discussions on English Wikipedia talk pages.,2,3
1440,203447304,"This allows us to rate comments on a seven-level toxicity scale, from 0/6 labels marked to 6/6 labels marked.",11,12
1441,203447304,"Correlation Our first experiment consists in computing the sentiment of each message in each of our three test corpora, and verifying how they correlate with the different toxicity scores of each of the corpora.",28,29
1442,203447304,"It can be seen that there is a clear negative correlation between toxicity and sentiment in the messages, as expected.",12,13
1443,203447304,"For example, mentioning sexual body parts will be labeled as toxicity level 5 even if they are used in a positive message, because they carry more potential risk.",11,12
1444,203447304,The toxicity detector we implemented in this experiment is a deep neural network inspired by the most successful systems in the Kaggle toxicity competition we used as a dataset.,1,2
1445,203447304,The toxicity detector we implemented in this experiment is a deep neural network inspired by the most successful systems in the Kaggle toxicity competition we used as a dataset.,22,23
1446,203447304,"We chose this binary approach to allow the network to learn to recognize toxicity, as opposed to types of toxic messages on Kaggle, keyword severity on Reddit, or a particular worker's opinions on Wikipedia.",13,14
1447,203447304,Our experiment consists in comparing the toxicity detection accuracy of our network when excluding or including sentiment information and in the presence of subversion.,6,7
1448,203447304,"Indeed, as mentioned in Sections 1 and 2, it is trivial for a subversive user to mask toxic keywords to bypass toxicity filters.",23,24
1449,203447304,It can be seen that sentiment information helps improve toxicity detection in a statisticallysignificant manner in all cases but one.,9,10
1450,203447304,"However, the introduction of subversion leads to an important drop in the accuracy of toxicity detection for the network that uses the text alone.",15,16
1451,203447304,"With subversion, including sentiment information improves the accuracy of toxicity detection by more than 0.5% in all experiments, and as much as 3% on the Kaggle dataset, along with a decrease in p-value in all cases.",10,11
1452,203447304,"Comparing the different corpora, it can be seen that the improvement is smallest and least significant in the Reddit dataset experiment, which was to be expected since it is also the dataset in which toxicity and sentiment had the weakest correlation in Table 5 .",36,37
1453,203447304,"We can note that our toxicity detection neural network performs very well nonetheless in all cases, even with subversion and without sentiment information.",5,6
1454,203447304,"Conclusion In this paper, we explored the relationship between sentiment and toxicity in social network messages.",12,13
1455,203447304,This tool allowed us to demonstrate that there exists a clear correlation between sentiment and toxicity.,15,16
1456,203447304,"Next, we added sentiment information to a toxicity detection neural network, and demonstrated that it does improve detection accuracy.",8,9
1457,203447304,"Finally, we simulated a subversive user who circumvents the toxicity filter by masking toxic keywords in their messages, and found that using sentiment information improved toxicity detection by as much as 3%.",10,11
1458,203447304,"Finally, we simulated a subversive user who circumvents the toxicity filter by masking toxic keywords in their messages, and found that using sentiment information improved toxicity detection by as much as 3%.",27,28
1459,203447304,Our work so far has focused on single-line messages and negative toxicity detection.,13,14
1460,203447304,"There are however several different types of toxicity, some of which correlate to different sentiments.",7,8
1461,203447304,Differentiating between these types of toxicity will strengthen the correlation to message sentiment and further improve our results.,5,6
1462,237513633,"On a different line of research, some work introduces conditional generation to reduce toxicity (Dathathri et al.,",14,15
1463,250391026,"2019) , toxicity 10 , verbal aggression (Kumar et al.,",3,4
1464,236486214,"2019) , toxicity 3 and verbal aggression (Kumar et al.,",3,4
1465,85531331,"We define the toxicity level (TL) of a comment x for a class y ∈ {1, . . . ,",3,4
1466,85531331,"We report the averaged sum over the six toxicity classes, e.g., 4.00 is equal to a positive example in four classes.",8,9
1467,248811036,This narrow focus on toxicity may inadvertently crowd out other F&I issues.,4,5
1468,248811036,"These include better frameworks of harms (to help anticipate issues beyond toxicity), appropriate datasets, additional frameworks to guide (and scale) qualitative evaluations, task-specific metrics to combat automatic metric reuse without proper validation, and approaches for measuring latent qualities such as voice, style, and topic (which remain longstanding open questions).",12,13
1469,232257929,"By building a dictionary of toxic words from the training data and by taking into account their frequency and ratio of toxicity, we come up with a simple model that performs as closely as about 2 percent difference in performance to the deep model's result.",21,22
1470,232257929,"If the word is found and its frequency as well as its toxicity ratio in the training set are higher than certain values, it is labeled as toxic.",12,13
1471,232257929,This ratio which we call toxicity ratio (defined below) along with the term frequency are the only parameters of the Bag-of-Words model.,5,6
1472,232257929,toxicity ratio = labeled as toxic frequency total frequency The test dataset also contains words that are bleeped.,0,1
1473,232257929,This can be achieved by specifying a high toxicity ratio for a word to be labeled as toxic.,8,9
1474,232257929,One parameter represents the minimum frequency with which a toxic word appears in the resized training set and the other one is its minimum toxicity ratio in the resized training data.,24,25
1475,232257929,"Since a larger ratio can be a sign of more toxicity, we choose 0.4 as the ratio and a frequency of 20 as the thresholds with which we apply the model on the test set.",10,11
1476,232257929,"In the first version of the Bag-of-Words model, which was found during our primary experiments, the minimum word frequency is 40 and the minimum toxicity ratio is 0.7.",30,31
1477,232257929,The frequency and toxicity ratio of these words can be seen in Table 4 .,3,4
1478,232257929,"The reason for this behavior can be attributed to the fact that models with higher thresholds both in terms of frequency and toxicity ratio tend to output more certain results, albeit fewer words than the ones that should be labeled as toxic.",22,23
1479,232257929,"However, when these results are combined with the output of the CharacterBERT, we see that the higher the toxicity ratio the better the results (Figure 4 ) until 0.7 which gives the maximum im-provement.",20,21
1480,232257929,The Bag-of-Words model labels the words based on their frequency and the ratio of toxicity in the training data.,18,19
1481,243865204,"H 2 O H 2 O 2 C 2 H 6 O Query Over the past several years, chemists have begun to rely increasingly on computational techniques for cataloging molecules and predicting chemical reactions, products, and properties, such as yield, toxicity, and water solubility (Wu et al.,",45,46
1482,249282716,"In this paper, we decompose the internal mechanisms of debiasing language models with respect to gender by applying causal mediation analysis to understand the influence of debiasing methods on toxicity detection as a downstream task.",30,31
1483,249282716,"2020) , to scrutinize the internal mechanisms of mitigating gender debiasing methods and their effects on toxicity analysis as a downstream task.",17,18
1484,249282716,"Specifically, they fine-tuned the model on toxicity detection datasets (Jigsaw and RtGender), and showed that the model learned biases to some extent.",9,10
1485,249282716,"2020) , we use toxicity detection as a downstream task since it has been shown to correlate with gender biases (Dixon et al.,",5,6
1486,249282716,Jigsaw Toxicity Detection The toxicity detection task basically means to distinguish whether the given comment is toxic or not.,4,5
1487,249282716,"The next three columns show the stereotype scores of the BERT and GPT2 fine-tuned for our downstream task (toxicity detection), and applied to the Jigsaw and RtGender corpora, respectively.",21,22
1488,249282716,"2020) suggesting that the BERT model fine-tuned on Jigsaw toxicity and RtGender, especially the latter, show an increase in direct gender bias measures compared to the baseline models.",12,13
1489,249282716,"Accuracy Table 3 shows the accuracy scores of the models on downstream task, toxicity detection.",14,15
1490,249282716,"We only tested these effects on one downstream task, namely, toxicity detection.",12,13
1491,248986638,"2021b; Smith and Williams, 2021) and decrease risk of toxicity (Ouyang et al.,",12,13
1492,235352965,"In this paper, we focus on the differences in the ways men and women annotate comments for toxicity, investigating how these differences result in models that amplify the opinions of male annotators.",18,19
1493,235352965,"As toxicity is such a subjective measure, its definition can vary significantly between different domains and annotators, leading to many contrasting approaches to toxicity detection such as evaluating the constructiveness of comments (Kolhatkar et al.,",1,2
1494,235352965,"As toxicity is such a subjective measure, its definition can vary significantly between different domains and annotators, leading to many contrasting approaches to toxicity detection such as evaluating the constructiveness of comments (Kolhatkar et al.,",25,26
1495,235352965,We investigate how the annotators' demographics affect the toxicity scores/labels and the trained models.,9,10
1496,235352965,"We then tailor the state-of-the-art BERT model to the tasks of toxicity and gender classification, using training and test sets built independently using the annotations of different genders to investigate bias.",17,18
1497,235352965,This leads to toxicity classifiers that are overly reliant on the opinions of annotators perceived to be male in order to make a classification.,3,4
1498,235352965,They reported differences in average toxicity scores and inter-annotator agreement between the groups.,5,6
1499,235352965,"2019) in the field of racial bias examined toxicity scores given to Twitter corpora, where the white annotators in the majority give higher toxicity scores to tweets exhibiting an African American English dialect, demonstrating how annotator opinions can propagate bias throughout the model.",9,10
1500,235352965,"2019) in the field of racial bias examined toxicity scores given to Twitter corpora, where the white annotators in the majority give higher toxicity scores to tweets exhibiting an African American English dialect, demonstrating how annotator opinions can propagate bias throughout the model.",25,26
1501,235352965,"2017) , uses disaggregated data and transforms the problem from the binary classification of toxicity to the prediction of the proportion of annotators who would classify a comment as toxic.",15,16
1502,235352965,"Data We use the toxicity corpus 1 from the Wikipedia Detox project (Wulczyn et al.,",4,5
1503,235352965,"2017) , which contains over 160k comments from English Wikipedia annotated with toxicity scores and the demographic information of the annotators, where each comment has been labelled by approximately 10 annotators using the toxicity categories displayed in Table 1 .",13,14
1504,235352965,"2017) , which contains over 160k comments from English Wikipedia annotated with toxicity scores and the demographic information of the annotators, where each comment has been labelled by approximately 10 annotators using the toxicity categories displayed in Table 1 .",35,36
1505,235352965,"This is due to the fact that male annotators were found to have a greater inter-annotator agreement than female annotators, meaning that they are likely to hold the majority opinion, and so it follows that the model will place a greater importance on the scores of male annotators when deciding the toxicity of a comment.",55,56
1506,235352965,"After reviewing the toxicity scores given by each group as a whole, we find that female annotators on average annotated 1.72% more comments as toxic than male annotators and assigned toxicity scores that were on average 0.048 lower than those given by their male counterparts, using the toxicity scores given in Table 1 .",3,4
1507,235352965,"After reviewing the toxicity scores given by each group as a whole, we find that female annotators on average annotated 1.72% more comments as toxic than male annotators and assigned toxicity scores that were on average 0.048 lower than those given by their male counterparts, using the toxicity scores given in Table 1 .",32,33
1508,235352965,"After reviewing the toxicity scores given by each group as a whole, we find that female annotators on average annotated 1.72% more comments as toxic than male annotators and assigned toxicity scores that were on average 0.048 lower than those given by their male counterparts, using the toxicity scores given in Table 1 .",50,51
1509,235352965,"Pre-processing While the different models built for this paper focus on two different tasks, namely toxicity and gender classification, the pre-processing steps remain largely the same.",18,19
1510,235352965,The dataset is then balanced by gender as previously described as well as being balanced by the toxicity score in a similar manner.,17,18
1511,235352965,"For gender classification, as only toxic data is used for training and testing, this means sampling the data evenly from comments given a toxicity score of -1 and those given a toxicity score of -2.",25,26
1512,235352965,"For gender classification, as only toxic data is used for training and testing, this means sampling the data evenly from comments given a toxicity score of -1 and those given a toxicity score of -2.",33,34
1513,235352965,"Similarly, the toxicity classification models take 25% of their data from the comments annotated as 'Toxic' and a further 25% from the 'Very Toxic' data, with the remaining 50% being randomly sampled from the 'Healthy' and 'Very Healthy' data.",3,4
1514,235352965,"Toxicity Classification To further explore the differences between male and female annotators, we adapt the BERT model to perform toxicity classification rather than gender classification.",20,21
1515,235352965,We test each of the models using test data of the same condition as well as the test data from all other toxicity classification models.,22,23
1516,235352965,This information could be leveraged by moderation systems by taking into account the demographic group the reader of a comment belongs to before determining the toxicity threshold at which a comment is removed from the system.,25,26
1517,235352965,"This bias indicates that toxicity models trained on this corpus will be more influenced by the opinions of male annotators, as the diversity of views given by the female annotators makes them unlikely to hold the majority opinion, and those who label comments containing offensive words as toxic are perceived to be male by the model.",4,5
1518,235352965,"Applying the discovered associations between gender and offensive language to models tasked with classifying the toxicity of comments, we find that toxic comments annotated by men are easier to classify than those annotated by women.",15,16
1519,233189538,"However, a major limitation of their data is that the labels are generated automatically using an existing toxicity classifier.",18,19
1520,233189538,8 This implies that their labels would not be accurate for comments where the original toxicity classifier it-self fails.,15,16
1521,233189538,"2018) find that their toxicity classifier fails on data where there were instances of sarcasm, toxicity without employing swear words, and rhetorical questions.",5,6
1522,233189538,"2018) find that their toxicity classifier fails on data where there were instances of sarcasm, toxicity without employing swear words, and rhetorical questions.",17,18
1523,233210607,Detecting which parts of a sentence contribute to that sentence's toxicity-rather than providing a sentence-level verdict of hatefulnesswould increase the interpretability of models and allow human moderators to better understand the outputs of the system.,11,12
1524,233210607,"We will begin with our main dataset in which span-level toxicity has been labeled (3.1), next we look at other datasets that were used to better train our models, namely the hate word list that was used (3.2.1) and the sentence-level hate speech data (3.2.2).",12,13
1525,250390750,"Due to its subtlety and the good intentions behind its use, the audience is not aware of the language's toxicity.",21,22
1526,233209981,"Organizers of the shared task provided participants with the trial, train, and test sets of English social media comments annotated at the span level indicating the presence or absence of text toxicity.",33,34
1527,233209981,Related Work Computational approaches to tackle text toxicity have recently gained a lot of interest due to the widespread use of social media.,7,8
1528,233209981,"Since moderation is crucial to promoting healthy online discussions, research on toxicity detection has been attracting much attention.",12,13
1529,233209981,"The goal of the task is to define a sequence of words (character offsets) that attribute to the toxicity of the text, for example: • Input. """,20,21
1530,233209981,"First step setup and results: • select subset of Jigsaw toxic classification data: all the targets with toxicity score ≥ 0.5 (L = 135168 objects) as class 1 and randomly sampled 3 * L objects with toxicity score < 0.5 as class 0; • stratified 80% train, 20% validation; • 0.968 AUC bert-base, 0.968 AUC bert-large, 0.942 AUC dehate-bert.",19,20
1531,233209981,"First step setup and results: • select subset of Jigsaw toxic classification data: all the targets with toxicity score ≥ 0.5 (L = 135168 objects) as class 1 and randomly sampled 3 * L objects with toxicity score < 0.5 as class 0; • stratified 80% train, 20% validation; • 0.968 AUC bert-base, 0.968 AUC bert-large, 0.942 AUC dehate-bert.",40,41
1532,233209981,"Hence, it was difficult to attribute the toxicity of those posts to particular spans.",8,9
1533,212717954,"From this massive corpus, they sampled 3 smaller datasets that they annotated for different types of abuse: • personal attack: abusive content directed at somebody's person rather than providing evidence; • aggression: malicious remark to a person or group on characteristics such as religion, nationality or gender; • toxicity: comment that can make other people want to leave the conversation.",56,57
1534,212717954,"Talk page 165 Based on machine learning models, this API scores messages for several types of abuse, e.g., toxicity, profanity, threat, insult.",21,22
1535,212717954,"In WikiConv, all messages are scored on their toxicity and severe toxicity.",9,10
1536,212717954,"In WikiConv, all messages are scored on their toxicity and severe toxicity.",12,13
1537,212717954,PreTox is composed of complete discussion threads with semi-automatically generated toxicity annotations.,12,13
1538,212717954,A binary toxicity annotation is created for each message using this heuristic.,2,3
1539,212717954,"The Aggression and Toxicity datasets also provides such a general binary score (aggression and toxicity, respectively).",15,16
1540,212717954,Each message in WikiConv is provided with two scores computed through the Google Perspective API: a toxicity and a severe_toxicity.,17,18
1541,212717954,A message is considered toxic if its toxicity score is above 0.64 and severely toxic if its severe_toxicity score is above 0.92.,7,8
1542,212717954,"Unsurprisingly, the best F -measure for the toxicity score is obtained with the Toxicity dataset.",8,9
1543,212717954,"Based on this observation, we can hypothesize that the method used to generate the toxicity and severe_toxicity scores may not really distinguish between Personal attack, Aggression and Toxicity, and relies on a more general definition of abuse.",15,16
1544,212717954,"The severe_toxicity score yields a higher Recall than the toxicity one for all 3 abuse types, but the Precision is only around 54%.",9,10
1545,236459853,Highlighting these toxic spans can help human moderators to interpret and identify easily this type of content on the Web instead of relying on a system that generates a score of unexplained toxicity per post.,32,33
1546,236459853,A toxic span is defined as a sequence of characters in words that attribute to the text's toxicity.,18,19
1547,244055107,"Finally, the DETOXIS task 5 recently introduced the first dataset of comments in response to news articles labeled at different toxicity levels.",21,22
1548,245130931,"Other examples include toxicity (Sap et al.,",3,4
1549,245130931,"2021) , for example, collect toxicity labels from 641 annotators, but only for 15 examples.",7,8
1550,245130931,2021) annotate toxicity.,3,4
1551,245130931,2021) annotate toxicity and types of toxicity.,3,4
1552,245130931,2021) annotate toxicity and types of toxicity.,7,8
1553,236459940,"Although many datasets and models focusing on toxicity detection have been released, most of them classify entire sequences of text, and do not highlight the individual words that make a text toxic.",7,8
1554,236459940,"Highlighting such spans can provide more information to human moderators in the form of attribution, instead of an unexplained toxicity score per post, and is thus a crucial step towards successful semi-automated moderation.",20,21
1555,236459940,A toxic span is defined as a sequence of words that contributes to a text's toxicity.,16,17
1556,236459940,"Such task-specific data may include count-based information, which has been shown to improve the performance of pre-trained models in sequence classification tasks (Lim and Madabushi, 2020; Prakash and Madabushi, 2020) , or domain-specific knowledge, such as information pertaining to word toxicity, which has been shown to be one of the most predictive features of offensive commentary (Noever, 2018) .",55,56
1557,236459940,Domain-specific information has been shown to be an effective measure of toxicity.,13,14
1558,236459940,"Noever (2018) evaluated the relative predictive value of 28 features of syntax, sentiment, emotion, and outlier word dictionaries for online toxicity detection.",25,26
1559,236459940,"Submissions to past toxicity detection tasks at SemEval, such as OffensEval and OffensEval-2020, highlight how effective BERT can be for toxicity detection.",3,4
1560,236459940,"Submissions to past toxicity detection tasks at SemEval, such as OffensEval and OffensEval-2020, highlight how effective BERT can be for toxicity detection.",22,23
1561,236459940,The success of these models in toxicity detection tasks led us to choose to use a BERT-based model for this work.,6,7
1562,236459940,"This word importance could contribute to the identification of a text's toxicity, as shown by Lim and Madabushi (2020); Prakash and Madabushi (2020) .",12,13
1563,236459940,"One of the most straightforward approaches for toxicity detection is to use a word list, whereby the toxicity of a sequence is determined by comparing the words it contains against a list of known toxic words.",7,8
1564,236459940,"One of the most straightforward approaches for toxicity detection is to use a word list, whereby the toxicity of a sequence is determined by comparing the words it contains against a list of known toxic words.",18,19
1565,236459940,"Such domain-specific information has been shown to be effective for toxicity detection (Noever, 2018; Pedersen, 2019) .",12,13
1566,236486115,"This could be partially motivated by the fact that the Misogyny Classifier could have generalized a stereotyped conception of reality from skewed data on Misogyny Detection, e.g. learning to associate a high degree of toxicity with neutral posts containing terms such as feminist or negative correlation about women in positions of responsibility, since we can hypothesise that most of the examples the system was trained on contained references to these identities in offensive context.",35,36
1567,232428158,"Jigsaw Multilingual (Jigsaw Multilingual, 2020) aims to improve toxicity detection by addressing the shortcomings of the monolingual setup.",11,12
1568,248780370,"Offensive content is therefore an umbrella term encompassing toxicity, hate speech, and abusive language (Fortuna et al.,",8,9
1569,248780370,4 This API provides an output from 0 to 1 corresponding to the toxicity of the input text.,13,14
1570,248780370,"Indeed, recent work has shown that popular toxicity detection and mitigation methods themselves -including ones used in this work -are biased (Röttger et al.,",8,9
1571,248780370,"2019) show that widely used hate-speech datasets contain correlations between surface markers of African American English and toxicity, and that models trained on these datasets may label tweets by self-identified African Americans as offensive up to two times more often than others.",20,21
1572,248780370,"Several recent works have shown bias in popular toxicity detection and mitigation methods (Sap et al.,",8,9
1573,248780370,B.1 PERSPECTIVE API Details The PERSPECTIVE API is a popular tool for toxicity detection created by Jigsaw and Google's Counter Abuse Technology team.,12,13
1574,248780370,"The developers define toxicity as define toxicity ""a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion.""",3,4
1575,248780370,"The developers define toxicity as define toxicity ""a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion.""",6,7
1576,248780370,"As noted in the main body of this paper, the API provides an output from 0 to 1 corresponding to the toxicity of the input text, and following previous work, we label an input text as toxic if the API produces a score ≥ 0.5.",22,23
1577,216868486,"2019; Fortuna and Nunes, 2018; Ranasinghe and Zampieri, 2020) , covering overlapping characteristics such as toxicity, hate speech, cyberbullying, and cyber-aggression.",20,21
1578,248780192,"Specifically, we would like to investigate whether the classifiers contain unintended biases, e.g. towards specific sexual orientations, according to well-known metrics proposed to detect unfairness within toxicity detection (Borkan et al.,",31,32
1579,53081574,"Second, we investigate the toxicity of deleted comments, and show that community moderation of undesired behavior takes place at a much higher rate than previously estimated.",5,6
1580,53081574,"We used the Perspective API 7 to score the toxicity of all addition and creation actions (which we refer to as ""comments"" here).",9,10
1581,53644609,"The labels of toxicity include 'toxic', 'severe toxic', 'obscene', 'threat', 'insult' and 'identity hate'.",3,4
1582,247519021,2020) proposed a multi-task learning model for predicting the presence of identity terms alongside the toxicity of a sentence.,18,19
1583,21697648,"2017) or, more generally, toxicity (Chandrasekharan et al.,",7,8
1584,21697648,"Several studies have sought to develop machine learning techniques to detect signatures of online toxicity, such as personal insults (Yin et al.,",14,15
1585,21697648,"Annotators Conversations Snippets To select candidate conversations to include in our collection, we use the toxicity classifier provided by the Perspective API, 5 which is trained on Wikipedia talk page comments that have been annotated by crowdworkers (Wulczyn et al.,",16,17
1586,21697648,"This provides a toxicity score t for all comments in our dataset, which we use to preselect two sets of conversations: (a) candidate conversations that are civil throughout, i.e., conversations in which all comments (including the initial exchange) are not labeled as toxic (t < 0.4); and (b) candidate conversations that turn toxic after the first (civil) exchange, i.e., conversations in which the N -th comment (N > 2) is labeled toxic (t ≥ 0.6), but all the preceding comments are not (t < 0.4).",3,4
1587,21697648,"We take particular care to not over-constrain crowdworker interpretations of what personal attacks may be, and to separate toxicity from civil disagreement, which is recognized as a key aspect of effective collaborations (Coser, 1956; De Dreu and Weingart, 2003) .",21,22
1588,21697648,"Trained toxicity: We also compare with the toxicity score of the exchange from the Perspective API classifier-a perhaps unfair reference point, since this supervised system was trained on additional human-labeled training examples from the same domain and since it was used to create the very data on which we evaluate.",1,2
1589,21697648,"Trained toxicity: We also compare with the toxicity score of the exchange from the Perspective API classifier-a perhaps unfair reference point, since this supervised system was trained on additional human-labeled training examples from the same domain and since it was used to create the very data on which we evaluate.",8,9
1590,21697648,This results in an accuracy of 60.5%; combining trained toxicity with our pragmatic features achieves 64.9%.,11,12
1591,21697648,"Additionally, since our procedure for collecting and vetting data focused on precision rather than recall, it might miss more subtle attacks that are overlooked by the toxicity classifier.",28,29
1592,84843035,"2017) , toxicity labels (Thain et al.,",3,4
1593,207863217,"While that application in (Reina, 2019) is targeting toxicity in online comments, a change of labels is sufficient to make the same model apply to propaganda detection.",11,12
1594,207863217,"The attributes are toxicity, severe toxicity, identity attack, insult, profanity, threat, sexually explicit, flirtation, inflammatory, obscene, likely to reject (by New York Times moderators) and unsubstantial.",3,4
1595,207863217,"The attributes are toxicity, severe toxicity, identity attack, insult, profanity, threat, sexually explicit, flirtation, inflammatory, obscene, likely to reject (by New York Times moderators) and unsubstantial.",6,7
1596,201666862,"To help limit their spread and impact, we propose and develop a news toxicity detector that can recognize various types of toxic content.",14,15
1597,201666862,We created a new dataset by crawling a website that for five years has been collecting Bulgarian news articles that were manually categorized into eight toxicity groups.,25,26
1598,201666862,"Alternatively, it turns out that an easy way to attract people's attention is to use some toxicity in the articles, as people are intrigued by the unusual.",18,19
1599,201666862,"Here we use this information by performing multi-class classification over the toxicity labels: fake news, sensations, hate speech, conspiracies, anti-democratic, pro-authoritarian, defamation, delusion.",13,14
1600,201666862,Work for Bulgarian We are aware of only one piece of previous work for Bulgarian that targets toxicity.,17,18
1601,201666862,Characteristic Value Data We used Media Scan 3 as a source of toxicity labels for Bulgarian media Web sites.,12,13
1602,201666862,We ended up with a little over 200 articles with some kind of toxicity.,13,14
1603,201666862,"In addition to this dataset of only toxic articles, we added some ""non-toxic"" articles, fetched from media without toxicity examples in Media Scan: we added a total of 96 articles from 25 media.",24,25
1604,201666862,Conclusion and Future Work We have presented experiments in detecting the toxicity of news articles.,11,12
1605,201666862,We created a new dataset by crawling a website that has been collecting Bulgarian news articles and manually categorized them in eight toxicity groups.,22,23
1606,248780452,2021) to measure toxicity and harmfulness of LLMs.,4,5
1607,248780452,We compte toxicity scores on the generated sentences by employing the Perspective API 4 which returns several toxicity scores between 0 and 1.,2,3
1608,248780452,We compte toxicity scores on the generated sentences by employing the Perspective API 4 which returns several toxicity scores between 0 and 1.,17,18
1609,248780452,"Second, it has been demonstrated that it has a high false alarm rate in scoring high toxicity to benign phrases (Hosseini et al.,",17,18
1610,248177981,"Text toxicity predictors are already used in deployed systems (Perspective API, 2021) and they are a crucial component for content moderation since online harassment is on the rise (Vogels, 2021) .",1,2
1611,248177981,The implications of measuring group fairness for the toxicity classification task studied in this paper are described in Section 3.,8,9
1612,248177981,We study the accuracy-fairness relationship in more than a dozen fine-tuned LMs for two different datasets that deal with prediction of text toxicity.,26,27
1613,248177981,We choose to focus on text toxicity as the prediction task.,6,7
1614,248177981,"Due to an increase in online harassment (Vogels, 2021) and the potential of both propagating harmful stereotypes of minority groups and/or inadvertently reducing their voices, the task of predicting toxicity in text has received increased attention in recent years (Kiritchenko et al.,",33,34
1615,248177981,"While we acknowledge that text toxicity presents different complex nuances (e.g., offensive text, harassment, hate speech), we focus on a binary task formulation.",5,6
1616,248177981,We adopt the definition of toxicity described in Borkan et al. (,5,6
1617,248177981,"Note that unlike statistical parity, equalized odds does allow the sensitive (e.g., mention of religion) and complementary (no religion) groups to have different toxicity (positive prediction) rates.",29,30
1618,248177981,"We find these questions important in the context of the ethics of using language models in text toxicity prediction, in particular, and in NLP research, in general.",17,18
1619,248177981,The samples are rated for toxicity and annotated with attributes for sensitive groups.,5,6
1620,248177981,Each sample in the dataset (see Table 4 for a few samples from the dataset) has a toxicity score and we consider anything higher than 0.5 to be toxic.,19,20
1621,201683127,"'s dataset of over 100,000 Wikipedia comments, 'toxicity' is defined in relation to how likely it is to make individuals leave a discussion (Wulczyn et al.,",9,10
1622,235097313,"2017) identify different interpersonal abuse, including toxicity, aggression and attacks.",8,9
1623,10260215,"For example, we are using temporal reasoning built on this work to investigate the liver toxicity of methotrexate across a large corpus of EHRs (Lin et al.,",16,17
1624,248512935,"For example, Rodriguez and Galeano (2018) defend Perspective (Google's toxicity classification model) by neutralizing adversarial inputs via a negated predicates list.",14,15
1625,247596779,"Perspective (Perspective): An API provided by Google, which when given text, returns a toxicity score.",17,18
1626,247596779,Correlations between classifier confidence (for Perspective we use its toxicity score) and language model alignment scores were analyzed.,10,11
1627,247596779,2018) working with Wikipedia Talk Pages show that toxicity classifiers disproportionately misclassify text with identity terms such as 'gay' and 'muslim' when they appear in benign contexts.,9,10
1628,236460356,"In large texts, the reason for toxicity might only be one or two words, however, the moderator will have to Text Toxic Span What a jerk! [",7,8
1629,236460356,"49, 50, 51, 52, 53, 54, 98, 99, 100, 101, 102] scan the entire text to find the toxicity.",29,30
1630,236460356,"Automated systems could assist human moderators further by noting which areas in the text are toxic, thus allowing the human to quickly check text for toxicity.",26,27
1631,236460356,"Specifically, given English toxic text, determine where the ""toxic spans"" occur, where a ""toxic span"" is the character positions in the text where the toxicity appears.",31,32
1632,236460356,"An additional benefit is that since toxicity can be subjective (Aroyo et al.,",6,7
1633,236460356,"2019) , examining how different datasets transfer to this task will help illuminate the similarities and differences between definitions of toxicity.",21,22
1634,236460356,The dataset chosen can affect performance on the test data as data can vary in their definitions of toxicity.,18,19
1635,236460356,Non-toxic words become associated with toxicity in training set.,7,8
1636,236460356,"An interesting example is ""trump"" being marked as a toxic span, which means the training data provides ""trump"" in enough toxic texts for our system to associate that with toxicity.",34,35
1637,236460356,Another reason for wrongly identified spans is that the toxicity isn't always clear.,9,10
1638,236460356,"This could be attributed to toxicity being a subjective idea (Aroyo et al.,",5,6
1639,236460356,This category has an overlap as well with the previous unclear category since toxicity can be subjective.,13,14
1640,236460356,"Leveraging Multiple Views via Ensemble of Datasets As toxicity can vary, it follows that different toxic datasets can identify different aspects of toxic spans.",8,9
1641,236460356,"For toxicity specifically, Karan and Šnajder (2019) examine detecting threads that could lead to toxic language in the future.",1,2
1642,247597244,"Perspective API (Perspective) by Google (Jigsaw) provides a toxicity model that classifies whether a post is ""rude, disrespectful, or unreasonable.""",12,13
1643,247597244,"The production model uses a CNN trained with fine-tuned GloVe word embeddings and provides ""toxicity"" probability.",17,18
1644,222291230,Toxic Comment Classification (TC) labels sentences (typically from social media platforms) with one or several toxicity classes.,19,20
1645,237572304,We present two novel unsupervised methods for eliminating toxicity in text.,8,9
1646,237572304,We use a well-performing paraphraser guided by style-trained language models to keep the text content and remove toxicity.,21,22
1647,237572304,"Finally, we present the first largescale comparative study of style transfer models on the task of toxicity removal.",17,18
1648,237572304,"Introduction Identification of toxicity in user texts is an active area of research (Zampieri et al.,",3,4
1649,237572304,"At each generation step, the distribution of the next token predicted by the main model P LM is modified using an additional class-conditional language model P D and the Bayes rule: P (x t |x <t , c) ∝ P LM (x t |x <t )P D (c|x t , x <t ) Here, x t is the current token, x <t is the prefix of the text, and c is the desired attribute (e.g. toxicity or sentiment) -one of C classes.",92,93
1650,237572304,Our reranker is a pre-trained toxicity classifier which chooses the least toxic hypothesis generated by the ParaGeDi model.,7,8
1651,237572304,"While in the original conditional BERT model the words are masked randomly, we select the words associated with toxicity.",19,20
1652,237572304,"This can be done in different ways, e.g. by training a word-level toxicity classifier or manually creating a vocabulary of rude and toxic words.",15,16
1653,237572304,We train a logistic bag-of-words toxicity classifier.,9,10
1654,237572304,We use the normalised weights from the classifier as toxicity score.,9,10
1655,237572304,"they are all commies who hate the USA For each word in a sentence, we compute the toxicity score and then define toxic words as the words with the score above a threshold t = max(t min , max(s 1 , s 2 , ..., s n )/2), where s 1 , s 2 , ..., s n are scores of all words in a sentence and t min = 0.2 is a minimum toxicity score.",18,19
1656,237572304,"they are all commies who hate the USA For each word in a sentence, we compute the toxicity score and then define toxic words as the words with the score above a threshold t = max(t min , max(s 1 , s 2 , ..., s n )/2), where s 1 , s 2 , ..., s n are scores of all words in a sentence and t min = 0.2 is a minimum toxicity score.",80,81
1657,237572304,To force the model to generate non-toxic words we calculate the toxicity of each token in BERT vocabulary and penalize the predicted probabilities of tokens with positive toxicities.,13,14
1658,237572304,Toxicity Classifier We train two binary classifiers of toxicity.,8,9
1659,237572304,"To prepare the toxic dataset, we divide the comments labelled as toxic into sentences (the original comments are often too long) and classify each of them with our toxicity classifier.",31,32
1660,237572304,"The test set is prepared analogously to the test set of the Jigsaw competition: we use 10,000 sentences with the highest toxicity score according to our classifier.",22,23
1661,237572304,Style accu-racy (ACC) is measured with a pre-trained toxicity classifier described in Section 5.1.,14,15
1662,237572304,"Machine Translation There is evidence that automatic translation tends to eliminate toxicity (Prabhumoye et al.,",11,12
1663,237572304,"Conversely, En→Fr→En yields a better output which keeps most of the original features, including toxicity.",16,17
1664,237572304,It turns out that the crucial features of CondBERT are multiword replacement which ensures high fluency and toxicity penalty which increases style strength.,17,18
1665,237572304,"2020) 0.29 0.69 0.80 0.15 ± 0.0027 En→Ig→En MT (baseline) 0.37 0.68 0.57 0.12 ± 0.0025 T5 paraphraser (baseline) 0.15 0.90 0.87 0.11 ± 0.0029 SST (Lee, 2020) 0.80 0.55 0.12 0.05 ± 0.0019 En→Fr→En MT (baseline) 0.06 0.91 0.81 0.04 ± 0.0019   phrase dataset (Wieting and Gimpel, 2018) with our toxicity classifier described in Section 5.1 and obtain 500,000 paraphrase pairs where one sentence is more toxic than the other (for more details on the data collection process please see Appendix D).",65,66
1666,237572304,"For style, we consider the accuracy of toxicity classifier that we used for the evaluation (ACC) and its version which returns the confidence instead of the binary label (ACC-soft).",8,9
1667,237572304,"CondBERT model is based on BERT which does not need any finetuning, and all style control is performed with a pre-trained toxicity classifier.",24,25
1668,237572304,"While annotators agree when labelling serious cases of toxicity such as hate speech (Fortuna and Nunes, 2018) , the labelling of less severe toxicity is subjective and depends on the annotator's background (Al Kuwatly et al.,",8,9
1669,237572304,"While annotators agree when labelling serious cases of toxicity such as hate speech (Fortuna and Nunes, 2018) , the labelling of less severe toxicity is subjective and depends on the annotator's background (Al Kuwatly et al.,",26,27
1670,237572304,This can cause the underestimation of certain types of toxicity.,9,10
1671,237572304,"To define the toxicity in the most objective feasible way, we adopt a data-driven approach as presented in detail formally in Appendix A. Both models we propose recognise toxicity based on a toxicitylabelled dataset and do not require any additional manually created dictionaries or rules.",3,4
1672,237572304,"To define the toxicity in the most objective feasible way, we adopt a data-driven approach as presented in detail formally in Appendix A. Both models we propose recognise toxicity based on a toxicitylabelled dataset and do not require any additional manually created dictionaries or rules.",31,32
1673,237572304,"Thus, their understanding of toxicity can be tuned with the input data.",5,6
1674,237572304,This ensures that given a corpus with unbiased toxicity labelling our models can produce unbiased detoxification.,8,9
1675,237572304,"A Definition of Text Detoxification Task In our work, we adhere to the data-driven definition of toxicity.",19,20
1676,237572304,"The toxicity is a particular binary value associated with a text: {toxic, neutral}.",1,2
1677,237572304,"Let us assume a set of two discreet mutually exclusive styles S = {s src , s tg } which corresponds to the source toxic and target neutral When removing the toxicity from a text, we inevitably change a part of its meaning, so full content preservation cannot be reached.",32,33
1678,237572304,"Finally, the greatest impact on the J metric is caused by eliminating the toxicity penalty.",14,15
1679,237572304,After this similarity-and length-based filtering we score each sentence with a RoBERTa-based toxicity classifier from Section 5.1 and keep only the pairs with the difference in toxicity scores of at least 50%.,18,19
1680,237572304,After this similarity-and length-based filtering we score each sentence with a RoBERTa-based toxicity classifier from Section 5.1 and keep only the pairs with the difference in toxicity scores of at least 50%.,32,33
1681,236460327,This baseline can be further improved by pre-training the RoBERTa model on a large dataset labeled for toxicity at the sentence level.,19,20
1682,236460327,"Therefore, the task of toxicity detection has attracted much attention in the NLP community (Wulczyn et al.,",5,6
1683,236460327,"Until recently, the majority of research on toxicity focused on classifying entire user messages as toxic or safe.",8,9
1684,236460327,"However, the surge of work on text detoxification, i.e., editing of text to keep its content and remove toxicity (Nogueira dos Santos et al.,",21,22
1685,236460327,"2020) , suggests that localizing toxicity within a sentence is also useful.",6,7
1686,236460327,They show that using such spans when training a toxicity classifier improves its accuracy and explainability and reduces unintended bias towards toxicity targets.,9,10
1687,236460327,They show that using such spans when training a toxicity classifier improves its accuracy and explainability and reduces unintended bias towards toxicity targets.,21,22
1688,236460327,"As far as we know, it is the first attempt to explicitly formulate toxicity detection as sequence labeling instead of classification of sentences.",14,15
1689,236460327,"Word-level toxicity classification can be formulated as a sequence labeling task, which also actively uses the pre-trained models mentioned above.",3,4
1690,236460327,This diversity of applications suggests that wordlevel toxicity detection can also benefit from pretrained models.,7,8
1691,236460327,"Besides that, toxicity itself has been successfully tackled with BERT-based models.",3,4
1692,236460327,Research on sentence-level toxicity extensively used BERT and other pre-trained models.,5,6
1693,236460327,"Both language-specific and multilingual BERT models were used to fine-tune toxicity classifiers (Leite et al.,",14,15
1694,236460327,This shows that BERT has information on toxicity.,7,8
1695,236460327,"Besides that, we train a model for sentence classification on the Jigsaw dataset of toxic comments and use the information from this model to detect toxicity at the subsentential level.",26,27
1696,236460327,"In our experiments, we test the hypothesis that the sentence-level toxicity labeling can be used for a sequence labeler that recognizes toxic spans in text.",13,14
1697,236460327,"We suggest three ways of incorporating this data: as a corpus for pre-training, pseudo-labeling, and for joint training of sentence-level and tokenlevel toxicity detection models.",31,32
1698,236460327,"The contributions of this work are the following: • We successfully use the dataset labeled for toxicity at the sentence level for token-level toxicity labeling, • We propose a model for joint sentence-and token-level toxicity detection, • We analyze the performance of our models, showing their limitations and reveal the ambiguities in the data.",17,18
1699,236460327,"The contributions of this work are the following: • We successfully use the dataset labeled for toxicity at the sentence level for token-level toxicity labeling, • We propose a model for joint sentence-and token-level toxicity detection, • We analyze the performance of our models, showing their limitations and reveal the ambiguities in the data.",26,27
1700,236460327,"The contributions of this work are the following: • We successfully use the dataset labeled for toxicity at the sentence level for token-level toxicity labeling, • We propose a model for joint sentence-and token-level toxicity detection, • We analyze the performance of our models, showing their limitations and reveal the ambiguities in the data.",42,43
1701,236460327,"Motivation Our intuition is that the toxicity is often lexicallybased, i.e., there are certain words that are considered offensive and make the whole sentence toxic.",6,7
1702,236460327,"To mitigate the lack of data, we leverage the additional dataset with toxicity information, namely, the Jigsaw toxic comments dataset 2 which features 140,000 user utterances labeled as toxic or safe.",13,14
1703,236460327,This model does not need to be trained on the data with token-level labeling but can get token-level toxicity information from sentence labels.,22,23
1704,236460327,"Consider a token to be toxic if its toxicity score is higher than the threshold, do not force the labels of tokens within a word to agree with each other.",8,9
1705,236460327,Consider a word to be toxic if the aggregated toxicity score of all its tokens is higher than the threshold.,9,10
1706,236460327,Attention-based LogReg Another approach to represent words is to take their attention weights from a RoBERTa-based sentence-level toxicity classifier (we train it on the Jigsaw dataset).,23,24
1707,236460327,This approach is motivated by the fact that a RoBERTa model trained to recognize toxicity puts more emphasis on certain words associated with sentence-level toxicity.,14,15
1708,236460327,This approach is motivated by the fact that a RoBERTa model trained to recognize toxicity puts more emphasis on certain words associated with sentence-level toxicity.,26,27
1709,236460327,"Conditional Random Fields We suggest that the toxicity level of a word can be contextdependent, so we also experiment with sequence labeling models.",7,8
1710,236460327,"In addition to that, we perform fine-tuning on an additional dataset with sentence-level toxicity labeling.",18,19
1711,232168787,"Not all topics are equally ""flammable"" in terms of toxicity: a calm discussion of turtles or fishing less often fuels inappropriate toxic dialogues than a discussion of politics or sexual minorities.",11,12
1712,232168787,"While toxicity in user-generated data is wellstudied, we aim at defining a more fine-grained notion of inappropriateness.",1,2
1713,232168787,"This is different from toxicity in two respects: (i) inappropriateness is topicrelated, and (ii) inappropriate message is not toxic but still unacceptable.",4,5
1714,232168787,Introduction The classification and prevention of toxicity (malicious behaviour) among users is an important problem for many Internet platforms.,6,7
1715,232168787,"Since communication on most social networks is predominantly textual, the classification of toxicity is usually solved by means of Natural Language Processing (NLP).",13,14
1716,232168787,"This includes toxicity, but also any answers which can express undesirable views and approve or prompt user towards harmful or illegal actions.",2,3
1717,232168787,Sensitive topics are just topics that should be considered with extra care and tend to often flame/catalyze toxicity.,19,20
1718,232168787,3 Related Work There exist a large number of English textual corpora labeled for the presence or absence of toxicity; some resources indicate the degree of toxicity and its topic.,19,20
1719,232168787,3 Related Work There exist a large number of English textual corpora labeled for the presence or absence of toxicity; some resources indicate the degree of toxicity and its topic.,27,28
1720,232168787,"However, the definition of the term ""toxicity"" itself is not agreed among the research community, so each research deals with different texts.",8,9
1721,232168787,"Some works refer to any unwanted behaviour as toxicity and do not make any further separation (Pavlopoulos et al.,",8,9
1722,232168787,"The Wikipedia Toxic comment datasets by Jigsaw (Jigsaw, 2018 (Jigsaw, , 2019 (Jigsaw, , 2020) ) are the largest English toxicity datasets available to date operate with multiple types of toxicity (toxic, obscene, threat, insult, identity hate, etc).",27,28
1723,232168787,"The Wikipedia Toxic comment datasets by Jigsaw (Jigsaw, 2018 (Jigsaw, , 2019 (Jigsaw, , 2020) ) are the largest English toxicity datasets available to date operate with multiple types of toxicity (toxic, obscene, threat, insult, identity hate, etc).",37,38
1724,232168787,"Besides directly classifying toxic messages for a topic, the notion of the topic in toxicity is also indirectly used to collect the data: Zampieri et al. (",15,16
1725,232168787,2019) pre-select messages for toxicity labeling based on their topic.,7,8
1726,232168787,"Such a topic-based view of toxicity causes unintended bias in toxicity detection -a false association of toxicity with a particular topic (LGBT, Islam, feminism, etc.) (",7,8
1727,232168787,"Such a topic-based view of toxicity causes unintended bias in toxicity detection -a false association of toxicity with a particular topic (LGBT, Islam, feminism, etc.) (",12,13
1728,232168787,"Such a topic-based view of toxicity causes unintended bias in toxicity detection -a false association of toxicity with a particular topic (LGBT, Islam, feminism, etc.) (",18,19
1729,232168787,This is in line with our work since we also acknowledge that there exist acceptable and unacceptable messages within toxicity-provoking topics.,19,20
1730,232168787,"The main drawback of topic-based toxicity detection in the existing research is the ad-hoc choice of topics: the authors select a small number of popular topics manually or based on the topics which emerge in the data often, as Ousidhoum et al. (",7,8
1731,232168787,"It contains toxic topics, but they are mixed with other parameters of toxicity (e.g. direction or severity).",13,14
1732,232168787,2020) is the only example of an extensive list of toxicity-provoking topics.,11,12
1733,232168787,"However, our definition is broader -sensitive topics are not only topics that attract toxicity, but they can also create unwanted dialogues of multiple types (e.g. incitement to law violation or to cause harm to oneself or others).",14,15
1734,232168787,This is different from the notion of toxicity which does not have to be topic-dependent.,7,8
1735,232168787,"Analogously to toxicity, inappropriateness does not often occur in randomly picked texts, so if we label all the messages we retrieve, the percentage of inappropriate utterances among them will be low.",2,3
1736,232168787,"The site is not moderated, suggesting a large amount of toxicity and controversy; this makes it a practical resource for our purposes.",11,12
1737,232168787,We filter out all messages containing obscene language and explicit toxicity.,10,11
1738,232168787,We identify toxicity with a BERT-based classifier for toxicity detection.,2,3
1739,232168787,We identify toxicity with a BERT-based classifier for toxicity detection.,10,11
1740,232168787,"As mentioned above, toxicity is beyond the scope of our work, because it has been researched before.",4,5
1741,233219395,We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets.,13,14
1742,233219395,"2020) or incorporating a toxicity discriminator during decoding (Dathathri et al.,",5,6
1743,233219395,"Our evaluation of these techniques shows that they are indeed effective at mitigating toxicity, but at what cost?",13,14
1744,233219395,"In particular, toxicity datasets often contain spurious correlations between the toxic label and the presence of AAE and minority identity mentions (Sap et al.,",3,4
1745,233219395,We detoxify models using controllable generation techniques that steer outputs away from toxicity.,12,13
1746,233219395,"This technique aims to erase an LM's knowledge of toxicity via catastrophic forgetting (McCloskey and Cohen, 1989) .",10,11
1747,233219395,"Here, we first train a toxicity classifier using the hidden states of the LM as features.",6,7
1748,233219395,"At generation time, the LM's hidden states are iteratively updated using a gradient from the toxicity classifier.",17,18
1749,233219395,"Filtering Finally, we consider output filtering, where we generate a fixed number of times (we use 10) from the LM and return the least toxic generation according to a toxicity classifier.",33,34
1750,233219395,We reuse the same toxicity classifier from PPLM.,4,5
1751,233219395,"4  The detoxification techniques are effective at removing toxicity: the perplexity on toxic data increases substantially (Figure 1 , toxic evaluation set).",9,10
1752,233219395,"They evaluate the model continuations based on toxicity and three measures of generation quality: topicality, fluency, and style.",7,8
1753,233219395,All detoxification methods generate less toxicity than the baseline GPT-2 model.,5,6
1754,233219395,"PPLM, GeDi, and Filtering use this data indirectly: they train a classifier or LM on the toxicity data and then incorporate this model into the LM's decoding strategy.",19,20
1755,233219395,"For example, DAPT will train LMs to not only forget toxicity but also forget AAE and minority identity mentions.",11,12
1756,233219395,6 Future Work: Towards Bias-Free Detoxification The harms that we have identified occur largely due to spurious correlations in toxicity datasets.,22,23
1757,248780142,"Similarly, recent work found separate empathy types were found to have different effects on toxicity reduction (Lahnala et al.,",15,16
1758,248811664,"Using empathetic data, we improve over recent work on controllable text generation that aims to reduce the toxicity of generated text.",18,19
1759,248811664,"We find we are able to dramatically reduce the size of finetuning data to 7.5-30k samples while at the same time making significant improvements over state-of-the-art toxicity mitigation of up to 3.4% absolute reduction (26% relative) from the original work on 2.3m samples, by strategically sampling data based on empathy scores.",34,35
1760,248811664,"Though the definitions of toxicity and empathy vary across literature, we observe an opposition between the concepts in terms of response appropriateness and intent toward others, which is the basis of the research question driving this work: is there an opposing relationship between toxic and empathetic language that can be leveraged to better model these phenomena?",4,5
1761,248811664,There is an unexplored negatively correlated relationship between toxicity and empathy.,8,9
1762,248811664,Specific categories of empathetic behavior have a stronger relation to the reduction of specific types of toxicity.,16,17
1763,248811664,We perform a set of experiments in which we leverage empathetic data to alter the toxicity of generated text.,15,16
1764,248811664,We use the predictions of a language model trained on empathetic data to alter the output of a large pretrained language model and demonstrate that using only a small volume of empathetic data can reduce toxicity more than a model simply trained on a large volume of non-toxic text.,35,36
1765,248811664,"Furthermore, we consider relationships between various facets of toxicity and empathy, particularly emphasizing the distinction between emotional empathy and cognitive empathy that is less commonly made in the NLP literature.",9,10
1766,248811664,We find that training on text with high cognitive empathy is more effective at reducing toxicity than text with emotional empathy.,15,16
1767,248811664,"In this section, we review the related work on toxicity, empathy, and controllable text generation.",10,11
1768,248811664,"The Jigsaw shared task provided a large volume of Wikipedia comments with human annotations of six classes of toxicity (Jigsaw, 2021b) .",18,19
1769,248811664,"SemEval-2021 hosted a task on toxic span detection, where one must identify the subsequence of a text that is responsible for the toxicity label (Pavlopoulos et al.,",23,24
1770,248811664,"2020) performed experiments across toxicity datasets, finding that within-class homogeneity and performance vary greatly.",5,6
1771,248811664,"They suggest that each dataset has its own ""flavor"" of toxicity, even for similarly defined concepts.",12,13
1772,248811664,"For toxicity, we use all types of toxicity currently available from the Perspective API.",1,2
1773,248811664,"For toxicity, we use all types of toxicity currently available from the Perspective API.",8,9
1774,248811664,"Related works often use only the toxicity score, while the API currently offers scores for eight attributes, the last two of which were listed as experimental at the time of use: Toxicity: A rude, disrespectful, or unreasonable comment that is likely to make people leave a discussion.",6,7
1775,248811664,"This attribute is much less sensitive to more mild forms of toxicity, such as comments that include positive uses of curse words.",11,12
1776,248811664,"Models The DExperts model combines the predictions of a base LM with expert LMs fine-tuned on data known to either contain a desired (e.g., empathy) or undesired attribute (e.g., toxicity).",36,37
1777,248811664,Using an opposing attribute to train the expert model should help minimize the probability of our undesired attribute (e.g. empathy used to oppose toxicity).,24,25
1778,248811664,The intuition behind the negative correlation between empathy and toxicity lies in the perceived appropriateness of language and a better understanding of the user.,9,10
1779,248811664,"2021) and the same metrics of toxicity, fluency, and diversity for comparability.",7,8
1780,248811664,Average max toxicity is the highest toxicity score given to the set and averaged over all 10k prompts.,2,3
1781,248811664,Average max toxicity is the highest toxicity score given to the set and averaged over all 10k prompts.,6,7
1782,248811664,Probability of toxicity is the chance of a continuation having a score of ⩾ 0.5 at least once in the set.,2,3
1783,248811664,"2 Our model comes close to the DExperts baseline with a difference of 1.4% toxicity probability, 1% average max toxicity, though perplexity shows a greater gap.",15,16
1784,248811664,"2 Our model comes close to the DExperts baseline with a difference of 1.4% toxicity probability, 1% average max toxicity, though perplexity shows a greater gap.",22,23
1785,248811664,Empathy Components Experiments We are also interested to know which type of empathy is most useful for mitigating toxicity.,18,19
1786,248811664,"4 With permutation test on both average max toxicity and 0% 20% 40% 60% 80% 100%  using their large model with 2.3m examples (compared to our 7.5k), is 3.4% absolute reduction in toxic probability (26% relative).",8,9
1787,248811664,"Agreement measured with Fleiss' kappa gave us 0.30 for both toxicity and fluency (fair), and 0.07 for topicality (poor).",11,12
1788,248811664,"We found our model significantly improved toxicity and fluency, but not topicality.",6,7
1789,248811664,toxicity probability p < 10 −5 .,0,1
1790,248811664,Analysis We are interested in better understanding two aspects of our results; how the types of empathy and toxicity affect each other and how the generation length affects the toxicity.,19,20
1791,248811664,Analysis We are interested in better understanding two aspects of our results; how the types of empathy and toxicity affect each other and how the generation length affects the toxicity.,30,31
1792,248811664,"Empathy and Toxicity Types: For a more in-depth analysis, we examine each type of toxic language provided by the Perspective API and how the toxicity varies with fine-tuning data volume.",28,29
1793,248811664,Our model performs best on most types of toxicity with the exception of the profanity and insult toxicity types.,8,9
1794,248811664,Our model performs best on most types of toxicity with the exception of the profanity and insult toxicity types.,17,18
1795,248811664,"Although the baseline performs better for these two cases, it performs worse for overall toxicity.",15,16
1796,248811664,"To investigate this, we calculated the average toxicity score for our best model that uses 7.5k examples for fine-tuning, our random fine-tuned baseline that uses the same amount of data, and the original DExperts large model.",8,9
1797,248811664,"Note that the average toxicity grouped by generation length cannot be grouped across prompts, so we do not use our previous evaluation metrics, but rather the average of the toxicity score given by the API.",4,5
1798,248811664,"Note that the average toxicity grouped by generation length cannot be grouped across prompts, so we do not use our previous evaluation metrics, but rather the average of the toxicity score given by the API.",32,33
1799,248811664,"The proportion at its highest reaches 1%, though small, may account for the higher performance of the DExperts baseline over our models for the profanity and insult toxicity types from Figure 2 .",30,31
1800,248811664,"We also notice that the average toxicity de- Figure 2 : Each plot shows the toxicity probabilities of a specific type of toxic language (e.g., profanity) as a function of the fine-tuning data size, for each of the models fine-tuned on the sets maximizing emotional reactions (ER), explorations (EX), and interpretations (IP), as well as on sets of random samples (R).",6,7
1801,248811664,"We also notice that the average toxicity de- Figure 2 : Each plot shows the toxicity probabilities of a specific type of toxic language (e.g., profanity) as a function of the fine-tuning data size, for each of the models fine-tuned on the sets maximizing emotional reactions (ER), explorations (EX), and interpretations (IP), as well as on sets of random samples (R).",15,16
1802,248811664,It is possible that it could be further evaluated and improved by adding empathetic annotations to a toxicity dataset such as this.,17,18
1803,248811664,This points to the need for the contextualization of the perception of toxicity as well as possible biases in our automatic evaluation.,12,13
1804,248811664,The linear transformation used in our language model encodes the assumption that toxicity and empathy are opposites.,12,13
1805,248811664,"Additionally, we believe it would be better to use a toxicity dataset that includes conversational context.",11,12
1806,248811664,"Conclusions In this work, we investigated empathy and toxicity, showing that the relationship between the two can be leveraged for mitigating toxic degeneration.",9,10
1807,248811664,We provided insight into the model performance across aspects of toxicity and generation length.,10,11
1808,222177236,Building a large annotated dataset for such veiled toxicity can be very expensive.,8,9
1809,222177236,"In this work, we propose a framework aimed at fortifying existing toxic speech detectors without a large labeled corpus of veiled toxicity.",22,23
1810,222177236,"We augment the toxic speech detector's training data with these discovered offensive examples, thereby making it more robust to veiled toxicity while preserving its utility in detecting overt toxicity.",22,23
1811,222177236,"We augment the toxic speech detector's training data with these discovered offensive examples, thereby making it more robust to veiled toxicity while preserving its utility in detecting overt toxicity.",30,31
1812,222177236,"2017) and thus are ineffective at detecting forms of veiled toxicity; e.g., codewords (Taylor et al.,",11,12
1813,222177236,3   Detecting veiled toxicity is hard: deep semantic analysis and large datasets are needed.,4,5
1814,222177236,"But veiled offenses are not represented in existing toxicity datasets (Waseem and Hovy, 2016; Davidson et al.,",8,9
1815,222177236,We propose a framework to surface veiled offenses and improve toxicity classifiers that are compromised in detecting them.,10,11
1816,222177236,"From an original classifier that detects veiled toxicity with an accuracy of 1%, we achieve up-to 51% accuracy in detecting veiled offenses while preserving the utility of the classifier in detecting overt offenses.",7,8
1817,222177236,"To the best of our knowledge, our work is the first in making toxic speech detectors robust against veiled toxicity with almost no annotated data.",20,21
1818,222177236,"Identifying Veiled Toxicity A typical toxicity classifier C might fail to identify veiled offenses because they are not well represented among toxic examples in its training data D. Moreover, the non-toxic portion of D might be polluted with (mislabeled) disguised offenses.",5,6
1819,222177236,"microaggressions.com/. Probing for veiled toxicity We explore several methods to define the influence I(x trn , x prb ) of a training example x trn ∈ D over a probing example x prb ∈ P. Embedding product Modern neural classifiers often consist of two parts: an encoding module f enc (•) that transforms the input to some hidden representation, and a projection layer f proj (•) that projects the output of the encoding module to the label space.",4,5
1820,222177236,We first extract veiled toxicity set.,4,5
1821,222177236,"We randomly sample 10K general reddits from no specific domains and measure their average Perspective API toxicity score tox general ≈ 0.17 on a scale [0,1].",17,18
1822,222177236,We then measure the Perspective API toxicity scores of the posts in SBIC that are offensive to at least one minority group.,6,7
1823,222177236,"First, although it was trained on comments from online forums such as discussions of Wikipedia and New York Times, it could misclassify SBIC examples due to a domain mismatch leading to different manifestations of overt toxicity.",37,38
1824,222177236,"In addition, as we discuss above, misclassifications can be attributed to novel lexicons of toxicity, to (intentional or unintentional) spelling variations, or to more subtly expressed implicit offenses.",16,17
1825,222177236,"Our set of veiled offenses covers any of these forms, as they are hidden from the original toxicity detection model.",18,19
1826,222177236,"11  We extract all SBIC posts annotated as nonoffensive and also sort their Perspective API toxicity scores from low to high in x 1 , x (2) , • • • , x (n ) .",16,17
1827,222177236,"For the SBIC posts that are identified as offensive, we extract those with Perspective API toxicity score > 0.8 (a recommended threshold by Perspective API for determining bad language) and consider them as our overtly offensive set.",16,17
1828,222177236,Student model evaluation Would a vanilla toxicity classifier recognize veiled offenses?,6,7
1829,222177236,All of the updated models still preserve the utility in recognizing the overt toxicity.,13,14
1830,222177236,"They got low toxicity scores from Perspective API, but were annotated as offensive to at least one social group according to the SBIC dataset.",3,4
1831,234679223,"We would like to remind our dataset users that there could have potential bias, toxicity, and subjective opinions in the selected conversations which may impact model training.",15,16
1832,248779948,2019) toxicity classifier 5 fine-tuned from RuBERT Conversational.,2,3
1833,248779948,That means that models produce the same or almost the same sentences for the language on which they were not fine-tuned so that toxicity is not eliminated.,25,26
1834,248780527,"Introduction Detection of toxicity (Zampieri et al.,",3,4
1835,248780527,We suggest that such messages could be automatically rewritten to keep the useful content intact and eliminate toxicity.,17,18
1836,248780527,"Detoxification is usually formulated as style transfer from toxic to neutral (non-toxic) style, so it uses non-parallel datasets labeled for toxicity and considers toxic and neutral sentences as two subcorpora.",27,28
1837,248780527,2018 ) create their own toxicity-labelled datasets of sentences from Reddit and Twitter.,5,6
1838,248780527,Crowdsourcing Tasks We ask crowd workers to generate paraphrases and then evaluate them for content preservation and toxicity.,17,18
1839,248780527,2 Task 1: Generation of Paraphrases The first crowdsourcing task asks users to eliminate toxicity in a given sentence while keeping the content (see the task interface in Figure 1 ).,15,16
1840,248780527,"Some sentences cannot be detoxified, because they do not contain toxicity or because they are meaningless.",12,13
1841,248780527,"Moreover, in some cases toxicity cannot be removed.",5,6
1842,248780527,"Task 3: Toxicity Check Finally, we check if the workers succeeded in removing toxicity.",15,16
1843,248780527,We fetch them from corpora labeled for toxicity and additionally filter them with a toxicity classifier (described in Section 3.3).,7,8
1844,248780527,We fetch them from corpora labeled for toxicity and additionally filter them with a toxicity classifier (described in Section 3.3).,14,15
1845,248780527,"However, we run Task 3 twice, because we need to check both parts of the pair for toxicity.",19,20
1846,248780527,"Analogously to the generation pipeline, we use a toxicity classifier to pre-select pairs of sentences where one sentence is toxic and the other one is neutral.",9,10
1847,248780527,"Crowdsourcing Settings Preprocessing To pre-select toxic sentences, we need a toxicity classifier.",13,14
1848,248780527,We plot the percentage of paraphrases which were filtered out by content and toxicity checks in Figure 8 .,13,14
1849,248780527,"We then sample 6,000 random pairs from this list and ask workers to evaluate them for toxicity and content preservation.",16,17
1850,248780527,Ethical Considerations The research on toxicity raises some ethical issues.,5,6
1851,248780527,"No toxicity this is all coming out of our darned pockets, and i am not confident!",1,2
1852,232035490,"Despite their popularity, toxicity detection tasks have focused majorly on sequence classification, rather than sequence tagging.",4,5
1853,232035490,Finding which spans make a comment or document toxic in nature is crucial in explaining the reasons behind their toxicity.,19,20
1854,232035490,"Additionally, such attributions would allow for more efficient semi-automated quality-based moderation of content, especially for verbose documents, in comparison to quantitative toxicity scores.",28,29
1855,232035490,2020) discuss context requirement for toxicity detection.,6,7
1856,232035490,"From the valid spans, the score is calculated as the average of start and end logit scores, as well as the mean of toxicity logits over the span under consideration.",25,26
1857,232035490,"The score is given as: S(i s , i e ) = ŝis + êie 2 + ie k=is tk e − s + 1 where i s and i e are start and end indices, ŝis and êie are start and end logits at those indices, and tk is toxicity logit at index k. A threshold, similar to Section 3.2 is tuned on the dev set.",55,56
1858,232035490,"For the Token Classification model, the targets are softmax outputs of toxicity logits of those tokens which the model predicts to be toxic, with a score greater than 0.5.",12,13
1859,232035490,"For all such toxicity logits as targets, we calculate attributions with respect to the embedding layer outputs for all the tokens, and average them to get token-wise importance scores.",3,4
1860,232035490,"Additionally, some words outside of the span may contribute to toxicity of a particular span.",11,12
1861,232035490,"The first instance of 'ignorant' does not seem to be as toxic as the second instance and therefore, more analysis needs to be done to determine the 'degree' of toxicity of the spans.",34,35
1862,195218693,"For instance, the sentence ""I am gay"" receives a high score on a toxicity model as seen in Table 1 .",16,17
1863,195218693,Our main intuition is that undesirable correlations between toxicity labels and instances of identity terms cause the model to learn unfair biases which can be corrected by incorporating priors on these identity terms.,8,9
1864,195218693,"As shown in our experiments, by setting a positive target attribution for known toxic words 1 , one can improve the performance of a toxicity classifier in a scarce data regime.",25,26
1865,195218693,"The dataset was annotated by human raters, where toxicity was defined as a ""rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion"" per Dixon et al. (",9,10
1866,195218693,"Please refer to the corresponding paper for more details about collection methodology, biases present in the data, and toxicity distribution per comment length.",20,21
1867,195218693,"However, the impact of our approach diminishes after adding more data, since the model starts to learn to focus on toxic words itself for predicting toxicity without the need for prior injection.",27,28
1868,196185011,"2018) , or inequality of sentiment or toxicity for various protected groups (Caliskan-Islam et al.,",8,9
1869,236460291,"We show that pre-trained convolutions are competitive against pre-trained Transformers via a set of experiments on a potpourri of NLP tasks, like toxicity detection, sentiment classification, news classification, query understanding and semantic parsing/compositional generalization (Kim and Linzen, 2020) .",27,28
1870,231847004,"We demonstrate that toxicity models, as exemplified by Perspective, are inadequate for the analysis of incivility in news.",3,4
1871,231847004,"Applying off-the-shelf tools for toxicity detection is appealingly convenient, but such use has not been validated for any domain, while uses in support of moderation efforts have been validated only for online comments.",8,9
1872,231847004,"We examine the feasibility of quantifying incivility in the news via the Jigsaw Perspective API, which has been trained on over a million online comments rated for toxicity and deployed in several scenarios to support moderator effort online 1 .",28,29
1873,231847004,"Incivility is more subtle and nuanced than toxicity, which includes identity slurs, profanity, and threats of violence along other unacceptable incivility.",7,8
1874,231847004,"To pinpoint some of the sources of the noise in predictions, we characterize individual words as likely triggers of errors in Perspective or sub-error triggers that lead to over-prediction of toxicity.",35,36
1875,231847004,Another category of incivility detection that more closely aligns with our work is toxicity prediction.,13,14
1876,231847004,2018) collected a dataset for toxicity identification in online comments on Wikipedia talk pages.,6,7
1877,231847004,"They defined toxicity as comments that are rude, disrespectful, or otherwise likely to make someone leave a discussion.",2,3
1878,231847004,2019) were released for the system to show how well the system is able to predict toxicity when certain identity words are mentioned in a text.,17,18
1879,231847004,"Simple templates such as ""I am <IDENTITY>"" were used to measure the toxicity associated with identity words.",16,17
1880,231847004,"More recently, many more incorrect associations with toxicity were discovered.",8,9
1881,231847004,"2019) found that Perspective returned a higher toxicity score when certain names are mentioned, and Hutchinson et al. (",8,9
1882,231847004,"I am a blind person"" had a significantly higher toxicity score than ""I am a tall person"".",10,11
1883,231847004,"Further, we propose a way to establish a reference set of words and then find words associated with markedly higher toxicity than the reference.",21,22
1884,231847004,This approach reveals a larger set of words which do not lead to errors but trigger uncommonly elevated predictions of toxicity in the lower ranges of the toxicity scale.,20,21
1885,231847004,This approach reveals a larger set of words which do not lead to errors but trigger uncommonly elevated predictions of toxicity in the lower ranges of the toxicity scale.,27,28
1886,231847004,"For automatic analysis, we obtain Perspective scores for each speaker turn as marked in the transcript and count the number of turns predicted as having toxicity 0.5 or greater.",26,27
1887,231847004,There is only one snippet with Perspective toxicity score over 0.5 for the civil to borderline civil segments from the news shows; this indicates it has good precision for binary identification of civil content.,7,8
1888,231847004,"The recall for incivility is not that good, with some of these snippets receiving low toxicity scores.",16,17
1889,231847004,"Their analysis, similar to other work on bias in word representations, starts with a small list of about 50 words to be analyzed in an attempt to find toxicity over-prediction triggers.",30,31
1890,231847004,"Given our text domain of interest (news) and the desiderata to characterize sources rather than individual text segments, we also find sub-errors, or words that do not lead to errors in toxicity prediction but have much higher than average toxicity associated with them compared to other words.",37,38
1891,231847004,"Given our text domain of interest (news) and the desiderata to characterize sources rather than individual text segments, we also find sub-errors, or words that do not lead to errors in toxicity prediction but have much higher than average toxicity associated with them compared to other words.",45,46
1892,231847004,7.63 0.10 Table 2 : Snippets from MSNBC rated as highly uncivil by humans but with low toxicity score from Perspective.,17,18
1893,231847004,The average toxicity of templates filled in with a given word now provides us with a word-specific incivility score by which we can compare the full set of selected words.,2,3
1894,231847004,The five templates we use are: We use Perspective to predict the toxicity of each template when WORD is substituted with each of the words in our list for analysis.,13,14
1895,231847004,We will consider words to be error triggers if at least one template was judged by Perspective to have toxicity greater than 0.5.,19,20
1896,231847004,"Sub-error triggers are words for which all five templates had toxicity lower than 0.5, but their average toxicity was markedly higher than that for other words.",12,13
1897,231847004,"Sub-error triggers are words for which all five templates had toxicity lower than 0.5, but their average toxicity was markedly higher than that for other words.",20,21
1898,231847004,"The average template toxicity for offensive words is 0.48, compared to 0.11 for the 2,606 nonoffensive words in the list we analyzed.",3,4
1899,231847004,"Of the 65 offensive words, 54% had at least one template with toxicity greater than 0.5.",14,15
1900,231847004,It produces toxicity scores below 0.5 for about half of the offensive words.,2,3
1901,231847004,"Overall, 89% of the offensive words meet either the error triggers or sub-error triggers criteria, confirming that these ranges of toxicity are the appropriate ones in which we should focus our attention in search of words that may have an unusually high association with toxicity.",25,26
1902,231847004,"Overall, 89% of the offensive words meet either the error triggers or sub-error triggers criteria, confirming that these ranges of toxicity are the appropriate ones in which we should focus our attention in search of words that may have an unusually high association with toxicity.",49,50
1903,231847004,They are ones for which Perspective over-predicts toxicity.,9,10
1904,231847004,Error Trigger Words for Perspective We consider a word to be an error trigger if at least one of the templates has a toxicity score of 0.5 or greater from Perspective.,23,24
1905,231847004,"Many of these identities, like gay and Muslim, were known triggers of incorrect toxicity prediction, and Perspective was specially altered to address this problem (Dixon et al.,",15,16
1906,231847004,Perhaps this is why words broadly related semantically are associated with toxicity.,11,12
1907,231847004,Sub-Error Triggers Sub-error triggers of incivility are words that are not offensive and for which Perspective returns a toxicity score below 0.5 for each of the five templates when the word is plugged in the template.,22,23
1908,231847004,"To establish a reference point for the expected average template civility of non-offensive words, we sample 742 words that appeared in at least 10 speaker turns (i.e., were fairly common) and ap-peared in speaker turns in the news shows that received low toxicity predictions from Perspective, below 0.15.",50,51
1909,231847004,We define sub-error triggers to be those whose average template toxicity score is two standard deviations higher than the average in the reference list.,12,13
1910,231847004,The second-person pronouns are likely spuriously associated with toxicity due to overrepresentation in direct toxic comments directed to other participants in the conversation.,10,11
1911,231847004,"Similarly, the association of female pronouns with toxicity is likely due to the fact that a large fraction of the indirect toxic comments online are targeted to women.",8,9
1912,231847004,Conclusion The work we presented was motivated by the desire to apply off-the-shelf methods for toxicity prediction to analyse civility in American news.,19,20
1913,225040675,Both researchers and social platforms have been focused on developing models to detect toxicity in online communication for a while now.,13,14
1914,225040675,"The low entry threshold and relative anonymity of the Internet have resulted not only in the exchange of information and content but also in the rise of trolling, hate speech, and overall toxicity 2 .",34,35
1915,225040675,"They tend to classify comments mentioning certain commonly harassed identities (e.g. containing words such as woman, black, jew or женщина, черный, еврей) as toxic, while the comment itself may lack any actual toxicity.",39,40
1916,225040675,Identity terms of frequently targeted social groups have higher toxicity scores since they are found more often in abusive and toxic comments than terms related to other social groups.,9,10
1917,225040675,Inappropriately high toxicity scores of terms related to specific social groups can potentially negate the benefits of using machine learning models to fight the spread of hate speech.,2,3
1918,225040675,"In 66 this paper, our main goal is to reduce the false toxicity scores of non-toxic comments that include identity terms empirically known to introduce model bias.",13,14
1919,225040675,2 Related Work Hate Speech Detection in Russian Little research has been done on the automatic detection of toxicity and hate speech in the Russian language.,18,19
1920,225040675,"2020) proposed a model that learns to predict the toxicity of a comment, as well as the protected identities present, in order to reduce unintended bias as shown by an increase in Generalized Mean of Bias AUCs.",10,11
1921,225040675,"In our setup, the loss from an extra classifier head is weighted equal to the loss from the toxicity classifier.",19,20
1922,225040675,"As the loss function for Multi-Task learning, we used the average loss score between two tasks: predicting the toxicity score, and predicting the protected identity class.",22,23
1923,225040675,"We showed that, for our dataset and for the benchmark from (Smetanin, 2020) , adding an extra task of predicting the class of a protected identity can indeed improve the quality of toxicity classification in terms of reducing unintended bias.",36,37
1924,231583180,"In future work, we intend to test the approach not only for repetition, but also for various other metrics, such as the toxicity level or bias of generated text.",25,26
1925,231861515,Experimenting with the largest toxicity detection dataset to date (Civil Comments) our model generates sentences that are more fluent and better at preserving the initial content compared to earlier text style transfer systems which we compare with using several scoring systems and human evaluation.,4,5
1926,231861515,"Consequently, we limit our setting to the unsupervised case where the comments are only annotated in attributes related to toxicity, such as the Civil Comments dataset (Borkan et al.,",20,21
1927,231861515,We summarize our investigations with the following research question: RQ: Can we fine-tune end-to-end a pre-trained text-to-text transformer to suggest civil rephrasings of rude comments using a dataset solely annotated in toxicity?,45,46
1928,231861515,"Here, we apply attribute transfer to a large dataset annotated in toxicity, but we also use the Yelp review dataset from Shen et al. (",12,13
1929,231861515,"Experiments Datasets We employed the largest publicly available toxicity detection dataset to date, which was used in the 'Jigsaw Unintended Bias in Toxicity Classification' Kaggle challenge.",8,9
1930,231861515,"Each of these comments was annotated by crowd raters (at least 3 each) for toxicity and toxicity subtypes (Borkan et al.,",16,17
1931,231861515,"Each of these comments was annotated by crowd raters (at least 3 each) for toxicity and toxicity subtypes (Borkan et al.,",18,19
1932,231861515,2019) toxicity classifier on the Civil Comments dataset.,2,3
1933,231861515,"5 Eventually, we created X T (respectively X C ) with sentences whose system-generated toxicity score (using our BERT classifier) is greater than 0.9 (respectively less than 0.1) to increase the dataset's polarity.",18,19
1934,231861515,The test ROC-AUC of the toxicity classifier is 0.98 with a precision of 0.95 and a recall of 0.38.,7,8
1935,231861515,"We employed a pre-trained BERT as our toxicity classifier, fine-tuned on the Civil Comments dataset (see Section 4.1).",9,10
1936,231861515,"Interestingly, our model is able to add toxicity in civil comments as shown by the examples in the Appendix Table 10 .",8,9
1937,231861515,The nature of large datasets labeled in toxicity and the lack of incentives for crowd-sourcing civil rephrasing annotation makes it expensive and difficult to train systems in a supervised framework.,7,8
1938,231861515,"In our case, evaluating whether the attribute is actually transferred requires to train an accurate toxicity classifier.",16,17
1939,231861515,"Algorithm 1: CAE-T5 training Input :T5's pre-trained parameters θ 0 , unpaired dataset labelled in toxicity X = X T ∪ X C Output :CAE-T5's fine-tuned parameters θ T for step τ ∈ [1; T ] do if τ %2 == 0 then Sample a mini-batch x of sentences in X T else Sample a mini-batch x of sentences in X C end θ ← θτ−1 θ ← θτ−1 xDAE ← f θ (η(x), α(x)) xCC ← f θ (f θ(x, ᾱ(x)), α(x)) DAE ← H(x, xDAE ) CC ← H(x, A.3 Appen settings Figure 3 and Figure 4 detail the guidelines we wrote on the crowdsourcing website Appen 11 , when we asked human crowd-workers to rate automatic rephrasings and to rephrase toxic comments.",22,23
1940,236478314,"For instance, when training on English the model would at times invent ""toxicity cardiaque "" for the French test set instead of extracting ""toxicité cardiaque"".",14,15
1941,236478036,We present experiments with two text classification datasets annotated with demographic information: the Trustpilot Corpus (sentiment) and CivilComments (toxicity).,22,23
1942,236478036,"We evaluate FSP across two architectures, two pruning strategies and two datasets, including multilingual sentiment classification and English toxicity classification.",20,21
1943,236478036,"2019) , 3 which contains comments annotated for toxicity, for the purpose of hate speech detection.",9,10
1944,236478036,"4 Likewise for CivilComments, we threshold comments with a toxicity rating > 0.5 as toxic, and otherwise label them as a non-toxic.",10,11
1945,236478036,"We measure group disparity, using fairness sensitivity to pruning, on the Trustpilot Corpus, a sentiment classification dataset covering 3 languages, as well as CivilComments, a toxicity classification dataset, for both feed-forward and recurrent neural networks.",30,31
1946,250390650,"2021) provide the only available resource of human-written or machine-generated responses with annotations, but the original task does not involve assessing counternarrative quality but rather classifying the contextual toxicity of dialogue re-  3 Counterspeech and Microintervention Generation Task Formalization Counternarrative generation can be viewed as a type of conditional or constrained text generation, in which the output is expected to oppose the input text.",34,35
1947,13696741,"2009) , highlighting rationale phrases in Both cohorts showed signs of optic nerve toxicity due to ethambutol.",14,15
1948,233365230,"For example, sentiment analysis, topic modeling, toxicity classification, and other language modeling techniques have become common in interactive user-facing systems.",9,10
1949,233365230,"However, the same technology has also incorrectly discovered a positive correlation between identity terms containing information on race or sexual orientation (e.g., the phrase ""I am a gay black woman"" received a high toxicity score) (developers.google.com).",38,39
1950,52194540,"Google and Jigsaw developed a tool called Perspective 7 that measures the ""toxicity"" of comments.",13,14
1951,52194540,The tool is published as an API and gives a toxicity score between 0 and 100 using a machine learning model.,10,11
1952,53593090,Language toxicity identification presents a gray area in the ethical debate surrounding freedom of speech and censorship.,1,2
1953,53593090,"In response, we focused on training a multi-label classifier to detect both the type and level of toxicity in online content.",20,21
1954,53593090,"Furthermore, building a representative and high volume annotated dataset of social media contents for multiple types of toxicity can be exhaustive.",18,19
1955,53593090,"Our research gave life to a language toxicity identification tool, which will be presented alongside this paper.",7,8
1956,226283547,"In the broad area of abusive language detection, there exists several other datasets collected and annotated for cyberbullying, toxicity, aggression and so on (we would not discuss those in-depth as they are out of the scope of this work).",20,21
1957,236459882,"While several toxicity detection datasets (Wulczyn et al.,",2,3
1958,236459882,"In fact, highlighting toxic spans can assist human moderators who often deal with lengthy comments, and who prefer attribution instead of just a system-generated unexplained toxicity score per post. *",29,30
1959,236459882,"We apply the trained model on the SemEval 2021 Task 5: Toxic Spans Detection test set comments to predict their toxicity, then, we use the LIME technique to explain the predictions (Figure 3 ).",21,22
1960,236459882,"By training the linear support vector machine classifier on the SemEval 2021 Task 5: Toxic Spans Detection test set, we guarantee that the model accurately predicts the toxicity of its comments with precision, recall, f-score, and accuracy of 1 (the model correctly predict the toxicity of all 2000 reviews in the test set).",29,30
1961,236459882,"By training the linear support vector machine classifier on the SemEval 2021 Task 5: Toxic Spans Detection test set, we guarantee that the model accurately predicts the toxicity of its comments with precision, recall, f-score, and accuracy of 1 (the model correctly predict the toxicity of all 2000 reviews in the test set).",52,53
1962,236459882,"In fact, if the model misclassifies the toxicity of the comments, the LIME explanations will be inaccurate since the latter will explain wrong predictions.",8,9
1963,247958065,"Specifically, we analyze the performance of classifiers trained on the Wiki and Founta datasets (expected to detect general toxicity and abuse) on COVID-related anti-Asian racism data.",20,21
1964,247958065,"As mentioned before, while EA-positive fits under the definition of 'toxicity' in Wiki-positive, the definition of EA-negative is inconsistent with the definition of Wiki-negative.",14,15
1965,248780439,"2019) and toxicity classification of Wikipedia Talks comments (Dixon et al.,",3,4
1966,248780439,"2) Toxicity Classification For toxicity classification, we use the WIKI dataset, which consists of just under 130,000 comments from the online forum Wikipedia Talks Pages (Dixon et al.,",5,6
1967,248780439,"2018 ) study of toxicity classification bias is an extensive evaluation set composed of 89,000 templates such as ""[IDENTITY] is [ATTRIBUTE] ,"" where the attributes include both positive (for non-toxic examples) and extremely negative words (for toxic   examples).",4,5
1968,235097625,"We introduce a new human-and-model-in-the-loop framework for evaluating the toxicity of such models, and compare a variety of existing methods in both the cases of non-adversarial and adversarial users that expose their weaknesses.",19,20
1969,235097625,"2) A novel method that directly ""bakes in"" toxicity-awareness to the generative model during training by modifying the target responses to incorporate safe responses to offensive input.",11,12
1970,235097625,"2019a) strictly focuses on detection of toxicity in human-generated utterances through several rounds of humans attempting to ""break"" a toxicity classifier, without addressing generation.",7,8
1971,235097625,"2019a) strictly focuses on detection of toxicity in human-generated utterances through several rounds of humans attempting to ""break"" a toxicity classifier, without addressing generation.",24,25
1972,235097625,"2019) showed that several contain correlations between surface markers of African American English and toxicity, and propose race and dialect priming as a way to mitigate this.",15,16
1973,236034557,"While issues of toxicity can perhaps be treated similarly to the pre-training data case (e.g. safety classifiers), fact checking is a separate area with ongoing work, e.g. Hassan et al. (",3,4
1974,248780380,Nozza (2021) studies multilingual toxicity classification and finds that models misinterpret nonhateful language-specific taboo interjections as hate speech in some languages.,6,7
1975,235097308,"A report from L1ght 3 , a company that specializes in measuring online toxicity, suggests that amid the growing threat of the coronavirus, there has been a 900% growth in hate speech towards China and Chinese people on Twitter since February 2020.",13,14
1976,248779933,"Here, we examine the interactions between political users to probe the mechanisms behind this toxicity.",15,16
1977,248779933,"Experimental Setup To test for affiliation-based hostility, we construct a mixed-effect linear regression model to estimate the toxicity of a reply to a comment.",22,23
1978,248779933,"We include a random effect for the subreddit in which the discussion takes place, which controls for the relative levels of toxicity in different subreddits (Rajadesingan et al.,",22,23
1979,248779933,"Finally, as toxic conversations may lead to more toxicity, we include a linear factor for the parent comment's toxicity.",9,10
1980,248779933,"Finally, as toxic conversations may lead to more toxicity, we include a linear factor for the parent comment's toxicity.",21,22
1981,248779933,"We follow the approach of previous work studying political toxicity on Reddit (Rajadesingan et al.,",9,10
1982,248779933,"To measure toxicity, we fine-tune a BERT (Devlin et al.,",2,3
1983,248779933,"Our toxicity model follows the setup of the topperforming SemEval system on the same data (Liu et al.,",1,2
1984,248779933,"We validate the toxicity scores on the Reddit data by evaluating the model on 150 manually annotated comments, which resulted in a 0.88 weighted-F1, indicating the model generalizes to toxicity on Reddit.",3,4
1985,248779933,"We validate the toxicity scores on the Reddit data by evaluating the model on 150 manually annotated comments, which resulted in a 0.88 weighted-F1, indicating the model generalizes to toxicity on Reddit.",33,34
1986,248779933,"For analysis, we use the model to assign each comment a toxicity score between 0 and 1.",12,13
1987,248779933,Results Regressing on the factors contributing to toxicity in replies shows three main findings (Figure 3 ; full regression in Appendix Table 9 ).,7,8
1988,248779933,"First, consistent with prior work, we find that controlling for subreddit-specific levels of toxicity, discussion in political communities is much more toxic, suggesting that these topics are a primary source of increased hostility.",17,18
1989,248779933,"Second, we find substantial affiliation-based toxicity, with increased toxicity particularly for interactions between cross-affiliation users.",8,9
1990,248779933,"Second, we find substantial affiliation-based toxicity, with increased toxicity particularly for interactions between cross-affiliation users.",12,13
1991,248779933,"Surprisingly, this increased toxicity is not due to an explicit flair signal; when users are commenting in a community where the flair is visible-which can include mixed-affiliation subreddits-users receive less toxic replies.",4,5
1992,248779933,"In three studies of political users, we show that (i) political users themselves drive hostility on the platform-with conservative users being the recipients of more toxicity, (ii) a small-but-very-active group of provocateurs simultaneously declare different affiliations and are a notable source of toxicity and controversiality on the platform, and (iii) changing political affiliation can be predicted, but performance varies considerably by user type.",30,31
1993,248779933,"In three studies of political users, we show that (i) political users themselves drive hostility on the platform-with conservative users being the recipients of more toxicity, (ii) a small-but-very-active group of provocateurs simultaneously declare different affiliations and are a notable source of toxicity and controversiality on the platform, and (iii) changing political affiliation can be predicted, but performance varies considerably by user type.",56,57
1994,248779933,Parent Toxicity: A floating number indicating the toxicity of the parent comment. •,8,9
1995,247940075,We en-riched the evaluation of this subset with the toxicity and the syntactic metrics.,11,12
1996,247940075,"It also achieves the second best results on BLEU-3, maximum syntactic depth and number of sentences, and the best results on toxicity and specificity (2.880) indicating the ability to produce complex, suitable, focused and diverse CNs.",23,24
1997,247940075,"It is also the best performing for specificity, maximum syntactic depth and number of sentences, and the second best for average syntactic depth and toxicity.",26,27
1998,247940075,"It obtains the second-highest scores in most of the human evaluation metrics and the lowest in toxicity, and it reaches a reasonable specificity score.",18,19
1999,247940075,"In almost all the cases, we observe that the stochastic decoding mechanisms perform better on syntactic and diversity metrics and on toxicity, while for the human evaluation metrics BS tends to be the best, except for specificity.",22,23
2000,247940075,"BART performs well with the stochastic decoding methods, in particular: Top p for overlap, diversity, syntactic metrics, and grammaticality; Top k for overlap metrics and toxicity, whereas Top pk is the best decoding approach on human evaluation and RR, and the second best on ROUGE and BLEU-1.",31,32
2001,225067001,It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model.,11,12
2002,232257853,Conclusion Ensuring that multilingual models are robust to both natural and adversarial code-mixing is important in today's increasingly multilingual world if they are to allow their target users to fully express themselves in human-machine conversations and to defend against adversarial users attempting to evade toxicity/misinformation detection systems.,49,50
2003,232257853,"On the other, the same adversarial attacks can be used by malicious actors to bypass toxicity/misinformation detection systems.",16,17
2004,248779886,"We asked users to test the Perspectives API toxicity model for content moderation, as an example of an application that can impact the general public in group-specific ways.",8,9
2005,248779886,"We have demonstrated AdaTest's effectiveness on classification models (sentiment analysis, QQP, toxicity, media selection, task detection), generation models (GPT-2, translation), and per-token models (NER), with models ranging from well-tested production systems to brand new applications.",15,16
2006,247222603,"The blood-brain barrier penetration (BBBP), clinical trail toxicity (ClinTox), HIV replication inhibition (HIV), and side effect resource (SIDER) datasets are classification tasks in which molecule SMILES strings and their binary labels are provided in each task.",12,13
2007,238856730,"2020b) , but are still prone to uttering problematic language, e.g., displaying toxicity or bias, or agreeing with offensive statements (Xu et al.,",15,16
2008,186206323,"2017) created three datasets from the English Wikipedia Talk Page: one annotated for personal attacks, one for toxicity, and one for aggression.",20,21
2009,186206323,2017) then improved the accuracy on the toxicity and personal attack datasets using RNNs.,8,9
2010,3920034,"For instance, to highlight the potential toxicity of coffee, Thesaurus Rex suggests comparisons with alcohol, tobacco or pesticide, as all have been categorized as toxic substances on the web.",7,8
2011,236460071,A lot of toxicity is regularly posted on these social media platforms.,3,4
2012,236460071,It is humanly impossible for these platforms to check each and everything posted by the users for hate or toxicity.,19,20
2013,236460071,This task tries to identify these spans that can be used further to provide insights into a generic text toxicity score.,19,20
2014,236460071,"There, however, exist systems that can score the toxicity of a full text, sentence or comment like (Zhang and Luo, 2018) , but they don't identify the exact part of that sentence that is toxic.",10,11
2015,236460071,They have morphed the origi-nal aim of the NER type tasks to train a model capable of identifying toxicity in a text that is already classified as toxic as a whole.,20,21
2016,236460307,The problem with the above approach is that it doesn't give moderators much knowledge about the reason for a sentence's toxicity.,22,23
2017,236460307,Highlighting toxic spans can help human moderators who frequently deal with long comments and prefer attribution rather than just an unexplained toxicity score.,21,22
2018,236460307,The task is concerned with developing systems that can recognise spans that contribute to the text's toxicity.,17,18
2019,236460307,"Undoubtedly, the hardest part is to identify spans that can account for the toxicity of the sample.",14,15
2020,236460307,"2021) that is, filter the most toxic samples (toxicity ≥ 0.80 ) from the Civil Comments dataset and select a random set of 10,000 samples.",11,12
2021,236460307,This process allowed the silver data to have similar toxicity distribution as the gold data.,9,10
2022,236459777,"2019) , which already comprises post-level toxicity annotations.",9,10
2023,236459777,"The task extends the prior work by identifying spans that make a text toxic, which can better explain why posts are offensive rather than just giving a system-generated unexplained toxicity score.",32,33
2024,236459974,Detection of toxic spans -detecting toxicity of contents in the granularity of tokens -is crucial for effective moderation of online discussions.,5,6
2025,236459974,"Since manual moderation is not feasible for the gigantic amount of textual data, automated toxicity detection has received significant attention with numerous datasets being released in recent years (Pavlopoulos et al.,",15,16
2026,236459974,"However, most of the existing work on toxicity detection labels the entire comment as toxic or non-toxic and does not provide information about which specific part of the comment is toxic.",8,9
2027,236459974,"In practice, human moderators (e.g., news portals moderators) can benefit from information on which character indices of the part of the comment that is toxic instead of just a system-generated unexplained toxicity score per post.",37,38
2028,236459974,"Sentence-level Labelled Data We used five other datasets which consisted of only sentence-level labels: • Jigsaw/Conversation AI toxic comment classification challenge dataset 2 -Composed of Wikipedia's talk page edits, it labels 223,549 posts into zero or more categories of toxicity (toxic, severely toxic, obscene, threat, insult, and identity hate).",48,49
2029,236459974,"This can be attributed to the comparatively higher toxicity of the sentence-level labeled datasets when compared with the contest dataset, which makes the models more expert on detecting highly toxic spans.",8,9
2030,236459974,"Out of the 52,640 toxic samples, only 7,629 samples were given a toxicity score of more than 0.5, thus greatly reducing the size of the additional dataset for token classification.",13,14
2031,236459974,"For example, the highlighted parts were given a toxicity score of more than 0.9 in the following sample: admins suck!!",9,10
2032,236459974,"5 Conclusion and Future Work In this paper, we proposed two solutions to improve the baseline transformer model fine-tuning for the span toxicity detection task.",25,26
2033,236460261,"Work done in (Brassard-Gourdeau and Khoury, 2019) explores different aspects of sentiment detection and their correlation to toxicity. (",22,23
2034,236460261,2020) covers the effect of context on toxicity. (,8,9
2035,236460261,2020) uses BERT and FastText for toxicity detection. (,7,8
2036,236459824,"With such a huge exchange of ideas, there is bound to be some toxicity within the comments.",14,15
2037,236459824,We then create output columns -one column for each 100-D word vector -and fill the columns with the toxicity labels from earlier.,20,21
2038,236459824,"The final table used for training and scoring contains columns for the original comment, the labelled comment, the cleaned comment, 100 dimension embeddings for each word in the comment and the rest of the columns zero-padded to the longest value, and toxicity labels for each word in the comment, also zero-padded to the longest value.",47,48
2039,236459824,"Out of all of the text, 93% of the words are non-toxic words, or in this case, words to be labelled negative for toxicity.",29,30
2040,232478589,"Firstly, because of the intrinsic subjectivity of toxicity, and secondly, due to toxicity not always coming from single words like insults or offends, but sometimes from whole expressions formed by words that may not be toxic individually.",8,9
2041,232478589,"Firstly, because of the intrinsic subjectivity of toxicity, and secondly, due to toxicity not always coming from single words like insults or offends, but sometimes from whole expressions formed by words that may not be toxic individually.",15,16
2042,232478589,"Following this idea of focusing on both single words and multi-word expressions, we study the impact of using a multi-depth DistilBERT model, which uses embeddings from different layers to estimate the final per-token toxicity.",41,42
2043,232478589,Related work Toxicity The task in which we are participating is not the first one to focus on text toxicity.,19,20
2044,232478589,"For this reason, we have cleaned the data using three simple steps and following the idea of toxicity coming from complete words but not from single characters.",18,19
2045,232478589,On one side this confirms our hypothesis that toxicity comes from words or expressions but not from characters.,8,9
2046,232478589,"In this step, we also use the information of the already-cleaned toxic offsets to create a per-token binary label regarding its toxicity.",26,27
2047,232478589,"Using all 6 also provides good results, which may imply that the first layer's output is also quite informative for this task in which words themselves already help predicting their toxicity.",32,33
2048,232478589,"People don't buy that poorly built Russian houses... In other cases, our system identifies toxicity when it is not annotated, although under our perspective the prediction seems correct.",17,18
2049,232478589,"Given that it seems there are more comments against some specific ethnic groups than others, the system associates certain racial references with racism and thus with toxicity.",27,28
2050,232478589,"Conclusion In this work, we have presented a solution for the SemEval-2021 Task 5: Toxic Spans Detection competition, which is a challenging task due to the subjectivity of toxicity and the requirement of context knowledge.",31,32
2051,236460217,"2013) , GloVe (Pennington et To obtain if a word is toxic or not after tokenization, we split the text by space and punctuation and map the indices of toxicity to corresponding words.",32,33
2052,236460217,"In false positive examples marked as underline, the words ""ass"", ""sucks"", and ""moron"" are predicted as toxic words where there exists no toxicity in these sentences.",31,32
2053,236460217,The position of words in sentence was not detected as toxicity by our Bi-LSTM-CRF model.,10,11
2054,236460217,This will cause difficulty for model training to detect toxicity.,9,10
2055,236460146,"Two models attempted to predict binary word toxicity based on ngram sequences, whilst 3 categorical span based models were trained to predict toxic token labels based on complete sequence tokens.",7,8
2056,236460146,"Background Toxic span detection is a development of binary toxicity detection which has garnered recent attention, in the form of shared-tasks and datasets (Wulczyn et al.,",9,10
2057,236460146,"Target Span detection asks systems to detect which specific series of characters are toxic, irrespective of the text's overall toxicity.",21,22
2058,236460146,"In section 5.3 on error analysis we compare model scores using a binary word level representation of toxicity, that scores both positive and negative prediction.",17,18
2059,236460146,"Motivation We intended for the word based models to learn local features in the tokens nearest the target word, and for the span based to learn the overall features that affected sub and multi word toxicity.",36,37
2060,236460146,The binary word level models treated the task as word toxicity prediction based on a sequences of words before and after the target word.,10,11
2061,236460146,The target word toxicity was represented as a binary value.,3,4
2062,236460146,"The words before and after the target word were used as model features, and the target word toxicity was represented as a binary value (Devlin et al.,",18,19
2063,236460146,Ensemble Model A Bidirectional LSTM model was used to predict token toxicity based on tokenised word features and component model predictions.,11,12
2064,236460146,"The model used transformer style feature representations to predict a sequence of categorical representations for token toxicity, as described in section 4.1.",16,17
2065,236459808,Many new developments to detect and mitigate toxicity are currently being evaluated.,7,8
2066,236459808,We are particularly interested in the correlation between toxicity and the emotions expressed in online posts.,8,9
2067,236459808,"While toxicity may be disguised by amending the wording of posts, emotions will not.",1,2
2068,236459808,"Therefore, we describe here an ensemble method to identify toxicity and classify the emotions expressed on a corpus of annotated posts published by Task 5 of Se-mEval 2021-our analysis shows that the majority of such posts express anger, sadness and fear.",10,11
2069,236459808,"Our method to identify toxicity combines a lexicon-based approach, which on its own achieves an F1 score of 61.07%, with a supervised learning approach, which on its own achieves an F1 score of 60%.",4,5
2070,236459808,"Along with cyberbullying, other forms of verbal abuse employed on social media, such as online harassment and hate speech, are now being collectively referred to as toxicity in language (Mohan et al.,",29,30
2071,236459808,We are interested in developing algorithms to recognise toxicity and measure its impact on the sentiment expressed.,8,9
2072,236459808,"Most of the data available to investigate toxicity classify whole comments or documents (Wulczyn et al.,",7,8
2073,236459808,"Emotions may be able to identify toxicity, regardless of wordings and spellings.",6,7
2074,236459808,"Thus, we dedicate part of this study to measure the correlation between toxicity and emotions.",13,14
2075,236459808,"Background The existing literature on toxicity focuses on two main aspects: the compilation and annotation of corpora for research purposes (Fortuna et al.,",5,6
2076,236459808,"At present, the detection of toxicity is largely based on state-of-the-art natural language processing techniques, typically involving machine learning.",6,7
2077,236459808,"Various other lexicons, handcrafted by domain experts who specialise on the identification of toxicity have been published too-for example, Textgain's Profanity and Offensive Words lexicon (De Smedt et al.,",14,15
2078,236459808,"In an attempt to mitigate toxicity and promote work on this area, the research community has released a number of annotated datasets for investigating different forms of toxicity (Waseem and Hovy, 2016; Waseem, 2016; Golbeck et al.,",5,6
2079,236459808,"In an attempt to mitigate toxicity and promote work on this area, the research community has released a number of annotated datasets for investigating different forms of toxicity (Waseem and Hovy, 2016; Waseem, 2016; Golbeck et al.,",28,29
2080,236459808,"Overall, toxicity detection and classification lacks a consistently labelled standard dataset for comparative evaluation (Schmidt and Wiegand, 2017) .",2,3
2081,236459808,"System Overview Although machine learning technology is being widely employed to detect toxic text automatically, the use of a lexicon to identify and prevent toxicity in social media still constitutes a valuable approach.",25,26
2082,236459808,"Indeed, the number of lexicons specialised on the detection of profanity, offensive speech and toxicity in general has grown steadily in recent times (De Smedt et al.,",16,17
2083,236459808,"words.txt) have a negative impact on the performance of toxicity detection, even if it is only by a small margin.",10,11
2084,236459808,"While our lexicon-based approach was considerably useful to identify toxicity, as we will show in Section 5, we recognise the value of machine learning approaches.",11,12
2085,236459808,"The success of the Perspective project undertaken by Google and Jigsaw to rate toxicity by means of machine learning (Jain et al.,",13,14
2086,236459808,"2018) , as well as the impact of the Perspective API to mitigate toxicity using machine learning certainly deserve our attention.",14,15
2087,236459808,"Therefore, we opted to employ spaCy (Explosion, 2021b), an open-source software library for natural language processing, to develop a supervised learning approach for the identification of toxicity.",34,35
2088,236459808,"We are interested in the identification of emotions expressed in text, because concealing emotions may be harder than disguising toxicity.",20,21
2089,236459808,"Clearly, fear, sadness and anger-the three emotions combined together-are more likely to occur than happiness and surprisethe two emotions combined together-which may characterise the toxicity of the dataset.",32,33
2090,236459808,"Conclusions In this paper, we have described the creation of a lexicon of toxic words and a supervised learning approach to identify toxicity in online posts.",23,24
2091,236459808,We have also explored the relationship between emotions and toxicity.,9,10
2092,236459808,"Although our study is still in progress, preliminary results indicate that there exists a correlation between emotions such as sadness and fear and toxicity.",24,25
2093,235248423,"This paper describes the system proposed by team Cisco for SemEval-2021 Task 5: Toxic Spans Detection, the first shared task focusing on detecting the spans in the text that attribute to its toxicity, in English language.",34,35
2094,235248423,"Future work includes independently incorporating both post level and sentence level context for determining the toxicity of a word, and also collating a dataset with toxic spans comprising of a healthy mixture of simple cuss words (which can always be attributed as toxic independant of the context) and words for which the toxicity of the word depends on the context in which it appears, thereby making better systems towards contextual toxic span detection.",15,16
2095,235248423,"Future work includes independently incorporating both post level and sentence level context for determining the toxicity of a word, and also collating a dataset with toxic spans comprising of a healthy mixture of simple cuss words (which can always be attributed as toxic independant of the context) and words for which the toxicity of the word depends on the context in which it appears, thereby making better systems towards contextual toxic span detection.",55,56
2096,236460342,"The first phase pretrains the model with heuristically-created spans, gathered from Reddit comments labeled for their toxicity.",19,20
2097,236460342,2016) on a toxicity classification task.,4,5
2098,236460342,The API returns a continuous score reflecting the degree of a comment's toxicity.,13,14
2099,236460342,This toxicity score is then converted into a binary label to use in training a LIME model to generate rationales for why a comment is (or is not) toxic.,1,2
2100,236460342,"This process led to a labeled dataset of 288.5M comments with binary toxicity labels, of which 9.4% were labeled toxic.",13,14
2101,236460342,"Here, we use a simple logistic regression (LR) model trained on TF-IDF features and use LIME to generate a rationale of the classifier's decision which identifies which words are contributing to the toxicity decision.",38,39
2102,236460342,They could be used in a toxicity-neutral way in many contexts.,6,7
2103,236460342,"Through comparing silver data with the gold data, we find the toxicity of some words is influenced by the broader linguistic environment.",12,13
2104,250391037,2021) proposed the use of a fine-tuned BERT model to detect veiled toxicity.,15,16
2105,237532529,"It is possible that MOVER is used for malicious purposes, since it does not explicitly filter input sentences with toxicity, bias or offensiveness.",20,21
2106,227231489,"In a related Kaggle competition in 2019, Jigsaw published a dataset for identifying toxicity and minimising bias in online comments (Jigsaw, 2019) .",14,15
2107,227231489,This dataset was created with the aim of reducing unintended bias in toxicity classification as a result of identity mentions.,12,13
2108,227231489,"The data has been labelled with identity mentions, such as Muslim, Gay or Black, and a toxicity score (TARGET) that represents the faction of human annotators who believe the post is toxic.",19,20
2109,248572126,"Recent work has shown that AI-driven abusive language or toxicity detection models disproportionately flag and penalize content that contains markers of identity terms even though they are not toxic or abusive (Gray and Stein, 2021; Haimson et al.,",11,12
2110,248572126,"3 Our work builds on recent literature that investigates the performance and misclassification rate of toxicity or hate speech detection models on test sets containing identity terms (Dixon et al.,",15,16
2111,233365150,"4, 5, 6 Each dataset contains different annotations, including comments rated on a scale of toxicity, comments labelled for hateful speech and abuse, comments labeled for constructiveness and tone, etc.",18,19
2112,233365150,We used the constructiveness and toxicity labels and flagged comments whenever the toxicity level was toxic or very toxic and not constructive.,5,6
2113,233365150,We used the constructiveness and toxicity labels and flagged comments whenever the toxicity level was toxic or very toxic and not constructive.,12,13
2114,233365150,"It contains annotations for attack, aggression and toxicity.",8,9
2115,220665491,API's like Perspective 2 have been developed to detect content toxicity using machine learning models.,11,12
2116,220665491,"Sentiment based approaches (Brassard-Gourdeau and Khoury, 2019) have also been used to detect toxicity.",18,19
2117,250391045,"However, growth in popularity also brings some challenges, including the toxicity associated with the content posted by users.",12,13
2118,250391045,"the phrase ""has no brain"" and the slang word ""tool"" are two offensive spans responsible for the toxicity of the text.",21,22
2119,250391045,The main limitation of these works is that they cannot recognize the spans in the text that are responsible for the toxicity of the text. (,22,23
2120,250391014,5 colab.research.google.com/ the community models pretrained on sentiment or toxicity language data present poor performance on the PCL data compared to the base model of RoBERTa.,9,10
2121,250391014,"6 Overall, for each model in the development stage, the difference between precision and recall is not vast, except for the tuned RoBERTa model and the toxicity model of RoBERTa.",29,30
2122,250391014,And the community models pretrained on relevant data such as sentiment and toxicity data turn out to be too specialized in their own task thus resulting in poor performance on the PCL data compared to the RoBERTa-base model.,12,13
2123,248524755,Recent research in Natural Language Processing (NLP) has advanced the development of various toxicity detection models with the intention of identifying and mitigating toxic language from existing systems.,15,16
2124,248524755,"Through automatic and human evaluations, we show that our defense is effective at avoiding toxic language generation even against imperceptible toxicity triggers while the generated language fits the conversation in terms of coherency and relevancy.",21,22
2125,248524755,We quantify the toxicity of each candidate attack utterance using either a single toxicity classifier or an ensemble of such classifiers; see Section 2.2 and Appendix A for more information.,3,4
2126,248524755,We quantify the toxicity of each candidate attack utterance using either a single toxicity classifier or an ensemble of such classifiers; see Section 2.2 and Appendix A for more information.,13,14
2127,248524755,"We use the average (for multiple classifiers) or raw (for a single classifier) output probability scores obtained by the toxicity classifiers, which we refer to as the toxicity score x i for example i, and select the final attack utterance amongst the n candidate adversarial examples considering three selection criteria.",23,24
2128,248524755,"We use the average (for multiple classifiers) or raw (for a single classifier) output probability scores obtained by the toxicity classifiers, which we refer to as the toxicity score x i for example i, and select the final attack utterance amongst the n candidate adversarial examples considering three selection criteria.",32,33
2129,248524755,"Thus, in UTSC-1, we select the most toxic utterance among all generated attack utterances according to toxicity scores from toxicity classifiers as our final attack utterance (i.e., arg max i∈[n] {x i }).",18,19
2130,248524755,"Thus, in UTSC-1, we select the most toxic utterance among all generated attack utterances according to toxicity scores from toxicity classifiers as our final attack utterance (i.e., arg max i∈[n] {x i }).",21,22
2131,248524755,"For UTSC-2, we first apply a threshold T to toxicity scores of the candidate utterances and label the utterances above this threshold as toxic.",10,11
2132,248524755,"Next, from the pool of all toxic utterances, we select the utterance with the lowest toxicity score (i.e., arg min i∈[n] {x i |x i ≥ T }).",17,18
2133,248524755,Adversary runs one toxicity classifier or combination of them (average toxicity score) and assigns a toxicity score to each Ex.,3,4
2134,248524755,Adversary runs one toxicity classifier or combination of them (average toxicity score) and assigns a toxicity score to each Ex.,11,12
2135,248524755,Adversary runs one toxicity classifier or combination of them (average toxicity score) and assigns a toxicity score to each Ex.,17,18
2136,248524755,"Lastly, in UTSC-3 we select the utterance with the lowest toxicity score, i.e., arg min i∈[n] {x i }.",11,12
2137,248524755,"Toxicity Detection Models To determine toxicity of the candidate attack utterances by the adversary, we utilize an ensemble of three different toxicity detection models: Toxic-bert 2 , Perspective API 3 , and Safety classifier (Xu et al.,",5,6
2138,248524755,"Toxicity Detection Models To determine toxicity of the candidate attack utterances by the adversary, we utilize an ensemble of three different toxicity detection models: Toxic-bert 2 , Perspective API 3 , and Safety classifier (Xu et al.,",22,23
2139,248524755,"While using an ensemble of the three models results in the most effective attacks, to ensure that the adversary is not simply overfitting the toxicity detection model but rather forcing the defender to actually generate toxic language, we also study the transferability of these attacks.",25,26
2140,248524755,We allow the adversary to only use one of the toxicity detection models to design its attack.,10,11
2141,248524755,"We then quantify toxicity using the other two toxicity detection methods, not accessed by the adversary.",3,4
2142,248524755,"We then quantify toxicity using the other two toxicity detection methods, not accessed by the adversary.",8,9
2143,248524755,We then discuss how well the attacks transfer to other toxicity detection classifiers.,10,11
2144,248524755,"Unless otherwise mentioned, for the UTSC attacks, the adversary uses an equally weighted ensemble of all three toxicity detection classifiers to chose the final attack utterance.",19,20
2145,248524755,"Thus, results confirm that the toxicity of the attack plays a significant role in attack effectiveness.",6,7
2146,248524755,"Attack Transferability Here, we discuss the transferability of our UTSC-1 attack toward different toxicity detection classifiers.",14,15
2147,248524755,"In Figure 4 , we demonstrate that even if the attacker only uses one of the toxicity detection models (Toxic-bert), it still can force the defender to generate toxic responses according to Perspective API and Safety classifier and have comparable performance to when it uses all the toxicity classifiers.",16,17
2148,248524755,"In Figure 4 , we demonstrate that even if the attacker only uses one of the toxicity detection models (Toxic-bert), it still can force the defender to generate toxic responses according to Perspective API and Safety classifier and have comparable performance to when it uses all the toxicity classifiers.",52,53
2149,248524755,This confirms that the attack is forcing the defender to generate actual toxic language rather than fooling the toxicity classifier.,18,19
2150,248524755,The results for UTSC-1 using other toxicity detection models can be found in Appendix B.1.,6,7
2151,248524755,"With regards to toxicity scores, attacks are rated to have competitive and comparable performances at around 20% effectiveness close to automatic results from Perspective API classifier.",3,4
2152,248524755,"For toxicity, we only have two ratings (toxic and not toxic).",1,2
2153,248524755,"The detection problem is rather straightforward, as the defense can simply run a toxicity classifier on the generated response.",14,15
2154,248524755,Methodology Our defense is based on a two-stage mechanism in which the defender first runs a toxicity detection model on its generated utterance.,18,19
2155,248524755,The first layer aims to detect which tokens in the defender's utterance is making Original Conversation • Defender runs a toxicity detection classifier on its generated utterance and finds out that it was tricked by the adversary.,21,22
2156,248524755,the toxicity detection model to label the utterance as being toxic.,1,2
2157,248524755,We then apply a toxicity classifier on this new utterance.,4,5
2158,248524755,2020) This baseline is also a two-stage approach like ours in which the defender first uses a toxicity classifier to detect if the utterance is toxic or not.,20,21
2159,248524755,"To this end, we expect this baseline to do almost perfectly in terms of avoiding toxic response generation given that the toxicity detection classifier is a good detector; however, in terms of conversational quality it will have worse relevancy and coherency scores compared to our method as shown in our human evaluations.",22,23
2160,248524755,"AMT Experiments We asked AMT workers to evaluate the defense quality according to relevancy and fluency, the coherency of the overall conversation, and the toxicity of the defense utterance.",26,27
2161,248524755,"Transferability to other toxicity detection classifiers: Results in Figure 7 demonstrate that even if the defender is using the interpretability results provided by the Toxic-bert classifier, it can still be effective in reducing toxicity according to Perspective API and Safety classifier on all attacks.",3,4
2162,248524755,"Transferability to other toxicity detection classifiers: Results in Figure 7 demonstrate that even if the defender is using the interpretability results provided by the Toxic-bert classifier, it can still be effective in reducing toxicity according to Perspective API and Safety classifier on all attacks.",37,38
2163,248524755,Transferability when UTSC attack uses different toxicity classifier than what the defender uses in its defense: We also noticed that even if the defender and the attacker do not use the same toxicity detectors the defense can be effective.,6,7
2164,248524755,Transferability when UTSC attack uses different toxicity classifier than what the defender uses in its defense: We also noticed that even if the defender and the attacker do not use the same toxicity detectors the defense can be effective.,33,34
2165,248524755,To see the results of our defense on all the combination of toxicity detectors used by the attacker for its selection criteria refer to Appendix B.1.,12,13
2166,248524755,"Thus, even if the Non Sequitur defense can be really effective in reducing the toxicity as it replaces the toxic utterance with a non-toxic templated sentence, it can create poor conversational experience as also rated by human annotators.",15,16
2167,248524755,"Among the ones that studied toxicity and other ethical considerations (Wallace et al.,",5,6
2168,248524755,"2019) ; Niu and Bansal (2018) studied adversarial attacks on conversational agents; however, their focus was on task oriented dialogue systems and also did not consider toxicity but accuracy as a metric.",31,32
2169,248524755,"We then proposed a defense mechanism that was shown to be effective through various automatic and human evaluations as well as its transferability to human attacks, general generation tasks, and different toxicity classifiers.",33,34
2170,248524755,"Selection Criteria Details in UTSC Attack For selection criteria, we used the average toxicity scores from three different classifiers (Perspective API, Toxic-bert, and Safety classifier) unless otherwise stated in which we either used the score from one toxicity classifier or the average score from two classifiers.",14,15
2171,248524755,"Selection Criteria Details in UTSC Attack For selection criteria, we used the average toxicity scores from three different classifiers (Perspective API, Toxic-bert, and Safety classifier) unless otherwise stated in which we either used the score from one toxicity classifier or the average score from two classifiers.",44,45
2172,248524755,"In addition to toxicity scores, we considered other selection criteria, such as length of the generated attack; however, we saw no significant signal in using the length.",3,4
2173,248524755,"Thus, we focused on using toxicity scores in the main text which as shown in the results play a significant role in attack effectiveness.",6,7
2174,248524755,"Notice that other selection criteria can be considered along with length and toxicity scores, such as perplexity score for fluency or other metrics; however, for this study, we considered these two cases.",12,13
2175,248524755,"Additionally, we report some statistics about toxicity scores of the adversary on the attack utterance as well as defender's toxicity score after the attack for UTSC-1, UTSC-2, and UTSC-3 attacks which can provide additional intuition on how toxic each attack is.",7,8
2176,248524755,"Additionally, we report some statistics about toxicity scores of the adversary on the attack utterance as well as defender's toxicity score after the attack for UTSC-1, UTSC-2, and UTSC-3 attacks which can provide additional intuition on how toxic each attack is.",21,22
2177,248524755,"In Figure 10 , we show that the defender and the attacker do not need to use the same toxicity detection classifiers for the defense to be effective.",19,20
2178,248524755,"In Figure 12 , we show that the defense trans-fers to other toxicity detection classifiers as well not only Toxic-bert for all the different combinations of the attacker toxicity detection classifiers.",14,15
2179,248524755,"In Figure 12 , we show that the defense trans-fers to other toxicity detection classifiers as well not only Toxic-bert for all the different combinations of the attacker toxicity detection classifiers.",32,33
2180,248524755,"Thus, results show that even if the defender is using Toxic-bert to perform the defense, according to both Perspective API and Safety classifiers the amount of toxicity is still decreased after the attack irrespective of what toxicity classifier the attacker is using.",30,31
2181,248524755,"Thus, results show that even if the defender is using Toxic-bert to perform the defense, according to both Perspective API and Safety classifiers the amount of toxicity is still decreased after the attack irrespective of what toxicity classifier the attacker is using.",40,41
2182,248524755,We used all the default thresholds set by the developers for all these toxicity detection classifiers and a threshold value of 0.5 for the Perspective API to detect whether an utterance is toxic or not.,13,14
2183,250390692,"This is in line with existing works showing that disagreement in toxicity annotation is inherent to the task and cannot always be solved through majority voting or adjudication (Aroyo et al.,",11,12
2184,227231646,"They were to be classified based on the toxicity (i.e. toxic, severe toxic, obscene, insult, identity hate, and threat).",8,9
2185,233024836,The task's main aim was to identify spans to which a given text's toxicity could be attributed.,15,16
2186,233024836,"Various toxicity detection datasets (Wulczyn et al.,",1,2
2187,233024836,"In semi-automated settings, a model merely generating a toxicity score for each comment, some of which can be very lengthy, is not of much help to human moderators.",11,12
2188,233024836,The task involves identifying text spans in a given toxic post that contributes towards the toxicity of that post.,15,16
2189,233024836,The task aims to promote the development of a system that would augment human moderators by giving them more insights into what actually contributes to the text's toxicity.,28,29
2190,233024836,"Hence, we extracted 40000 toxic samples from the Civil Comments Dataset, which were labeled with a toxicity score of 0.7 or higher, and used these to perform four iterations of semisupervised model training (Fig.",18,19
2191,233024836,"First of all, the data annotations have many issues, leading to a lower F1 score even though the predicted In some cases, the annotations are not uniform in what toxicity label they assign to the same word over different text samples.",32,33
2192,233024836,Such a case arose when the annotators had difficulty in attributing toxicity to a particular span.,11,12
2193,233024836,"For e.g., in the phrase ""no more Chinese,"" our model only predicts the word Chinese as toxic, whereas the complete phrase attributes to the toxicity of the sentence.",29,30
2194,233024836,"Moreover, considering the subjectivity involved in span detection, the task can also be expanded to report severity scores of spans and classify the type of toxicity.",27,28
2195,248884476,"For instance, debiasing methods that use information about toxic language may benefit from additional information given by detailed explanations of toxicity in text (Ma et al.,",21,22
2196,248884476,"Furthermore, detailed explanations of toxicity may facilitate human interaction with toxicity detection systems (Rosenfeld and Richardson, 2019) .",5,6
2197,248884476,"Furthermore, detailed explanations of toxicity may facilitate human interaction with toxicity detection systems (Rosenfeld and Richardson, 2019) .",11,12
2198,248884476,They can also help humans who work with toxicity classifiers use more information about the input when making decisions about toxic speech.,8,9
2199,248884476,"As mentioned earlier, explanations of toxicity can help with downstream tasks such as debiasing or decision making by humans, thus there has been increasing demand for explainable machine learning classifiers (Ribeiro et al.,",6,7
2200,248884476,"Recent work around explainable toxicity classification introduced Social Bias Frames (Sap et al.,",4,5
2201,248884476,"2020) , a formal framework which combines explanations of toxicity along with toxicity classifications along multiple dimensions.",10,11
2202,248884476,"2020) , a formal framework which combines explanations of toxicity along with toxicity classifications along multiple dimensions.",13,14
2203,248884476,"To fill this gap, our work proposes to leverage different types of knowledge to provide rich context and background for toxicity explanation.",21,22
2204,248884476,"Toxic Text Understanding Prior work around toxicity understanding mainly focuses on detection (Schmidt and Wiegand, 2017) .",6,7
2205,248884476,"Recently, knowledge enhanced approaches have also been used for toxicity detection.",10,11
2206,248884476,2020) use Social Bias Frames to produce both toxicity classifications and explanations of toxicity.,9,10
2207,248884476,2020) use Social Bias Frames to produce both toxicity classifications and explanations of toxicity.,14,15
2208,248884476,"Similarly, our approach attempts to explain toxicity by leveraging different sources of knowledge to provide more context and grounding for the models to generate explanations.",7,8
2209,248884476,"Knowledge Enhanced MIXGEN This section presents our selected three different types of knowledge-expert knowledge, explicit knowledge and implicit knowledge, and our MIX-GEN models for toxicity explanation.",30,31
2210,248884476,"This type of expert knowledge provides useful insights and heuristics for the toxicity explanation task, if they are available.",12,13
2211,248884476,2020) along with toxicity classification models.,4,5
2212,248884476,"The join embedding architecture uses attention weights from the toxicity classifiers to inform the text generation model about parts of the input post relevant to toxicity classification, thus providing a heuristic for the related toxicity explanation task.",9,10
2213,248884476,"The join embedding architecture uses attention weights from the toxicity classifiers to inform the text generation model about parts of the input post relevant to toxicity classification, thus providing a heuristic for the related toxicity explanation task.",25,26
2214,248884476,"The join embedding architecture uses attention weights from the toxicity classifiers to inform the text generation model about parts of the input post relevant to toxicity classification, thus providing a heuristic for the related toxicity explanation task.",35,36
2215,248884476,Table 1 : Some pros and cons about different types of knowledge used for toxicity explanation.,14,15
2216,248884476,"2020) , we train the GPT pretrained model from huggingface to generate the toxicity classifications, the Target Minority, and the Implied Stereotype as a string, when prompted with the input post. •",14,15
2217,248884476,"This is likely because the Expert model relies on toxicity classifications, which weren't available in the implicit hate corpus.",9,10
2218,248884476,"The expert model likely focuses on tokens that trigger toxicity classifications, which makes it less likely to focus on other relevant tokens.",9,10
2219,248884476,"Ethical Considerations Models such as the one proposed in this paper, which output toxicity classifications of text or speech and reasoning behind such classifications should be used with care.",14,15
2220,248834604,Context: Tylenol toxicity -Initially treated with NAC protocol.,3,4
2221,227231558,"2020) , the authors categorize the hate language as abusive language, aggression, cyberbullying, hatefulness, insults, personal attacks, provocation, racism, sexism, threats or toxicity.",32,33
2222,236460340,The code of this study is available at https://github.com/Chenrj233/ semeval2021_task5 Introduction Existing toxicity detection datasets and models classify the entire comment or document and do not identify the range that makes the text toxic.,12,13
2223,236460340,A system that accurately locates the toxicity range in the text is crucial in achieving semi-automatic review.,6,7
2224,236460340,We define a sequence of words that attribute to the text's toxicity as the toxic span.,12,13
2225,236460340,"We improved the performance of the basic model by reducing the number of categories for each token, selecting the appropriate loss function, adding some additional information to the representation vector of the tokens during classification, and finally obtaining a model that can detect the toxicity in a text.",47,48
2226,52162266,"2017) , which consists of roughly 100,000 Wikipedia revision comments labeled via crowsourcing for aggression, toxicity and the presence of personal attacks.",17,18
2227,236486169,They consider spurious correlations in sentiment classification and toxicity detection.,8,9
2228,238583331,"The initial seed of conversations is sampled from WikiConv based on an automatic measure of toxicity that ranges from 0 (no toxicity) to 1 (Hua et al.,",15,16
2229,238583331,"The initial seed of conversations is sampled from WikiConv based on an automatic measure of toxicity that ranges from 0 (no toxicity) to 1 (Hua et al.,",22,23
2230,238583331,2 A conversation is included as a potential example of derailment if the N th comment in it has a toxicity score higher than 0.6 and all preceding comments have a score lower than 0.4.,20,21
2231,238583331,"To estimate the levels of noise in CMV we obtain per-turn toxicity scores for conversations in that data set from detoxify (Hanu and Unitary team, 2020).",13,14
2232,238583331,"The occurrence of toxicity in a conversations prior to the last turn can therefore be seen as noise, and 31.9 percent is a high rate of noise to have in a fairly small data set.",3,4
2233,250390775,Introduction Many developed nations are now considering deep learning approaches for tackling textual toxicity in social media.,13,14
2234,250390775,"So, textual classification in many non-dominant languages remains rudimentary, leaving the communities unequipped against the increasing toxicity and abusive comments on social platforms.",20,21
2235,243865593,"Liu and Avci (2019) train a toxicity classifier using an attribution prior based on Integrated Gradients, a gradient-based method of feature attribution (Sundararajan et al.,",8,9
2236,247628078,"2021) and toxicity (Wulczyn et al.,",3,4
2237,235803170,"The degree of toxicity depends on complicated subjective measures, for instance, the receiver's perception of the dialect of the speaker (Sap et al.,",3,4
2238,235294296,"We expect this can be applied to improve factuality, coherence, and reduce bias and toxicity in language model generations.",16,17
2239,222125009,"In this work we examine gender bias and toxicity of text generations in the context of various styles from the Image-Chat dataset (Shuster et al.,",8,9
2240,222125009,"Notably, after tuning the model to reduce toxicity and gender bias, we find that human preference for this model does not diminish.",8,9
2241,222125009,"To mitigate this problem, we first measure our models' toxicity using an openly available blocklist 3 and an offensive language classifier presented in Dinan et al. (",11,12
2242,222125009,"We define the term ""toxicity"" to mean the ratio between the number of offensive utterances and the total number of utterances generated by the model.",5,6
2243,222125009,The results in Table 9 indicate that positive styles reduce the level of toxicity by a large margin for both metrics (classifier and blocklist).,13,14
2244,222125009,"The results also align well with our previous experiments on degendering, as toxicity is reduced across all styles after applying the degendering process.",13,14
2245,59336626,The dataset is augmented with an external toxicity dataset 4 .,7,8
2246,229923551,"For a given input text, P provides percentage scores across attributes such as ""toxicity"" and ""profanity"".",15,16
2247,235313847,"Having tools to analyze these models is crucial to identifying and forestalling problems in generation, such as toxicity (Gehman et al.,",18,19
2248,235422654,"A rise in task diversity There has been an increasing amount of subjective tasks with genuine ambiguity: judging toxicity of online discussions (Aroyo et al.,",19,20
2249,237568724,2020 ) provide an in-depth analysis with respect to toxicity and fake news of OPENWEBTEXT.,11,12
2250,199379639,"d) The crude incidence of late rectal toxicity ≥ G2 was 14.0% and 12.3% for the arm A and B, respectively.",8,9
2251,199379639,"Identifying the textual boundaries of an outcome presents a challenge: for the example d, it can be ""the crude incidence of late rectal toxicity ≥ G2"" or ""late rectal toxicity ≥ G2""; for the example f, it can be ""the proportion of patents who remained relapsefree at Week 26"", or ""remained relapse-free at Week 26"", or simply ""relapse-free"".",26,27
2252,199379639,"Identifying the textual boundaries of an outcome presents a challenge: for the example d, it can be ""the crude incidence of late rectal toxicity ≥ G2"" or ""late rectal toxicity ≥ G2""; for the example f, it can be ""the proportion of patents who remained relapsefree at Week 26"", or ""remained relapse-free at Week 26"", or simply ""relapse-free"".",34,35
2253,10575510,"Note that the noun phrases ""15-18: 抑制氧消耗实验(inhibition of oxygen consumption test)"" and ""20-23:大型蚤急性毒性实验(large-scale flea acute toxicity test)"" are parsed as siblings in the Baseline KyotoEBMT system, while in our Re-trained partial parsing model they are parsed as modifier-head dependencies, which are isomorphic to the Japanese parse tree.",28,29
2254,2310572,"None of the patient findings are particularly specific, but RECONSIDER places the correct diagnosis in Oth place, and determines that most of the diseases near the top of the differential are ""whole body"" diseases, a group containing most toxicity diseases.",43,44
2255,2310572,"If this differential were selected from among a few hundred diseases, or even from a knowledge base of toxicity diseases, the result would he more open to a variety of less favorable interpretations.",19,20
2256,196196857,"Further applications to NLP span aggression, toxicity and emotion detection (Srivastava et al.,",7,8
2257,235790530,"We find that an uncertainty-based strategy consistently outperforms the widely used strategy based on toxicity scores, and moreover that the choice of review strategy drastically changes the overall system performance.",16,17
2258,235790530,"Due to the sheer scale of user-generated text, modern content moderation systems often employ machine learning algorithms to automatically classify user comments based on their toxicity, with the goal of flagging a collection of likely policy-violating content for human experts to review (Etim, 2017) .",28,29
2259,235790530,"This has raised questions about how current toxicity detection models will perform in realistic online environments, as well as the potential consequences for moderation systems (Rainie et al.,",7,8
2260,235790530,"Finally, (3) we present a large benchmark study to evaluate the performance of five classic and state-of-the-art uncertainty approaches on CoToMoD under two different moderation review approaches (based on the uncertainty score and on the toxicity score, respectively).",45,46
2261,235790530,"We find that both the model's predictive and uncertainty quality contribute to the performance of the final system, and that the uncertainty-based review strategy outperforms the toxicity strategy across a variety of models and range of human review capacities.",30,31
2262,235790530,"Robustness to distribution shift has been applied to toxicity classification in other works (Adragna et al.,",8,9
2263,235790530,"Background: Uncertainty Quantification for Deep Toxicity Classification Types of Uncertainty Consider modeling a toxicity dataset D = {y i , x i } N i=1 using a deep classifier f W (x).",14,15
2264,235790530,"Here the x i are example comments, y i ∼ p * (y|x i ) are toxicity labels drawn from a data generating process p * (e.g., the human annotation process), and W are the parameters of the deep neural network.",18,19
2265,235790530,"For example, the toxicity label y i for a comment can vary between 0 and 1 depending on raters' different understandings of the comment or of the annotation guidelines.",4,5
2266,235790530,"For example, at evaluation time, the toxicity classifier may encounter neologisms or misspellings that did not appear in the training data, making it more likely to make a mistake (van Aken et al.,",8,9
2267,235790530,"These metrics reflect the origins of these systems in classification problems, such as for detecting / classifying online abuse, harassment, or toxicity (Yin et al.,",24,25
2268,235790530,"Moreover, OC-Acc can be sensitive to the intrinsic class imbalance in the toxicity datasets, appearing overly optimistic for model predictions that are biased toward negative class, similar to traditional accuracy metrics (Borkan et al.,",15,16
2269,235790530,"2 ) in Appendix B. CoToMoD: An Evaluation Benchmark for Real-world Collaborative Moderation In a realistic industrial setting, toxicity detection models are often trained on a well-curated dataset with clean annotations, and then deployed to an environment that contains a more diverse range of sociolinguistic phenomena, and additionally exhibits systematic shifts in the lexical and topical distributions when compared to the training corpus.",22,23
2270,235790530,"First, we experiment with a common toxicity-based review strategy (Jigsaw, 2019; Salganik and Lee, 2020) .",7,8
2271,235790530,"Specifically, the model sends comments for review in decreasing order of the predicted toxicity score (i.e., the predictive probability p(y|x)), equivalent to a review score u tox (x) = p(y|x).",14,15
2272,235790530,Which strategy performs best depends on the toxicity distribution in the dataset and the available review capacity α.,7,8
2273,235790530,We then evaluate the models' collaboration performance under both the uncertainty-and the toxicity-based review strategies (Section 6.2).,15,16
2274,235790530,"Collaboration Performance Effect of Review Strategy For the AUC performance of the collaborative system, the uncertaintybased review strategy consistently outperforms the toxicity-based review strategy.",22,23
2275,235790530,"For example, in the in-domain environment (Wikipedia Talk corpus), using the uncertainty-rather than toxicity-based review strategy yields larger OC-AUROC improvements than any modeling change; this holds across all measured review fractions.",21,22
2276,235790530,"As shown, the efficiency of the toxicity-based strategy starts to improve as the review fraction increases, leading to a cross-over with the uncertainty-based strategy at high fractions.",7,8
2277,235790530,"This is likely caused by the fact that in toxicity classification, the false positive rate exceeds the false negative rate.",9,10
2278,235790530,"This highlights the impact of the toxicity distribution of the data on the best review strategy: because the proportion of toxic examples is much lower in CivilComments than in the Wikipedia Talk Corpus, the cross-over between the uncertainty and toxicity review strategies correspondingly occurs at lower review fractions.",6,7
2279,235790530,"This highlights the impact of the toxicity distribution of the data on the best review strategy: because the proportion of toxic examples is much lower in CivilComments than in the Wikipedia Talk Corpus, the cross-over between the uncertainty and toxicity review strategies correspondingly occurs at lower review fractions.",43,44
2280,235790530,"For example, the OC-AUCs using the toxicity strategy are still lower than those with the uncertainty strategy even for high review fractions.",9,10
2281,235790530,"We considered two canonical strategies for collaborative review: one based on the toxicity scores, and a new one using model uncertainty.",13,14
2282,235790530,"We found that the uncertaintybased review strategy outperforms the toxicity strategy across a variety of models and range of human review capacities, yielding a > 30% absolute in-crease in how efficiently the model uses human decisions and ∼ 0.01 and ∼ 0.05 absolute increases in the collaborative system's AUROC and AUPRC, respectively.",9,10
2283,235790530,"The interaction between the data distribution and best review strategy demonstrated by the crossover between the two strategies' performance out-ofdomain) emphasizes the implicit trade-off between false positives and false negatives in the two review strategies: because toxicity is rare, prioritizing comments for review in order of toxicity reduces the false positive rate while potentially increasing the false negative rate.",43,44
2284,235790530,"The interaction between the data distribution and best review strategy demonstrated by the crossover between the two strategies' performance out-ofdomain) emphasizes the implicit trade-off between false positives and false negatives in the two review strategies: because toxicity is rare, prioritizing comments for review in order of toxicity reduces the false positive rate while potentially increasing the false negative rate.",54,55
2285,235790530,"In particular, dataset bias remains a significant issue: statistical correlation between the annotated toxicity labels and various surface-level cues may lead models to learn to overly rely on e.g. lexical or dialectal patterns (Zhou et al.,",15,16
2286,235790530,"One key direction is to develop better review strategies than the ones discussed here: though the uncertainty-based strategy outperforms the toxicity-based one, there may be room for further improvement.",23,24
2287,235790530,"Furthermore, constraints on the moderation process may necessitate different review strategies: for example, if content can only be removed with moderator approval, we could experiment with a hybrid strategy which sends a mixture of high toxicity and high uncertainty content for human review.",39,40
2288,235790530,"Given a trained toxicity model, a review policy and a dataset, let us denote r as the event that an example gets reviewed, and c as the event that model prediction is correct.",3,4
2289,235790530,"Similiar to Review Efficiency, we find little difference in performance between different uncertainty models, and that the uncertainty-based policy outperforms toxicity-based policy especially in the low review capacity setting.",24,25
2290,235790530,"For large review fractions (α > 0.1), the toxicity-based review in fact outperforms the uncertainty review.",11,12
2291,235790530,Dashed Line: toxicity-based strategy.,3,4
2292,235790530,"This is the only plot for which we observe a major crossover: training with cross-entropy, the efficiency for toxicity-based review spikes above the uncertainty-based review efficiency at α = 0.02 before converging back toward it with increasing α.",22,23
2293,235790530,"training with cross-entropy, the efficiency for toxicity-based review spikes above the uncertainty-based review efficiency at α = 0.02 before converging back toward it with increasing α.",9,10
2294,44522989,"Finally, we examine the relation between constructiveness and toxicity.",9,10
2295,44522989,"News comments may be filtered according to different criteria, for example, based on their toxicity and/or constructiveness.",16,17
2296,44522989,"A number of approaches have been proposed for toxicity (e.g., Kwok and Wang, 2013; Waseem and Hovy, 2016; Wulczyn et al.,",8,9
2297,44522989,"Finally, we examine the relationship between toxicity and constructiveness.",7,8
2298,44522989,"Toxicity in news comments In the context of filtering news comments, we are also interested in the relationship between constructiveness and toxicity.",22,23
2299,44522989,"We propose the label toxicity for a range of phenomena, including verbal abuse, offensive comments and hate speech.",4,5
2300,44522989,"To better understand the nature of toxicity and its relationship with constructiveness, we extended our CrowdFlower annotation.",6,7
2301,44522989,"For the 1,121 comments described in Section 2, we also asked anno-tators to identify toxicity.",17,18
2302,44522989,The distribution of toxicity levels by constructiveness label is shown in Table 3 .,3,4
2303,44522989,"The most important result of this annotation experiment is that there were no significant differences in toxicity levels between constructive and non-constructive comments, i.e., constructive comments were as likely to be toxic (in its three categories) as non-constructive comments.",16,17
2304,44522989,"We conclude, then, that constructiveness and toxicity are orthogonal categories.",8,9
2305,44522989,"The results also suggest that it is important to consider constructiveness of comments along with toxicity when filtering comments, as aggressive constructive debate might be a good feature of online discussion.",15,16
2306,44522989,"Given these results, the classification of constructiveness and toxicity should probably be treated as separate problems.",9,10
2307,44522989,"Through an annotation experiment, we studied the relationship between constructiveness and toxicity, and found that constructive comments are just as likely to be toxic (or not toxic) as nonconstructive comments.",12,13
2308,44522989,We also plan to investigate the relation between toxicity and constructiveness more deeply.,8,9
2309,237532535,"To measure the levels of toxicity, they employ hate lexicons, with the help of which they compute the College Hate Index (CHX), as fractions of hateful keywords in each community compared to other subreddits banned for violating the hate-limiting policies of Reddit.",5,6
2310,237532535,"Quarantining is a form of intervention that Reddit applies, where communities are indicated as potentially problematic and users have to deliberately choose to enter them, after being warned about toxicity levels within.",31,32
2311,237532535,"For example, it might prove meaningful to extract high-level representations like sentiment features or toxicity scores.",17,18
2312,31199034,"Second, we are interested in exploring the relation between constructiveness and toxicity.",12,13
2313,209536752,"On the other hand, it could be necessary to merge corpora in order to evaluate the impact of processing negation in specific tasks such as information extraction in the biomedical and clinical domain, drug-drug interactions, clinical events detection, biomolecular events extraction, sentiment analysis, and constructiveness and toxicity detection.",54,55
2314,209536752,"For drug-drug interactions, bio-molecular events extraction, and constructiveness and toxicity detection, it could only be analyzed in English; and for clinical events detection, it could only be evaluated in Spanish.",15,16
2315,209536752,"Information extraction in the biomedical and clinical domain Drug-drug interactions Clinical events detection Bio-molecular events extraction Sentiment analysis Constructiveness and toxicity detection English Swedish Stockholm Electronic Patient Record Dutch EMC Dutch Japanese Review and Newspaper Japanese Chinese CNeSp German German negation and speculation Italian Fact-Ita Bank Negation corpora are not available due to legal and ethical issues, which makes it difficult to study negation in this domain, a domain in which processing negation is crucial because the health of patients is at stake.",24,25
2316,208157933,"In all our approaches, the DNN outputs represent the toxicity of a comment.",10,11
2317,208157933,In our work we exploited only the toxicity part of the corpus.,7,8
2318,208157933,"The following toxicity rates are used by annotators: very toxic, toxic, neither, healthy, very healthy.",2,3
2319,235683199,"Such toxicity may result in platform-wide decreases in user participation and engagement which, combined with external pressure (e.g., bad press), may motivate platform managers to moderate harmful behavior (Saleem and Ruths, 2018; Habib et al.,",1,2
2320,236486092,"We still note some encouraging signs for all of these communities: r/explainlikeimfive and r/askscience have similar structures and purposes, and r/askscience was found in 2015 to show medium supportiveness and very low toxicity when compared to other subreddits (see a hackerfall post, thecut.com write-up and supporting data).",40,41
2321,236486092,"However, further analysis of whether and to what extent these rules reduce toxicity is still needed.",13,14
2322,220250520,"In the above example, the ADR mentions 'oww', 'toxic', and 'equivalent to torture' are mapped to the concepts 'pain (10033371)', 'drug toxicity (10013746)' and 'feeling unwell (10016370)' respectively.",36,37
2323,53503062,One question appearing was how to merge datasets using different classification systems (e.g. aggression vs. toxicity).,16,17
2324,53503062,"We used the Toxicity dataset, from the Toxic comment classification challenge in Kaggle, as an additional source of data, and we proceeded with the conversion from toxicity to aggression.",29,30
2325,53503062,"2016 ), profanity (Dictionary, 2017) , toxicity (Jigsaw, 2017) , flaming (Guermazi et al.,",10,11
2326,53503062,One question is if it is possible the combination of datasets annotated with different classification systems (e.g. toxicity and aggression).,18,19
2327,53503062,We developed two systems (aggression data vs. aggression + toxicity data) that were tested in two different scenarios (Facebook data vs. Social Media).,10,11
2328,53503062,"for English data: training with the provided dataset, classification algorithm with parallel random forests and testing in Facebook data (Fb ag rf); training with the provided dataset plus the dataset classified on toxicity, classification algorithm with parallel random forests and testing in Facebook data (Fb ag tox rf); training with the provided dataset, classification algorithm with parallel random forests and testing in Social Media data (Sm ag rf); training with the provided dataset plus the dataset classified on toxicity, classification algorithm with parallel random forests and testing in Social Media data (Sm ag tox rf).",37,38
2329,53503062,"for English data: training with the provided dataset, classification algorithm with parallel random forests and testing in Facebook data (Fb ag rf); training with the provided dataset plus the dataset classified on toxicity, classification algorithm with parallel random forests and testing in Facebook data (Fb ag tox rf); training with the provided dataset, classification algorithm with parallel random forests and testing in Social Media data (Sm ag rf); training with the provided dataset plus the dataset classified on toxicity, classification algorithm with parallel random forests and testing in Social Media data (Sm ag tox rf).",91,92
2330,53503062,We conducted an experiment where we combined a toxicity dataset with the original aggression dataset used in this shared task.,8,9
2331,53503062,We found no alternative dataset with text classified for aggression and therefore we had to merge datasets using different classes (aggression vs. toxicity).,23,24
2332,53503062,"Additionally, we also found lack of clear definitions in the toxicity dataset.",11,12
2333,59336566,"We In this work, we have proposed to automatically detect toxicity and aggression in comments, we show that with minimal preprocessing techniques we are able to achieve a good model performance and demonstrated how OOV words and semantic sense are learnt implicitly with random initialisation.",11,12
2334,26382438,"Figure 1 (b) indicates the specific effects of audiovisual toxicity caused by ""desferrioxamine"" can only be established by parsing multiple statements describing the secondary links identified through the perception of ""audiovisual defects"".",11,12
2335,26382438,Ocular and auditory toxicity in hemodialyzed patients receiving desferrioxamine.,3,4
2336,26382438,"During an 18-month period of study 41 hemodialyzed patients receiving desferrioxamine (10-40 mg/kg BW/3 times weekly) for the first time were monitored for detection of audiovisual toxicity....... Visual toxicity was of retinal origin and was characterized by a tritan-type dyschromatopsy, sometimes associated with a loss of visual acuity and pigmentary retinal deposits.",34,35
2337,26382438,"During an 18-month period of study 41 hemodialyzed patients receiving desferrioxamine (10-40 mg/kg BW/3 times weekly) for the first time were monitored for detection of audiovisual toxicity....... Visual toxicity was of retinal origin and was characterized by a tritan-type dyschromatopsy, sometimes associated with a loss of visual acuity and pigmentary retinal deposits.",37,38
2338,26382438,Auditory toxicity was characterized by a mid-to high-frequency neurosensorial hearing loss and the lesion was of the cochlear type...... Please make both abstracts as a figure.,1,2
2339,168170119,"2018) directly measured biases in the Google Perspective API classifier, 1 trained on data from Wikipedia talk comments, finding that it tended to give high toxicity scores to innocuous statements like ""I am a gay man"".",28,29
2340,249848125,"and ""I saw his ass yesterday"" both score above 90% for toxicity (Sap et al.,",14,15
2341,249848125,"Even Twitter accounts belonging to drag queens have been rated higher in terms of average toxicity than the accounts associated with white nationalists (Oliva et al.,",15,16
2342,249848125,"For example, the median toxicity score for language on transgendercirclejerk, a ""parody [subreddit] for trans people"", is as high as 90% (Kurrek et al.,",5,6
2343,249848125,CLS] c [SEP] (PERSPECTIVE) We use a publicly available commercial tool for toxicity detection 2 .,17,18
2344,249848125," for toxicity detection with both commercial 4 and academic applications (Cuthbertson et al.,",2,3
2345,226283752,"We obtain the toxicity scores for 100 random comments for each of the DEG, NDG, HOM, and APR labels.",3,4
2346,226283752,Further analysis of toxicity scores across comments underlines the challenges faced by existing models.,3,4
2347,226283752,"First, instances of slur reclamation re- ceived high toxicity scores.",9,10
2348,226283752,"World can suck my shenis"" and ""When I've got a guy I'm crushing on I will sometimes say 'He makes me feel like a silly [f-slur] all over again'"" have toxicity scores above 0.93.",41,42
2349,226283752,"Second, recollections of past harassment received high toxicity scores.",8,9
2350,226283752,"Finally, counter speech received high toxicity scores.",6,7
2351,226283667,"This attribute is much less sensitive to more mild forms of toxicity, such as comments that include positive uses of curse words."" •",11,12
2352,226283667,"Prior Work There is much recent work on detecting incivility (also referred to as toxicity, abusive language, offensive language, etc.)",15,16
2353,174803474,2017) that tries to identify toxicity of comments in the discourse section of Wikipedia.,6,7
2354,174803474,"These results stand in contrast to the Kaggle competition on toxicity detection, where such baselines performed nearly as well as the best (GRU-based) model, and all models achieved high levels of performance (>0.98 area under receiver operating characteristic curve).",10,11
2355,174803474,2014) is more challenging than simple toxicity detection.,7,8
2356,174803474,"This suggests that while there is some overlap between the two tasks (toxicity detection and incivility detection), the differences between the tasks make it difficult to directly leverage the data from one task in the other.",13,14
2357,250390878,"We compare these against a uniform random baseline and a competitive baseline of a commercial model for recognizing toxic language, Perspective API using 0.5 as a cut-off for determining toxicity.",32,33
2358,250390481,"1 The immense popularity of social platforms resulted in a significant rise in the toxicity of the discourse, ranging from cyberbullying to explicit hate speech and calls for violence against individuals and groups (Waseem and Hovy, 2016; Mondal et al.,",14,15
2359,250390661,Related Work: Interactive Dataset and Model Exploration A variety of methods and tools that enable dataset users to explore and familiarize themselves with the contents of the datasets have been proposed 2021 ) tackle the problem of interrogating a toxicity detection model using a tool they call RECAST.,40,41
2360,250391002,"Tasks such as toxicity detection, hate speech detection, and online harassment detection have been developed for identifying interactions involving offensive speech.",3,4
2361,250391002,"Introduction Tasks such as the detection of toxicity, hate speech, and online harassment have been developed to identify and intervene in situations that have the potential to cause significant social harm.",7,8
2362,250391002,We contribute: 1) a framing of offensiveness that accounts for socially productive uses of denotatively offensive language; 2) a general understanding of offensive language that builds from definitions of hate speech and toxicity to show the difficulty of operationalizing relational context; 3) specific challenges and directions for improving how we operationalize relational aspects of offensiveness.,36,37
2363,250391002,"Building from definitions of hate speech and toxicity, we establish a working definition of offensiveness that can better account for missing social context.",7,8
2364,250391002,"In contrast to hate speech and other definitions of offensive language, toxicity is specifically oriented around the measurable outcome of language use.",12,13
2365,250391002,"Hate speech and toxicity help us to construct a relational view for grounding a more robust definition of offensiveness-that is, a view that considers the social relations among targets of offensive language, and the producers, receivers, and perceivers (each of whom can also be a target) of denotatively offensive speech.",3,4
2366,250391002,2021 ) describe erroneously high toxicity probabilities (provided by the Perspective API) for language from drag queens on Twitter 2 .,5,6
2367,250391002,"Although toxicity is distinct from offensiveness, their analysis shows how the difficulty of implementing a relational approach and detecting relational context in practice can cause the concept to be misapplied.",1,2
2368,250391002,"This difficulty applies not only to classifying toxicity, but also classifying hate speech, offensiveness, and other language rooted in relational context.",7,8
2369,250391002,The authors compare tweets that contain queer vernacular produced by drag queens to racist tweets written by white supremacists that are predicted to have low toxicity.,25,26
2370,250391002,we are not focused here on toxicity.,6,7
2371,250391002,"Rather, we assess the above example in terms of offensiveness and disambiguate between hate speech, toxicity and offensive speech not only at the definitional level but also at a conceptual level.",17,18
2372,250391002,"For example, applying Perspective API's definition of toxicity is complicated by the fact that, presented without context to a perceiver or data annotator, this language may be indistinguishable from rude, uncivil, indecent and impolite exchange.",9,10
2373,250391002,"By bringing focus to the likelihood of offensive language to cause a person to disengage from interaction, toxicity as defined by Jigsaw provides an example of incorporating measurable ends into the definition of the classification task.",18,19
2374,250391002,"Nonetheless, capturing offensive language under the label 'toxicity' is an interesting departure from other labels used to describe offensiveness, such as 'misogynous' or 'aggressive', because of its focus on using observable behavior as a metric.",9,10
2375,184483646,"This threshold was to be treated as a measure of toxicity, filtering the online toxic content, prior to display of contents in the client's browser.",10,11
2376,238634750,"For instance, toxicity prediction models were shown to have biases towards mentions of certain identity terms (Dixon et al.,",3,4
2377,238634750,"Similarly these models are shown to overestimate the prevalence of toxicity in African American Vernacular English (Sap et al.,",10,11
2378,238634750,"2020) show that discussions about mental illness are often associated with topics such as gun violence, homelessness, and drugs, likely the reason for the learned association of mental illness related terms with toxicity.",36,37
2379,248780355,"Abusive language is a broad term often used to describe the posts and comments on social media platforms written to cyberbully, spread toxicity, spread hate, hurt others based on sex, caste, or creed (Pamungkas et al.,",23,24
2380,248780526,"There are instances of communal hatred, hate-speech, toxicity and bullying.",11,12
2381,232270100,A detailed review of the datasets that covered online toxicity is provided below.,9,10
2382,232270100,"Besides the previous annotation guidelines, and based on the domain and context of the proposed dataset,we made the annotators aware of specific phrases and terms which look normal/neutral while they indicate toxicity.",36,37
2383,226283936,The Perspective team at Jigsaw calibrated the scores of their toxicity classifier using isotonic regression (PerspectiveScoreNorm).,10,11
2384,226283936,1 Research design We evaluate the toxicity classifier from Perspective and the hate speech classifier provided by Davidson et al. (,6,7
2385,226283936,"The 1,000 tweets in each sample were given to annotators with the definitions of toxicity and hate provided by the original authors.",14,15
2386,226283936,Annotators differ in their perception of toxicity and hate.,6,7
2387,226283936,"2017) and the fact that for the Perspective classifier we had only the definition of toxicity: ""a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion.""",16,17
2388,226283936,"Figure 1b shows a curve fit to the annotations provided by each annotator for Sample 1 against the classification scores returned by Perspective's toxicity classifier, using the spline regression (in Stan).",24,25
2389,226283936,In this section we focus only on Perspective's toxicity classifier.,9,10
2390,226283936,"As expected, the estimated prevalence of toxic tweets directed at Lewis depends on where this arbitrary threshold is set-he received only two tweets with toxicity > 0.8, but 10 with toxicity > 0.7, a 5-fold increase.",27,28
2391,226283936,"As expected, the estimated prevalence of toxic tweets directed at Lewis depends on where this arbitrary threshold is set-he received only two tweets with toxicity > 0.8, but 10 with toxicity > 0.7, a 5-fold increase.",34,35
2392,226283936,It can be achieved by summing the toxicity probabilities within each unit of time (e.g. every hour).,7,8
2393,226283936,The principle behind this is that if there are 50 tweets each with 0.2 probability of toxicity then it should be likely that ∼10 will be toxic -and summing the probabilities best capture this.,16,17
2394,247762301,"Recently organized workshops and shared tasks have fostered discussions around detection of hate speech, toxicity, misogyny, sexism, racism and abusive content (Zampieri et al.,",15,16
2395,248780536,"Although social media has provided this minority with a platform to express themselves by sharing their experiences and build a strong, healthy community, there has been an increasing amount of general toxicity on the internet (Craig and McInroy, 2014) .",33,34
2396,232075938,"In the last few years, several studies have been conducted regarding offensive languages such as hate speech, aggression, toxicity and abusive language.",21,22
2397,249538400,"First, hate speech classification is the problem of detecting statements that are likely to cause harm and inject toxicity in online discourse.",19,20
2398,226283508,"However, there remain difficulties in detecting subtler forms of toxicity which may be implicit, require idiosyncratic knowledge, familiarity with the conversation context, or familiarity with particular cultural tropes (Kohli et al.,",10,11
2399,226283508,"These two features of online conversations can sometimes enhance commenters' sensitivity to subtler forms of toxicity like sarcasm, condescension, or dismissiveness, amplifying their negative impact on conversations despite the fact that these attributes may be less (or not at all) harmful in other specific contexts.",16,17
2400,226283508,"Third, there is an even greater risk of identifying 'false positives' and 'false negatives', since many of the expressions used in subtle forms of toxicity can also be deployed for positive contributions.",30,31
2401,226283508,"We also include correlations with the 'toxicity' scores produced by Jigsaw's Perspective API (perspectiveapi.com), which again confirms that our attributes, in particular those other than antagonistic and hostile, capture something distinct from overt toxicity.",7,8
2402,226283508,"We also include correlations with the 'toxicity' scores produced by Jigsaw's Perspective API (perspectiveapi.com), which again confirms that our attributes, in particular those other than antagonistic and hostile, capture something distinct from overt toxicity.",41,42
2403,226283508,"In particular, we note that although there is a substantial body of research on more extreme forms of negative contributions, such as toxicity, the subtler forms of unhealthy comments in our typology are often similarly prevalent online.",24,25
2404,226283508,"Our analysis also shows that the sub-attributes are largely independent from overt toxicity, and mostly correlated with unhealthy contributions.",14,15
2405,226283543,"The concept of ""severe toxicity"" is annotated in the Kaggle Toxic Comment Classification Challenge ( 2018 ), where it is defined it as ""rude, disrespectful, or unreasonable comments that are very likely to make people leave a discussion.""",5,6
2406,222378486,"Several terms have been used to refer to the general concept of harmful online behavior, including toxicity (Hosseini et al.,",17,18
2407,222378486,"2017) ), collected from English Wikipedia talk pages and annotated for toxicity.",13,14
2408,222378486,Our main contributions are as follows: • We identify topics included in the Wiki-dataset and manually examine keywords associated with the topics to heuristically determine topics' generalizability and their potential association with toxicity. •,36,37
2409,222378486,Looking at the toxicity annotations we observe that 47% of the Toxic comments belong to these topics.,3,4
2410,222378486,"This indicates that the Wiki-dataset, annotated for toxicity, is not well suited for detecting sexist or racist tweets.",10,11
2411,226221766,2017) targeted Google's Perspective system that detects text toxicity.,10,11
2412,226221766,"They showcased that toxicity scores could be significantly reduced with addition of characters and introduction of spaces and full stops (i.e., periods (""."") )",3,4
