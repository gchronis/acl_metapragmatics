{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57a21dd-c16e-4a53-81ca-7ffeb5a07bec",
   "metadata": {},
   "source": [
    "# Look at uses of a target word \"toxic/ity\" over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db96dea7-920a-479d-8236-36063b4d32b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 02:20:42.304554: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import fastparquet\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import textwrap\n",
    "from scipy.spatial.distance import cosine\n",
    "import spacy\n",
    "from collections import defaultdict \n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8677b60-7adb-49d9-8079-6557fe32f718",
   "metadata": {},
   "source": [
    "Load in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6441b64-3c4f-4867-8725-547fecdecfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"toxic\", \"toxicity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52732ea7-c8fa-4920-a088-0716ffb7492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for target in targets:\n",
    "        dfs.append(pd.read_csv('./data/logic_words/{}.csv'.format(target)) )\n",
    "tokens = pd.concat(dfs)\n",
    "tokens['lemma'] = targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b46b3fdf-75f0-4ade-82f8-8c37a72729d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6711"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79600ec2-ed61-4cc2-9a8c-26a61028e76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>corpus_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_idx</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>1094</td>\n",
       "      <td>250390569</td>\n",
       "      <td>In this paper, we create a dataset for toxic positivity classification from Twitter and an inspirational quote website.</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>1576</td>\n",
       "      <td>201682311</td>\n",
       "      <td>More specifically, we will show that the optimal confidence threshold above which a player can be considered toxic increases as a conversation evolves, and that the rate of this increase interacts with the amount of training material.</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>1577</td>\n",
       "      <td>201682311</td>\n",
       "      <td>2018 ) do an in-depth error analysis for various approaches to toxic comment classification, and propose an ensemble method to outperform them.</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744</th>\n",
       "      <td>1744</td>\n",
       "      <td>237562875</td>\n",
       "      <td>We further train these models on Wikidata triples, which again has the potential to amplify harmful and toxic biases.</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>1284</td>\n",
       "      <td>247519233</td>\n",
       "      <td>2021) as the toxicity classifier (CLF).</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172</td>\n",
       "      <td>236486094</td>\n",
       "      <td>This means that clearly context sensitive posts (e.g., in an edge case, ones that all OC coders found as toxic while all IC coders found as non toxic) are rare.</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  corpus_id  \\\n",
       "1094        1094  250390569   \n",
       "1576        1576  201682311   \n",
       "1577        1577  201682311   \n",
       "1744        1744  237562875   \n",
       "1284        1284  247519233   \n",
       "172          172  236486094   \n",
       "\n",
       "                                                                                                                                                                                                                                        sentence  \\\n",
       "1094                                                                                                                     In this paper, we create a dataset for toxic positivity classification from Twitter and an inspirational quote website.   \n",
       "1576  More specifically, we will show that the optimal confidence threshold above which a player can be considered toxic increases as a conversation evolves, and that the rate of this increase interacts with the amount of training material.   \n",
       "1577                                                                                             2018 ) do an in-depth error analysis for various approaches to toxic comment classification, and propose an ensemble method to outperform them.   \n",
       "1744                                                                                                                       We further train these models on Wikidata triples, which again has the potential to amplify harmful and toxic biases.   \n",
       "1284                                                                                                                                                                                                     2021) as the toxicity classifier (CLF).   \n",
       "172                                                                             This means that clearly context sensitive posts (e.g., in an edge case, ones that all OC coders found as toxic while all IC coders found as non toxic) are rare.   \n",
       "\n",
       "      start_idx  end_idx  lemma  \n",
       "1094          9       10  toxic  \n",
       "1576         18       19  toxic  \n",
       "1577         13       14  toxic  \n",
       "1744         18       19  toxic  \n",
       "1284          4        5  toxic  \n",
       "172          30       31  toxic  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02eb686a-a83f-421f-b603-a0b893946790",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parquet_file = \"/Volumes/data_gabriella_chronis/corpora/acl-publication-info.74k.parquet\"\n",
    "\n",
    "df = pd.read_parquet(parquet_file, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693366bc-3ee2-48bb-906c-d683ee0c3678",
   "metadata": {},
   "source": [
    "Left hand join the large file to the token file. or do a constant lookup??. maybe just get the year columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c9077fa-a693-4031-ad79-9e72cf74518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokens.join(df.set_index(\"corpus_paper_id\"), on=\"corpus_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c021b-195c-47ec-bd9e-87e30bc342b0",
   "metadata": {},
   "source": [
    "Add a decade column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28129e79-4cbc-41aa-832a-6327939abf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"year\"] = data[\"year\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f64b8258-96ef-41f6-ba70-4ed5a94f2567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[\"decade\"] = ( data['year'] //10)*10\n",
    "\n",
    "#bins = pd.IntervalIndex.from_tuples([(0, 1), (2, 3), (4, 5)])\n",
    "bins = [1950, 1960, 1970, 1980, 1990, 2000, 2005, 2010, 2012, 2014, 2016, 2018, 2020]\n",
    "data[\"decade\"] = pd.cut(data['year'], bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5eef632-538c-4752-b39a-a31748022abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2005.0, 2010.0], NaN, (2000.0, 2005.0], (2010.0, 2012.0], (2018.0, 2020.0], (2016.0, 2018.0], (2014.0, 2016.0], (1990.0, 2000.0], (2012.0, 2014.0], (1980.0, 1990.0]]\n",
       "Categories (12, interval[int64, right]): [(1950, 1960] < (1960, 1970] < (1970, 1980] < (1980, 1990] ... (2012, 2014] < (2014, 2016] < (2016, 2018] < (2018, 2020]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"decade\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b52692-d939-45e7-807c-fbdd00382e4a",
   "metadata": {},
   "source": [
    "### Look at 10 example sentences from each decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4850f67d-9a1e-4349-9578-76a20a996af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decade</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(2000, 2005]</td>\n",
       "      <td>The PROPERTY predicate is the result of the normalization of strings expressing physical or chemical properties of the toxic product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(2000, 2005]</td>\n",
       "      <td>The analysis phase can thus be seen as a paraphrase detection phase, as it unifies in a same representation different ways of expressing similar information about toxic products.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>(2005, 2010]</td>\n",
       "      <td>Generally speaking, violations of long-span constituents have a more negative impact on performance than short-span violations if these violations are toxic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>(2005, 2010]</td>\n",
       "      <td>Getting back to GST, let us consider a sentence, (1) The bailout plan was likely to depend on private investors to purchase the toxic assets that wiped out the capital of many banks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>(2005, 2010]</td>\n",
       "      <td>Figure 1 illustrates a simplified two-dimensional semantic space and the changes to semantics that would occur as \"blick\" begins to co-occur with toxic-related words.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(2010, 2012]</td>\n",
       "      <td>The term cytotoxic will be translated into German as zytotoxisch whereas in French it can be translated as cytotoxique or toxique pour les cellules 'toxic to the cells'.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(2010, 2012]</td>\n",
       "      <td>These components may be either free (i.e. they can occur in texts as autonomous lexical items like toxicity in cardiotoxicity) or bound (i.e. they cannot occur as autonomous lexical items, in that case they correspond to bound morphemes likecardioin cardiotoxicity).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>(2010, 2012]</td>\n",
       "      <td>toxic?,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>(2012, 2014]</td>\n",
       "      <td>The intuition here is that to know whether few apples are toxic, it is sufficient to know which apples are toxic; those non-apple toxicants are irrelevant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>(2012, 2014]</td>\n",
       "      <td>During the 2011 East Japan Earthquake and Tsunami Disaster, we had found a number of false information spread on Twitter, e.g., \"The Cosmo Oil explosion causes toxic rain.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>(2014, 2016]</td>\n",
       "      <td>Moreover, according to the etiology of the lesion and the disease associated with it (toxic, metabolic, traumatic or degenerative diseases), types of dysarthria vary with respect to pathophysiologies determining the kind of deficits in the motor execution and/or control of speech movements (deficits in speed, range, strength, rigidity/steadiness, tonus, precision/accuracy, and/or coordination) (Murdoch, 1998 , Duffy 2013) .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>(2014, 2016]</td>\n",
       "      <td>8) Effect of calcium chloride and 4aminopyridine therapy on desipramine toxicity in rats . . .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2742</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>In turn, these toxic attacks are especially disruptive in Wikipedia since they undermine the social fabric of the community as well as the ability of editors to contribute (Henner and Sefidari, 2016) .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>This provides a toxicity score t for all comments in our dataset, which we use to preselect two sets of conversations: (a) candidate conversations that are civil throughout, i.e., conversations in which all comments (including the initial exchange) are not labeled as toxic (t &lt; 0.4); and (b) candidate conversations that turn toxic after the first (civil) exchange, i.e., conversations in which the N -th comment (N &gt; 2) is labeled toxic (t ≥ 0.6), but all the preceding comments are not (t &lt; 0.4).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4104</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>Indeed, the term \"toxic\" is a general description of subreddits with habitual and even emphasized disalignment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2645</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>As patterns of toxic and hateful behavior on Reddit are more well-studied (Chandrasekharan et al.,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>The distribution of toxicity levels by constructiveness label is shown in Table 3 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>Toxicity judgment is well-suited for the community-driven approach because toxic language is one component of the broader global problem of online harassment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>The following forms are recognized by our parser: (1) evaluative expressions in attribute-value form, where the attribute is one of the concepts of the controversial issue concept lattice: Vaccine development is very expensive, car exhaust is toxic. (</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4089</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>The question posed was: How toxic is the comment?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>Related work Offensive language serves many purposes in everyday discourse: from deliberate effect in humour to self-directed profanity to toxic or abusive intent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>This can be exploited when dealing with strongly imbalanced datasets, as often the case in toxic comment classification and related tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4099</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>It could be the case, in some situations, that a moderator may allow a somewhat toxic comment if it contributes to the conversation, i.e., if it is constructive. (</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>Moderation of toxic behavior.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2645</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>As patterns of toxic and hateful behavior on Reddit are more well-studied (Chandrasekharan et al.,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>In \"simple\" mode, users rate toxicity on a three-point scale; in \"advanced\" mode, users also classify the type of toxicity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>Toxicity dataset: This dataset was created like the previous one, but contains more comments (159,686), now labeled as toxic (reject) or not (accept).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>Related work Offensive language serves many purposes in everyday discourse: from deliberate effect in humour to self-directed profanity to toxic or abusive intent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>9 The Jigsaw Toxicity Kaggle Competition: goo.gl/ N6UGPK nearly 33% of toxic comments are removed within a day; And over 82% of severely toxic comments are deleted within a day.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>160K comments labeled as being toxic or not.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>For example, one of the cluster contained more of neutral words, another cluster contained highly aggressive and abusive words, and the third cluster contained some toxic words along with place and country names related to one's origin which were used in some foul comments.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4104</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>Indeed, the term \"toxic\" is a general description of subreddits with habitual and even emphasized disalignment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>We developed two systems (aggression data vs. aggression + toxicity data) that were tested in two different scenarios (Facebook data vs. Social Media).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>Moderation of toxic behavior.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4122</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We refer to these collectively with the generic term of toxic speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Both researchers and social platforms have been focused on developing models to detect toxicity in online communication for a while now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>It contains human-labeled English Wikipedia comments in six different classes of toxic language.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>On top of the BERT [CLS] representation, we added a FFNN of 128 hidden nodes and a sigmoid to yield the toxicity probability.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4255</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>In 2018, Kaggle hosted a Toxic Comment Classification competition in association with Jigsaw, which focused on classifying Wikipedia comments into one of six categories: insult, obscene, severe toxic, threat &amp; identity hate and toxic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Figure 1 illustrates how phrases in the African American English dialect (AAE) are labelled by a publicly available toxicity detection tool as much Figure 1 : Phrases in African American English (AAE), their non-AAE equivalents (from Spears, 1998), and toxicity scores from PerspectiveAPI.com.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>In our work, we study toxic degeneration by both out-of-the-box and controlled models using 100K naturally occurring prompts, including some that do not contain identity mentions (see Figure 1 ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>A limitation of attribution-based explanations for offensive language detection seems to be a focus on words that are toxic regardless of the context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Then we trained a multi-class classifier with nine categories: eight toxic and one non-toxic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>These attempts to bypass the toxicity detection system are called subverting the system, and toxic users doing it are referred to as subversive users.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We characterize toxicity in prompted generations with two metrics: 1) the expected maxi-mum toxicity over k = 25 generations, which we estimate with a mean and standard deviation; and 2) the empirical probability of generating a span with TOXICITY 0.5 at least once over k = 25 generations.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>As expected, the estimated prevalence of toxic tweets directed at Lewis depends on where this arbitrary threshold is set-he received only two tweets with toxicity &gt; 0.8, but 10 with toxicity &gt; 0.7, a 5-fold increase.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Amievalita-misogynous' appears to be close to Davidson's 'offensive' and Toxkaggle's 'toxicity', but it is also not so far away from Waseem's 'sexism'.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>In comparison, 70.98% of the tweets in the FDCL18 test set that were labeled as AAE were also annotated as toxic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>A common problem of these models is the presence of bias towards some words (e.g. woman, black, jew or женщина, черный, еврей) that are not toxic, but serve as triggers for the classifier due to model caveats.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Y) A catalytic converter is an exhaust emission control device that converts toxic gases and pollutants in exhaust gas from an internal combustion engine into less-toxic pollutants by catalyzing a redox reaction (an oxidation and a reduction reaction).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>PERSPECTIVE The third context-insensitive classifier is a CNN-based model for toxicity detection, trained on millions of user comments from online publishers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4069</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Our best systems in each of the three OffensEval tasks placed in the middle of the comparative evaluation, ranking 57 th of 103 in task A, 39 th of 75 in task B, and 44 th of 65 in task C. Introduction Social media is notorious for providing a platform for offensive, toxic, and hateful speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>In our setup, the loss from an extra classifier head is weighted equal to the loss from the toxicity classifier.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4152</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>To perform the binary classification (toxic or not toxic), for each comment, we decided to use the following majority vote labelling: if [(# of very toxic and toxic annotations) &gt; (# of healthy and very healthy annotations)] and [(# of very toxic and toxic annotations) &gt; 2] comment is toxic otherwise comment is non-toxic Some examples of the toxic comments are: \"You are a big fat idiot, stop spamming my userspace\", \"What the fuck is your problem?\", \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Moreover, a small but significant subset of manually labeled posts (5% in one of our experiments) end up having the opposite toxicity labels if the annotators are not provided with context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>An example is considered toxic if its toxicity or severe toxicity scores are above 0.64 or 0.92, respectively.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>1 https://github.com/ShannonAI/CorefQA Original Passage In addition , many people were poisoned when toxic gas was released.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>The performance of Perspective in this subtask is particularly interesting, considering that the training data for these models were not labeled for offensiveness, but rather for other attributes such as toxicity, threats, and insults.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Specifically, we include a pre-trained toxicity scorer that scores a given text on six dimensions of toxic content.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Indeed, as mentioned in Sections 1 and 2, it is trivial for a subversive user to mask toxic keywords to bypass toxicity filters.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Figure 1 illustrates how phrases in the African American English dialect (AAE) are labelled by a publicly available toxicity detection tool as much Figure 1 : Phrases in African American English (AAE), their non-AAE equivalents (from Spears, 1998), and toxicity scores from PerspectiveAPI.com.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Yet, for the game League of Legends, researchers found that this 1% of the player population only accounted for 5% of the toxic speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>unexpected, since their labels suggest that the main difference between these two categories is the intensity of the expressed toxicity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Our work so far has focused on single-line messages and negative toxicity detection.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We focused on the threat class, but also replicated our results on another toxic class ( §4.6).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>This paper presents the application of two strong baseline systems for toxicity detection and evaluates their performance in identifying and categorizing offensive language in social media.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We found that 89.8% of the toxic sentences were annotated as toxic, compared to 87.6% in the attacked toxic sentences.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>In this work, we propose a framework aimed at fortifying existing toxic speech detectors without a large labeled corpus of veiled toxicity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We address the task of automatically detecting toxic content in user generated texts.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>The perceived toxicity of some comments may be increasing when context is provided, but for other comments it may be decreasing, and these effects may be partially cancelling each other when measuring the change in toxicity ratio.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>The toxicity ratios of CAT-LARGE (Fig.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Figure 1a shows the results for toxicity prediction (Jigsaw, 2017) , which outputs a score ∈ [0, 1], with higher scores indicating more toxicity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3128</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We pick the least toxic m posts as our veiled offensive language set so that 1 m m i=1 tox(x (i) ) = tox general .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Though trained on a much larger corpus, GPT-3's unprompted toxicity also mirrors Figure 2 : Neural models generate toxicity, even with no prompting.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We also found no evidence that context actually improves the performance of toxicity classifiers, having tried both simple and more powerful classifiers, having experimented with several methods to make the classifiers context aware, and having also considered the effect of gold labels obtained out of context vs. gold labels obtained by showing context to the annotators.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>The psychological-communicative Reduced cues approach (Sproull and Kiesler, 1986) argues that properties of online environments may cause toxic online disinhibition (Suler, 2004) : people feel less restraint because of the absence of social-context cues, anonymity, invisibility, asynchronicity, or minimization of authority.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>@user @user @user @user @user @user @user what a stupid incompetent devious and toxic pm !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>2020) , which is among the most effective methods we tested at avoiding toxicity with toxic prompts.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Apart from the growing volume of popular press concerning toxicity online, the increased interest in research into offensive language is partly due to the recent Workshops on Abusive Language Online, 4 as well as other fora, such as GermEval for German texts, 5 or TA-COS 6 and TRAC (Kumar et al.,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Prompted Toxicity in Neural Models Using REALTOXICITYPROMPTS and the same generation procedures outlined in §3, we measure toxic degeneration in out-of-the-box neural language models.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Our results show that while toxic prompts unsurprisingly yield higher toxicity in generations, nontoxic prompts still can still cause toxic generations at non-trivial rates (Table 2 ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We then use our model for a black-box attack against Google Perspective API for detecting toxic sentences, and find that 42% of our generated sentences are misclassified by the API, while humans agree that the sentences are toxic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>To obtain a stratified range of prompt toxicity, 10 we sample 25K sentences from four equal-width toxicity ranges ([0,.25), ..., [.75,1] ), for a total of 100K sentences.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>2017) used deep-learning based models specifically they employed RNN with a novel classification-specific attention mechanism and achieve state-of-the-art results on identifying attack and toxic content in Wikipedia comments.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>An alternate approach would be to have models detect situations that are likely to lead to toxic comments.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3222</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>To demonstrate the effectiveness our technique, we apply it to two tasks: (1) mitigating unintended bias in text classifiers by neutralizing identity terms; (2) improving classifier performance in a scarce data setting by forcing the model to focus on toxic terms.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>To investigate the second question, concerning the effect of context on the performance of toxicity classifiers, we created a larger dataset of 20k comments; 10k comments were annotated out of context, 10k in context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>D Frequency of Identity-terms in Toxic Samples and Overall To give a better understanding of how the weights change the distribution of datasets, we compare the original Jigsaw Toxicity dataset and the one with calculated weights for the frequency of a selection of identity-terms in toxic samples and overall, as shown in Table 8 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>2017) , toxic comments (Wulczyn et al.,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>They got low toxicity scores from Perspective API, but were annotated as offensive to at least one social group according to the SBIC dataset.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>In contrast, the Table 5 : Two examples of toxic/non-toxic comments that show the effects of the different shielding methods.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>A comment that contains a single swear word is easier to perturb to be classified as non-toxic than a comment that is toxic in its entirety.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2410</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>This indicates that the Wiki-dataset, annotated for toxicity, is not well suited for detecting sexist or racist tweets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>An example is considered toxic if its toxicity or severe toxicity scores are above 0.64 or 0.92, respectively.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>However, other researchers have created different taxonomies based on sub-kinds of toxic language (Table 2 ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4210</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>In this paper, we introduce the first publicly-available Levantine Hate Speech and Abusive (L-HSAB) Twitter dataset with the objective to be a benchmark dataset for automatic detection of online Levantine toxic contents.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>By deleting four words, more than 80% of the comments that were previously correctly classified as toxic (true positives) are classified as non-toxic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>A Class overlap and interpretation of \"toxicity\"  To see if our results generalize beyond threat, we experimented on the identity-hate class in Kaggle's toxic comment classification dataset.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>To perform the binary classification (toxic or not toxic), for each comment, we decided to use the following majority vote labelling: if [(# of very toxic and toxic annotations) &gt; (# of healthy and very healthy annotations)] and [(# of very toxic and toxic annotations) &gt; 2] comment is toxic otherwise comment is non-toxic Some examples of the toxic comments are: \"You are a big fat idiot, stop spamming my userspace\", \"What the fuck is your problem?\", \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3242</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We can also see that both the baseline and our method start to catch up with the rule based approach, where we give positive prediction if the toxic word is in the sentence, and eventually outperform it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>The presence of toxic content has become a major problem for many online communities.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>During a conversation, HaRe keeps track of toxicity estimates for all participants separately, updating the estimate for each speaker every time s/he makes an utterance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We considered the version of the dataset that corresponds to the Kaggle competition: \"Toxic Comment Classification Challenge\" (Google, 2018) which features 7 classes of toxicity: toxic, severe toxic, obscene, threat, insult, identity hate and non-toxic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>However, it also brings up opportunities for harsh discussions that can easily reach uncivilized, hateful, offensive or toxic levels (Shaw, 2011) .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>2017) then improved the accuracy on the toxicity and personal attack datasets using RNNs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Models trained with few examples are only accurate in predicting the majority class (non-toxic).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>2017) , we learn a 2-dimensional representation of toxicity and non-toxicity for every token in GPT-2's vocabulary, which we then use to boost the likelihood of non-toxic tokens.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3117</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>To make the improvement from applying our technique more visible, we fine-tuned the 117M GPT-2 with only toxic or negative sentences when producing the baseline models, and obtained GPT-senti-ext and GPT-toxic-ext, which frequently produce negative or toxic generated continuations.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2797</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>The explosion was triggered by an oil leak, though local media has not reported any toxic chemical spills.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>This threshold was to be treated as a measure of toxicity, filtering the online toxic content, prior to display of contents in the client's browser.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Related Work Different abusive and offense language identification problems have been explored in the literature ranging from aggression to cyber bullying, hate speech, toxic comments, and offensive language.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4136</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>DNN classifiers The task of toxic comment detection can be viewed from two perspectives: • A binary classification task: The neural network is directly trained to decide if a comment is toxic or nontoxic. •</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Our work so far has focused on single-line messages and negative toxicity detection.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>The attributes are toxicity, severe toxicity, identity attack, insult, profanity, threat, sexually explicit, flirtation, inflammatory, obscene, likely to reject (by New York Times moderators) and unsubstantial.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2212</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Hate speech, aggressive content, cyberbullying, and toxic comments are all different forms of offensive content (Schmidt &amp; Wiegand, 2017) .Figure 1 shows an example of an offensive tweet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Bias Estimation Method The construction of toxic language and hate speech corpora is commonly conducted based on keywords and/or hashtags.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>A) \"The green portions of a potato are toxic\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>2016) , gender (Reddy and Knight, 2016) or toxicity (Hosseini et al.,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3244</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We apply this technique to model fairness in toxic comment classification.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We conclude that a small but significant subset of manually labeled posts end up having wrong toxicity labels if the annotators are not provided with context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>The texts of the dataset were extracted from Wikipedia comments and have been labeled by human raters for six categories of toxic behavior: toxic, severe-toxic, obscene, threat, insult, and identity-hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Furthermore, by virtue of its online processing of the conversation, our system can provide substantial prior notice of upcoming derailment, triggering on average 3 comments (or 3 hours) before an overtly toxic comment is posted.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>More specifically, 57.4% of comments containing \"gay\" are toxic, while only 9.6% of all samples are toxic, as shown in Table 1 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>To obtain a stratified range of prompt toxicity, 10 we sample 25K sentences from four equal-width toxicity ranges ([0,.25), ..., [.75,1] ), for a total of 100K sentences.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>2018) , who find that the prevalence of abusive or toxic content online roughly ranges between 0.1% and 3%, and suggest that these corpora merely reflect the \"natural\" rates of toxicity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>However, an in-depth analysis of how sentiment can benefit toxicity detection has not been done in any of these papers, and a study of the use of sentiment in a subversive context has never been done.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Experiments Data We experiment with four datasets for two binary classification tasks: sentiment classification and toxicity detection.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Human validation To validate that toxic sentences remain toxic after our attack, we showed 5 independent annotators a total of 150 sentences from three classes: toxic sentences, non-toxic sentences, and attacked sentences (attacked by DISTFLIP-5).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Next, we added sentiment information to a toxicity detection neural network, and demonstrated that it does improve detection accuracy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>A Class overlap and interpretation of \"toxicity\"  To see if our results generalize beyond threat, we experimented on the identity-hate class in Kaggle's toxic comment classification dataset.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Hence, in this paper we focus on the following two foundational research questions: • RQ1: How often does context affect the toxicity of posts as perceived by humans in online conversations?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>To our surprise, the attention mechanism often marks punctuation as relevant in non-toxic comments.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>An example is considered toxic if its toxicity or severe toxicity scores are above 0.64 or 0.92, respectively.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We used this dataset to show that the perceived toxicity of a significant subset of posts (5.2% in our experiment) changes when context is (or is not) provided.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>For example, the toxicity classifier may unfairly over-predict the toxic class for comments discussing certain demographic groups.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>In this paper, we use adversarial training to mitigate this bias, introducing a hate speech classifier that learns to detect toxic sentences while demoting confounds corresponding to AAE texts.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We report ROC AUC, because both datasets are heavily unbalanced, with toxic comments being rare (Fig.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3985</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>They were to be classified based on the toxicity (i.e. toxic, severe toxic, obscene, insult, identity hate, and threat).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4126</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>In this article, we investigate several approaches based on different state-of-the-art DNN models and word representations for the task of automatic toxic comment detection.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>2017) language models: GPT-1, 5 https://github.com/conversationai/ perspectiveapi/blob/master/3-concepts/ score-normalization.md 6 To assess PERSPECTIVE API on human-generated text, the first three authors performed manual judgments of toxicity of a sample of 100 documents from OWTC, and found an 88% pairwise agreement (Pearson ⇢=0.83) with TOXICITY scores.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>In the past years, toxic comments and offensive speech are polluting the internet and manual inspection of these comments is becoming a tiresome task to manage.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>The authors create a manually labeled data set and perform an extensive study on which pragmatic and rhetorical devices are indicative of conversation toxicity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Toxic comments: The Toxic Comment Classification Challenge 5 was an open competition at Kaggle, which provided participants with comments from Wikipedia organized in six classes: toxic, severe toxic, obscene, threat, insult, identity hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>All of the updated models still preserve the utility in recognizing the overt toxicity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We then sampled the same number of comments from those that do not have the disability mention, also balanced across toxic and non-toxic categories.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>part being clearly negative -and increases the score sufficiently for the message to be classified as toxic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4044</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>From there we can observe that both BERT (toxic, toxicnorm), and GloVe-based (capsuleglove, cnnglove) neural networks are clearly the strongest models in the ensemble.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>2020) , which is among the most effective methods we tested at avoiding toxicity with toxic prompts.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>To obtain a stratified range of prompt toxicity, 10 we sample 25K sentences from four equal-width toxicity ranges ([0,.25), ..., [.75,1] ), for a total of 100K sentences.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>They were to be classified based on the toxicity (i.e. toxic, severe toxic, obscene, insult, identity hate, and threat).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>A promising alternative approach to reduce toxic discourse without recourse to censorship is so-called counter speech, which broadly refers to citizens' response to hateful speech in order to stop it, reduce its consequences, and discourage it [Benesch et al.,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>In contrast, the Table 5 : Two examples of toxic/non-toxic comments that show the effects of the different shielding methods.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>For instance, we discovered that we had to explicitly emphasize to all annotators the difference between when a reporter's words and viewpoints are toxic themselves, to when a politically toxic event or statement is reported, and that we are only interested in the first case.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3232</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>First, we tackle the problem of unintended bias in toxic comment classification (Dixon et al.,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Flagged messages are deemed toxic and, unlike WCC, all remaining messages are considered as non-toxic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Much NLP research is focused on finding and classifying offensive or toxic language, which is then either directly censored or flagged for human moderators to review.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We saw that the proportion between offense, toxicity, abuse or hate messages can vary across different datasets, and this factor greatly impacts the classifiers' performance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>This demonstrates that GPT-2 significantly increased the vocabulary range of the training set, specifically with offensive words likely to be relevant for toxic language classification.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We collected and share two datasets for investigating our research questions around the effect of context on the annotation of toxic comments (RQ1) and its detection by automated systems (RQ2).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Without any preset semantics of toxic content, they came up with the tool that could be manipulated through a modifiable threshold.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>After filtering out non-English segments and segments that were too long (&gt;50 words or &gt;1000 characters) or too short (&lt;5 words), we kept all the remaining comments with any toxic label (approx.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Prompted Toxicity in Neural Models Using REALTOXICITYPROMPTS and the same generation procedures outlined in §3, we measure toxic degeneration in out-of-the-box neural language models.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4132</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>The DNN network classifies these sequences of word embeddings as toxic or non-toxic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2407</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>2017) ), collected from English Wikipedia talk pages and annotated for toxicity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>To make the improvement from applying our technique more visible, we fine-tuned the 117M GPT-2 with only toxic or negative sentences when producing the baseline models, and obtained GPT-senti-ext and GPT-toxic-ext, which frequently produce negative or toxic generated continuations.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>We asked annotators whether sentences are toxic, and measured average annotated toxicity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>This part contains 160k comments from English Wikipedia talk pages, each labelled by approximately 10 annotators via crowd-sourcing, on a spectrum of how toxic/healthy the comment is with regard to the conversation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4148</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>To perform the binary classification (toxic or not toxic), for each comment, we decided to use the following majority vote labelling: if [(# of very toxic and toxic annotations) &gt; (# of healthy and very healthy annotations)] and [(# of very toxic and toxic annotations) &gt; 2] comment is toxic otherwise comment is non-toxic Some examples of the toxic comments are: \"You are a big fat idiot, stop spamming my userspace\", \"What the fuck is your problem?\", \"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            decade  \\\n",
       "30    (2000, 2005]   \n",
       "21    (2000, 2005]   \n",
       "1247  (2005, 2010]   \n",
       "661   (2005, 2010]   \n",
       "2127  (2005, 2010]   \n",
       "10    (2010, 2012]   \n",
       "9     (2010, 2012]   \n",
       "2144  (2010, 2012]   \n",
       "841   (2012, 2014]   \n",
       "835   (2012, 2014]   \n",
       "792   (2014, 2016]   \n",
       "839   (2014, 2016]   \n",
       "2742  (2016, 2018]   \n",
       "2746  (2016, 2018]   \n",
       "4104  (2016, 2018]   \n",
       "2645  (2016, 2018]   \n",
       "2302  (2016, 2018]   \n",
       "1154  (2016, 2018]   \n",
       "1608  (2016, 2018]   \n",
       "4089  (2016, 2018]   \n",
       "1983  (2016, 2018]   \n",
       "905   (2016, 2018]   \n",
       "4099  (2016, 2018]   \n",
       "2699  (2016, 2018]   \n",
       "2645  (2016, 2018]   \n",
       "720   (2016, 2018]   \n",
       "380   (2016, 2018]   \n",
       "1983  (2016, 2018]   \n",
       "2707  (2016, 2018]   \n",
       "382   (2016, 2018]   \n",
       "4207  (2016, 2018]   \n",
       "4104  (2016, 2018]   \n",
       "2327  (2016, 2018]   \n",
       "2699  (2016, 2018]   \n",
       "4122  (2018, 2020]   \n",
       "1913  (2018, 2020]   \n",
       "674   (2018, 2020]   \n",
       "90    (2018, 2020]   \n",
       "4255  (2018, 2020]   \n",
       "298   (2018, 2020]   \n",
       "572   (2018, 2020]   \n",
       "983   (2018, 2020]   \n",
       "2756  (2018, 2020]   \n",
       "1425  (2018, 2020]   \n",
       "347   (2018, 2020]   \n",
       "2391  (2018, 2020]   \n",
       "1230  (2018, 2020]   \n",
       "1890  (2018, 2020]   \n",
       "3260  (2018, 2020]   \n",
       "2259  (2018, 2020]   \n",
       "80    (2018, 2020]   \n",
       "4069  (2018, 2020]   \n",
       "1921  (2018, 2020]   \n",
       "4152  (2018, 2020]   \n",
       "25    (2018, 2020]   \n",
       "934   (2018, 2020]   \n",
       "2132  (2018, 2020]   \n",
       "97    (2018, 2020]   \n",
       "1417  (2018, 2020]   \n",
       "1448  (2018, 2020]   \n",
       "298   (2018, 2020]   \n",
       "1574  (2018, 2020]   \n",
       "1233  (2018, 2020]   \n",
       "1459  (2018, 2020]   \n",
       "672   (2018, 2020]   \n",
       "91    (2018, 2020]   \n",
       "405   (2018, 2020]   \n",
       "3119  (2018, 2020]   \n",
       "1450  (2018, 2020]   \n",
       "57    (2018, 2020]   \n",
       "73    (2018, 2020]   \n",
       "255   (2018, 2020]   \n",
       "3128  (2018, 2020]   \n",
       "331   (2018, 2020]   \n",
       "86    (2018, 2020]   \n",
       "4208  (2018, 2020]   \n",
       "2255  (2018, 2020]   \n",
       "565   (2018, 2020]   \n",
       "95    (2018, 2020]   \n",
       "535   (2018, 2020]   \n",
       "352   (2018, 2020]   \n",
       "384   (2018, 2020]   \n",
       "343   (2018, 2020]   \n",
       "1790  (2018, 2020]   \n",
       "1457  (2018, 2020]   \n",
       "3222  (2018, 2020]   \n",
       "41    (2018, 2020]   \n",
       "1637  (2018, 2020]   \n",
       "774   (2018, 2020]   \n",
       "1830  (2018, 2020]   \n",
       "2360  (2018, 2020]   \n",
       "977   (2018, 2020]   \n",
       "2410  (2018, 2020]   \n",
       "1469  (2018, 2020]   \n",
       "61    (2018, 2020]   \n",
       "4210  (2018, 2020]   \n",
       "974   (2018, 2020]   \n",
       "684   (2018, 2020]   \n",
       "4145  (2018, 2020]   \n",
       "3242  (2018, 2020]   \n",
       "2302  (2018, 2020]   \n",
       "961   (2018, 2020]   \n",
       "512   (2018, 2020]   \n",
       "1321  (2018, 2020]   \n",
       "2009  (2018, 2020]   \n",
       "934   (2018, 2020]   \n",
       "361   (2018, 2020]   \n",
       "3117  (2018, 2020]   \n",
       "2272  (2018, 2020]   \n",
       "2797  (2018, 2020]   \n",
       "2375  (2018, 2020]   \n",
       "2749  (2018, 2020]   \n",
       "4136  (2018, 2020]   \n",
       "1459  (2018, 2020]   \n",
       "1594  (2018, 2020]   \n",
       "2212  (2018, 2020]   \n",
       "1318  (2018, 2020]   \n",
       "1076  (2018, 2020]   \n",
       "1420  (2018, 2020]   \n",
       "3244  (2018, 2020]   \n",
       "39    (2018, 2020]   \n",
       "1666  (2018, 2020]   \n",
       "2139  (2018, 2020]   \n",
       "1628  (2018, 2020]   \n",
       "343   (2018, 2020]   \n",
       "369   (2018, 2020]   \n",
       "1432  (2018, 2020]   \n",
       "1168  (2018, 2020]   \n",
       "399   (2018, 2020]   \n",
       "1456  (2018, 2020]   \n",
       "511   (2018, 2020]   \n",
       "33    (2018, 2020]   \n",
       "965   (2018, 2020]   \n",
       "933   (2018, 2020]   \n",
       "38    (2018, 2020]   \n",
       "1835  (2018, 2020]   \n",
       "1884  (2018, 2020]   \n",
       "58    (2018, 2020]   \n",
       "3985  (2018, 2020]   \n",
       "4126  (2018, 2020]   \n",
       "326   (2018, 2020]   \n",
       "1083  (2018, 2020]   \n",
       "930   (2018, 2020]   \n",
       "2750  (2018, 2020]   \n",
       "1829  (2018, 2020]   \n",
       "431   (2018, 2020]   \n",
       "2338  (2018, 2020]   \n",
       "4044  (2018, 2020]   \n",
       "565   (2018, 2020]   \n",
       "343   (2018, 2020]   \n",
       "2184  (2018, 2020]   \n",
       "4174  (2018, 2020]   \n",
       "2360  (2018, 2020]   \n",
       "2588  (2018, 2020]   \n",
       "3232  (2018, 2020]   \n",
       "2613  (2018, 2020]   \n",
       "4268  (2018, 2020]   \n",
       "1236  (2018, 2020]   \n",
       "677   (2018, 2020]   \n",
       "59    (2018, 2020]   \n",
       "4252  (2018, 2020]   \n",
       "1800  (2018, 2020]   \n",
       "535   (2018, 2020]   \n",
       "4132  (2018, 2020]   \n",
       "2407  (2018, 2020]   \n",
       "2271  (2018, 2020]   \n",
       "403   (2018, 2020]   \n",
       "4138  (2018, 2020]   \n",
       "4148  (2018, 2020]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 sentence  \n",
       "30                                                                                                                                                                                                                                                                                                                                                                                  The PROPERTY predicate is the result of the normalization of strings expressing physical or chemical properties of the toxic product.  \n",
       "21                                                                                                                                                                                                                                                                                                                                     The analysis phase can thus be seen as a paraphrase detection phase, as it unifies in a same representation different ways of expressing similar information about toxic products.  \n",
       "1247                                                                                                                                                                                                                                                                                                                                                        Generally speaking, violations of long-span constituents have a more negative impact on performance than short-span violations if these violations are toxic.  \n",
       "661                                                                                                                                                                                                                                                                                                                                Getting back to GST, let us consider a sentence, (1) The bailout plan was likely to depend on private investors to purchase the toxic assets that wiped out the capital of many banks.  \n",
       "2127                                                                                                                                                                                                                                                                                                                                               Figure 1 illustrates a simplified two-dimensional semantic space and the changes to semantics that would occur as \"blick\" begins to co-occur with toxic-related words.  \n",
       "10                                                                                                                                                                                                                                                                                                                                              The term cytotoxic will be translated into German as zytotoxisch whereas in French it can be translated as cytotoxique or toxique pour les cellules 'toxic to the cells'.  \n",
       "9                                                                                                                                                                                                                                              These components may be either free (i.e. they can occur in texts as autonomous lexical items like toxicity in cardiotoxicity) or bound (i.e. they cannot occur as autonomous lexical items, in that case they correspond to bound morphemes likecardioin cardiotoxicity).  \n",
       "2144                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              toxic?,  \n",
       "841                                                                                                                                                                                                                                                                                                                                                           The intuition here is that to know whether few apples are toxic, it is sufficient to know which apples are toxic; those non-apple toxicants are irrelevant.  \n",
       "835                                                                                                                                                                                                                                                                                                                                          During the 2011 East Japan Earthquake and Tsunami Disaster, we had found a number of false information spread on Twitter, e.g., \"The Cosmo Oil explosion causes toxic rain.\"  \n",
       "792                                                                           Moreover, according to the etiology of the lesion and the disease associated with it (toxic, metabolic, traumatic or degenerative diseases), types of dysarthria vary with respect to pathophysiologies determining the kind of deficits in the motor execution and/or control of speech movements (deficits in speed, range, strength, rigidity/steadiness, tonus, precision/accuracy, and/or coordination) (Murdoch, 1998 , Duffy 2013) .  \n",
       "839                                                                                                                                                                                                                                                                                                                                                                                                                        8) Effect of calcium chloride and 4aminopyridine therapy on desipramine toxicity in rats . . .  \n",
       "2742                                                                                                                                                                                                                                                                                                            In turn, these toxic attacks are especially disruptive in Wikipedia since they undermine the social fabric of the community as well as the ability of editors to contribute (Henner and Sefidari, 2016) .  \n",
       "2746  This provides a toxicity score t for all comments in our dataset, which we use to preselect two sets of conversations: (a) candidate conversations that are civil throughout, i.e., conversations in which all comments (including the initial exchange) are not labeled as toxic (t < 0.4); and (b) candidate conversations that turn toxic after the first (civil) exchange, i.e., conversations in which the N -th comment (N > 2) is labeled toxic (t ≥ 0.6), but all the preceding comments are not (t < 0.4).  \n",
       "4104                                                                                                                                                                                                                                                                                                                                                                                                      Indeed, the term \"toxic\" is a general description of subreddits with habitual and even emphasized disalignment.  \n",
       "2645                                                                                                                                                                                                                                                                                                                                                                                                                   As patterns of toxic and hateful behavior on Reddit are more well-studied (Chandrasekharan et al.,  \n",
       "2302                                                                                                                                                                                                                                                                                                                                                                                                                                  The distribution of toxicity levels by constructiveness label is shown in Table 3 .  \n",
       "1154                                                                                                                                                                                                                                                                                                                                                       Toxicity judgment is well-suited for the community-driven approach because toxic language is one component of the broader global problem of online harassment.  \n",
       "1608                                                                                                                                                                                                                                                          The following forms are recognized by our parser: (1) evaluative expressions in attribute-value form, where the attribute is one of the concepts of the controversial issue concept lattice: Vaccine development is very expensive, car exhaust is toxic. (  \n",
       "4089                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The question posed was: How toxic is the comment?  \n",
       "1983                                                                                                                                                                                                                                                                                                                                                  Related work Offensive language serves many purposes in everyday discourse: from deliberate effect in humour to self-directed profanity to toxic or abusive intent.  \n",
       "905                                                                                                                                                                                                                                                                                                                                                                            This can be exploited when dealing with strongly imbalanced datasets, as often the case in toxic comment classification and related tasks.  \n",
       "4099                                                                                                                                                                                                                                                                                                                                                  It could be the case, in some situations, that a moderator may allow a somewhat toxic comment if it contributes to the conversation, i.e., if it is constructive. (  \n",
       "2699                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Moderation of toxic behavior.  \n",
       "2645                                                                                                                                                                                                                                                                                                                                                                                                                   As patterns of toxic and hateful behavior on Reddit are more well-studied (Chandrasekharan et al.,  \n",
       "720                                                                                                                                                                                                                                                                                                                                                                                           In \"simple\" mode, users rate toxicity on a three-point scale; in \"advanced\" mode, users also classify the type of toxicity.  \n",
       "380                                                                                                                                                                                                                                                                                                                                                                Toxicity dataset: This dataset was created like the previous one, but contains more comments (159,686), now labeled as toxic (reject) or not (accept).  \n",
       "1983                                                                                                                                                                                                                                                                                                                                                  Related work Offensive language serves many purposes in everyday discourse: from deliberate effect in humour to self-directed profanity to toxic or abusive intent.  \n",
       "2707                                                                                                                                                                                                                                                                                                                                    9 The Jigsaw Toxicity Kaggle Competition: goo.gl/ N6UGPK nearly 33% of toxic comments are removed within a day; And over 82% of severely toxic comments are deleted within a day.  \n",
       "382                                                                                                                                                                                                                                                                                                                                                                                                                                                                          160K comments labeled as being toxic or not.  \n",
       "4207                                                                                                                                                                                                                                   For example, one of the cluster contained more of neutral words, another cluster contained highly aggressive and abusive words, and the third cluster contained some toxic words along with place and country names related to one's origin which were used in some foul comments.  \n",
       "4104                                                                                                                                                                                                                                                                                                                                                                                                      Indeed, the term \"toxic\" is a general description of subreddits with habitual and even emphasized disalignment.  \n",
       "2327                                                                                                                                                                                                                                                                                                                                                              We developed two systems (aggression data vs. aggression + toxicity data) that were tested in two different scenarios (Facebook data vs. Social Media).  \n",
       "2699                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Moderation of toxic behavior.  \n",
       "4122                                                                                                                                                                                                                                                                                                                                                                                                                                                We refer to these collectively with the generic term of toxic speech.  \n",
       "1913                                                                                                                                                                                                                                                                                                                                                                             Both researchers and social platforms have been focused on developing models to detect toxicity in online communication for a while now.  \n",
       "674                                                                                                                                                                                                                                                                                                                                                                                                                      It contains human-labeled English Wikipedia comments in six different classes of toxic language.  \n",
       "90                                                                                                                                                                                                                                                                                                                                                                                          On top of the BERT [CLS] representation, we added a FFNN of 128 hidden nodes and a sigmoid to yield the toxicity probability.  \n",
       "4255                                                                                                                                                                                                                                                                           In 2018, Kaggle hosted a Toxic Comment Classification competition in association with Jigsaw, which focused on classifying Wikipedia comments into one of six categories: insult, obscene, severe toxic, threat & identity hate and toxic.  \n",
       "298                                                                                                                                                                                                                 Figure 1 illustrates how phrases in the African American English dialect (AAE) are labelled by a publicly available toxicity detection tool as much Figure 1 : Phrases in African American English (AAE), their non-AAE equivalents (from Spears, 1998), and toxicity scores from PerspectiveAPI.com.  \n",
       "572                                                                                                                                                                                                                                                                                                                   In our work, we study toxic degeneration by both out-of-the-box and controlled models using 100K naturally occurring prompts, including some that do not contain identity mentions (see Figure 1 ).  \n",
       "983                                                                                                                                                                                                                                                                                                                                                                A limitation of attribution-based explanations for offensive language detection seems to be a focus on words that are toxic regardless of the context.  \n",
       "2756                                                                                                                                                                                                                                                                                                                                                                                                                        Then we trained a multi-class classifier with nine categories: eight toxic and one non-toxic.  \n",
       "1425                                                                                                                                                                                                                                                                                                                                                               These attempts to bypass the toxicity detection system are called subverting the system, and toxic users doing it are referred to as subversive users.  \n",
       "347                                                                                                                                                                                                                    We characterize toxicity in prompted generations with two metrics: 1) the expected maxi-mum toxicity over k = 25 generations, which we estimate with a mean and standard deviation; and 2) the empirical probability of generating a span with TOXICITY 0.5 at least once over k = 25 generations.  \n",
       "2391                                                                                                                                                                                                                                                                                             As expected, the estimated prevalence of toxic tweets directed at Lewis depends on where this arbitrary threshold is set-he received only two tweets with toxicity > 0.8, but 10 with toxicity > 0.7, a 5-fold increase.  \n",
       "1230                                                                                                                                                                                                                                                                                                                                                              Amievalita-misogynous' appears to be close to Davidson's 'offensive' and Toxkaggle's 'toxicity', but it is also not so far away from Waseem's 'sexism'.  \n",
       "1890                                                                                                                                                                                                                                                                                                                                                                                                    In comparison, 70.98% of the tweets in the FDCL18 test set that were labeled as AAE were also annotated as toxic.  \n",
       "3260                                                                                                                                                                                                                                                                                                    A common problem of these models is the presence of bias towards some words (e.g. woman, black, jew or женщина, черный, еврей) that are not toxic, but serve as triggers for the classifier due to model caveats.  \n",
       "2259                                                                                                                                                                                                                                                         Y) A catalytic converter is an exhaust emission control device that converts toxic gases and pollutants in exhaust gas from an internal combustion engine into less-toxic pollutants by catalyzing a redox reaction (an oxidation and a reduction reaction).  \n",
       "80                                                                                                                                                                                                                                                                                                                                                         PERSPECTIVE The third context-insensitive classifier is a CNN-based model for toxicity detection, trained on millions of user comments from online publishers.  \n",
       "4069                                                                                                                                                                                                               Our best systems in each of the three OffensEval tasks placed in the middle of the comparative evaluation, ranking 57 th of 103 in task A, 39 th of 75 in task B, and 44 th of 65 in task C. Introduction Social media is notorious for providing a platform for offensive, toxic, and hateful speech.  \n",
       "1921                                                                                                                                                                                                                                                                                                                                                                                                     In our setup, the loss from an extra classifier head is weighted equal to the loss from the toxicity classifier.  \n",
       "4152                                               To perform the binary classification (toxic or not toxic), for each comment, we decided to use the following majority vote labelling: if [(# of very toxic and toxic annotations) > (# of healthy and very healthy annotations)] and [(# of very toxic and toxic annotations) > 2] comment is toxic otherwise comment is non-toxic Some examples of the toxic comments are: \"You are a big fat idiot, stop spamming my userspace\", \"What the fuck is your problem?\", \"  \n",
       "25                                                                                                                                                                                                                                                                                                                          Moreover, a small but significant subset of manually labeled posts (5% in one of our experiments) end up having the opposite toxicity labels if the annotators are not provided with context.  \n",
       "934                                                                                                                                                                                                                                                                                                                                                                                                        An example is considered toxic if its toxicity or severe toxicity scores are above 0.64 or 0.92, respectively.  \n",
       "2132                                                                                                                                                                                                                                                                                                                                                                                         1 https://github.com/ShannonAI/CorefQA Original Passage In addition , many people were poisoned when toxic gas was released.  \n",
       "97                                                                                                                                                                                                                                                                            The performance of Perspective in this subtask is particularly interesting, considering that the training data for these models were not labeled for offensiveness, but rather for other attributes such as toxicity, threats, and insults.  \n",
       "1417                                                                                                                                                                                                                                                                                                                                                                                                  Specifically, we include a pre-trained toxicity scorer that scores a given text on six dimensions of toxic content.  \n",
       "1448                                                                                                                                                                                                                                                                                                                                                                                     Indeed, as mentioned in Sections 1 and 2, it is trivial for a subversive user to mask toxic keywords to bypass toxicity filters.  \n",
       "298                                                                                                                                                                                                                 Figure 1 illustrates how phrases in the African American English dialect (AAE) are labelled by a publicly available toxicity detection tool as much Figure 1 : Phrases in African American English (AAE), their non-AAE equivalents (from Spears, 1998), and toxicity scores from PerspectiveAPI.com.  \n",
       "1574                                                                                                                                                                                                                                                                                                                                                                              Yet, for the game League of Legends, researchers found that this 1% of the player population only accounted for 5% of the toxic speech.  \n",
       "1233                                                                                                                                                                                                                                                                                                                                                                             unexpected, since their labels suggest that the main difference between these two categories is the intensity of the expressed toxicity.  \n",
       "1459                                                                                                                                                                                                                                                                                                                                                                                                                                 Our work so far has focused on single-line messages and negative toxicity detection.  \n",
       "672                                                                                                                                                                                                                                                                                                                                                                                                                       We focused on the threat class, but also replicated our results on another toxic class ( §4.6).  \n",
       "91                                                                                                                                                                                                                                                                                                                          This paper presents the application of two strong baseline systems for toxicity detection and evaluates their performance in identifying and categorizing offensive language in social media.  \n",
       "405                                                                                                                                                                                                                                                                                                                                                                                                We found that 89.8% of the toxic sentences were annotated as toxic, compared to 87.6% in the attacked toxic sentences.  \n",
       "3119                                                                                                                                                                                                                                                                                                                                                                          In this work, we propose a framework aimed at fortifying existing toxic speech detectors without a large labeled corpus of veiled toxicity.  \n",
       "1450                                                                                                                                                                                                                                                                                                                                                                                                                                We address the task of automatically detecting toxic content in user generated texts.  \n",
       "57                                                                                                                                                                                                                                                                                 The perceived toxicity of some comments may be increasing when context is provided, but for other comments it may be decreasing, and these effects may be partially cancelling each other when measuring the change in toxicity ratio.  \n",
       "73                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The toxicity ratios of CAT-LARGE (Fig.  \n",
       "255                                                                                                                                                                                                                                                                                                                                                                     Figure 1a shows the results for toxicity prediction (Jigsaw, 2017) , which outputs a score ∈ [0, 1], with higher scores indicating more toxicity.  \n",
       "3128                                                                                                                                                                                                                                                                                                                                                                                                   We pick the least toxic m posts as our veiled offensive language set so that 1 m m i=1 tox(x (i) ) = tox general .  \n",
       "331                                                                                                                                                                                                                                                                                                                                                                  Though trained on a much larger corpus, GPT-3's unprompted toxicity also mirrors Figure 2 : Neural models generate toxicity, even with no prompting.  \n",
       "86                                                                                                                                  We also found no evidence that context actually improves the performance of toxicity classifiers, having tried both simple and more powerful classifiers, having experimented with several methods to make the classifiers context aware, and having also considered the effect of gold labels obtained out of context vs. gold labels obtained by showing context to the annotators.  \n",
       "4208                                                                                                                                                                                The psychological-communicative Reduced cues approach (Sproull and Kiesler, 1986) argues that properties of online environments may cause toxic online disinhibition (Suler, 2004) : people feel less restraint because of the absence of social-context cues, anonymity, invisibility, asynchronicity, or minimization of authority.  \n",
       "2255                                                                                                                                                                                                                                                                                                                                                                                                                           @user @user @user @user @user @user @user what a stupid incompetent devious and toxic pm !  \n",
       "565                                                                                                                                                                                                                                                                                                                                                                                                                  2020) , which is among the most effective methods we tested at avoiding toxicity with toxic prompts.  \n",
       "95                                                                                                                                                                                                              Apart from the growing volume of popular press concerning toxicity online, the increased interest in research into offensive language is partly due to the recent Workshops on Abusive Language Online, 4 as well as other fora, such as GermEval for German texts, 5 or TA-COS 6 and TRAC (Kumar et al.,  \n",
       "535                                                                                                                                                                                                                                                                                                                               Prompted Toxicity in Neural Models Using REALTOXICITYPROMPTS and the same generation procedures outlined in §3, we measure toxic degeneration in out-of-the-box neural language models.  \n",
       "352                                                                                                                                                                                                                                                                                                                              Our results show that while toxic prompts unsurprisingly yield higher toxicity in generations, nontoxic prompts still can still cause toxic generations at non-trivial rates (Table 2 ).  \n",
       "384                                                                                                                                                                                                                                                                                We then use our model for a black-box attack against Google Perspective API for detecting toxic sentences, and find that 42% of our generated sentences are misclassified by the API, while humans agree that the sentences are toxic.  \n",
       "343                                                                                                                                                                                                                                                                                                                                            To obtain a stratified range of prompt toxicity, 10 we sample 25K sentences from four equal-width toxicity ranges ([0,.25), ..., [.75,1] ), for a total of 100K sentences.  \n",
       "1790                                                                                                                                                                                                                                                                                    2017) used deep-learning based models specifically they employed RNN with a novel classification-specific attention mechanism and achieve state-of-the-art results on identifying attack and toxic content in Wikipedia comments.  \n",
       "1457                                                                                                                                                                                                                                                                                                                                                                                                           An alternate approach would be to have models detect situations that are likely to lead to toxic comments.  \n",
       "3222                                                                                                                                                                                                                                             To demonstrate the effectiveness our technique, we apply it to two tasks: (1) mitigating unintended bias in text classifiers by neutralizing identity terms; (2) improving classifier performance in a scarce data setting by forcing the model to focus on toxic terms.  \n",
       "41                                                                                                                                                                                                                                                                                              To investigate the second question, concerning the effect of context on the performance of toxicity classifiers, we created a larger dataset of 20k comments; 10k comments were annotated out of context, 10k in context.  \n",
       "1637                                                                                                                                                                         D Frequency of Identity-terms in Toxic Samples and Overall To give a better understanding of how the weights change the distribution of datasets, we compare the original Jigsaw Toxicity dataset and the one with calculated weights for the frequency of a selection of identity-terms in toxic samples and overall, as shown in Table 8 .  \n",
       "774                                                                                                                                                                                                                                                                                                                                                                                                                                                                               2017) , toxic comments (Wulczyn et al.,  \n",
       "1830                                                                                                                                                                                                                                                                                                                                                                       They got low toxicity scores from Perspective API, but were annotated as offensive to at least one social group according to the SBIC dataset.  \n",
       "2360                                                                                                                                                                                                                                                                                                                                                                                        In contrast, the Table 5 : Two examples of toxic/non-toxic comments that show the effects of the different shielding methods.  \n",
       "977                                                                                                                                                                                                                                                                                                                                                                          A comment that contains a single swear word is easier to perturb to be classified as non-toxic than a comment that is toxic in its entirety.  \n",
       "2410                                                                                                                                                                                                                                                                                                                                                                                              This indicates that the Wiki-dataset, annotated for toxicity, is not well suited for detecting sexist or racist tweets.  \n",
       "1469                                                                                                                                                                                                                                                                                                                                                                                                       An example is considered toxic if its toxicity or severe toxicity scores are above 0.64 or 0.92, respectively.  \n",
       "61                                                                                                                                                                                                                                                                                                                                                                                                          However, other researchers have created different taxonomies based on sub-kinds of toxic language (Table 2 ).  \n",
       "4210                                                                                                                                                                                                                                                                                         In this paper, we introduce the first publicly-available Levantine Hate Speech and Abusive (L-HSAB) Twitter dataset with the objective to be a benchmark dataset for automatic detection of online Levantine toxic contents.  \n",
       "974                                                                                                                                                                                                                                                                                                                                                                By deleting four words, more than 80% of the comments that were previously correctly classified as toxic (true positives) are classified as non-toxic.  \n",
       "684                                                                                                                                                                                                                                                                                                                        A Class overlap and interpretation of \"toxicity\"  To see if our results generalize beyond threat, we experimented on the identity-hate class in Kaggle's toxic comment classification dataset.  \n",
       "4145                                               To perform the binary classification (toxic or not toxic), for each comment, we decided to use the following majority vote labelling: if [(# of very toxic and toxic annotations) > (# of healthy and very healthy annotations)] and [(# of very toxic and toxic annotations) > 2] comment is toxic otherwise comment is non-toxic Some examples of the toxic comments are: \"You are a big fat idiot, stop spamming my userspace\", \"What the fuck is your problem?\", \"  \n",
       "3242                                                                                                                                                                                                                                                                                                          We can also see that both the baseline and our method start to catch up with the rule based approach, where we give positive prediction if the toxic word is in the sentence, and eventually outperform it.  \n",
       "2302                                                                                                                                                                                                                                                                                                                                                                                                                                The presence of toxic content has become a major problem for many online communities.  \n",
       "961                                                                                                                                                                                                                                                                                                                                             During a conversation, HaRe keeps track of toxicity estimates for all participants separately, updating the estimate for each speaker every time s/he makes an utterance.  \n",
       "512                                                                                                                                                                                                                                                         We considered the version of the dataset that corresponds to the Kaggle competition: \"Toxic Comment Classification Challenge\" (Google, 2018) which features 7 classes of toxicity: toxic, severe toxic, obscene, threat, insult, identity hate and non-toxic.  \n",
       "1321                                                                                                                                                                                                                                                                                                                                                                  However, it also brings up opportunities for harsh discussions that can easily reach uncivilized, hateful, offensive or toxic levels (Shaw, 2011) .  \n",
       "2009                                                                                                                                                                                                                                                                                                                                                                                                                            2017) then improved the accuracy on the toxicity and personal attack datasets using RNNs.  \n",
       "934                                                                                                                                                                                                                                                                                                                                                                                                                      Models trained with few examples are only accurate in predicting the majority class (non-toxic).  \n",
       "361                                                                                                                                                                                                                                                                                                                                    2017) , we learn a 2-dimensional representation of toxicity and non-toxicity for every token in GPT-2's vocabulary, which we then use to boost the likelihood of non-toxic tokens.  \n",
       "3117                                                                                                                                                                                                                                                                                                             Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias.  \n",
       "2272                                                                                                                                                                                                                           To make the improvement from applying our technique more visible, we fine-tuned the 117M GPT-2 with only toxic or negative sentences when producing the baseline models, and obtained GPT-senti-ext and GPT-toxic-ext, which frequently produce negative or toxic generated continuations.  \n",
       "2797                                                                                                                                                                                                                                                                                                                                                                                                           The explosion was triggered by an oil leak, though local media has not reported any toxic chemical spills.  \n",
       "2375                                                                                                                                                                                                                                                                                                                                                                 This threshold was to be treated as a measure of toxicity, filtering the online toxic content, prior to display of contents in the client's browser.  \n",
       "2749                                                                                                                                                                                                                                                                                                     Related Work Different abusive and offense language identification problems have been explored in the literature ranging from aggression to cyber bullying, hate speech, toxic comments, and offensive language.  \n",
       "4136                                                                                                                                                                                                                                                                                                       DNN classifiers The task of toxic comment detection can be viewed from two perspectives: • A binary classification task: The neural network is directly trained to decide if a comment is toxic or nontoxic. •  \n",
       "1459                                                                                                                                                                                                                                                                                                                                                                                                                                 Our work so far has focused on single-line messages and negative toxicity detection.  \n",
       "1594                                                                                                                                                                                                                                                                                                   The attributes are toxicity, severe toxicity, identity attack, insult, profanity, threat, sexually explicit, flirtation, inflammatory, obscene, likely to reject (by New York Times moderators) and unsubstantial.  \n",
       "2212                                                                                                                                                                                                                                                                                                                          Hate speech, aggressive content, cyberbullying, and toxic comments are all different forms of offensive content (Schmidt & Wiegand, 2017) .Figure 1 shows an example of an offensive tweet.  \n",
       "1318                                                                                                                                                                                                                                                                                                                                                                           Bias Estimation Method The construction of toxic language and hate speech corpora is commonly conducted based on keywords and/or hashtags.  \n",
       "1076                                                                                                                                                                                                                                                                                                                                                                                                                                                                       A) \"The green portions of a potato are toxic\".  \n",
       "1420                                                                                                                                                                                                                                                                                                                                                                                                                                                2016) , gender (Reddy and Knight, 2016) or toxicity (Hosseini et al.,  \n",
       "3244                                                                                                                                                                                                                                                                                                                                                                                                                                           We apply this technique to model fairness in toxic comment classification.  \n",
       "39                                                                                                                                                                                                                                                                                                                                                         We conclude that a small but significant subset of manually labeled posts end up having wrong toxicity labels if the annotators are not provided with context.  \n",
       "1666                                                                                                                                                                                                                                                                                                         The texts of the dataset were extracted from Wikipedia comments and have been labeled by human raters for six categories of toxic behavior: toxic, severe-toxic, obscene, threat, insult, and identity-hate.  \n",
       "2139                                                                                                                                                                                                                                                                                Furthermore, by virtue of its online processing of the conversation, our system can provide substantial prior notice of upcoming derailment, triggering on average 3 comments (or 3 hours) before an overtly toxic comment is posted.  \n",
       "1628                                                                                                                                                                                                                                                                                                                                                                                     More specifically, 57.4% of comments containing \"gay\" are toxic, while only 9.6% of all samples are toxic, as shown in Table 1 .  \n",
       "343                                                                                                                                                                                                                                                                                                                                            To obtain a stratified range of prompt toxicity, 10 we sample 25K sentences from four equal-width toxicity ranges ([0,.25), ..., [.75,1] ), for a total of 100K sentences.  \n",
       "369                                                                                                                                                                                                                                                                                                                            2018) , who find that the prevalence of abusive or toxic content online roughly ranges between 0.1% and 3%, and suggest that these corpora merely reflect the \"natural\" rates of toxicity.  \n",
       "1432                                                                                                                                                                                                                                                                                                             However, an in-depth analysis of how sentiment can benefit toxicity detection has not been done in any of these papers, and a study of the use of sentiment in a subversive context has never been done.  \n",
       "1168                                                                                                                                                                                                                                                                                                                                                                              Experiments Data We experiment with four datasets for two binary classification tasks: sentiment classification and toxicity detection.  \n",
       "399                                                                                                                                                                                                                                                               Human validation To validate that toxic sentences remain toxic after our attack, we showed 5 independent annotators a total of 150 sentences from three classes: toxic sentences, non-toxic sentences, and attacked sentences (attacked by DISTFLIP-5).  \n",
       "1456                                                                                                                                                                                                                                                                                                                                                                               Next, we added sentiment information to a toxicity detection neural network, and demonstrated that it does improve detection accuracy.  \n",
       "511                                                                                                                                                                                                                                                                                                                        A Class overlap and interpretation of \"toxicity\"  To see if our results generalize beyond threat, we experimented on the identity-hate class in Kaggle's toxic comment classification dataset.  \n",
       "33                                                                                                                                                                                                                                                                                                                         Hence, in this paper we focus on the following two foundational research questions: • RQ1: How often does context affect the toxicity of posts as perceived by humans in online conversations?  \n",
       "965                                                                                                                                                                                                                                                                                                                                                                                                                   To our surprise, the attention mechanism often marks punctuation as relevant in non-toxic comments.  \n",
       "933                                                                                                                                                                                                                                                                                                                                                                                                        An example is considered toxic if its toxicity or severe toxicity scores are above 0.64 or 0.92, respectively.  \n",
       "38                                                                                                                                                                                                                                                                                                                                                       We used this dataset to show that the perceived toxicity of a significant subset of posts (5.2% in our experiment) changes when context is (or is not) provided.  \n",
       "1835                                                                                                                                                                                                                                                                                                                                                                                   For example, the toxicity classifier may unfairly over-predict the toxic class for comments discussing certain demographic groups.  \n",
       "1884                                                                                                                                                                                                                                                                                                                    In this paper, we use adversarial training to mitigate this bias, introducing a hate speech classifier that learns to detect toxic sentences while demoting confounds corresponding to AAE texts.  \n",
       "58                                                                                                                                                                                                                                                                                                                                                                                                                  We report ROC AUC, because both datasets are heavily unbalanced, with toxic comments being rare (Fig.  \n",
       "3985                                                                                                                                                                                                                                                                                                                                                                                             They were to be classified based on the toxicity (i.e. toxic, severe toxic, obscene, insult, identity hate, and threat).  \n",
       "4126                                                                                                                                                                                                                                                                                                                                        In this article, we investigate several approaches based on different state-of-the-art DNN models and word representations for the task of automatic toxic comment detection.  \n",
       "326                                                                                                                                               2017) language models: GPT-1, 5 https://github.com/conversationai/ perspectiveapi/blob/master/3-concepts/ score-normalization.md 6 To assess PERSPECTIVE API on human-generated text, the first three authors performed manual judgments of toxicity of a sample of 100 documents from OWTC, and found an 88% pairwise agreement (Pearson ⇢=0.83) with TOXICITY scores.  \n",
       "1083                                                                                                                                                                                                                                                                                                                                                     In the past years, toxic comments and offensive speech are polluting the internet and manual inspection of these comments is becoming a tiresome task to manage.  \n",
       "930                                                                                                                                                                                                                                                                                                                                                      The authors create a manually labeled data set and perform an extensive study on which pragmatic and rhetorical devices are indicative of conversation toxicity.  \n",
       "2750                                                                                                                                                                                                                                                                      Toxic comments: The Toxic Comment Classification Challenge 5 was an open competition at Kaggle, which provided participants with comments from Wikipedia organized in six classes: toxic, severe toxic, obscene, threat, insult, identity hate.  \n",
       "1829                                                                                                                                                                                                                                                                                                                                                                                                                              All of the updated models still preserve the utility in recognizing the overt toxicity.  \n",
       "431                                                                                                                                                                                                                                                                                                                                                                  We then sampled the same number of comments from those that do not have the disability mention, also balanced across toxic and non-toxic categories.  \n",
       "2338                                                                                                                                                                                                                                                                                                                                                                                                         part being clearly negative -and increases the score sufficiently for the message to be classified as toxic.  \n",
       "4044                                                                                                                                                                                                                                                                                                                                              From there we can observe that both BERT (toxic, toxicnorm), and GloVe-based (capsuleglove, cnnglove) neural networks are clearly the strongest models in the ensemble.  \n",
       "565                                                                                                                                                                                                                                                                                                                                                                                                                  2020) , which is among the most effective methods we tested at avoiding toxicity with toxic prompts.  \n",
       "343                                                                                                                                                                                                                                                                                                                                            To obtain a stratified range of prompt toxicity, 10 we sample 25K sentences from four equal-width toxicity ranges ([0,.25), ..., [.75,1] ), for a total of 100K sentences.  \n",
       "2184                                                                                                                                                                                                                                                                                                                                                                                             They were to be classified based on the toxicity (i.e. toxic, severe toxic, obscene, insult, identity hate, and threat).  \n",
       "4174                                                                                                                                                                                                                                                 A promising alternative approach to reduce toxic discourse without recourse to censorship is so-called counter speech, which broadly refers to citizens' response to hateful speech in order to stop it, reduce its consequences, and discourage it [Benesch et al.,  \n",
       "2360                                                                                                                                                                                                                                                                                                                                                                                        In contrast, the Table 5 : Two examples of toxic/non-toxic comments that show the effects of the different shielding methods.  \n",
       "2588                                                                                                                                                                                                                                 For instance, we discovered that we had to explicitly emphasize to all annotators the difference between when a reporter's words and viewpoints are toxic themselves, to when a politically toxic event or statement is reported, and that we are only interested in the first case.  \n",
       "3232                                                                                                                                                                                                                                                                                                                                                                                                                       First, we tackle the problem of unintended bias in toxic comment classification (Dixon et al.,  \n",
       "2613                                                                                                                                                                                                                                                                                                                                                                                                               Flagged messages are deemed toxic and, unlike WCC, all remaining messages are considered as non-toxic.  \n",
       "4268                                                                                                                                                                                                                                                                                                                                               Much NLP research is focused on finding and classifying offensive or toxic language, which is then either directly censored or flagged for human moderators to review.  \n",
       "1236                                                                                                                                                                                                                                                                                                                                       We saw that the proportion between offense, toxicity, abuse or hate messages can vary across different datasets, and this factor greatly impacts the classifiers' performance.  \n",
       "677                                                                                                                                                                                                                                                                                                                             This demonstrates that GPT-2 significantly increased the vocabulary range of the training set, specifically with offensive words likely to be relevant for toxic language classification.  \n",
       "59                                                                                                                                                                                                                                                                                                                      We collected and share two datasets for investigating our research questions around the effect of context on the annotation of toxic comments (RQ1) and its detection by automated systems (RQ2).  \n",
       "4252                                                                                                                                                                                                                                                                                                                                                                                  Without any preset semantics of toxic content, they came up with the tool that could be manipulated through a modifiable threshold.  \n",
       "1800                                                                                                                                                                                                                                                                                                                   After filtering out non-English segments and segments that were too long (>50 words or >1000 characters) or too short (<5 words), we kept all the remaining comments with any toxic label (approx.  \n",
       "535                                                                                                                                                                                                                                                                                                                               Prompted Toxicity in Neural Models Using REALTOXICITYPROMPTS and the same generation procedures outlined in §3, we measure toxic degeneration in out-of-the-box neural language models.  \n",
       "4132                                                                                                                                                                                                                                                                                                                                                                                                                                 The DNN network classifies these sequences of word embeddings as toxic or non-toxic.  \n",
       "2407                                                                                                                                                                                                                                                                                                                                                                                                                                     2017) ), collected from English Wikipedia talk pages and annotated for toxicity.  \n",
       "2271                                                                                                                                                                                                                           To make the improvement from applying our technique more visible, we fine-tuned the 117M GPT-2 with only toxic or negative sentences when producing the baseline models, and obtained GPT-senti-ext and GPT-toxic-ext, which frequently produce negative or toxic generated continuations.  \n",
       "403                                                                                                                                                                                                                                                                                                                                                                                                                             We asked annotators whether sentences are toxic, and measured average annotated toxicity.  \n",
       "4138                                                                                                                                                                                                                                                                                              This part contains 160k comments from English Wikipedia talk pages, each labelled by approximately 10 annotators via crowd-sourcing, on a spectrum of how toxic/healthy the comment is with regard to the conversation.  \n",
       "4148                                               To perform the binary classification (toxic or not toxic), for each comment, we decided to use the following majority vote labelling: if [(# of very toxic and toxic annotations) > (# of healthy and very healthy annotations)] and [(# of very toxic and toxic annotations) > 2] comment is toxic otherwise comment is non-toxic Some examples of the toxic comments are: \"You are a big fat idiot, stop spamming my userspace\", \"What the fuck is your problem?\", \"  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.style.set_properties(subset=['sentence'], **{'width': '300px'})\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "\n",
    "data.groupby('decade').sample(frac=.1, replace=True) [['decade', 'sentence' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c00929-2885-4a8d-9021-5630ffebb6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = data.groupby('decade').sample(5) [['decade', 'sentence' ]]\n",
    "save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3099077-c538-4ba9-a016-7b47f63aebe8",
   "metadata": {},
   "source": [
    "rational | ˈraSH(ə)nəl |\n",
    "adjective\n",
    "1. based on or in accordance with reason or logic: I'm sure there's a perfectly rational explanation.\n",
    " - (of a person) able to think clearly, sensibly, and logically: Andrea's upset—she's not being very rational.\n",
    " - endowed with the capacity to reason: man is a rational being.\n",
    "2. Mathematics (of a number, quantity, or expression) expressible, or containing quantities that are expressible, as a ratio of whole numbers. When expressed as a decimal, a rational number has a finite or recurring expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e6d68-7651-4d38-a9a2-cfd82b4fc302",
   "metadata": {},
   "source": [
    "### ACL Human"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10adc45-65d8-412d-94c1-4b53c9e9f778",
   "metadata": {},
   "source": [
    "|decade | notes |\n",
    "|---------|-------------------|\n",
    "|1950 |  |\n",
    "|1960 | |\n",
    "|1970 | |\n",
    "|1980 | |\n",
    "|1990 |  |\n",
    "|2000 | |\n",
    "|2010 |  |\n",
    "|2020 |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d5d8e-fb89-4a6b-9f8e-3f5c93e10340",
   "metadata": {},
   "source": [
    "Main senses found in ACL are the logical formalism sense, the computer logic sense. Later on we get logic in the justifiable by reason sense, as it becomes a task. Initially, logical forms are a representation of natural language. The task is: can we model natural language using logical formalisms? These kinds of logics are seen as insufficient with the advent of feature-based statistical methods. There is a switch, and the task is: can we model logical processes (of thought) using other kinds of representations--statistical representations. \n",
    "\n",
    "There's another potential change here in the extension of computer logic to 'business logic'---a term which can be specific to the logic of a business encoded in a particular program or a more abstract process that can have some digital and some analog components but which is supposed to operate with the regularity of an algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3a8a6-7f8c-4181-8ab3-5d21fe0618a0",
   "metadata": {},
   "source": [
    "## COCA HUMAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef109fe5-9217-4c22-9dbe-d7e95759b639",
   "metadata": {},
   "source": [
    "|senses | snippets|\n",
    "|---|--|\n",
    "|system of codification or set of principles (often to point out a flaw; limited or not totalizing systems of reason) | by this logic, with this kind of logic, the logic employed to suggest continuity w/ populism|\n",
    "|symbolic/mathematical | Isn't logic required by math , Math is based on logic, curses aren't. |\n",
    "| justifiable by reason| there had to be some logic left in the world |\n",
    "| suggested course of action | wealth-creation logic, logic that constructs and maintains sustemic racism in Bolivia |\n",
    "| computer program | | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9238f56-4650-46e7-ba08-b3d53c71bebb",
   "metadata": {},
   "source": [
    "synonyms: sagacity, wisdom, soundness, judgment, rationality, coherence, chain of reasoning, argument, dialectics, deduction\n",
    "\n",
    "Would we call these examples of the formalism sense of logic polysemous? Let's see if we can make the same subtitutions.\n",
    "\n",
    "1.  In this way the LLFs have a more natural appearance than, for instance, the formulas of *first order logic*. (ACL, emphasis added)\n",
    "   - (a)   the formulas of deduction\n",
    "   - (b) * the formulas of wisdom\n",
    "\n",
    "2.  what with your well-honed expertise in \"freshman logic\" (COCA, emphasis adde)\n",
    "   - (a)   expertise in deduction\n",
    "   - (b)   expertise in wisdom\n",
    "\n",
    "(I realize they aren't the same, but they could very well be paraphrases, given that FOL is the standard in freshman logic)\n",
    "\n",
    "The point is that the potential substitutions in these otherwise same senses don't line up. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184864d-fc2f-49c6-9f2a-3d43cb71ea75",
   "metadata": {},
   "source": [
    "Let's try the same with another sense, the \n",
    "\n",
    "1. We believe that either a three-way modal logic entailment task or a two-way probabilistic *logic entailment* task on its own could make perfect sense. (ACL)\n",
    "  - (a)  chain of reasoning entailment task\n",
    "  - (b)  ? argument entailment task\n",
    "  - (c)  deduction entailment task\n",
    "  - (d)  rationality entailment task\n",
    "  - (e)  sagacity entailment task\n",
    "\n",
    "2. ... never play it), the barrier to RPGs is more knowing their rules and logic and how to find things in menus. Broadly speaking, FPSs feel more like ... (COCA)\n",
    "  - (a)  knowing their rules and chain of reasoning\n",
    "  - (b)  knowing their rules and argument\n",
    "  - (c)  knowing their rules and deduction\n",
    "  - (d)  knowing their rules and rationality\n",
    "  - (e) ? knowing their rules and sagacity\n",
    "\n",
    "3. More complex cases are still based on the usual rules of *propositional logic* such as modus ponens, ((p~q).q) ~q). (ACL)\n",
    "  - (a)  the usual rules of *propositional chain of reasoning*\n",
    "  - (b)  the usual rules of *propositional argument*\n",
    "  - (c)  the usual rules of *propositional deduction*\n",
    "  - (d)  ? the usual rules of *propositional rationality*\n",
    "  - (e)  ?? the usual rules of *propositional sagacity*\n",
    "\n",
    "4. That is the logic Truman used to justify bombing Hiroshima (COCA)\n",
    " - (a) the chain of reasoning that truman used\n",
    " - (b) the argument that truman used\n",
    " - (c) the wisdom that truman used (the semantic felicity here would seem to depend on moral/ethical stance)\n",
    " - (d) the deduction that truman used\n",
    " - (e) the sagacity that truman used\n",
    "\n",
    "Sense (1) falls more closely into the \"justifiable by reason\" sense than the \"formalism\" sense most commonly used in ACL publications. Perhaps it's not exactly the same flavor as the \"justifiable by reason\" sense used in 4. In any case, I imagine with the advent of connectionism and feture-based statistical machine learning, one sees a tendency towards the logic as a system of reasoning to be represented---the object being modeled---as opposed to logic as the model. The sense of logic as a system of inference or a course of action made necessary by application of logical methods is one which is not totalizing. These are partial logics, and often referred to point out a flaw or a limitation or a blind-spot in a particular line of reasoning. When this sense is used in machine learning, I hypothesize that this tendency will be less prevalent, due to the emergence of this use out of a meaning of logic which is supposed to be totalizing---a totalizing model of grammar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4cf7a4-5bef-4c90-baa0-4399de377855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
