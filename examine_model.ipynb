{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc2a5fe-a736-4d5d-af29-251dcc70a33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18 13:08:34.313301: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import fastparquet\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import textwrap\n",
    "from scipy.spatial.distance import cosine\n",
    "import spacy\n",
    "from collections import defaultdict \n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f1306f4-7f55-41c7-9b74-a02845836aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "_COCA_DIR = '/Volumes/data_gabriella_chronis/corpora/coca.2017.parquet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f9081ad-4ef2-4214-af05-0aa49313df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'model'\n",
    "tokens = pd.read_csv('./collected_tokens/acl/model.csv'.format(target))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bc9aebf-c43b-4fe8-a7c8-2cd97de8f8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>corpus_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199355</th>\n",
       "      <td>199355</td>\n",
       "      <td>231698495</td>\n",
       "      <td>Training Objective To model label-correlation, we combined LCA loss with binary cross-entropy (BCE) and trained them jointly.</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189201</th>\n",
       "      <td>189201</td>\n",
       "      <td>223957053</td>\n",
       "      <td>Since our focus is on pairwise sentence scoring, we model this task as a question vs. question (title/headline) binary classification task.</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428693</th>\n",
       "      <td>428693</td>\n",
       "      <td>250390802</td>\n",
       "      <td>These prompts are created from simply cutting full sentences at a fixed position, thus the prompts have no constraints that will trigger a language model to follow up texts with gender pronouns.</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429577</th>\n",
       "      <td>429577</td>\n",
       "      <td>249455708</td>\n",
       "      <td>Our distantly supervised aspect name generation model is able to improve the Rouge-L score by 16%.</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33349</th>\n",
       "      <td>33349</td>\n",
       "      <td>10473972</td>\n",
       "      <td>As described in Section 4, we take three steps to learn the parameters for both the log-linear model of SMT and the CPTM.</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379647</th>\n",
       "      <td>379647</td>\n",
       "      <td>203688701</td>\n",
       "      <td>Note that all classifiers except the bag-of-words model take as input 100 dimensional fastText skipgram embeddings (Bojanowski et al.,</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590823</th>\n",
       "      <td>590823</td>\n",
       "      <td>275330</td>\n",
       "      <td>This method has similar benefits to FEDA in terms of classifier combination, but also attempts to model domain relationships.</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346373</th>\n",
       "      <td>346373</td>\n",
       "      <td>201666359</td>\n",
       "      <td>Training the NER model With the newly obtained training data from active learning, we attempt to improve the original transferred model.</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775736</th>\n",
       "      <td>775736</td>\n",
       "      <td>222378267</td>\n",
       "      <td>The pipelined nature of our approach allows us to replace the current BERTSum (Liu and Lapata, 2019) extractor with any arbitrary, black-box model that retrieves important sentences.</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641542</th>\n",
       "      <td>641542</td>\n",
       "      <td>186206852</td>\n",
       "      <td>Our model's scores are similar to the previous state of the art, a bidirectional LSTM-CRF model with character features (Dernoncourt et al.,</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51376</th>\n",
       "      <td>51376</td>\n",
       "      <td>15125161</td>\n",
       "      <td>In addition, we can encapsulate previous approaches in terms of features in our model.</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551631</th>\n",
       "      <td>551631</td>\n",
       "      <td>41488455</td>\n",
       "      <td>Different system setups were experimentally tested for the CRF model and the different options available within the MALLET toolkit were considered.</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539219</th>\n",
       "      <td>539219</td>\n",
       "      <td>16112861</td>\n",
       "      <td>The model only looks up the new LM scores for the affected words and updates the total score if the new search state passes the first acceptance check.</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127836</th>\n",
       "      <td>127836</td>\n",
       "      <td>248505793</td>\n",
       "      <td>A5} in their ranked order by the pairwise RoBERTa-Base model.</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392187</th>\n",
       "      <td>392187</td>\n",
       "      <td>15726188</td>\n",
       "      <td>Furthermore, it uses a language model to rerank output sentences.</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  corpus_id  \\\n",
       "199355      199355  231698495   \n",
       "189201      189201  223957053   \n",
       "428693      428693  250390802   \n",
       "429577      429577  249455708   \n",
       "33349        33349   10473972   \n",
       "379647      379647  203688701   \n",
       "590823      590823     275330   \n",
       "346373      346373  201666359   \n",
       "775736      775736  222378267   \n",
       "641542      641542  186206852   \n",
       "51376        51376   15125161   \n",
       "551631      551631   41488455   \n",
       "539219      539219   16112861   \n",
       "127836      127836  248505793   \n",
       "392187      392187   15726188   \n",
       "\n",
       "                                                                                                                                                                                                  sentence  \\\n",
       "199355                                                                       Training Objective To model label-correlation, we combined LCA loss with binary cross-entropy (BCE) and trained them jointly.   \n",
       "189201                                                         Since our focus is on pairwise sentence scoring, we model this task as a question vs. question (title/headline) binary classification task.   \n",
       "428693  These prompts are created from simply cutting full sentences at a fixed position, thus the prompts have no constraints that will trigger a language model to follow up texts with gender pronouns.   \n",
       "429577                                                                                                  Our distantly supervised aspect name generation model is able to improve the Rouge-L score by 16%.   \n",
       "33349                                                                            As described in Section 4, we take three steps to learn the parameters for both the log-linear model of SMT and the CPTM.   \n",
       "379647                                                              Note that all classifiers except the bag-of-words model take as input 100 dimensional fastText skipgram embeddings (Bojanowski et al.,   \n",
       "590823                                                                       This method has similar benefits to FEDA in terms of classifier combination, but also attempts to model domain relationships.   \n",
       "346373                                                            Training the NER model With the newly obtained training data from active learning, we attempt to improve the original transferred model.   \n",
       "775736              The pipelined nature of our approach allows us to replace the current BERTSum (Liu and Lapata, 2019) extractor with any arbitrary, black-box model that retrieves important sentences.   \n",
       "641542                                                        Our model's scores are similar to the previous state of the art, a bidirectional LSTM-CRF model with character features (Dernoncourt et al.,   \n",
       "51376                                                                                                               In addition, we can encapsulate previous approaches in terms of features in our model.   \n",
       "551631                                                 Different system setups were experimentally tested for the CRF model and the different options available within the MALLET toolkit were considered.   \n",
       "539219                                             The model only looks up the new LM scores for the affected words and updates the total score if the new search state passes the first acceptance check.   \n",
       "127836                                                                                                                                       A5} in their ranked order by the pairwise RoBERTa-Base model.   \n",
       "392187                                                                                                                                   Furthermore, it uses a language model to rerank output sentences.   \n",
       "\n",
       "        start_idx  end_idx  \n",
       "199355          3        4  \n",
       "189201         10       11  \n",
       "428693         25       26  \n",
       "429577          6        7  \n",
       "33349          20       21  \n",
       "379647         11       12  \n",
       "590823         17       18  \n",
       "346373          3        4  \n",
       "775736         28       29  \n",
       "641542         19       20  \n",
       "51376          14       15  \n",
       "551631          9       10  \n",
       "539219          1        2  \n",
       "127836         12       13  \n",
       "392187          6        7  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae3b8f9-4d99-49ec-a534-fee3ba843b6f",
   "metadata": {},
   "source": [
    "Here is how many tokens of model there are in the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b20f158-02fa-461d-9476-0ca7c4047d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "868208"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40087669-2083-4d39-829c-b83694f50f50",
   "metadata": {},
   "source": [
    "We have a few questions about the usage of this word. \n",
    "\n",
    "\n",
    "1. collocates\n",
    "2. how often is model used as a subject or an object? \n",
    "3. what kind of usage types are found in the data\n",
    "4. how often is model used as a noun and how often as a verb?\n",
    "\n",
    "We'd also like to look at these phenomena over time. hypotheses:\n",
    "1. more subject-y over time\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86ec0d-0a53-478f-809e-4155d6e45229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
