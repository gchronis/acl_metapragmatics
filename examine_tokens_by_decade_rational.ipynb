{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57a21dd-c16e-4a53-81ca-7ffeb5a07bec",
   "metadata": {},
   "source": [
    "# Look at uses of a target word \"rational\" over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db96dea7-920a-479d-8236-36063b4d32b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 13:37:01.476954: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import fastparquet\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import textwrap\n",
    "from scipy.spatial.distance import cosine\n",
    "import spacy\n",
    "from collections import defaultdict \n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8677b60-7adb-49d9-8079-6557fe32f718",
   "metadata": {},
   "source": [
    "Load in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6441b64-3c4f-4867-8725-547fecdecfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"rational\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52732ea7-c8fa-4920-a088-0716ffb7492e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/logic_words/rational.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/logic_words/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/logic_words/rational.csv'"
     ]
    }
   ],
   "source": [
    "tokens = pd.read_csv('./data/logic_words/{}.csv'.format(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b3fdf-75f0-4ade-82f8-8c37a72729d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb686a-a83f-421f-b603-a0b893946790",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parquet_file = \"/Volumes/data_gabriella_chronis/corpora/acl-publication-info.74k.parquet\"\n",
    "\n",
    "df = pd.read_parquet(parquet_file, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693366bc-3ee2-48bb-906c-d683ee0c3678",
   "metadata": {},
   "source": [
    "Left hand join the large file to the token file. or do a constant lookup??. maybe just get the year columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9077fa-a693-4031-ad79-9e72cf74518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokens.join(df.set_index(\"corpus_paper_id\"), on=\"corpus_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c021b-195c-47ec-bd9e-87e30bc342b0",
   "metadata": {},
   "source": [
    "Add a decade column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28129e79-4cbc-41aa-832a-6327939abf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"year\"] = data[\"year\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64b8258-96ef-41f6-ba70-4ed5a94f2567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[\"decade\"] = ( data['year'] //10)*10\n",
    "\n",
    "#bins = pd.IntervalIndex.from_tuples([(0, 1), (2, 3), (4, 5)])\n",
    "bins = [1950, 1960, 1970, 1980, 1990, 2000, 2005, 2010, 2012, 2014, 2016, 2018, 2020]\n",
    "data[\"decade\"] = pd.cut(data['year'], bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eef632-538c-4752-b39a-a31748022abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"decade\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b52692-d939-45e7-807c-fbdd00382e4a",
   "metadata": {},
   "source": [
    "### Look at 10 example sentences from each decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850f67d-9a1e-4349-9578-76a20a996af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.style.set_properties(subset=['sentence'], **{'width': '300px'})\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "\n",
    "data.groupby('decade').sample(5, replace=True) [['decade', 'sentence' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c00929-2885-4a8d-9021-5630ffebb6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = data.groupby('decade').sample(5) [['decade', 'sentence' ]]\n",
    "save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3099077-c538-4ba9-a016-7b47f63aebe8",
   "metadata": {},
   "source": [
    "rational | ˈraSH(ə)nəl |\n",
    "adjective\n",
    "1. based on or in accordance with reason or logic: I'm sure there's a perfectly rational explanation.\n",
    " - (of a person) able to think clearly, sensibly, and logically: Andrea's upset—she's not being very rational.\n",
    " - endowed with the capacity to reason: man is a rational being.\n",
    "2. Mathematics (of a number, quantity, or expression) expressible, or containing quantities that are expressible, as a ratio of whole numbers. When expressed as a decimal, a rational number has a finite or recurring expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e6d68-7651-4d38-a9a2-cfd82b4fc302",
   "metadata": {},
   "source": [
    "### ACL Human"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10adc45-65d8-412d-94c1-4b53c9e9f778",
   "metadata": {},
   "source": [
    "|decade | notes |\n",
    "|---------|-------------------|\n",
    "|1950 |  |\n",
    "|1960 | |\n",
    "|1970 | |\n",
    "|1980 | |\n",
    "|1990 |  |\n",
    "|2000 | |\n",
    "|2010 |  |\n",
    "|2020 |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d5d8e-fb89-4a6b-9f8e-3f5c93e10340",
   "metadata": {},
   "source": [
    "Main senses found in ACL are the logical formalism sense, the computer logic sense. Later on we get logic in the justifiable by reason sense, as it becomes a task. Initially, logical forms are a representation of natural language. The task is: can we model natural language using logical formalisms? These kinds of logics are seen as insufficient with the advent of feature-based statistical methods. There is a switch, and the task is: can we model logical processes (of thought) using other kinds of representations--statistical representations. \n",
    "\n",
    "There's another potential change here in the extension of computer logic to 'business logic'---a term which can be specific to the logic of a business encoded in a particular program or a more abstract process that can have some digital and some analog components but which is supposed to operate with the regularity of an algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3a8a6-7f8c-4181-8ab3-5d21fe0618a0",
   "metadata": {},
   "source": [
    "## COCA HUMAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef109fe5-9217-4c22-9dbe-d7e95759b639",
   "metadata": {},
   "source": [
    "|senses | snippets|\n",
    "|---|--|\n",
    "|system of codification or set of principles (often to point out a flaw; limited or not totalizing systems of reason) | by this logic, with this kind of logic, the logic employed to suggest continuity w/ populism|\n",
    "|symbolic/mathematical | Isn't logic required by math , Math is based on logic, curses aren't. |\n",
    "| justifiable by reason| there had to be some logic left in the world |\n",
    "| suggested course of action | wealth-creation logic, logic that constructs and maintains sustemic racism in Bolivia |\n",
    "| computer program | | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9238f56-4650-46e7-ba08-b3d53c71bebb",
   "metadata": {},
   "source": [
    "synonyms: sagacity, wisdom, soundness, judgment, rationality, coherence, chain of reasoning, argument, dialectics, deduction\n",
    "\n",
    "Would we call these examples of the formalism sense of logic polysemous? Let's see if we can make the same subtitutions.\n",
    "\n",
    "1.  In this way the LLFs have a more natural appearance than, for instance, the formulas of *first order logic*. (ACL, emphasis added)\n",
    "   - (a)   the formulas of deduction\n",
    "   - (b) * the formulas of wisdom\n",
    "\n",
    "2.  what with your well-honed expertise in \"freshman logic\" (COCA, emphasis adde)\n",
    "   - (a)   expertise in deduction\n",
    "   - (b)   expertise in wisdom\n",
    "\n",
    "(I realize they aren't the same, but they could very well be paraphrases, given that FOL is the standard in freshman logic)\n",
    "\n",
    "The point is that the potential substitutions in these otherwise same senses don't line up. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184864d-fc2f-49c6-9f2a-3d43cb71ea75",
   "metadata": {},
   "source": [
    "Let's try the same with another sense, the \n",
    "\n",
    "1. We believe that either a three-way modal logic entailment task or a two-way probabilistic *logic entailment* task on its own could make perfect sense. (ACL)\n",
    "  - (a)  chain of reasoning entailment task\n",
    "  - (b)  ? argument entailment task\n",
    "  - (c)  deduction entailment task\n",
    "  - (d)  rationality entailment task\n",
    "  - (e)  sagacity entailment task\n",
    "\n",
    "2. ... never play it), the barrier to RPGs is more knowing their rules and logic and how to find things in menus. Broadly speaking, FPSs feel more like ... (COCA)\n",
    "  - (a)  knowing their rules and chain of reasoning\n",
    "  - (b)  knowing their rules and argument\n",
    "  - (c)  knowing their rules and deduction\n",
    "  - (d)  knowing their rules and rationality\n",
    "  - (e) ? knowing their rules and sagacity\n",
    "\n",
    "3. More complex cases are still based on the usual rules of *propositional logic* such as modus ponens, ((p~q).q) ~q). (ACL)\n",
    "  - (a)  the usual rules of *propositional chain of reasoning*\n",
    "  - (b)  the usual rules of *propositional argument*\n",
    "  - (c)  the usual rules of *propositional deduction*\n",
    "  - (d)  ? the usual rules of *propositional rationality*\n",
    "  - (e)  ?? the usual rules of *propositional sagacity*\n",
    "\n",
    "4. That is the logic Truman used to justify bombing Hiroshima (COCA)\n",
    " - (a) the chain of reasoning that truman used\n",
    " - (b) the argument that truman used\n",
    " - (c) the wisdom that truman used (the semantic felicity here would seem to depend on moral/ethical stance)\n",
    " - (d) the deduction that truman used\n",
    " - (e) the sagacity that truman used\n",
    "\n",
    "Sense (1) falls more closely into the \"justifiable by reason\" sense than the \"formalism\" sense most commonly used in ACL publications. Perhaps it's not exactly the same flavor as the \"justifiable by reason\" sense used in 4. In any case, I imagine with the advent of connectionism and feture-based statistical machine learning, one sees a tendency towards the logic as a system of reasoning to be represented---the object being modeled---as opposed to logic as the model. The sense of logic as a system of inference or a course of action made necessary by application of logical methods is one which is not totalizing. These are partial logics, and often referred to point out a flaw or a limitation or a blind-spot in a particular line of reasoning. When this sense is used in machine learning, I hypothesize that this tendency will be less prevalent, due to the emergence of this use out of a meaning of logic which is supposed to be totalizing---a totalizing model of grammar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4cf7a4-5bef-4c90-baa0-4399de377855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
