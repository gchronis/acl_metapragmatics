{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a571ea-203d-4386-a584-00c0b970cd26",
   "metadata": {},
   "source": [
    "# The Semantics of \"Language Models\"\n",
    "\n",
    "There is a lot of debate in the field and in the industry about whether large language models \"understand language\" (CITE), whether they are \"capable of reference\" (CITE). Some, like Geoffrey Hinton, would take a victory lap---decades later, we've proven the first connectionists right. Champions of industry speak of AGI as if it's right around the corner. Others, like Emily Bender, warn that the \"AI hype\" is unfounded. Language models can never \"understand\" language without a way to ground the symbols they manipulate. At its most sinister, AI hype (and its close counterpart AI doom---often entertained by the exact same parties) misdirects the attention of the public away from the real and present harms of machine learning and NLP applications in today's society, from amplifying systemic racism to overhauling labor and expectations of productivity. \n",
    "\n",
    "In some sense, the arguments of both camps require that there is no animacy within the language model---yet. (The AI dehypers decouple animacy/agency from harm.) \n",
    "\n",
    "This all speaks to our overt metapragmatics of language models---the debate in lets call it philosopy of language models resembled is an exercise of power. It determines who has the authority to declare a speaker as a legitimate subject.\n",
    "\n",
    "There's an idea that the legitimacy of the subject inheres in it somehow, in its properties, and not in interaction, or perception / understanding of others. \n",
    "\n",
    "One philosopher with much to say on the subject of subjecthood was Hegel. In hegel's phenomenology, the subject can not exist without also being object. The self exists as a self inasmuch as it relates to an other and another relates to it. His view of subjecthood relies on mutual recognition. \n",
    "\n",
    "Does what we say about language models match up with how we talk about them? I suspect that over the last decade, models have become (linguistically, grammatically) more like subjects than objects. \n",
    "\n",
    "I hypothesize that over time we will see\n",
    "\n",
    "1. higher number of collocations with more agentive verbs, esp. verbs of cognition (e.g. acquiring concepts)\n",
    "2. increased use in subject-y positions\n",
    "3. higher association with subject-like semantic features\n",
    "4. (potential)Compared to coca (controlling for fashion model sense) we will see language models getting more animate\n",
    "\n",
    "\n",
    "\n",
    "A point about anthropology\n",
    "\n",
    "This is a test of overt/covert language ideology. The concept of ideology has been turned all about---the idea of ideology in the marxist sense of an unconscious bias away from the truth has been criticized. Along with the idea of a 'gap' between overt political/linguistic ideologies that people profess and ideologies they enact. This is a tricky problem and we want to kind of sidestep it. But we also want to say, look---these people say language models are display 'human-like' abilities, these dont. But they both talk about them in the same way. \n",
    "\n",
    "Where does negation come into this? As in somebody stating in a paper that 'language models *don't* display human-like abilities. \n",
    "\n",
    "I also suspect we'll see a shift from verbal to nominal uses, and that the nominal uses will have new senses. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40418dd2-98e8-44af-b6fd-66bb0692b4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 16:17:29.639386: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import spacy\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de0da65-dde6-4af3-a648-d9679593734c",
   "metadata": {},
   "source": [
    "## Loading and building the dataset\n",
    "\n",
    "Let's look at some uses of the word 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b15134-ac85-4ebc-8662-234cccda4d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'model'\n",
    "\n",
    "tokens = pd.read_csv('./collected_tokens/acl/{}.csv'.format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f976d235-8a46-4d97-b543-646ec9e9a15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>corpus_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18022704</td>\n",
       "      <td>Since the similarity measure based on the vector space model is a rough estimation, minor errors made at the stage of context vector extraction are acceptable.</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18022704</td>\n",
       "      <td>e) Words with similar contexts might not be synonyms: A disadvantage does exist when the context vector model is used.</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18022704</td>\n",
       "      <td>Therefore, the vector space model should incorporate the taxonomy approach to solve this phenomenon.</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>18022704</td>\n",
       "      <td>Conclusions In this paper, we have adopted the context vector model to measure word similarity.</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>16703040</td>\n",
       "      <td>Based on a review of our misclassified instances, we are surprised that our classifier did not learn a better model based on style features (F 1 =.60).</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  corpus_id  \\\n",
       "0           0   18022704   \n",
       "1           1   18022704   \n",
       "2           2   18022704   \n",
       "3           3   18022704   \n",
       "4           4   16703040   \n",
       "\n",
       "                                                                                                                                                          sentence  \\\n",
       "0  Since the similarity measure based on the vector space model is a rough estimation, minor errors made at the stage of context vector extraction are acceptable.   \n",
       "1                                           e) Words with similar contexts might not be synonyms: A disadvantage does exist when the context vector model is used.   \n",
       "2                                                             Therefore, the vector space model should incorporate the taxonomy approach to solve this phenomenon.   \n",
       "3                                                                  Conclusions In this paper, we have adopted the context vector model to measure word similarity.   \n",
       "4          Based on a review of our misclassified instances, we are surprised that our classifier did not learn a better model based on style features (F 1 =.60).   \n",
       "\n",
       "   start_idx  end_idx  \n",
       "0          9       10  \n",
       "1         19       20  \n",
       "2          5        6  \n",
       "3         11       12  \n",
       "4         20       21  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c92c4f01-5196-40d9-b6a2-310cb36517ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "868208"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tokens of 'model\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc65c434-173d-4855-a9c1-0a836d7e504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the paper metadata from our ACL corpus to the tokens \n",
    "\n",
    "df = parquet_file = \"/Volumes/data_gabriella_chronis/corpora/acl-publication-info.74k.parquet\"\n",
    "\n",
    "df = pd.read_parquet(parquet_file, engine='pyarrow')\n",
    "data = tokens.join(df.set_index(\"corpus_paper_id\"), on=\"corpus_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8d93153-30b1-4df3-a44f-7ffd69902abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast year from string to int so we can order it\n",
    "data[\"year\"] = data[\"year\"].astype(int)\n",
    "\n",
    "# create decade bins\n",
    "bins = [1950, 1960, 1970, 1980, 1990, 2000, 2005, 2010, 2012, 2014, 2016, 2018, 2020]\n",
    "data[\"decade\"] = pd.cut(data['year'], bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62ab4904-ed1b-4c1a-85b6-ff0ba2c727dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dev purposes let's work with a smaller section of the data for now\n",
    "\n",
    "#data = data.iloc[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9352f02f-966b-400e-bf7d-d8e4df3439ab",
   "metadata": {},
   "source": [
    "### Look at ten sentences from each decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e7df071-01fa-4952-92e5-e0cf3ce47885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decade</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>606708</th>\n",
       "      <td>(1950, 1960]</td>\n",
       "      <td>We did write this paper about model English entirely in model English to show how much familiarity can be combined with complete regularity in a model language.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612857</th>\n",
       "      <td>(1950, 1960]</td>\n",
       "      <td>From here on, however, -that is, in the MT from the pivot language into any of the model output languages -we would in every case have a mechanical correlation between two regularized languages.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606703</th>\n",
       "      <td>(1950, 1960]</td>\n",
       "      <td>Rules such as these can make English as free of inflections as Chinese or as the most model artificial language.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612855</th>\n",
       "      <td>(1950, 1960]</td>\n",
       "      <td>His paper caused a very lively discussion as a result of which I can say that \"model TL-s,\" especially his \"model target English\" will constitute an important item in the mechanization of the translation process.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612862</th>\n",
       "      <td>(1950, 1960]</td>\n",
       "      <td>A model language, as defined by Dr. Dodd, means any language in which the rules of syntax have been regularized, and in which familiarity of words is a governing criterion.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833619</th>\n",
       "      <td>(1960, 1970]</td>\n",
       "      <td>In the framework of Tarski's formulation, the method can be stated as follows: Given a \"model\" of the language L, consisting of an individual domain D and a semantic interpretation function ~b for L over P, the extension of any well-formed expression E in L is defined as the set of values for E of all semantic interpretation functions #' over P which differ from # at most on their assignments to the free variables of E. Now if one considers the domain of possible models for L, the intension ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613146</th>\n",
       "      <td>(1960, 1970]</td>\n",
       "      <td>The model draws on ii) iii) iv) and v) of the technological devices mentioned above, i) As is standarg practise now on Information Retrieval, the model uses a Thesaurus.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618061</th>\n",
       "      <td>(1960, 1970]</td>\n",
       "      <td>In this model we distinguisL five structural levels and tzo binary relations, ex:~ansion and coordination.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833965</th>\n",
       "      <td>(1960, 1970]</td>\n",
       "      <td>From this (and other corroborative forms) we postulate that an earlier stage of Russian had one form for this verb stem, namely /mog/, and that before the vowel /i/ /g/ later became /~/. The proposal in this paper is to reverse the bottom-to-top model of the comparative method and that of internal reconstruction into a top-to-bottom generative model where the input forms are reconstructed leKical items and the rules are the set of postulated sound changes for the language.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833476</th>\n",
       "      <td>(1960, 1970]</td>\n",
       "      <td>Accordingly, I propose initially cert@in extensions and modifications of the theory to make it in some sense 1-2 a model of performance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806671</th>\n",
       "      <td>(1970, 1980]</td>\n",
       "      <td>Charniak's model i n c l u d e s two o t h e r components.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618517</th>\n",
       "      <td>(1970, 1980]</td>\n",
       "      <td>This model, which includes extensively idiomatic postpositional expressions as terminals, is quite effective for the development of the Japanese language processor receptive to a reasonable variety of sentential forms and applicable to relatively wide fields.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807299</th>\n",
       "      <td>(1970, 1980]</td>\n",
       "      <td>AUTOMATIC INDEXING AND C L A S S I F I C A T I O N I N INFORMATION SYSTEMS The report describes a model for automatic indexing and c~assification of docurnen~s and for the inquiries In information systems that operate in dialog mode.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416241</th>\n",
       "      <td>(1970, 1980]</td>\n",
       "      <td>I n t h e case o f imperatives, the goal i s t h a t the hearer modify h i s model such t h a t i n t h a t w o r l d t h e speaker wants t h e hearer t o b r i n g about t h e t r u t h o f t h e ordered f a c t , and t h a t c e r t a i n s o c i a l consequences w i l l f o l l o w From non-compliance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415758</th>\n",
       "      <td>(1970, 1980]</td>\n",
       "      <td>The audience model which makes these decisions will presumably prefer to work from pre-calculated observations so as to avoid delay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630984</th>\n",
       "      <td>(1980, 1990]</td>\n",
       "      <td>As Phase 2 gets under way, we are already increasing the model's coverage.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788130</th>\n",
       "      <td>(1980, 1990]</td>\n",
       "      <td>Children evidently never make mistakes on the relative order of auxiliaries, which is consistent with the reversibility model, but they do mistakenly combine do with tensed verb forms (Pinker, 1984) .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799342</th>\n",
       "      <td>(1980, 1990]</td>\n",
       "      <td>We have developed a \"probabilistic spectral mapping\" technique for adapting a model from one speaker to a new speaker based on a small amount of speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799340</th>\n",
       "      <td>(1980, 1990]</td>\n",
       "      <td>We have also developed the \"stochastic segment model\", which can model the correlation between different parts of the phoneme directly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862094</th>\n",
       "      <td>(1980, 1990]</td>\n",
       "      <td>We have made the semantic interpretation rules clear according to the structure of the world model and syntactic information of the input sentences.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825387</th>\n",
       "      <td>(1990, 2000]</td>\n",
       "      <td>The theoretical model adopted here is based on Lakoff and Johnson ( [3] ) and Johnson ( [4] ), which applied to polysemy the cognitive framework of prototypicality developed by Rosch and her associates (e.g. [8] ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805302</th>\n",
       "      <td>(1990, 2000]</td>\n",
       "      <td>Through a process of human-supervised training on a small corpus of text, a statistical model is then developed which is used to rank the parses produced by the grammar for a given input.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631748</th>\n",
       "      <td>(1990, 2000]</td>\n",
       "      <td>Since the end of the 1980s, large corpora and powerful computational devices have allowed us to construct Nagao's model and to expand the model to deal with not only translation, but also other tasks such as parsing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407276</th>\n",
       "      <td>(1990, 2000]</td>\n",
       "      <td>ps(~ I~) = f,~s (25) ep Y, E8 Subcategorization Preference in Parsing a Sentence Suppose that, after estimating parameters of subcategorization preference from the training corpus £ of verb-noun collocations, we obtain the set ,5 of active features and the model ps(ep ] v) incorporating these features.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825815</th>\n",
       "      <td>(1990, 2000]</td>\n",
       "      <td>Since an interpretation model or background is necessary for processing linguistic information, Database Semantics can use as its model a database in a DBMS that provides both lexical meaning and world knowledge information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220976</th>\n",
       "      <td>(2000, 2005]</td>\n",
       "      <td>We consider that two sets of metrics behave similarly if the automatic translation ¦ is as close to the model 6 as 6 C , 6 C C are to each other for both sets of metrics.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161190</th>\n",
       "      <td>(2000, 2005]</td>\n",
       "      <td>This section describes in detail this translation model, which is called the tuple n-gram model.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827487</th>\n",
       "      <td>(2000, 2005]</td>\n",
       "      <td>HMM model (one order) is described as: New Words is another important kind of OOV words, especially in closed test.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208164</th>\n",
       "      <td>(2000, 2005]</td>\n",
       "      <td>If the unknown word has no context, a set of unknown word information, which has defined through corpus analysis, will be generated and the best one will be selected, as its semantic concept, by using the semantic tagging model.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809568</th>\n",
       "      <td>(2000, 2005]</td>\n",
       "      <td>Furthermore, knowledge sources (dialogue history, task record, world knowledge model, domain model, generic model, and user model) and problems that arise when interacting with an external knowledge source are discussed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412194</th>\n",
       "      <td>(2005, 2010]</td>\n",
       "      <td>Although events in previous turns cannot impact the current judgment given this model topology, it is possible to incorporate dialog history by creating features with a time lag.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29107</th>\n",
       "      <td>(2005, 2010]</td>\n",
       "      <td>For this, we employ a simple language model trained on dependency structures and compute the probability for the trigram \"In industry faced-as-head\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805382</th>\n",
       "      <td>(2005, 2010]</td>\n",
       "      <td>Label Comparison Model This model compares the two label assignments for a certain string.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438612</th>\n",
       "      <td>(2005, 2010]</td>\n",
       "      <td>They found that the translation-based model performed best.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7515</th>\n",
       "      <td>(2005, 2010]</td>\n",
       "      <td>Solution-Space Generalizations In an ideal model, each sentence would be broken down into its constituent words and every possible substitution of each word would be a possibility interpretation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583118</th>\n",
       "      <td>(2010, 2012]</td>\n",
       "      <td>Instead, it learns a correction model from sentence-aligned corrected learners' corpora.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847318</th>\n",
       "      <td>(2010, 2012]</td>\n",
       "      <td>Related work Semantic annotation of text requires that annotators can express complex bits of knowledge through the editor data model.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75674</th>\n",
       "      <td>(2010, 2012]</td>\n",
       "      <td>2010) we took both the Collobert &amp; Weston model (200 unscaled dimensions) and the HLBL model (100 scaled dimensions).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798111</th>\n",
       "      <td>(2010, 2012]</td>\n",
       "      <td>Thus, we may evaluate the extent to which our model is sensitive to social dynamics within pairs by the extent to which it is able to distinguish between true conversation between Real Pairs of speaker and synthetic conversation between Constructed Pairs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682956</th>\n",
       "      <td>(2010, 2012]</td>\n",
       "      <td>This also distinguishes it from previous work on dependency parse re-ranking (Hall, 2007) as we are not re-ranking/re-scoring the output of a base model but using a single decoding algorithm and learned model at training and testing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4233</th>\n",
       "      <td>(2012, 2014]</td>\n",
       "      <td>To evaluate the DL of a proposed set of candidate segmentations, we need to calculate the difference in DL between the current model, and the model that would result from committing to the candidate segmentations: DL ( Φ ′ , D ) − DL (Φ, D) = DL ( D|Φ ′ ) − DL (D|Φ) + DL ( Φ ′ ) − DL (Φ) The model lengths are trivial, as we merely have to encode the rules that are removed and inserted according to our encoding scheme and plug in the summed lengths in the above equation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318546</th>\n",
       "      <td>(2012, 2014]</td>\n",
       "      <td>Topic Covariance To quantify the relationship between topics in the model, we calculate the topic covariance metric for each pair of topics.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334135</th>\n",
       "      <td>(2012, 2014]</td>\n",
       "      <td>An approach using a TSP reordering model by Visweswariah et al. (</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16583</th>\n",
       "      <td>(2012, 2014]</td>\n",
       "      <td>For example, the inclusion of pension 'pension' among the exceptions prevents the system from analyzing it as the deverbal action noun of penser 'think' (24a), following the model of pression 'pressure' derived from presser 'press' (24b). (</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646363</th>\n",
       "      <td>(2012, 2014]</td>\n",
       "      <td>Experimental Details In this section, we provide details about taskspecific implementations of the supervised sourceside model and the word-alignment filtering techniques (steps 2 and 3 in Algorithm 1 respectively); we also briefly describe the setup of the cross-lingual experiments for each task.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543097</th>\n",
       "      <td>(2014, 2016]</td>\n",
       "      <td>Our model is a particular instantiation of this simple idea.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848105</th>\n",
       "      <td>(2014, 2016]</td>\n",
       "      <td>We also trained a 500dimensional continuous bag-of-words model on de.wiki'15 that ignored all words with less than 25 occurrences within a window size of 10; it was trained with negative sampling (value 3) and erroneously also with hierarchical softmax.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114323</th>\n",
       "      <td>(2014, 2016]</td>\n",
       "      <td>The NUR model ranks candidate utterances with respect to their suitability in relation to a given context using neural networks; in addition, a dialogue system based on the model converses with humans using highly ranked utterances.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808621</th>\n",
       "      <td>(2014, 2016]</td>\n",
       "      <td>2014) extended this experiment to identify adjective-noun metaphors using similar features, as well as porting the model to two further languages (Spanish and Farsi), achieving F-scores in the range of 0.72 to 0.85.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580239</th>\n",
       "      <td>(2014, 2016]</td>\n",
       "      <td>We show that when learning entity classifiers under minimal supervision, using a seed set, our approach is more effective in controlling model capacity than standard techniques for linear classifiers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481452</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words ambiguity and polysemy leads to performance improvement.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482179</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>Intuitively, this attention mechanism allows the model to proofread the previously generated abstract and improve it by better capturing long-term dependency and relevance to the title.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554011</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>2016) also used a WG model; their approach consists of three main components: (i) a merging stage based on Multiword Expressions (MWE), (ii) a mapping strategy based on synonymy between words and (iii) a reranking step to identify the best compression candidates generated using a POS-based language model (POS-LM).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601436</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>In some papers, however, there is a tendency to compare models only on one or two datasets, either because of time restraints or because the model is tailored to a specific task.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576683</th>\n",
       "      <td>(2016, 2018]</td>\n",
       "      <td>In order to tune the parameters of the model, we sample 10% of the training set as the validation data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763639</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>For each SBAR, we observe whether the first (highest) split point inside the clause (border tokens included) chosen by each model is (1) right before the main verb/auxiliary verb, (2) right before the first token or right after the last token of the clause, or (3) the other tokens in the clause.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288927</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Moreover, the inference model, which is trained on gold-standard observations, is used on noisy target sentences.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40915</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>In this paper, the default setting \"GCN\" is the 1-layer GCN-based joint model with the dynamic hard adjacency matrix, which achieves the best relation performance on ACE05 dataset.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493993</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Introduction Probabilistic and game-theoretic approaches to REG base on the maximization of utilities (Frank and Goodman, 2012; Goodman and Frank, 2016) or on corpus frequencies of attributes that are selected according to random float values (e.g. the PRO model, Gompel et al.,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577298</th>\n",
       "      <td>(2018, 2020]</td>\n",
       "      <td>Both models perform similarly on this data, with the BGT model having a small edge consistent with the overall gap between these two models.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              decade  \\\n",
       "606708  (1950, 1960]   \n",
       "612857  (1950, 1960]   \n",
       "606703  (1950, 1960]   \n",
       "612855  (1950, 1960]   \n",
       "612862  (1950, 1960]   \n",
       "833619  (1960, 1970]   \n",
       "613146  (1960, 1970]   \n",
       "618061  (1960, 1970]   \n",
       "833965  (1960, 1970]   \n",
       "833476  (1960, 1970]   \n",
       "806671  (1970, 1980]   \n",
       "618517  (1970, 1980]   \n",
       "807299  (1970, 1980]   \n",
       "416241  (1970, 1980]   \n",
       "415758  (1970, 1980]   \n",
       "630984  (1980, 1990]   \n",
       "788130  (1980, 1990]   \n",
       "799342  (1980, 1990]   \n",
       "799340  (1980, 1990]   \n",
       "862094  (1980, 1990]   \n",
       "825387  (1990, 2000]   \n",
       "805302  (1990, 2000]   \n",
       "631748  (1990, 2000]   \n",
       "407276  (1990, 2000]   \n",
       "825815  (1990, 2000]   \n",
       "220976  (2000, 2005]   \n",
       "161190  (2000, 2005]   \n",
       "827487  (2000, 2005]   \n",
       "208164  (2000, 2005]   \n",
       "809568  (2000, 2005]   \n",
       "412194  (2005, 2010]   \n",
       "29107   (2005, 2010]   \n",
       "805382  (2005, 2010]   \n",
       "438612  (2005, 2010]   \n",
       "7515    (2005, 2010]   \n",
       "583118  (2010, 2012]   \n",
       "847318  (2010, 2012]   \n",
       "75674   (2010, 2012]   \n",
       "798111  (2010, 2012]   \n",
       "682956  (2010, 2012]   \n",
       "4233    (2012, 2014]   \n",
       "318546  (2012, 2014]   \n",
       "334135  (2012, 2014]   \n",
       "16583   (2012, 2014]   \n",
       "646363  (2012, 2014]   \n",
       "543097  (2014, 2016]   \n",
       "848105  (2014, 2016]   \n",
       "114323  (2014, 2016]   \n",
       "808621  (2014, 2016]   \n",
       "580239  (2014, 2016]   \n",
       "481452  (2016, 2018]   \n",
       "482179  (2016, 2018]   \n",
       "554011  (2016, 2018]   \n",
       "601436  (2016, 2018]   \n",
       "576683  (2016, 2018]   \n",
       "763639  (2018, 2020]   \n",
       "288927  (2018, 2020]   \n",
       "40915   (2018, 2020]   \n",
       "493993  (2018, 2020]   \n",
       "577298  (2018, 2020]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   sentence  \n",
       "606708                                                                                                                                                                                                                                                                                                                                                     We did write this paper about model English entirely in model English to show how much familiarity can be combined with complete regularity in a model language.  \n",
       "612857                                                                                                                                                                                                                                                                                                                   From here on, however, -that is, in the MT from the pivot language into any of the model output languages -we would in every case have a mechanical correlation between two regularized languages.  \n",
       "606703                                                                                                                                                                                                                                                                                                                                                                                                     Rules such as these can make English as free of inflections as Chinese or as the most model artificial language.  \n",
       "612855                                                                                                                                                                                                                                                                                                 His paper caused a very lively discussion as a result of which I can say that \"model TL-s,\" especially his \"model target English\" will constitute an important item in the mechanization of the translation process.  \n",
       "612862                                                                                                                                                                                                                                                                                                                                         A model language, as defined by Dr. Dodd, means any language in which the rules of syntax have been regularized, and in which familiarity of words is a governing criterion.  \n",
       "833619  In the framework of Tarski's formulation, the method can be stated as follows: Given a \"model\" of the language L, consisting of an individual domain D and a semantic interpretation function ~b for L over P, the extension of any well-formed expression E in L is defined as the set of values for E of all semantic interpretation functions #' over P which differ from # at most on their assignments to the free variables of E. Now if one considers the domain of possible models for L, the intension ...  \n",
       "613146                                                                                                                                                                                                                                                                                                                                            The model draws on ii) iii) iv) and v) of the technological devices mentioned above, i) As is standarg practise now on Information Retrieval, the model uses a Thesaurus.  \n",
       "618061                                                                                                                                                                                                                                                                                                                                                                                                           In this model we distinguisL five structural levels and tzo binary relations, ex:~ansion and coordination.  \n",
       "833965                        From this (and other corroborative forms) we postulate that an earlier stage of Russian had one form for this verb stem, namely /mog/, and that before the vowel /i/ /g/ later became /~/. The proposal in this paper is to reverse the bottom-to-top model of the comparative method and that of internal reconstruction into a top-to-bottom generative model where the input forms are reconstructed leKical items and the rules are the set of postulated sound changes for the language.  \n",
       "833476                                                                                                                                                                                                                                                                                                                                                                             Accordingly, I propose initially cert@in extensions and modifications of the theory to make it in some sense 1-2 a model of performance.  \n",
       "806671                                                                                                                                                                                                                                                                                                                                                                                                                                                           Charniak's model i n c l u d e s two o t h e r components.  \n",
       "618517                                                                                                                                                                                                                                                  This model, which includes extensively idiomatic postpositional expressions as terminals, is quite effective for the development of the Japanese language processor receptive to a reasonable variety of sentential forms and applicable to relatively wide fields.  \n",
       "807299                                                                                                                                                                                                                                                                            AUTOMATIC INDEXING AND C L A S S I F I C A T I O N I N INFORMATION SYSTEMS The report describes a model for automatic indexing and c~assification of docurnen~s and for the inquiries In information systems that operate in dialog mode.  \n",
       "416241                                                                                                                                                                                                   I n t h e case o f imperatives, the goal i s t h a t the hearer modify h i s model such t h a t i n t h a t w o r l d t h e speaker wants t h e hearer t o b r i n g about t h e t r u t h o f t h e ordered f a c t , and t h a t c e r t a i n s o c i a l consequences w i l l f o l l o w From non-compliance.  \n",
       "415758                                                                                                                                                                                                                                                                                                                                                                                 The audience model which makes these decisions will presumably prefer to work from pre-calculated observations so as to avoid delay.  \n",
       "630984                                                                                                                                                                                                                                                                                                                                                                                                                                           As Phase 2 gets under way, we are already increasing the model's coverage.  \n",
       "788130                                                                                                                                                                                                                                                                                                             Children evidently never make mistakes on the relative order of auxiliaries, which is consistent with the reversibility model, but they do mistakenly combine do with tensed verb forms (Pinker, 1984) .  \n",
       "799342                                                                                                                                                                                                                                                                                                                                                             We have developed a \"probabilistic spectral mapping\" technique for adapting a model from one speaker to a new speaker based on a small amount of speech.  \n",
       "799340                                                                                                                                                                                                                                                                                                                                                                              We have also developed the \"stochastic segment model\", which can model the correlation between different parts of the phoneme directly.  \n",
       "862094                                                                                                                                                                                                                                                                                                                                                                 We have made the semantic interpretation rules clear according to the structure of the world model and syntactic information of the input sentences.  \n",
       "825387                                                                                                                                                                                                                                                                                               The theoretical model adopted here is based on Lakoff and Johnson ( [3] ) and Johnson ( [4] ), which applied to polysemy the cognitive framework of prototypicality developed by Rosch and her associates (e.g. [8] ).  \n",
       "805302                                                                                                                                                                                                                                                                                                                          Through a process of human-supervised training on a small corpus of text, a statistical model is then developed which is used to rank the parses produced by the grammar for a given input.  \n",
       "631748                                                                                                                                                                                                                                                                                             Since the end of the 1980s, large corpora and powerful computational devices have allowed us to construct Nagao's model and to expand the model to deal with not only translation, but also other tasks such as parsing.  \n",
       "407276                                                                                                                                                                                                      ps(~ I~) = f,~s (25) ep Y, E8 Subcategorization Preference in Parsing a Sentence Suppose that, after estimating parameters of subcategorization preference from the training corpus £ of verb-noun collocations, we obtain the set ,5 of active features and the model ps(ep ] v) incorporating these features.  \n",
       "825815                                                                                                                                                                                                                                                                                     Since an interpretation model or background is necessary for processing linguistic information, Database Semantics can use as its model a database in a DBMS that provides both lexical meaning and world knowledge information.  \n",
       "220976                                                                                                                                                                                                                                                                                                                                           We consider that two sets of metrics behave similarly if the automatic translation ¦ is as close to the model 6 as 6 C , 6 C C are to each other for both sets of metrics.  \n",
       "161190                                                                                                                                                                                                                                                                                                                                                                                                                     This section describes in detail this translation model, which is called the tuple n-gram model.  \n",
       "827487                                                                                                                                                                                                                                                                                                                                                                                                  HMM model (one order) is described as: New Words is another important kind of OOV words, especially in closed test.  \n",
       "208164                                                                                                                                                                                                                                                                                 If the unknown word has no context, a set of unknown word information, which has defined through corpus analysis, will be generated and the best one will be selected, as its semantic concept, by using the semantic tagging model.  \n",
       "809568                                                                                                                                                                                                                                                                                         Furthermore, knowledge sources (dialogue history, task record, world knowledge model, domain model, generic model, and user model) and problems that arise when interacting with an external knowledge source are discussed.  \n",
       "412194                                                                                                                                                                                                                                                                                                                                   Although events in previous turns cannot impact the current judgment given this model topology, it is possible to incorporate dialog history by creating features with a time lag.  \n",
       "29107                                                                                                                                                                                                                                                                                                                                                                 For this, we employ a simple language model trained on dependency structures and compute the probability for the trigram \"In industry faced-as-head\".  \n",
       "805382                                                                                                                                                                                                                                                                                                                                                                                                                           Label Comparison Model This model compares the two label assignments for a certain string.  \n",
       "438612                                                                                                                                                                                                                                                                                                                                                                                                                                                          They found that the translation-based model performed best.  \n",
       "7515                                                                                                                                                                                                                                                                                                                    Solution-Space Generalizations In an ideal model, each sentence would be broken down into its constituent words and every possible substitution of each word would be a possibility interpretation.  \n",
       "583118                                                                                                                                                                                                                                                                                                                                                                                                                             Instead, it learns a correction model from sentence-aligned corrected learners' corpora.  \n",
       "847318                                                                                                                                                                                                                                                                                                                                                                               Related work Semantic annotation of text requires that annotators can express complex bits of knowledge through the editor data model.  \n",
       "75674                                                                                                                                                                                                                                                                                                                                                                                                 2010) we took both the Collobert & Weston model (200 unscaled dimensions) and the HLBL model (100 scaled dimensions).  \n",
       "798111                                                                                                                                                                                                                                                      Thus, we may evaluate the extent to which our model is sensitive to social dynamics within pairs by the extent to which it is able to distinguish between true conversation between Real Pairs of speaker and synthetic conversation between Constructed Pairs.  \n",
       "682956                                                                                                                                                                                                                                                                            This also distinguishes it from previous work on dependency parse re-ranking (Hall, 2007) as we are not re-ranking/re-scoring the output of a base model but using a single decoding algorithm and learned model at training and testing.  \n",
       "4233                             To evaluate the DL of a proposed set of candidate segmentations, we need to calculate the difference in DL between the current model, and the model that would result from committing to the candidate segmentations: DL ( Φ ′ , D ) − DL (Φ, D) = DL ( D|Φ ′ ) − DL (D|Φ) + DL ( Φ ′ ) − DL (Φ) The model lengths are trivial, as we merely have to encode the rules that are removed and inserted according to our encoding scheme and plug in the summed lengths in the above equation.  \n",
       "318546                                                                                                                                                                                                                                                                                                                                                                         Topic Covariance To quantify the relationship between topics in the model, we calculate the topic covariance metric for each pair of topics.  \n",
       "334135                                                                                                                                                                                                                                                                                                                                                                                                                                                    An approach using a TSP reordering model by Visweswariah et al. (  \n",
       "16583                                                                                                                                                                                                                                                                      For example, the inclusion of pension 'pension' among the exceptions prevents the system from analyzing it as the deverbal action noun of penser 'think' (24a), following the model of pression 'pressure' derived from presser 'press' (24b). (  \n",
       "646363                                                                                                                                                                                                           Experimental Details In this section, we provide details about taskspecific implementations of the supervised sourceside model and the word-alignment filtering techniques (steps 2 and 3 in Algorithm 1 respectively); we also briefly describe the setup of the cross-lingual experiments for each task.  \n",
       "543097                                                                                                                                                                                                                                                                                                                                                                                                                                                         Our model is a particular instantiation of this simple idea.  \n",
       "848105                                                                                                                                                                                                                                                        We also trained a 500dimensional continuous bag-of-words model on de.wiki'15 that ignored all words with less than 25 occurrences within a window size of 10; it was trained with negative sampling (value 3) and erroneously also with hierarchical softmax.  \n",
       "114323                                                                                                                                                                                                                                                                             The NUR model ranks candidate utterances with respect to their suitability in relation to a given context using neural networks; in addition, a dialogue system based on the model converses with humans using highly ranked utterances.  \n",
       "808621                                                                                                                                                                                                                                                                                              2014) extended this experiment to identify adjective-noun metaphors using similar features, as well as porting the model to two further languages (Spanish and Farsi), achieving F-scores in the range of 0.72 to 0.85.  \n",
       "580239                                                                                                                                                                                                                                                                                                             We show that when learning entity classifiers under minimal supervision, using a seed set, our approach is more effective in controlling model capacity than standard techniques for linear classifiers.  \n",
       "481452                                                                                                                                                                                                                                                                                                                                                  Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words ambiguity and polysemy leads to performance improvement.  \n",
       "482179                                                                                                                                                                                                                                                                                                                            Intuitively, this attention mechanism allows the model to proofread the previously generated abstract and improve it by better capturing long-term dependency and relevance to the title.  \n",
       "554011                                                                                                                                                                                          2016) also used a WG model; their approach consists of three main components: (i) a merging stage based on Multiword Expressions (MWE), (ii) a mapping strategy based on synonymy between words and (iii) a reranking step to identify the best compression candidates generated using a POS-based language model (POS-LM).  \n",
       "601436                                                                                                                                                                                                                                                                                                                                   In some papers, however, there is a tendency to compare models only on one or two datasets, either because of time restraints or because the model is tailored to a specific task.  \n",
       "576683                                                                                                                                                                                                                                                                                                                                                                                                              In order to tune the parameters of the model, we sample 10% of the training set as the validation data.  \n",
       "763639                                                                                                                                                                                                             For each SBAR, we observe whether the first (highest) split point inside the clause (border tokens included) chosen by each model is (1) right before the main verb/auxiliary verb, (2) right before the first token or right after the last token of the clause, or (3) the other tokens in the clause.  \n",
       "288927                                                                                                                                                                                                                                                                                                                                                                                                    Moreover, the inference model, which is trained on gold-standard observations, is used on noisy target sentences.  \n",
       "40915                                                                                                                                                                                                                                                                                                                                  In this paper, the default setting \"GCN\" is the 1-layer GCN-based joint model with the dynamic hard adjacency matrix, which achieves the best relation performance on ACE05 dataset.  \n",
       "493993                                                                                                                                                                                                                               Introduction Probabilistic and game-theoretic approaches to REG base on the maximization of utilities (Frank and Goodman, 2012; Goodman and Frank, 2016) or on corpus frequencies of attributes that are selected according to random float values (e.g. the PRO model, Gompel et al.,  \n",
       "577298                                                                                                                                                                                                                                                                                                                                                                         Both models perform similarly on this data, with the BGT model having a small edge consistent with the overall gap between these two models.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.style.set_properties(subset=['sentence'], **{'width': '300px'})\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "\n",
    "#data.groupby('decade').sample(frac=.1, replace=True) [['decade', 'sentence' ]]\n",
    "data.groupby('decade').sample(5) [['decade', 'sentence' ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23158706-db3d-4b04-a0fc-e83ef8ce49c5",
   "metadata": {},
   "source": [
    "How many tokens of 'meaning' do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d95e79d-b0e1-42f8-9c5e-0b335441bc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decade\n",
       "(1950, 1960]        58\n",
       "(1960, 1970]       263\n",
       "(1970, 1980]      1223\n",
       "(1980, 1990]      7771\n",
       "(1990, 2000]     26080\n",
       "(2000, 2005]     32413\n",
       "(2005, 2010]     80479\n",
       "(2010, 2012]     47085\n",
       "(2012, 2014]     55540\n",
       "(2014, 2016]     72666\n",
       "(2016, 2018]    115437\n",
       "(2018, 2020]    218505\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('decade').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61832893-3495-4b9d-bff3-95d88324f2cf",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "\n",
    "Let's build all the collocations, for one year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "951fa4f1-e1fd-4629-9b20-8b7ad9d34d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_data = data[data['year'] ==2008]\n",
    "sentences = year_data['sentence'].str.cat(sep=' ')\n",
    "toks = nltk.word_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70731a6d-76b0-42b5-aa89-6b7f52a2273d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'low', 'resources', 'we', 'use']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ec731-5ac4-400c-b218-24282edc6704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9443389-c188-447c-ab10-e448b89fcdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('language', 'model'), ('translation', 'model'), ('model', 'is'), ('model', '.'), ('our', 'model'), ('model', ','), (',', 'model'), ('reordering', 'model'), ('.', 'model'), ('log-linear', 'model'), ('model', 'the'), ('CRF', 'model'), ('(', 'model'), ('model', 'for'), ('alignment', 'model'), ('model', 'a'), ('probabilistic', 'model'), ('model', 'was'), ('joint', 'model'), ('distortion', 'model'), ('model', 'can'), ('this', 'model'), ('generative', 'model'), ('of', 'model'), ('acoustic', 'model'), ('twin-candidate', 'model'), ('and', 'model'), ('transliteration', 'model'), ('model', 'with'), ('model', 'trained'), ('IBM', 'model'), ('model', 'has'), ('regression', 'model'), ('graphical', 'model'), ('single-candidate', 'model'), ('model', '('), ('in', 'model'), ('model', \"'s\"), ('entropy', 'model'), ('model', 'and')]\n"
     ]
    }
   ],
   "source": [
    "# Ngrams with 'creature' as a member\n",
    "model_filter = lambda *w: 'model' not in w\n",
    "\n",
    "## Bigrams\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "   toks )\n",
    "# only bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "# only bigrams that contain 'creature'\n",
    "finder.apply_ngram_filter(model_filter)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print(finder.nbest(bigram_measures.likelihood_ratio, 40))\n",
    "\n",
    "\n",
    "## Trigrams\n",
    "finder = TrigramCollocationFinder.from_words(\n",
    "   toks )\n",
    "# only trigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "# only trigrams that contain 'creature'\n",
    "\n",
    "# filter = lambda w: \"model\" not in w\n",
    "# finder.apply_ngram_filter(filter)\n",
    "# # return the 10 n-grams with the highest PMI\n",
    "# print(finder.nbest(trigram_measures.likelihood_ratio, 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ec87f-c368-415c-b501-69b59ca46b01",
   "metadata": {},
   "source": [
    "# Dependency Parsing\n",
    "\n",
    "Lets do a dependency parse of the word 'model' in these sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb01a19-e1a6-4e98-9f42-355b8229c524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33712060-f5d4-4d20-a35f-e5680378931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adefdb2-0e30-45cb-9525-cd6a48d61635",
   "metadata": {},
   "source": [
    "Let's look at the spacy attributes for one sentence from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2f8f7e1-5f59-4944-ad3d-9d9942c95554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                     Pos-Tag    Dep        Head                 Pos-Head   NER        Lefts                                              Rights                                            \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "In                        ADP        prep       has                  VERB                                                                     experiment                                        \n",
      "our                       PRON       poss       experiment           NOUN                                                                                                                       \n",
      "experiment                NOUN       pobj       In                   ADP                   our                                                                                                  \n",
      ",                         PUNCT      punct      has                  VERB                                                                                                                       \n",
      "the                       DET        det        model                NOUN                                                                                                                       \n",
      "model                     NOUN       nsubj      has                  VERB                  the                                                trained-,                                         \n",
      "trained                   VERB       acl        model                NOUN                                                                     on                                                \n",
      "on                        ADP        prep       trained              VERB                                                                     model                                             \n",
      "a                         DET        det        model                NOUN                                                                                                                       \n",
      "largest                   ADJ        amod       model                NOUN                                                                                                                       \n",
      "data                      NOUN       nmod       model                NOUN                                                                                                                       \n",
      "i.e.                      X          amod       model                NOUN                                                                                                                       \n",
      ",                         PUNCT      punct      model                NOUN                                                                                                                       \n",
      "English                   ADJ        amod       model                NOUN       LANGUAGE                                                                                                        \n",
      "model                     NOUN       pobj       on                   ADP                   a-largest-data-i.e.-,-English                                                                        \n",
      ",                         PUNCT      punct      model                NOUN                                                                                                                       \n",
      "has                       VERB       advcl      has                  VERB                  In-,-model                                         WER                                               \n",
      "the                       DET        det        WER                  PROPN                                                                                                                      \n",
      "lowest                    ADJ        amod       WER                  PROPN                                                                                                                      \n",
      "WER                       PROPN      dobj       has                  VERB       ORG        the-lowest                                                                                           \n",
      ",                         PUNCT      punct      has                  VERB                                                                                                                       \n",
      "whereas                   SCONJ      advmod     has                  VERB                                                                                                                       \n",
      ",                         PUNCT      punct      has                  VERB                                                                                                                       \n",
      "the                       DET        det        model                NOUN                                                                                                                       \n",
      "model                     NOUN       nsubj      has                  VERB                  the                                                trained-,                                         \n",
      "trained                   VERB       acl        model                NOUN                                                                     on                                                \n",
      "on                        ADP        prep       trained              VERB                                                                     data                                              \n",
      "the                       DET        det        data                 NOUN                                                                                                                       \n",
      "smallest                  ADJ        amod       data                 NOUN                                                                                                                       \n",
      "data                      NOUN       pobj       on                   ADP                   the-smallest                                       i.e.-Amharic                                      \n",
      "i.e.                      X          advmod     data                 NOUN                                                                     ,                                                 \n",
      ",                         PUNCT      punct      i.e.                 X                                                                                                                          \n",
      "Amharic                   PROPN      appos      data                 NOUN       ORG                                                                                                             \n",
      ",                         PUNCT      punct      model                NOUN                                                                                                                       \n",
      "has                       VERB       ROOT       has                  VERB                  has-,-whereas-,-model                              WER-.                                             \n",
      "the                       DET        det        WER                  PROPN                                                                                                                      \n",
      "highest                   ADJ        amod       WER                  PROPN                                                                                                                      \n",
      "WER                       PROPN      dobj       has                  VERB       ORG        the-highest                                                                                          \n",
      ".                         PUNCT      punct      has                  VERB                                                                                                                       \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"In our experiment, the model trained on a largest data i.e., English model, has the lowest WER, whereas, the model trained on the smallest data i.e., Amharic, has the highest WER.\")\n",
    "print(\"{:25} {:10} {:10} {:20} {:10} {:10} {:50} {:50}\".format('Token','Pos-Tag','Dep','Head','Pos-Head','NER', 'Lefts', 'Rights'))\n",
    "print(\"_\" * 150)\n",
    "for token in doc:\n",
    "    print(\"{:25} {:10} {:10} {:20} {:10} {:10} {:50} {:50}\".format(token.text, token.pos_ , token.dep_, token.head.text, token.head.pos_, token.ent_type_,  \"-\".join([w.text for w in token.lefts]), \"-\".join([w.text for w in token.rights])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc249292-6972-4091-922e-d2d6fa151c62",
   "metadata": {},
   "source": [
    "We want to extract the dependency info for model where we have model in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53eb3778-86cc-4e35-8577-6f66ecd3c636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "868208it [2:55:06, 82.63it/s] \n"
     ]
    }
   ],
   "source": [
    "spacy_docs = []\n",
    "for idx, row in tqdm(data.iterrows()):\n",
    "    doc = nlp(row.sentence)\n",
    "    spacy_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7327f9b0-a384-45b8-87c5-21c6bbd3e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Open a file and use dump() \n",
    "# # with open('model_tokens_parsed.pkl', 'wb') as file: \n",
    "      \n",
    "# #     # A new file will be created \n",
    "# #     pickle.dump(spacy_docs, file) \n",
    "\n",
    "# # with open('model_tokens_parsed.pkl', 'wb') as file: \n",
    "      \n",
    "# #     # A new file will be created \n",
    "# #     pickle.dump(spacy_docs, file) \n",
    "\n",
    "\n",
    "\n",
    "# ##########\n",
    "# # with open('model_tokens_parsed.pkl', 'rb') amodel_tokens[0].leftss file: \n",
    "      \n",
    "# #     spacy_docs = pickle.load(file) \n",
    "\n",
    "\n",
    "# spacy_docs = pd.read_pickle(r'model_tokens_parsed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3515a123-ffcf-4874-8e64-719f618a6622",
   "metadata": {},
   "source": [
    "Now we want to extract some of the token attributes into a dataframe of potentially useful information about each usage of the target word 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2810c662-4c59-40d0-bd8e-92e6ad181808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get out the token info for just the model tokens\n",
    "model_tokens = []\n",
    "for spacy_doc in spacy_docs:\n",
    "    model_toks = [tok for tok in spacy_doc if tok.text == \"model\"]\n",
    "    token = model_toks[0] # extract the first token of 'model' in the sentence\n",
    "    token_attributes = {\n",
    "        \"token_text\": token.text,\n",
    "        \"token_pos\": token.pos_ ,\n",
    "        \"token_dep\": token.dep_,\n",
    "        \"token_head_text\": token.head.text,\n",
    "        \"token_head_pos\": token.head.pos_,\n",
    "        \"token_ent_type\": token.ent_type_, \n",
    "        \"token_lefts\": \"-\".join([w.text for w in token.lefts]),\n",
    "        \"token_rights\": \"-\".join([w.text for w in token.rights])\n",
    "    }\n",
    "    \n",
    "    model_tokens.append(token_attributes) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca54374-59c7-43f4-861e-1aa93a1622c1",
   "metadata": {},
   "source": [
    "Confirm we got spaCy token data looking how we want in a nice cute dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fef4ed1f-cec2-4503-82c3-ac1c2d63c2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_text': 'model',\n",
       " 'token_pos': 'NOUN',\n",
       " 'token_dep': 'pobj',\n",
       " 'token_head_text': 'on',\n",
       " 'token_head_pos': 'ADP',\n",
       " 'token_ent_type': '',\n",
       " 'token_lefts': 'the-vector-space',\n",
       " 'token_rights': ''}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3da1ba3-5713-409b-9647-e6a7f0dc70b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "token_text                    model\n",
       "token_pos                      NOUN\n",
       "token_dep                      pobj\n",
       "token_head_text                  on\n",
       "token_head_pos                  ADP\n",
       "token_ent_type                     \n",
       "token_lefts        the-vector-space\n",
       "token_rights                       \n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to a dataframe\n",
    "\n",
    "tokens_df = pd.DataFrame.from_records(model_tokens)\n",
    "tokens_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87628cb1-7ed7-45b9-a39e-728940993260",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df.to_csv('model_spacy_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d62726d3-1787-47a4-9049-f8422aa9714a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>corpus_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_idx</th>\n",
       "      <th>acl_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>full_text</th>\n",
       "      <th>pdf_hash</th>\n",
       "      <th>numcitedby</th>\n",
       "      <th>...</th>\n",
       "      <th>isbn</th>\n",
       "      <th>decade</th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_pos</th>\n",
       "      <th>token_dep</th>\n",
       "      <th>token_head_text</th>\n",
       "      <th>token_head_pos</th>\n",
       "      <th>token_ent_type</th>\n",
       "      <th>token_lefts</th>\n",
       "      <th>token_rights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18022704</td>\n",
       "      <td>Since the similarity measure based on the vector space model is a rough estimation, minor errors made at the stage of context vector extraction are acceptable.</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>O02-2002</td>\n",
       "      <td>There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required an...</td>\n",
       "      <td>There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required an...</td>\n",
       "      <td>0b09178ac8d17a92f16140365363d8df88c757d0</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>(2000, 2005]</td>\n",
       "      <td>model</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>on</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td>the-vector-space</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  corpus_id  \\\n",
       "0           0   18022704   \n",
       "\n",
       "                                                                                                                                                          sentence  \\\n",
       "0  Since the similarity measure based on the vector space model is a rough estimation, minor errors made at the stage of context vector extraction are acceptable.   \n",
       "\n",
       "   start_idx  end_idx    acl_id  \\\n",
       "0          9       10  O02-2002   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              abstract  \\\n",
       "0  There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required an...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             full_text  \\\n",
       "0  There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required an...   \n",
       "\n",
       "                                   pdf_hash  numcitedby  ...  isbn  \\\n",
       "0  0b09178ac8d17a92f16140365363d8df88c757d0          14  ...  None   \n",
       "\n",
       "         decade token_text  token_pos token_dep token_head_text  \\\n",
       "0  (2000, 2005]      model       NOUN      pobj              on   \n",
       "\n",
       "  token_head_pos token_ent_type       token_lefts token_rights  \n",
       "0            ADP                 the-vector-space               \n",
       "\n",
       "[1 rows x 34 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join the datadrame to our data\n",
    "\n",
    "\n",
    "tokens_data = pd.concat([data, tokens_df], axis=1)\n",
    "tokens_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c81d7c-1d1a-434b-88f2-14420d0a5048",
   "metadata": {},
   "source": [
    "What are the different types of dependency relations 'model' shows up in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62642c13-e51d-4aa0-9e30-f4161c3b3b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pobj', 'nsubjpass', 'nsubj', 'dobj', 'attr', 'conj', 'ROOT',\n",
       "       'compound', 'appos', 'npadvmod', 'xcomp', 'poss', 'nmod', 'advcl',\n",
       "       'oprd', 'ccomp', 'acl', 'amod', 'punct', 'dative', 'relcl', 'dep',\n",
       "       'acomp', 'meta', 'pcomp', 'parataxis', 'advmod', 'csubj',\n",
       "       'csubjpass', 'agent', 'intj'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_data.token_dep.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556f1c6-8cff-4b8e-a000-2588cbca675b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70ec29c5-b324-40b4-9e21-90c95fd5102d",
   "metadata": {},
   "source": [
    "### Analyzing the 'Subjectivity' of 'Model'\n",
    "\n",
    "Now that we finally have data in the right shape, we are set up to ask, for instance, the following:\n",
    "\n",
    "For a given decade, what is the proportion of usages of 'model' used as a noun or a verb?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c5383ff-0743-47a7-b05f-e5c896a2bee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>token_pos</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>VERB</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decade</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1950, 1960]</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1960, 1970]</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1970, 1980]</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1980, 1990]</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1990, 2000]</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2000, 2005]</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2005, 2010]</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2010, 2012]</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2012, 2014]</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2014, 2016]</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2016, 2018]</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2018, 2020]</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "token_pos      ADJ  ADV  NOUN  PROPN  VERB\n",
       "decade                                    \n",
       "(1950, 1960]  0.02  0.0  0.97    0.0  0.02\n",
       "(1960, 1970]  0.00  0.0  0.97    0.0  0.03\n",
       "(1970, 1980]  0.00  0.0  0.96    0.0  0.03\n",
       "(1980, 1990]  0.01  0.0  0.95    0.0  0.04\n",
       "(1990, 2000]  0.00  0.0  0.96    0.0  0.04\n",
       "(2000, 2005]  0.00  0.0  0.97    0.0  0.03\n",
       "(2005, 2010]  0.00  0.0  0.96    0.0  0.03\n",
       "(2010, 2012]  0.00  0.0  0.97    0.0  0.03\n",
       "(2012, 2014]  0.00  0.0  0.96    0.0  0.04\n",
       "(2014, 2016]  0.00  0.0  0.96    0.0  0.04\n",
       "(2016, 2018]  0.00  0.0  0.97    0.0  0.03\n",
       "(2018, 2020]  0.00  0.0  0.97    0.0  0.02"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Group by year and property, then count occurrences\n",
    "count_df = tokens_data.groupby(['decade', 'token_pos']).size().reset_index(name='count')\n",
    "\n",
    "# # Step 2: Calculate total counts for each year\n",
    "total_counts = tokens_data.groupby('decade').size().reset_index(name='total')\n",
    "total_counts\n",
    "\n",
    "# # Step 3: Merge the counts with the total counts\n",
    "merged_df = pd.merge(count_df, total_counts, on='decade')\n",
    "\n",
    "# # Step 4: Calculate proportions\n",
    "merged_df['proportion'] = merged_df['count'] / merged_df['total']\n",
    "merged_df['proportion'] = merged_df['proportion'].round(2)\n",
    "\n",
    "# merged_df\n",
    "\n",
    "# # Display the final result\n",
    "#print(merged_df[['decade', 'token_dep', 'proportion']])\n",
    "\n",
    "pivot = pd.pivot_table(merged_df, values='proportion', index=['decade'],\n",
    "                       columns=['token_pos'], aggfunc=\"sum\")\n",
    "pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f312a701-3259-4896-9fc8-e64910ccdf70",
   "metadata": {},
   "source": [
    "for a given decade, what is the proportion of usages of 'model' that appear as a subject versus an object, or some other syntactic position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "813913d8-8f48-4337-a418-165ee1d4fcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>token_dep</th>\n",
       "      <th>ROOT</th>\n",
       "      <th>acl</th>\n",
       "      <th>acomp</th>\n",
       "      <th>advcl</th>\n",
       "      <th>advmod</th>\n",
       "      <th>agent</th>\n",
       "      <th>amod</th>\n",
       "      <th>appos</th>\n",
       "      <th>attr</th>\n",
       "      <th>ccomp</th>\n",
       "      <th>...</th>\n",
       "      <th>nsubj</th>\n",
       "      <th>nsubjpass</th>\n",
       "      <th>oprd</th>\n",
       "      <th>parataxis</th>\n",
       "      <th>pcomp</th>\n",
       "      <th>pobj</th>\n",
       "      <th>poss</th>\n",
       "      <th>punct</th>\n",
       "      <th>relcl</th>\n",
       "      <th>xcomp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decade</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1950, 1960]</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1960, 1970]</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1970, 1980]</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1980, 1990]</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1990, 2000]</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2000, 2005]</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2005, 2010]</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2010, 2012]</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2012, 2014]</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2014, 2016]</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2016, 2018]</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2018, 2020]</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "token_dep     ROOT   acl  acomp  advcl  advmod  agent  amod  appos  attr  \\\n",
       "decade                                                                     \n",
       "(1950, 1960]  0.00  0.02    0.0   0.00     0.0    0.0  0.03   0.00  0.00   \n",
       "(1960, 1970]  0.00  0.00    0.0   0.00     0.0    0.0  0.00   0.02  0.03   \n",
       "(1970, 1980]  0.04  0.01    0.0   0.00     0.0    0.0  0.01   0.05  0.04   \n",
       "(1980, 1990]  0.01  0.00    0.0   0.01     0.0    0.0  0.02   0.01  0.02   \n",
       "(1990, 2000]  0.01  0.00    0.0   0.01     0.0    0.0  0.00   0.02  0.02   \n",
       "(2000, 2005]  0.01  0.00    0.0   0.00     0.0    0.0  0.00   0.02  0.02   \n",
       "(2005, 2010]  0.02  0.00    0.0   0.01     0.0    0.0  0.00   0.02  0.02   \n",
       "(2010, 2012]  0.02  0.00    0.0   0.00     0.0    0.0  0.00   0.02  0.02   \n",
       "(2012, 2014]  0.02  0.00    0.0   0.01     0.0    0.0  0.00   0.02  0.02   \n",
       "(2014, 2016]  0.02  0.00    0.0   0.01     0.0    0.0  0.00   0.02  0.02   \n",
       "(2016, 2018]  0.02  0.00    0.0   0.01     0.0    0.0  0.00   0.02  0.02   \n",
       "(2018, 2020]  0.02  0.00    0.0   0.00     0.0    0.0  0.00   0.02  0.02   \n",
       "\n",
       "token_dep     ccomp  ...  nsubj  nsubjpass  oprd  parataxis  pcomp  pobj  \\\n",
       "decade               ...                                                   \n",
       "(1950, 1960]   0.00  ...   0.00       0.05   0.0        0.0    0.0  0.17   \n",
       "(1960, 1970]   0.02  ...   0.22       0.06   0.0        0.0    0.0  0.44   \n",
       "(1970, 1980]   0.00  ...   0.14       0.07   0.0        0.0    0.0  0.34   \n",
       "(1980, 1990]   0.00  ...   0.16       0.08   0.0        0.0    0.0  0.38   \n",
       "(1990, 2000]   0.00  ...   0.18       0.07   0.0        0.0    0.0  0.36   \n",
       "(2000, 2005]   0.00  ...   0.19       0.07   0.0        0.0    0.0  0.33   \n",
       "(2005, 2010]   0.00  ...   0.19       0.07   0.0        0.0    0.0  0.32   \n",
       "(2010, 2012]   0.00  ...   0.18       0.06   0.0        0.0    0.0  0.31   \n",
       "(2012, 2014]   0.00  ...   0.19       0.06   0.0        0.0    0.0  0.30   \n",
       "(2014, 2016]   0.00  ...   0.21       0.06   0.0        0.0    0.0  0.28   \n",
       "(2016, 2018]   0.00  ...   0.24       0.05   0.0        0.0    0.0  0.28   \n",
       "(2018, 2020]   0.00  ...   0.23       0.05   0.0        0.0    0.0  0.27   \n",
       "\n",
       "token_dep     poss  punct  relcl  xcomp  \n",
       "decade                                   \n",
       "(1950, 1960]  0.00   0.00   0.00   0.00  \n",
       "(1960, 1970]  0.00   0.00   0.01   0.00  \n",
       "(1970, 1980]  0.01   0.01   0.00   0.02  \n",
       "(1980, 1990]  0.00   0.00   0.00   0.02  \n",
       "(1990, 2000]  0.01   0.00   0.00   0.02  \n",
       "(2000, 2005]  0.01   0.00   0.00   0.01  \n",
       "(2005, 2010]  0.01   0.00   0.00   0.01  \n",
       "(2010, 2012]  0.01   0.00   0.00   0.01  \n",
       "(2012, 2014]  0.01   0.00   0.00   0.01  \n",
       "(2014, 2016]  0.01   0.00   0.00   0.01  \n",
       "(2016, 2018]  0.01   0.00   0.00   0.01  \n",
       "(2018, 2020]  0.02   0.00   0.00   0.01  \n",
       "\n",
       "[12 rows x 31 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Group by year and property, then count occurrences\n",
    "count_df = tokens_data.groupby(['decade', 'token_dep']).size().reset_index(name='count')\n",
    "\n",
    "# # Step 2: Calculate total counts for each year\n",
    "total_counts = tokens_data.groupby('decade').size().reset_index(name='total')\n",
    "total_counts\n",
    "\n",
    "# # Step 3: Merge the counts with the total counts\n",
    "merged_df = pd.merge(count_df, total_counts, on='decade')\n",
    "\n",
    "# # Step 4: Calculate proportions\n",
    "merged_df['proportion'] = merged_df['count'] / merged_df['total']\n",
    "merged_df['proportion'] = merged_df['proportion'].round(2)\n",
    "\n",
    "# merged_df\n",
    "\n",
    "# # Display the final result\n",
    "#print(merged_df[['decade', 'token_dep', 'proportion']])\n",
    "\n",
    "pivot = pd.pivot_table(merged_df, values='proportion', index=['decade'],\n",
    "                       columns=['token_dep'], aggfunc=\"sum\")\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e6248-0158-44cd-b72e-cbd60d491772",
   "metadata": {},
   "outputs": [],
   "source": [
    "This analysis looks at all uses of model, including verbal uses. What happens when we focus only on nominal uses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42b7b2-a2a8-4d33-baf9-62e6987b4b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by year and property, then count occurrences\n",
    "noun_data = tokens_data[tokens_data.token_pos != \"VERB\"]\n",
    "count_df = noun_data.groupby(['decade', 'token_dep']).size().reset_index(name='count')\n",
    "\n",
    "# # Step 2: Calculate total counts for each year\n",
    "total_counts = noun_data.groupby('decade').size().reset_index(name='total')\n",
    "total_counts\n",
    "\n",
    "# # Step 3: Merge the counts with the total counts\n",
    "merged_df = pd.merge(count_df, total_counts, on='decade')\n",
    "\n",
    "# # Step 4: Calculate proportions\n",
    "merged_df['proportion'] = merged_df['count'] / merged_df['total']\n",
    "merged_df['proportion'] = merged_df['proportion'].round(2)\n",
    "\n",
    "# merged_df\n",
    "\n",
    "# # Display the final result\n",
    "#print(merged_df[['decade', 'token_dep', 'proportion']])\n",
    "\n",
    "pivot = pd.pivot_table(merged_df, values='proportion', index=['decade'],\n",
    "                       columns=['token_dep'], aggfunc=\"sum\")\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1e523-53a7-46f0-b0fe-d335bc7965f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
